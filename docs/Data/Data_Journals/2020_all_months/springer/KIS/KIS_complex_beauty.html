<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>KIS_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="kis---163">KIS - 163</h2>
<ul>
<li><details>
<summary>
(2020). Big high-dimension data cube designs for hybrid memory
systems. <em>KIS</em>, <em>62</em>(12), 4717–4746. (<a
href="https://doi.org/10.1007/s10115-020-01505-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In Big Data cubes with hundreds of dimensions and billions of tuples, the indexing and query operations are a challenge and the reason is the time-space exponential complexity when a full cube is computed. Therefore, solutions based on RAM may not be practical and the solutions based on hybrid memory (RAM and disk) become viable alternatives. In this paper, we propose a hybrid approach, named bCubing, to index and query high-dimension data cubes with high number of tuples in a single machine and using RAM and disk memory systems. We evaluated bCubing in terms of runtime and memory consumption, comparing it with the Frag-Cubing, HIC and H-Frag approaches. bCubing showed to be faster and used less RAM than Frag-Cubing, HIC and H-Frag. bCubing indexed and allowed to query a data cube with 1.2 billion tuples and 60 dimensions, consuming only 84 GB of RAM, which means 35% less memory than HIC. The complex holistic measures mode and median were computed in multidimensional queries, and bCubing was, on average, 50% faster than HIC.},
  archive      = {J_KIS},
  author       = {Silva, Rodrigo Rocha and Hirata, Celso Massaki and de Castro Lima, Joubert},
  doi          = {10.1007/s10115-020-01505-9},
  journal      = {Knowledge and Information Systems},
  month        = {12},
  number       = {12},
  pages        = {4717-4746},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Big high-dimension data cube designs for hybrid memory systems},
  volume       = {62},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Automatic role identification for research teams with
ranking multi-view machines. <em>KIS</em>, <em>62</em>(12), 4681–4716.
(<a href="https://doi.org/10.1007/s10115-020-01504-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Research teams have been well recognized as the norm in modern scientific discovery. Rather than a loose collection of researchers, a well-performing research team is composed of a number of researchers, each of them playing a particular role (i.e., principal investigator, sub-investigator or research staff) for a short- or long-term effort. Role analysis for research teams would help gain insights into the dynamics of teams and the behavior of their members. In this paper, we address the problem of research role identification for large research institutes in which similar yet separated teams coexist. In particular, we represent a research team as teamwork networks and generate the feature representation of each member using a number of network metrics. Afterward, we propose RankMVM, short for Ranking Multi-View Machines, to learn the role identification model. Compared with traditional predictive models, RankMVM is advantageous in exploring high-order feature interactions in an efficient way, as well as handling the specific challenges of the research role identification task, including partially ordered learning targets and sparse feature interactions. In the experiments, we assess the performance on a real-world research team dataset. Extensive experimental evaluations verify the advantages of our proposed research role identification approach.},
  archive      = {J_KIS},
  author       = {Ni, Weijian and Guo, Haoyu and Liu, Tong and Zeng, Qingtian},
  doi          = {10.1007/s10115-020-01504-w},
  journal      = {Knowledge and Information Systems},
  month        = {12},
  number       = {12},
  pages        = {4681-4716},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Automatic role identification for research teams with ranking multi-view machines},
  volume       = {62},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). SLA-driven resource re-allocation for SQL-like queries in
the cloud. <em>KIS</em>, <em>62</em>(12), 4653–4680. (<a
href="https://doi.org/10.1007/s10115-020-01501-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cloud computing has become a widely used environment for database querying. In this context, the goal of a query optimizer is to satisfy the needs of tenants and maximize the provider’s benefit. Resource allocation is an important step toward achieving this goal. Allocation methods are based on analytical formulas and statistics collected from a catalog to estimate the cost of various possible allocations and then choose the best one. However, the allocation initially chosen is not necessarily the optimal one because of the approximate nature of the analytical formulas and the fact that the catalog may not be up to date. To solve this problem, existing work was proposed to collect statistics during the execution of the query and then trigger a re-allocation if suboptimality is detected. However, these proposals consider that queries have the same level of priority. Unlike the existing work, we propose in this paper a method of statistics collector placement and resource re-allocation by taking into account that the cloud is a multi-tenant environment and queries have different services-level agreements. In the experimental section, we show that our method provides a better benefit for the provider compared to state-of-the-art methods.},
  archive      = {J_KIS},
  author       = {Kandi, Mohamed Mehdi and Yin, Shaoyi and Hameurlain, Abdelkader},
  doi          = {10.1007/s10115-020-01501-z},
  journal      = {Knowledge and Information Systems},
  month        = {12},
  number       = {12},
  pages        = {4653-4680},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {SLA-driven resource re-allocation for SQL-like queries in the cloud},
  volume       = {62},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Cross-domain recommender system using generalized canonical
correlation analysis. <em>KIS</em>, <em>62</em>(12), 4625–4651. (<a
href="https://doi.org/10.1007/s10115-020-01499-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recommender systems provide personalized recommendations to the users from a large number of possible options in online stores. Matrix factorization is a well-known and accurate collaborative filtering approach for recommender system, which suffers from cold-start problem for new users and items. When new users join the system, it will take some time before they enter some ratings in the system, until that time, there are not enough ratings to learn the matrix factorization model. Using auxiliary data such as user’s demographic, ratings and reviews in relevant domains, is an effective solution to reduce the new user problem. In this paper, we used the data of users activity from auxiliary domains to build domain-independent users representation that could be used to predict users ratings in the target domains. We proposed an iterative method which applied MAX-VAR generalized canonical correlation analysis (GCCA) on user’s latent factors learned from matrix factorization on each domain. Also, to improve the capability of GCCA to learn latent factors for new users, we propose a generalized canonical correlation analysis by inverse sum of selection matrices (GCCA-ISSM) approach, which provides better recommendations in cold-start scenarios. The proposed approach is extended using content-based features like topic models extracted from user’s reviews. We demonstrate the accuracy and effectiveness of the proposed approaches on cross-domain rating predictions using comprehensive experiments on Amazon and MovieLens datasets.},
  archive      = {J_KIS},
  author       = {Hashemi, Seyed Mohammad and Rahmati, Mohammad},
  doi          = {10.1007/s10115-020-01499-4},
  journal      = {Knowledge and Information Systems},
  month        = {12},
  number       = {12},
  pages        = {4625-4651},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Cross-domain recommender system using generalized canonical correlation analysis},
  volume       = {62},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Exploring the collective human behavior in cascading
systems: A comprehensive framework. <em>KIS</em>, <em>62</em>(12),
4599–4623. (<a
href="https://doi.org/10.1007/s10115-020-01506-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The collective behavior describing spontaneously emerging social processes and events is ubiquitous in both physical society and online social media. The knowledge of collective behavior is critical in understanding and predicting social movements, fads, riots, and so on. However, detecting, quantifying, and modeling the collective behavior in cascading systems at large scale are seldom explored. In this paper, we examine a real-world online social media with more than 1.7 million information spreading records. We observe evident collective behavior in information cascading systems and then propose metrics to quantify the collectivity. We find that previous information cascading models cannot capture the collective behavior in the real-world data and thus never utilize it. Furthermore, we propose a comprehensive generative framework with a latent user interest layer to capture the collective behavior. Our framework accurately models the information cascades with respect to dynamics, popularity, structure, and collectivity. By leveraging the knowledge behind collective behavior, our model successfully predicts the popularity and participants of information cascades without temporal features or early stage information. Our framework may serve as a more generalized one in modeling cascading systems, and, together with empirical discovery and applications, advance our understanding of human behavior.},
  archive      = {J_KIS},
  author       = {Lu, Yunfei and Yu, Linyun and Zhang, Tianyang and Zang, Chengxi and Cui, Peng and Song, Chaoming and Zhu, Wenwu},
  doi          = {10.1007/s10115-020-01506-8},
  journal      = {Knowledge and Information Systems},
  month        = {12},
  number       = {12},
  pages        = {4599-4623},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Exploring the collective human behavior in cascading systems: A comprehensive framework},
  volume       = {62},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A fuzzy-AHP-based approach to select software architecture
based on quality attributes (FASSA). <em>KIS</em>, <em>62</em>(12),
4569–4597. (<a
href="https://doi.org/10.1007/s10115-020-01496-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The software system design phase has recently received increasing attention due to continuous growth in both the size and complexity of software systems. As a key concept of this phase, software architecture plays an important role in the software extension cycle to the extent that the success of a software project is often determined by the degree of its design efficiency. In addition, software architecture evaluation is a fundamental step toward its subsequent validation. This paper is an attempt to propose an innovative method, based on fuzzy logic, to evaluate software architecture that addresses the inherent problems of existing methods found in the literature. The method can be used for complete design or even reconstruction of the architecture. Given the multi-faceted nature of the problem of evaluation and selection of an optimal architecture, we have employed a multi-objective decision technique, namely fuzzy hierarchical analysis process, which solves the problems associated with uncertainties and inaccuracies by incorporating fuzzy logic.},
  archive      = {J_KIS},
  author       = {Moaven, Shahrouz and Habibi, Jafar},
  doi          = {10.1007/s10115-020-01496-7},
  journal      = {Knowledge and Information Systems},
  month        = {12},
  number       = {12},
  pages        = {4569-4597},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {A fuzzy-AHP-based approach to select software architecture based on quality attributes (FASSA)},
  volume       = {62},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Deep end-to-end learning for price prediction of second-hand
items. <em>KIS</em>, <em>62</em>(12), 4541–4568. (<a
href="https://doi.org/10.1007/s10115-020-01495-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent years have witnessed the rapid development of online shopping and ecommerce websites, e.g., eBay and OLX. Online shopping markets offer millions of products for sale each day. These products are categorized into many product categories. It is crucial for sellers to correctly estimate the price of the second-hand item. State-of-the-art methods can predict the price of only one item category. In addition, none of the existing methods utilized the price range of a given second-hand item in the prediction task, as there are several advertisements for the same product at different prices. In this vein, as the first contribution, we propose a deep model architecture for predicting the price of a second-hand item based on the image and textual description of the item for different sets of item types. This proposed method utilizes a deep neural network involving long short-term memory (LSTM) and convolutional neural network architectures for price prediction. The proposed model achieved a better mean absolute error accuracy score in comparison with the support vector machine baseline model. In addition, the second contribution includes twofold. First, we propose forecasting the minimum and maximum prices of the second-hand item. The models used for the forecasting task utilize linear regression, LSTM, and seasonal autoregressive integrated moving average methods. Second, we propose utilizing the model of the first contribution in predicting the item quality score. Then, the item quality score and the forecasted minimum and maximum prices are combined to provide the item’s final predicted price. Using a dataset crawled from a website for second-hand items, the proposed method of combining the predicted second-hand item quality score with the forecasted minimum and maximum price outperforms the other models in all of the used accuracy metrics with a significant performance gap.},
  archive      = {J_KIS},
  author       = {Fathalla, Ahmed and Salah, Ahmad and Li, Kenli and Li, Keqin and Francesco, Piccialli},
  doi          = {10.1007/s10115-020-01495-8},
  journal      = {Knowledge and Information Systems},
  month        = {12},
  number       = {12},
  pages        = {4541-4568},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Deep end-to-end learning for price prediction of second-hand items},
  volume       = {62},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Surface pattern-enhanced relation extraction with global
constraints. <em>KIS</em>, <em>62</em>(12), 4509–4540. (<a
href="https://doi.org/10.1007/s10115-020-01502-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Relation extraction is one of the most important tasks in information extraction. The traditional works either use sentences or surface patterns (i.e., the shortest dependency paths of sentences) to build extraction models. Intuitively, the integration of these two kinds of methods will further obtain more robust and effective extraction models, which is, however, ignored in most of the existing works. In this paper, we aim to learn the embeddings of surface patterns to further augment the sentence-based models. To achieve this purpose, we propose a novel pattern embedding learning framework with the weighted multi-dimensional attention mechanism. To suppress noise in the training dataset, we mine the global statistics between patterns and relations and introduce two kinds of prior knowledge to guide the pattern embedding learning. Based on the learned embeddings, we present two augmentation strategies to improve the existing relation extraction models. We conduct extensive experiments on two popular datasets (i.e., NYT and KnowledgeNet) and observe promising performance improvements.},
  archive      = {J_KIS},
  author       = {Jiang, Haiyun and Liu, JunTao and Zhang, Sheng and Yang, Deqing and Xiao, Yanghua and Wang, Wei},
  doi          = {10.1007/s10115-020-01502-y},
  journal      = {Knowledge and Information Systems},
  month        = {12},
  number       = {12},
  pages        = {4509-4540},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Surface pattern-enhanced relation extraction with global constraints},
  volume       = {62},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A descriptive framework for the field of knowledge
management. <em>KIS</em>, <em>62</em>(12), 4481–4508. (<a
href="https://doi.org/10.1007/s10115-020-01492-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Despite the extensive evolution of knowledge management (KM), the field lacks an integrated description. This situation leads to difficulties in research, teaching, and learning. To bridge this gap, this study surveys 2842 articles from top-ranked KM journals to provide a descriptive framework that guides future research in the field of knowledge management. This study also seeks to provide a comprehensive depiction of current research in the field and categorizes these research activities into higher-level categories using grounded theory approach and topic modeling technique. The results show that KM studies are classified into four core research categories: technological, business, people, and domains/applications dimensions. An additional concern addressed in this study is the major research methodologies used in this field. The results raise awareness of the development of KM discipline and hold implications for research methodologies and research trends in the selected KM journals. The results obtained from this study also provide practitioners with a useful quality reference source. The framework and the components included provide researchers, practitioners, and educators with an ontology of KM topics, where they can cover deficiencies in research and provide an agenda for future research.},
  archive      = {J_KIS},
  author       = {Harb, Yousra and Abu-Shanab, Emad},
  doi          = {10.1007/s10115-020-01492-x},
  journal      = {Knowledge and Information Systems},
  month        = {12},
  number       = {12},
  pages        = {4481-4508},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {A descriptive framework for the field of knowledge management},
  volume       = {62},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Sentiment lexicons and non-english languages: A survey.
<em>KIS</em>, <em>62</em>(12), 4445–4480. (<a
href="https://doi.org/10.1007/s10115-020-01497-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The ever-increasing number of Internet users and online services, such as Amazon, Twitter and Facebook has rapidly motivated people to not just transact using the Internet but to also voice their opinions about products, services, policies, etc. Sentiment analysis is a field of study to extract and analyze public views and opinions. However, current research within this field mainly focuses on building systems and resources using the English language. The primary objective of this study is to examine existing research in building sentiment lexicon systems and to classify the methods with respect to non-English datasets. Additionally, the study also reviewed the tools used to build sentiment lexicons for non-English languages, ranging from those using machine translation to graph-based methods. Shortcomings are highlighted with the approaches along with recommendations to improve the performance of each approach and areas for further study and research.},
  archive      = {J_KIS},
  author       = {Kaity, Mohammed and Balakrishnan, Vimala},
  doi          = {10.1007/s10115-020-01497-6},
  journal      = {Knowledge and Information Systems},
  month        = {12},
  number       = {12},
  pages        = {4445-4480},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Sentiment lexicons and non-english languages: A survey},
  volume       = {62},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Multi-objective bonobo optimizer (MOBO): An intelligent
heuristic for multi-criteria optimization. <em>KIS</em>,
<em>62</em>(11), 4407–4444. (<a
href="https://doi.org/10.1007/s10115-020-01503-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Non-traditional optimization tools have proved their potential in solving various types of optimization problems. These problems deal with either single objective or multiple/many objectives. Bonobo Optimizer (BO) is an intelligent and adaptive metaheuristic optimization algorithm inspired from the social behavior and reproductive strategies of bonobos. There is no study in the literature to extend this BO to solve multi-objective optimization problems. This paper presents a Multi-objective Bonobo Optimizer (MOBO) to solve different optimization problems. Three different versions of MOBO are proposed in this paper, each using a different method, such as non-dominated sorting with adaptation of grid approach; a ranking scheme for sorting of population with crowding distance approach; decomposition technique, wherein the solutions are obtained by dividing a multi-objective problem into a number of single-objective problems. The performances of all three different versions of the proposed MOBO had been tested on a set of thirty diversified benchmark test functions, and the results were compared with that of four other well-known multi-objective optimization techniques available in the literature. The obtained results showed that the first two versions of the proposed algorithms either outperformed or performed competitively in terms of convergence and diversity compared to the others. However, the third version of the proposed techniques was found to have the poor performance.},
  archive      = {J_KIS},
  author       = {Das, Amit Kumar and Nikum, Ankit Kumar and Krishnan, Siva Vignesh and Pratihar, Dilip Kumar},
  doi          = {10.1007/s10115-020-01503-x},
  journal      = {Knowledge and Information Systems},
  month        = {11},
  number       = {11},
  pages        = {4407-4444},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Multi-objective bonobo optimizer (MOBO): An intelligent heuristic for multi-criteria optimization},
  volume       = {62},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Pythagorean fuzzy MULTIMOORA method based on distance
measure and score function: Its application in multicriteria decision
making process. <em>KIS</em>, <em>62</em>(11), 4373–4406. (<a
href="https://doi.org/10.1007/s10115-020-01491-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The MULTIMOORA method is better than some of the existing decision making methods. However, it has not been improved to process Pythagorean fuzzy sets (PFSs). The decision results of the MULTIMOORA method greatly depend on the distance measure and score function. Although there are many studies focusing on proposing distance measures and score functions for PFSs, they still show some defects. In this paper, we propose two novel distance measures and a novel score function for PFSs for proposing a novel Pythagorean fuzzy MULTIMOORA method. To this end, two distance measures, Dice distance and Jaccard distance, are proposed for computing the deviation degree between two PFSs, and their general forms are also discussed. Afterward, a novel score function based on determinacy degree and indeterminacy degree is put forward for approximately representing PFSs. Then, the original MULTIMOORA method is extended by using the Dice distance and score function and it is used to solve the multicriteria decision making problems under the PFS information context. Finally, a real case for evaluating solid-state disk productions is handled using the proposed Pythagorean fuzzy MULTIMOORA method and another case for evaluating energy projects is given to verify the advantages of our studies by comparing them with the existing Pythagorean fuzzy distance measures, score functions, and decision making methods.},
  archive      = {J_KIS},
  author       = {Huang, Chao and Lin, Mingwei and Xu, Zeshui},
  doi          = {10.1007/s10115-020-01491-y},
  journal      = {Knowledge and Information Systems},
  month        = {11},
  number       = {11},
  pages        = {4373-4406},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Pythagorean fuzzy MULTIMOORA method based on distance measure and score function: Its application in multicriteria decision making process},
  volume       = {62},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). An experimental study of graph-based semi-supervised
classification with additional node information. <em>KIS</em>,
<em>62</em>(11), 4337–4371. (<a
href="https://doi.org/10.1007/s10115-020-01500-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The volume of data generated by internet and social networks is increasing every day, and there is a clear need for efficient ways of extracting useful information from them. As this information can take different forms, it is important to use all the available data representations for prediction; this is often referred to multi-view learning. In this paper, we consider semi-supervised classification using both regular, plain, tabular, data and structural information coming from a network structure (feature-rich networks). Sixteen techniques are compared and can be divided in three families: the first one uses only the plain features to fit a classification model, the second uses only the network structure, and the last combines both information sources. These three settings are investigated on 10 real-world datasets. Furthermore, network embedding and well-known autocorrelation indicators from spatial statistics are also studied. Possible applications are automatic classification of web pages or other linked documents, of nodes in a social network, or of proteins in a biological complex system, to name a few. Based on our findings, we draw some general conclusions and advice to tackle this particular classification task: it is clearly observed that some dataset labelings can be better explained by their graph structure or by their features set.},
  archive      = {J_KIS},
  author       = {Lebichot, Bertrand and Saerens, Marco},
  doi          = {10.1007/s10115-020-01500-0},
  journal      = {Knowledge and Information Systems},
  month        = {11},
  number       = {11},
  pages        = {4337-4371},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {An experimental study of graph-based semi-supervised classification with additional node information},
  volume       = {62},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). SNOWL model: Social networks unification-based semantic data
integration. <em>KIS</em>, <em>62</em>(11), 4297–4336. (<a
href="https://doi.org/10.1007/s10115-020-01498-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Integrating social networks data in the process of promoting business and marketing applications is widely addressed by several researchers. However, regarding the isolation between social network platforms managing such data has become a challenging task facing data scientist. In this respect, the present paper is designed to put forward a special semantic data integration approach, whereby a unified presentation and access to social networks data can be maintained. To this end, the novel SNOWL (Social Network OWL) ontology aims to provide a new social network content modeling, following the UPON Lite ontology-construction methodology. The advanced ontology is not created from scratch; it is but a continuation of some previously devised ontologies, elaborated to integrate an additional selection of newly incorporated social entities, such as content and user popularity. Additionally, and for an effective advantage of the model to be gained, a special mapping of the social networks data has been firstly implemented to the designed ontology, developed on the basis of the RML mapping language. Secondly, the SNOWL ontology is evaluated through the OOPS! Pitfall tool. Finally, a set of SPARQL-based services has also been designed on top of the SNOWL ontology in a bid to ensure a unified access to the mapped social data.},
  archive      = {J_KIS},
  author       = {Sebei, Hiba and Hadj Taieb, Mohamed Ali and Ben Aouicha, Mohamed},
  doi          = {10.1007/s10115-020-01498-5},
  journal      = {Knowledge and Information Systems},
  month        = {11},
  number       = {11},
  pages        = {4297-4336},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {SNOWL model: Social networks unification-based semantic data integration},
  volume       = {62},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A joint optimization framework for better community
detection based on link prediction in social networks. <em>KIS</em>,
<em>62</em>(11), 4277–4296. (<a
href="https://doi.org/10.1007/s10115-020-01490-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Real-world network data can be incomplete (e.g., the social connections are partially observed) due to reasons such as graph sampling and privacy concerns. Consequently, communities detected based on such incomplete network information could be not as reliable as the ones identified based on the fully observed network. While existing studies first predict missing links and then detect communities, in this paper, a joint optimization framework, Communities detected on Predicted Edges, is proposed. Our goal to improve the quality of community detection through learning the probability of unseen links and the probability of community affiliation of nodes simultaneously. Link prediction and community detection are mutually reinforced to generate better results of both tasks. Experiments conducted on a number of well-known network data show that the proposed COPE stably outperforms several state-of-the-art community detection algorithms.},
  archive      = {J_KIS},
  author       = {Zhang, Shu-Kai and Li, Cheng-Te and Lin, Shou-De},
  doi          = {10.1007/s10115-020-01490-z},
  journal      = {Knowledge and Information Systems},
  month        = {11},
  number       = {11},
  pages        = {4277-4296},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {A joint optimization framework for better community detection based on link prediction in social networks},
  volume       = {62},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Service cost-based resource optimization and load balancing
for edge and cloud environment. <em>KIS</em>, <em>62</em>(11),
4255–4275. (<a
href="https://doi.org/10.1007/s10115-020-01489-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The application of edge clouds is becoming more and more widespread. The resource optimization is one of the important research contents of edge cloud. Generally, the edge cloud has limited computing resources and energy. Resource optimization can make tasks perform efficiently and reduce costs. Therefore, achieving high energy efficiency while ensuring a satisfying user experience is critical. This paper proposes the resource optimization and load balancing model. By considering factors such as user preferences, SLA and cost, the algorithm of resource optimization determines the resources scheme of edge cloud. The data movement after resource optimization is achieved through migration strategies. The load balancing of the edge cloud environment can be ensured. The results of the experiment prove that our proposed algorithm can better control costs.},
  archive      = {J_KIS},
  author       = {Li, Chunlin and Tang, Jianhang and Luo, Youlong},
  doi          = {10.1007/s10115-020-01489-6},
  journal      = {Knowledge and Information Systems},
  month        = {11},
  number       = {11},
  pages        = {4255-4275},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Service cost-based resource optimization and load balancing for edge and cloud environment},
  volume       = {62},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Discovering dependencies with reliable mutual information.
<em>KIS</em>, <em>62</em>(11), 4223–4253. (<a
href="https://doi.org/10.1007/s10115-020-01494-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider the task of discovering functional dependencies in data for target attributes of interest. To solve it, we have to answer two questions: How do we quantify the dependency in a model-agnostic and interpretable way as well as reliably against sample size and dimensionality biases? How can we efficiently discover the exact or $$\alpha $$ -approximate top-k dependencies? We address the first question by adopting information-theoretic notions. Specifically, we consider the mutual information score, for which we propose a reliable estimator that enables robust optimization in high-dimensional data. To address the second question, we then systematically explore the algorithmic implications of using this measure for optimization. We show the problem is NP-hard and justify worst-case exponential-time as well as heuristic search methods. We propose two bounding functions for the estimator, which we use as pruning criteria in branch-and-bound search to efficiently mine dependencies with approximation guarantees. Empirical evaluation shows that the derived estimator has desirable statistical properties, the bounding functions lead to effective exact and greedy search algorithms, and when combined, qualitative experiments show the framework indeed discovers highly informative dependencies.},
  archive      = {J_KIS},
  author       = {Mandros, Panagiotis and Boley, Mario and Vreeken, Jilles},
  doi          = {10.1007/s10115-020-01494-9},
  journal      = {Knowledge and Information Systems},
  month        = {11},
  number       = {11},
  pages        = {4223-4253},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Discovering dependencies with reliable mutual information},
  volume       = {62},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Integrating researchers’ scientific production information
through ogmios. <em>KIS</em>, <em>62</em>(11), 4199–4222. (<a
href="https://doi.org/10.1007/s10115-020-01479-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nowadays, many R&amp;I institutions are presently implementing mechanisms to measure and rate their scientific production so as to comply with current legislation and to support research management and decision making. In many cases, they rely on the implementation of current research information systems (CRIS). This is a challenging task that often requires major human intervention and supervision to manually include all scientific production, projects, patents, etc., in the system. In this paper, we present Ogmios, a system that aims to reduce the time and effort of this process. Ogmios is a CRIS that automatically extracts and combines information from different sources, such as academic social networks or academic search engines. This redundancy helps to reduce potential errors. Additionally, Ogmios relies on other sources, such as online subscription-based scientific citation indexing services, to add metadata to information collected for ranking purposes. We have assessed the performance of this system with a sample of 216 researchers from the University of Málaga; 815 profiles were retrieved and validated with an accuracy of over 90% in profile detection. The main contribution of this work is Ogmios’s autonomous capacity to retrieve and combine all necessary information on scientific profiles and production from different data sources and, also, its adaptability to any university or research institution.},
  archive      = {J_KIS},
  author       = {Verdugo, Nahuel and Guzmán, Eduardo and Urdiales, Cristina},
  doi          = {10.1007/s10115-020-01479-8},
  journal      = {Knowledge and Information Systems},
  month        = {11},
  number       = {11},
  pages        = {4199-4222},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Integrating researchers’ scientific production information through ogmios},
  volume       = {62},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Fast LSTM by dynamic decomposition on cloud and distributed
systems. <em>KIS</em>, <em>62</em>(11), 4169–4197. (<a
href="https://doi.org/10.1007/s10115-020-01487-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Long short-term memory (LSTM) is a powerful deep learning technique that has been widely used in many real-world data-mining applications such as language modeling and machine translation. In this paper, we aim to minimize the latency of LSTM inference on cloud systems without losing accuracy. If an LSTM model does not fit in cache, the latency due to data movement will likely be greater than that due to computation. In this case, we reduce model parameters. If, as in most applications we consider, the LSTM models are able to fit the cache of cloud server processors, we focus on reducing the number of floating point operations, which has a corresponding linear impact on the latency of the inference calculation. Thus, in our system, we dynamically reduce model parameters or flops depending on which most impacts latency. Our inference system is based on singular value decomposition and canonical polyadic decomposition. Our system is accurate and low latency. We evaluate our system based on models from a series of real-world applications like language modeling, computer vision, question answering, and sentiment analysis. Users of our system can use either pre-trained models or start from scratch. Our system achieves 15 $$\times $$ average speedup for six real-world applications without losing accuracy in inference. We also design and implement a distributed optimization system with dynamic decomposition, which can significantly reduce the energy cost and accelerate the training process.},
  archive      = {J_KIS},
  author       = {You, Yang and He, Yuxiong and Rajbhandari, Samyam and Wang, Wenhan and Hsieh, Cho-Jui and Keutzer, Kurt and Demmel, James},
  doi          = {10.1007/s10115-020-01487-8},
  journal      = {Knowledge and Information Systems},
  month        = {11},
  number       = {11},
  pages        = {4169-4197},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Fast LSTM by dynamic decomposition on cloud and distributed systems},
  volume       = {62},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). An efficient projection-based method for high utility
itemset mining using a novel pruning approach on the utility matrix.
<em>KIS</em>, <em>62</em>(11), 4141–4167. (<a
href="https://doi.org/10.1007/s10115-020-01485-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {High utility itemset mining is an important extension of frequent itemset mining which considers unit profits and quantities of items as external and internal utilities, respectively. Since the utility function has not downward closure property, an overestimated value of utility is obtained using an anti-monotonic upper bound of utility function to prune the search space and improve the efficiency of high utility itemset mining methods. Transaction-weighted utilization (TWU) of itemset was the first and one of the most important functions which has been used as the anti-monotonic upper bound of utility by various algorithms. A variety of high utility itemset mining methods have attempted to tighten the utility upper bound and have exploited appropriate pruning strategies to improve mining efficiency. Although TWU and its improved alternatives have attempted to increase the efficiency of high utility itemset mining methods by pruning their search spaces, they suffer from a significant number of generated candidates which are high-TWU but are not high utility itemsets. Calculating the actual utilities of low utility candidates needs to multiple scanning of the dataset and thus imposes a huge overhead to the mining methods, which can cause to lose the pruning benefits of the upper bounds. Proposing appropriate pruning strategies, exploiting efficient data structures, and using tight anti-monotonic upper bounds can overcome this problem and lead to significant performance improvement in high utility itemset mining methods. In this paper, a new projection-based method, called MAHI (matrix-aided high utility itemset mining), is introduced which uses a novel utility matrix-based pruning strategy, called MA-prune to improve the high utility itemset mining performance in terms of execution time. The experimental results show that MAHI is faster than former algorithms.},
  archive      = {J_KIS},
  author       = {Sohrabi, Mohammad Karim},
  doi          = {10.1007/s10115-020-01485-w},
  journal      = {Knowledge and Information Systems},
  month        = {11},
  number       = {11},
  pages        = {4141-4167},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {An efficient projection-based method for high utility itemset mining using a novel pruning approach on the utility matrix},
  volume       = {62},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Precise temporal slot filling via truth finding with
data-driven commonsense. <em>KIS</em>, <em>62</em>(10), 4113–4139. (<a
href="https://doi.org/10.1007/s10115-020-01493-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The task of temporal slot filling (TSF) is to extract values of specific attributes for a given entity, called “facts”, as well as temporal tags of the facts, from text data. While existing work denoted the temporal tags as single time slots, in this paper, we introduce and study the task of Precise TSF (PTSF), that is to fill two precise temporal slots including the beginning and ending time points. Based on our observation from a news corpus, most of the facts should have the two points, however, fewer than 0.1% of them have time expressions in the documents. On the other hand, the documents’ post time, though often available, is not as precise as the time expressions of being the time a fact was valid. Therefore, directly decomposing the time expressions or using an arbitrary post-time period cannot provide accurate results for PTSF. The challenge of PTSF lies in finding precise time tags in noisy and incomplete temporal contexts in the text. To address the challenge, we propose an unsupervised approach based on the philosophy of truth finding. The approach has two modules that mutually enhance each other: One is a reliability estimator of fact extractors conditionally on the temporal contexts; the other is a fact trustworthiness estimator based on the extractor’s reliability. Commonsense knowledge (e.g., one country has only one president at a specific time) was automatically generated from data and used for inferring false claims based on trustworthy facts. For the purpose of evaluation, we manually collect hundreds of temporal facts from Wikipedia as ground truth, including country’s presidential terms and sport team’s player career history. Experiments on a large news dataset demonstrate the accuracy and efficiency of our proposed algorithm.},
  archive      = {J_KIS},
  author       = {Wang, Xueying and Jiang, Meng},
  doi          = {10.1007/s10115-020-01493-w},
  journal      = {Knowledge and Information Systems},
  month        = {10},
  number       = {10},
  pages        = {4113-4139},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Precise temporal slot filling via truth finding with data-driven commonsense},
  volume       = {62},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Efficient computation of the convex hull on sets of points
stored in a k-tree compact data structure. <em>KIS</em>,
<em>62</em>(10), 4091–4111. (<a
href="https://doi.org/10.1007/s10115-020-01486-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we present two algorithms to obtain the convex hull of a set of points that are stored in the compact data structure called $$k^2$$ - $$tree$$ . This problem consists in given a set of points P in the Euclidean space obtaining the smallest convex region (polygon) containing P. Traditional algorithms to compute the convex hull do not scale well for large databases, such as spatial databases, since the data does not reside in main memory. We use the $$k^2$$ - $$tree$$ compact data structure to represent, in main memory, efficiently a binary adjacency matrix representing points over a 2D space. This structure allows an efficient navigation in a compressed form. The experimentations performed over synthetical and real data show that our proposed algorithms are more efficient. In fact they perform over four order of magnitude compared with algorithms with time complexity of $$O(n \log n)$$ .},
  archive      = {J_KIS},
  author       = {Castro, Juan Felipe and Romero, Miguel and Gutiérrez, Gilberto and Caniupán, Mónica and Quijada-Fuentes, Carlos},
  doi          = {10.1007/s10115-020-01486-9},
  journal      = {Knowledge and Information Systems},
  month        = {10},
  number       = {10},
  pages        = {4091-4111},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Efficient computation of the convex hull on sets of points stored in a k-tree compact data structure},
  volume       = {62},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Bag of biterms modeling for short texts. <em>KIS</em>,
<em>62</em>(10), 4055–4090. (<a
href="https://doi.org/10.1007/s10115-020-01482-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Analyzing texts from social media encounters many challenges due to their unique characteristics of shortness, massiveness, and dynamic. Short texts do not provide enough context information, causing the failure of the traditional statistical models. Furthermore, many applications often face with massive and dynamic short texts, causing various computational challenges to the current batch learning algorithms. This paper presents a novel framework, namely bag of biterms modeling (BBM), for modeling massive, dynamic, and short text collections. BBM comprises of two main ingredients: (1) the concept of bag of biterms (BoB) for representing documents, and (2) a simple way to help statistical models to include BoB. Our framework can be easily deployed for a large class of probabilistic models, and we demonstrate its usefulness with two well-known models: latent Dirichlet allocation (LDA) and hierarchical Dirichlet process (HDP). By exploiting both terms (words) and biterms (pairs of words), the major advantages of BBM are: (1) it enhances the length of the documents and makes the context more coherent by emphasizing the word connotation and co-occurrence via bag of biterms, and (2) it inherits inference and learning algorithms from the primitive to make it straightforward to design online and streaming algorithms for short texts. Extensive experiments suggest that BBM outperforms several state-of-the-art models. We also point out that the BoB representation performs better than the traditional representations (e.g., bag of words, tf-idf) even for normal texts.},
  archive      = {J_KIS},
  author       = {Tuan, Anh Phan and Tran, Bach and Nguyen, Thien Huu and Van, Linh Ngo and Than, Khoat},
  doi          = {10.1007/s10115-020-01482-z},
  journal      = {Knowledge and Information Systems},
  month        = {10},
  number       = {10},
  pages        = {4055-4090},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Bag of biterms modeling for short texts},
  volume       = {62},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Regularising LSTM classifier by transfer learning for
detecting misogynistic tweets with small training set. <em>KIS</em>,
<em>62</em>(10), 4029–4054. (<a
href="https://doi.org/10.1007/s10115-020-01481-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Supervised machine learning methods depend highly on the quality of the training dataset and the underlying model. In particular, neural network models, that have shown great success in dealing with natural language problems, require a large dataset to learn a vast number of parameters. However, it is not always easy to build a large (labelled) dataset. For example, due to the complex nature of tweets and the manual labour involved, it is hard to create a large Twitter data set with the misogynistic label. In this paper, we propose to regularise a long short-term memory (LSTM) classifier using a pretrained LSTM-based language model (LM) to build an accurate classification model with a small training set. We explain transfer learning (TL) with a Bayesian interpretation and show that TL can be viewed as an uncertainty regularisation technique in Bayesian inference. We show that a LM pre-trained on a sequence of general to task-specific domain datasets can be used to regularise a LSTM classifier effectively when a small training dataset is available. Empirical analysis with two small Twitter datasets reveals that an LSTM model trained in this way can outperform the state-of-the-art classification models.},
  archive      = {J_KIS},
  author       = {Bashar, Md Abul and Nayak, Richi and Suzor, Nicolas},
  doi          = {10.1007/s10115-020-01481-0},
  journal      = {Knowledge and Information Systems},
  month        = {10},
  number       = {10},
  pages        = {4029-4054},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Regularising LSTM classifier by transfer learning for detecting misogynistic tweets with small training set},
  volume       = {62},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Fisher-regularized supervised and semi-supervised extreme
learning machine. <em>KIS</em>, <em>62</em>(10), 3995–4027. (<a
href="https://doi.org/10.1007/s10115-020-01484-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The structural information of data contains useful prior knowledge and thus is important for designing classifiers. Extreme learning machine (ELM) has been a potential technique in handling classification problems. However, it only simply considers the prior class-based structural information and ignores the prior knowledge from statistics and geometry of data. In this paper, to capture more structural information of the data, we first propose a Fisher-regularized extreme learning machine (called Fisher-ELM) by applying Fisher regularization into the ELM learning framework, the main goals of which is to build an optimal hyperplane such that the output weight and within-class scatter are minimized simultaneously. The proposed Fisher-ELM reflects both the global characteristics and local properties of samples. Intuitively, the Fisher-ELM can approximatively fulfill the Fisher criterion and can obtain good statistical separability. Then, we exploit graph structural formulation to obtain semi-supervised Fisher-ELM version (called Lap-FisherELM) by introducing manifold regularization that characterizes the geometric information of the marginal distribution embedded in unlabeled samples. An efficient successive overrelaxation algorithm is used to solve the proposed Fisher-ELM and Lap-FisherELM, which converges linearly to a solution, and can process very large datasets that need not reside in memory. The proposed Fisher-ELM and Lap-FisherELM do not need to deal with the extra matrix and burden the computations related to the variable switching, which makes them more suitable for relatively large-scale problems. Experiments on several datasets verify the effectiveness of the proposed methods.},
  archive      = {J_KIS},
  author       = {Ma, Jun and Wen, Yakun and Yang, Liming},
  doi          = {10.1007/s10115-020-01484-x},
  journal      = {Knowledge and Information Systems},
  month        = {10},
  number       = {10},
  pages        = {3995-4027},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Fisher-regularized supervised and semi-supervised extreme learning machine},
  volume       = {62},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A search space optimization method for fuzzy access control
auditing. <em>KIS</em>, <em>62</em>(10), 3973–3994. (<a
href="https://doi.org/10.1007/s10115-020-01480-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As data become an increasingly important asset for organizations, so does the access control policies that protect aforesaid data. Many subjects (public, researchers, etc.) are interested in accessing these data, leading to the desire for simple access control. However, some scenarios use vague concepts, such as the “researcher’s expertise”, when making access control decisions. Therefore, access control models based on fuzzy logic have been proposed to handle these scenarios. However, subject attributes can change between access requests and are processed in non-trivial ways by these models to reach a decision. This makes it difficult to audit the capabilities of subjects and their permissions over resources, and consequently, the number of application scenarios naturally suffers. Hence, the contribution of this paper lies in proposing an optimized auditing algorithm that allows fuzzy policies to be validated before being used. An assessment is also carried out to validate the method and its effectiveness.},
  archive      = {J_KIS},
  author       = {Regateiro, Diogo Domingues and Pereira, Óscar Mortágua and Aguiar, Rui L.},
  doi          = {10.1007/s10115-020-01480-1},
  journal      = {Knowledge and Information Systems},
  month        = {10},
  number       = {10},
  pages        = {3973-3994},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {A search space optimization method for fuzzy access control auditing},
  volume       = {62},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Mining evolutions of complex spatial objects using a
single-attributed directed acyclic graph. <em>KIS</em>, <em>62</em>(10),
3931–3971. (<a
href="https://doi.org/10.1007/s10115-020-01478-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Directed acyclic graphs (DAGs) are used in many domains ranging from computer science to bioinformatics, including industry and geoscience. They enable to model complex evolutions where spatial objects (e.g., soil erosion) may move, (dis)appear, merge or split. We study a new graph-based representation, called attributed DAG (a-DAG). It enables to capture interactions between objects as well as information on objects (e.g., characteristics or events). In this paper, we focus on pattern mining in such data. Our patterns, called weighted paths, offer a good trade-off between expressiveness and complexity. Frequency and compactness constraints are used to filter out uninteresting patterns. These constraints lead to an exact condensed representation (without loss of information) in the single-graph setting. A depth-first search strategy and an optimized data structure are proposed to achieve the efficiency of weighted path discovery. It does a progressive extension of patterns based on database projections. Relevance, scalability and genericity are illustrated by means of qualitative and quantitative results when mining various real and synthetic datasets. In particular, we show how such an approach can be used to monitor soil erosion using remote sensing and geographical information system (GIS) data.},
  archive      = {J_KIS},
  author       = {Flouvat, Frédéric and Selmaoui-Folcher, Nazha and Sanhes, Jérémy and Mu, Chengcheng and Pasquier, Claude and Boulicaut, Jean-François},
  doi          = {10.1007/s10115-020-01478-9},
  journal      = {Knowledge and Information Systems},
  month        = {10},
  number       = {10},
  pages        = {3931-3971},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Mining evolutions of complex spatial objects using a single-attributed directed acyclic graph},
  volume       = {62},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Learning sequence-to-sequence affinity metric for
near-online multi-object tracking. <em>KIS</em>, <em>62</em>(10),
3911–3930. (<a
href="https://doi.org/10.1007/s10115-020-01488-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose a sequence-to-sequence affinity metric for the data association of near-online multi-object tracking. The proposed metric learns the affinity between track sequence consisting of the already associated detections and hypothesis sequence consisting of detections in the near future. With the potential hypothesis sequences, we leverage the idea that if a track sequence has a high affinity for a hypothesis sequence, and the hypothesis sequence also shares a close affinity for a current detection, then the affinity between the track sequence and the detection is high. By using the short hypothesis sequence as a “bridge”, the proposed sequence-to-sequence affinity metric enhances the conventional track sequence to detection affinity metric and improves its robustness to object occlusion and missing. Besides, in order to eliminate the negative effects of false alarms, we propose a false alarm model using both appearance and scale features of detection. The robustness of the proposed affinity metric allows us to use a simple greedy data association algorithm. Experimental results on the challenging MOT16 and MOT17 benchmarks demonstrate the effectiveness of our method.},
  archive      = {J_KIS},
  author       = {Feng, Weijiang and Lan, Long and Zhang, Xiang and Luo, Zhigang},
  doi          = {10.1007/s10115-020-01488-7},
  journal      = {Knowledge and Information Systems},
  month        = {10},
  number       = {10},
  pages        = {3911-3930},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Learning sequence-to-sequence affinity metric for near-online multi-object tracking},
  volume       = {62},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Modeling, learning, and simulating human activities of daily
living with behavior trees. <em>KIS</em>, <em>62</em>(10), 3881–3910.
(<a href="https://doi.org/10.1007/s10115-020-01476-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Autonomy is a key factor in the quality of life of a person. With the aging of the population, an increasing number of people suffers from a reduced level of autonomy. That compromises their capacity of performing their daily activities and causes safety issues. The new concept of ambient assisted living (AAL), and more specifically its application in smart homes for supporting elderly people, constitutes a great avenue of the solution. However, to be able to automatically assist a user carrying out is activities, researchers and engineers face three main challenges in the development of smart homes: (i) how to represent the activity models, (ii) how to automatically construct theses models based on historical data and (iii) how to be able to simulate the user behavior for tests and calibration purpose. Most of recent works addressing these challenges exploit simple models of activity with no semantic, or use logically complex ones or else use probabilistically rigid representations. In this paper, we propose a global approach to address the three challenges. We introduce a new way of modeling human activities in smart homes based on behavior trees which are used in the video game industry. We then present an algorithmic way to automatically learn these models with sensors logs. We use a simulator that we have developed to validate our approach.},
  archive      = {J_KIS},
  author       = {Francillette, Yannick and Bouchard, Bruno and Bouchard, Kévin and Gaboury, Sébastien},
  doi          = {10.1007/s10115-020-01476-x},
  journal      = {Knowledge and Information Systems},
  month        = {10},
  number       = {10},
  pages        = {3881-3910},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Modeling, learning, and simulating human activities of daily living with behavior trees},
  volume       = {62},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A systematic survey on collaborator finding systems in
scientific social networks. <em>KIS</em>, <em>62</em>(10), 3837–3879.
(<a href="https://doi.org/10.1007/s10115-020-01483-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The increasing number of researchers and scientists participating in online communities has induced big challenges for users who are looking for researchers who are interested. As a result, finding potential collaborators among the huge amount of online information is going to be even much more important in the future. Collaborator recommendation is a kind of expert recommendation in scientific fields. A number of published papers have proposed new algorithms for an expert or a collaborator finding and tacking a narrower point of view. For instance, some of these papers have particularly considered a collaborator finding problem. New scientific social networks, such as ResearchGate, Academia, Mendeley, and so on, have provided some facilities to their users for finding new collaborators. In this paper, first of all, we review proposed models for an expert and a collaborator finding in scientific and academic social networks in a systematic manner. Next, collaborator finding facilities in online scientific social networks are evaluated. Finally, the defects and open challenges of the models are looked into and some propositions for the future works are presented.},
  archive      = {J_KIS},
  author       = {Roozbahani, Zahra and Rezaeenour, Jalal and Emamgholizadeh, Hanif and Jalaly Bidgoly, Amir},
  doi          = {10.1007/s10115-020-01483-y},
  journal      = {Knowledge and Information Systems},
  month        = {10},
  number       = {10},
  pages        = {3837-3879},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {A systematic survey on collaborator finding systems in scientific social networks},
  volume       = {62},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). PragmaticOIE: A pragmatic open information extraction for
portuguese language. <em>KIS</em>, <em>62</em>(9), 3811–3836. (<a
href="https://doi.org/10.1007/s10115-020-01442-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Information extraction (IE) involves the extraction of useful facts from texts. IE approaches have been categorized into two types: Traditional IE and Open IE. Traditional IE recognizes a predefined set of relationships between the arguments, and it has typically been applied to specific domains. Open IE extracts relationship descriptors expressing any semantic relationship between a pair of arguments in different domains. Although a sentence can have a different meaning, given the context and intention used, a single semantic analysis does not guarantee useful extractions. Extractions depend on the context and the intention inherited in a sentence that goes beyond the semantic meaning. Thus, a pragmatic analysis enhances the set of extractions by considering the contextual and intentional aspects. As a consequence, new facts can be extracted from this set of sentences. The combination of inference, context, and intention enables the extraction of implicit facts from texts achieving a first pragmatic level. This novel approach increases the number of facts, extracting relationships from a sentence analyzing inference, context, and intention. This is the first method to analyze a first pragmatic level from a sentence within a set of Portuguese text documents. Our method was performed over a set of Portuguese text documents and outperforms the most relevant related work comparing accuracy, number of extracted facts, and minimality measures.},
  archive      = {J_KIS},
  author       = {Sena, Cleiton Fernando Lima and Claro, Daniela Barreiro},
  doi          = {10.1007/s10115-020-01442-7},
  journal      = {Knowledge and Information Systems},
  month        = {9},
  number       = {9},
  pages        = {3811-3836},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {PragmaticOIE: A pragmatic open information extraction for portuguese language},
  volume       = {62},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). TAILOR: Time-aware facility location recommendation based on
massive trajectories. <em>KIS</em>, <em>62</em>(9), 3783–3810. (<a
href="https://doi.org/10.1007/s10115-020-01477-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In traditional facility location recommendations, the objective is to select the best locations which maximize the coverage or convenience of users. However, since users’ behavioral habits are often influenced by time, the temporal impacts should not be neglected in recommendation. In this paper, we study the problem of time-aware facility location recommendation problem, taking the time factor into account. To solve this problem, we develop a framework, TAILOR, which incorporates the temporal influence, user-coverage, and user-convenience. Based on TAILOR, we derive a greedy algorithm with (1- $$\frac{1}{e}$$ )-approximation and an online algorithm with ( $$\frac{1}{4}$$ )-competitive ratio. Extensive experimental evaluation and two case studies demonstrate the efficiency and effectiveness of the proposed approaches.},
  archive      = {J_KIS},
  author       = {Qi, Zhixin and Wang, Hongzhi and He, Tao and Wang, Chunnan and Li, Jianzhong and Gao, Hong},
  doi          = {10.1007/s10115-020-01477-w},
  journal      = {Knowledge and Information Systems},
  month        = {9},
  number       = {9},
  pages        = {3783-3810},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {TAILOR: Time-aware facility location recommendation based on massive trajectories},
  volume       = {62},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Rule extraction from neural network trained using deep
belief network and back propagation. <em>KIS</em>, <em>62</em>(9),
3753–3781. (<a
href="https://doi.org/10.1007/s10115-020-01473-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Representing the knowledge learned by neural networks in the form of interpretable rules is a prudent technique to justify the decisions made by neural networks. Heretofore many algorithms exist to extract symbolic rules from neural networks, but among them, a few extract rules from deep neural networks trained using deep learning techniques. So, this paper proposes an algorithm to extract rules from a multi-hidden layer neural network, pre-trained using deep belief network and fine-tuned using back propagation. The algorithm analyzes each node of a layer and extracts knowledge from each layer separately. The process of knowledge extraction from the first hidden layer is different from the other layers. Consecutively, the algorithm combines all the knowledge extracted and refines them to construct a final ruleset consisting of symbolic rules. The algorithm further subdivides the subspace of a rule in the ruleset if it satisfies certain conditions. Results show that the algorithm extracted rules with higher accuracy compared to some existing rule extraction algorithms. Other than accuracy, the efficacy of the extracted rules is also validated with fidelity and various other performance measures.},
  archive      = {J_KIS},
  author       = {Chakraborty, Manomita and Biswas, Saroj Kumar and Purkayastha, Biswajit},
  doi          = {10.1007/s10115-020-01473-0},
  journal      = {Knowledge and Information Systems},
  month        = {9},
  number       = {9},
  pages        = {3753-3781},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Rule extraction from neural network trained using deep belief network and back propagation},
  volume       = {62},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Survival neural networks for time-to-event prediction in
longitudinal study. <em>KIS</em>, <em>62</em>(9), 3727–3751. (<a
href="https://doi.org/10.1007/s10115-020-01472-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Time-to-event prediction has been an important practical task for longitudinal studies in many fields such as manufacturing, medicine, and healthcare. While most of the conventional survival analysis approaches suffer from the presence of censored failures and statistically circumscribed assumptions, few attempts have been made to develop survival learning machines that explore the underlying relationship between repeated measures of covariates and failure-free survival probability. This requires a purely dynamic-data-driven prediction approach, free of survival models or statistical assumptions. To this end, we propose two real-time survival networks: a time-dependent survival neural network (TSNN) with a feed-forward architecture and a recurrent survival neural network (RSNN) incorporating long short-term memory units. The TSNN additively estimates a latent failure risk arising from the repeated measures and performs multiple binary classifications to generate prognostics of survival probability, while the RSNN with time-dependent input covariates implicitly estimates the relation between these covariates and the survival probability. We propose a novel survival learning criterion to train the neural networks by minimizing the censoring Kullback–Leibler divergence, which guarantees monotonicity of the resulting probability. Besides the failure-event AUC, C-index, and censoring Brier score, we redefine a survival time estimate to evaluate the performance of the competing models. Experiments on four datasets demonstrate the great promise of our approach in real applications.},
  archive      = {J_KIS},
  author       = {Zhang, Jianfei and Chen, Lifei and Ye, Yanfang and Guo, Gongde and Chen, Rongbo and Vanasse, Alain and Wang, Shengrui},
  doi          = {10.1007/s10115-020-01472-1},
  journal      = {Knowledge and Information Systems},
  month        = {9},
  number       = {9},
  pages        = {3727-3751},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Survival neural networks for time-to-event prediction in longitudinal study},
  volume       = {62},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Tell me something my friends do not know: Diversity
maximization in social networks. <em>KIS</em>, <em>62</em>(9),
3697–3726. (<a
href="https://doi.org/10.1007/s10115-020-01456-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Social media have a great potential to improve information dissemination in our society, yet they have been held accountable for a number of undesirable effects, such as polarization and filter bubbles. It is thus important to understand these negative phenomena and develop methods to combat them. In this paper, we propose a novel approach to address the problem of breaking filter bubbles in social media. We do so by aiming to maximize the diversity of the information exposed to connected social-media users. We formulate the problem of maximizing the diversity of exposure as a quadratic-knapsack problem. We show that the proposed diversity-maximization problem is inapproximable, and thus, we resort to polynomial nonapproximable algorithms, inspired by solutions developed for the quadratic-knapsack problem, as well as scalable greedy heuristics. We complement our algorithms with instance-specific upper bounds, which are used to provide empirical approximation guarantees for the given problem instances. Our experimental evaluation shows that a proposed greedy algorithm followed by randomized local search is the algorithm of choice given its quality-vs.-efficiency trade-off.},
  archive      = {J_KIS},
  author       = {Matakos, Antonis and Tu, Sijing and Gionis, Aristides},
  doi          = {10.1007/s10115-020-01456-1},
  journal      = {Knowledge and Information Systems},
  month        = {9},
  number       = {9},
  pages        = {3697-3726},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Tell me something my friends do not know: Diversity maximization in social networks},
  volume       = {62},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Decision model change patterns for dynamic system evolution.
<em>KIS</em>, <em>62</em>(9), 3665–3696. (<a
href="https://doi.org/10.1007/s10115-020-01469-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the modern digital era, information systems must operate in increasingly interconnected and dynamic environments, which force them to be changeable yet consistent. Such modern information systems are usually decision- and knowledge-intensive. A recently introduced standard, the decision model and notation (DMN), has been adopted in both industry and academia as a suitable method for modelling decisions and decision rules. Noteworthy is that, despite the dynamic nature of modern knowledge-intensive systems, DMN was only studied and implemented in a static fashion, as decision schema change patterns have not received any attention so far. This paper identifies and analyses the change patterns that can occur in a DMN decision model. A change in the decision model can require the triggering of other changes in order to safeguard consistency. As such, this paper will also investigate for each change pattern which further changes should be performed to ensure model consistency. The patterns presented in this paper will not only facilitate the understanding of decision change management and within-model consistency, but can also be capitalised on for developing and implementing flexible decision management systems. To illustrate this, we present a modelling environment prototype that provides modelling support when applying the proposed change patterns.},
  archive      = {J_KIS},
  author       = {Hasić, Faruk and Corea, Carl and Blatt, Jonas and Delfmann, Patrick and Serral, Estefanía},
  doi          = {10.1007/s10115-020-01469-w},
  journal      = {Knowledge and Information Systems},
  month        = {9},
  number       = {9},
  pages        = {3665-3696},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Decision model change patterns for dynamic system evolution},
  volume       = {62},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Enhancing unsupervised domain adaptation by discriminative
relevance regularization. <em>KIS</em>, <em>62</em>(9), 3641–3664. (<a
href="https://doi.org/10.1007/s10115-020-01466-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unsupervised domain adaptation (UDA) serves to transfer specific knowledge from massive labeled source domain data to unlabeled target domain data via mitigating domain shift. In this paper, we propose a discriminative relevance regularization term (DRR) to enhance the performance of UDA by reducing the domain shift from the aspect of semantic relevance across domains. In particular, DRR is formulated as the min–max rank problem which seeks a projection matrix to minimize the rank of intra-class projected features and maximize the rank of the means of inter-class projected features simultaneously. To test the potential effectiveness of DRR, we design a relevance regularized distribution adaptation method (RRDA) and relevance regularized adaptation networks (RRAN) for image classification, and a relevance regularized self-supervised learning method (RRSL) for semantic segmentation by incorporation of DRR. The corresponding optimization algorithms are proposed to solve them. Experiments of cross-domain image classification show that both RRDA and RRAN outperform several state-of-the-art compared methods. Moreover, experiments of domain-adaptation semantic segmentation on two synthetic-to-real segmentation datasets demonstrate the capacity of RRSL. Such results imply the efficacy of DRR on both image classification and semantic segmentation tasks.},
  archive      = {J_KIS},
  author       = {Zhang, Wenju and Zhang, Xiang and Lan, Long and Luo, Zhigang},
  doi          = {10.1007/s10115-020-01466-z},
  journal      = {Knowledge and Information Systems},
  month        = {9},
  number       = {9},
  pages        = {3641-3664},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Enhancing unsupervised domain adaptation by discriminative relevance regularization},
  volume       = {62},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). “Just-in-time” generation of datasets by considering
structured representations of given consent for GDPR compliance.
<em>KIS</em>, <em>62</em>(9), 3615–3640. (<a
href="https://doi.org/10.1007/s10115-020-01468-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data processing is increasingly becoming the subject of various policies and regulations, such as the European General Data Protection Regulation (GDPR) that came into effect in May 2018. One important aspect of GDPR is informed consent, which captures one’s permission for using one’s personal information for specific data processing purposes. Organizations must demonstrate that they comply with these policies. The fines that come with non-compliance are of such importance that it has driven research in facilitating compliance verification. The state-of-the-art primarily focuses on, for instance, the analysis of prescriptive models and posthoc analysis on logs to check whether data processing is compliant to GDPR. We argue that GDPR compliance can be facilitated by ensuring datasets used in processing activities are compliant with consent from the very start. The problem addressed in this paper is how we can generate datasets that comply with given consent “just-in-time”. We propose RDF and OWL ontologies to represent the consent that an organization has collected and its relationship with data processing purposes. We use this ontology to annotate schemas, allowing us to generate declarative mappings that transform (relational) data into RDF driven by the annotations. We furthermore demonstrate how we can create compliant datasets by altering the results of the mapping. The use of RDF and OWL allows us to implement the entire process in a declarative manner using SPARQL. We have integrated all components in a service that furthermore captures provenance information for each step, further contributing to the transparency that is needed towards facilitating compliance verification. We demonstrate the approach with a synthetic dataset simulating users (re-)giving, withdrawing, and rejecting their consent on data processing purposes of systems. In summary, it is argued that the approach facilitates transparency and compliance verification from the start, reducing the need for posthoc compliance analysis common in the state-of-the-art.},
  archive      = {J_KIS},
  author       = {Debruyne, Christophe and Pandit, Harshvardhan J. and Lewis, Dave and O’Sullivan, Declan},
  doi          = {10.1007/s10115-020-01468-x},
  journal      = {Knowledge and Information Systems},
  month        = {9},
  number       = {9},
  pages        = {3615-3640},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {“Just-in-time” generation of datasets by considering structured representations of given consent for GDPR compliance},
  volume       = {62},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Empower rumor events detection from chinese microblogs with
multi-type individual information. <em>KIS</em>, <em>62</em>(9),
3585–3614. (<a
href="https://doi.org/10.1007/s10115-020-01463-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Online social media has become an ideal place in spreading rumor events with its convenience in communication and information dissemination, which raises the difficulty in debunking rumor events automatically. To deal with such a challenge, traditional classification approaches relying on manually labeled features have to face a daunting number of human efforts. With the consideration of the realness of a rumor event, it will be verified and authenticated with multi-type individual information, especially with individuals’ emotional expressions to events and their own credibility. This paper presents a novel two-layer GRU model for rumor events detection based on multi-type individual information (MII) and a dynamic time-series (DTS) algorithm, named as MII–DTS-GRU. Specifically, MII refers to adopt the sentiment dictionary to identify fine-grained human emotional expressions to events and fuse with the individual credibility. Besides, the DTS algorithm retains the time distribution of social events. Experimental results on Sina Weibo datasets show that our model achieves a high accuracy of 96.3% and demonstrate that our proposed MII–DTS-GRU model outperforms the state-of-the-art models on rumor events detection.},
  archive      = {J_KIS},
  author       = {Wang, Zhihong and Guo, Yi},
  doi          = {10.1007/s10115-020-01463-2},
  journal      = {Knowledge and Information Systems},
  month        = {9},
  number       = {9},
  pages        = {3585-3614},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Empower rumor events detection from chinese microblogs with multi-type individual information},
  volume       = {62},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). EAFIM: Efficient apriori-based frequent itemset mining
algorithm on spark for big transactional data. <em>KIS</em>,
<em>62</em>(9), 3565–3583. (<a
href="https://doi.org/10.1007/s10115-020-01464-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Frequent itemset mining is considered a popular tool to discover knowledge from transactional datasets. It also serves as the basis for association rule mining. Several algorithms have been proposed to find frequent patterns in which the apriori algorithm is considered as the earliest proposed. Apriori has two significant bottlenecks associated with it: first, repeated scanning of input dataset and second, the requirement of generation of all the candidate itemsets before counting its support value. These bottlenecks reduce the effectiveness of apriori for large-scale datasets. Reasonable efforts have been made to diminish these bottlenecks so that efficiency can be improved. Especially, when the data size is larger, even distributed and parallel environments like MapReduce does not perform well due to the iterative nature of the algorithm that incurs high disk overhead. Apache Spark, on the other hand, is gaining significant attention in the field of big data processing because of its in-memory processing capabilities. Apart from utilizing the parallel and distributed computing environment of Spark, the proposed scheme named efficient apriori-based frequent itemset mining (EAFIM) presents two novel methods to improve the efficiency further. Unlike apriori, it generates the candidates ‘on-the-fly,’ i.e., candidate generation, and count of its support values go simultaneously when the input dataset is being scanned. Also, instead of using the original input dataset in each iteration, it calculates the updated input dataset by removing useless items and transactions. Reduction in size of the input dataset for higher iterations enables EAFIM to perform better. Extensive experiments were conducted to analyze the efficiency and scalability of EAFIM, which outperforms other existing methodologies.},
  archive      = {J_KIS},
  author       = {Raj, Shashi and Ramesh, Dharavath and Sreenu, M. and Sethi, Krishan Kumar},
  doi          = {10.1007/s10115-020-01464-1},
  journal      = {Knowledge and Information Systems},
  month        = {9},
  number       = {9},
  pages        = {3565-3583},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {EAFIM: Efficient apriori-based frequent itemset mining algorithm on spark for big transactional data},
  volume       = {62},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A novel possibilistic artificial immune-based classifier for
course learning outcome enhancement. <em>KIS</em>, <em>62</em>(9),
3535–3563. (<a
href="https://doi.org/10.1007/s10115-020-01465-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose PAIRS3: a possibilistic classification approach based on artificial immune recognition system (AIRS) and the possibility theory. PAIRS3 is applied to address shortcomings in student attainment rates of course learning outcomes by predicting effective remedial actions through learning from assessment rubrics instances. For most of assessment rubric instances, it is difficult to determine the unique most effective remedial action to take. Consequently, each rubric instance will be labeled with uncertain remedial actions which are elicited from quality assurance experts. Elements from possibility theory are used to (1) model the uncertainty about the most effective remedial action labeling each rubric instance and (2) adapt several parts of the standard AIRS algorithm in order to address the uncertainty in class labels. The performance of the proposed method is evaluated against an academic, university level, assessment dataset that has been built progressively over multiple academic semesters. Despite the uncertainty related to the class labels in the dataset, PAIRS3 showed a good performance in terms of accuracy level (close to 75%). Also, when compared to existing state-of-the-art possibilistic classifiers such as PAIRS2, non-specificity possibilistic decision trees (NSPDT), and cluster-based possibilistic decision trees (Clust-PDT), PAIRS3 achieved better accuracy improvement ranging from 10% (in case of Clust-PDT) to 17% (in case of PAIRS2).},
  archive      = {J_KIS},
  author       = {Jenhani, Ilyes and Elhassan, Ammar and Ben Brahim, Ghassen},
  doi          = {10.1007/s10115-020-01465-0},
  journal      = {Knowledge and Information Systems},
  month        = {9},
  number       = {9},
  pages        = {3535-3563},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {A novel possibilistic artificial immune-based classifier for course learning outcome enhancement},
  volume       = {62},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Discovery of evolving companion from trajectory data
streams. <em>KIS</em>, <em>62</em>(9), 3509–3533. (<a
href="https://doi.org/10.1007/s10115-020-01471-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The widespread use of position-tracking devices leads to vast volumes of spatial–temporal data aggregated in the form of the trajectory data streams. Extracting useful knowledge from moving object trajectories can benefit many applications, such as traffic monitoring, military surveillance, and weather forecasting. Most of the knowledge gleaned from the trajectory data illustrates different kinds of group patterns, i.e., objects that travel together for some time. In the real world, the trajectory of the moving objects can change with time. Thus, existing approaches can miss a new pattern because they have a stringent requirement for moving object participators in a group movement pattern. To address this issue, we introduced a new type of moving object group pattern called an evolving companion. It allows some members of the group to leave and join anytime if some participators stay connected for all time intervals. In this pattern discovery, we model an incremental discovery solution to retrieve the evolving companion efficiently over the data stream. We evaluated the efficiency and effectiveness of our approach on two real vehicles and one synthetic dataset. Our method performed well compared with existing pattern discovery methods; for example, it was about 50% faster than Tang et al.’s buddy-based clustering method.},
  archive      = {J_KIS},
  author       = {Shein, Thi Thi and Puntheeranurak, Sutheera and Imamura, Makoto},
  doi          = {10.1007/s10115-020-01471-2},
  journal      = {Knowledge and Information Systems},
  month        = {9},
  number       = {9},
  pages        = {3509-3533},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Discovery of evolving companion from trajectory data streams},
  volume       = {62},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Measuring time-sensitive user influence in twitter.
<em>KIS</em>, <em>62</em>(9), 3481–3508. (<a
href="https://doi.org/10.1007/s10115-020-01459-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Identification of the influential users is one of the most practical analyses in social networks. The importance of this analysis stems from the fact that such users can affect their followers “/friends” viewpoints. This study aims at introducing two new indices to identify the most influential users in the Twitter social network. Four sets of features extracted from user activities, user profile, tweets, and actions performed on tweets are deployed to create the proposed indices. The available methods of detecting the most influential Twitterers either consider a limited set of features or do not accurately measure the effect of each feature. The indices proposed in this paper consider a comprehensive set of features and also provide a time-sensitive rank which can be used to measure the dynamic nature of influence. Moreover, the relative impact of each feature is computed and considered in the indices. We employ the indices to discover the influential Twitter users posting on Paris attacks in 2015, in a comprehensive analysis. The influence trend of users’ tweets in a 21-day period discloses that 76% of the users do not succeed in posting a second influential tweet. Results reveal that the proposed indices can detect both the publicly recognized sources (like celebrities) and also the less known individuals which gain credit by posting several influential tweets after a specific event. We further compare the proposed indices with other available approaches.},
  archive      = {J_KIS},
  author       = {Rezaie, Behzad and Zahedi, Morteza and Mashayekhi, Hoda},
  doi          = {10.1007/s10115-020-01459-y},
  journal      = {Knowledge and Information Systems},
  month        = {9},
  number       = {9},
  pages        = {3481-3508},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Measuring time-sensitive user influence in twitter},
  volume       = {62},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Bayesian network classifiers using ensembles and smoothing.
<em>KIS</em>, <em>62</em>(9), 3457–3480. (<a
href="https://doi.org/10.1007/s10115-020-01458-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Bayesian network classifiers are, functionally, an interesting class of models, because they can be learnt out-of-core, i.e. without needing to hold the whole training data in main memory. The selective K-dependence Bayesian network classifier (SKDB) is state of the art in this class of models and has shown to rival random forest (RF) on problems with categorical data. In this paper, we introduce an ensembling technique for SKDB, called ensemble of SKDB (ESKDB). We show that ESKDB significantly outperforms RF on categorical and numerical data, as well as rivalling XGBoost. ESKDB combines three main components: (1) an effective strategy to vary the networks that is built by single classifiers (to make it an ensemble), (2) a stochastic discretization method which allows to both tackle numerical data as well as further increases the variance between different components of our ensemble and (3) a superior smoothing technique to ensure proper calibration of ESKDB’s probabilities. We conduct a large set of experiments with 72 datasets to study the properties of ESKDB (through a sensitivity analysis) and show its competitiveness with the state of the art.},
  archive      = {J_KIS},
  author       = {Zhang, He and Petitjean, François and Buntine, Wray},
  doi          = {10.1007/s10115-020-01458-z},
  journal      = {Knowledge and Information Systems},
  month        = {9},
  number       = {9},
  pages        = {3457-3480},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Bayesian network classifiers using ensembles and smoothing},
  volume       = {62},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A survey on influence maximization in a social network.
<em>KIS</em>, <em>62</em>(9), 3417–3455. (<a
href="https://doi.org/10.1007/s10115-020-01461-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Given a social network with diffusion probabilities as edge weights and a positive integer k, which k nodes should be chosen for initial injection of information to maximize the influence in the network? This problem is popularly known as the Social Influence Maximization Problem (SIM Problem). This is an active area of research in computational social network analysis domain, since one and half decades or so. Due to its practical importance in various domains, such as viral marketing, target advertisement and personalized recommendation, the problem has been studied in different variants, and different solution methodologies have been proposed over the years. This paper presents a survey on the progress in and around SIM Problem. At last, it discusses current research trends and future research directions as well.},
  archive      = {J_KIS},
  author       = {Banerjee, Suman and Jenamani, Mamata and Pratihar, Dilip Kumar},
  doi          = {10.1007/s10115-020-01461-4},
  journal      = {Knowledge and Information Systems},
  month        = {9},
  number       = {9},
  pages        = {3417-3455},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {A survey on influence maximization in a social network},
  volume       = {62},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A survey on context awareness in big data analytics for
business applications. <em>KIS</em>, <em>62</em>(9), 3387–3415. (<a
href="https://doi.org/10.1007/s10115-020-01462-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The concept of context awareness has been in existence since the 1990s. Though initially applied exclusively in computer science, over time it has increasingly been adopted by many different application domains such as business, health and military. Contexts change continuously because of objective reasons, such as economic situation, political matter and social issues. The adoption of big data analytics by businesses is facilitating such change at an even faster rate in much complicated ways. The potential benefits of embedding contextual information into an application are already evidenced by the improved outcomes of the existing context-aware methods in those applications. Since big data is growing very rapidly, context awareness in big data analytics has become more important and timely because of its proven efficiency in big data understanding and preparation, contributing to extracting the more and accurate value of big data. Many surveys have been published on context-based methods such as context modelling and reasoning, workflow adaptations, computational intelligence techniques and mobile ubiquitous systems. However, to our knowledge, no survey of context-aware methods on big data analytics for business applications supported by enterprise level software has been published to date. To bridge this research gap, in this paper first, we present a definition of context, its modelling and evaluation techniques, and highlight the importance of contextual information for big data analytics. Second, the works in three key business application areas that are context-aware and/or exploit big data analytics have been thoroughly reviewed. Finally, the paper concludes by highlighting a number of contemporary research challenges, including issues concerning modelling, managing and applying business contexts to big data analytics.},
  archive      = {J_KIS},
  author       = {Dinh, Loan Thi Ngoc and Karmakar, Gour and Kamruzzaman, Joarder},
  doi          = {10.1007/s10115-020-01462-3},
  journal      = {Knowledge and Information Systems},
  month        = {9},
  number       = {9},
  pages        = {3387-3415},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {A survey on context awareness in big data analytics for business applications},
  volume       = {62},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A scalable and effective rough set theory-based approach for
big data pre-processing. <em>KIS</em>, <em>62</em>(8), 3321–3386. (<a
href="https://doi.org/10.1007/s10115-020-01467-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A big challenge in the knowledge discovery process is to perform data pre-processing, specifically feature selection, on a large amount of data and high dimensional attribute set. A variety of techniques have been proposed in the literature to deal with this challenge with different degrees of success as most of these techniques need further information about the given input data for thresholding, need to specify noise levels or use some feature ranking procedures. To overcome these limitations, rough set theory (RST) can be used to discover the dependency within the data and reduce the number of attributes enclosed in an input data set while using the data alone and requiring no supplementary information. However, when it comes to massive data sets, RST reaches its limits as it is highly computationally expensive. In this paper, we propose a scalable and effective rough set theory-based approach for large-scale data pre-processing, specifically for feature selection, under the Spark framework. In our detailed experiments, data sets with up to 10,000 attributes have been considered, revealing that our proposed solution achieves a good speedup and performs its feature selection task well without sacrificing performance. Thus, making it relevant to big data.},
  archive      = {J_KIS},
  author       = {Chelly Dagdia, Zaineb and Zarges, Christine and Beck, Gaël and Lebbah, Mustapha},
  doi          = {10.1007/s10115-020-01467-y},
  journal      = {Knowledge and Information Systems},
  month        = {8},
  number       = {8},
  pages        = {3321-3386},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {A scalable and effective rough set theory-based approach for big data pre-processing},
  volume       = {62},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). CAMAR: A broad learning based context-aware recommender for
mobile applications. <em>KIS</em>, <em>62</em>(8), 3291–3319. (<a
href="https://doi.org/10.1007/s10115-020-01440-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The emergence of a large number of mobile apps brings challenges to locate appropriate apps for users, which makes mobile app recommendation an imperative task. In this paper, we first conduct detailed data analysis to show the characteristics of mobile apps which are different with conventional items (e.g., movies, books). Considering the specific property of mobile apps, we propose a broad learning approach for context-aware mobile app recommendation with tensor analysis (CAMAR). Specifically, we utilize a tensor-based framework to effectively integrate app category information and multi-view features on users and apps to facilitate the performance of app recommendation. The multi-dimensional structure is employed to capture the hidden relationships among the app categories and multi-view features. We develop an efficient factorization method which applies Tucker decomposition to jointly learn the full-order interactions among the app categories and features without physically building the tensor. Furthermore, we employ a group $$\ell _{1}$$ -norm regularization to learn the group-wise feature importance of each view with respect to each app category. Experiments on two real-world datasets demonstrate the effectiveness of the proposed method.},
  archive      = {J_KIS},
  author       = {Liang, Tingting and He, Lifang and Lu, Chun-Ta and Chen, Liang and Ying, Haochao and Yu, Philip S. and Wu, Jian},
  doi          = {10.1007/s10115-020-01440-9},
  journal      = {Knowledge and Information Systems},
  month        = {8},
  number       = {8},
  pages        = {3291-3319},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {CAMAR: A broad learning based context-aware recommender for mobile applications},
  volume       = {62},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). The examination of the effect of the criterion for neural
network’s learning on the effectiveness of the qualitative analysis of
multidimensional data. <em>KIS</em>, <em>62</em>(8), 3263–3289. (<a
href="https://doi.org/10.1007/s10115-020-01441-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A variety of multidimensional visualization methods are applied for the qualitative analysis of multidimensional data. One of the multidimensional data visualization methods is a method using autoassociative neural networks. In order to perform visualizations of n-dimensional data, such a network has n inputs, n outputs and one of the interlayers consisting of two outputs whose values represent coordinates of the analyzed sample’s image on the screen. Such a criterion for the network’s learning consists in that the same value as the one at the ith input appears at each ith output. If the network is trained in this way, the whole information from n inputs was compressed to two outputs of the interlayer and then decompressed to n network outputs. The paper shows the application of different learning criteria can be more beneficial from the point of view of the results’ readability. Overall analysis was conducted on seven-dimensional real data representing three coal classes, five-dimensional data representing printed characters, 216-dimensional data representing hand-written digits and, additionally, in order to illustrate additional explanations using artificially generated seven-dimensional data. Readability of results of the qualitative analysis of these data was compared using the multidimensional visualization utilizing neural networks for different learning criteria. Also, the obtained results of applying all analyzed criteria on 20 randomly selected sets of multidimensional data obtained from one of the publicly available repositories are presented.},
  archive      = {J_KIS},
  author       = {Jamróz, Dariusz},
  doi          = {10.1007/s10115-020-01441-8},
  journal      = {Knowledge and Information Systems},
  month        = {8},
  number       = {8},
  pages        = {3263-3289},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {The examination of the effect of the criterion for neural network’s learning on the effectiveness of the qualitative analysis of multidimensional data},
  volume       = {62},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Recommender systems with selfish users. <em>KIS</em>,
<em>62</em>(8), 3239–3262. (<a
href="https://doi.org/10.1007/s10115-020-01460-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recommender systems are a fundamental component of contemporary social-media platforms and require feedback submitted from users in order to fulfill their goal. On the other hand, the raise of advocacy about user-controlled data repositories supports the selective submission of data by user through intelligent software agents residing at the user end. These agents are endowed with the task of protecting user privacy by applying a “soft filter” on personal data provided to the system. In this work, we pose the question: “how should the software agent control the user feedback submitted to a recommender system in a way that is most privacy preserving, while the user still enjoys most of the benefits of the recommender system?”. We consider a set of such agents, each of which aims to protect the privacy of its serving user by submitting to the recommender system server a version of her real rating profile. The fact that issued recommendations to a user depend on the collective rating profiles by all agents gives rise to a novel game-theoretic setup that unveils the trade-off between privacy preservation of each user and the quality of recommendation they receive. Privacy is quantified through a distance metric between declared and an “initial” random rating profile; the latter is assumed to provide a “neutral” starting point for the disclosure of the real profile. We allow different users to have different perception of their privacy through a user-dependent utility function of this distance. The quality of recommendations for each user depends on submitted ratings of all users, including the ratings of the user to whom the recommendation is provided. We prove the existence of a Nash equilibrium point (NEP), and we derive conditions for that. We show that user strategies converge to the NEP after an iterative best-response strategy update sequence that involves circulation of aggregate quantities in the system and no revelation of real ratings. We also present various modes of user cooperation in rating declaration, by which users mutually benefit in terms of privacy. We evaluate and compare cooperative and selfish strategies in their performance in terms of privacy preservation and recommendation quality through real movie datasets.},
  archive      = {J_KIS},
  author       = {Halkidi, Maria and Koutsopoulos, Iordanis},
  doi          = {10.1007/s10115-020-01460-5},
  journal      = {Knowledge and Information Systems},
  month        = {8},
  number       = {8},
  pages        = {3239-3262},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Recommender systems with selfish users},
  volume       = {62},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Feature extraction from null and non-null spaces of kernel
local discriminant embedding. <em>KIS</em>, <em>62</em>(8), 3217–3238.
(<a href="https://doi.org/10.1007/s10115-020-01457-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Extracting discriminative features and reducing the dimensionality of data are two main objectives of manifold learning. Among different techniques, nonlinear manifold learning methods have been proposed in order to extract features from data which are not linearly distributed. Kernel trick is one of the famous nonlinear techniques which helps to project the data without an explicit mapping which can be used in combination with different linear techniques (e.g., Linear discriminant analysis and local discriminant embedding (LDE)). In this paper, we propose a Two Subspace-based Kernel Local Discriminant Embedding (TSKLDE) method which extract features from both non-null and null space of the within-class locality preserving scatter matrix of LDE in the kernel space. We evaluated the proposed algorithm using three publicly available face databases. The obtained results demonstrate that the use of both features in TSKLDE leads to more noise tolerant features compared to other kernel methods and to higher discriminant ability than many existing manifold learning techniques.},
  archive      = {J_KIS},
  author       = {Bosaghzadeh, A. and Dornaika, F.},
  doi          = {10.1007/s10115-020-01457-0},
  journal      = {Knowledge and Information Systems},
  month        = {8},
  number       = {8},
  pages        = {3217-3238},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Feature extraction from null and non-null spaces of kernel local discriminant embedding},
  volume       = {62},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Simple supervised dissimilarity measure: Bolstering
iForest-induced similarity with class information without learning.
<em>KIS</em>, <em>62</em>(8), 3203–3216. (<a
href="https://doi.org/10.1007/s10115-020-01454-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing distance metric learning methods require optimisation to learn a feature space to transform data—this makes them computationally expensive in large datasets. In classification tasks, they make use of class information to learn an appropriate feature space. In this paper, we present a simple supervised dissimilarity measure which does not require learning or optimisation. It uses class information to measure dissimilarity of two data instances in the input space directly. It is a supervised version of an existing data-dependent dissimilarity measure called $$m_\mathrm{e}$$ . Our empirical results in k-NN and LVQ classification tasks show that the proposed simple supervised dissimilarity measure generally produces predictive accuracy better than or at least as good as existing state-of-the-art supervised and unsupervised dissimilarity measures.},
  archive      = {J_KIS},
  author       = {Wells, Jonathan R. and Aryal, Sunil and Ting, Kai Ming},
  doi          = {10.1007/s10115-020-01454-3},
  journal      = {Knowledge and Information Systems},
  month        = {8},
  number       = {8},
  pages        = {3203-3216},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Simple supervised dissimilarity measure: Bolstering iForest-induced similarity with class information without learning},
  volume       = {62},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Lexifield: A system for the automatic building of lexicons
by semantic expansion of short word lists. <em>KIS</em>, <em>62</em>(8),
3181–3201. (<a
href="https://doi.org/10.1007/s10115-020-01451-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present Lexifield, a fully automatic language-independent system for building domain-specific lexicons from a short list of terms defining the domain. Lexifield relies on a pre-trained word embedding model, a definition dictionary and a dictionary of synonyms. To evaluate this system, four lexicons have been generated: one lexicon in French for the topic “son” (“sound”) and three lexicons in English for the topics “sound”, “taste” and “odour”. As compared to other word embedding-based systems and a state-of-the-art sensorial lexicon, Sensicon, our system achieves better precision and recall on reference lists extracted from manually created resources such as Roget’s Thesaurus.},
  archive      = {J_KIS},
  author       = {Mpouli, Suzanne and Beigbeder, Michel and Largeron, Christine},
  doi          = {10.1007/s10115-020-01451-6},
  journal      = {Knowledge and Information Systems},
  month        = {8},
  number       = {8},
  pages        = {3181-3201},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Lexifield: A system for the automatic building of lexicons by semantic expansion of short word lists},
  volume       = {62},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). SUM-optimal histograms for approximate query processing.
<em>KIS</em>, <em>62</em>(8), 3155–3180. (<a
href="https://doi.org/10.1007/s10115-020-01450-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we study the problem of the SUM query approximation with histograms. We define a new kind of histogram called the SUM-optimal histogram which can provide better estimation result for the SUM queries than the traditional equi-depth and V-optimal histograms. We propose three methods for the histogram construction. The first one is a dynamic programming method, and the other two are approximate methods. We use a greedy strategy to insert separators into a histogram and use the stochastic gradient descent method to improve the accuracy of separators. The experimental results indicate that our method can provide better estimations for the SUM queries than the equi-depth and V-optimal histograms.},
  archive      = {J_KIS},
  author       = {Zhang, Meifan and Wang, Hongzhi and Li, Jianzhong and Gao, Hong},
  doi          = {10.1007/s10115-020-01450-7},
  journal      = {Knowledge and Information Systems},
  month        = {8},
  number       = {8},
  pages        = {3155-3180},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {SUM-optimal histograms for approximate query processing},
  volume       = {62},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Improved covering-based collaborative filtering for new
users’ personalized recommendations. <em>KIS</em>, <em>62</em>(8),
3133–3154. (<a
href="https://doi.org/10.1007/s10115-020-01455-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {User-based collaborative filtering (UBCF) is widely used in recommender systems (RSs) as one of the most successful approaches, but traditional UBCF cannot provide recommendations with satisfactory accuracy and diversity simultaneously. Covering-based collaborative filtering (CBCF) is a useful approach that we have proposed in our previous work, which greatly improves the traditional UBCF and could provide satisfactory recommendations to an active user which often has sufficient rating information. However, different from an active user, a new user in RSs often has special characteristics (e.g., fewer ratings or ratings concentrating on popular items), and the previous CBCF approach cannot provide satisfactory recommendations for a new user. In this paper, aiming to provide personalized recommendations for a new user, through a detailed analysis of the characteristics of new users, we reconstruct a decision class to improve the previous CBCF and utilize the covering reduction algorithm in covering-based rough sets to remove redundant candidate neighbors for a new user. Furthermore, unlike the previous CBCF, our improved CBCF could provide personalized recommendations without needing special additional information. Experimental results suggest that for the sparse datasets that often occur in real RSs, the improved CBCF significantly outperforms those of existing work and can provide personalized recommendations for a new user with satisfactory accuracy and diversity simultaneously.},
  archive      = {J_KIS},
  author       = {Zhang, Zhipeng and Kudo, Yasuo and Murai, Tetsuya and Ren, Yonggong},
  doi          = {10.1007/s10115-020-01455-2},
  journal      = {Knowledge and Information Systems},
  month        = {8},
  number       = {8},
  pages        = {3133-3154},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Improved covering-based collaborative filtering for new users’ personalized recommendations},
  volume       = {62},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). L-BiX: Incremental sliding-window aggregation over data
streams using linear bidirectional aggregating indexes. <em>KIS</em>,
<em>62</em>(8), 3107–3131. (<a
href="https://doi.org/10.1007/s10115-020-01444-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The number of real-time information sources, or so-called streams, has rapidly increased, leading to a greater demand for complex analyses over streams. Although many stream analysis methods exist, aggregation is fundamental to ascertain higher levels of knowledge from raw data. In particular, sliding-window aggregation, where aggregations over sliding windows are repeatedly computed, is useful in many real-life applications. Two stacks is the state-of-the-art method to compute sliding-window aggregations incrementally with a O(1) time complexity. However, its performance seriously degrades as the window size increases due to the high overhead to maintain its index. To address this problem, this paper proposes a linear bidirectional index (L-BiX) that exploits two kinds of partial aggregations. Specifically, forward (old-to-new) and backward (new-to-old) aggregations allow efficient computations in an incremental manner. The proposed algorithm requires the same time complexity as two stacks (O(1)). Our experimental evaluation shows that the throughput of L-BiX can be faster by up to 1.71 times than that of two stacks with a 50% smaller memory footprint.},
  archive      = {J_KIS},
  author       = {Bou, Savong and Kitagawa, Hiroyuki and Amagasa, Toshiyuki},
  doi          = {10.1007/s10115-020-01444-5},
  journal      = {Knowledge and Information Systems},
  month        = {8},
  number       = {8},
  pages        = {3107-3131},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {L-BiX: Incremental sliding-window aggregation over data streams using linear bidirectional aggregating indexes},
  volume       = {62},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Online anomaly search in time series: Significant online
discords. <em>KIS</em>, <em>62</em>(8), 3083–3106. (<a
href="https://doi.org/10.1007/s10115-020-01453-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The aim of this work is to obtain a useful anomaly definition for online analysis of time series. The idea is to develop an anomaly concept which is sustainable for long-lived and frequent streamings. As a solution, we provide an adaptation of the discord concept, which has been successfully used for anomaly detection on time series. An online approach implies the frequent processing of a data streaming for timely providing anomaly alerts. This requires a modification since discord search is not exactly decomposable in its original definition. With a statistical approach, allowing to rate the significance of the discords of each analysis, it has been possible to obtain a solution where the number of false positives is minimized. The new online anomalies are called significant online discords (sods). As a novel feature, sod search determines the quantity of anomalies in the time series under investigation. The search for sods has been implemented and its properties validated with synthetic and real data. As a result, we found that sods can be considered as a useful new tool for anomaly detection in fast streaming time series or Big Data contexts.},
  archive      = {J_KIS},
  author       = {Avogadro, Paolo and Palonca, Luca and Dominoni, Matteo Alessandro},
  doi          = {10.1007/s10115-020-01453-4},
  journal      = {Knowledge and Information Systems},
  month        = {8},
  number       = {8},
  pages        = {3083-3106},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Online anomaly search in time series: Significant online discords},
  volume       = {62},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Decision support for personalized hospital choice using the
DEX hierarchical model with SMAA. <em>KIS</em>, <em>62</em>(8),
3059–3082. (<a
href="https://doi.org/10.1007/s10115-020-01448-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Despite an ever-growing personalized demand for patients’ hospital choice, little systematic work has examined the decision process that considers the diversity of medical service demand. In this paper, we develop an intelligence decision framework to explore multi-source uncertain information in hospital choice. The framework employs a novel SMAA-DEX method to generate a ranking list of hospital alternatives based on Decision EXpert (DEX) hierarchical model and stochastic multicriteria acceptability analysis (SMAA) in personalized hospital choice. To conduct the multi-source information fusion under uncertainty, the SMAA-DEX method produces the central weight vector considering the ordinal weight and random weight and estimates the holistic acceptability indices for each alternative. By collecting hospital statistics, third-party evaluations and personal patient information in the real world, we verify our method for personalized hospital choice in terms of different preferences such as distance, ranking and income. The results of the experiments demonstrate the effectiveness of the proposed approach, which not only effectively processes various types of hospital choice, but also accomplishes uncertain reasoning of multi-source online information.},
  archive      = {J_KIS},
  author       = {Chen, Yi and Ding, Shuai and Zheng, Handong and Zhang, Yanchun and Yang, Shanlin},
  doi          = {10.1007/s10115-020-01448-1},
  journal      = {Knowledge and Information Systems},
  month        = {8},
  number       = {8},
  pages        = {3059-3082},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Decision support for personalized hospital choice using the DEX hierarchical model with SMAA},
  volume       = {62},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Supervised learning as an inverse problem based on
non-smooth loss function. <em>KIS</em>, <em>62</em>(8), 3039–3058. (<a
href="https://doi.org/10.1007/s10115-020-01439-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper is concerned by solving supervised machine learning problem as an inverse problem. Recently, many works have focused on defining a relationship between supervised learning and the well-known inverse problems. However, this connection between the learning problem and the inverse one has been done in the particular case where the inverse problem is reformulated as a minimization problem with a quadratic cost functional ( $$L^2$$ cost functional). Although, it is well known that the cost functional can be $$L^1$$ , $$L^2$$ or any positive function that measures the gap between the predicted data and the observed one. Indeed, the use of $$L^1$$ loss function for supervised learning problem gives more consistent results (see Rosasco et al. in Neural Comput 16:1063–1076, 2004). This strengthens the idea of reformulating the inverse problem, associated to machine learning problem, into a minimization problem using $$ L^{1}$$ functional. However, the $$L^{1}$$ loss function is non-differentiable, which precludes the use of standard optimization tools. To overcome this difficulty, we propose in this paper a new technique of approximation based on the reformulation of the associated inverse problem into a minimizing one of a slanting cost functional Chen et al. (MIS Q Manag Inf Syst 36:1165–1188, 2012), which is solved using Tikhonov regularization and Newton’s method. This approach leads to an efficient numerical algorithm allowing us to solve supervised learning problem in the most general framework. To confirm this, we present some numerical results showing the efficiency of the proposed approach. Furthermore, the numerical experiment validation is made through academic and real-life data. Thus, the comparison with existing methods and numerical stability of the algorithm is presented in order to show that our approach is better in terms of convergence speed and quality of predicted models.},
  archive      = {J_KIS},
  author       = {Lyaqini, Soufiane and Quafafou, Mohamed and Nachaoui, Mourad and Chakib, Abdelkrim},
  doi          = {10.1007/s10115-020-01439-2},
  journal      = {Knowledge and Information Systems},
  month        = {8},
  number       = {8},
  pages        = {3039-3058},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Supervised learning as an inverse problem based on non-smooth loss function},
  volume       = {62},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Exploiting review embedding and user attention for item
recommendation. <em>KIS</em>, <em>62</em>(8), 3015–3038. (<a
href="https://doi.org/10.1007/s10115-020-01447-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As a valuable source of user preferences and item properties, reviews have been widely leveraged in many approaches to enhance the performance of recommender systems. Although encouraging success has been obtained, there are two more weaknesses need to be addressed. (1) Most approaches represent users or items merely based on the modeling of review texts, but ignore the potential and latent preferences beyond textual information. (2) Existing methods tend to blindly merge all the previous reviews for user profiling. However, it may be less effective because different interacted items may play distinct roles. Hence, indiscriminately aligning interacted items may limit the model flexibility and performance. In this paper, with the desire to fill these gaps, we design a novel attentive deep review-based recommendation method. In specific, we complement the item representation by an auxiliary vector, based on which a user is then attentively profiled by her posted items to predict the likeness for the target item. Extensive experiments on five real-world datasets demonstrate that our model can not only significantly outperform the state-of-the-art methods, but also provide intuitive explanations to the recommendations.},
  archive      = {J_KIS},
  author       = {Sun, Yatong and Guo, Guibing and Chen, Xu and Zhang, Penghai and Wang, Xingwei},
  doi          = {10.1007/s10115-020-01447-2},
  journal      = {Knowledge and Information Systems},
  month        = {8},
  number       = {8},
  pages        = {3015-3038},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Exploiting review embedding and user attention for item recommendation},
  volume       = {62},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Random walk-based entity representation learning and
re-ranking for entity search. <em>KIS</em>, <em>62</em>(8), 2989–3013.
(<a href="https://doi.org/10.1007/s10115-020-01445-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Linked Data (LD) has become a valuable source of factual records, and entity search is a fundamental task in LD. The task is, given a query consisting of a set of keywords, to retrieve a set of relevant entities in LD. The state-of-the-art approaches for entity search are based on information retrieval techniques. This paper first examines these approaches with a traditional evaluation metric, recall@k, to reveal their potential for improvement. To obtain evidence for the potentials, an investigation is carried out on the relationship between queries and answer entities in terms of path lengths on a graph of LD. On the basis of the investigation, learning representations of entities are dealt with. The existing methods of entity search are based on heuristics that determine relevant fields (i.e., predicates and related entities) to constitute entity representations. Since the heuristics require burdensome human decisions, this paper is aimed at removing the burden with a graph proximity measurement. To this end, in this paper, RWRDoc is proposed. It is an RWR (random walk with restart)-based representation learning method that learns representations of entities by using weighted combinations of representations of reachable entities w.r.t. RWR. RWRDoc is mainly designed to improve recall scores; therefore, as shown in experiments, it lacks capability in ranking. In order to improve the ranking qualities, this paper proposes a personalized PageRank-based re-ranking method, PPRSD (Personalized PageRank-based Score Distribution), for the retrieved results. PPRSD distributes relevance scores calculated by text-based entity search methods in a personalized PageRank manner. Experimental evaluations showcase that RWRDoc can improve search qualities in terms of recall@1000 and PPRSD can compensate for RWRDoc’s insufficient ranking capability, and the evaluations confirmed this compensation.},
  archive      = {J_KIS},
  author       = {Komamizu, Takahiro},
  doi          = {10.1007/s10115-020-01445-4},
  journal      = {Knowledge and Information Systems},
  month        = {8},
  number       = {8},
  pages        = {2989-3013},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Random walk-based entity representation learning and re-ranking for entity search},
  volume       = {62},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A survey of state-of-the-art approaches for emotion
recognition in text. <em>KIS</em>, <em>62</em>(8), 2937–2987. (<a
href="https://doi.org/10.1007/s10115-020-01449-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Emotion recognition in text is an important natural language processing (NLP) task whose solution can benefit several applications in different fields, including data mining, e-learning, information filtering systems, human–computer interaction, and psychology. Explicit emotion recognition in text is the most addressed problem in the literature. The solution to this problem is mainly based on identifying keywords. Implicit emotion recognition is the most challenging problem to solve because such emotion is typically hidden within the text, and thus, its solution requires an understanding of the context. There are four main approaches for implicit emotion recognition in text: rule-based approaches, classical learning-based approaches, deep learning approaches, and hybrid approaches. In this paper, we critically survey the state-of-the-art research for explicit and implicit emotion recognition in text. We present the different approaches found in the literature, detail their main features, discuss their advantages and limitations, and compare them within tables. This study shows that hybrid approaches and learning-based approaches that utilize traditional text representation with distributed word representation outperform the other approaches on benchmark corpora. This paper also identifies the sets of features that lead to the best-performing approaches; highlights the impacts of simple NLP tasks, such as part-of-speech tagging and parsing, on the performances of these approaches; and indicates some open problems.},
  archive      = {J_KIS},
  author       = {Alswaidan, Nourah and Menai, Mohamed El Bachir},
  doi          = {10.1007/s10115-020-01449-0},
  journal      = {Knowledge and Information Systems},
  month        = {8},
  number       = {8},
  pages        = {2937-2987},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {A survey of state-of-the-art approaches for emotion recognition in text},
  volume       = {62},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Cyber security incidents analysis and classification in a
case study of korean enterprises. <em>KIS</em>, <em>62</em>(7),
2917–2935. (<a
href="https://doi.org/10.1007/s10115-020-01452-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The increasing amount and complexity of Cyber security attacks in recent years have made text analysis and data mining techniques an important factor in discovering features of such attacks and detecting future security threats. In this paper, we report on the results of a recent case study that involved the analysis of a community data set collected from five small and medium companies in Korea. The data set represents Cyber security incidents and response actions. We investigated in the study the kind of problems concerned with the prediction of response actions to future incidents from features of past incidents. Our analysis is based on text mining methods, such as n-gram and bag-of-words, as well as on machine learning algorithms for the classification of incidents and their response actions. Based on the results of the study, we also suggest an experience-sharing model, which we use to demonstrate how companies may share their trained classifiers without the sharing of their individual data sets in a collaborative environment.},
  archive      = {J_KIS},
  author       = {Mohasseb, Alaa and Aziz, Benjamin and Jung, Jeyong and Lee, Julak},
  doi          = {10.1007/s10115-020-01452-5},
  journal      = {Knowledge and Information Systems},
  month        = {7},
  number       = {7},
  pages        = {2917-2935},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Cyber security incidents analysis and classification in a case study of korean enterprises},
  volume       = {62},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Review selection based on content quality. <em>KIS</em>,
<em>62</em>(7), 2893–2915. (<a
href="https://doi.org/10.1007/s10115-020-01474-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Consumer-generated reviews have become increasingly important in decision-making processes for customers. Meanwhile, the overwhelming quantity of review data makes it extremely difficult to find useful information from it. A considerable amount of studies have attempted to address this problem by selecting reviews that might be helpful for and preferred by users. However, the performance of existing methods is far from ideal. One reason is because of lacking effective criteria to assess the quality of reviews. In this paper, we propose two novel measures, i.e. feature relevance and feature comprehensiveness, to assess the quality of reviews in terms of review content. A review selection approach is presented to select a set of reviews with high quality based on the two measures. Experiments on real-world review datasets show that our proposed method can assess the review quality effectively to improve the performance of review selection.},
  archive      = {J_KIS},
  author       = {Tian, Nan and Xu, Yue and Li, Yuefeng},
  doi          = {10.1007/s10115-020-01474-z},
  journal      = {Knowledge and Information Systems},
  month        = {7},
  number       = {7},
  pages        = {2893-2915},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Review selection based on content quality},
  volume       = {62},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Coverage-based query subtopic diversification leveraging
semantic relevance. <em>KIS</em>, <em>62</em>(7), 2873–2891. (<a
href="https://doi.org/10.1007/s10115-020-01470-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Generally, users are reserved in describing their search intention when submitting queries into the search engine. Therefore, a large number of search queries are usually short, ambiguous and tend to have multiple interpretations. With the gigantic size of the web, ignoring the information needs underlying such queries can misguide the search engine. To mitigate these issues, an effective approach is to diversify the search results considering the query subtopics with diverse intents. The task of identifying possible subtopics with diverse intents underlying a query is known as subtopic mining. This paper is aimed at mining and diversifying subtopics underlying a query. Our method first exacts noun phrases containing the query terms from the top-retrieved web documents. We also extract query suggestions and completions from commercial search engines. The extracted candidates highly related to the query are then selected as subtopics. We introduce a new relatedness score function to estimate the degree of relatedness between the query and the candidate. To estimate the relevancy between the query and the subtopic, this paper introduces a semantic relevance measure using a locally trained sentence embedding model. Finally, we propose a novel coverage-based diversification technique to rank the subtopics combining their relevancy and the coverage estimated by the web documents. The experimental results on two NTCIR English subtopic mining datasets demonstrate that our proposed method achieves new state-of-the-art performance and significantly outperforms some known related methods in terms of relevance (D-nDCG) and diversity (D#-nDCG) metric at cut of 10.},
  archive      = {J_KIS},
  author       = {Shajalal, Md. and Aono, Masaki},
  doi          = {10.1007/s10115-020-01470-3},
  journal      = {Knowledge and Information Systems},
  month        = {7},
  number       = {7},
  pages        = {2873-2891},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Coverage-based query subtopic diversification leveraging semantic relevance},
  volume       = {62},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A compact firefly algorithm for matching biomedical
ontologies. <em>KIS</em>, <em>62</em>(7), 2855–2871. (<a
href="https://doi.org/10.1007/s10115-020-01443-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Biomedical ontologies have gained particular relevance in the life science domain due to its prominent role in representing knowledge in this domain. However, the existing biomedical ontologies could define the same biomedical concept in different ways, which yields the biomedical ontology heterogeneous problem. To implement the inter-operability among the biomedical ontologies, it is critical to establish the semantic links between heterogenous biomedical concepts, so-called biomedical ontology matching. Since modeling the ontology matching problem is a complex and time-consuming task, swarm intelligent algorithm (SIA) becomes a state-of-the-art methodology for solving this problem. However, when addressing the biomedical ontology matching problem, the existing SIA-based matchers tend to be inefficient due to biomedical ontology’s large-scale concepts and complex semantic relationships. In this work, we propose a compact firefly algorithm (CFA), where the explicit representation of the population is replaced by a probability distribution and two compact movement operators are presented to save the memory consumption and runtime of the population-based SIAs. We exploit the anatomy track, disease and phenotype track and biodiversity and ecology track from the ontology alignment evaluation initiative (OAEI) to test CFA-based matcher’s performance. The experimental results show that CFA can improve the FA-based matcher’s memory consumption and runtime by, respectively, 68.92% and 38.97% on average, and its results significantly outperform other SIA-based matchers and OAEI’s participants.},
  archive      = {J_KIS},
  author       = {Xue, Xingsi},
  doi          = {10.1007/s10115-020-01443-6},
  journal      = {Knowledge and Information Systems},
  month        = {7},
  number       = {7},
  pages        = {2855-2871},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {A compact firefly algorithm for matching biomedical ontologies},
  volume       = {62},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). PCA-based drift and shift quantification framework for
multidimensional data. <em>KIS</em>, <em>62</em>(7), 2835–2854. (<a
href="https://doi.org/10.1007/s10115-020-01438-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Concept drift is a serious problem confronting machine learning systems in a dynamic and ever-changing world. In order to manage concept drift it may be useful to first quantify it by measuring the distance between distributions that generate data before and after a drift. There is a paucity of methods to do so in the case of multidimensional numeric data. This paper provides an in-depth analysis of the PCA-based change detection approach, identifies shortcomings of existing methods and shows how this approach can be used to measure a drift, not merely detect it.},
  archive      = {J_KIS},
  author       = {Goldenberg, Igor and Webb, Geoffrey I.},
  doi          = {10.1007/s10115-020-01438-3},
  journal      = {Knowledge and Information Systems},
  month        = {7},
  number       = {7},
  pages        = {2835-2854},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {PCA-based drift and shift quantification framework for multidimensional data},
  volume       = {62},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A multi-breakpoints approach for symbolic discretization of
time series. <em>KIS</em>, <em>62</em>(7), 2795–2834. (<a
href="https://doi.org/10.1007/s10115-020-01437-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Time series discretization is a technique commonly used to tackle time series classification problems. This manuscript presents an enhanced multi-objective approach for the symbolic discretization of time series called eMODiTS. The method proposed uses a different breakpoints vector, defined per each word segment, to increase the search space of the discretization schemes. eMODiTS’ search mechanism is the well-known evolutionary multi-objective algorithm NSGA-II, which finds a set of possible solutions according to entropy, complexity, and information loss estimations. Final solutions were appraised depending on the misclassification rate computed through the decision tree classifier. The trees obtained also produce graphical and significant information from the regions, relationships, or patterns in each database. Our proposal was compared against ten state-of-the-art time symbolic discretization algorithms. The results suggest that our proposal finds a suitable discretization scheme regarding classification, dimensionality, cardinality reduction, and information loss.},
  archive      = {J_KIS},
  author       = {Márquez-Grajales, Aldo and Acosta-Mesa, Héctor-Gabriel and Mezura-Montes, Efrén and Graff, Mario},
  doi          = {10.1007/s10115-020-01437-4},
  journal      = {Knowledge and Information Systems},
  month        = {7},
  number       = {7},
  pages        = {2795-2834},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {A multi-breakpoints approach for symbolic discretization of time series},
  volume       = {62},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Fast and memory-efficient algorithms for high-order tucker
decomposition. <em>KIS</em>, <em>62</em>(7), 2765–2794. (<a
href="https://doi.org/10.1007/s10115-019-01435-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-aspect data appear frequently in web-related applications. For example, product reviews are quadruplets of the form (user, product, keyword, timestamp), and search-engine logs are quadruplets of the form (user, keyword, location, timestamp). How can we analyze such web-scale multi-aspect data on an off-the-shelf workstation with a limited amount of memory? Tucker decomposition has been used widely for discovering patterns in such multi-aspect data, which are naturally expressed as large but sparse tensors. However, existing Tucker decomposition algorithms have limited scalability, failing to decompose large-scale high-order ($$\ge $$ 4) tensors, since they explicitly materialize intermediate data, whose size grows exponentially with the order. To address this problem, which we call “Materialization Bottleneck,” we propose S-HOT, a scalable algorithm for high-order Tucker decomposition. S-HOT minimizes materialized intermediate data by using an on-the-fly computation, and it is optimized for disk-resident tensors that are too large to fit in memory. We theoretically analyze the amount of memory and the number of data scans required by S-HOT. Moreover, we empirically show that S-HOT handles tensors with higher order, dimensionality, and rank than baselines. For example, S-HOT successfully decomposes a real-world tensor from the Microsoft Academic Graph on an off-the-shelf workstation, while all baselines fail. Especially, in terms of dimensionality, S-HOT decomposes 1000 $$\times $$ larger tensors than baselines.},
  archive      = {J_KIS},
  author       = {Zhang, Jiyuan and Oh, Jinoh and Shin, Kijung and Papalexakis, Evangelos E. and Faloutsos, Christos and Yu, Hwanjo},
  doi          = {10.1007/s10115-019-01435-1},
  journal      = {Knowledge and Information Systems},
  month        = {7},
  number       = {7},
  pages        = {2765-2794},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Fast and memory-efficient algorithms for high-order tucker decomposition},
  volume       = {62},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Navigation leads for exploratory search and navigation in
digital libraries. <em>KIS</em>, <em>62</em>(7), 2739–2764. (<a
href="https://doi.org/10.1007/s10115-019-01434-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Exploratory search (in contrary to the traditional lookup search) is characterized by the search tasks that have exploration, learning, and investigation as their goals. An example of this task in the domain of digital libraries is exploration of a new domain, a task that is typically performed by a researcher novice, such as a master’s or a doctoral student. To support the researcher novices in this task, we proposed an approach of exploratory search and navigation using navigation leads, with which we augment the search results, and which serve as navigation starting points allowing users to follow a specific path by filtering only documents pertinent to the selected lead. In this paper, we present a method of selection of navigation leads considering their navigational value in the form of a corpus relevance. We examined this method by the means of an offline evaluation on the dataset from a bookmarking service Annota. We showed that considering the corpus relevance helps to cover significantly more (relevant) documents when conducting the exploratory search. In addition, our relevance metric combining document and corpus relevance of a lead outperformed the popularity metric based on the frequency of the term in the document corpus.},
  archive      = {J_KIS},
  author       = {Moro, Robert and Bielikova, Maria},
  doi          = {10.1007/s10115-019-01434-2},
  journal      = {Knowledge and Information Systems},
  month        = {7},
  number       = {7},
  pages        = {2739-2764},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Navigation leads for exploratory search and navigation in digital libraries},
  volume       = {62},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Improving the <span class="math display"><em>ϵ</em></span>
-approximate algorithm for probabilistic classifier chains.
<em>KIS</em>, <em>62</em>(7), 2709–2738. (<a
href="https://doi.org/10.1007/s10115-020-01436-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Probabilistic Classifier Chains are a multi-label classification method which has gained the attention of researchers in recent years. This is because of their ability to optimally estimate the entire joint conditional probability of a label combination through the product rule of probability. Their main drawback is that they require performing an exhaustive search in order to obtain Bayes optimal predictions. This means computing this probability for all possible label combinations before taking a label combination with the highest value of probability. This is the reason why several works have been published in recent years that avoid exploring all combinations, while maintaining optimality. Approaches such as greedy search, beam search and Monte Carlo reduce the computational cost, but at the cost of not ensuring Bayes optimal predictions (although, in general, they provide close to optimal solutions). Methods based on a heuristic search provide optimal predictions, but the computational time has not been as good as expected. In this respect, the $$\epsilon $$-approximate algorithm has been found to be the best inference approach among those that provide Bayes optimal predictions, not only for its optimality, but also for its computational time. However, this paper both theoretically and experimentally shows that it sometimes performs some backtracking during the search for optimal predictions which may prolong the prediction time. The aim of this paper is thus to improve this algorithm by achieving a more direct search. Specifically, it enhances the criterion under which the next node to be expanded is chosen by adding heuristic information, although it is only applicable for linear-based models. The experiments carried out confirm that the improved $$\epsilon $$-approximate algorithm explores fewer nodes and reduces the computational time of the original version.},
  archive      = {J_KIS},
  author       = {Fdez-Díaz, Miriam and Fdez-Díaz, Laura and Mena, Deiner and Montañés, Elena and Quevedo, José Ramón and Coz, Juan José del},
  doi          = {10.1007/s10115-020-01436-5},
  journal      = {Knowledge and Information Systems},
  month        = {7},
  number       = {7},
  pages        = {2709-2738},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Improving the $$\epsilon $$ -approximate algorithm for probabilistic classifier chains},
  volume       = {62},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Fraud detection via behavioral sequence embedding.
<em>KIS</em>, <em>62</em>(7), 2685–2708. (<a
href="https://doi.org/10.1007/s10115-019-01433-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fraud detection is usually compared to finding a needle in a haystack and remains a challenging task because fraudulent acts are buried in massive amounts of normal behavior and true intentions may be disguised in a single snapshot. Indeed, fraudulent incidents usually take place in consecutive time steps to gain illegal benefits, which provides unique clues for probing fraudulent behavior by considering a complete behavioral sequence rather than detecting fraud from a snapshot of behavior. Additionally, fraudulent behavior may involve different parties, such that the interaction patterns between sources and targets can help distinguish fraudulent acts from normal behavior. Therefore, in this paper, we model the attributed behavioral sequences generated from consecutive behaviors in order to capture the sequential patterns, while those that deviate from the pattern can be detected as fraudulence. Considering the characteristics of the behavioral sequence, we propose a novel model, NHA-LSTM, by augmenting the traditional LSTM with a modified forget gate, where the interval time between consecutive time steps is considered. Furthermore, we design a self-historical attention mechanism to allow for long time dependencies, which can help identify repeated or cyclical appearances. In addition, we propose an enhanced network embedding method, FraudWalk, to construct embeddings for the nodes in the interaction network with regard to higher-order interactions and particular time constraints for revealing potential group fraudulence. The node embeddings, along with the feature vectors, are fed into the model to capture the interactions between sources and targets. To validate the effectiveness of sequential behavior embeddings, we experiment on a real-world telecommunication dataset with prediction and classification tasks based on the learned embeddings. The experimental results show that the learned embeddings can better identify fraudulent behavior. Finally, we visualize the weights of the attention mechanism to provide a rational interpretation of human behavioral patterns.},
  archive      = {J_KIS},
  author       = {Liu, Guannan and Guo, Jia and Zuo, Yuan and Wu, Junjie and Guo, Ren-yong},
  doi          = {10.1007/s10115-019-01433-3},
  journal      = {Knowledge and Information Systems},
  month        = {7},
  number       = {7},
  pages        = {2685-2708},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Fraud detection via behavioral sequence embedding},
  volume       = {62},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Verifying the manipulation of data objects according to
business process and data models. <em>KIS</em>, <em>62</em>(7),
2653–2683. (<a
href="https://doi.org/10.1007/s10115-019-01431-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Business processes read and write data objects, usually stored in databases. Although data models and activity-oriented business process models originate from different paradigms, they need to work together properly. The data object states are transformed during each process instance by the activities of the process model. It is therefore necessary to verify whether the states of the data objects are correct according to the process model, and to discover the states of the stored data objects. This implies determining the relation between the data objects stored in the database, the data objects involved in the process, and the activities that within the business process that create the data objects and modify their states. In order to verify the business process annotated with data states and to reduce the existing gap between data model and business process model, we propose a methodology that includes enlarging the capability to describe data states in business processes; verifying the completeness and consistency of the data states described in accordance with their relation to the business process model; and discovering the states of the data objects stored in the database according to the business process model where they are managed. The methodology is supported by a framework that enables a natural-like language to be employed to describe the states, to apply the necessary algorithms to verify the consistency and completeness of the model, and to determine the states of the stored data objects according to the model described. To validate our proposal, an extension of Activiti$$^{TM}$$ has been implemented and applied to a real example as an illustration of its applicability.},
  archive      = {J_KIS},
  author       = {Pérez-Álvarez, José Miguel and Gómez-López, María Teresa and Eshuis, Rik and Montali, Marco and Gasca, Rafael M.},
  doi          = {10.1007/s10115-019-01431-5},
  journal      = {Knowledge and Information Systems},
  month        = {7},
  number       = {7},
  pages        = {2653-2683},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Verifying the manipulation of data objects according to business process and data models},
  volume       = {62},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Location histogram privacy by sensitive location hiding and
target histogram avoidance/resemblance. <em>KIS</em>, <em>62</em>(7),
2613–2651. (<a
href="https://doi.org/10.1007/s10115-019-01432-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A location histogram is comprised of the number of times a user has visited locations as they move in an area of interest, and it is often obtained from the user in the context of applications such as recommendation and advertising. However, a location histogram that leaves a user’s computer or device may threaten privacy when it contains visits to locations that the user does not want to disclose (sensitive locations), or when it can be used to profile the user in a way that leads to price discrimination and unsolicited advertising (e.g., as “wealthy” or “minority member”). Our work introduces two privacy notions to protect a location histogram from these threats: Sensitive Location Hiding, which aims at concealing all visits to sensitive locations, and Target Avoidance/Resemblance, which aims at concealing the similarity/dissimilarity of the user’s histogram to a target histogram that corresponds to an undesired/desired profile. We formulate an optimization problem around each notion: Sensitive Location Hiding ($${ SLH}$$), which seeks to construct a histogram that is as similar as possible to the user’s histogram but associates all visits with nonsensitive locations, and Target Avoidance/Resemblance ($${ TA}$$/$${ TR}$$), which seeks to construct a histogram that is as dissimilar/similar as possible to a given target histogram but remains useful for getting a good response from the application that analyzes the histogram. We develop an optimal algorithm for each notion, which operates on a notion-specific search space graph and finds a shortest or longest path in the graph that corresponds to a solution histogram. In addition, we develop a greedy heuristic for the $${ TA}$$/$${ TR}$$ problem, which operates directly on a user’s histogram. Our experiments demonstrate that all algorithms are effective at preserving the distribution of locations in a histogram and the quality of location recommendation. They also demonstrate that the heuristic produces near-optimal solutions while being orders of magnitude faster than the optimal algorithm for $${ TA}$$/$${ TR}$$.},
  archive      = {J_KIS},
  author       = {Loukides, Grigorios and Theodorakopoulos, George},
  doi          = {10.1007/s10115-019-01432-4},
  journal      = {Knowledge and Information Systems},
  month        = {7},
  number       = {7},
  pages        = {2613-2651},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Location histogram privacy by sensitive location hiding and target histogram Avoidance/Resemblance},
  volume       = {62},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). On spatial keyword covering. <em>KIS</em>, <em>62</em>(7),
2577–2612. (<a
href="https://doi.org/10.1007/s10115-020-01446-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article introduces and solves a spatial keyword cover problem (SK-Cover for short), which aims to identify the group of spatio-textual objects covering all the keywords in a query and minimizing a distance cost function that leads to fewer objects in the answer set. In a broad sense, SK-Cover has been actively studied in the literature of spatial keyword search, such as the m-closest keywords query and the collective spatial keyword query. However, these existing works focus on minimizing only the largest pairwise distance even though the actual spatial cost is highly influenced by the number of objects in the answer group. Motivated by this, the present article further generalizes the problem definition in such a way that the total cost takes the cardinality of the group as well as the spatial distance. We prove that SK-Cover is not only NP-hard but also does not allow an approximation better than $$O(\log {|T|})$$ in polynomial time, where T is the set of query keywords. We first establish an $$O(\log {|T|})$$-approximation algorithm, which is asymptotically optimal in terms of the approximability of SK-Cover, together with effective accessing strategies and pruning rules to improve the overall efficiency and scalability. Despite the NP-hardness of SK-Cover, this article also develops exact solutions that find the optimal group of objects in a reasonably fast manner in practice, especially when it is required to cover a relatively small number of query keywords. In addition to our algorithmic results, we empirically show that our approximation algorithm always achieves the best accuracy and the efficiency comparable to that of a state-of-the-art algorithm intended for $$m\hbox {CK}$$, a problem similar to yet theoretically easier than SK-Cover, and also demonstrate that our exact algorithm using the proposed approximation scheme runs much faster than the baseline algorithm adapted from the existing solution for $$m\hbox {CK}$$.},
  archive      = {J_KIS},
  author       = {Choi, Dong-Wan and Pei, Jian and Lin, Xuemin},
  doi          = {10.1007/s10115-020-01446-3},
  journal      = {Knowledge and Information Systems},
  month        = {7},
  number       = {7},
  pages        = {2577-2612},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {On spatial keyword covering},
  volume       = {62},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Case notion discovery and recommendation: Automated event
log building on databases. <em>KIS</em>, <em>62</em>(7), 2539–2575. (<a
href="https://doi.org/10.1007/s10115-019-01430-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Process mining techniques use event logs as input. When analyzing complex databases, these event logs can be built in many ways. Events need to be grouped into traces corresponding to a case. Different groupings provide different views on the data. Building event logs is usually a time-consuming, manual task. This paper provides a precise view on the case notion on databases, which enables the automatic computation of event logs. Also, it provides a way to assess event log quality, used to rank event logs with respect to their interestingness. The computational cost of building an event log can be avoided by predicting the interestingness of a case notion, before the corresponding event log is computed. This makes it possible to give recommendations to users, so they can focus on the analysis of the most promising process views. Finally, the accuracy of the predictions and the quality of the rankings generated by our unsupervised technique are evaluated in comparison to the existing regression techniques as well as to state-of-the-art learning to rank algorithms from the information retrieval field. The results show that our prediction technique succeeds at discovering interesting event logs and provides valuable recommendations to users about the perspectives on which to focus the efforts during the analysis.},
  archive      = {J_KIS},
  author       = {de Murillas, E. González López and Reijers, H. A. and van der Aalst, W. M. P.},
  doi          = {10.1007/s10115-019-01430-6},
  journal      = {Knowledge and Information Systems},
  month        = {7},
  number       = {7},
  pages        = {2539-2575},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Case notion discovery and recommendation: Automated event log building on databases},
  volume       = {62},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Label similarity-based weighted soft majority voting and
pairing for crowdsourcing. <em>KIS</em>, <em>62</em>(7), 2521–2538. (<a
href="https://doi.org/10.1007/s10115-020-01475-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Crowdsourcing services provide an efficient and relatively inexpensive approach to obtain substantial amounts of labeled data by employing crowd workers. It is obvious that the labeling qualities of crowd workers directly affect the quality of the labeled data. However, existing label aggregation strategies seldom consider the differences in the quality of workers labeling different instances. In this paper, we argue that a single worker may even have different labeling qualities on different instances. Based on this premise, we propose four new strategies by assigning different weights to workers when labeling different instances. In our proposed strategies, we first use the similarity among worker labels to estimate the specific quality of the worker on different instances, and then we build a classifier to estimate the overall quality of the worker across all instances. Finally, we combine these two qualities to define the weight of the worker labeling a particular instance. Extensive experimental results show that our proposed strategies significantly outperform other existing state-of-the-art label aggregation strategies.},
  archive      = {J_KIS},
  author       = {Tao, Fangna and Jiang, Liangxiao and Li, Chaoqun},
  doi          = {10.1007/s10115-020-01475-y},
  journal      = {Knowledge and Information Systems},
  month        = {7},
  number       = {7},
  pages        = {2521-2538},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Label similarity-based weighted soft majority voting and pairing for crowdsourcing},
  volume       = {62},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A survey of recent methods on deriving topics from twitter:
Algorithm to evaluation. <em>KIS</em>, <em>62</em>(7), 2485–2519. (<a
href="https://doi.org/10.1007/s10115-019-01429-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, studies related to topic derivation in Twitter have gained a lot of interest from businesses and academics. The interconnection between users and information has made social media, especially Twitter, an ultimate platform for propagation of information about events in real time. Many applications require topic derivation from this social media platform. These include, for example, disaster management, outbreak detection, situation awareness, surveillance, and market analysis. Deriving topics from Twitter is challenging due to the short content of the individual posts. The environment itself is also highly dynamic. This paper presents a review of recent methods proposed to derive topics from social media platform from algorithms to evaluations. With regard to algorithms, we classify them based on the features they exploit, such as content, social interactions, and temporal aspects. In terms of evaluations, we discuss the datasets and metrics generally used to evaluate the methods. Finally, we highlight the gaps in the research this far and the problems that remain to be addressed.},
  archive      = {J_KIS},
  author       = {Nugroho, Robertus and Paris, Cecile and Nepal, Surya and Yang, Jian and Zhao, Weiliang},
  doi          = {10.1007/s10115-019-01429-z},
  journal      = {Knowledge and Information Systems},
  month        = {7},
  number       = {7},
  pages        = {2485-2519},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {A survey of recent methods on deriving topics from twitter: Algorithm to evaluation},
  volume       = {62},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Enhancing supervised bug localization with metadata and
stack-trace. <em>KIS</em>, <em>62</em>(6), 2461–2484. (<a
href="https://doi.org/10.1007/s10115-019-01426-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Locating relevant source files for a given bug report is an important task in software development and maintenance. To make the locating process easier, information retrieval methods have been widely used to compute the content similarities between bug reports and source files. In addition to content similarities, various other sources of information such as the metadata and the stack-trace in the bug report can be used to enhance the localization accuracy. In this paper, we propose a supervised topic modeling approach for automatically locating the relevant source files of a bug report. In our approach, we take into account the following five key observations. First, supervised modeling can effectively make use of the existing fixing histories. Second, certain words in bug reports tend to appear multiple times in their relevant source files. Third, longer source files tend to have more bugs. Fourth, metainformation brings additional guidance on the search space. Fifth, buggy source files could be already contained in the stack-trace. By integrating the above five observations, we experimentally show that the proposed method can achieve up to 67.1% improvement in terms of prediction accuracy over its best competitors and scales linearly with the size of the data.},
  archive      = {J_KIS},
  author       = {Wang, Yaojing and Yao, Yuan and Tong, Hanghang and Huo, Xuan and Li, Ming and Xu, Feng and Lu, Jian},
  doi          = {10.1007/s10115-019-01426-2},
  journal      = {Knowledge and Information Systems},
  month        = {6},
  number       = {6},
  pages        = {2461-2484},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Enhancing supervised bug localization with metadata and stack-trace},
  volume       = {62},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). English–arabic collocation extraction to enhance arabic
collocation identification. <em>KIS</em>, <em>62</em>(6), 2439–2459. (<a
href="https://doi.org/10.1007/s10115-019-01428-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Bilingual collocation extraction could improve the performance of monolingual extraction. This is especially true for the English–Arabic pair, as difficulties of Arabic collocation extraction can be overcome. We present in this paper two novel approaches for extracting both monolingual and bilingual collocations. The monolingual extraction approach is hybrid, based on linguistic patterns and statistical measures. We propose during statistical filtering to combine vector-based measures with different association measures via a voting procedure. The bilingual extraction capitalizes on different cues (position, frequency, cross-language correspondence between POS-patterns, distribution, translation). It allows enhancing the monolingual collocation extraction by considering not only collocation equivalents with direct translation. Indeed, it can validate unconfirmed collocations because they translate confirmed ones. The results showed, in particular, how the extraction of Arabic collocations can be improved by extracting English–Arabic ones. The precision of extracting Arabic collocations moved upward, respectively, from about 86 to 96%.},
  archive      = {J_KIS},
  author       = {Zribi, Chiraz Ben Othmane},
  doi          = {10.1007/s10115-019-01428-0},
  journal      = {Knowledge and Information Systems},
  month        = {6},
  number       = {6},
  pages        = {2439-2459},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {English–Arabic collocation extraction to enhance arabic collocation identification},
  volume       = {62},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Missing data imputation using decision trees and fuzzy
clustering with iterative learning. <em>KIS</em>, <em>62</em>(6),
2419–2437. (<a
href="https://doi.org/10.1007/s10115-019-01427-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Various imputation approaches have been proposed to address the issue of missing values in data mining and machine learning applications. To improve the accuracy of missing data imputation, this paper proposes a new method called DIFC by integrating the merits of decision tress and fuzzy clustering into an iterative learning approach. To compare the performance of the DIFC method against five effective imputation methods, extensive experiments are conducted on six widely used datasets with numerical and categorical missing data, and with various amounts and types of missing values. The experimental results show that the DIFC method outperforms other methods in terms of imputation accuracy. Further experiments on the effect of missing value types demonstrate the robustness of the DIFC method in dealing with different types of missing values. This paper contributes to missing data imputation research by providing an accurate and robust method.},
  archive      = {J_KIS},
  author       = {Nikfalazar, Sanaz and Yeh, Chung-Hsing and Bedingfield, Susan and Khorshidi, Hadi A.},
  doi          = {10.1007/s10115-019-01427-1},
  journal      = {Knowledge and Information Systems},
  month        = {6},
  number       = {6},
  pages        = {2419-2437},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Missing data imputation using decision trees and fuzzy clustering with iterative learning},
  volume       = {62},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Semi-supervised multi-label feature learning via label
enlarged discriminant analysis. <em>KIS</em>, <em>62</em>(6), 2383–2417.
(<a href="https://doi.org/10.1007/s10115-019-01409-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-label learning arises frequently in various domains including computer vision and machine learning and has attracted great interest in the last decades. However, current multi-label classification methods may be deficient in many real applications with following two constraints: (1) lack of sufficient labeled data and (2) high dimensionality in feature space. To address these challenges, in this paper, we propose a new semi-supervised multi-label feature learning algorithm named as label enlarged discriminant analysis. Different from supervised multi-label learning methods, the proposed algorithm can utilize the information from both labeled data and unlabeled data in an effective way. The proposed algorithm enlarges the multi-label information from the labeled data to the unlabeled data through a special designed multi-label label propagation method. Thus, it can take both labeled and unlabeled data into consideration. It then learns a transformation matrix to perform feature learning to reduce the high dimensionality by incorporating the enlarged multi-label information. In this way, the proposed algorithm can preserve more discriminative information by utilizing both labeled and unlabeled data simultaneously. We have analyzed in theory and extensive experimental results are carried out upon several data sets. They all validate the effectiveness of the proposed algorithm.},
  archive      = {J_KIS},
  author       = {Guo, Baolin and Tao, Hong and Hou, Chenping and Yi, Dongyun},
  doi          = {10.1007/s10115-019-01409-3},
  journal      = {Knowledge and Information Systems},
  month        = {6},
  number       = {6},
  pages        = {2383-2417},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Semi-supervised multi-label feature learning via label enlarged discriminant analysis},
  volume       = {62},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Extreme pivots: A pivot selection strategy for faster metric
search. <em>KIS</em>, <em>62</em>(6), 2349–2382. (<a
href="https://doi.org/10.1007/s10115-019-01423-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This manuscript presents the extreme pivots (EP) metric index, a data structure, to speed up exact proximity searching in the metric space model. For the EP, we designed an automatic rule to select the best pivots for a dataset working on limited memory resources. The net effect is that our approach solves queries efficiently with a small memory footprint, and without a prohibitive construction time. In contrast with other related structures, our performance is achieved automatically without dealing directly with the index’s parameters, using optimization techniques over a model of the index. The EP’s model is studied in-depth in this contribution. In practical terms, an interested user only needs to provide the available memory and a sample of the query distribution as parameters. The resulting index is quickly built, and has a good trade-off among memory usage, preprocessing, and search time. We provide an extensive experimental comparison with state-of-the-art searching methods. We also carefully compared the performance of metric indexes in several scenarios, firstly with synthetic data to characterize performance as a function of the intrinsic dimension and the size of the database, and also in different real-world datasets with excellent results.},
  archive      = {J_KIS},
  author       = {Ruiz, Guillermo and Chavez, Edgar and Ruiz, Ubaldo and Tellez, Eric S.},
  doi          = {10.1007/s10115-019-01423-5},
  journal      = {Knowledge and Information Systems},
  month        = {6},
  number       = {6},
  pages        = {2349-2382},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Extreme pivots: A pivot selection strategy for faster metric search},
  volume       = {62},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Collaborative online ranking algorithms for multitask
learning. <em>KIS</em>, <em>62</em>(6), 2327–2348. (<a
href="https://doi.org/10.1007/s10115-019-01406-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {There are many applications in which it is desirable to rank or order instances that belong to several different but related problems or tasks. Although unique, the individual ranking problem often shares characteristics with other problems in the group. Conventional ranking methods treat each task independently without considering the latent commonalities. In this paper, we study the problem of learning to rank instances that belong to multiple related tasks from the multitask learning perspective. We consider a case in which the information that is learned for a task can be used to enhance the learning of other tasks and propose a collaborative multitask ranking method that learns several ranking models for each of the related tasks together. The proposed algorithms operate in rounds by learning models from a sequence of data instances one at a time. In each round, our algorithms receive an instance that belongs to a task and make a prediction using the task’s ranking model. The model is then updated by leveraging both the task-specific data and the information provided by other models in a collaborative way. The experimental results demonstrate that our algorithms can improve the overall performance of ranking multiple correlated tasks collaboratively. Furthermore, our algorithms can scale well to large amounts of data and are particularly suitable for real-world applications in which data arrive continuously.},
  archive      = {J_KIS},
  author       = {Li, Guangxia and Zhao, Peilin and Mei, Tao and Yang, Peng and Shen, Yulong and Chang, Julian Kuiyu and Hoi, Steven C. H.},
  doi          = {10.1007/s10115-019-01406-6},
  journal      = {Knowledge and Information Systems},
  month        = {6},
  number       = {6},
  pages        = {2327-2348},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Collaborative online ranking algorithms for multitask learning},
  volume       = {62},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Outsourcing analyses on privacy-protected multivariate
categorical data stored in untrusted clouds. <em>KIS</em>,
<em>62</em>(6), 2301–2326. (<a
href="https://doi.org/10.1007/s10115-019-01424-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Outsourcing data storage and computation to the cloud is appealing due to the cost savings it entails. However, when the data to be outsourced contain private information, appropriate protection mechanisms should be implemented by the data controller. Data splitting, which consists of fragmenting the data and storing them in separate clouds for the sake of privacy preservation, is an interesting alternative to encryption in terms of flexibility and efficiency. However, multivariate analyses on data split among various clouds are challenging, and they are even harder when data are nominal categorical (i.e., textual, non-ordinal), because the standard arithmetic operators cannot be used. In this article, we tackle the problem of outsourcing multivariate analyses on nominal data split over several honest-but-curious clouds. Specifically, we propose several secure protocols to outsource to multiple clouds the computation of a variety of multivariate analyses on nominal categorical data (frequency-based and semantic-based). Our protocols have been designed to outsource as much workload as possible to the clouds, in order to retain the cost-saving benefits of cloud computing while ensuring that the outsourced stay split and hence privacy-protected versus the clouds. The experiments we report on the Amazon cloud service show that by using our protocols the controller can save nearly all the runtime because it can integrate partial results received from the clouds with very little computation.},
  archive      = {J_KIS},
  author       = {Domingo-Ferrer, Josep and Sánchez, David and Ricci, Sara and Muñoz-Batista, Mónica},
  doi          = {10.1007/s10115-019-01424-4},
  journal      = {Knowledge and Information Systems},
  month        = {6},
  number       = {6},
  pages        = {2301-2326},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Outsourcing analyses on privacy-protected multivariate categorical data stored in untrusted clouds},
  volume       = {62},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Incremental community discovery via latent network
representation and probabilistic inference. <em>KIS</em>,
<em>62</em>(6), 2281–2300. (<a
href="https://doi.org/10.1007/s10115-019-01422-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most of the community detection algorithms assume that the complete network structure $$\mathcal {G}=(\mathcal {V},\mathcal {E})$$ is available in advance for analysis. However, in reality this may not be true due to several reasons, such as privacy constraints and restricted access, which result in a partial snapshot of the entire network. In addition, we may be interested in identifying the community information of only a selected subset of nodes (denoted by $$\mathcal {V}_{{\mathrm{T}}} \subseteq \mathcal {V}$$), rather than obtaining the community structure of all the nodes in $$\mathcal {G}$$. To this end, we propose an incremental community detection method that repeats two stages—(i) network scan and (ii) community update. In the first stage, our method selects an appropriate node in such a way that the discovery of its local neighborhood structure leads to an accurate community detection in the second stage. We propose a novel criterion, called Information Gain, based on existing network embedding algorithms (Deepwalk and node2vec) to scan a node. The proposed community update stage consists of expectation–maximization and Markov Random Field-based denoising strategy. Experiments with 5 diverse networks with known ground-truth community structure show that our algorithm achieves 10.2% higher accuracy on average over state-of-the-art algorithms for both network scan and community update steps.},
  archive      = {J_KIS},
  author       = {Cui, Zhe and Park, Noseong and Chakraborty, Tanmoy},
  doi          = {10.1007/s10115-019-01422-6},
  journal      = {Knowledge and Information Systems},
  month        = {6},
  number       = {6},
  pages        = {2281-2300},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Incremental community discovery via latent network representation and probabilistic inference},
  volume       = {62},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Scalable recovery of missing blocks in time series with high
and low cross-correlations. <em>KIS</em>, <em>62</em>(6), 2257–2280. (<a
href="https://doi.org/10.1007/s10115-019-01421-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Missing values are very common in real-world data including time-series data. Failures in power, communication or storage can leave occasional blocks of data missing in multiple series, affecting not only real-time monitoring but also compromising the quality of data analysis. Traditional recovery (imputation) techniques often leverage the correlation across time series to recover missing blocks in multiple series. These recovery techniques, however, assume high correlation and fall short in recovering missing blocks when the series exhibit variations in correlation. In this paper, we introduce a novel approach called CDRec to recover large missing blocks in time series with high and low correlations. CDRec relies on the centroid decomposition (CD) technique to recover multiple time series at a time. We also propose and analyze a new algorithm called Incremental Scalable Sign Vector to efficiently compute CD in long time series. We empirically evaluate the accuracy and the efficiency of our recovery technique on several real-world datasets that represent a broad range of applications. The results show that our recovery is orders of magnitude faster than the most accurate algorithm while producing superior results in terms of recovery.},
  archive      = {J_KIS},
  author       = {Khayati, Mourad and Cudré-Mauroux, Philippe and Böhlen, Michael H.},
  doi          = {10.1007/s10115-019-01421-7},
  journal      = {Knowledge and Information Systems},
  month        = {6},
  number       = {6},
  pages        = {2257-2280},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Scalable recovery of missing blocks in time series with high and low cross-correlations},
  volume       = {62},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A decision support system for service recovery in affective
computing: An experimental investigation. <em>KIS</em>, <em>62</em>(6),
2225–2256. (<a
href="https://doi.org/10.1007/s10115-019-01419-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the development of information technology, information technology not only improves human’s living environments and the quality of life but also increases the productivity of industries. Information technology helps businesses to provide customers with quality services using existing resources and make the right and effective service decisions. Accordingly, businesses have to not only understand customer needs but also pay attention to customer emotions when they make service decisions. This study aims to build a service recovery decision support system by adopting affective computing, artificial neural networks and decision trees approaches. Three experiments are conducted to evaluate the feasibility and performance of the service recovery decision support system. The experiment results show that the service recovery decision support system can have the high performance of customer recognition. Meanwhile, customer emotion can be a clue to enable businesses to make the right service decisions in service recovery.},
  archive      = {J_KIS},
  author       = {Hsieh, Yen-Hao and Chen, Szu-Chieh},
  doi          = {10.1007/s10115-019-01419-1},
  journal      = {Knowledge and Information Systems},
  month        = {6},
  number       = {6},
  pages        = {2225-2256},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {A decision support system for service recovery in affective computing: An experimental investigation},
  volume       = {62},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Fast generation of sequential patterns with item constraints
from concise representations. <em>KIS</em>, <em>62</em>(6), 2191–2223.
(<a href="https://doi.org/10.1007/s10115-019-01418-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Constraint-based frequent sequence mining is an important and necessary task in data mining since it shows results very close to the requirements and interests of users. Most existing algorithms for performing this task are based on a traditional approach that mines patterns directly from a sequence database (SDB). However, in fact, SDBs are often very large. The algorithms thus often exhibit poor performance because the number of generated candidates and the search space are enormous, especially for low minimum support thresholds. In addition, these algorithms must read an SDB again when a constraint is changed by the user. In the context of frequently varied constraints, repeatedly scanning SDBs consume much time. To address this issue, we propose a novel approach for generating frequent sequences with various constraints from the two sets of frequent closed sequences ($$ {{\mathcal{F}}{\mathcal{C}}{\mathcal{S}}} $$) and frequent generator sequences ($$ {{\mathcal{F}}{\mathcal{G}}{\mathcal{S}}} $$), which are the concise representations of the set $$ {{\mathcal{F}}{\mathcal{S}}} $$ of all frequent sequences. The proposed approach is based on novel theoretical results that show an explicit relationship between $$ {{\mathcal{F}}{\mathcal{S}}} $$ and these two sets and have been strictly proved. The approach is then used to develop an efficient algorithm named MFS-IC for quickly generating frequent sequences with item constraints, a task that has many real-life applications. Extensive experiments on real-life and synthetic databases show that the proposed MFS-IC algorithm outperforms state-of-the-art algorithms, which directly mine frequent sequences with constraints from an SDB, in terms of runtime, memory usage and scalability.},
  archive      = {J_KIS},
  author       = {Duong, Hai and Truong, Tin and Tran, Anh and Le, Bac},
  doi          = {10.1007/s10115-019-01418-2},
  journal      = {Knowledge and Information Systems},
  month        = {6},
  number       = {6},
  pages        = {2191-2223},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Fast generation of sequential patterns with item constraints from concise representations},
  volume       = {62},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). DBkWik: Extracting and integrating knowledge from thousands
of wikis. <em>KIS</em>, <em>62</em>(6), 2169–2190. (<a
href="https://doi.org/10.1007/s10115-019-01415-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Popular cross-domain knowledge graphs, such as DBpedia and YAGO, are built from Wikipedia, and therefore similar in coverage. In contrast, Wikifarms like Fandom contain Wikis for specific topics, which are often complementary to the information contained in Wikipedia, and thus DBpedia and YAGO. Extracting these Wikis with the DBpedia extraction framework is possible, but results in many isolated knowledge graphs. In this paper, we show how to create one consolidated knowledge graph, called DBkWik, from thousands of Wikis. We perform entity resolution and schema matching, and show that the resulting large-scale knowledge graph is complementary to DBpedia. Furthermore, we discuss the potential use of DBkWik as a benchmark for knowledge graph matching.},
  archive      = {J_KIS},
  author       = {Hertling, Sven and Paulheim, Heiko},
  doi          = {10.1007/s10115-019-01415-5},
  journal      = {Knowledge and Information Systems},
  month        = {6},
  number       = {6},
  pages        = {2169-2190},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {DBkWik: Extracting and integrating knowledge from thousands of wikis},
  volume       = {62},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Online sales prediction via trend alignment-based multitask
recurrent neural networks. <em>KIS</em>, <em>62</em>(6), 2139–2167. (<a
href="https://doi.org/10.1007/s10115-019-01404-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {While business trends are constantly evolving, the timely prediction of sales volume offers precious information for companies to achieve a healthy balance between supply and demand. In practice, sales prediction is formulated as a time series prediction problem which aims to predict the future sales volume for different products with the observation of various influential factors (e.g. brand, season, discount, etc.) and corresponding historical sales records. To perform accurate sales prediction under the offline setting, we gain insights from the encoder–decoder recurrent neural network (RNN) structure and have proposed a novel framework named TADA (Chen et al., in: ICDM, 2018) to carry out trend alignment with dual-attention, multitask RNNs for sales prediction. However, the sales data accumulates at a fast rate and is updated on a regular basis, rendering it difficult for the trained model to maintain the prediction accuracy with new data. In this light, we further extend the model into TADA$$^+$$, which is enhanced by an online learning module based on our innovative similarity-based reservoir. To construct the data reservoir for model retraining, different from most existing random sampling-based reservoir, our similarity-based reservoir selects data samples that are “hard” for the model to mine apparent dynamic patterns. The experimental results on two real-world datasets comprehensively show the superiority of TADA and TADA$$^+$$ in both online and offline sales prediction tasks against other state-of-the-art competitors.},
  archive      = {J_KIS},
  author       = {Chen, Tong and Yin, Hongzhi and Chen, Hongxu and Wang, Hao and Zhou, Xiaofang and Li, Xue},
  doi          = {10.1007/s10115-019-01404-8},
  journal      = {Knowledge and Information Systems},
  month        = {6},
  number       = {6},
  pages        = {2139-2167},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Online sales prediction via trend alignment-based multitask recurrent neural networks},
  volume       = {62},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Review of english literature on figurative language applied
to social networks. <em>KIS</em>, <em>62</em>(6), 2105–2137. (<a
href="https://doi.org/10.1007/s10115-019-01425-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For a long time, figurative language was studied merely from linguistic perspectives, yet it has lately captured the attention of other fields, such as natural language processing, sentiment analysis, and machine learning. The increasing interest in figurative language calls for a clear overview of figurative language research. To address this need, we present a review of English literature on figurative language applied to social networks in a five-year period: from 2013 to 2017. The aim of this review is to identify the most commonly researched figurative devices, as well as their discriminant features, detection approaches and methods, and languages in which they are studied. To this end, we analyze and evaluate 521 research works and present 45 primary studies. The results show that sarcasm is the most studied figurative device, with 56% of the total frequencies. Also, 87% of the studies are based on the supervised machine learning approach, and the support vector machine classifier has been the most used to detect the different types of figurative language (i.e., figurative devices). Similarly, more than half of the literature focuses on figurative language in English.},
  archive      = {J_KIS},
  author       = {del Pilar Salas-Zárate, María and Alor-Hernández, Giner and Sánchez-Cervantes, José Luis and Paredes-Valverde, Mario Andrés and García-Alcaraz, Jorge Luis and Valencia-García, Rafael},
  doi          = {10.1007/s10115-019-01425-3},
  journal      = {Knowledge and Information Systems},
  month        = {6},
  number       = {6},
  pages        = {2105-2137},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Review of english literature on figurative language applied to social networks},
  volume       = {62},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020a). Correction to: Memory-based random walk for multi-query
local community detection. <em>KIS</em>, <em>62</em>(5), 2103–2104. (<a
href="https://doi.org/10.1007/s10115-019-01414-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the published article, Figure 9(a) and Figure 9(b) are the same figure.},
  archive      = {J_KIS},
  author       = {Bian, Yuchen and Luo, Dongsheng and Yan, Yaowei and Cheng, Wei and Wang, Wei and Zhang, Xiang},
  doi          = {10.1007/s10115-019-01414-6},
  journal      = {Knowledge and Information Systems},
  month        = {5},
  number       = {5},
  pages        = {2103-2104},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Correction to: Memory-based random walk for multi-query local community detection},
  volume       = {62},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020b). Memory-based random walk for multi-query local community
detection. <em>KIS</em>, <em>62</em>(5), 2067–2101. (<a
href="https://doi.org/10.1007/s10115-019-01398-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Local community detection, which aims to find a target community containing a set of query nodes, has recently drawn intense research interest. The existing local community detection methods usually assume all query nodes are from the same community and only find a single target community. This is a strict requirement and does not allow much flexibility. In many real-world applications, however, we may not have any prior knowledge about the community memberships of the query nodes, and different query nodes may be from different communities. To address this limitation of the existing methods, we propose a novel memory-based random walk method, MRW, that can simultaneously identify multiple target local communities to which the query nodes belong. In MRW, each query node is associated with a random walker. Different from commonly used memoryless random walk models, MRW records the entire visiting history of each walker. The visiting histories of walkers can help unravel whether they are from the same community or not. Intuitively, walkers with similar visiting histories are more likely to be in the same community. Moreover, MRW allows walkers with similar visiting histories to reinforce each other so that they can better capture the community structure instead of being biased to the query nodes. We provide rigorous theoretical foundations for the proposed method and develop efficient algorithms to identify multiple target local communities simultaneously. Comprehensive experimental evaluations on a variety of real-world datasets demonstrate the effectiveness and efficiency of the proposed method.},
  archive      = {J_KIS},
  author       = {Bian, Yuchen and Luo, Dongsheng and Yan, Yaowei and Cheng, Wei and Wang, Wei and Zhang, Xiang},
  doi          = {10.1007/s10115-019-01398-3},
  journal      = {Knowledge and Information Systems},
  month        = {5},
  number       = {5},
  pages        = {2067-2101},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Memory-based random walk for multi-query local community detection},
  volume       = {62},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Sequential pattern sampling with norm-based utility.
<em>KIS</em>, <em>62</em>(5), 2029–2065. (<a
href="https://doi.org/10.1007/s10115-019-01417-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sequential pattern mining has been introduced by Agrawal and Srikant (in: Proceedings of ICDE’95, pp 3–14, 1995) 2 decades ago, and its usefulness has been widely proved for different mining tasks and application fields such as web usage mining, text mining, bioinformatics, fraud detection and so on. Since 1995, despite numerous optimization proposals, sequential pattern mining remains a costly task that often generates too many patterns. This limit, also reached by itemset mining, was circumvented by pattern sampling. Pattern sampling is a non-exhaustive method for instantly discovering relevant patterns that ensures a good interactivity while providing strong statistical guarantees due to its random nature. Curiously, such an approach investigated for different kinds of patterns including itemsets and subgraphs has not yet been applied to sequential patterns. In this paper, we propose the first method dedicated to sequential pattern sampling. In addition to address sequential data, the originality of our approach is to introduce a class of interestingness measures relying on the norm of the sequence, named norm-based utilities. In particular, it enables to add constraints on the norm of sampled patterns to control the length of the drawn patterns and to avoid the pitfall of the “long tail” where the rarest patterns flood the user. We propose a new two-step random procedure integrating this class of measures, named $${\textsc {NUSSampling}}$$ that randomly draws sequential patterns according to frequency weighted by a norm-based utility. We demonstrate that this method performs an exact sampling according to the underlying measure. Moreover, despite the use of rejection sampling, the experimental study shows that $${\textsc {NUSSampling}}$$ remains efficient. We especially focus on the interest of norm constraints and exponential decays that help to draw general patterns of the “head”. We also illustrate how to benefit from these sampled patterns to instantly build an associative classifier dedicated to sequences. This classification approach rivals state-of-the-art proposals showing the interest of sequential pattern sampling with norm-based utility.},
  archive      = {J_KIS},
  author       = {Diop, Lamine and Diop, Cheikh Talibouya and Giacometti, Arnaud and Li, Dominique and Soulet, Arnaud},
  doi          = {10.1007/s10115-019-01417-3},
  journal      = {Knowledge and Information Systems},
  month        = {5},
  number       = {5},
  pages        = {2029-2065},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Sequential pattern sampling with norm-based utility},
  volume       = {62},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). From decision knowledge to e-government expert systems: The
case of income taxation for foreign artists in belgium. <em>KIS</em>,
<em>62</em>(5), 2011–2028. (<a
href="https://doi.org/10.1007/s10115-019-01416-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Since the introduction of the Decision model and notation (DMN), the standard has successfully been adopted in both industry and academia. However, no clear modelling guidelines can be found regarding the development of DMN decision models. For approaching this gap, this paper discusses modelling methodologies for the DMN standard, both at the decision requirements level as well as at the decision logic level. For that purpose, we capitalise on existing guidelines in related fields, such as decision table modelling, information systems engineering, and software engineering. We adapt, expand, and restructure the guidelines to conform to DMN model development. Additionally, we provide a real-life case that was modelled using the suggested modelling strategies and consequently, we deploy the model as a government e-service for tax management. The real-life case is concerned with clarifying tax regulations for visiting performing artists in Belgium, and it was carried out in cooperation with oKo, a Belgian industry federation for the arts, and the Ministry of Culture. The decision model was built in the Avola tool and implemented as an e-service in order to demonstrate the suitability of the DMN standard for the deployment of expert system e-government services.},
  archive      = {J_KIS},
  author       = {Hasić, Faruk and Vanthienen, Jan},
  doi          = {10.1007/s10115-019-01416-4},
  journal      = {Knowledge and Information Systems},
  month        = {5},
  number       = {5},
  pages        = {2011-2028},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {From decision knowledge to e-government expert systems: The case of income taxation for foreign artists in belgium},
  volume       = {62},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Machine learning friendly set version of
johnson–lindenstrauss lemma. <em>KIS</em>, <em>62</em>(5), 1961–2009.
(<a href="https://doi.org/10.1007/s10115-019-01412-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The widely discussed and applied Johnson–Lindenstrauss (JL) Lemma has an existential form saying that for each set of data points Q in n-dimensional space, there exists a transformation f into an $$n&#39;$$-dimensional space ($$n&#39;&lt;n$$) such that for each pair $$\mathbf {u},\mathbf {v} \in Q$$$$(1-\delta )\Vert \mathbf {u}-\mathbf {v}\Vert ^2 \le \Vert f(\mathbf {u})-f(\mathbf {v})\Vert ^2 \le (1+\delta )\Vert \mathbf {u}-\mathbf {v}\Vert ^2 $$ for a user-defined error parameter $$\delta $$. Furthermore, it is asserted that with some finite probability the transformation f may be found as a random projection (with scaling) onto the $$n&#39;$$ dimensional subspace so that after sufficiently many repetitions of random projection, f will be found with user-defined success rate $$1-\epsilon $$. In this paper, we make a novel use of the JL Lemma. We prove a theorem stating that we can choose the target dimensionality in a random projection-type JL linear transformation in such a way that with probability $$1-\epsilon $$ all of data points from Q fall into predefined error range $$\delta $$ for any user-predefined failure probability $$\epsilon $$ when performing a single random projection. This result is important for applications such as data clustering where we want to have a priori dimensionality reducing transformation instead of attempting a (large) number of them, as with traditional Johnson–Lindenstrauss Lemma. Furthermore, we investigate an important issue whether or not the projection according to JL Lemma is really useful when conducting data processing, that is whether the solutions to the clustering in the projected space apply to the original space. In particular, we take a closer look at the k-means algorithm and prove that a good solution in the projected space is also a good solution in the original space. Furthermore, under proper assumptions local optima in the original space are also ones in the projected space. We investigate also a broader issue of preserving clusterability under JL Lemma projection. We define the conditions for which clusterability property of the original space is transmitted to the projected space, so that a broad class of clustering algorithms for the original space is applicable in the projected space.},
  archive      = {J_KIS},
  author       = {Kłopotek, Mieczysław A.},
  doi          = {10.1007/s10115-019-01412-8},
  journal      = {Knowledge and Information Systems},
  month        = {5},
  number       = {5},
  pages        = {1961-2009},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Machine learning friendly set version of Johnson–Lindenstrauss lemma},
  volume       = {62},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). <span class="math display">SecDM</span>: Privacy-preserving
data outsourcing framework with differential privacy. <em>KIS</em>,
<em>62</em>(5), 1923–1960. (<a
href="https://doi.org/10.1007/s10115-019-01405-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data-as-a-service (DaaS) is a cloud computing service that emerged as a viable option to businesses and individuals for outsourcing and sharing their collected data with other parties. Although the cloud computing paradigm provides great flexibility to consumers with respect to computation and storage capabilities, it imposes serious concerns about the confidentiality of the outsourced data as well as the privacy of the individuals referenced in the data. In this paper we formulate and address the problem of querying encrypted data in a cloud environment such that query processing is confidential and the result is differentially private. We propose a framework where the data provider uploads an encrypted index of her anonymized data to a DaaS service provider that is responsible for answering range count queries from authorized data miners for the purpose of data mining. To satisfy the confidentiality requirement, we leverage attribute-based encryption to construct a secure kd-tree index over the differentially private data for fast access. We also utilize the exponential variant of the ElGamal cryptosystem to efficiently perform homomorphic operations on encrypted data. Experiments on real-life data demonstrate that our proposed framework preserves data utility, can efficiently answer range queries, and is scalable with increasing data size.},
  archive      = {J_KIS},
  author       = {Dagher, Gaby G. and Fung, Benjamin C. M. and Mohammed, Noman and Clark, Jeremy},
  doi          = {10.1007/s10115-019-01405-7},
  journal      = {Knowledge and Information Systems},
  month        = {5},
  number       = {5},
  pages        = {1923-1960},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {$${\textsf {SecDM}}$$: Privacy-preserving data outsourcing framework with differential privacy},
  volume       = {62},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Improved algorithms for extrinsic author verification.
<em>KIS</em>, <em>62</em>(5), 1903–1921. (<a
href="https://doi.org/10.1007/s10115-019-01408-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Author verification is a fundamental problem in authorship attribution, and it suits most relevant applications where it is not possible to predefine a closed set of suspects. So far, the most successful approaches attempt to sample the non-target class (all documents by all other authors) and transform author verification to a binary classification task. Moreover, they follow the instance-based paradigm (all documents of known authorship are treated separately). In this paper, we propose two algorithms, one instance-based and one profile-based (all known documents are treated cumulatively) that are able to outperform state-of-the-art methods in several benchmark datasets. We demonstrate that the proposed methods are capable of taking advantage of the availability of multiple documents of known authorship and that they are robust when text length is reduced.},
  archive      = {J_KIS},
  author       = {Potha, Nektaria and Stamatatos, Efstathios},
  doi          = {10.1007/s10115-019-01408-4},
  journal      = {Knowledge and Information Systems},
  month        = {5},
  number       = {5},
  pages        = {1903-1921},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Improved algorithms for extrinsic author verification},
  volume       = {62},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). An effective few-shot learning approach via
location-dependent partial differential equation. <em>KIS</em>,
<em>62</em>(5), 1881–1901. (<a
href="https://doi.org/10.1007/s10115-019-01400-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, learning-based partial differential equation (L-PDE) has achieved success in few-shot learning area, while its feature weighting mechanism and recognition stability require further improvement. To address these issues, we propose a novel model called “location-dependent PDE” (LD-PDE) based on Navier–Stokes equation and rotational invariants in this paper. To our best knowledge, LD-PDE is the first application of the Navier–Stokes equation to achieve image recognition as a high-level vision task. Specifically, we formulate the feature variation with respect to each time step as a linear combination of rotational invariants in LD-PDE. Meanwhile, we design location-dependent mechanism to adaptively weight each invariant in an attention-based approach, which provides hierarchical discrimination in the spatial domain. Once the ultimate feature is learned, we measure the model error with the cross-entropy loss and update the parameters by the coordinate descent algorithm. As a verification, experimental results on face recognition datasets show that LD-PDE method outperforms the state-of-the-art approaches with few training samples. Moreover, compared to L-PDE, LD-PDE achieves a much more stable recognition with low sensitivity to its hyper-parameters.},
  archive      = {J_KIS},
  author       = {Wang, Haotian and Zhao, Zhenyu and Tang, Yuhua},
  doi          = {10.1007/s10115-019-01400-y},
  journal      = {Knowledge and Information Systems},
  month        = {5},
  number       = {5},
  pages        = {1881-1901},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {An effective few-shot learning approach via location-dependent partial differential equation},
  volume       = {62},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Recurrent random forest for the assessment of popularity in
social media. <em>KIS</em>, <em>62</em>(5), 1847–1879. (<a
href="https://doi.org/10.1007/s10115-019-01410-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Popularity in social media is mostly interpreted by drawing a relationship between a social media account and its followers. Although understanding popularity from social media has been explored for about a decade, to our knowledge, the extent to which the account owners put efforts to enhance their popularity has not been evaluated in detail. In this paper, we focus on Twitter, a popular social media, and consider the case study of the 2016 US elections. More specifically, we aim to assess whether candidates endeavor to improve their style of tweeting over time to be more attractive to their followers. An ad hoc-defined predictive model based on a recurrent random forest is used for this purpose. To this end, we build a classification model whose features are obtained from the characteristics of a set of content/sentiment information extracted from the tweets. Next, we derive an index of social media popularity for both candidates. Results show that Trump wisely exploited Twitter to attract more people by tweeting in a well-organized and desirable manner and that his tweeting style has increased his popularity in social media. The differences in the tweeting styles of the two presidential candidates and the links between the sentiments arising from candidates’ tweets and their popularity index are also investigated.},
  archive      = {J_KIS},
  author       = {Tavazoee, Farideh and Conversano, Claudio and Mola, Francesco},
  doi          = {10.1007/s10115-019-01410-w},
  journal      = {Knowledge and Information Systems},
  month        = {5},
  number       = {5},
  pages        = {1847-1879},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Recurrent random forest for the assessment of popularity in social media},
  volume       = {62},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Structured query construction via knowledge graph embedding.
<em>KIS</em>, <em>62</em>(5), 1819–1846. (<a
href="https://doi.org/10.1007/s10115-019-01401-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In order to facilitate the accesses of general users to knowledge graphs, an increasing effort is being exerted to construct graph-structured queries of given natural language questions. At the core of the construction is to deduce the structure of the target query and determine the vertices/edges which constitute the query. Existing query construction methods rely on question understanding and conventional graph-based algorithms which lead to inefficient and degraded performances facing complex natural language questions over knowledge graphs with large scales. In this paper, we focus on this problem and propose a novel framework standing on recent knowledge graph embedding techniques. Our framework first encodes the underlying knowledge graph into a low-dimensional embedding space by leveraging generalized local knowledge graphs. Given a natural language question, the learned embedding representations of the knowledge graph are utilized to compute the query structure and assemble vertices/edges into the target query. Extensive experiments were conducted on the benchmark dataset, and the results demonstrate that our framework outperforms state-of-the-art baseline models regarding effectiveness and efficiency.},
  archive      = {J_KIS},
  author       = {Wang, Ruijie and Wang, Meng and Liu, Jun and Cochez, Michael and Decker, Stefan},
  doi          = {10.1007/s10115-019-01401-x},
  journal      = {Knowledge and Information Systems},
  month        = {5},
  number       = {5},
  pages        = {1819-1846},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Structured query construction via knowledge graph embedding},
  volume       = {62},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Detecting outliers with one-class selective transfer
machine. <em>KIS</em>, <em>62</em>(5), 1781–1818. (<a
href="https://doi.org/10.1007/s10115-019-01407-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose an outlier detection method from an unlabeled target dataset by exploiting an unlabeled source dataset. Detecting outliers has attracted attention of data miners for over two decades, since such outliers can be crucial in decision making, knowledge discovery, and fraud detection, to name but a few. The fact that outliers are scarce and often tedious to label motivated researchers to propose detection methods from an unlabeled dataset, some of which borrow strengths from relevant labeled datasets in the framework of transfer learning. He et al. tackled a more challenging situation in which the input datasets coming from multiple tasks are all unlabeled. Their method, ML-OCSVM, conducts multi-task learning with one-class support vector machines (SVMs) and yields a mean model plus task-specific increments to detect outliers in the test datasets of the multiple tasks. We inherit a part of their problem setting, taking only unlabeled datasets in the input, but increase the difficulty by assuming only one source dataset in addition to the target dataset. Consequently, the source dataset consists of examples relevant to the target task as well as examples that are less relevant. To cope with this situation, we extend Selective Transfer Machine, which weights individual examples in the framework of covariate shift and learns an SVM classifier, to our one-class setting by replacing the binary SVMs with one-class SVMs. Experiments on two public datasets and an artificial dataset show that our method mostly outperforms baseline methods, including ML-OCSVM and a state-of-the-art ensemble anomaly detection method, in F1 score and AUC.},
  archive      = {J_KIS},
  author       = {Fujita, Hirofumi and Matsukawa, Tetsu and Suzuki, Einoshin},
  doi          = {10.1007/s10115-019-01407-5},
  journal      = {Knowledge and Information Systems},
  month        = {5},
  number       = {5},
  pages        = {1781-1818},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Detecting outliers with one-class selective transfer machine},
  volume       = {62},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Queries of k-discriminative paths on road networks.
<em>KIS</em>, <em>62</em>(5), 1751–1780. (<a
href="https://doi.org/10.1007/s10115-019-01397-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we study the problem of searching k-discriminative paths on road networks. Given a source node src and a destination node dest on a road network, we aim to search k paths between src and dest, where these k paths satisfy the multi-objective goal including the minimization of the path overlapping and the minimization of the path length. Specifically, the requirement of minimizing the overlapping among paths, which is a NP-hard issue, is highly demanded in applications of disaster rescue management such as the evacuation plan. In this paper, we consider the deployment of k-discriminative paths for various applications, including queries for emergency-purpose applications, queries of multi-objective Pareto front for pre-schedule transportation plan, and queries with multiple sources and destinations for the regional evacuation. Due to its NP-hard nature, the heuristic strategy based on the ant colony optimization is devised in this work. As validated by our experimental studies on real road networks, the proposed algorithm can achieve the discovery of k-discriminative paths efficiently and effectively, showing its prominent advantages to be a practicable service for evacuation-related applications.},
  archive      = {J_KIS},
  author       = {Chang, Chien-Wei and Chen, Chu-Di and Chuang, Kun-Ta},
  doi          = {10.1007/s10115-019-01397-4},
  journal      = {Knowledge and Information Systems},
  month        = {5},
  number       = {5},
  pages        = {1751-1780},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Queries of K-discriminative paths on road networks},
  volume       = {62},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). CDLFM: Cross-domain recommendation for cold-start users via
latent feature mapping. <em>KIS</em>, <em>62</em>(5), 1723–1750. (<a
href="https://doi.org/10.1007/s10115-019-01396-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Collaborative filtering (CF) is a widely adopted technique in recommender systems. Traditional CF models mainly focus on predicting the user preference to items in a single domain, such as the movie domain or the music domain. A major challenge for such models is the data sparsity, and especially, CF cannot make accurate predictions for the cold-start users who have no ratings at all. Although cross-domain collaborative filtering (CDCF) is proposed for effectively transferring knowledge across different domains, it is still difficult for existing CDCF models to tackle the cold-start users in the target domain due to the extreme data sparsity. In this paper, we propose the cross-domain latent feature mapping (CDLFM) model for the cold-start users in the target domain. Firstly, in order to alleviate the data sparsity in single domain and provide essential knowledge for next step, we take users’ rating behaviors into consideration and propose the matrix factorization by incorporating user similarities. Next, to transfer knowledge across domains, we propose the neighborhood-based cross-domain latent feature mapping method. For each cold-start user, we learn his/her feature mapping function based on his/her neighbor linked users. By adopting gradient boosting trees and multilayer perceptron to model the cross-domain feature mapping function, two CDLFM models named CDLFM-GBT and CDLFM-MLP are proposed. Experimental results on two real datasets demonstrate the superiority of our proposed model against other state-of-the-art methods.},
  archive      = {J_KIS},
  author       = {Wang, Xinghua and Peng, Zhaohui and Wang, Senzhang and Yu, Philip S. and Fu, Wenjing and Xu, Xiaokang and Hong, Xiaoguang},
  doi          = {10.1007/s10115-019-01396-5},
  journal      = {Knowledge and Information Systems},
  month        = {5},
  number       = {5},
  pages        = {1723-1750},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {CDLFM: Cross-domain recommendation for cold-start users via latent feature mapping},
  volume       = {62},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). An FPA and GA-based hybrid evolutionary algorithm for
analyzing clusters. <em>KIS</em>, <em>62</em>(5), 1701–1722. (<a
href="https://doi.org/10.1007/s10115-019-01413-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Clustering is a technique employed for data mining and analysis. k-means is one of the algorithms utilized for clustering. However, the answer derived using this algorithm is dependent on the initial solution and hence easily retrieves the optimal local answers. To overcome the disadvantages of this algorithm, in this paper a combination of pollination of flowers algorithm and genetic algorithm, named FPAGA, is presented. Combination algorithms are used to diversify the search space of the solution and to improve its capability. To elaborate, crossover and discarding of pollens operator are utilized to increase the population diversity, while elitism operator is employed to improve the local search capabilities. Five datasets are selected to evaluate the performance of the proposed algorithm. The evaluation results demonstrate not only greater accuracy but also better stability compared to the FPA, GA, FA, DE, and k-means algorithms. Moreover, faster convergence is evident, according to the obtained statistical results.},
  archive      = {J_KIS},
  author       = {Fatahi, Mohammad and Moradi, Sadegh},
  doi          = {10.1007/s10115-019-01413-7},
  journal      = {Knowledge and Information Systems},
  month        = {5},
  number       = {5},
  pages        = {1701-1722},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {An FPA and GA-based hybrid evolutionary algorithm for analyzing clusters},
  volume       = {62},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Locally and globally explainable time series tweaking.
<em>KIS</em>, <em>62</em>(5), 1671–1700. (<a
href="https://doi.org/10.1007/s10115-019-01389-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Time series classification has received great attention over the past decade with a wide range of methods focusing on predictive performance by exploiting various types of temporal features. Nonetheless, little emphasis has been placed on interpretability and explainability. In this paper, we formulate the novel problem of explainable time series tweaking, where, given a time series and an opaque classifier that provides a particular classification decision for the time series, we want to find the changes to be performed to the given time series so that the classifier changes its decision to another class. We show that the problem is $${\mathbf {NP}}$$-hard, and focus on three instantiations of the problem using global and local transformations. In the former case, we investigate the k-nearest neighbor classifier and provide an algorithmic solution to the global time series tweaking problem. In the latter case, we investigate the random shapelet forest classifier and focus on two instantiations of the local time series tweaking problem, which we refer to as reversible and irreversible time series tweaking, and propose two algorithmic solutions for the two problems along with simple optimizations. An extensive experimental evaluation on a variety of real datasets demonstrates the usefulness and effectiveness of our problem formulation and solutions.},
  archive      = {J_KIS},
  author       = {Karlsson, Isak and Rebane, Jonathan and Papapetrou, Panagiotis and Gionis, Aristides},
  doi          = {10.1007/s10115-019-01389-4},
  journal      = {Knowledge and Information Systems},
  month        = {5},
  number       = {5},
  pages        = {1671-1700},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Locally and globally explainable time series tweaking},
  volume       = {62},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A benchmarking tool for the generation of bipartite network
models with overlapping communities. <em>KIS</em>, <em>62</em>(4),
1641–1669. (<a
href="https://doi.org/10.1007/s10115-019-01411-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many real-world networks display hidden community structures with important potential implications in their dynamics. Many algorithms highly relevant to network analysis have been introduced to unveil community structures. Accurate assessment and comparison of alternative solutions are typically approached by benchmarking the target algorithm(s) on a set of diverse networks that exhibit a broad range of controlled features, ensuring the assessment contemplates multiple representative properties. Tools have been developed to synthesize bipartite networks, but none of the previous solutions address the issue of generating networks with overlapping community structures. This is the motivation for the BNOC tool introduced in this paper. It allows synthesizing bipartite networks that mimic a wide range of features from real-world networks, including overlapping community structures. Multiple parameters ensure flexibility in controlling the scale and topological properties of the networks and embedded communities. BNOC’s applicability is illustrated assessing and comparing two popular overlapping community detection algorithms on bipartite networks, namely HLC and OSLOM. Results reveal interesting features of the algorithms in this scenario and confirm the relevant role played by a suitable benchmarking tool. Finally, to validate our approach, we present results comparing networks synthesized with BNOC with those obtained with an existing benchmarking tool and with already established sets of synthetic networks, in two different scenarios.},
  archive      = {J_KIS},
  author       = {Valejo, Alan and Góes, Fabiana and Romanetto, Luzia and Ferreira de Oliveira, Maria Cristina and de Andrade Lopes, Alneu},
  doi          = {10.1007/s10115-019-01411-9},
  journal      = {Knowledge and Information Systems},
  month        = {4},
  number       = {4},
  pages        = {1641-1669},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {A benchmarking tool for the generation of bipartite network models with overlapping communities},
  volume       = {62},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Finding events in temporal networks: Segmentation meets
densest subgraph discovery. <em>KIS</em>, <em>62</em>(4), 1611–1639. (<a
href="https://doi.org/10.1007/s10115-019-01403-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we study the problem of discovering a timeline of events in a temporal network. We model events as dense subgraphs that occur within intervals of network activity. We formulate the event discovery task as an optimization problem, where we search for a partition of the network timeline into k non-overlapping intervals, such that the intervals span subgraphs with maximum total density. The output is a sequence of dense subgraphs along with corresponding time intervals, capturing the most interesting events during the network lifetime. A naïve solution to our optimization problem has polynomial but prohibitively high running time. We adapt existing recent work on dynamic densest subgraph discovery and approximate dynamic programming to design a fast approximation algorithm. Next, to ensure richer structure, we adjust the problem formulation to encourage coverage of a larger set of nodes. This problem is NP-hard; however, we show that on static graphs a simple greedy algorithm leads to approximate solution due to submodularity. We extend this greedy approach for temporal networks, but we lose the approximation guarantee in the process. Finally, we demonstrate empirically that our algorithms recover solutions with good quality.},
  archive      = {J_KIS},
  author       = {Rozenshtein, Polina and Bonchi, Francesco and Gionis, Aristides and Sozio, Mauro and Tatti, Nikolaj},
  doi          = {10.1007/s10115-019-01403-9},
  journal      = {Knowledge and Information Systems},
  month        = {4},
  number       = {4},
  pages        = {1611-1639},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Finding events in temporal networks: Segmentation meets densest subgraph discovery},
  volume       = {62},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Consistent updating of databases with marked nulls.
<em>KIS</em>, <em>62</em>(4), 1571–1609. (<a
href="https://doi.org/10.1007/s10115-019-01402-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper revisits the problem of consistency maintenance when insertions or deletions are performed on a valid database containing marked nulls. This problem comes back to light in real-world linked data or RDF databases when blank nodes are associated with null values. This paper proposes solutions for the main problems one has to face when dealing with updates and constraints, namely update determinism, minimal change and leanness of an RDF graph instance. The update semantics is formally introduced and the notion of core is used to ensure a database as small as possible (i.e.   the RDF graph leanness). Our algorithms allow the use of constraints such as tuple-generating dependencies, offering a way for solving many practical problems.},
  archive      = {J_KIS},
  author       = {Chabin, Jacques and Halfeld-Ferrari, Mirian and Laurent, Dominique},
  doi          = {10.1007/s10115-019-01402-w},
  journal      = {Knowledge and Information Systems},
  month        = {4},
  number       = {4},
  pages        = {1571-1609},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Consistent updating of databases with marked nulls},
  volume       = {62},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Measuring similarity and relatedness using multiple semantic
relations in WordNet. <em>KIS</em>, <em>62</em>(4), 1539–1569. (<a
href="https://doi.org/10.1007/s10115-019-01387-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Semantic similarity and relatedness computation has attracted an increasing amount of attention among researchers. The majority of previous studies, including edge-based and information content-based methods, rely on a single semantic relationship in WordNet such as the “is-a” relation. However, a performance ceiling may have been created by semantic unicity and inadequate calculation in solely “is-a” relation-based measurements, i.e., the computed results for some word pairs are too small and significantly deviate from human judgments. For this problem, we propose the following solutions: (1) We introduce the notion of the nearest common descendant to provide a supplement for commonalities between concepts according to genetics theory. (2) We design various targeted methods for different incomplete semantic relations. Therefore, various semantic relations can participate in similarity and relatedness computations in their most appropriate manners. (3) We utilize the cross-use of incomplete semantic relations similar-to and antonymy to solve the challenge of adjective and adverb similarity/relatedness measurements in WordNet. (4) We propose a targeted independent computation and largest contribution aggregation method to break through the performance ceiling of similarity/relatedness measurements based on single “is-a” relations. We conduct evaluations of our proposed model using seven extensively employed datasets. These evaluations indicate that our method significantly improves the performance of the existing methods based on single “is-a” relations. Their best Pearson coefficient with human judgments on both the MC30 and RG65 is increased to 0.9. With the development and enrichment of semantic relations in WordNet, our proposed model can be expected to have a more prominent role.},
  archive      = {J_KIS},
  author       = {Zhu, Xinhua and Yang, Xuechen and Huang, Yanyi and Guo, Qingsong and Zhang, Bo},
  doi          = {10.1007/s10115-019-01387-6},
  journal      = {Knowledge and Information Systems},
  month        = {4},
  number       = {4},
  pages        = {1539-1569},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Measuring similarity and relatedness using multiple semantic relations in WordNet},
  volume       = {62},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). On the calculation of the strength of threats. <em>KIS</em>,
<em>62</em>(4), 1511–1538. (<a
href="https://doi.org/10.1007/s10115-019-01399-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Threats are used in persuasive negotiation dialogues when a proponent agent tries to persuade an opponent of him to accept a proposal. Depending on the information the proponent has modeled about his opponent(s), he may generate more than one threat, in which case he has to evaluate them in order to select the most adequate to be sent. One way to evaluate the generated threats is by calculating their strengths, i.e., the persuasive force of each threat. Related work considers mainly two criteria to do such evaluation: the certainty level of the beliefs that compose the threat and the importance of the goal of the opponent. This article aims to study the components of threats and propose further criteria that lead to improve their evaluation and to select more effective threats during the dialogue. Thus, the contribution of this paper is a model for calculating the strength of threats that is mainly based on the status of the goal of the opponent and the credibility of the proponent. The model is empirically evaluated and the results demonstrate that the proposed model is more efficient than previous works in terms of the number of exchanged arguments, and the number of reached agreements.},
  archive      = {J_KIS},
  author       = {Morveli Espinoza, Mariela and Possebom, Ayslan Trevizan and Tacla, Cesar Augusto},
  doi          = {10.1007/s10115-019-01399-2},
  journal      = {Knowledge and Information Systems},
  month        = {4},
  number       = {4},
  pages        = {1511-1538},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {On the calculation of the strength of threats},
  volume       = {62},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Effective construction of classifiers with the k-NN method
supported by a concept ontology. <em>KIS</em>, <em>62</em>(4),
1497–1510. (<a
href="https://doi.org/10.1007/s10115-019-01391-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In analysing sensor data, it usually proves beneficial to use domain knowledge in the classification process in order to narrow down the search space of relevant features. However, it is often not effective when decision trees or the k-NN method is used. Therefore, the authors herein propose to build an appropriate concept ontology based on expert knowledge. The use of an ontology-based metric enables mutual similarity to be determined between objects covered by respective concept ontology, taking into consideration interrelations of features at various levels of abstraction. Using a set of medical data collected with the Holter method, it is shown that predicting coronary disease with the use of the approach proposed is much more accurate than in the case of not only the k-NN method using classical metrics, but also most other known classifiers. It is also proved in this paper that the expert determination of appropriate structure of ontology is of key importance, while subsequent selection of appropriate weights can be automated.},
  archive      = {J_KIS},
  author       = {Bazan, Jan and Bazan-Socha, Stanisława and Ochab, Marcin and Buregwa-Czuma, Sylwia and Nowakowski, Tomasz and Woźniak, Mirosław},
  doi          = {10.1007/s10115-019-01391-w},
  journal      = {Knowledge and Information Systems},
  month        = {4},
  number       = {4},
  pages        = {1497-1510},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Effective construction of classifiers with the k-NN method supported by a concept ontology},
  volume       = {62},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Micro- and macro-level churn analysis of large-scale mobile
games. <em>KIS</em>, <em>62</em>(4), 1465–1496. (<a
href="https://doi.org/10.1007/s10115-019-01394-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As mobile devices become more and more popular, mobile gaming has emerged as a promising market with billion-dollar revenue. A variety of mobile game platforms and services have been developed around the world. A critical challenge for these platforms and services is to understand the churn behavior in mobile games, which usually involves churn at micro-level (between an app and a specific user) and macro-level (between an app and all its users). Accurate micro-level churn prediction and macro-level churn ranking will benefit many stakeholders such as game developers, advertisers, and platform operators. In this paper, we present the first large-scale churn analysis for mobile games that supports both micro-level churn prediction and macro-level churn ranking. For micro-level churn prediction, in view of the common limitations of the state-of-the-art methods built upon traditional machine learning models, we devise a novel semi-supervised and inductive embedding model that jointly learns the prediction function and the embedding function for user–app relationships. We model these two functions by deep neural networks with a unique edge embedding technique that is able to capture both contextual information and relationship dynamics. We also design a novel attributed random walk technique that takes into consideration both topological adjacency and attribute similarities. To address macro-level churn ranking, we propose to construct a relationship graph with estimated micro-level churn probabilities as edge weights and adapt link analysis algorithms on the graph. We devise a simple algorithm SimSum and adapt two more advanced algorithms PageRank and HITS. The performance of our solutions to the two-level churn analysis problem is evaluated on real-world data collected from the Samsung Game Launcher platform. The data includes tens of thousands of mobile games and hundreds of millions of user–app interactions. The experimental results with this data demonstrate the superiority of our proposed models against existing state-of-the-art methods.},
  archive      = {J_KIS},
  author       = {Liu, Xi and Xie, Muhe and Wen, Xidao and Chen, Rui and Ge, Yong and Duffield, Nick and Wang, Na},
  doi          = {10.1007/s10115-019-01394-7},
  journal      = {Knowledge and Information Systems},
  month        = {4},
  number       = {4},
  pages        = {1465-1496},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Micro- and macro-level churn analysis of large-scale mobile games},
  volume       = {62},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Parallel co-location mining with MapReduce and NoSQL
systems. <em>KIS</em>, <em>62</em>(4), 1433–1463. (<a
href="https://doi.org/10.1007/s10115-019-01381-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rapid growth of georeferenced data, large-scale data processing and analysis methods are needed for spatial big data. Spatial co-location pattern mining is an interesting and important issue in spatial data mining area which discovers the subsets of features whose objects are frequently located together in geographic proximity. There are several works for efficiently processing co-location pattern discovery; however, they may be insufficient for large dense spatial data because the mining task takes up a lot of processing time and memory. In this work, we leveraged the power of a modern distributed computing platform, Hadoop, and developed an algorithm (called ParColoc) for parallel co-location mining on the MapReduce framework. This study explored challenge issues in designing the parallel co-location mining algorithm and solved them with adopting a spatial declusteirng technique and a NoSQL system. We conducted an experimental evaluation with real-world data and synthetic data to examine the effectiveness of proposed methods. The experiment result shows that ParColoc is a promising method for parallel co-location mining in cloud computing environment.},
  archive      = {J_KIS},
  author       = {Yoo, Jin Soung and Boulware, Douglas and Kimmey, David},
  doi          = {10.1007/s10115-019-01381-y},
  journal      = {Knowledge and Information Systems},
  month        = {4},
  number       = {4},
  pages        = {1433-1463},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Parallel co-location mining with MapReduce and NoSQL systems},
  volume       = {62},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Sentiment analysis on big sparse data streams with limited
labels. <em>KIS</em>, <em>62</em>(4), 1393–1432. (<a
href="https://doi.org/10.1007/s10115-019-01392-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sentiment analysis is an important task in order to gain insights over the huge amounts of opinionated texts generated on a daily basis in social media like Twitter. Despite its huge amount, standard supervised learning methods won’t work upon such sort of data due to lack of labels and the impracticality of (human) labeling at this scale. In this work, we leverage distant supervision and semi-supervised learning to annotate a big stream of tweets from 2015 which consists of 228 million tweets without retweets (and 275 million with retweets). We present the insights from our annotation process regarding the effect of different semi-supervised learning approaches, namely Self-Learning, Co-Training and Expectation–Maximization. Moreover, we propose two annotation modes, the batch mode where all labeled and unlabeled data are available to the algorithms from the beginning and a lightweight streaming mode that processes the data in batches based on their arrival time in the stream. Our experiments show that stream processing with a sliding window of three months achieves comparable results to batch processing while being more efficient. Finally, to tackle the class imbalance problem, as our dataset is imbalanced toward the positive sentiment class, and its aggravation by the semi-supervised learning methods, we employ data augmentation in the semi-supervised learning process in order to equalize the class distribution. Our results show that semi-supervised learning coupled with data augmentation outperforms significantly the default semi-supervised annotation process. We make the so-called TSentiment15 sentiment-annotated dataset available to the community to be used for evaluation purposes and for developing new methods.},
  archive      = {J_KIS},
  author       = {Iosifidis, Vasileios and Ntoutsi, Eirini},
  doi          = {10.1007/s10115-019-01392-9},
  journal      = {Knowledge and Information Systems},
  month        = {4},
  number       = {4},
  pages        = {1393-1432},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Sentiment analysis on big sparse data streams with limited labels},
  volume       = {62},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A new last aggregation method of multi-attributes group
decision making based on concepts of TODIM, WASPAS and TOPSIS under
interval-valued intuitionistic fuzzy uncertainty. <em>KIS</em>,
<em>62</em>(4), 1371–1391. (<a
href="https://doi.org/10.1007/s10115-019-01390-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to the complexity of decision making under uncertainty and the existence of various and often conflicting criteria, several methods have been proposed to facilitate decision making, and fuzzy logic has been used successfully to address this issue. This paper presents a new framework for solving multi-attributes group decision-making problems under fuzzy environments. The proposed algorithm has several features. First of all, the TODIM (an acronym in Portuguese for interactive multi-criteria decision making) method under interval-valued intuitionistic fuzzy uncertainty is employed. Moreover, objective and subjective weights for each decision maker are used to address this last aggregation approach. To consider weights of attributes, knowledge measure in addition to a new mathematical approach is introduced. A new aggregation and ranking method based on the WASPAS and TOPSIS methods, namely WT method, is presented and applied in this paper. Finally, the effectiveness of the proposed framework is shown by comparing the results with two different real-world applications in the literature.},
  archive      = {J_KIS},
  author       = {Davoudabadi, R. and Mousavi, S. Meysam and Mohagheghi, V.},
  doi          = {10.1007/s10115-019-01390-x},
  journal      = {Knowledge and Information Systems},
  month        = {4},
  number       = {4},
  pages        = {1371-1391},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {A new last aggregation method of multi-attributes group decision making based on concepts of TODIM, WASPAS and TOPSIS under interval-valued intuitionistic fuzzy uncertainty},
  volume       = {62},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Multi-label crowd consensus via joint matrix factorization.
<em>KIS</em>, <em>62</em>(4), 1341–1369. (<a
href="https://doi.org/10.1007/s10115-019-01386-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Crowdsourcing is a useful and economic approach to annotate data. Various computational solutions have been developed to pursue a consensus of high quality. However, available solutions mainly target single-label tasks, and they neglect correlations among labels. In this paper, we introduce a multi-label crowd consensus (MLCC) model based on a joint matrix factorization. Specifically, MLCC selectively and jointly factorizes the sample-label association matrices into products of individual and shared low-rank matrices. As such, it makes use of the robustness of low-rank matrix approximation to noisy annotations and diminishes the impact of unreliable annotators by assigning small weights to their annotation matrices. To obtain coherent low-rank matrices, MLCC additionally leverages the shared low-rank matrix to model correlations among labels, and the individual low-rank matrices to measure the similarity between annotators. MLCC then computes the low-rank matrices and weights via a unified objective function, and adopts an alternative optimization technique to iteratively optimize them. Finally, MLCC uses the optimized low-rank matrices and weights to compute the consensus labels. Our experimental results demonstrate that MLCC outperforms competitive methods in inferring consensus labels. Besides identifying spammers, MLCC achieves robustness against their incorrect annotations, by crediting them small, or zero, weights.},
  archive      = {J_KIS},
  author       = {Tu, Jinzheng and Yu, Guoxian and Domeniconi, Carlotta and Wang, Jun and Xiao, Guoqiang and Guo, Maozu},
  doi          = {10.1007/s10115-019-01386-7},
  journal      = {Knowledge and Information Systems},
  month        = {4},
  number       = {4},
  pages        = {1341-1369},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Multi-label crowd consensus via joint matrix factorization},
  volume       = {62},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). ProSecCo: Progressive sequence mining with convergence
guarantees. <em>KIS</em>, <em>62</em>(4), 1313–1340. (<a
href="https://doi.org/10.1007/s10115-019-01393-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present ProSecCo, an algorithm for the progressive mining of frequent sequences from large transactional datasets: It processes the dataset in blocks and it outputs, after having analyzed each block, a high-quality approximation of the collection of frequent sequences. ProSecCo can be used for interactive data exploration, as the intermediate results enable the user to make informed decisions as the computation proceeds. These intermediate results have strong probabilistic approximation guarantees and the final output is the exact collection of frequent sequences. Our correctness analysis uses the Vapnik–Chervonenkis (VC) dimension, a key concept from statistical learning theory. The results of our experimental evaluation of ProSecCo on real and artificial datasets show that it produces fast-converging high-quality results almost immediately. Its practical performance is even better than what is guaranteed by the theoretical analysis, and ProSecCo can even be faster than existing state-of-the-art non-progressive algorithms. Additionally, our experimental results show that ProSecCo uses a constant amount of memory, and orders of magnitude less than other standard, non-progressive, sequential pattern mining algorithms.},
  archive      = {J_KIS},
  author       = {Servan-Schreiber, Sacha and Riondato, Matteo and Zgraggen, Emanuel},
  doi          = {10.1007/s10115-019-01393-8},
  journal      = {Knowledge and Information Systems},
  month        = {4},
  number       = {4},
  pages        = {1313-1340},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {ProSecCo: Progressive sequence mining with convergence guarantees},
  volume       = {62},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Trajectory splicing. <em>KIS</em>, <em>62</em>(4),
1279–1312. (<a
href="https://doi.org/10.1007/s10115-019-01382-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With continued development of location-based systems, large amounts of trajectories become available which record moving objects’ locations across time. If the trajectories collected by different location-based systems come from the same moving object, they are spliceable trajectories, which contribute to representing holistic behaviors of the moving object. In this paper, we consider how to efficiently identify spliceable trajectories. More specifically, we first formalize a spliced model to capture spliceable trajectories where their times are disjoint, and the distances between them are close. Next, to efficiently implement the model, we design three components: a disjoint time index, a directed acyclic graph of sub-trajectory location connections, and two splice algorithms. The disjoint time index saves a disjoint time set of each trajectory for querying disjoint time trajectories efficiently. The directed acyclic graph contributes to discovering groups of spliceable trajectories. Based on the identified groups, the splice algorithm findmaxCTR finds maximal groups containing all spliceable trajectories. Although the splice algorithm is efficient in some practical applications, its running time is exponential. Therefore, an approximate algorithm findApproxMaxCTR is proposed to find trajectories which can be spliced with each other with a certain probability within polynomial run time. Finally, experiments on two datasets demonstrate that the model and its components are effective and efficient.},
  archive      = {J_KIS},
  author       = {Lu, Qiang and Wang, Rencai and Yang, Bin and Wang, Zhiguang},
  doi          = {10.1007/s10115-019-01382-x},
  journal      = {Knowledge and Information Systems},
  month        = {4},
  number       = {4},
  pages        = {1279-1312},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Trajectory splicing},
  volume       = {62},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Reasoning with smart objects’ affordance for personalized
behavior monitoring in pervasive information systems. <em>KIS</em>,
<em>62</em>(4), 1255–1278. (<a
href="https://doi.org/10.1007/s10115-019-01357-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The miniaturization of sensors and their integration in everyday appliances have opened the way for ecologically monitoring people’s behavior based on their interaction with smart objects. Thanks to behavior monitoring, mobile, and ubiquitous information systems in the areas of e-health, home automation, and smart cities are becoming more and more “smart,” being able to dynamically adapt themselves to the current users’ context and situation. However, human behavior is characterized by large variability due to individual habits, physical disabilities or cognitive impairment. This aspect makes behavior monitoring a challenging task. On the one side, execution variability makes it hard to acquire sufficiently large activity datasets needed by supervised learning methods. On the other side, being based on a strict definition of activities in terms of constituting simpler actions, existing knowledge-based frameworks fall short in adapting to the specific characteristics of the subject. Hence, the variability of activity execution by different subjects calls for personalized methods to capture human activities and interaction in smart spaces at a fine-grained level. In this paper, we address this challenge by proposing a novel hybrid reasoning framework to capture fine-grained interaction with smart objects considering the specific features of individuals. Our model has its roots in the well-founded psychological theory of affordances, i.e., those features of an object that naturally explain its possible uses and how it should be used. The core of the framework is the ontological model of smart objects affordance, expressed through the OWL 2 Web Ontology Language. Through a use case in pervasive healthcare, we show how our framework can be applied to personalized recognition of abnormal behaviors. In particular, we tackle a particularly challenging issue: how to recognize early behavioral symptoms of mild cognitive impairment in subjects with physical disabilities. Moreover, an extensive experimental evaluation with real-world datasets acquired from 24 subjects shows the effectiveness of our framework in recognizing human activities and fine-grained manipulative gestures in different pervasive computing environments.},
  archive      = {J_KIS},
  author       = {Matassa, Assunta and Riboni, Daniele},
  doi          = {10.1007/s10115-019-01357-y},
  journal      = {Knowledge and Information Systems},
  month        = {4},
  number       = {4},
  pages        = {1255-1278},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Reasoning with smart objects’ affordance for personalized behavior monitoring in pervasive information systems},
  volume       = {62},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Two approaches for clustering algorithms with
relational-based data. <em>KIS</em>, <em>62</em>(3), 1229–1253. (<a
href="https://doi.org/10.1007/s10115-019-01384-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {It is well known that relational databases still play an important role for many companies around the world. For this reason, the use of data mining methods to discover knowledge in large relational databases has become an interesting research issue. In the context of unsupervised data mining, for instance, the conventional clustering algorithms cannot handle the particularities of the relational databases in an efficient way. There are some clustering algorithms for relational datasets proposed in the literature. However, most of these methods apply complex and/or specific procedures to handle the relational nature of data, or the relational-based methods do not capture the relational nature in an efficient way. Aiming to contribute to this important topic, in this paper, we will present two simple and generic approaches to handle relational-based data for clustering algorithms. One of them treats the relational data through the use of a hierarchical structure, while the second approach applies a weight structure based on relationship and attribute information. In presenting these two approaches, we aim to tackle relational-based dataset in a simple and efficient way, improving the efficiency of corporations that handle relational-based in the unsupervised data mining context. In order to evaluate the effectiveness of the presented approaches, a comparative analysis will be conducted, comparing the proposed approaches with some existing approaches and with a baseline approach. In all analyzed approaches, we will use two well-known types of clustering algorithms (agglomerative hierarchical and K-means). In order to perform this analysis, we will use two internal and one external clusters as validity measures.},
  archive      = {J_KIS},
  author       = {Xavier-Junior, João C. and Canuto, Anne M. P. and Gonçalves, Luiz M. G.},
  doi          = {10.1007/s10115-019-01384-9},
  journal      = {Knowledge and Information Systems},
  month        = {3},
  number       = {3},
  pages        = {1229-1253},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Two approaches for clustering algorithms with relational-based data},
  volume       = {62},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). High average-utility sequential pattern mining based on
uncertain databases. <em>KIS</em>, <em>62</em>(3), 1199–1228. (<a
href="https://doi.org/10.1007/s10115-019-01385-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The emergence and proliferation of the internet of things (IoT) devices have resulted in the generation of big and uncertain data due to the varied accuracy and decay of sensors and their different sensitivity ranges. Since data uncertainty plays an important role in IoT data, mining the useful information from uncertain dataset has become an important issue in recent decades. Past works focus on mining the high sequential patterns from the uncertain database. However, the utility of a derived sequence increases along with the size of the sequence, which is an unfair measure to evaluate the utility of a sequence since any combination of a high-utility sequence will also be the high-utility sequence, even though the utility of a sequence is merely low. In this paper, we address the limitation of the previous potential high-utility sequential pattern mining and present a potentially high average-utility sequential pattern mining framework for discovering the set of potentially high average-utility sequential patterns (PHAUSPs) from the uncertain dataset by considering the size of a sequence, which can provide a fair measure of the patterns than the previous works. First, a baseline potentially high average-utility sequential pattern algorithm and three pruning strategies are introduced to completely mine the set of the desired PHAUSPs. To reduce the computational cost and accelerate the mining process, a projection algorithm called PHAUP is then designed, which leads to a reduction in the size of candidates of the desired patterns. Several experiments in terms of runtime, number of candidates, memory overhead, number of discovered pattern, and scalability are then evaluated on both real-life and artificial datasets, and the results showed that the proposed algorithm achieves promising performance, especially the PHAUP approach.},
  archive      = {J_KIS},
  author       = {Lin, Jerry Chun-Wei and Li, Ting and Pirouz, Matin and Zhang, Ji and Fournier-Viger, Philippe},
  doi          = {10.1007/s10115-019-01385-8},
  journal      = {Knowledge and Information Systems},
  month        = {3},
  number       = {3},
  pages        = {1199-1228},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {High average-utility sequential pattern mining based on uncertain databases},
  volume       = {62},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Information-preserving abstractions of event data in process
mining. <em>KIS</em>, <em>62</em>(3), 1143–1197. (<a
href="https://doi.org/10.1007/s10115-019-01376-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Process mining aims at obtaining information about processes by analysing their past executions in event logs, event streams, or databases. Discovering a process model from a finite amount of event data thereby has to correctly infer infinitely many unseen behaviours. Thereby, many process discovery techniques leverage abstractions on the finite event data to infer and preserve behavioural information of the underlying process. However, the fundamental information-preserving properties of these abstractions are not well understood yet. In this paper, we study the information-preserving properties of the “directly follows” abstraction and its limitations. We overcome these by proposing and studying two new abstractions which preserve even more information in the form of finite graphs. We then show how and characterize when process behaviour can be unambiguously recovered through characteristic footprints in these abstractions. Our characterization defines large classes of practically relevant processes covering various complex process patterns. We prove that the information and the footprints preserved in the abstractions suffice to unambiguously rediscover the exact process model from a finite event log. Furthermore, we show that all three abstractions are relevant in practice to infer process models from event logs and outline the implications on process mining techniques.},
  archive      = {J_KIS},
  author       = {Leemans, Sander J. J. and Fahland, Dirk},
  doi          = {10.1007/s10115-019-01376-9},
  journal      = {Knowledge and Information Systems},
  month        = {3},
  number       = {3},
  pages        = {1143-1197},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Information-preserving abstractions of event data in process mining},
  volume       = {62},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Accelerating pattern-based time series classification: A
linear time and space string mining approach. <em>KIS</em>,
<em>62</em>(3), 1113–1141. (<a
href="https://doi.org/10.1007/s10115-019-01378-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Subsequences-based time series classification algorithms provide interpretable and generally more accurate classification models compared to the nearest neighbor approach, albeit at a considerably higher computational cost. A number of discretized time series-based algorithms have been proposed to reduce the computational complexity of these algorithms; however, the asymptotic time complexity of the proposed algorithms is also cubic or higher-order polynomial. We present a remarkably fast and resource-efficient time series classification approach which employs a linear time and space string mining algorithm for extracting frequent patterns from discretized time series data. Compared to other subsequence or pattern-based classification algorithms, the proposed approach only requires a few parameters, which can be chosen arbitrarily and do not require any fine-tuning for different datasets. The time series data are discretized using symbolic aggregate approximation, and frequent patterns are extracted using a string mining algorithm. An independence test is used to select the most discriminative frequent patterns, which are subsequently used to create a transformed version of the time series data. Finally, a classification model can be trained using any off-the-shelf algorithm. Extensive empirical evaluations demonstrate the competitive classification accuracy of our approach compared to other state-of-the-art approaches. The experiments also show that our approach is at least one to two orders of magnitude faster than the existing pattern-based methods due to the extremely fast frequent pattern extraction, which is the most computationally intensive process in pattern-based time series classification approaches.},
  archive      = {J_KIS},
  author       = {Raza, Atif and Kramer, Stefan},
  doi          = {10.1007/s10115-019-01378-7},
  journal      = {Knowledge and Information Systems},
  month        = {3},
  number       = {3},
  pages        = {1113-1141},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Accelerating pattern-based time series classification: A linear time and space string mining approach},
  volume       = {62},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Local low-rank hawkes processes for modeling temporal
user–item interactions. <em>KIS</em>, <em>62</em>(3), 1089–1112. (<a
href="https://doi.org/10.1007/s10115-019-01379-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hawkes processes have become very popular in modeling multiple recurrent user–item interaction events that exhibit mutual-excitation properties in various domains. Generally, modeling the interaction sequence of each user–item pair as an independent Hawkes process is ineffective since the prediction accuracy of future event occurrences for users and items with few observed interactions is low. On the other hand, multivariate Hawkes processes (MHPs) can be used to handle multi-dimensional random processes where different dimensions are correlated with each other. However, an MHP either fails to describe the correct mutual influence between dimensions or become computational inhibitive in most real-world events involving a large collection of users and items. To tackle this challenge, we propose local low-rank Hawkes processes to model large-scale user–item interactions, which efficiently captures the correlations of Hawkes processes in different dimensions. In addition, we design an efficient convex optimization algorithm to estimate model parameters and present a parallel algorithm to further increase the computation efficiency. Extensive experiments on real-world datasets demonstrate the performance improvements of our model in comparison with the state of the art.},
  archive      = {J_KIS},
  author       = {Shang, Jin and Sun, Mingxuan},
  doi          = {10.1007/s10115-019-01379-6},
  journal      = {Knowledge and Information Systems},
  month        = {3},
  number       = {3},
  pages        = {1089-1112},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Local low-rank hawkes processes for modeling temporal user–item interactions},
  volume       = {62},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Dynamically updating approximations based on multi-threshold
tolerance relation in incomplete interval-valued decision information
systems. <em>KIS</em>, <em>62</em>(3), 1063–1087. (<a
href="https://doi.org/10.1007/s10115-019-01377-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the development of society, data noise and other factors will cause the incompleteness of information systems. Objects may increase or decrease over time in information systems. The classical information system can be extended to the incomplete interval-valued decision information system (IIDIS) that is the researching object of this paper. Incremental learning technique is a significant method for solving approximate sets under dynamic data. This article defines a multi-threshold tolerance relation based on the set pair analysis theory and establishes a rough set model in IIDIS. Then, several methods and algorithms for statically/dynamically solving approximate sets are shown. Finally, comparative experiments from six UCI data sets show both dynamic algorithms take less time than the static algorithm to calculate the approximate sets no matter how object set changes.},
  archive      = {J_KIS},
  author       = {Lin, Bingyan and Zhang, Xiaoyan and Xu, Weihua and Wu, Yanxue},
  doi          = {10.1007/s10115-019-01377-8},
  journal      = {Knowledge and Information Systems},
  month        = {3},
  number       = {3},
  pages        = {1063-1087},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Dynamically updating approximations based on multi-threshold tolerance relation in incomplete interval-valued decision information systems},
  volume       = {62},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Evidential positive opinion influence measures for viral
marketing. <em>KIS</em>, <em>62</em>(3), 1037–1062. (<a
href="https://doi.org/10.1007/s10115-019-01375-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The viral marketing is a relatively new form of marketing that exploits social networks to promote a brand, a product, etc. The idea behind it is to find a set of influencers on the network that can trigger a large cascade of propagation and adoptions. In this paper, we will introduce an evidential opinion-based influence maximization model for viral marketing. Besides, our approach tackles three opinion-based scenarios for viral marketing in the real world. The first scenario concerns influencers who have a positive opinion about the product. The second scenario deals with influencers who have a positive opinion about the product and produces effects on users who also have a positive opinion. The third scenario involves influence users who have a positive opinion about the product and produce effects on the negative opinion of other users concerning the product in question. Next, we proposed six influence measures, two for each scenario. We also use an influence maximization model that the set of detected influencers for each scenario. Finally, we show the performance of the proposed model with each influence measure through some experiments conducted on a generated dataset and a real-world dataset collected from Twitter.},
  archive      = {J_KIS},
  author       = {Jendoubi, Siwar and Martin, Arnaud},
  doi          = {10.1007/s10115-019-01375-w},
  journal      = {Knowledge and Information Systems},
  month        = {3},
  number       = {3},
  pages        = {1037-1062},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Evidential positive opinion influence measures for viral marketing},
  volume       = {62},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A systematic framework of predicting customer revisit with
in-store sensors. <em>KIS</em>, <em>62</em>(3), 1005–1035. (<a
href="https://doi.org/10.1007/s10115-019-01373-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, there is a growing number of off-line stores that are willing to conduct customer behavior analysis. In particular, predicting revisit intention is of prime importance, because converting first-time visitors to loyal customers is very profitable. Thanks to noninvasive monitoring, shopping behaviors and revisit statistics become available from a large proportion of customers who turn on their mobile devices. In this paper, we propose a systematic framework to predict the revisit intention of customers using Wi-Fi signals captured by in-store sensors. Using data collected from seven flagship stores in downtown Seoul, we achieved 67–80% prediction accuracy for all customers and 64–72% prediction accuracy for first-time visitors. The performance improvement by considering customer mobility was 4.7–24.3%. Furthermore, we provide an in-depth analysis regarding the effect of data collection period as well as visit frequency on the prediction performance and present the robustness of our model on missing customers. We released some tutorials and benchmark datasets for revisit prediction at https://github.com/kaist-dmlab/revisit.},
  archive      = {J_KIS},
  author       = {Kim, Sundong and Lee, Jae-Gil},
  doi          = {10.1007/s10115-019-01373-y},
  journal      = {Knowledge and Information Systems},
  month        = {3},
  number       = {3},
  pages        = {1005-1035},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {A systematic framework of predicting customer revisit with in-store sensors},
  volume       = {62},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Identifying at-risk students based on the phased prediction
model. <em>KIS</em>, <em>62</em>(3), 987–1003. (<a
href="https://doi.org/10.1007/s10115-019-01374-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Identifying at-risk students is one of the most important issues in online education. During different stages of a semester, students display various online learning behaviors. Therefore, we propose a phased prediction model to predict at-risk students at different stages of a semester. We analyze students’ individual characteristics and online learning behaviors, extract features that are closely related to their learning performance, and propose combined feature sets based on a time window constraint strategy and a learning time threshold constraint strategy. The results of our experiments show that the precision of the proposed model in different phases is from 90.4 to 93.6%.},
  archive      = {J_KIS},
  author       = {Chen, Yan and Zheng, Qinghua and Ji, Shuguang and Tian, Feng and Zhu, Haiping and Liu, Min},
  doi          = {10.1007/s10115-019-01374-x},
  journal      = {Knowledge and Information Systems},
  month        = {3},
  number       = {3},
  pages        = {987-1003},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Identifying at-risk students based on the phased prediction model},
  volume       = {62},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Integrating learned and explicit document features for
reputation monitoring in social media. <em>KIS</em>, <em>62</em>(3),
951–985. (<a href="https://doi.org/10.1007/s10115-019-01383-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Currently, monitoring reputation in social media is probably one of the most lucrative applications of information retrieval methods. However, this task poses new challenges due to the dynamicity of contents and the need for early detection of topics that affect the reputations of companies. Addressing this problem with learning mechanisms that are based on training data sets is challenging, given that unseen features play a crucial role. However, learning processes are necessary to capture domain features and dependency phenomena. In this work, based on observational information theory, we define a document representation framework that enables the combination of explicit text features and supervised and unsupervised signals into a single representation model. Our theoretical analysis demonstrates that the observation information quantity (OIQ) generalizes the most popular representation methods, in addition to capturing quantitative values, which is required for integrating signals from learning processes. In other words, the OIQ allows us to give the same treatment to features that are currently managed separately. Empirically, our experiments on the reputation-monitoring scenario demonstrated that adding features progressively from supervised (in particular, Bayesian inference over annotated data) and unsupervised learning methods (in particular, proximity to clusters) increases the similarity estimation performance. This result is verified under various similarity criteria (pointwise mutual information, Jaccard and Lin’s distances and the information contrast model). According to our formal analysis, the OIQ is the first representation model that captures the informativeness (specificity) of quantitative features in the document representation.},
  archive      = {J_KIS},
  author       = {Giner, Fernando and Amigó, Enrique and Verdejo, Felisa},
  doi          = {10.1007/s10115-019-01383-w},
  journal      = {Knowledge and Information Systems},
  month        = {3},
  number       = {3},
  pages        = {951-985},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Integrating learned and explicit document features for reputation monitoring in social media},
  volume       = {62},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Exploiting patterns to explain individual predictions.
<em>KIS</em>, <em>62</em>(3), 927–950. (<a
href="https://doi.org/10.1007/s10115-019-01368-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Users need to understand the predictions of a classifier, especially when decisions based on the predictions can have severe consequences. The explanation of a prediction reveals the reason why a classifier makes a certain prediction, and it helps users to accept or reject the prediction with greater confidence. This paper proposes an explanation method called Pattern Aided Local Explanation (PALEX) to provide instance-level explanations for any classifier. PALEX takes a classifier, a test instance and a frequent pattern set summarizing the training data of the classifier as inputs, and then outputs the supporting evidence that the classifier considers important for the prediction of the instance. To study the local behavior of a classifier in the vicinity of the test instance, PALEX uses the frequent pattern set from the training data as an extra input to guide generation of new synthetic samples in the vicinity of the test instance. Contrast patterns are also used in PALEX to identify locally discriminative features in the vicinity of a test instance. PALEX is particularly effective for scenarios where there exist multiple explanations. In our experiments, we compare PALEX to several state-of-the-art explanation methods over a range of benchmark datasets and find that it can identify explanations with both high precision and high recall.},
  archive      = {J_KIS},
  author       = {Jia, Yunzhe and Bailey, James and Ramamohanarao, Kotagiri and Leckie, Christopher and Ma, Xingjun},
  doi          = {10.1007/s10115-019-01368-9},
  journal      = {Knowledge and Information Systems},
  month        = {3},
  number       = {3},
  pages        = {927-950},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Exploiting patterns to explain individual predictions},
  volume       = {62},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Kernel conditional clustering and kernel conditional
semi-supervised learning. <em>KIS</em>, <em>62</em>(3), 899–925. (<a
href="https://doi.org/10.1007/s10115-019-01334-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The results of clustering are often affected by covariates that are independent of the clusters one would like to discover. Traditionally, alternative clustering algorithms can be used to solve such clustering problems. However, these suffer from at least one of the following problems: (1) Continuous covariates or nonlinearly separable clusters cannot be handled; (2) assumptions are made about the distribution of the data; (3) one or more hyper-parameters need to be set. The presence of covariates also has an effect in a different type of problem such as semi-supervised learning. To the best of our knowledge, there is no existing method addressing the semi-supervised learning setting in the presence of covariates. Here we propose two novel algorithms, named kernel conditional clustering (KCC) and kernel conditional semi-supervised learning (KCSSL), whose objectives are derived from a kernel-based conditional dependence measure. KCC is parameter-light and makes no assumptions about the cluster structure, the covariates, or the distribution of the data, while KCSSL is fully parameter-free. On both simulated and real-world datasets, the proposed KCC and KCSSL algorithms perform better than state-of-the-art methods. The former detects the ground truth cluster structures more accurately, and the latter makes more accurate predictions.},
  archive      = {J_KIS},
  author       = {He, Xiao and Gumbsch, Thomas and Roqueiro, Damian and Borgwardt, Karsten},
  doi          = {10.1007/s10115-019-01334-5},
  journal      = {Knowledge and Information Systems},
  month        = {3},
  number       = {3},
  pages        = {899-925},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Kernel conditional clustering and kernel conditional semi-supervised learning},
  volume       = {62},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Nearest base-neighbor search on spatial datasets.
<em>KIS</em>, <em>62</em>(3), 867–897. (<a
href="https://doi.org/10.1007/s10115-019-01360-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a nearest base-neighbor (NBN) search that can be applied to a clustered nearest neighbor problem on spatial datasets with static properties. Given two sets of data points R and S, a query point q, distance threshold δ and cardinality threshold k, the NBN query retrieves a nearest point r (called the base-point) in R where more than k points in S are located within the distance δ. In this paper, we formally define a base-point and NBN problem. As the brute-force approach to this problem in massive datasets has large computational and I/O costs, we propose in-memory and external memory processing techniques for NBN queries. In particular, our proposed in-memory algorithms are used to minimize I/Os in the external memory algorithms. Furthermore, we devise a solution-based index, which we call the neighborhood-augmented grid, to dramatically reduce the search space. A performance study is conducted both on synthetic and real datasets. Our experimental results show the efficiency of our proposed approach.},
  archive      = {J_KIS},
  author       = {Jang, Hong-Jun and Hyun, Kyeong-Seok and Chung, Jaehwa and Jung, Soon-Young},
  doi          = {10.1007/s10115-019-01360-3},
  journal      = {Knowledge and Information Systems},
  month        = {3},
  number       = {3},
  pages        = {867-897},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Nearest base-neighbor search on spatial datasets},
  volume       = {62},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Framework for extreme imbalance classification:
SWIM—sampling with the majority class. <em>KIS</em>, <em>62</em>(3),
841–866. (<a href="https://doi.org/10.1007/s10115-019-01380-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The class imbalance problem is a pervasive issue in many real-world domains. Oversampling methods that inflate the rare class by generating synthetic data are amongst the most popular techniques for resolving class imbalance. However, they concentrate on the characteristics of the minority class and use them to guide the oversampling process. By completely overlooking the majority class, they lose a global view on the classification problem and, while alleviating the class imbalance, may negatively impact learnability by generating borderline or overlapping instances. This becomes even more critical when facing extreme class imbalance, where the minority class is strongly underrepresented and on its own does not contain enough information to conduct the oversampling process. We propose a framework for synthetic oversampling that, unlike existing resampling methods, is robust on cases of extreme imbalance. The key feature of the framework is that it uses the density of the well-sampled majority class to guide the generation process. We demonstrate implementations of the framework using the Mahalanobis distance and a radial basis function. We evaluate over 25 benchmark datasets and show that the framework offers a distinct performance improvement over the existing state-of-the-art in oversampling techniques.},
  archive      = {J_KIS},
  author       = {Bellinger, Colin and Sharma, Shiven and Japkowicz, Nathalie and Zaïane, Osmar R.},
  doi          = {10.1007/s10115-019-01380-z},
  journal      = {Knowledge and Information Systems},
  month        = {3},
  number       = {3},
  pages        = {841-866},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Framework for extreme imbalance classification: SWIM—sampling with the majority class},
  volume       = {62},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Generating synthetic positive and negative business process
traces through abduction. <em>KIS</em>, <em>62</em>(2), 813–839. (<a
href="https://doi.org/10.1007/s10115-019-01372-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As recent years have seen the rise of a new discipline commonly addressed as process mining, focused on the management of business processes, two tasks have gained increasing attention in research: process discovery and compliance monitoring. In both these fields, the demand for event log benchmarks with predefined characteristics has determined the design of various methodologies and tools for synthetic log generation. However, artificially created as well as real-life logs often contain positive examples only (i.e. process instances deemed as compliant w.r.t. the model), while the presence of negative process instances (i.e. non-compliant traces) can be crucial to correctly evaluate the performance and robustness of a novel process discovery or conformance checking technique. In this work, we investigate positive and negative trace generation in case of both declarative and procedural model specifications and we present our abduction-based approach to log synthesis. The theoretical study is concretely applied in a software prototype for log generation, which takes as input a declarative or structured workflow model and emits logs containing positive and negative traces. The approach provides both a highly expressive notation for the description of the business model and the ability to generate logs with various customizable features. The final comparative study of other existing log generators reveals several advantages of the proposed approach and draws the direction of future improvements.},
  archive      = {J_KIS},
  author       = {Loreti, Daniela and Chesani, Federico and Ciampolini, Anna and Mello, Paola},
  doi          = {10.1007/s10115-019-01372-z},
  journal      = {Knowledge and Information Systems},
  month        = {2},
  number       = {2},
  pages        = {813-839},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Generating synthetic positive and negative business process traces through abduction},
  volume       = {62},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Measuring the diversity of recommendations: A
preference-aware approach for evaluating and adjusting diversity.
<em>KIS</em>, <em>62</em>(2), 787–811. (<a
href="https://doi.org/10.1007/s10115-019-01371-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {While recent research has highlighted the importance of identifying diverse aspects of user preferences in terms of the quality of recommendations, most of the widely used performance measures tend to consider the accuracy of the recommendations and ignore other important aspects such as preference for diversity in recommendations. This is despite the emerging consensus that improving the diversity in recommendations allows users to discover a wider variety of items and encourage them to extend their range of interests in domains such as books, movies, and music. By proposing a novel diversity evaluation metric, this paper aims to address the problem of measuring the diversity with respect to the distribution of preferences for diversity among recommendation system users. We perform several experiments in order to provide a better understanding of the diversity preferences of users and present the results of diversity evaluations of several recommendation methods. These experiments highlight the accuracy–diversity trade-off and show that higher accuracy does not lead to higher performance in terms of the diversity of the recommendations and that the users’ preferred level of diversity should be considered when designing and evaluating recommender systems. This paper also proposes our Diversity Adjustment algorithm that modifies the diversity of recommendations to suit each user’s preferences while preserving the accuracy. Our experiments suggest that diversifying the recommendations without considering the user’s preferences can lead to a dramatic decline in accuracy, while adjusting the diversity based on users’ diversity needs can support recommender systems in maintaining overall accuracy.},
  archive      = {J_KIS},
  author       = {Meymandpour, Rouzbeh and Davis, Joseph G.},
  doi          = {10.1007/s10115-019-01371-0},
  journal      = {Knowledge and Information Systems},
  month        = {2},
  number       = {2},
  pages        = {787-811},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Measuring the diversity of recommendations: A preference-aware approach for evaluating and adjusting diversity},
  volume       = {62},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A new order relation for pythagorean fuzzy numbers and
application to multi-attribute group decision making. <em>KIS</em>,
<em>62</em>(2), 751–785. (<a
href="https://doi.org/10.1007/s10115-019-01369-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes a new order relation for Pythagorean fuzzy numbers (PFNs) and applies to multi-attribute group decision making (MAGDM). The main contributions are outlined as five aspects: (1) the concepts of relative distance and information reliability of PFN are proposed. Then, a new order relation is developed to compare PFNs. Moreover, the new order relation of PFNs is demonstrated to be an admissible order. (2) Knowledge measure of PFN is defined to describe the amount of information. The desirable properties of knowledge measure of PFN are studied concretely. (3) For MAGDM with PFNs, the comprehensive distance between individual Pythagorean fuzzy matrices and a mean one are defined. Then, the decision makers’ weights are obtained by the comprehensive distances. Thus, a collective Pythagorean fuzzy matrix is derived by using the Pythagorean fuzzy weighted average operator. (4) To determine attribute weights, a multi-objective programming model is constructed by maximizing the overall knowledge measure of each alternative. This model is further transformed into a single-objective mathematical program to resolve. (5) According to the defined new order relation of PFNs, the ranking order of alternatives is generated by the comprehensive values of alternatives. Therefore, a new method is proposed to solve MAGDM with PFNs. Finally, an example of venture capital investment selection is provided to illustrate the effectiveness of the proposed method.},
  archive      = {J_KIS},
  author       = {Wan, Shu-Ping and Jin, Zhen and Dong, Jiu-Ying},
  doi          = {10.1007/s10115-019-01369-8},
  journal      = {Knowledge and Information Systems},
  month        = {2},
  number       = {2},
  pages        = {751-785},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {A new order relation for pythagorean fuzzy numbers and application to multi-attribute group decision making},
  volume       = {62},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Unsupervised online change point detection in
high-dimensional time series. <em>KIS</em>, <em>62</em>(2), 719–750. (<a
href="https://doi.org/10.1007/s10115-019-01366-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A critical problem in time series analysis is change point detection, which identifies the times when the underlying distribution of a time series abruptly changes. However, several shortcomings limit the use of some existing techniques in real-world applications. First, several change point detection techniques are offline methods, where the whole time series needs to be stored before change point detection can be performed. These methods are not applicable to streaming time series. Second, most techniques assume that the time series is low-dimensional and hence have problems handling high-dimensional time series, where not all dimensions may cause the change. Finally, most methods require user-defined parameters that need to be chosen based on the observed data, which limits their applicability to new unseen data. To address these issues, we propose an Information Gain-based method that does not require prior distributional knowledge for detecting change points and handles high-dimensional time series. The advantages of our proposed method compared to the state-of-the-art algorithms are demonstrated from theoretical basis, as well as via experiments on four synthetic and three real-world human activity datasets.},
  archive      = {J_KIS},
  author       = {Zameni, Masoomeh and Sadri, Amin and Ghafoori, Zahra and Moshtaghi, Masud and Salim, Flora D. and Leckie, Christopher and Ramamohanarao, Kotagiri},
  doi          = {10.1007/s10115-019-01366-x},
  journal      = {Knowledge and Information Systems},
  month        = {2},
  number       = {2},
  pages        = {719-750},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Unsupervised online change point detection in high-dimensional time series},
  volume       = {62},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Dynamic clustering of interval data based on hybrid <span
class="math display"><em>L</em><sub><em>q</em></sub></span> distance.
<em>KIS</em>, <em>62</em>(2), 687–718. (<a
href="https://doi.org/10.1007/s10115-019-01367-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dynamic clustering defines partitions within data and prototypes to each partition. Distance metrics are responsible for checking the closeness between instances and prototypes. Considering the literature about interval data, distances depend on interval bounds and the information inside the intervals is ignored. This paper proposes new distances, which explore the information inside of intervals. It also presents a mapping of intervals to points, which preserves their spatial location and internal variation. We formulate a new hybrid distance for interval data based on the well-known $$L_q$$ distance for point data. This new distance allows for a weighted formulation of the hybridism. Hence, we propose a Hybrid $$L_q$$ distance, a Weighted Hybrid $$L_q$$ distance, as well as the adaptive version of the Hybrid $$L_q$$ distance for interval data. Experiments with synthetic and real interval data sets illustrate the usefulness of the hybrid approach to improve dynamic clustering for interval data.},
  archive      = {J_KIS},
  author       = {de Souza, Leandro Carlos and de Souza, Renata Maria Cardoso Rodrigues and do Amaral, Getúlio José Amorim},
  doi          = {10.1007/s10115-019-01367-w},
  journal      = {Knowledge and Information Systems},
  month        = {2},
  number       = {2},
  pages        = {687-718},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Dynamic clustering of interval data based on hybrid $$L_q$$ distance},
  volume       = {62},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Anomaly detection of event sequences using multiple temporal
resolutions and markov chains. <em>KIS</em>, <em>62</em>(2), 669–686.
(<a href="https://doi.org/10.1007/s10115-019-01365-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Streaming data services, such as video-on-demand, are getting increasingly more popular, and they are expected to account for more than 80% of all Internet traffic in 2020. In this context, it is important for streaming service providers to detect deviations in service requests due to issues or changing end-user behaviors in order to ensure that end-users experience high quality in the provided service. Therefore, in this study we investigate to what extent sequence-based Markov models can be used for anomaly detection by means of the end-users’ control sequences in the video streams, i.e., event sequences such as play, pause, resume and stop. This anomaly detection approach is further investigated over three different temporal resolutions in the data, more specifically: 1 h, 1 day and 3 days. The proposed anomaly detection approach supports anomaly detection in ongoing streaming sessions as it recalculates the probability for a specific session to be anomalous for each new streaming control event that is received. Two experiments are used for measuring the potential of the approach, which gives promising results in terms of precision, recall, $$F_1$$-score and Jaccard index when compared to k-means clustering of the sessions.},
  archive      = {J_KIS},
  author       = {Boldt, Martin and Borg, Anton and Ickin, Selim and Gustafsson, Jörgen},
  doi          = {10.1007/s10115-019-01365-y},
  journal      = {Knowledge and Information Systems},
  month        = {2},
  number       = {2},
  pages        = {669-686},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Anomaly detection of event sequences using multiple temporal resolutions and markov chains},
  volume       = {62},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Leader-aware community detection in complex networks.
<em>KIS</em>, <em>62</em>(2), 639–668. (<a
href="https://doi.org/10.1007/s10115-019-01362-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Community structures are very common in complex networks. Detecting these communities is important for understanding the hidden features of networks. Besides, each community usually has one leader, which presents its significant influence over the whole community. However, most existing methods just focus on the problem of graph clustering, ignoring the role of community leaders. To solve this problem, in this paper, we propose a novel leader-aware community detection algorithm, which can find community structures as well as leaders of each community. This algorithm measures the leadership of each node and lets each one adhere to its local leader, forming dependence trees. Once all dependence trees are definitely settled, the community structures emerge because one tree actually is a cluster. Additionally, each root node of the tree is exactly the leader of corresponding community. This method can quickly determine the belonging of each node. Experimental results on real-world and benchmark networks demonstrate the effectiveness and the efficiency of our algorithm compared with other state-of-the-art approaches.},
  archive      = {J_KIS},
  author       = {Sun, Heli and Du, Hongxia and Huang, Jianbin and Li, Yang and Sun, Zhongbin and He, Liang and Jia, Xiaolin and Zhao, Zhongmeng},
  doi          = {10.1007/s10115-019-01362-1},
  journal      = {Knowledge and Information Systems},
  month        = {2},
  number       = {2},
  pages        = {639-668},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Leader-aware community detection in complex networks},
  volume       = {62},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Measures of uncertainty for knowledge bases. <em>KIS</em>,
<em>62</em>(2), 611–637. (<a
href="https://doi.org/10.1007/s10115-019-01363-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper investigates measures of uncertainty for knowledge bases by using their knowledge structures. Knowledge structures of knowledge bases are first introduced. Then, dependence and independence between knowledge structures of knowledge bases are proposed, which are characterized by inclusion degree. Next, measures of uncertainty for a given knowledge base are studied, and it is proved that the proposed measures are based on the knowledge structure of this knowledge base. Finally, a numerical experiment is conducted to show performance of the proposed measures and effectiveness analysis is done from two aspects of dispersion and correlation in statistics. These results will be significant for understanding the essence of uncertainty for knowledge bases.},
  archive      = {J_KIS},
  author       = {Li, Zhaowen and Zhang, Gangqiang and Wu, Wei-Zhi and Xie, Ningxin},
  doi          = {10.1007/s10115-019-01363-0},
  journal      = {Knowledge and Information Systems},
  month        = {2},
  number       = {2},
  pages        = {611-637},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Measures of uncertainty for knowledge bases},
  volume       = {62},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Random walk-based ranking in signed social networks: Model
and algorithms. <em>KIS</em>, <em>62</em>(2), 571–610. (<a
href="https://doi.org/10.1007/s10115-019-01364-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {How can we rank nodes in signed social networks? Relationships between nodes in a signed network are represented as positive (trust) or negative (distrust) edges. Many social networks have adopted signed networks to express trust between users. Consequently, ranking friends or enemies in signed networks has received much attention from the data mining community. The ranking problem, however, is challenging because it is difficult to interpret negative edges. Traditional random walk-based methods such as PageRank and random walk with restart cannot provide effective rankings in signed networks since they assume only positive edges. Although several methods have been proposed by modifying traditional ranking models, they also fail to account for proper rankings due to the lack of ability to consider complex edge relations. In this paper, we propose Signed Random Walk with Restart (SRWR), a novel model for personalized ranking in signed networks. We introduce a signed random surfer so that she considers negative edges by changing her sign for walking. Our model provides proper rankings considering signed edges based on the signed random walk. We develop two methods for computing SRWR scores: SRWR-Iter and SRWR-Pre which are iterative and preprocessing methods, respectively. SRWR-Iter naturally follows the definition of SRWR, and iteratively updates SRWR scores until convergence. SRWR-Pre enables fast ranking computation which is important for the performance of applications of SRWR. Through extensive experiments, we demonstrate that SRWR achieves the best accuracy for link prediction, predicts trolls $$4\times $$ more accurately, and shows a satisfactory performance for inferring missing signs of edges compared to other competitors. In terms of efficiency, SRWR-Pre preprocesses a signed network $$4.5 \times $$ faster and requires $$11 \times $$ less memory space than other preprocessing methods; furthermore, SRWR-Pre computes SRWR scores up to $$14 \times $$ faster than other methods in the query phase.},
  archive      = {J_KIS},
  author       = {Jung, Jinhong and Jin, Woojeong and Kang, U},
  doi          = {10.1007/s10115-019-01364-z},
  journal      = {Knowledge and Information Systems},
  month        = {2},
  number       = {2},
  pages        = {571-610},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Random walk-based ranking in signed social networks: Model and algorithms},
  volume       = {62},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Automatic attribute construction for basketball modelling.
<em>KIS</em>, <em>62</em>(2), 541–570. (<a
href="https://doi.org/10.1007/s10115-019-01361-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We address the problem of automatic extraction of patterns in the sequence of events in basketball games and construction of statistical models for generating a plausible simulation of a match between two distinct teams. We present a method for automatic construction of an attribute space which requires very little expert knowledge. The attributes are defined as the ratio between the number of entries and exits from higher-level concepts that are identified as groups of similar in-game events. The similarity between events is determined by the similarity between probability distributions describing the preceding and the following events in the observed sequences of game progression. The methodology is general and is applicable to any sports game that can be modelled as a random walk through the state space. Experiments on basketball show that automatically generated attributes are as informative as those derived using expert knowledge. Furthermore, the obtained simulations are in line with empirical data.},
  archive      = {J_KIS},
  author       = {Vračar, Petar and Štrumbelj, Erik and Kononenko, Igor},
  doi          = {10.1007/s10115-019-01361-2},
  journal      = {Knowledge and Information Systems},
  month        = {2},
  number       = {2},
  pages        = {541-570},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Automatic attribute construction for basketball modelling},
  volume       = {62},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Clustering analysis using a novel locality-informed grey
wolf-inspired clustering approach. <em>KIS</em>, <em>62</em>(2),
507–539. (<a href="https://doi.org/10.1007/s10115-019-01358-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Grey wolf optimizer (GWO) is known as one of the recent popular metaheuristic algorithms inspired from the social collaboration and team hunting activities of grey wolves in nature. This algorithm benefits from stochastic operators, but it is still prone to stagnation in local optima and premature convergence when solving problems with a large number of variables (e.g., clustering problems). To alleviate this shortcoming, the GWO algorithm is hybridized with the well-known tabu search (TS). To investigate the performance of the proposed hybrid GWO and TS (GWOTS), it is compared with well-regarded metaheuristics on various clustering datasets. The comprehensive experiments and analysis verify that the proposed GWOTS shows an improved performance compared to GWO and can be utilized for clustering applications.},
  archive      = {J_KIS},
  author       = {Aljarah, Ibrahim and Mafarja, Majdi and Heidari, Ali Asghar and Faris, Hossam and Mirjalili, Seyedali},
  doi          = {10.1007/s10115-019-01358-x},
  journal      = {Knowledge and Information Systems},
  month        = {2},
  number       = {2},
  pages        = {507-539},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Clustering analysis using a novel locality-informed grey wolf-inspired clustering approach},
  volume       = {62},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). HEEL: Exploratory entity linking for heterogeneous
information networks. <em>KIS</em>, <em>62</em>(2), 485–506. (<a
href="https://doi.org/10.1007/s10115-019-01354-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A heterogeneous information network (HIN) is a ubiquitous data model, consisting of multiple types of entities and relations. Names of entities in HINs are inherently ambiguous, making it difficult to fully disambiguate a HIN. In this paper, we introduce the task of exploratory entity linking for HINs. Given a partially disambiguated HIN, we aim at linking ambiguous names to disambiguated entities in the HIN if their referent entities are present. We also try to “explore” other alternatives by discovering new entities and adding them to the HIN. A partial classification EM-based approach is proposed to address this task. We present a constrained probability propagation model to link surface names to entities in the HIN. New entity detection process is modeled as a maximum edge weight clique problem. Experiments illustrate that our method outperforms state-of-the-art methods for entity linking with HINs and author name disambiguation.},
  archive      = {J_KIS},
  author       = {Wang, Chengyu and He, Xiaofeng and Zhou, Aoying},
  doi          = {10.1007/s10115-019-01354-1},
  journal      = {Knowledge and Information Systems},
  month        = {2},
  number       = {2},
  pages        = {485-506},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {HEEL: Exploratory entity linking for heterogeneous information networks},
  volume       = {62},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Dataset-transformation: Improving clustering by enhancing
the structure with DipScaling and DipTransformation. <em>KIS</em>,
<em>62</em>(2), 457–484. (<a
href="https://doi.org/10.1007/s10115-019-01388-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A data set might have a well-defined structure, but this does not necessarily lead to good clustering results. If the structure is hidden in an unfavourable scaling, clustering will usually fail. The aim of this work is to present techniques—DipScaling and DipTransformation—which enhance the data set by rescaling and transforming its features and thus emphasizing and accentuating its structure. If the structure is sufficiently clear, clustering algorithms will perform far better. We refer to such techniques as “Dataset-Transformations” and try to provide a mathematical framework for them. To show that our algorithms work well, we have conducted extensive experiments on several real-world data sets, where we improve clustering not only for k-means, which is our main focus but also for other standard clustering approaches.},
  archive      = {J_KIS},
  author       = {Schelling, Benjamin and Plant, Claudia},
  doi          = {10.1007/s10115-019-01388-5},
  journal      = {Knowledge and Information Systems},
  month        = {2},
  number       = {2},
  pages        = {457-484},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Dataset-transformation: Improving clustering by enhancing the structure with DipScaling and DipTransformation},
  volume       = {62},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Relevant feature selection and ensemble classifier design
using bi-objective genetic algorithm. <em>KIS</em>, <em>62</em>(2),
423–455. (<a href="https://doi.org/10.1007/s10115-019-01341-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the era of digital boom, single classifier cannot perform well in various datasets. Ensemble classifier aims to bridge this performance gap by combining multiple classifiers of diverse characteristics to get better generalization. But classifier selection highly depends on the dataset, and its efficiency degrades tremendously due to the presence of irrelevant features. Feature selection aids the performance of classifier by removing those irrelevant features. Initially, we have proposed a bi-objective genetic algorithm-based feature selection method (FSBOGA), where nonlinear, uniform, hybrid cellular automata are used to generate an initial population. Objective functions are defined using lower bound approximation of rough set theory and Kullback–Leibler divergence method of information theory to select unambiguous and informative features. The replacement strategy for creation of next-generation population is based on the Pareto optimal solution with respect to both the objective functions. Next, a novel bi-objective genetic algorithm-based ensemble classification method (CCBOGA) is devised to ensemble the individual classifiers designed using obtained reduced datasets. It is observed that the constructed ensemble classifier performs better than the individual classifiers. The performances of proposed FSBOGA and CCBOGA are investigated on some popular datasets and compared with the state-of-the-art algorithms to demonstrate their effectiveness.},
  archive      = {J_KIS},
  author       = {Das, Asit Kumar and Pati, Soumen Kumar and Ghosh, Arka},
  doi          = {10.1007/s10115-019-01341-6},
  journal      = {Knowledge and Information Systems},
  month        = {2},
  number       = {2},
  pages        = {423-455},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Relevant feature selection and ensemble classifier design using bi-objective genetic algorithm},
  volume       = {62},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Multi-agent system application for music features
extraction, meta-classification and context analysis. <em>KIS</em>,
<em>62</em>(1), 401–422. (<a
href="https://doi.org/10.1007/s10115-018-1319-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Manual music classification is a slow and costly process. Most recent works about music auto-classification such as genre or emotions make this process easier, but are focused on a single task. In this work, a music multi-classification platform is presented. This platform is based on multi-agent systems, allowing to distribute the extraction, classification, and service tasks among agents. The platform performs a musical genre and emotional classification and provides context information of songs from social networks such as Twitter and Last.fm. The methods chosen based on meta-classifiers to perform single-label and multi-label classification obtain great results. In the case of multi-label classification, better results are obtained than in other previous works.},
  archive      = {J_KIS},
  author       = {Pérez-Marcos, Javier and Jiménez-Bravo, Diego M. and De Paz, Juan F. and Villarrubia González, Gabriel and López, Vivian F. and Gil, Ana B.},
  doi          = {10.1007/s10115-018-1319-2},
  journal      = {Knowledge and Information Systems},
  month        = {1},
  number       = {1},
  pages        = {401-422},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Multi-agent system application for music features extraction, meta-classification and context analysis},
  volume       = {62},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Exploiting block co-occurrence to control block sizes for
entity resolution. <em>KIS</em>, <em>62</em>(1), 359–400. (<a
href="https://doi.org/10.1007/s10115-019-01347-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The problem of identifying duplicated entities in a dataset has gained increasing importance during the last decades. Due to the large size of the datasets, this problem can be very costly to be solved due to its intrinsic quadratic complexity. Both researchers and practitioners have developed a variety of techniques aiming to speed up a solution to this problem. One of these techniques is called blocking, an indexing technique that splits the dataset into a set of blocks, such that each block contains entities that share a common property evaluated by a blocking key function. In order to improve the efficacy of the blocking technique, multiple blocking keys may be used, and thus, a set of blocking results is generated. In this paper, we investigate how to control the size of the blocks generated by the use of multiple blocking keys and maintain reasonable quality results, which is measured by the quality of the produced blocks. By controlling the size of the blocks, we can reduce the overall cost of solving an entity resolution problem and facilitate the execution of a variety of tasks (e.g., real-time and privacy-preserving entity resolution). For doing so, we propose many heuristics which exploit the co-occurrence of entities among the generated blocks for pruning, splitting and merging blocks. The experimental results we carry out using four datasets confirm the adequacy of the proposed heuristics for generating block sizes within a predefined range threshold as well as maintaining reasonable blocking quality results.},
  archive      = {J_KIS},
  author       = {Nascimento, Dimas Cassimiro and Pires, Carlos Eduardo Santos and Mestre, Demetrio Gomes},
  doi          = {10.1007/s10115-019-01347-0},
  journal      = {Knowledge and Information Systems},
  month        = {1},
  number       = {1},
  pages        = {359-400},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Exploiting block co-occurrence to control block sizes for entity resolution},
  volume       = {62},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Analysis of loss functions for fast single-class
classification. <em>KIS</em>, <em>62</em>(1), 337–358. (<a
href="https://doi.org/10.1007/s10115-019-01395-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider neural network training, in applications in which there are many possible classes, but at test time, the task is a binary classification task of determining whether the given example belongs to a specific class. We define the single logit classification (SLC) task: training the network so that at test time, it would be possible to accurately identify whether the example belongs to a given class in a computationally efficient manner, based only on the output logit for this class. We propose a natural principle, the Principle of Logit Separation, as a guideline for choosing and designing losses suitable for the SLC task. We show that the cross-entropy loss function is not aligned with the Principle of Logit Separation. In contrast, there are known loss functions, as well as novel batch loss functions that we propose, which are aligned with this principle. Our experiments show that indeed in almost all cases, losses that are aligned with the Principle of Logit Separation obtain at least 20% relative accuracy improvement in the SLC task compared to losses that are not aligned with it, and sometimes considerably more. Furthermore, we show that fast SLC does not cause any drop in binary classification accuracy, compared to standard classification in which all logits are computed, and yields a speedup which grows with the number of classes.},
  archive      = {J_KIS},
  author       = {Keren, Gil and Sabato, Sivan and Schuller, Björn},
  doi          = {10.1007/s10115-019-01395-6},
  journal      = {Knowledge and Information Systems},
  month        = {1},
  number       = {1},
  pages        = {337-358},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Analysis of loss functions for fast single-class classification},
  volume       = {62},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Constructing biomedical domain-specific knowledge graph with
minimum supervision. <em>KIS</em>, <em>62</em>(1), 317–336. (<a
href="https://doi.org/10.1007/s10115-019-01351-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Domain-specific knowledge graph is an effective way to represent complex domain knowledge in a structured format and has shown great success in real-world applications. Most existing work on knowledge graph construction and completion shares several limitations in that sufficient external resources such as large-scale knowledge graphs and concept ontologies are required as the starting point. However, such extensive domain-specific labeling is highly time-consuming and requires special expertise, especially in biomedical domains. Therefore, knowledge extraction from unstructured contexts with minimum supervision is crucial in biomedical fields. In this paper, we propose a versatile approach for knowledge graph construction with minimum supervision based on unstructured biomedical domain-specific contexts including the steps of entity recognition, unsupervised entity and relation embedding, latent relation generation via clustering, relation refinement and relation assignment to assign cluster-level labels. The experimental results based on 24,687 unstructured biomedical science abstracts show that the proposed framework can effectively extract 16,192 structured facts with high precision. Moreover, we demonstrate that the constructed knowledge graph is a sufficient resource for the task of knowledge graph completion and new knowledge inference from unseen contexts.},
  archive      = {J_KIS},
  author       = {Yuan, Jianbo and Jin, Zhiwei and Guo, Han and Jin, Hongxia and Zhang, Xianchao and Smith, Tristram and Luo, Jiebo},
  doi          = {10.1007/s10115-019-01351-4},
  journal      = {Knowledge and Information Systems},
  month        = {1},
  number       = {1},
  pages        = {317-336},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Constructing biomedical domain-specific knowledge graph with minimum supervision},
  volume       = {62},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). An intelligence approach for group stock portfolio
optimization with a trading mechanism. <em>KIS</em>, <em>62</em>(1),
287–316. (<a href="https://doi.org/10.1007/s10115-019-01353-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Optimizing a stock portfolio from a given financial dataset is always a very attractive task, as various factors should be considered. Hence, many methods based on evolutionary algorithms have been developed in the past decades to deal with the portfolio optimization problem. To provide a more flexible stock portfolio, we propose an algorithm to optimize a group stock portfolio by using a grouping genetic algorithm. In accordance with the optimized group stock portfolio, many stock portfolios can be generated and provided to investors. Each chromosome in the genetic algorithm is composed of a grouping part, a stock part and a stock portfolio part. The grouping and stock parts are used to indicate how to divide stocks into groups. The stock portfolio part is used to represent how many stocks should be selected from groups to form a portfolio and what units should be purchased. Four fitness functions are designed to evaluate each individual. Each of them is composed of the group balance, the unit balance, the stock price balance and the portfolio satisfaction. Genetic operations, including crossover, mutation and inversion, are then executed to obtain new offspring to find the best solution. Furthermore, the proposed approach with a trading mechanism is designed to get a more useful group stock portfolio. Experiments on 31 stocks in accordance with four scenarios were conducted to show the merits and effectiveness of the proposed approach.},
  archive      = {J_KIS},
  author       = {Chen, Chun-Hao and Lu, Cheng-Yu and Lin, Cheng-Bon},
  doi          = {10.1007/s10115-019-01353-2},
  journal      = {Knowledge and Information Systems},
  month        = {1},
  number       = {1},
  pages        = {287-316},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {An intelligence approach for group stock portfolio optimization with a trading mechanism},
  volume       = {62},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Mining maritime traffic conflict trajectories from a massive
AIS data. <em>KIS</em>, <em>62</em>(1), 259–285. (<a
href="https://doi.org/10.1007/s10115-019-01355-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The growing volume of maritime traffic is proving a hindrance to navigational safety. Researchers have sought to improve the safety of maritime transportation by conducting statistical analysis on historical collision data in order to identify the causes of maritime collisions. However, this approach is hindered by the limited number of incidents that can be collected in a given area over a given period of time. Automatic Identification System (AIS) has made available enormous quantities of maritime traffic data. Trajectory data are collected through the electronic exchange of navigational data among ships and terrestrial and satellite base stations. Due to a massive AIS data of recording ship movement, such data provide great opportunity to discover maritime traffic knowledge of movement behavior analysis, route estimation, and the detection of anomalous behaviors. Our objective in this paper was to identify potential between-ship traffic conflicts through the discovery of AIS data. Traffic conflict refers to trajectories that could lead to a collision if the ships do not take any evasive action. In other words, conflicting trajectories can be treated as a near-collision cases for analysis. The prevention of collisions requires an efficient method by which to extract conflicting trajectories from a massive collection of AIS data. To this end, we developed a framework CCT Discovery that allows the automated identification of clusters of conflicting trajectories (CCTs) from AIS data without supervision. Experiments based on real-world data demonstrate the efficacy of the proposed framework in terms of accuracy and efficiency. For improvement in the navigational traffic safety, the discovered data of conflict trajectory can contribute to numerous applications, such as collision situation awareness and prediction, anti-collision behaviors modeling and recommendation, and conflict area analysis for maritime traffic flow management.},
  archive      = {J_KIS},
  author       = {Lei, Po-Ruey},
  doi          = {10.1007/s10115-019-01355-0},
  journal      = {Knowledge and Information Systems},
  month        = {1},
  number       = {1},
  pages        = {259-285},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Mining maritime traffic conflict trajectories from a massive AIS data},
  volume       = {62},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Rate of change analysis for interestingness measures.
<em>KIS</em>, <em>62</em>(1), 239–258. (<a
href="https://doi.org/10.1007/s10115-019-01352-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The use of association rule mining techniques in diverse contexts and domains has resulted in the creation of numerous interestingness measures. This, in turn, has motivated researchers to come up with various classification schemes for these measures. One popular approach to classify the objective measures is to assess the set of mathematical properties they satisfy in order to help practitioners select the right measure for a given problem. In this research, we discuss the insufficiency of the existing properties in the literature to capture certain behaviors of interestingness measures. This motivates us to adopt an approach where a measure is described by how it varies if there is a unit change in the frequency count $$(f_{11},f_{10},f_{01},f_{00})$$, at different preexisting states of the counts. This rate of change analysis is formally defined as the first partial derivative of the measure with respect to the various frequency counts. We use this analysis to define two novel properties, unit-null asymptotic invariance (UNAI) and unit-null zero rate (UNZR). UNAI looks at the asymptotic effect of adding frequency patterns, while UNZR looks at the initial effect of adding frequency patterns when they do not preexist in the dataset. We present a comprehensive analysis of 50 interestingness measures and classify them in accordance with the two properties. We also present multiple empirical studies, involving both synthetic and real-world datasets, which are used to cluster various measures according to the rule ranking patterns of the measures. The study concludes with the observation that classification of measures using the empirical clusters shares significant similarities to the classification of measures done through the properties presented in this research.},
  archive      = {J_KIS},
  author       = {Sudarsanam, Nandan and Kumar, Nishanth and Sharma, Abhishek and Ravindran, Balaraman},
  doi          = {10.1007/s10115-019-01352-3},
  journal      = {Knowledge and Information Systems},
  month        = {1},
  number       = {1},
  pages        = {239-258},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Rate of change analysis for interestingness measures},
  volume       = {62},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Histogram-based clustering of multiple data streams.
<em>KIS</em>, <em>62</em>(1), 203–238. (<a
href="https://doi.org/10.1007/s10115-019-01350-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper introduces a strategy for clustering online multiple data streams. We assume that several sources are used for recording, over time, data about some physical phenomena. Each source provides repeated measurements at a very high frequency so that it is not possible to store the whole amount of data into some easy-to-access media, but data are available only in batches. Our aim is to discover a partition of the sources (e.g. sensors) into homogeneous clusters, analysing the incoming streams of data. The proposed strategy is based on processing the incoming data batches independently, through an initial summarization of the data batches by histograms and, then, by means of a local clustering performed on the histograms which provides a further data summarization. To keep track of the data proximities among the data streams over time, we use local clustering outputs for updating a proximity matrix. The final partitioning of the streams is obtained by a clustering based on such proximity matrix. Through an application on real and simulated data, we show the effectiveness of our strategy in finding homogeneous groups of sources of data streams.},
  archive      = {J_KIS},
  author       = {Balzanella, Antonio and Verde, Rosanna},
  doi          = {10.1007/s10115-019-01350-5},
  journal      = {Knowledge and Information Systems},
  month        = {1},
  number       = {1},
  pages        = {203-238},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Histogram-based clustering of multiple data streams},
  volume       = {62},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Mutual clustering on comparative texts via heterogeneous
information networks. <em>KIS</em>, <em>62</em>(1), 175–202. (<a
href="https://doi.org/10.1007/s10115-019-01356-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Currently, many intelligence systems contain the texts from multi-sources, e.g., bulletin board system posts, tweets and news. These texts can be “comparative” since they may be semantically correlated and thus provide us with different perspectives toward the same topics or events. To better organize the multi-sourced texts and obtain more comprehensive knowledge, we propose to study the novel problem of Mutual Clustering on Comparative Texts (MCCT), which aims to cluster the comparative texts simultaneously and collaboratively. The MCCT problem is difficult to address because 1) comparative texts usually present different data formats and structures and thus, they are hard to organize and 2) there lacks an effective method to connect the semantically correlated comparative texts to facilitate clustering them in an unified way. To this aim, in this paper, we propose a Heterogeneous Information Network-based Text clustering framework HINT. HINT first models multi-sourced texts (e.g. news and tweets) as heterogeneous information networks by introducing the shared “anchor texts” to connect the comparative texts. Next, two similarity matrices based on HINT as well as a transition matrix for cross-text-source knowledge transfer are constructed. Comparative texts clustering are then conducted by utilizing the constructed matrices. Finally, a mutual clustering algorithm is also proposed to further unify the separate clustering results of the comparative texts by introducing a clustering consistency constraint. We conduct extensive experimental on three tweets-news datasets, and the results demonstrate the effectiveness and robustness of the proposed method in addressing the MCCT problem.},
  archive      = {J_KIS},
  author       = {Cao, Jianping and Wang, Senzhang and Wen, Danyan and Peng, Zhaohui and Yu, Philip S. and Wang, Fei-yue},
  doi          = {10.1007/s10115-019-01356-z},
  journal      = {Knowledge and Information Systems},
  month        = {1},
  number       = {1},
  pages        = {175-202},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Mutual clustering on comparative texts via heterogeneous information networks},
  volume       = {62},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A review of mobile sensing systems, applications, and
opportunities. <em>KIS</em>, <em>62</em>(1), 145–174. (<a
href="https://doi.org/10.1007/s10115-019-01346-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mobile phones, vehicles, appliances, and other types of devices have sensors in the last few years. On the good side, this makes the world increasingly interconnected every day. However, this interconnection generates Big Data that cannot be processed using traditional tools because of its volume, variety, and speed. This paper contributes with a review of mobile sensing systems, including their applications, shortcomings, and opportunities. A taxonomy covering the different systems revised is proposed. Moreover, the main characteristics of mobile sensing architectures are explained and research-related works are studied into the context of these characteristics. Multi-agent systems (MASs) are considered as a perfect match to create large-scale, multi-device, and multi-purpose mobile sensing systems with the potential of obtaining information from heterogeneous devices, open sources, and social networks. Finally, the paper also contributes with the overview of a MAS architecture that aims to leverage these features while the studied dimensions observed in the reviewed literature are covered.},
  archive      = {J_KIS},
  author       = {Laport-López, Francisco and Serrano, Emilio and Bajo, Javier and Campbell, Andrew T.},
  doi          = {10.1007/s10115-019-01346-1},
  journal      = {Knowledge and Information Systems},
  month        = {1},
  number       = {1},
  pages        = {145-174},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {A review of mobile sensing systems, applications, and opportunities},
  volume       = {62},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Resolution-based rewriting for horn- <span
class="math display">𝒮ℋℐ𝒬</span> ontologies. <em>KIS</em>,
<em>62</em>(1), 107–143. (<a
href="https://doi.org/10.1007/s10115-019-01345-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {An important approach to query answering over description logic (DL) ontologies is via rewriting the input ontology and query into languages such as (disjunctive) datalog, for which scalable data saturation systems exist. This approach has been studied for DLs of different expressivities such as DL-Lite, $$\mathcal {ELHI}$$ and Horn-$$\mathcal {SHIQ}$$. When it comes to expressive languages resolution is an important technique that can be applied to obtain the rewritings. This is mainly because it allows for the design of general-purpose algorithms that can be easily lifted to support languages of high expressivity. In the current work we present an efficient resolution-based rewriting algorithm tailor-made for the expressive DL language Horn-$$\mathcal {SHIQ}$$. Our algorithm avoids performing many unnecessary inferences, which is one of the main problems of resolution-based algorithms. This is achieved by careful analysis of the complex axioms structure supported in Horn-$$\mathcal {SHIQ}$$. Moreover, we have implemented the proposed algorithm and obtained very encouraging results when conducting extensive experimental evaluation.},
  archive      = {J_KIS},
  author       = {Trivela, Despoina and Stoilos, Giorgos and Chortaras, Alexandros and Stamou, Giorgos},
  doi          = {10.1007/s10115-019-01345-2},
  journal      = {Knowledge and Information Systems},
  month        = {1},
  number       = {1},
  pages        = {107-143},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Resolution-based rewriting for horn- $$\mathcal {SHIQ}$$ ontologies},
  volume       = {62},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). An integrated trust establishment model for the internet of
agents. <em>KIS</em>, <em>62</em>(1), 79–105. (<a
href="https://doi.org/10.1007/s10115-019-01348-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Internet of Agents (IoA) is an emerging field of research that extends the concept of Internet of Things (IoT) by augmenting internal reasoning and intelligence capability to, traditionally, naïve things used in IoT. In a fully automated IoA environment, agents tend to choose trustworthy partners to rely on for fulfilling their objectives, particularly when the agents cannot guarantee that potential interacting partners share the same core beliefs or make accurate statements regarding their competence and abilities. Trust establishment mechanisms can be used to direct trustees, rather than trustors, to build a higher level of trust and have greater impact on the results of their interactions when such interactions are based on trust. This paper presents ITE, a trust establishment model that integrates the two major sources of information to produce a comprehensive assessment of a trustor’s needs in IoA. Specifically, ITE attempts to incorporate explicit and implicit feedbacks to provide trust establishment under a wider range of circumstances.},
  archive      = {J_KIS},
  author       = {Aref, Abdullah and Tran, Thomas},
  doi          = {10.1007/s10115-019-01348-z},
  journal      = {Knowledge and Information Systems},
  month        = {1},
  number       = {1},
  pages        = {79-105},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {An integrated trust establishment model for the internet of agents},
  volume       = {62},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A scalable privacy-preserving framework for temporal record
linkage. <em>KIS</em>, <em>62</em>(1), 45–78. (<a
href="https://doi.org/10.1007/s10115-019-01370-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Record linkage (RL) is the process of identifying matching records from different databases that refer to the same entity. In many applications, it is common that the attribute values of records that belong to the same entity evolve over time, for example people can change their surname or address. Therefore, to identify the records that refer to the same entity over time, RL should make use of temporal information such as the time-stamp of when a record was created and/or update last. However, if RL needs to be conducted on information about people, due to privacy and confidentiality concerns organisations are often not willing or allowed to share sensitive data in their databases, such as personal medical records or location and financial details, with other organisations. This paper proposes a scalable framework for privacy-preserving temporal record linkage that can link different databases while ensuring the privacy of sensitive data in these databases. We propose two protocols that can be used in different linkage scenarios with and without a third party. Our protocols use Bloom filter encoding which incorporates the temporal information available in records during the linkage process. Our approaches first securely calculate the probabilities of entities changing attribute values in their records over a period of time. Based on these probabilities, we then generate a set of masking Bloom filters to adjust the similarities between record pairs. We provide a theoretical analysis of the complexity and privacy of our techniques and conduct an empirical study on large real databases containing several millions of records. The experimental results show that our approaches can achieve better linkage quality compared to non-temporal PPRL while providing privacy to individuals in the databases that are being linked.},
  archive      = {J_KIS},
  author       = {Ranbaduge, Thilina and Christen, Peter},
  doi          = {10.1007/s10115-019-01370-1},
  journal      = {Knowledge and Information Systems},
  month        = {1},
  number       = {1},
  pages        = {45-78},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {A scalable privacy-preserving framework for temporal record linkage},
  volume       = {62},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Scalability and sparsity issues in recommender datasets: A
survey. <em>KIS</em>, <em>62</em>(1), 1–43. (<a
href="https://doi.org/10.1007/s10115-018-1254-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recommender systems have been widely used in various domains including movies, news, music with an aim to provide the most relevant proposals to users from a variety of available options. Recommender systems are designed using techniques from many fields, some of which are: machine learning, information retrieval, data mining, linear algebra and artificial intelligence. Though in-memory nearest-neighbor computation is a typical approach for collaborative filtering due to its high recommendation accuracy; its performance on scalability is still poor given a huge user and item base and availability of only few ratings (i.e., data sparsity) in archetypal merchandising applications. In order to alleviate scalability and sparsity issues in recommender systems, several model-based approaches were proposed in the past. However, if research in recommender system is to achieve its potential, there is a need to understand the prominent techniques used directly to build recommender systems or for preprocessing recommender datasets, along with its strengths and weaknesses. In this work, we present an overview of some of the prominent traditional as well as advanced techniques that can effectively handle data dimensionality and data sparsity. The focus of this survey is to present an overview of the applicability of some advanced techniques, particularly clustering, biclustering, matrix factorization, graph-theoretic, and fuzzy techniques in recommender systems. In addition, it highlights the applicability and recent research works done using each technique.},
  archive      = {J_KIS},
  author       = {Singh, Monika},
  doi          = {10.1007/s10115-018-1254-2},
  journal      = {Knowledge and Information Systems},
  month        = {1},
  number       = {1},
  pages        = {1-43},
  shortjournal = {Knowl. Inf. Syst.},
  title        = {Scalability and sparsity issues in recommender datasets: A survey},
  volume       = {62},
  year         = {2020},
}
</textarea>
</details></li>
</ul>

</body>
</html>
