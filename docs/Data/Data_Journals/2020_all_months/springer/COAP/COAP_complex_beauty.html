<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>COAP_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="coap---97">COAP - 97</h2>
<ul>
<li><details>
<summary>
(2020). On the use of polynomial models in multiobjective
directional direct search. <em>COAP</em>, <em>77</em>(3), 897–918. (<a
href="https://doi.org/10.1007/s10589-020-00233-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Polynomial interpolation or regression models are an important tool in Derivative-free Optimization, acting as surrogates of the real function. In this work, we propose the use of these models in the multiobjective framework of directional direct search, namely the one of Direct Multisearch. Previously evaluated points are used to build quadratic polynomial models, which are minimized in an attempt of generating nondominated points of the true function, defining a search step for the algorithm. Numerical results state the competitiveness of the proposed approach.},
  archive      = {J_COAP},
  author       = {Brás, C. P. and Custódio, A. L.},
  doi          = {10.1007/s10589-020-00233-8},
  journal      = {Computational Optimization and Applications},
  number       = {3},
  pages        = {897-918},
  shortjournal = {Comput. Optim. Appl.},
  title        = {On the use of polynomial models in multiobjective directional direct search},
  volume       = {77},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Expected residual minimization method for monotone
stochastic tensor complementarity problem. <em>COAP</em>,
<em>77</em>(3), 871–896. (<a
href="https://doi.org/10.1007/s10589-020-00222-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we first introduce a new class of structured tensors, named strictly positive semidefinite tensors, and show that a strictly positive semidefinite tensor is not an $$R_0$$ tensor. We focus on the stochastic tensor complementarity problem (STCP), where the expectation of the involved tensor is a strictly positive semidefinite tensor. We denote such an STCP as a monotone STCP. Based on three popular NCP functions, the min function, the Fischer–Burmeister (FB) function and the penalized FB function, as well as the special structure of the monotone STCP, we introduce three new NCP functions and establish the expected residual minimization (ERM) formulation of the monotone STCP. We show that the solution set of the ERM problem is nonempty and bounded if the solution set of the expected value (EV) formulation for such an STCP is nonempty and bounded. Moreover, an approximate regularized model is proposed to weaken the conditions for nonemptiness and boundedness of the solution set of the ERM problem in practice. Numerical results indicate that the performance of the ERM method is better than that of the EV method.},
  archive      = {J_COAP},
  author       = {Ming, Zhenyu and Zhang, Liping and Qi, Liqun},
  doi          = {10.1007/s10589-020-00222-x},
  journal      = {Computational Optimization and Applications},
  number       = {3},
  pages        = {871-896},
  shortjournal = {Comput. Optim. Appl.},
  title        = {Expected residual minimization method for monotone stochastic tensor complementarity problem},
  volume       = {77},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A lagrange multiplier method for semilinear elliptic state
constrained optimal control problems. <em>COAP</em>, <em>77</em>(3),
831–869. (<a href="https://doi.org/10.1007/s10589-020-00223-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper we apply an augmented Lagrange method to a class of semilinear elliptic optimal control problems with pointwise state constraints. We show strong convergence of subsequences of the primal variables to a local solution of the original problem as well as weak convergence of the adjoint states and weak-* convergence of the multipliers associated to the state constraint. Moreover, we show existence of stationary points in arbitrary small neighborhoods of local solutions of the original problem. Additionally, various numerical results are presented.},
  archive      = {J_COAP},
  author       = {Karl, Veronika and Neitzel, Ira and Wachsmuth, Daniel},
  doi          = {10.1007/s10589-020-00223-w},
  journal      = {Computational Optimization and Applications},
  number       = {3},
  pages        = {831-869},
  shortjournal = {Comput. Optim. Appl.},
  title        = {A lagrange multiplier method for semilinear elliptic state constrained optimal control problems},
  volume       = {77},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Hybrid riemannian conjugate gradient methods with global
convergence properties. <em>COAP</em>, <em>77</em>(3), 811–830. (<a
href="https://doi.org/10.1007/s10589-020-00224-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents Riemannian conjugate gradient methods and global convergence analyses under the strong Wolfe conditions. The main idea of the proposed methods is to combine the good global convergence properties of the Dai–Yuan method with the efficient numerical performance of the Hestenes–Stiefel method. One of the proposed algorithms is a generalization to Riemannian manifolds of the hybrid conjugate gradient method of the Dai and Yuan in Euclidean space. The proposed methods are compared well numerically with the existing methods for solving several Riemannian optimization problems. Python implementations of the methods used in the numerical experiments are available at https://github.com/iiduka-researches/202008-hybrid-rcg .},
  archive      = {J_COAP},
  author       = {Sakai, Hiroyuki and Iiduka, Hideaki},
  doi          = {10.1007/s10589-020-00224-9},
  journal      = {Computational Optimization and Applications},
  number       = {3},
  pages        = {811-830},
  shortjournal = {Comput. Optim. Appl.},
  title        = {Hybrid riemannian conjugate gradient methods with global convergence properties},
  volume       = {77},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Riemannian conjugate gradient methods with inverse
retraction. <em>COAP</em>, <em>77</em>(3), 779–810. (<a
href="https://doi.org/10.1007/s10589-020-00219-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a new class of Riemannian conjugate gradient (CG) methods, in which inverse retraction is used instead of vector transport for search direction construction. In existing methods, differentiated retraction is often used for vector transport to move the previous search direction to the current tangent space. However, a different perspective is adopted here, motivated by the fact that inverse retraction directly measures the displacement from the current to the previous points in terms of tangent vectors at the current point. The proposed algorithm is implemented with the Fletcher–Reeves and the Dai–Yuan formulae, respectively, and global convergence is established using modifications of the Riemannian Wolfe conditions. Computational details of the practical inverse retractions over the Stiefel and fixed-rank manifolds are discussed. Numerical results obtained for the Brockett cost function minimization problem, the joint diagonalization problem, and the low-rank matrix completion problem demonstrate the potential effectiveness of Riemannian CG with inverse retraction.},
  archive      = {J_COAP},
  author       = {Zhu, Xiaojing and Sato, Hiroyuki},
  doi          = {10.1007/s10589-020-00219-6},
  journal      = {Computational Optimization and Applications},
  number       = {3},
  pages        = {779-810},
  shortjournal = {Comput. Optim. Appl.},
  title        = {Riemannian conjugate gradient methods with inverse retraction},
  volume       = {77},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A proximal-point outer approximation algorithm.
<em>COAP</em>, <em>77</em>(3), 755–777. (<a
href="https://doi.org/10.1007/s10589-020-00216-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many engineering and scientific applications, e.g. resource allocation, control of hybrid systems, scheduling, etc., require the solution of mixed-integer non-linear problems (MINLPs). Problems of such class combine the high computational burden arising from considering discrete variables with the complexity of non-linear functions. As a consequence, the development of algorithms able to efficiently solve medium-large MINLPs is still an interesting field of research. In the last decades, several approaches to tackle MINLPs have been developed. Some of such approaches, usually defined as exact methods, aim at finding a globally optimal solution for a given MINLP at expense of a long execution time, while others, generally defined as heuristics, aim at discovering suboptimal feasible solutions in the shortest time possible. Among the various proposed paradigms, outer approximation (OA) and feasibility pump (FP), respectively as exact method and as heuristic, deserve a special mention for the number of relevant publications and successful implementations related to them. In this paper we present a new exact method for convex mixed-integer non-linear programming called proximal outer approximation (POA). POA blends the fundamental ideas behind FP into the general OA scheme that attepts to yield faster and more robust convergence with respect to OA while retaining the good performances in terms of fast generation of feasible solutions of FP.},
  archive      = {J_COAP},
  author       = {De Mauri, Massimo and Gillis, Joris and Swevers, Jan and Pipeleers, Goele},
  doi          = {10.1007/s10589-020-00216-9},
  journal      = {Computational Optimization and Applications},
  number       = {3},
  pages        = {755-777},
  shortjournal = {Comput. Optim. Appl.},
  title        = {A proximal-point outer approximation algorithm},
  volume       = {77},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Convergence rates for an inexact ADMM applied to separable
convex optimization. <em>COAP</em>, <em>77</em>(3), 729–754. (<a
href="https://doi.org/10.1007/s10589-020-00221-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Convergence rates are established for an inexact accelerated alternating direction method of multipliers (I-ADMM) for general separable convex optimization with a linear constraint. Both ergodic and non-ergodic iterates are analyzed. Relative to the iteration number k, the convergence rate is $$\mathcal{{O}}(1/k)$$ in a convex setting and $$\mathcal{{O}}(1/k^2)$$ in a strongly convex setting. When an error bound condition holds, the algorithm is 2-step linearly convergent. The I-ADMM is designed so that the accuracy of the inexact iteration preserves the global convergence rates of the exact iteration, leading to better numerical performance in the test problems.},
  archive      = {J_COAP},
  author       = {Hager, William W. and Zhang, Hongchao},
  doi          = {10.1007/s10589-020-00221-y},
  journal      = {Computational Optimization and Applications},
  number       = {3},
  pages        = {729-754},
  shortjournal = {Comput. Optim. Appl.},
  title        = {Convergence rates for an inexact ADMM applied to separable convex optimization},
  volume       = {77},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A sequential partial linearization algorithm for the
symmetric eigenvalue complementarity problem. <em>COAP</em>,
<em>77</em>(3), 711–728. (<a
href="https://doi.org/10.1007/s10589-020-00226-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we introduce a Sequential Partial Linearization (SPL) algorithm for finding a solution of the symmetric Eigenvalue Complementarity Problem (EiCP). The algorithm can also be used for the computation of a stationary point of a standard fractional quadratic program. A first version of the SPL algorithm employs a line search technique and possesses global convergence to a solution of the EiCP under a simple condition related to the minimum eigenvalue of one of the matrices of the problem. Furthermore, it is shown that this condition is verified for a simpler version of the SPL algorithm that does not require a line search technique. The main computational effort of the SPL algorithm is the solution of a strictly convex standard quadratic problem, which is efficiently solved by a finitely convergent block principal pivoting algorithm. Numerical results of the solution of test problems from different sources indicate that the SPL algorithm is in general efficient for the solution of the symmetric EiCP in terms of the number of iterations, accuracy of the solution and total computational effort.},
  archive      = {J_COAP},
  author       = {Fukushima, Masao and Júdice, Joaquim and de Oliveira, Welington and Sessa, Valentina},
  doi          = {10.1007/s10589-020-00226-7},
  journal      = {Computational Optimization and Applications},
  number       = {3},
  pages        = {711-728},
  shortjournal = {Comput. Optim. Appl.},
  title        = {A sequential partial linearization algorithm for the symmetric eigenvalue complementarity problem},
  volume       = {77},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Momentum and stochastic momentum for stochastic gradient,
newton, proximal point and subspace descent methods. <em>COAP</em>,
<em>77</em>(3), 653–710. (<a
href="https://doi.org/10.1007/s10589-020-00220-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper we study several classes of stochastic optimization algorithms enriched with heavy ball momentum. Among the methods studied are: stochastic gradient descent, stochastic Newton, stochastic proximal point and stochastic dual subspace ascent. This is the first time momentum variants of several of these methods are studied. We choose to perform our analysis in a setting in which all of the above methods are equivalent: convex quadratic problems. We prove global non-asymptotic linear convergence rates for all methods and various measures of success, including primal function values, primal iterates, and dual function values. We also show that the primal iterates converge at an accelerated linear rate in a somewhat weaker sense. This is the first time a linear rate is shown for the stochastic heavy ball method (i.e., stochastic gradient descent method with momentum). Under somewhat weaker conditions, we establish a sublinear convergence rate for Cesàro averages of primal iterates. Moreover, we propose a novel concept, which we call stochastic momentum, aimed at decreasing the cost of performing the momentum step. We prove linear convergence of several stochastic methods with stochastic momentum, and show that in some sparse data regimes and for sufficiently small momentum parameters, these methods enjoy better overall complexity than methods with deterministic momentum. Finally, we perform extensive numerical testing on artificial and real datasets, including data coming from average consensus problems.},
  archive      = {J_COAP},
  author       = {Loizou, Nicolas and Richtárik, Peter},
  doi          = {10.1007/s10589-020-00220-z},
  journal      = {Computational Optimization and Applications},
  number       = {3},
  pages        = {653-710},
  shortjournal = {Comput. Optim. Appl.},
  title        = {Momentum and stochastic momentum for stochastic gradient, newton, proximal point and subspace descent methods},
  volume       = {77},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Issues on the use of a modified bunch and kaufman
decomposition for large scale newton’s equation. <em>COAP</em>,
<em>77</em>(3), 627–651. (<a
href="https://doi.org/10.1007/s10589-020-00225-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this work, we deal with Truncated Newton methods for solving large scale (possibly nonconvex) unconstrained optimization problems. In particular, we consider the use of a modified Bunch and Kaufman factorization for solving the Newton equation, at each (outer) iteration of the method. The Bunch and Kaufman factorization of a tridiagonal matrix is an effective and stable matrix decomposition, which is well exploited in the widely adopted SYMMBK (Bunch and Kaufman in Math Comput 31:163–179, 1977; Chandra in Conjugate gradient methods for partial differential equations, vol 129, 1978; Conn et al. in Trust-region methods. MPS-SIAM series on optimization, Society for Industrial Mathematics, Philadelphia, 2000; HSL, A collection of Fortran codes for large scale scientific computation, http://www.hsl.rl.ac.uk/ ; Marcia in Appl Numer Math 58:449–458, 2008) routine. It can be used to provide conjugate directions, both in the case of $$1\times 1$$ and $$2\times 2$$ pivoting steps. The main drawback is that the resulting solution of Newton’s equation might not be gradient–related, in the case the objective function is nonconvex. Here we first focus on some theoretical properties, in order to ensure that at each iteration of the Truncated Newton method, the search direction obtained by using an adapted Bunch and Kaufman factorization is gradient–related. This allows to perform a standard Armijo-type linesearch procedure, using a bounded descent direction. Furthermore, the results of an extended numerical experience using large scale CUTEst problems is reported, showing the reliability and the efficiency of the proposed approach, both on convex and nonconvex problems.},
  archive      = {J_COAP},
  author       = {Caliciotti, Andrea and Fasano, Giovanni and Potra, Florian and Roma, Massimo},
  doi          = {10.1007/s10589-020-00225-8},
  journal      = {Computational Optimization and Applications},
  number       = {3},
  pages        = {627-651},
  shortjournal = {Comput. Optim. Appl.},
  title        = {Issues on the use of a modified bunch and kaufman decomposition for large scale newton’s equation},
  volume       = {77},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020a). COAP 2019 best paper prize: Paper of andreas tillmann.
<em>COAP</em>, <em>77</em>(3), 623–626. (<a
href="https://doi.org/10.1007/s10589-020-00235-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_COAP},
  doi          = {10.1007/s10589-020-00235-6},
  journal      = {Computational Optimization and Applications},
  number       = {3},
  pages        = {623-626},
  shortjournal = {Comput. Optim. Appl.},
  title        = {COAP 2019 best paper prize: Paper of andreas tillmann},
  volume       = {77},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020b). COAP 2019 best paper prize: Paper of s. Gratton, c. W.
Royer, l. N. Vicente, and z. zhang. <em>COAP</em>, <em>77</em>(3),
617–621. (<a href="https://doi.org/10.1007/s10589-020-00236-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_COAP},
  doi          = {10.1007/s10589-020-00236-5},
  journal      = {Computational Optimization and Applications},
  number       = {3},
  pages        = {617-621},
  shortjournal = {Comput. Optim. Appl.},
  title        = {COAP 2019 best paper prize: Paper of s. gratton, c. w. royer, l. n. vicente, and z. zhang},
  volume       = {77},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Consistent treatment of incompletely converged iterative
linear solvers in reverse-mode algorithmic differentiation.
<em>COAP</em>, <em>77</em>(2), 597–616. (<a
href="https://doi.org/10.1007/s10589-020-00214-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Algorithmic differentiation (AD) is a widely-used approach to compute derivatives of numerical models. Many numerical models include an iterative process to solve non-linear systems of equations. To improve efficiency and numerical stability, AD is typically not applied to the linear solvers. Instead, the differentiated linear solver call is replaced with hand-produced derivative code that exploits the linearity of the original call. In practice, the iterative linear solvers are often stopped prematurely to recompute the linearisation of the non-linear outer loop. We show that in the reverse-mode of AD, the derivatives obtained with partial convergence become inconsistent with the original and the tangent-linear models, resulting in inaccurate adjoints. We present a correction term that restores consistency between adjoint and tangent-linear gradients if linear systems are only partially converged. We prove the consistency of this correction term and show in numerical experiments that the accuracy of adjoint gradients of an incompressible flow solver applied to an industrial test case is restored when the correction term is used.},
  archive      = {J_COAP},
  author       = {Akbarzadeh, Siamak and Hückelheim, Jan and Müller, Jens-Dominik},
  doi          = {10.1007/s10589-020-00214-x},
  journal      = {Computational Optimization and Applications},
  number       = {2},
  pages        = {597-616},
  shortjournal = {Comput. Optim. Appl.},
  title        = {Consistent treatment of incompletely converged iterative linear solvers in reverse-mode algorithmic differentiation},
  volume       = {77},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Global optimization via inverse distance weighting and
radial basis functions. <em>COAP</em>, <em>77</em>(2), 571–595. (<a
href="https://doi.org/10.1007/s10589-020-00215-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Global optimization problems whose objective function is expensive to evaluate can be solved effectively by recursively fitting a surrogate function to function samples and minimizing an acquisition function to generate new samples. The acquisition step trades off between seeking for a new optimization vector where the surrogate is minimum (exploitation of the surrogate) and looking for regions of the feasible space that have not yet been visited and that may potentially contain better values of the objective function (exploration of the feasible space). This paper proposes a new global optimization algorithm that uses inverse distance weighting (IDW) and radial basis functions (RBF) to construct the acquisition function. Rather arbitrary constraints that are simple to evaluate can be easily taken into account. Compared to Bayesian optimization, the proposed algorithm, that we call GLIS (GLobal minimum using Inverse distance weighting and Surrogate radial basis functions), is competitive and computationally lighter, as we show in a set of benchmark global optimization and hyperparameter tuning problems. MATLAB and Python implementations of GLIS are available at http://cse.lab.imtlucca.it/~bemporad/glis .},
  archive      = {J_COAP},
  author       = {Bemporad, Alberto},
  doi          = {10.1007/s10589-020-00215-w},
  journal      = {Computational Optimization and Applications},
  number       = {2},
  pages        = {571-595},
  shortjournal = {Comput. Optim. Appl.},
  title        = {Global optimization via inverse distance weighting and radial basis functions},
  volume       = {77},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Oracle-based algorithms for binary two-stage robust
optimization. <em>COAP</em>, <em>77</em>(2), 539–569. (<a
href="https://doi.org/10.1007/s10589-020-00207-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this work we study binary two-stage robust optimization problems with objective uncertainty. We present an algorithm to calculate efficiently lower bounds for the binary two-stage robust problem by solving alternately the underlying deterministic problem and an adversarial problem. For the deterministic problem any oracle can be used which returns an optimal solution for every possible scenario. We show that the latter lower bound can be implemented in a branch and bound procedure, where the branching is performed only over the first-stage decision variables. All results even hold for non-linear objective functions which are concave in the uncertain parameters. As an alternative solution method we apply a column-and-constraint generation algorithm to the binary two-stage robust problem with objective uncertainty. We test both algorithms on benchmark instances of the uncapacitated single-allocation hub-location problem and of the capital budgeting problem. Our results show that the branch and bound procedure outperforms the column-and-constraint generation algorithm.},
  archive      = {J_COAP},
  author       = {Kämmerling, Nicolas and Kurtz, Jannis},
  doi          = {10.1007/s10589-020-00207-w},
  journal      = {Computational Optimization and Applications},
  number       = {2},
  pages        = {539-569},
  shortjournal = {Comput. Optim. Appl.},
  title        = {Oracle-based algorithms for binary two-stage robust optimization},
  volume       = {77},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). On a numerical shape optimization approach for a class of
free boundary problems. <em>COAP</em>, <em>77</em>(2), 509–537. (<a
href="https://doi.org/10.1007/s10589-020-00212-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper is devoted to a numerical method for the approximation of a class of free boundary problems of Bernoulli’s type, reformulated as optimal shape design problems with appropriate shape functionals. We show the existence of the shape derivative of the cost functional on a class of admissible domains and compute its shape derivative by using the formula proposed in Boulkhemair (SIAM J Control Optim 55(1):156–171, 2017) and Boulkhemair and Chakib (J Convex Anal 21(1):67–87, 2014), that is, by means of support functions. On the numerical level, this allows us to avoid the tedious computations of the method based on vector fields. A gradient method combined with a boundary element method is performed for the approximation of this problem, in order to overcome the re-meshing task required by the finite element method. Finally, we present some numerical results and simulations concerning practical applications, showing the effectiveness of the proposed approach.},
  archive      = {J_COAP},
  author       = {Boulkhemair, A. and Chakib, A. and Nachaoui, A. and Niftiyev, A. A. and Sadik, A.},
  doi          = {10.1007/s10589-020-00212-z},
  journal      = {Computational Optimization and Applications},
  number       = {2},
  pages        = {509-537},
  shortjournal = {Comput. Optim. Appl.},
  title        = {On a numerical shape optimization approach for a class of free boundary problems},
  volume       = {77},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Weak convergence of iterative methods for solving
quasimonotone variational inequalities. <em>COAP</em>, <em>77</em>(2),
491–508. (<a href="https://doi.org/10.1007/s10589-020-00217-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this work, we introduce self-adaptive methods for solving variational inequalities with Lipschitz continuous and quasimonotone mapping(or Lipschitz continuous mapping without monotonicity) in real Hilbert space. Under suitable assumptions, the convergence of algorithms are established without the knowledge of the Lipschitz constant of the mapping. The results obtained in this paper extend some recent results in the literature. Some preliminary numerical experiments and comparisons are reported.},
  archive      = {J_COAP},
  author       = {Liu, Hongwei and Yang, Jun},
  doi          = {10.1007/s10589-020-00217-8},
  journal      = {Computational Optimization and Applications},
  number       = {2},
  pages        = {491-508},
  shortjournal = {Comput. Optim. Appl.},
  title        = {Weak convergence of iterative methods for solving quasimonotone variational inequalities},
  volume       = {77},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). The distance between convex sets with minkowski sum
structure: Application to collision detection. <em>COAP</em>,
<em>77</em>(2), 465–490. (<a
href="https://doi.org/10.1007/s10589-020-00211-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The distance between sets is a long-standing computational geometry problem. In robotics, the distance between convex sets with Minkowski sum structure plays a fundamental role in collision detection. However, it is typically nontrivial to be computed, even if the projection onto each component set admits explicit formula. In this paper, we explore the problem of calculating the distance between convex sets arising from robotics. Upon the recent progress in convex optimization community, the proposed model can be efficiently solved by the recent hot-investigated first-order methods, e.g., alternating direction method of multipliers or primal-dual hybrid gradient method. Preliminary numerical results demonstrate that those first-order methods are fairly efficient in solving distance problems in robotics.},
  archive      = {J_COAP},
  author       = {Wang, Xiangfeng and Zhang, Junping and Zhang, Wenxing},
  doi          = {10.1007/s10589-020-00211-0},
  journal      = {Computational Optimization and Applications},
  number       = {2},
  pages        = {465-490},
  shortjournal = {Comput. Optim. Appl.},
  title        = {The distance between convex sets with minkowski sum structure: Application to collision detection},
  volume       = {77},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A variation of broyden class methods using householder
adaptive transforms. <em>COAP</em>, <em>77</em>(2), 433–463. (<a
href="https://doi.org/10.1007/s10589-020-00209-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this work we introduce and study novel Quasi Newton minimization methods based on a Hessian approximation Broyden Class-type updating scheme, where a suitable matrix $$\tilde{B}_k$$ is updated instead of the current Hessian approximation $$B_k$$ . We identify conditions which imply the convergence of the algorithm and, if exact line search is chosen, its quadratic termination. By a remarkable connection between the projection operation and Krylov spaces, such conditions can be ensured using low complexity matrices $$\tilde{B}_k$$ obtained projecting $$B_k$$ onto algebras of matrices diagonalized by products of two or three Householder matrices adaptively chosen step by step. Experimental tests show that the introduction of the adaptive criterion, which theoretically guarantees the convergence, considerably improves the robustness of the minimization schemes when compared with a non-adaptive choice; moreover, they show that the proposed methods could be particularly suitable to solve large scale problems where L-BFGS is not able to deliver satisfactory performance.},
  archive      = {J_COAP},
  author       = {Cipolla, S. and Di Fiore, C. and Zellini, P.},
  doi          = {10.1007/s10589-020-00209-8},
  journal      = {Computational Optimization and Applications},
  number       = {2},
  pages        = {433-463},
  shortjournal = {Comput. Optim. Appl.},
  title        = {A variation of broyden class methods using householder adaptive transforms},
  volume       = {77},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Acceleration techniques for level bundle methods in weakly
smooth convex constrained optimization. <em>COAP</em>, <em>77</em>(2),
411–432. (<a href="https://doi.org/10.1007/s10589-020-00208-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We develop a unified level-bundle method, called accelerated constrained level-bundle (ACLB) algorithm, for solving constrained convex optimization problems. where the objective and constraint functions can be nonsmooth, weakly smooth, and/or smooth. ACLB employs Nesterov’s accelerated gradient technique, and hence retains the iteration complexity as that of existing bundle-type methods if the objective or one of the constraint functions is nonsmooth. More importantly, ACLB can significantly reduce iteration complexity when the objective and all constraints are (weakly) smooth. In addition, if the objective contains a nonsmooth component which can be written as a specific form of maximum, we show that the iteration complexity of this component can be much lower than that for general nonsmooth objective function. Numerical results demonstrate the effectiveness of the proposed algorithm.},
  archive      = {J_COAP},
  author       = {Chen, Yunmei and Ye, Xiaojing and Zhang, Wei},
  doi          = {10.1007/s10589-020-00208-9},
  journal      = {Computational Optimization and Applications},
  number       = {2},
  pages        = {411-432},
  shortjournal = {Comput. Optim. Appl.},
  title        = {Acceleration techniques for level bundle methods in weakly smooth convex constrained optimization},
  volume       = {77},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A new method based on the proximal bundle idea and gradient
sampling technique for minimizing nonsmooth convex functions.
<em>COAP</em>, <em>77</em>(2), 379–409. (<a
href="https://doi.org/10.1007/s10589-020-00213-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we combine the positive aspects of the gradient sampling (GS) and bundle methods, as the most efficient methods in nonsmooth optimization, to develop a robust method for solving unconstrained nonsmooth convex optimization problems. The main aim of the proposed method is to take advantage of both GS and bundle methods, meanwhile avoiding their drawbacks. At each iteration of this method, to find an efficient descent direction, the GS technique is utilized for constructing a local polyhedral model for the objective function. If necessary, via an iterative improvement process, this initial polyhedral model is improved by some techniques inspired by the bundle and GS methods. The convergence of the method is studied, which reveals that the global convergence property of our method is independent of the number of gradient evaluations required to establish and improve the initial polyhedral models. Thus, the presented method needs much fewer gradient evaluations in comparison to the original GS method. Furthermore, by means of numerical simulations, we show that the presented method provides promising results in comparison with GS methods, especially for large scale problems. Moreover, in contrast with some bundle methods, our method is not very sensitive to the accuracy of supplied gradients.},
  archive      = {J_COAP},
  author       = {Maleknia, M. and Shamsi, M.},
  doi          = {10.1007/s10589-020-00213-y},
  journal      = {Computational Optimization and Applications},
  number       = {2},
  pages        = {379-409},
  shortjournal = {Comput. Optim. Appl.},
  title        = {A new method based on the proximal bundle idea and gradient sampling technique for minimizing nonsmooth convex functions},
  volume       = {77},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). On the interplay between acceleration and identification for
the proximal gradient algorithm. <em>COAP</em>, <em>77</em>(2), 351–378.
(<a href="https://doi.org/10.1007/s10589-020-00218-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we study the interplay between acceleration and structure identification for the proximal gradient algorithm. While acceleration is generally beneficial in terms of functional decrease, we report and analyze several cases where its interplay with identification has negative effects on the algorithm behavior (iterates oscillation, loss of structure, etc.). Then, we present a generic method that tames acceleration when structure identification may be at stake; it benefits from a convergence rate that matches the one of the accelerated proximal gradient under some qualifying condition. We show empirically that the proposed method is much more stable in terms of subspace identification compared to the accelerated proximal gradient method while keeping a similar functional decrease.},
  archive      = {J_COAP},
  author       = {Bareilles, Gilles and Iutzeler, Franck},
  doi          = {10.1007/s10589-020-00218-7},
  journal      = {Computational Optimization and Applications},
  number       = {2},
  pages        = {351-378},
  shortjournal = {Comput. Optim. Appl.},
  title        = {On the interplay between acceleration and identification for the proximal gradient algorithm},
  volume       = {77},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). An explicit tikhonov algorithm for nested variational
inequalities. <em>COAP</em>, <em>77</em>(2), 335–350. (<a
href="https://doi.org/10.1007/s10589-020-00210-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider nested variational inequalities consisting in a (upper-level) variational inequality whose feasible set is given by the solution set of another (lower-level) variational inequality. Purely hierarchical convex bilevel optimization problems and certain multi-follower games are particular instances of nested variational inequalities. We present an explicit and ready-to-implement Tikhonov-type solution method for such problems. We give conditions that guarantee the convergence of the proposed method. Moreover, inspired by recent works in the literature, we provide a convergence rate analysis. In particular, for the simple bilevel instance, we are able to obtain enhanced convergence results.},
  archive      = {J_COAP},
  author       = {Lampariello, Lorenzo and Neumann, Christoph and Ricci, Jacopo M. and Sagratella, Simone and Stein, Oliver},
  doi          = {10.1007/s10589-020-00210-1},
  journal      = {Computational Optimization and Applications},
  number       = {2},
  pages        = {335-350},
  shortjournal = {Comput. Optim. Appl.},
  title        = {An explicit tikhonov algorithm for nested variational inequalities},
  volume       = {77},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Nonlinear optimal control: A numerical scheme based on
occupation measures and interval analysis. <em>COAP</em>,
<em>77</em>(1), 307–334. (<a
href="https://doi.org/10.1007/s10589-020-00198-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents an approximation scheme for optimal control problems using finite-dimensional linear programs and interval analysis. This is done in two parts. Following Vinter approach (SIAM J Control Optim 31(2):518–538, 1993) and using occupation measures, the optimal control problem is written into a linear programming problem of infinite-dimension (weak formulation). Thanks to Interval arithmetic, we provide a relaxation of this infinite-dimensional linear programming problem by a finite dimensional linear programming problem. A proof that the optimal value of the finite dimensional linear programming problem is a lower bound to the optimal value of the control problem is given. Moreover, according to the fineness of the discretization and the size of the chosen test function family, obtained optimal values of each finite dimensional linear programming problem form a sequence of lower bounds which converges to the optimal value of the initial optimal control problem. Examples will illustrate the principle of the methodology.},
  archive      = {J_COAP},
  author       = {Delanoue, Nicolas and Lhommeau, Mehdi and Lagrange, Sébastien},
  doi          = {10.1007/s10589-020-00198-8},
  journal      = {Computational Optimization and Applications},
  number       = {1},
  pages        = {307-334},
  shortjournal = {Comput. Optim. Appl.},
  title        = {Nonlinear optimal control: A numerical scheme based on occupation measures and interval analysis},
  volume       = {77},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A second-order shape optimization algorithm for solving the
exterior bernoulli free boundary problem using a new boundary cost
functional. <em>COAP</em>, <em>77</em>(1), 251–305. (<a
href="https://doi.org/10.1007/s10589-020-00199-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The exterior Bernoulli problem is rephrased into a shape optimization problem using a new type of objective function called the Dirichlet-data-gap cost function which measures the $$L^2$$ -distance between the Dirichlet data of two state functions. The first-order shape derivative of the cost function is explicitly determined via the chain rule approach. Using the same technique, the second-order shape derivative of the cost function at the solution of the free boundary problem is also computed. The gradient and Hessian informations are then used to formulate an efficient second-order gradient-based descent algorithm to numerically solve the minimization problem. The feasibility of the proposed method is illustrated through various numerical examples.},
  archive      = {J_COAP},
  author       = {Rabago, Julius Fergy T. and Azegami, Hideyuki},
  doi          = {10.1007/s10589-020-00199-7},
  journal      = {Computational Optimization and Applications},
  number       = {1},
  pages        = {251-305},
  shortjournal = {Comput. Optim. Appl.},
  title        = {A second-order shape optimization algorithm for solving the exterior bernoulli free boundary problem using a new boundary cost functional},
  volume       = {77},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Inverse point source location with the helmholtz equation on
a bounded domain. <em>COAP</em>, <em>77</em>(1), 213–249. (<a
href="https://doi.org/10.1007/s10589-020-00205-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The problem of recovering acoustic sources, more specifically monopoles, from point-wise measurements of the corresponding acoustic pressure at a limited number of frequencies is addressed. To this purpose, a family of sparse optimization problems in measure space in combination with the Helmholtz equation on a bounded domain is considered. A weighted norm with unbounded weight near the observation points is incorporated into the formulation. Optimality conditions and conditions for recovery in the small noise case are discussed, which motivates concrete choices of the weight. The numerical realization is based on an accelerated conditional gradient method in measure space and a finite element discretization.},
  archive      = {J_COAP},
  author       = {Pieper, Konstantin and Tang, Bao Quoc and Trautmann, Philip and Walter, Daniel},
  doi          = {10.1007/s10589-020-00205-y},
  journal      = {Computational Optimization and Applications},
  number       = {1},
  pages        = {213-249},
  shortjournal = {Comput. Optim. Appl.},
  title        = {Inverse point source location with the helmholtz equation on a bounded domain},
  volume       = {77},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Convergence rates of subgradient methods for quasi-convex
optimization problems. <em>COAP</em>, <em>77</em>(1), 183–212. (<a
href="https://doi.org/10.1007/s10589-020-00194-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Quasi-convex optimization acts a pivotal part in many fields including economics and finance; the subgradient method is an effective iterative algorithm for solving large-scale quasi-convex optimization problems. In this paper, we investigate the quantitative convergence theory, including the iteration complexity and convergence rates, of various subgradient methods for solving quasi-convex optimization problems in a unified framework. In particular, we consider a sequence satisfying a general (inexact) basic inequality, and investigate the global convergence theorem and the iteration complexity when using the constant, diminishing or dynamic stepsize rules. More importantly, we establish the linear (or sublinear) convergence rates of the sequence under an additional assumption of weak sharp minima of Hölderian order and upper bounded noise. These convergence theorems are applied to establish the iteration complexity and convergence rates of several subgradient methods, including the standard/inexact/conditional subgradient methods, for solving quasi-convex optimization problems under the assumptions of the Hölder condition and/or the weak sharp minima of Hölderian order.},
  archive      = {J_COAP},
  author       = {Hu, Yaohua and Li, Jiawen and Yu, Carisa Kwok Wai},
  doi          = {10.1007/s10589-020-00194-y},
  journal      = {Computational Optimization and Applications},
  number       = {1},
  pages        = {183-212},
  shortjournal = {Comput. Optim. Appl.},
  title        = {Convergence rates of subgradient methods for quasi-convex optimization problems},
  volume       = {77},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Make <span
class="math display"><em>ℓ</em><sub>1</sub></span> regularization
effective in training sparse CNN. <em>COAP</em>, <em>77</em>(1),
163–182. (<a href="https://doi.org/10.1007/s10589-020-00202-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Compressed Sensing using $$\ell _1$$ regularization is among the most powerful and popular sparsification technique in many applications, but why has it not been used to obtain sparse deep learning model such as convolutional neural network (CNN)? This paper is aimed to provide an answer to this question and to show how to make it work. Following Xiao (J Mach Learn Res 11(Oct):2543–2596, 2010), We first demonstrate that the commonly used stochastic gradient decent and variants training algorithm is not an appropriate match with $$\ell _1$$ regularization and then replace it with a different training algorithm based on a regularized dual averaging (RDA) method. The RDA method of Xiao (J Mach Learn Res 11(Oct):2543–2596, 2010) was originally designed specifically for convex problem, but with new theoretical insight and algorithmic modifications (using proper initialization and adaptivity), we have made it an effective match with $$\ell _1$$ regularization to achieve a state-of-the-art sparsity for the highly non-convex CNN compared to other weight pruning methods without compromising accuracy (achieving 95\% sparsity for ResNet-18 on CIFAR-10, for example).},
  archive      = {J_COAP},
  author       = {He, Juncai and Jia, Xiaodong and Xu, Jinchao and Zhang, Lian and Zhao, Liang},
  doi          = {10.1007/s10589-020-00202-1},
  journal      = {Computational Optimization and Applications},
  number       = {1},
  pages        = {163-182},
  shortjournal = {Comput. Optim. Appl.},
  title        = {Make $$\ell _1$$ regularization effective in training sparse CNN},
  volume       = {77},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). On the resolution of misspecified convex optimization and
monotone variational inequality problems. <em>COAP</em>, <em>77</em>(1),
125–161. (<a href="https://doi.org/10.1007/s10589-020-00193-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider a misspecified optimization problem, requiring the minimization of a function $$f(\cdot;\theta ^*)$$ over a closed and convex set X where $$\theta ^*$$ is an unknown vector of parameters that may be learnt by a parallel learning process. Here, we develop coupled schemes that generate iterates $$(x_k,\theta _k)$$ as $$k \rightarrow \infty$$ , then $$x_k \rightarrow x^*$$ , a minimizer of $$f(\cdot;\theta ^*)$$ over X and $$\theta _k \rightarrow \theta ^*$$ . In the first part of the paper, we consider the solution of problems where f is either smooth or nonsmooth. In smooth strongly convex regimes, we demonstrate that such schemes still display a linear rate of convergence, albeit with larger constants. When strong convexity assumptions are weakened, it can be shown that the convergence in function value sees a modification in the canonical convergence rate of $${{{\mathcal {O}}}}(1/K)$$ by an additive factor proportional to $$\Vert \theta _0-\theta ^*\Vert$$ where $$\Vert \theta _0-\theta ^*\Vert$$ represents the initial misspecification in $$\theta ^*$$ . In addition, when the learning problem is assumed to be merely convex but admits a suitable weak-sharpness property, then the convergence rate deteriorates to $${\mathcal {O}}(1/\sqrt{K})$$ . In both convex and strongly convex regimes, diminishing steplength schemes are also provided and are less reliant on the knowledge of problem parameters. Finally, we present an averaging-based subgradient scheme that displays a rate of $${\mathcal {O}}(1/\sqrt{K})+ \mathcal{O}(\|\theta_0-\theta^*\|(1/K))$$ , implying no effect on the canonical rate of $${{{\mathcal {O}}}}(1/\sqrt{K})$$ . In the second part of the paper, we consider the solution of misspecified monotone variational inequality problems, motivated by the need to contend with more general equilibrium problems as well as the possibility of misspecification in the constraints. In this context, we first present a constant steplength misspecified extragradient scheme and prove its asymptotic convergence. This scheme is reliant on problem parameters (such as Lipschitz constants) and leads to a misspecified variant of iterative Tikhonov regularization, an avenue that does not necessitate the knowledge of such constants.},
  archive      = {J_COAP},
  author       = {Ahmadi, Hesam and Shanbhag, Uday V.},
  doi          = {10.1007/s10589-020-00193-z},
  journal      = {Computational Optimization and Applications},
  number       = {1},
  pages        = {125-161},
  shortjournal = {Comput. Optim. Appl.},
  title        = {On the resolution of misspecified convex optimization and monotone variational inequality problems},
  volume       = {77},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Convergence study of indefinite proximal ADMM with a
relaxation factor. <em>COAP</em>, <em>77</em>(1), 91–123. (<a
href="https://doi.org/10.1007/s10589-020-00206-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The alternating direction method of multipliers (ADMM) is widely used to solve separable convex programming problems. At each iteration, the classical ADMM solves the subproblems exactly. For many problems arising from practical applications, it is usually impossible or too expensive to obtain the exact solution of a subproblem. To overcome this, a special proximal term is added to ease the solvability of a subproblem. In the literature, the proximal term can be relaxed to be indefinite while still with a convergence guarantee; this relaxation permits the adoption of larger step sizes to solve the subproblem, which particularly accelerates its performance. A large value of the relaxation factor introduced in the dual step of ADMM also plays a vital role in accelerating its performance. However, it is still not clear whether these two acceleration strategies can be used simultaneously with no restriction on the penalty parameter. In this paper, we answer this question affirmatively and conduct a rigorous convergence analysis for indefinite proximal ADMM with a relaxation factor (IP-ADMM $$_{r}$$ ), reveal the relationships between the parameter in the indefinite proximal term and the relaxation factor to ensure its global convergence, and establish the worst-case convergence rate in the ergodic sense. Finally, some numerical results on basis pursuit and total variation-based denoising with box constraint problems are presented to verify the efficiency of IP-ADMM $$_{r}$$ .},
  archive      = {J_COAP},
  author       = {Tao, Min},
  doi          = {10.1007/s10589-020-00206-x},
  journal      = {Computational Optimization and Applications},
  number       = {1},
  pages        = {91-123},
  shortjournal = {Comput. Optim. Appl.},
  title        = {Convergence study of indefinite proximal ADMM with a relaxation factor},
  volume       = {77},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). An active-set algorithmic framework for non-convex
optimization problems over the simplex. <em>COAP</em>, <em>77</em>(1),
57–89. (<a href="https://doi.org/10.1007/s10589-020-00195-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we describe a new active-set algorithmic framework for minimizing a non-convex function over the unit simplex. At each iteration, the method makes use of a rule for identifying active variables (i.e., variables that are zero at a stationary point) and specific directions (that we name active-set gradient related directions) satisfying a new “nonorthogonality” type of condition. We prove global convergence to stationary points when using an Armijo line search in the given framework. We further describe three different examples of active-set gradient related directions that guarantee linear convergence rate (under suitable assumptions). Finally, we report numerical experiments showing the effectiveness of the approach.},
  archive      = {J_COAP},
  author       = {Cristofari, Andrea and De Santis, Marianna and Lucidi, Stefano and Rinaldi, Francesco},
  doi          = {10.1007/s10589-020-00195-x},
  journal      = {Computational Optimization and Applications},
  number       = {1},
  pages        = {57-89},
  shortjournal = {Comput. Optim. Appl.},
  title        = {An active-set algorithmic framework for non-convex optimization problems over the simplex},
  volume       = {77},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). An augmented lagrangian algorithm for multi-objective
optimization. <em>COAP</em>, <em>77</em>(1), 29–56. (<a
href="https://doi.org/10.1007/s10589-020-00204-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose an adaptation of the classical augmented Lagrangian method for dealing with multi-objective optimization problems. Specifically, after a brief review of the literature, we give a suitable definition of Augmented Lagrangian for equality and inequality constrained multi-objective problems. We exploit this object in a general computational scheme that is proved to converge, under mild assumptions, to weak Pareto points of such problems. We then provide a modified version of the algorithm which is more suited for practical implementations, proving again convergence properties under reasonable hypotheses. Finally, computational experiments show that the proposed methods not only do work in practice, but are also competitive with respect to state-of-the-art methods.},
  archive      = {J_COAP},
  author       = {Cocchi, G. and Lapucci, M.},
  doi          = {10.1007/s10589-020-00204-z},
  journal      = {Computational Optimization and Applications},
  number       = {1},
  pages        = {29-56},
  shortjournal = {Comput. Optim. Appl.},
  title        = {An augmented lagrangian algorithm for multi-objective optimization},
  volume       = {77},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). On the convergence of steepest descent methods for
multiobjective optimization. <em>COAP</em>, <em>77</em>(1), 1–27. (<a
href="https://doi.org/10.1007/s10589-020-00192-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper we consider the classical unconstrained nonlinear multiobjective optimization problem. For such a problem, it is particularly interesting to compute as many points as possible in an effort to approximate the so-called Pareto front. Consequently, to solve the problem we define an “a posteriori” algorithm whose generic iterate is represented by a set of points rather than by a single one. The proposed algorithm takes advantage of a linesearch with extrapolation along steepest descent directions with respect to (possibly not all of) the objective functions. The sequence of sets of points produced by the algorithm defines a set of “linked” sequences of points. We show that each linked sequence admits at least one limit point (not necessarily distinct from those obtained by other sequences) and that every limit point is Pareto-stationary. We also report numerical results on a collection of multiobjective problems that show efficiency of the proposed approach over more classical ones.},
  archive      = {J_COAP},
  author       = {Cocchi, G. and Liuzzi, G. and Lucidi, S. and Sciandrone, M.},
  doi          = {10.1007/s10589-020-00192-0},
  journal      = {Computational Optimization and Applications},
  number       = {1},
  pages        = {1-27},
  shortjournal = {Comput. Optim. Appl.},
  title        = {On the convergence of steepest descent methods for multiobjective optimization},
  volume       = {77},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). On the complexity of a hybrid proximal extragradient
projective method for solving monotone inclusion problems.
<em>COAP</em>, <em>76</em>(3), 991–1019. (<a
href="https://doi.org/10.1007/s10589-020-00200-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In a series of papers (Solodov and Svaiter in J Convex Anal 6(1):59–70, 1999; Set-Valued Anal 7(4):323–345, 1999; Numer Funct Anal Optim 22(7–8):1013–1035, 2001) Solodov and Svaiter introduced new inexact variants of the proximal point method with relative error tolerances. Point-wise and ergodic iteration-complexity bounds for one of these methods, namely the hybrid proximal extragradient method (1999) were established by Monteiro and Svaiter (SIAM J Optim 20(6):2755–2787, 2010). Here, we extend these results to a more general framework, by establishing point-wise and ergodic iteration-complexity bounds for the inexact proximal point method studied by Solodov and Svaiter (2001). Using this framework we derive global convergence results and iteration-complexity bounds for a family of projective splitting methods for solving monotone inclusion problems, which generalize the projective splitting methods introduced and studied by Eckstein and Svaiter (SIAM J Control Optim 48(2):787–811, 2009).},
  archive      = {J_COAP},
  author       = {Sicre, Mauricio Romero},
  doi          = {10.1007/s10589-020-00200-3},
  journal      = {Computational Optimization and Applications},
  number       = {3},
  pages        = {991-1019},
  shortjournal = {Comput. Optim. Appl.},
  title        = {On the complexity of a hybrid proximal extragradient projective method for solving monotone inclusion problems},
  volume       = {76},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A regularization method for constrained nonlinear least
squares. <em>COAP</em>, <em>76</em>(3), 961–989. (<a
href="https://doi.org/10.1007/s10589-020-00201-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a regularization method for nonlinear least-squares problems with equality constraints. Our approach is modeled after those of Arreckx and Orban (SIAM J Optim 28(2):1613–1639, 2018. https://doi.org/10.1137/16M1088570) and Dehghani et al. (INFOR Inf Syst Oper Res, 2019. https://doi.org/10.1080/03155986.2018.1559428) and applies a selective regularization scheme that may be viewed as a reformulation of an augmented Lagrangian. Our formulation avoids the occurrence of the operator $$A(x)^T A(x)$$, where A is the Jacobian of the nonlinear residual, which typically contributes to the density and ill conditioning of subproblems. Under boundedness of the derivatives, we establish global convergence to a KKT point or a stationary point of an infeasibility measure. If second derivatives are Lipschitz continuous and a second-order sufficient condition is satisfied, we establish superlinear convergence without requiring a constraint qualification to hold. The convergence rate is determined by a Dennis–Moré-type condition. We describe our implementation in the Julia language, which supports multiple floating-point systems. We illustrate a simple progressive scheme to obtain solutions in quadruple precision. Because our approach is similar to applying an SQP method with an exact merit function on a related problem, we show that our implementation compares favorably to IPOPT in IEEE double precision.},
  archive      = {J_COAP},
  author       = {Orban, Dominique and Siqueira, Abel Soares},
  doi          = {10.1007/s10589-020-00201-2},
  journal      = {Computational Optimization and Applications},
  number       = {3},
  pages        = {961-989},
  shortjournal = {Comput. Optim. Appl.},
  title        = {A regularization method for constrained nonlinear least squares},
  volume       = {76},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A class of benders decomposition methods for variational
inequalities. <em>COAP</em>, <em>76</em>(3), 935–959. (<a
href="https://doi.org/10.1007/s10589-019-00157-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We develop new variants of Benders decomposition methods for variational inequality problems. The construction is done by applying the general class of Dantzig–Wolfe decomposition of Luna et al. (Math Program 143(1–2):177–209, 2014) to an appropriately defined dual of the given variational inequality, and then passing back to the primal space. As compared to previous decomposition techniques of the Benders kind for variational inequalities, the following improvements are obtained. Instead of rather specific single-valued monotone mappings, the framework includes a rather broad class of multi-valued maximally monotone ones, and single-valued nonmonotone. Subproblems’ solvability is guaranteed instead of assumed, and approximations of the subproblems’ mapping are allowed (which may lead, in particular, to further decomposition of subproblems, which may otherwise be not possible). In addition, with a certain suitably chosen approximation, variational inequality subproblems become simple bound-constrained optimization problems, thus easier to solve.},
  archive      = {J_COAP},
  author       = {Luna, Juan Pablo and Sagastizábal, Claudia and Solodov, Mikhail},
  doi          = {10.1007/s10589-019-00157-y},
  journal      = {Computational Optimization and Applications},
  number       = {3},
  pages        = {935-959},
  shortjournal = {Comput. Optim. Appl.},
  title        = {A class of benders decomposition methods for variational inequalities},
  volume       = {76},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A note on solving nonlinear optimization problems in
variable precision. <em>COAP</em>, <em>76</em>(3), 917–933. (<a
href="https://doi.org/10.1007/s10589-020-00190-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This short note considers an efficient variant of the trust-region algorithm with dynamic accuracy proposed by Carter (SIAM J Sci Stat Comput 14(2):368–388, 1993) and by Conn et al. (Trust-region methods. MPS-SIAM series on optimization, SIAM, Philadelphia, 2000) as a tool for very high-performance computing, an area where it is critical to allow multi-precision computations for keeping the energy dissipation under control. Numerical experiments are presented indicating that the use of the considered method can bring substantial savings in objective function’s and gradient’s evaluation “energy costs” by efficiently exploiting multi-precision computations.},
  archive      = {J_COAP},
  author       = {Gratton, S. and Toint, Ph. L.},
  doi          = {10.1007/s10589-020-00190-2},
  journal      = {Computational Optimization and Applications},
  number       = {3},
  pages        = {917-933},
  shortjournal = {Comput. Optim. Appl.},
  title        = {A note on solving nonlinear optimization problems in variable precision},
  volume       = {76},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). On the extension of the hager–zhang conjugate gradient
method for vector optimization. <em>COAP</em>, <em>76</em>(3), 889–916.
(<a href="https://doi.org/10.1007/s10589-019-00146-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The extension of the Hager–Zhang (HZ) nonlinear conjugate gradient method for vector optimization is discussed in the present research. In the scalar minimization case, this method generates descent directions whenever, for example, the line search satisfies the standard Wolfe conditions. We first show that, in general, the direct extension of the HZ method for vector optimization does not yield descent (in the vector sense) even when an exact line search is employed. By using a sufficiently accurate line search, we then propose a self-adjusting HZ method which possesses the descent property. The proposed HZ method with suitable parameters reduces to the classical one in the scalar minimization case. Global convergence of the new scheme is proved without regular restarts and any convex assumption. Finally, numerical experiments illustrating the practical behavior of the approach are presented, and comparisons with the Hestenes–Stiefel conjugate gradient and the steepest descent methods are discussed.},
  archive      = {J_COAP},
  author       = {Gonçalves, M. L. N. and Prudente, L. F.},
  doi          = {10.1007/s10589-019-00146-1},
  journal      = {Computational Optimization and Applications},
  number       = {3},
  pages        = {889-916},
  shortjournal = {Comput. Optim. Appl.},
  title        = {On the extension of the Hager–Zhang conjugate gradient method for vector optimization},
  volume       = {76},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Non-monotone inexact restoration method for nonlinear
programming. <em>COAP</em>, <em>76</em>(3), 867–888. (<a
href="https://doi.org/10.1007/s10589-019-00129-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper deals with a new variant of the inexact restoration method of Fischer and Friedlander (Comput Optim Appl 46:333–346, 2010) for nonlinear programming. We propose an algorithm that replaces the monotone line search performed in the tangent phase by a non-monotone one, using the sharp Lagrangian as merit function. Convergence to feasible points satisfying the convex approximate gradient projection condition is proved under mild assumptions. Numerical results on representative test problems show that the proposed approach outperforms the monotone version when a suitable non-monotone parameter is chosen and is also competitive against other globalization strategies for inexact restoration.},
  archive      = {J_COAP},
  author       = {Francisco, Juliano B. and Gonçalves, Douglas S. and Bazán, Fermín S. V. and Paredes, Lila L. T.},
  doi          = {10.1007/s10589-019-00129-2},
  journal      = {Computational Optimization and Applications},
  number       = {3},
  pages        = {867-888},
  shortjournal = {Comput. Optim. Appl.},
  title        = {Non-monotone inexact restoration method for nonlinear programming},
  volume       = {76},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Applying the pattern search implicit filtering algorithm for
solving a noisy problem of parameter identification. <em>COAP</em>,
<em>76</em>(3), 835–866. (<a
href="https://doi.org/10.1007/s10589-020-00182-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Our contribution in this paper is twofold. First, the global convergence analysis of the recently proposed pattern search implicit filtering algorithm (PSIFA), aimed at linearly constrained noisy minimization problems, is revisited to address more general locally Lipschitz objective functions corrupted by noise. Second, PSIFA is applied for solving the damped harmonic oscillator parameter identification problem. This problem can be formulated as a linearly constrained optimization problem, for which the constraints are related to the features of the damping. Such a formulation rests upon a very expensive objective function whose evaluation comprises the numerical solution of an ordinary differential equation (ODE), with intrinsic numerical noise. Computational experimentation encompasses distinct choices for the ODE solvers, and a comparative analysis of the most effective options against the pattern search and the implicit filtering algorithms.},
  archive      = {J_COAP},
  author       = {Diniz-Ehrhardt, M. A. and Ferreira, D. G. and Santos, S. A.},
  doi          = {10.1007/s10589-020-00182-2},
  journal      = {Computational Optimization and Applications},
  number       = {3},
  pages        = {835-866},
  shortjournal = {Comput. Optim. Appl.},
  title        = {Applying the pattern search implicit filtering algorithm for solving a noisy problem of parameter identification},
  volume       = {76},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A penalty-free method with superlinear convergence for
equality constrained optimization. <em>COAP</em>, <em>76</em>(3),
801–833. (<a href="https://doi.org/10.1007/s10589-019-00117-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose a new penalty-free method for solving nonlinear equality constrained optimization. This method uses different trust regions to cope with the nonlinearity of the objective function and the constraints instead of using a penalty function or a filter. To avoid Maratos effect, we do not make use of the second order correction or the nonmonotone technique, but utilize the value of the Lagrangian function instead of the objective function in the acceptance criterion of the trial step. The feasibility restoration phase is not necessary, which is often used in filter methods or some other penalty-free methods. Global and superlinear convergence are established for the method under standard assumptions. Preliminary numerical results are reported, which demonstrate the usefulness of the proposed method.},
  archive      = {J_COAP},
  author       = {Chen, Zhongwen and Dai, Yu-Hong and Liu, Jiangyan},
  doi          = {10.1007/s10589-019-00117-6},
  journal      = {Computational Optimization and Applications},
  number       = {3},
  pages        = {801-833},
  shortjournal = {Comput. Optim. Appl.},
  title        = {A penalty-free method with superlinear convergence for equality constrained optimization},
  volume       = {76},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Towards an efficient augmented lagrangian method for convex
quadratic programming. <em>COAP</em>, <em>76</em>(3), 767–800. (<a
href="https://doi.org/10.1007/s10589-019-00161-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Interior point methods have attracted most of the attention in the recent decades for solving large scale convex quadratic programming problems. In this paper we take a different route as we present an augmented Lagrangian method for convex quadratic programming based on recent developments for nonlinear programming. In our approach, box constraints are penalized while equality constraints are kept within the subproblems. The motivation for this approach is that Newton’s method can be efficient for minimizing a piecewise quadratic function. Moreover, since augmented Lagrangian methods do not rely on proximity to the central path, some of the inherent difficulties in interior point methods can be avoided. In addition, a good starting point can be easily exploited, which can be relevant for solving subproblems arising from sequential quadratic programming, in sensitivity analysis and in branch and bound techniques. We prove well-definedness and finite convergence of the method proposed. Numerical experiments on separable strictly convex quadratic problems formulated from the Netlib collection show that our method can be competitive with interior point methods, in particular when a good initial point is available and a second-order Lagrange multiplier update is used.},
  archive      = {J_COAP},
  author       = {Bueno, Luís Felipe and Haeser, Gabriel and Santos, Luiz-Rafael},
  doi          = {10.1007/s10589-019-00161-2},
  journal      = {Computational Optimization and Applications},
  number       = {3},
  pages        = {767-800},
  shortjournal = {Comput. Optim. Appl.},
  title        = {Towards an efficient augmented lagrangian method for convex quadratic programming},
  volume       = {76},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). An augmented lagrangian method for quasi-equilibrium
problems. <em>COAP</em>, <em>76</em>(3), 737–766. (<a
href="https://doi.org/10.1007/s10589-020-00180-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose an Augmented Lagrangian algorithm for solving a general class of possible non-convex problems called quasi-equilibrium problems (QEPs). We define an Augmented Lagrangian bifunction associated with QEPs, introduce a secondary QEP as a measure of infeasibility and we discuss several special classes of QEPs within our theoretical framework. For obtaining global convergence under a new weak constraint qualification, we extend the notion of an Approximate Karush–Kuhn–Tucker (AKKT) point for QEPs (AKKT-QEP), showing that in general it is not necessarily satisfied at a solution, differently from its counterpart in optimization. We study some particular cases where AKKT-QEP does hold at a solution, while discussing the solvability of the subproblems of the algorithm. We also present illustrative numerical experiments.},
  archive      = {J_COAP},
  author       = {Bueno, L. F. and Haeser, G. and Lara, F. and Rojas, F. N.},
  doi          = {10.1007/s10589-020-00180-4},
  journal      = {Computational Optimization and Applications},
  number       = {3},
  pages        = {737-766},
  shortjournal = {Comput. Optim. Appl.},
  title        = {An augmented lagrangian method for quasi-equilibrium problems},
  volume       = {76},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Inexact restoration with subsampled trust-region methods for
finite-sum minimization. <em>COAP</em>, <em>76</em>(3), 701–736. (<a
href="https://doi.org/10.1007/s10589-020-00196-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Convex and nonconvex finite-sum minimization arises in many scientific computing and machine learning applications. Recently, first-order and second-order methods where objective functions, gradients and Hessians are approximated by randomly sampling components of the sum have received great attention. We propose a new trust-region method which employs suitable approximations of the objective function, gradient and Hessian built via random subsampling techniques. The choice of the sample size is deterministic and ruled by the inexact restoration approach. We discuss local and global properties for finding approximate first- and second-order optimal points and function evaluation complexity results. Numerical experience shows that the new procedure is more efficient, in terms of overall computational cost, than the standard trust-region scheme with subsampled Hessians.},
  archive      = {J_COAP},
  author       = {Bellavia, Stefania and Krejić, Nataša and Morini, Benedetta},
  doi          = {10.1007/s10589-020-00196-w},
  journal      = {Computational Optimization and Applications},
  number       = {3},
  pages        = {701-736},
  shortjournal = {Comput. Optim. Appl.},
  title        = {Inexact restoration with subsampled trust-region methods for finite-sum minimization},
  volume       = {76},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). The block-wise circumcentered–reflection method.
<em>COAP</em>, <em>76</em>(3), 675–699. (<a
href="https://doi.org/10.1007/s10589-019-00155-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The elementary Euclidean concept of circumcenter has recently been employed to improve two aspects of the classical Douglas–Rachford method for projecting onto the intersection of affine subspaces. The so-called circumcentered–reflection method is able to both accelerate the average reflection scheme by the Douglas–Rachford method and cope with the intersection of more than two affine subspaces. We now introduce the technique of circumcentering in blocks, which, more than just an option over the basic algorithm of circumcenters, turns out to be an elegant manner of generalizing the method of alternating projections. Linear convergence for this novel block-wise circumcenter framework is derived and illustrated numerically. Furthermore, we prove that the original circumcentered–reflection method essentially finds the best approximation solution in one single step if the given affine subspaces are hyperplanes.},
  archive      = {J_COAP},
  author       = {Behling, Roger and Bello-Cruz, J.-Yunier and Santos, Luiz-Rafael},
  doi          = {10.1007/s10589-019-00155-0},
  journal      = {Computational Optimization and Applications},
  number       = {3},
  pages        = {675-699},
  shortjournal = {Comput. Optim. Appl.},
  title        = {The block-wise circumcentered–reflection method},
  volume       = {76},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A modified proximal point method for DC functions on
hadamard manifolds. <em>COAP</em>, <em>76</em>(3), 649–673. (<a
href="https://doi.org/10.1007/s10589-020-00173-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study the convergence of a modified proximal point method for DC functions in Hadamard manifolds. We use the iteration computed by the proximal point method for DC function extended to the Riemannian context by Souza and Oliveira (J Glob Optim 63:797–810, 2015) to define a descent direction which improves the convergence of the method. Our method also accelerates the classical proximal point method for convex functions. We illustrate our results with some numerical experiments.},
  archive      = {J_COAP},
  author       = {Almeida, Yldenilson Torres and da Cruz Neto, João Xavier and Oliveira, Paulo Roberto and Souza, João Carlos de Oliveira},
  doi          = {10.1007/s10589-020-00173-3},
  journal      = {Computational Optimization and Applications},
  number       = {3},
  pages        = {649-673},
  shortjournal = {Comput. Optim. Appl.},
  title        = {A modified proximal point method for DC functions on hadamard manifolds},
  volume       = {76},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). An inexact proximal generalized alternating direction method
of multipliers. <em>COAP</em>, <em>76</em>(3), 621–647. (<a
href="https://doi.org/10.1007/s10589-020-00191-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes and analyzes an inexact variant of the proximal generalized alternating direction method of multipliers (ADMM) for solving separable linearly constrained convex optimization problems. In this variant, the first subproblem is approximately solved using a relative error condition whereas the second one is assumed to be easy to solve. In many ADMM applications, one of the subproblems has a closed-form solution; for instance, $$\ell _1$$ regularized convex composite optimization problems. The proposed method possesses iteration-complexity bounds similar to its exact version. More specifically, it is shown that, for a given tolerance $$\rho &gt;0$$ , an approximate solution of the Lagrangian system associated to the problem under consideration is obtained in at most $$\mathcal {O}(1/\rho ^2)$$ (resp. $$\mathcal {O}(1/\rho )$$ in the ergodic case) iterations. Numerical experiments are presented to illustrate the performance of the proposed scheme.},
  archive      = {J_COAP},
  author       = {Adona, V. A. and Gonçalves, M. L. N. and Melo, J. G.},
  doi          = {10.1007/s10589-020-00191-1},
  journal      = {Computational Optimization and Applications},
  number       = {3},
  pages        = {621-647},
  shortjournal = {Comput. Optim. Appl.},
  title        = {An inexact proximal generalized alternating direction method of multipliers},
  volume       = {76},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Preface of the special issue dedicated to the XII brazilian
workshop on continuous optimization. <em>COAP</em>, <em>76</em>(3),
615–619. (<a href="https://doi.org/10.1007/s10589-020-00203-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_COAP},
  author       = {Birgin, Ernesto G.},
  doi          = {10.1007/s10589-020-00203-0},
  journal      = {Computational Optimization and Applications},
  number       = {3},
  pages        = {615-619},
  shortjournal = {Comput. Optim. Appl.},
  title        = {Preface of the special issue dedicated to the XII brazilian workshop on continuous optimization},
  volume       = {76},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A computationally useful algebraic representation of
nonlinear disjunctive convex sets using the perspective function.
<em>COAP</em>, <em>76</em>(2), 589–614. (<a
href="https://doi.org/10.1007/s10589-020-00176-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nonlinear disjunctive convex sets arise naturally in the formulation or solution methods of many discrete–continuous optimization problems. Often, a tight algebraic representation of the disjunctive convex set is sought, with the tightest such representation involving the characterization of the convex hull of the disjunctive convex set. In the most general case, this can be explicitly expressed through the use of the perspective function in higher dimensional space—the so-called extended formulation of the convex hull of a disjunctive convex set. However, there are a number of challenges in using this characterization in computation which prevents its wide-spread use, including issues that arise because of the functional form of the perspective function. In this paper, we propose an explicit algebraic representation of a fairly large class of nonlinear disjunctive convex sets using the perspective function that addresses this latter computational challenge. This explicit representation can be used to generate (tighter) algebraic reformulations for a variety of different problems containing disjunctive convex sets, and we report illustrative computational results using this representation for several nonlinear disjunctive problems.},
  archive      = {J_COAP},
  author       = {Furman, Kevin C. and Sawaya, Nicolas W. and Grossmann, Ignacio E.},
  doi          = {10.1007/s10589-020-00176-0},
  journal      = {Computational Optimization and Applications},
  number       = {2},
  pages        = {589-614},
  shortjournal = {Comput. Optim. Appl.},
  title        = {A computationally useful algebraic representation of nonlinear disjunctive convex sets using the perspective function},
  volume       = {76},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A differentiable path-following algorithm for computing
perfect stationary points. <em>COAP</em>, <em>76</em>(2), 571–588. (<a
href="https://doi.org/10.1007/s10589-020-00181-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper is concerned with the computation of perfect stationary point, which is a strict refinement of stationary point. A differentiable homotopy method is developed for finding perfect stationary points of continuous functions on convex polytopes. We constitute an artificial problem by introducing a continuously differentiable function of an extra variable. With the optimality conditions of this problem and a fixed point argument, a differentiable homotopy mapping is constructed. As the extra variable becomes close to zero, the homotopy path naturally provides a sequence of closely approximate stationary points on perturbed polytopes, and converges to a perfect stationary point on the original polytope. Numerical experiments are implemented to further illustrate the effectiveness of our method.},
  archive      = {J_COAP},
  author       = {Zhan, Yang and Li, Peixuan and Dang, Chuangyin},
  doi          = {10.1007/s10589-020-00181-3},
  journal      = {Computational Optimization and Applications},
  number       = {2},
  pages        = {571-588},
  shortjournal = {Comput. Optim. Appl.},
  title        = {A differentiable path-following algorithm for computing perfect stationary points},
  volume       = {76},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Feasibility and a fast algorithm for euclidean distance
matrix optimization with ordinal constraints. <em>COAP</em>,
<em>76</em>(2), 535–569. (<a
href="https://doi.org/10.1007/s10589-020-00189-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Euclidean distance matrix optimization with ordinal constraints (EDMOC) has found important applications in sensor network localization and molecular conformation. It can also be viewed as a matrix formulation of multidimensional scaling, which is to embed n points in a r-dimensional space such that the resulting distances follow the ordinal constraints. The ordinal constraints, though proved to be quite useful, may result in only zero solution when too many are added, leaving the feasibility of EDMOC as a question. In this paper, we first study the feasibility of EDMOC systematically. We show that if $$r\ge n-2$$, EDMOC always admits a nontrivial solution. Otherwise, it may have only zero solution. The latter interprets the numerical observations of ’crowding phenomenon’. Next we overcome two obstacles in designing fast algorithms for EDMOC, i.e., the low-rankness and the potential huge number of ordinal constraints. We apply the technique developed in Zhou et al. (IEEE Trans Signal Process 66(3):4331–4346 2018) to take the low rank constraint as the conditional positive semidefinite cone with rank cut. This leads to a majorization penalty approach. The ordinal constraints are left to the subproblem, which is exactly the weighted isotonic regression, and can be solved by the enhanced implementation of Pool Adjacent Violators Algorithm (PAVA). Extensive numerical results demonstrate the superior performance of the proposed approach over some state-of-the-art solvers.},
  archive      = {J_COAP},
  author       = {Lu, Si-Tong and Zhang, Miao and Li, Qing-Na},
  doi          = {10.1007/s10589-020-00189-9},
  journal      = {Computational Optimization and Applications},
  number       = {2},
  pages        = {535-569},
  shortjournal = {Comput. Optim. Appl.},
  title        = {Feasibility and a fast algorithm for euclidean distance matrix optimization with ordinal constraints},
  volume       = {76},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). The pontryagin maximum principle for solving fokker–planck
optimal control problems. <em>COAP</em>, <em>76</em>(2), 499–533. (<a
href="https://doi.org/10.1007/s10589-020-00187-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The characterization and numerical solution of two non-smooth optimal control problems governed by a Fokker–Planck (FP) equation are investigated in the framework of the Pontryagin maximum principle (PMP). The two FP control problems are related to the problem of determining open- and closed-loop controls for a stochastic process whose probability density function is modelled by the FP equation. In both cases, existence and PMP characterisation of optimal controls are proved, and PMP-based numerical optimization schemes are implemented that solve the PMP optimality conditions to determine the controls sought. Results of experiments are presented that successfully validate the proposed computational framework and allow to compare the two control strategies.},
  archive      = {J_COAP},
  author       = {Breitenbach, Tim and Borzì, Alfio},
  doi          = {10.1007/s10589-020-00187-x},
  journal      = {Computational Optimization and Applications},
  number       = {2},
  pages        = {499-533},
  shortjournal = {Comput. Optim. Appl.},
  title        = {The pontryagin maximum principle for solving Fokker–Planck optimal control problems},
  volume       = {76},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Algorithms for stochastic optimization with function or
expectation constraints. <em>COAP</em>, <em>76</em>(2), 461–498. (<a
href="https://doi.org/10.1007/s10589-020-00179-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper considers the problem of minimizing an expectation function over a closed convex set, coupled with a function or expectation constraint on either decision variables or problem parameters. We first present a new stochastic approximation (SA) type algorithm, namely the cooperative SA (CSA), to handle problems with the constraint on devision variables. We show that this algorithm exhibits the optimal $${{{\mathcal {O}}}}(1/\epsilon ^2)$$ rate of convergence, in terms of both optimality gap and constraint violation, when the objective and constraint functions are generally convex, where $$\epsilon$$ denotes the optimality gap and infeasibility. Moreover, we show that this rate of convergence can be improved to $${{{\mathcal {O}}}}(1/\epsilon )$$ if the objective and constraint functions are strongly convex. We then present a variant of CSA, namely the cooperative stochastic parameter approximation (CSPA) algorithm, to deal with the situation when the constraint is defined over problem parameters and show that it exhibits similar optimal rate of convergence to CSA. It is worth noting that CSA and CSPA are primal methods which do not require the iterations on the dual space and/or the estimation on the size of the dual variables. To the best of our knowledge, this is the first time that such optimal SA methods for solving function or expectation constrained stochastic optimization are presented in the literature.},
  archive      = {J_COAP},
  author       = {Lan, Guanghui and Zhou, Zhiqiang},
  doi          = {10.1007/s10589-020-00179-x},
  journal      = {Computational Optimization and Applications},
  number       = {2},
  pages        = {461-498},
  shortjournal = {Comput. Optim. Appl.},
  title        = {Algorithms for stochastic optimization with function or expectation constraints},
  volume       = {76},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Quantitative analysis for a class of two-stage stochastic
linear variational inequality problems. <em>COAP</em>, <em>76</em>(2),
431–460. (<a href="https://doi.org/10.1007/s10589-020-00185-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper considers a class of two-stage stochastic linear variational inequality problems whose first stage problems are stochastic linear box-constrained variational inequality problems and second stage problems are stochastic linear complementary problems having a unique solution. We first give conditions for the existence of solutions to both the original problem and its perturbed problems. Next we derive quantitative stability assertions of this two-stage stochastic problem under total variation metrics via the corresponding residual function. Moreover, we study the discrete approximation problem. The convergence and the exponential rate of convergence of optimal solution sets are obtained under moderate assumptions respectively. Finally, through solving a non-cooperative game in which each player’s problem is a parameterized two-stage stochastic program, we numerically illustrate our theoretical results.},
  archive      = {J_COAP},
  author       = {Jiang, Jie and Chen, Xiaojun and Chen, Zhiping},
  doi          = {10.1007/s10589-020-00185-z},
  journal      = {Computational Optimization and Applications},
  number       = {2},
  pages        = {431-460},
  shortjournal = {Comput. Optim. Appl.},
  title        = {Quantitative analysis for a class of two-stage stochastic linear variational inequality problems},
  volume       = {76},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Inexact first-order primal–dual algorithms. <em>COAP</em>,
<em>76</em>(2), 381–430. (<a
href="https://doi.org/10.1007/s10589-020-00186-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We investigate the convergence of a recently popular class of first-order primal–dual algorithms for saddle point problems under the presence of errors in the proximal maps and gradients. We study several types of errors and show that, provided a sufficient decay of these errors, the same convergence rates as for the error-free algorithm can be established. More precisely, we prove the (optimal) $$O\left( 1/N\right)$$ convergence to a saddle point in finite dimensions for the class of non-smooth problems considered in this paper, and prove a $$O\left( 1/N^2\right)$$ or even linear $$O\left( \theta ^N\right)$$ convergence rate if either the primal or dual objective respectively both are strongly convex. Moreover we show that also under a slower decay of errors we can establish rates, however slower and directly depending on the decay of the errors. We demonstrate the performance and practical use of the algorithms on the example of nested algorithms and show how they can be used to split the global objective more efficiently.},
  archive      = {J_COAP},
  author       = {Rasch, Julian and Chambolle, Antonin},
  doi          = {10.1007/s10589-020-00186-y},
  journal      = {Computational Optimization and Applications},
  number       = {2},
  pages        = {381-430},
  shortjournal = {Comput. Optim. Appl.},
  title        = {Inexact first-order primal–dual algorithms},
  volume       = {76},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Accelerating incremental gradient optimization with
curvature information. <em>COAP</em>, <em>76</em>(2), 347–380. (<a
href="https://doi.org/10.1007/s10589-020-00183-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper studies an acceleration technique for incremental aggregated gradient (IAG) method through the use of curvature information for solving strongly convex finite sum optimization problems. These optimization problems of interest arise in large-scale learning applications. Our technique utilizes a curvature-aided gradient tracking step to produce accurate gradient estimates incrementally using Hessian information. We propose and analyze two methods utilizing the new technique, the curvature-aided IAG (CIAG) method and the accelerated CIAG (A-CIAG) method, which are analogous to gradient method and Nesterov’s accelerated gradient method, respectively. Setting $$\kappa$$ to be the condition number of the objective function, we prove the R linear convergence rates of $$1 - \frac{4c_0 \kappa }{(\kappa +1)^2}$$ for the CIAG method, and $$1 - \sqrt{\frac{c_1}{2\kappa }}$$ for the A-CIAG method, where $$c_0,c_1 \le 1$$ are constants inversely proportional to the distance between the initial point and the optimal solution. When the initial iterate is close to the optimal solution, the R linear convergence rates match with the gradient and accelerated gradient method, albeit CIAG and A-CIAG operate in an incremental setting with strictly lower computation complexity. Numerical experiments confirm our findings. The source codes used for this paper can be found on http://github.com/hoitowai/ciag/.},
  archive      = {J_COAP},
  author       = {Wai, Hoi-To and Shi, Wei and Uribe, César A. and Nedić, Angelia and Scaglione, Anna},
  doi          = {10.1007/s10589-020-00183-1},
  journal      = {Computational Optimization and Applications},
  number       = {2},
  pages        = {347-380},
  shortjournal = {Comput. Optim. Appl.},
  title        = {Accelerating incremental gradient optimization with curvature information},
  volume       = {76},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). An efficient adaptive accelerated inexact proximal point
method for solving linearly constrained nonconvex composite problems.
<em>COAP</em>, <em>76</em>(2), 305–346. (<a
href="https://doi.org/10.1007/s10589-020-00188-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes an efficient adaptive variant of a quadratic penalty accelerated inexact proximal point (QP-AIPP) method proposed earlier by the authors. Both the QP-AIPP method and its variant solve linearly set constrained nonconvex composite optimization problems using a quadratic penalty approach where the generated penalized subproblems are solved by a variant of the underlying AIPP method. The variant, in turn, solves a given penalized subproblem by generating a sequence of proximal subproblems which are then solved by an accelerated composite gradient algorithm. The main difference between AIPP and its variant is that the proximal subproblems in the former are always convex while the ones in the latter are not necessarily convex due to the fact that their prox parameters are chosen as aggressively as possible so as to improve efficiency. The possibly nonconvex proximal subproblems generated by the AIPP variant are also tentatively solved by a novel adaptive accelerated composite gradient algorithm based on the validity of some key convergence inequalities. As a result, the variant generates a sequence of proximal subproblems where the stepsizes are adaptively changed according to the responses obtained from the calls to the accelerated composite gradient algorithm. Finally, numerical results are given to demonstrate the efficiency of the proposed AIPP and QP-AIPP variants.},
  archive      = {J_COAP},
  author       = {Kong, Weiwei and Melo, Jefferson G. and Monteiro, Renato D. C.},
  doi          = {10.1007/s10589-020-00188-w},
  journal      = {Computational Optimization and Applications},
  number       = {2},
  pages        = {305-346},
  shortjournal = {Comput. Optim. Appl.},
  title        = {An efficient adaptive accelerated inexact proximal point method for solving linearly constrained nonconvex composite problems},
  volume       = {76},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Numerically tractable optimistic bilevel problems.
<em>COAP</em>, <em>76</em>(2), 277–303. (<a
href="https://doi.org/10.1007/s10589-020-00178-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider a class of optimistic bilevel problems. Specifically, we address bilevel problems in which at the lower level the objective function is fully convex and the feasible set does not depend on the upper level variables. We show that this nontrivial class of mathematical programs is sufficiently broad to encompass significant real-world applications and proves to be numerically tractable. From this respect, we establish that the stationary points for a relaxation of the original problem can be obtained addressing a suitable generalized Nash equilibrium problem. The latter game is proven to be convex and with a nonempty solution set. Leveraging this correspondence, we provide a provably convergent, easily implementable scheme to calculate stationary points of the relaxed bilevel program. As witnessed by some numerical experiments on an application in economics, this algorithm turns out to be numerically viable also for big dimensional problems.},
  archive      = {J_COAP},
  author       = {Lampariello, Lorenzo and Sagratella, Simone},
  doi          = {10.1007/s10589-020-00178-y},
  journal      = {Computational Optimization and Applications},
  number       = {2},
  pages        = {277-303},
  shortjournal = {Comput. Optim. Appl.},
  title        = {Numerically tractable optimistic bilevel problems},
  volume       = {76},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A comparison of solution approaches for the numerical
treatment of or-constrained optimization problems. <em>COAP</em>,
<em>76</em>(1), 233–275. (<a
href="https://doi.org/10.1007/s10589-020-00169-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mathematical programs with or-constraints form a new class of disjunctive optimization problems with inherent practical relevance. In this paper, we provide a comparison of three different solution methods for the numerical treatment of this problem class which are inspired by classical approaches from disjunctive programming. First, we study the replacement of the or-constraints as nonlinear inequality constraints using suitable NCP-functions. Second, we transfer the or-constrained program into a mathematical program with switching or complementarity constraints which can be treated with the aid of well-known relaxation methods. Third, a direct Scholtes-type relaxation of the or-constraints is investigated. A numerical comparison of all these approaches which is based on three essentially different model programs from or-constrained optimization closes the paper.},
  archive      = {J_COAP},
  author       = {Mehlitz, Patrick},
  doi          = {10.1007/s10589-020-00169-z},
  journal      = {Computational Optimization and Applications},
  number       = {1},
  pages        = {233-275},
  shortjournal = {Comput. Optim. Appl.},
  title        = {A comparison of solution approaches for the numerical treatment of or-constrained optimization problems},
  volume       = {76},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Efficient local search procedures for quadratic fractional
programming problems. <em>COAP</em>, <em>76</em>(1), 201–232. (<a
href="https://doi.org/10.1007/s10589-020-00175-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The problem of minimizing the sum of a convex quadratic function and the ratio of two quadratic functions can be reformulated as a Celis–Dennis–Tapia (CDT) problem and, thus, according to some recent results, can be polynomially solved. However, the degree of the known polynomial approaches for these problems is fairly large and that justifies the search for efficient local search procedures. In this paper the CDT reformulation of the problem is exploited to define a local search algorithm. On the theoretical side, its convergence to a stationary point is proved. On the practical side it is shown, through different numerical experiments, that the main cost of the algorithm is a single Schur decomposition to be performed during the initialization phase. The theoretical and practical results for this algorithm are further strengthened in a special case.},
  archive      = {J_COAP},
  author       = {Consolini, Luca and Locatelli, Marco and Wang, Jiulin and Xia, Yong},
  doi          = {10.1007/s10589-020-00175-1},
  journal      = {Computational Optimization and Applications},
  number       = {1},
  pages        = {201-232},
  shortjournal = {Comput. Optim. Appl.},
  title        = {Efficient local search procedures for quadratic fractional programming problems},
  volume       = {76},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Error estimates for the finite element approximation of
bilinear boundary control problems. <em>COAP</em>, <em>76</em>(1),
155–199. (<a href="https://doi.org/10.1007/s10589-020-00171-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article a special class of nonlinear optimal control problems involving a bilinear term in the boundary condition is studied. These kind of problems arise for instance in the identification of an unknown space-dependent Robin coefficient from a given measurement of the state, or when the Robin coefficient can be controlled in order to reach a desired state. Necessary and sufficient optimality conditions are derived and several discretization approaches for the numerical solution of the optimal control problem are investigated. Considered are both a full discretization and the postprocessing approach meaning that we compute an improved control by a pointwise evaluation of the first-order optimality condition. For both approaches finite element error estimates are shown and the validity of these results is confirmed by numerical experiments.},
  archive      = {J_COAP},
  author       = {Winkler, Max},
  doi          = {10.1007/s10589-020-00171-5},
  journal      = {Computational Optimization and Applications},
  number       = {1},
  pages        = {155-199},
  shortjournal = {Comput. Optim. Appl.},
  title        = {Error estimates for the finite element approximation of bilinear boundary control problems},
  volume       = {76},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A discretization algorithm for nonsmooth convex
semi-infinite programming problems based on bundle methods.
<em>COAP</em>, <em>76</em>(1), 125–153. (<a
href="https://doi.org/10.1007/s10589-020-00170-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a discretization algorithm for solving a class of nonsmooth convex semi-infinite programming problems that is based on a bundle method. Instead of employing the inexact calculation to evaluate the lower level problem, we shall carry out a discretization scheme. The discretization method is used to get a number of discretized problems which are solved by the bundle method. In particular, the subproblem used to generate a new point is independent of the number of constraints of the discretized problem. We apply a refinement-step which can be used to guarantee the convergence of the bundle method for the discretized problems as well as reduce the cost of the evaluations for the constraint functions during iteration. In addition we adopt an aggregation technique to manage the bundle information coming from previous steps. Both theoretical convergence analysis and preliminary computational results are reported. The results obtained have shown the good performance of the new algorithm.},
  archive      = {J_COAP},
  author       = {Pang, Li-Ping and Wu, Qi and Wang, Jin-He and Wu, Qiong},
  doi          = {10.1007/s10589-020-00170-6},
  journal      = {Computational Optimization and Applications},
  number       = {1},
  pages        = {125-153},
  shortjournal = {Comput. Optim. Appl.},
  title        = {A discretization algorithm for nonsmooth convex semi-infinite programming problems based on bundle methods},
  volume       = {76},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Composite convex optimization with global and local inexact
oracles. <em>COAP</em>, <em>76</em>(1), 69–124. (<a
href="https://doi.org/10.1007/s10589-020-00174-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce new global and local inexact oracle concepts for a wide class of convex functions in composite convex minimization. Such inexact oracles naturally arise in many situations, including primal–dual frameworks, barrier smoothing, and inexact evaluations of gradients and Hessians. We also provide examples showing that the class of convex functions equipped with the newly inexact oracles is larger than standard self-concordant and Lipschitz gradient function classes. Further, we investigate several properties of convex and/or self-concordant functions under our inexact oracles which are useful for algorithmic development. Next, we apply our theory to develop inexact proximal Newton-type schemes for minimizing general composite convex optimization problems equipped with such inexact oracles. Our theoretical results consist of new optimization algorithms accompanied with global convergence guarantees to solve a wide class of composite convex optimization problems. When the first objective term is additionally self-concordant, we establish different local convergence results for our method. In particular, we prove that depending on the choice of accuracy levels of the inexact second-order oracles, we obtain different local convergence rates ranging from linear and superlinear to quadratic. In special cases, where convergence bounds are known, our theory recovers the best known rates. We also apply our settings to derive a new primal–dual method for composite convex minimization problems involving linear operators. Finally, we present some representative numerical examples to illustrate the benefit of the new algorithms.},
  archive      = {J_COAP},
  author       = {Sun, Tianxiao and Necoara, Ion and Tran-Dinh, Quoc},
  doi          = {10.1007/s10589-020-00174-2},
  journal      = {Computational Optimization and Applications},
  number       = {1},
  pages        = {69-124},
  shortjournal = {Comput. Optim. Appl.},
  title        = {Composite convex optimization with global and local inexact oracles},
  volume       = {76},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A dual spectral projected gradient method for
log-determinant semidefinite problems. <em>COAP</em>, <em>76</em>(1),
33–68. (<a href="https://doi.org/10.1007/s10589-020-00166-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We extend the result on the spectral projected gradient method by Birgin et al. in 2000 to a log-determinant semidefinite problem with linear constraints and propose a spectral projected gradient method for the dual problem. Our method is based on alternate projections on the intersection of two convex sets, which first projects onto the box constraints and then onto a set defined by a linear matrix inequality. By exploiting structures of the two projections, we show that the same convergence properties can be obtained for the proposed method as Birgin’s method where the exact orthogonal projection onto the intersection of two convex sets is performed. Using the convergence properties, we prove that the proposed algorithm attains the optimal value or terminates in a finite number of iterations. The efficiency of the proposed method is illustrated with the numerical results on randomly generated synthetic/deterministic data and gene expression data, in comparison with other methods including the inexact primal–dual path-following interior-point method, the Newton-CG primal proximal-point algorithm, the adaptive spectral projected gradient method, and the adaptive Nesterov’s smooth method. For the gene expression data, our results are compared with the quadratic approximation for sparse inverse covariance estimation method. We show that our method outperforms the other methods in obtaining a better objective value fast.},
  archive      = {J_COAP},
  author       = {Nakagaki, Takashi and Fukuda, Mituhiro and Kim, Sunyoung and Yamashita, Makoto},
  doi          = {10.1007/s10589-020-00166-2},
  journal      = {Computational Optimization and Applications},
  number       = {1},
  pages        = {33-68},
  shortjournal = {Comput. Optim. Appl.},
  title        = {A dual spectral projected gradient method for log-determinant semidefinite problems},
  volume       = {76},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A derivative-free optimization algorithm for the efficient
minimization of functions obtained via statistical averaging.
<em>COAP</em>, <em>76</em>(1), 1–31. (<a
href="https://doi.org/10.1007/s10589-020-00172-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper considers the efficient minimization of the infinite time average of a stationary ergodic process in the space of a handful of design parameters which affect it. Problems of this class, derived from physical or numerical experiments which are sometimes expensive to perform, are ubiquitous in engineering applications. In such problems, any given function evaluation, determined with finite sampling, is associated with a quantifiable amount of uncertainty, which may be reduced via additional sampling. The present paper proposes a new optimization algorithm to adjust the amount of sampling associated with each function evaluation, making function evaluations more accurate (and, thus, more expensive), as required, as convergence is approached. The work builds on our algorithm for Delaunay-based Derivative-free Optimization via Global Surrogates ($${\varDelta }$$-DOGS, see JOGO https://doi.org/10.1007/s10898-015-0384-2). The new algorithm, dubbed $$\alpha $$-DOGS, substantially reduces the overall cost of the optimization process for problems of this important class. Further, under certain well-defined conditions, rigorous proof of convergence to the global minimum of the problem considered is established.},
  archive      = {J_COAP},
  author       = {Beyhaghi, Pooriya and Alimo, Ryan and Bewley, Thomas},
  doi          = {10.1007/s10589-020-00172-4},
  journal      = {Computational Optimization and Applications},
  number       = {1},
  pages        = {1-31},
  shortjournal = {Comput. Optim. Appl.},
  title        = {A derivative-free optimization algorithm for the efficient minimization of functions obtained via statistical averaging},
  volume       = {76},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Saddle points of rational functions. <em>COAP</em>,
<em>75</em>(3), 817–832. (<a
href="https://doi.org/10.1007/s10589-019-00141-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper concerns saddle points of rational functions, under general constraints. Based on optimality conditions, we propose an algorithm for computing saddle points. It uses Lasserre’s hierarchy of semidefinite relaxation. The algorithm can get a saddle point if it exists, or it can detect its nonexistence if it does not. Numerical experiments show that the algorithm is efficient for computing saddle points of rational functions.},
  archive      = {J_COAP},
  author       = {Zhou, Guangming and Wang, Qin and Zhao, Wenjie},
  doi          = {10.1007/s10589-019-00141-6},
  journal      = {Computational Optimization and Applications},
  number       = {3},
  pages        = {817-832},
  shortjournal = {Comput. Optim. Appl.},
  title        = {Saddle points of rational functions},
  volume       = {75},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Higher-degree tensor eigenvalue complementarity problems.
<em>COAP</em>, <em>75</em>(3), 799–816. (<a
href="https://doi.org/10.1007/s10589-019-00159-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we study the higher-degree tensor eigenvalue complementarity problem (HDTEiCP). We give an upper bound for the number of the higher-degree complementarity eigenvalues for the generic HDTEiCP. A semidefinite relaxation algorithm is proposed for computing all the higher-degree complementarity eigenvalues sequentially, as well as the corresponding eigenvectors, and the convergence of the algorithm is discussed. Some numerical results are also given.},
  archive      = {J_COAP},
  author       = {Zhao, Ruixue and Fan, Jinyan},
  doi          = {10.1007/s10589-019-00159-w},
  journal      = {Computational Optimization and Applications},
  number       = {3},
  pages        = {799-816},
  shortjournal = {Comput. Optim. Appl.},
  title        = {Higher-degree tensor eigenvalue complementarity problems},
  volume       = {75},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Iterative methods for computing u-eigenvalues of
non-symmetric complex tensors with application in quantum entanglement.
<em>COAP</em>, <em>75</em>(3), 779–798. (<a
href="https://doi.org/10.1007/s10589-019-00126-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The purpose of this paper is to study the problem of computing unitary eigenvalues (U-eigenvalues) of non-symmetric complex tensors. By means of symmetric embedding of complex tensors, the relationship between U-eigenpairs of a non-symmetric complex tensor and unitary symmetric eigenpairs (US-eigenpairs) of its symmetric embedding tensor is established. An Algorithm 3.1 is given to compute the U-eigenvalues of non-symmetric complex tensors by means of symmetric embedding. Another Algorithm 3.2, is proposed to directly compute the U-eigenvalues of non-symmetric complex tensors, without the aid of symmetric embedding. Finally, a tensor version of the well-known Gauss–Seidel method is developed. Efficiency of these three algorithms are compared by means of various numerical examples. These algorithms are applied to compute the geometric measure of entanglement of quantum multipartite non-symmetric pure states.},
  archive      = {J_COAP},
  author       = {Zhang, Mengshi and Ni, Guyan and Zhang, Guofeng},
  doi          = {10.1007/s10589-019-00126-5},
  journal      = {Computational Optimization and Applications},
  number       = {3},
  pages        = {779-798},
  shortjournal = {Comput. Optim. Appl.},
  title        = {Iterative methods for computing U-eigenvalues of non-symmetric complex tensors with application in quantum entanglement},
  volume       = {75},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Tensor neural network models for tensor singular value
decompositions. <em>COAP</em>, <em>75</em>(3), 753–777. (<a
href="https://doi.org/10.1007/s10589-020-00167-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Tensor decompositions have become increasingly prevalent in recent years. Traditionally, tensors are represented or decomposed as a sum of rank-one outer products using either the CANDECOMP/PARAFAC, the Tucker model, or some variations thereof. The motivation of these decompositions is to find an approximate representation for a given tensor. The main propose of this paper is to develop two neural network models for finding an approximation based on t-product for a given third-order tensor. Theoretical analysis shows that each of the neural network models ensures the convergence performance. The computer simulation results further substantiate that the models can find effectively the left and right singular tensor subspace.},
  archive      = {J_COAP},
  author       = {Wang, Xuezhong and Che, Maolin and Wei, Yimin},
  doi          = {10.1007/s10589-020-00167-1},
  journal      = {Computational Optimization and Applications},
  number       = {3},
  pages        = {753-777},
  shortjournal = {Comput. Optim. Appl.},
  title        = {Tensor neural network models for tensor singular value decompositions},
  volume       = {75},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). SDP relaxation algorithms for <span
class="math display"><strong>P</strong>(<strong>P</strong><sub>0</sub>)</span>-tensor
detection. <em>COAP</em>, <em>75</em>(3), 739–752. (<a
href="https://doi.org/10.1007/s10589-019-00145-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {$$\mathbf {P}$$-tensor and $$\mathbf {P}_0$$-tensor are introduced in tensor complementarity problem, which have wide applications in game theory. In this paper, we establish SDP relaxation algorithms for detecting $$\mathbf {P}(\mathbf {P}_0)$$-tensor. We first reformulate $$\mathbf {P}(\mathbf {P}_0)$$-tensor detection problem as polynomial optimization problems. Then we propose the SDP relaxation algorithms for solving the reformulated polynomial optimization problems. Numerical examples are reported to show the efficiency of the proposed algorithms.},
  archive      = {J_COAP},
  author       = {Wang, Xiao and Zhang, Xinzhen and Zhou, Guangming},
  doi          = {10.1007/s10589-019-00145-2},
  journal      = {Computational Optimization and Applications},
  number       = {3},
  pages        = {739-752},
  shortjournal = {Comput. Optim. Appl.},
  title        = {SDP relaxation algorithms for $$\mathbf {P}(\mathbf {P}_0)$$-tensor detection},
  volume       = {75},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). An inexact augmented lagrangian method for computing
strongly orthogonal decompositions of tensors. <em>COAP</em>,
<em>75</em>(3), 701–737. (<a
href="https://doi.org/10.1007/s10589-019-00128-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A strongly orthogonal decomposition of a tensor is a rank one tensor decomposition with the two component vectors in each mode of any two rank one tensors are either colinear or orthogonal. A strongly orthogonal decomposition with few number of rank one tensors is favorable in applications, which can be represented by a matrix-tensor multiplication with orthogonal factor matrices and a sparse tensor; and such a decomposition with the minimum number of rank one tensors is a strongly orthogonal rank decomposition. Any tensor has a strongly orthogonal rank decomposition. In this article, computing a strongly orthogonal rank decomposition is equivalently reformulated as solving an optimization problem. Different from the ill-posedness of the usual optimization reformulation for the tensor rank decomposition problem, the optimization reformulation of the strongly orthogonal rank decomposition of a tensor is well-posed. Each feasible solution of the optimization problem gives a strongly orthogonal decomposition of the tensor; and a global optimizer gives a strongly orthogonal rank decomposition, which is however difficult to compute. An inexact augmented Lagrangian method is proposed to solve the optimization problem. The augmented Lagrangian subproblem is solved by a proximal alternating minimization method, with the advantage that each subproblem has a closed formula solution and the factor matrices are kept orthogonal during the iteration. Thus, the algorithm always can return a feasible solution and thus a strongly orthogonal decomposition for any given tensor. Global convergence of this algorithm to a critical point is established without any further assumption. Extensive numerical experiments are conducted, and show that the proposed algorithm is quite promising in both efficiency and accuracy.},
  archive      = {J_COAP},
  author       = {Hu, Shenglong},
  doi          = {10.1007/s10589-019-00128-3},
  journal      = {Computational Optimization and Applications},
  number       = {3},
  pages        = {701-737},
  shortjournal = {Comput. Optim. Appl.},
  title        = {An inexact augmented lagrangian method for computing strongly orthogonal decompositions of tensors},
  volume       = {75},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). On semi-infinite systems of convex polynomial inequalities
and polynomial optimization problems. <em>COAP</em>, <em>75</em>(3),
669–699. (<a href="https://doi.org/10.1007/s10589-020-00168-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider the semi-infinite system of polynomial inequalities of the form $$\begin{aligned} {{\mathbf {K}}}:={x\in {{\mathbb {R}}}^m\mid p(x,y)\ge 0,\quad \forall y\in S\subseteq {{\mathbb {R}}}^n}, \end{aligned}$$where p(x, y) is a real polynomial in the variables x and the parameters y, the index set S is a basic semialgebraic set in $${{\mathbb {R}}}^n$$, $$-p(x,y)$$ is convex in x for every $$y\in S$$. We propose a procedure to construct approximate semidefinite representations of $${{\mathbf {K}}}$$. There are two indices to index these approximate semidefinite representations. As two indices increase, these semidefinite representation sets expand and contract, respectively, and can approximate $${{\mathbf {K}}}$$ as closely as possible under some assumptions. In some special cases, we can fix one of the two indices or both. Then, we consider the optimization problem of minimizing a convex polynomial over $${{\mathbf {K}}}$$. We present an SDP relaxation method for this optimization problem by similar strategies used in constructing approximate semidefinite representations of $${{\mathbf {K}}}$$. Under certain assumptions, some approximate minimizers of the optimization problem can also be obtained from the SDP relaxations. In some special cases, we show that the SDP relaxation for the optimization problem is exact and all minimizers can be extracted.},
  archive      = {J_COAP},
  author       = {Guo, Feng and Sun, Xiaoxia},
  doi          = {10.1007/s10589-020-00168-0},
  journal      = {Computational Optimization and Applications},
  number       = {3},
  pages        = {669-699},
  shortjournal = {Comput. Optim. Appl.},
  title        = {On semi-infinite systems of convex polynomial inequalities and polynomial optimization problems},
  volume       = {75},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Stochastic structured tensors to stochastic complementarity
problems. <em>COAP</em>, <em>75</em>(3), 649–668. (<a
href="https://doi.org/10.1007/s10589-019-00144-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper is concerned with the stochastic structured tensors to stochastic complementarity problems. The definitions and properties of stochastic structured tensors, such as the stochastic strong P-tensors, stochastic P-tensors, stochastic $$P_{0}$$-tensors, stochastic strictly semi-positive tensors and stochastic S-tensors are given. It is shown that the expected residual minimization formulation (ERM) of the stochastic structured tensor complementarity problem has a nonempty and bounded solution set. Interestingly, we partially answer the open questions proposed by Che et al. (Optim Lett 13:261–279, 2019). We also consider the expected value method of stochastic structured tensor complementarity problem with finitely many elements probability space. Finally, based on the expected residual minimization formulation (ERM) of the stochastic structured tensor complementarity problem, a projected gradient method is proposed for solving the stochastic structured tensor complementarity problem and the related numerical results are also given to show the efficiency of the proposed method.},
  archive      = {J_COAP},
  author       = {Du, Shouqiang and Che, Maolin and Wei, Yimin},
  doi          = {10.1007/s10589-019-00144-3},
  journal      = {Computational Optimization and Applications},
  number       = {3},
  pages        = {649-668},
  shortjournal = {Comput. Optim. Appl.},
  title        = {Stochastic structured tensors to stochastic complementarity problems},
  volume       = {75},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A semidefinite relaxation method for second-order cone
polynomial complementarity problems. <em>COAP</em>, <em>75</em>(3),
629–647. (<a href="https://doi.org/10.1007/s10589-019-00162-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper discusses how to compute all real solutions of the second-order cone tensor complementarity problem when there are finitely many ones. For this goal, we first formulate the second-order cone tensor complementarity problem as two polynomial optimization problems. Based on the reformulation, a semidefinite relaxation method is proposed by solving a finite number of semidefinite relaxations with some assumptions. Numerical experiments are given to show the efficiency of the method.},
  archive      = {J_COAP},
  author       = {Cheng, Lulu and Zhang, Xinzhen},
  doi          = {10.1007/s10589-019-00162-1},
  journal      = {Computational Optimization and Applications},
  number       = {3},
  pages        = {629-647},
  shortjournal = {Comput. Optim. Appl.},
  title        = {A semidefinite relaxation method for second-order cone polynomial complementarity problems},
  volume       = {75},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). On the tensor spectral p-norm and its dual norm via
partitions. <em>COAP</em>, <em>75</em>(3), 609–628. (<a
href="https://doi.org/10.1007/s10589-020-00177-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a generalization of the spectral norm and the nuclear norm of a tensor via arbitrary tensor partitions, a much richer concept than block tensors. We show that the spectral p-norm and the nuclear p-norm of a tensor can be lower and upper bounded by manipulating the spectral p-norms and the nuclear p-norms of subtensors in an arbitrary partition of the tensor for $$1\le p\le \infty$$. Hence, it generalizes and answers affirmatively the conjecture proposed by Li (SIAM J Matrix Anal Appl 37:1440–1452, 2016) for a tensor partition and $$p=2$$. We study the relations of the norms of a tensor, the norms of matrix unfoldings of the tensor, and the bounds via the norms of matrix slices of the tensor. Various bounds of the tensor spectral and nuclear norms in the literature are implied by our results.},
  archive      = {J_COAP},
  author       = {Chen, Bilian and Li, Zhening},
  doi          = {10.1007/s10589-020-00177-z},
  journal      = {Computational Optimization and Applications},
  number       = {3},
  pages        = {609-628},
  shortjournal = {Comput. Optim. Appl.},
  title        = {On the tensor spectral p-norm and its dual norm via partitions},
  volume       = {75},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Preface to the special issue on optimization with
polynomials and tensors. <em>COAP</em>, <em>75</em>(3), 607–608. (<a
href="https://doi.org/10.1007/s10589-020-00184-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_COAP},
  author       = {Nie, Jiawang},
  doi          = {10.1007/s10589-020-00184-0},
  journal      = {Computational Optimization and Applications},
  number       = {3},
  pages        = {607-608},
  shortjournal = {Comput. Optim. Appl.},
  title        = {Preface to the special issue on optimization with polynomials and tensors},
  volume       = {75},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Empirical study of exact algorithms for the multi-objective
spanning tree. <em>COAP</em>, <em>75</em>(2), 561–605. (<a
href="https://doi.org/10.1007/s10589-019-00154-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The multi-objective spanning tree (MoST) is an extension of the minimum spanning tree problem (MST) that, as well as its single-objective counterpart, arises in several practical applications. However, unlike the MST, for which there are polynomial-time algorithms that solve it, the MoST is NP-hard. Several researchers proposed techniques to solve the MoST, each of those methods with specific potentialities and limitations. In this study, we examine those methods and divide them into two categories regarding their outcomes: Pareto optimal sets and Pareto optimal fronts. To compare the techniques from the two groups, we investigated their behavior on 2, 3 and 4-objective instances from different classes. We report the results of a computational experiment on 8100 complete and grid graphs in which we analyze specific features of each algorithm as well as the computational effort required to solve the instances.},
  archive      = {J_COAP},
  author       = {Fernandes, I. F. C. and Goldbarg, E. F. G. and Maia, S. M. D. M. and Goldbarg, M. C.},
  doi          = {10.1007/s10589-019-00154-1},
  journal      = {Computational Optimization and Applications},
  number       = {2},
  pages        = {561-605},
  shortjournal = {Comput. Optim. Appl.},
  title        = {Empirical study of exact algorithms for the multi-objective spanning tree},
  volume       = {75},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Solution methods for a min–max facility location problem
with regional customers considering closest euclidean distances.
<em>COAP</em>, <em>75</em>(2), 537–560. (<a
href="https://doi.org/10.1007/s10589-019-00163-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study a facility location problem where a single facility serves multiple customers each represented by a (possibly non-convex) region in the plane. The aim of the problem is to locate a single facility in the plane so that the maximum of the closest Euclidean distances between the facility and the customer regions is minimized. Assuming that each customer region is mixed-integer second order cone representable, we firstly give a mixed-integer second order cone programming formulation of the problem. Secondly, we consider a solution method based on the Minkowski sums of sets. Both of these solution methods are extended to the constrained case in which the facility is to be located on a (possibly non-convex) subset of the plane. Finally, these two methods are compared in terms of solution quality and time with extensive computational experiments.},
  archive      = {J_COAP},
  author       = {Dolu, Nazlı and Hastürk, Umur and Tural, Mustafa Kemal},
  doi          = {10.1007/s10589-019-00163-0},
  journal      = {Computational Optimization and Applications},
  number       = {2},
  pages        = {537-560},
  shortjournal = {Comput. Optim. Appl.},
  title        = {Solution methods for a min–max facility location problem with regional customers considering closest euclidean distances},
  volume       = {75},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Fast feasibility check of the multi-material vertical
alignment problem in road design. <em>COAP</em>, <em>75</em>(2),
515–536. (<a href="https://doi.org/10.1007/s10589-019-00160-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {When building a road, it is critical to select a vertical alignment which ensures design and safety constraints. Finding such a vertical alignment is not necessarily a feasible problem, and the models describing it generally involve a large number of variables and constraints. This paper is dedicated to rapidly proving the feasibility or the infeasibility of a Mixed Integer Linear Program (MILP) modeling the vertical alignment problem. To do so, we take advantage of the particular structure of the MILP, and we prove that only a few of the MILP’s constraints determine the feasibility of the problem. In addition, we propose a method to build a feasible solution to the MILP that does not involve integer variables. This enables time saving to proving the feasibility of the vertical alignment problem and to find a feasible vertical alignment, as emphasized by numerical results. It is on average 75 times faster to prove the feasibility and 10 times faster to build a feasible solution.},
  archive      = {J_COAP},
  author       = {Monnet, Dominique and Hare, Warren and Lucet, Yves},
  doi          = {10.1007/s10589-019-00160-3},
  journal      = {Computational Optimization and Applications},
  number       = {2},
  pages        = {515-536},
  shortjournal = {Comput. Optim. Appl.},
  title        = {Fast feasibility check of the multi-material vertical alignment problem in road design},
  volume       = {75},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A branch-and-cut algorithm for solving mixed-integer
semidefinite optimization problems. <em>COAP</em>, <em>75</em>(2),
493–513. (<a href="https://doi.org/10.1007/s10589-019-00153-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider a cutting-plane algorithm for solving mixed-integer semidefinite optimization (MISDO) problems. In this algorithm, the positive semidefinite (psd) constraint is relaxed, and the resultant mixed-integer linear optimization problem is solved repeatedly, imposing at each iteration a valid inequality for the psd constraint. We prove the convergence properties of the algorithm. Moreover, to speed up the computation, we devise a branch-and-cut algorithm, in which valid inequalities are dynamically added during a branch-and-bound procedure. We test the computational performance of our cutting-plane and branch-and-cut algorithms for three types of MISDO problem: random instances, computing restricted isometry constants, and robust truss topology design. Our experimental results demonstrate that, for many problem instances, our branch-and-cut algorithm delivered superior performance compared with general-purpose MISDO solvers in terms of computational efficiency and stability.},
  archive      = {J_COAP},
  author       = {Kobayashi, Ken and Takano, Yuich},
  doi          = {10.1007/s10589-019-00153-2},
  journal      = {Computational Optimization and Applications},
  number       = {2},
  pages        = {493-513},
  shortjournal = {Comput. Optim. Appl.},
  title        = {A branch-and-cut algorithm for solving mixed-integer semidefinite optimization problems},
  volume       = {75},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A line search exact penalty method for nonlinear
semidefinite programming. <em>COAP</em>, <em>75</em>(2), 467–491. (<a
href="https://doi.org/10.1007/s10589-019-00158-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we present a line search exact penalty method for solving nonlinear semidefinite programming (SDP) problem. Compared with the traditional sequential semidefinite programming (SSDP) method which requires that the subproblem at every iterate point is compatible, this method is more practical. We first use a robust subproblem, which is always feasible, to get a detective step, then compute a search direction either from a traditional SSDP subproblem or a quadratic optimization subproblem with the penalty term. This two-phase strategy with the $$l_1$$ exact penalty function is employed to promote the global convergence, which is analyzed without assuming any constraint qualifications. Some preliminary numerical results are reported.},
  archive      = {J_COAP},
  author       = {Zhao, Qi and Chen, Zhongwen},
  doi          = {10.1007/s10589-019-00158-x},
  journal      = {Computational Optimization and Applications},
  number       = {2},
  pages        = {467-491},
  shortjournal = {Comput. Optim. Appl.},
  title        = {A line search exact penalty method for nonlinear semidefinite programming},
  volume       = {75},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Secant update version of quasi-newton PSB with weighted
multisecant equations. <em>COAP</em>, <em>75</em>(2), 441–466. (<a
href="https://doi.org/10.1007/s10589-019-00164-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Quasi-Newton methods are often used in the frame of non-linear optimization. In those methods, the quality and cost of the estimate of the Hessian matrix has a major influence on the efficiency of the optimization algorithm, which has a huge impact for computationally costly problems. One strategy to create a more accurate estimate of the Hessian consists in maximizing the use of available information during this computation. This is done by combining different characteristics. The Powell-Symmetric-Broyden method (PSB) imposes, for example, the satisfaction of the last secant equation, which is called secant update property, and the symmetry of the Hessian (Powell in Nonlinear Programming 31–65, 1970). Imposing the satisfaction of more secant equations should be the next step to include more information into the Hessian. However, Schnabel proved that this is impossible (Schnabel in quasi-Newton methods using multiple secant equations, 1983). Penalized PSB (pPSB), works around the impossibility by giving a symmetric Hessian and penalizing the non-satisfaction of the multiple secant equations by using weight factors (Gratton et al. in Optim Methods Softw 30(4):748–755, 2015). Doing so, he loses the secant update property. In this paper, we combine the properties of PSB and pPSB by adding to pPSB the secant update property. This gives us the secant update penalized PSB (SUpPSB). This new formula that we propose also avoids matrix inversions, which makes it easier to compute. Next to that, SUpPSB also performs globally better compared to pPSB.},
  archive      = {J_COAP},
  author       = {Boutet, Nicolas and Haelterman, Rob and Degroote, Joris},
  doi          = {10.1007/s10589-019-00164-z},
  journal      = {Computational Optimization and Applications},
  number       = {2},
  pages        = {441-466},
  shortjournal = {Comput. Optim. Appl.},
  title        = {Secant update version of quasi-newton PSB with weighted multisecant equations},
  volume       = {75},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A self-adaptive method for pseudomonotone equilibrium
problems and variational inequalities. <em>COAP</em>, <em>75</em>(2),
423–440. (<a href="https://doi.org/10.1007/s10589-019-00156-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we introduce and analyze a new algorithm for solving equilibrium problem involving pseudomonotone and Lipschitz-type bifunction in real Hilbert space. The algorithm requires only a strongly convex programming problem per iteration. A weak and a strong convergence theorem are established without the knowledge of the Lipschitz-type constants of the bifunction. As a special case of equilibrium problem, the variational inequality is also considered. Finally, numerical experiments are performed to illustrate the advantage of the proposed algorithm.},
  archive      = {J_COAP},
  author       = {Yang, Jun and Liu, Hongwei},
  doi          = {10.1007/s10589-019-00156-z},
  journal      = {Computational Optimization and Applications},
  number       = {2},
  pages        = {423-440},
  shortjournal = {Comput. Optim. Appl.},
  title        = {A self-adaptive method for pseudomonotone equilibrium problems and variational inequalities},
  volume       = {75},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Relative-error inertial-relaxed inexact versions of
douglas-rachford and ADMM splitting algorithms. <em>COAP</em>,
<em>75</em>(2), 389–422. (<a
href="https://doi.org/10.1007/s10589-019-00165-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper derives new inexact variants of the Douglas-Rachford splitting method for maximal monotone operators and the alternating direction method of multipliers (ADMM) for convex optimization. The analysis is based on a new inexact version of the proximal point algorithm that includes both an inertial step and overrelaxation. We apply our new inexact ADMM method to LASSO and logistic regression problems and obtain somewhat better computational performance than earlier inexact ADMM methods.},
  archive      = {J_COAP},
  author       = {Alves, M. Marques and Eckstein, Jonathan and Geremia, Marina and Melo, Jefferson G.},
  doi          = {10.1007/s10589-019-00165-y},
  journal      = {Computational Optimization and Applications},
  number       = {2},
  pages        = {389-422},
  shortjournal = {Comput. Optim. Appl.},
  title        = {Relative-error inertial-relaxed inexact versions of douglas-rachford and ADMM splitting algorithms},
  volume       = {75},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Optimally linearizing the alternating direction method of
multipliers for convex programming. <em>COAP</em>, <em>75</em>(2),
361–388. (<a href="https://doi.org/10.1007/s10589-019-00152-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The alternating direction method of multipliers (ADMM) is being widely used in a variety of areas; its different variants tailored for different application scenarios have also been deeply researched in the literature. Among them, the linearized ADMM has received particularly wide attention in many areas because of its efficiency and easy implementation. To theoretically guarantee convergence of the linearized ADMM, the step size for the linearized subproblems, or the reciprocal of the linearization parameter, should be sufficiently small. On the other hand, small step sizes decelerate the convergence numerically. Hence, it is interesting to probe the optimal (largest) value of the step size that guarantees convergence of the linearized ADMM. This analysis is lacked in the literature. In this paper, we provide a rigorous mathematical analysis for finding this optimal step size of the linearized ADMM and accordingly set up the optimal version of the linearized ADMM in the convex programming context. The global convergence and worst-case convergence rate measured by the iteration complexity of the optimal version of linearized ADMM are proved as well.},
  archive      = {J_COAP},
  author       = {He, Bingsheng and Ma, Feng and Yuan, Xiaoming},
  doi          = {10.1007/s10589-019-00152-3},
  journal      = {Computational Optimization and Applications},
  number       = {2},
  pages        = {361-388},
  shortjournal = {Comput. Optim. Appl.},
  title        = {Optimally linearizing the alternating direction method of multipliers for convex programming},
  volume       = {75},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A conjugate direction based simplicial decomposition
framework for solving a specific class of dense convex quadratic
programs. <em>COAP</em>, <em>75</em>(2), 321–360. (<a
href="https://doi.org/10.1007/s10589-019-00151-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many real-world applications can usually be modeled as convex quadratic problems. In the present paper, we want to tackle a specific class of quadratic programs having a dense Hessian matrix and a structured feasible set. We hence carefully analyze a simplicial decomposition like algorithmic framework that handles those problems in an effective way. We introduce a new master solver, called Adaptive Conjugate Direction Method, and embed it in our framework. We also analyze the interaction of some techniques for speeding up the solution of the pricing problem. We report extensive numerical experiments based on a benchmark of almost 1400 instances from specific and generic quadratic problems. We show the efficiency and robustness of the method when compared to a commercial solver (Cplex).},
  archive      = {J_COAP},
  author       = {Bettiol, Enrico and Létocart, Lucas and Rinaldi, Francesco and Traversi, Emiliano},
  doi          = {10.1007/s10589-019-00151-4},
  journal      = {Computational Optimization and Applications},
  number       = {2},
  pages        = {321-360},
  shortjournal = {Comput. Optim. Appl.},
  title        = {A conjugate direction based simplicial decomposition framework for solving a specific class of dense convex quadratic programs},
  volume       = {75},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Heuristics for the single machine weighted sum of completion
times scheduling problem with periodic maintenance. <em>COAP</em>,
<em>75</em>(1), 291–320. (<a
href="https://doi.org/10.1007/s10589-019-00142-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper tackles the single machine scheduling problem with periodic preventive maintenance in order to minimize the weighted sum of completion times. This criterion is certainly less studied than the makespan but it remains nonetheless interesting on the theoretical and practical levels. Indeed, the weights can quantify the holding cost per unit of time of the products to transform. Thus, this criterion can represent the global holding cost. This problem is proved to be NP-hard and a mixed integer linear programming formulation is proposed to solve small size instances of the problem. To solve large instances, we proposed three properties for this problem which generalize already existing works. These properties have been of great use in designing efficient heuristics capable of solving instances with up to 1000 jobs. To evaluate the performances of the proposed heuristics, lower bounds based on special cases of the problem are provided. Computational experiments show that the average percentage error of the best heuristic is less than 10\%.},
  archive      = {J_COAP},
  author       = {Krim, Hanane and Benmansour, Rachid and Duvivier, David and Aït-Kadi, Daoud and Hanafi, Said},
  doi          = {10.1007/s10589-019-00142-5},
  journal      = {Computational Optimization and Applications},
  number       = {1},
  pages        = {291-320},
  shortjournal = {Comput. Optim. Appl.},
  title        = {Heuristics for the single machine weighted sum of completion times scheduling problem with periodic maintenance},
  volume       = {75},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A proximal point method for difference of convex functions
in multi-objective optimization with application to group dynamic
problems. <em>COAP</em>, <em>75</em>(1), 263–290. (<a
href="https://doi.org/10.1007/s10589-019-00139-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider the constrained multi-objective optimization problem of finding Pareto critical points of difference of convex functions. The new approach proposed by Bento et al. (SIAM J Optim 28:1104–1120, 2018) to study the convergence of the proximal point method is applied. Our method minimizes at each iteration a convex approximation instead of the (non-convex) objective function constrained to a possibly non-convex set which assures the vector improving process. The motivation comes from the famous Group Dynamic problem in Behavioral Sciences where, at each step, a group of (possible badly informed) agents tries to increase his joint payoff, in order to be able to increase the payoff of each of them. In this way, at each step, this ascent process guarantees the stability of the group. Some encouraging preliminary numerical results are reported.},
  archive      = {J_COAP},
  author       = {de Carvalho Bento, Glaydston and Bitar, Sandro Dimy Barbosa and da Cruz Neto, João Xavier and Soubeyran, Antoine and de Oliveira Souza, João Carlos},
  doi          = {10.1007/s10589-019-00139-0},
  journal      = {Computational Optimization and Applications},
  number       = {1},
  pages        = {263-290},
  shortjournal = {Comput. Optim. Appl.},
  title        = {A proximal point method for difference of convex functions in multi-objective optimization with application to group dynamic problems},
  volume       = {75},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Rank-two update algorithm versus frank–wolfe algorithm with
away steps for the weighted euclidean one-center problem. <em>COAP</em>,
<em>75</em>(1), 237–262. (<a
href="https://doi.org/10.1007/s10589-019-00148-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The weighted Euclidean one-center (WEOC) problem is one of the classic problems in facility location theory, which is also a generalization of the minimum enclosing ball (MEB) problem. Given m points in $${\mathbb {R}}^{n}$$, the WEOC problem computes a center point $$c\in {\mathbb {R}}^{n}$$ that minimizes the maximum weighted Euclidean distance to m given points. The rank-two update algorithm is an effective method for solving the minimum volume enclosing ellipsoid (MVEE) problem. It updates only two components of the solution at each iteration, which was previously proposed in Cong et al. (Comput Optim Appl 51(1):241–257, 2012). In this paper, we further develop and analyze the rank-two update algorithm for solving the WEOC problem. At each iteration, the calculation of the optimal step-size for the WEOC problem needs to distinguish four different cases, which is a challenge in comparison with the MVEE problem. We establish the theoretical results of the complexity and the core set size of the rank-two update algorithm for the WEOC problem, which are the generalizations of the currently best-known results for the MEB problem. In addition, by constructing an important inequality for the WEOC problem, we establish the linear convergence of this rank-two update algorithm. Numerical experiments show that the rank-two update algorithm is comparable to the Frank–Wolfe algorithm with away steps for the WEOC problem. In particular, the rank-two update algorithm is more efficient than the Frank–Wolfe algorithm with away steps for problem instances with $$m\gg n$$ under high precision.},
  archive      = {J_COAP},
  author       = {Cong, Wei-jie and Wang, Le and Sun, Hui},
  doi          = {10.1007/s10589-019-00148-z},
  journal      = {Computational Optimization and Applications},
  number       = {1},
  pages        = {237-262},
  shortjournal = {Comput. Optim. Appl.},
  title        = {Rank-two update algorithm versus Frank–Wolfe algorithm with away steps for the weighted euclidean one-center problem},
  volume       = {75},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Modified jacobian smoothing method for nonsmooth
complementarity problems. <em>COAP</em>, <em>75</em>(1), 207–235. (<a
href="https://doi.org/10.1007/s10589-019-00136-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper is devoted to solving a nonsmooth complementarity problem where the mapping is locally Lipschitz continuous but not continuously differentiable everywhere. We reformulate this nonsmooth complementarity problem as a system of nonsmooth equations with the max function and then propose an approximation to the reformulation by simultaneously smoothing the mapping and the max function. Based on the approximation, we present a modified Jacobian smoothing method for the nonsmooth complementarity problem. We show the Jacobian consistency of the function associated with the approximation, under which we establish the global and fast local convergence for the method under suitable assumptions. Finally, to show the effectiveness of the proposed method, we report our numerical experiments on some examples based on MCPLIB/GAMSLIB libraries or network Nash–Cournot game is proposed.},
  archive      = {J_COAP},
  author       = {Chen, Pin-Bo and Zhang, Peng and Zhu, Xide and Lin, Gui-Hua},
  doi          = {10.1007/s10589-019-00136-3},
  journal      = {Computational Optimization and Applications},
  number       = {1},
  pages        = {207-235},
  shortjournal = {Comput. Optim. Appl.},
  title        = {Modified jacobian smoothing method for nonsmooth complementarity problems},
  volume       = {75},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Large-scale unconstrained optimization using separable cubic
modeling and matrix-free subspace minimization. <em>COAP</em>,
<em>75</em>(1), 169–205. (<a
href="https://doi.org/10.1007/s10589-019-00138-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a new algorithm for solving large-scale unconstrained optimization problems that uses cubic models, matrix-free subspace minimization, and secant-type parameters for defining the cubic terms. We also propose and analyze a specialized trust-region strategy to minimize the cubic model on a properly chosen low-dimensional subspace, which is built at each iteration using the Lanczos process. For the convergence analysis we present, as a general framework, a model trust-region subspace algorithm with variable metric and we establish asymptotic as well as complexity convergence results. Preliminary numerical results, on some test functions and also on the well-known disk packing problem, are presented to illustrate the performance of the proposed scheme when solving large-scale problems.},
  archive      = {J_COAP},
  author       = {Brás, C. P. and Martínez, J. M. and Raydan, M.},
  doi          = {10.1007/s10589-019-00138-1},
  journal      = {Computational Optimization and Applications},
  number       = {1},
  pages        = {169-205},
  shortjournal = {Comput. Optim. Appl.},
  title        = {Large-scale unconstrained optimization using separable cubic modeling and matrix-free subspace minimization},
  volume       = {75},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). An improved dai–kou conjugate gradient algorithm for
unconstrained optimization. <em>COAP</em>, <em>75</em>(1), 145–167. (<a
href="https://doi.org/10.1007/s10589-019-00143-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {It is gradually accepted that the loss of orthogonality of the gradients in a conjugate gradient algorithm may decelerate the convergence rate to some extent. The Dai–Kou conjugate gradient algorithm (SIAM J Optim 23(1):296–320, 2013), called CGOPT, has attracted many researchers’ attentions due to its numerical efficiency. In this paper, we present an improved Dai–Kou conjugate gradient algorithm for unconstrained optimization, which only consists of two kinds of iterations. In the improved Dai–Kou conjugate gradient algorithm, we develop a new quasi-Newton method to improve the orthogonality by solving the subproblem in the subspace and design a modified strategy for the choice of the initial stepsize for improving the numerical performance. The global convergence of the improved Dai–Kou conjugate gradient algorithm is established without the strict assumptions in the convergence analysis of other limited memory conjugate gradient methods. Some numerical results suggest that the improved Dai–Kou conjugate gradient algorithm (CGOPT (2.0)) yields a tremendous improvement over the original Dai–Kou CG algorithm (CGOPT (1.0)) and is slightly superior to the latest limited memory conjugate gradient software package CG$$\_ $$DESCENT (6.8) developed by Hager and Zhang (SIAM J Optim 23(4):2150–2168, 2013) for the CUTEr library.},
  archive      = {J_COAP},
  author       = {Liu, Zexian and Liu, Hongwei and Dai, Yu-Hong},
  doi          = {10.1007/s10589-019-00143-4},
  journal      = {Computational Optimization and Applications},
  number       = {1},
  pages        = {145-167},
  shortjournal = {Comput. Optim. Appl.},
  title        = {An improved Dai–Kou conjugate gradient algorithm for unconstrained optimization},
  volume       = {75},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A unified convergence framework for nonmonotone inexact
decomposition methods. <em>COAP</em>, <em>75</em>(1), 113–144. (<a
href="https://doi.org/10.1007/s10589-019-00150-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this work we propose a general framework that provides a unified convergence analysis for nonmonotone decomposition algorithms. The main motivation to embed nonmonotone strategies within a decomposition approach lies in the fact that enforcing the reduction of the objective function could be unnecessarily expensive, taking into account that groups of variables are individually updated. We define different search directions and line searches satisfying the conditions required by the presented nonmonotone decomposition framework to obtain global convergence. We employ a set of large-scale network equilibrium problems as a computational example to show the advantages of a nonmonotone algorithm over its monotone counterpart. In conclusion, a new smart implementation for decomposition methods has been derived to solve numerical issues on large-scale partially separable functions.},
  archive      = {J_COAP},
  author       = {Galli, Leonardo and Galligari, Alessandro and Sciandrone, Marco},
  doi          = {10.1007/s10589-019-00150-5},
  journal      = {Computational Optimization and Applications},
  number       = {1},
  pages        = {113-144},
  shortjournal = {Comput. Optim. Appl.},
  title        = {A unified convergence framework for nonmonotone inexact decomposition methods},
  volume       = {75},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A global hybrid derivative-free method for high-dimensional
systems of nonlinear equations. <em>COAP</em>, <em>75</em>(1), 93–112.
(<a href="https://doi.org/10.1007/s10589-019-00149-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This work concerns the numerical solution of high-dimensional systems of nonlinear equations, when derivatives are not available for use, but assuming that all functions defining the problem are continuously differentiable. A hybrid approach is taken, based on a derivative-free iterative method, organized in two phases. The first phase is defined by derivative-free versions of a fixed-point method that employs spectral parameters to define the steplength along the residual direction. The second phase consists on a matrix-free inexact Newton method that employs the Generalized Minimal Residual algorithm to solve the linear system that computes the search direction. This second phase will only take place if the first one fails to find a better point after a predefined number of reductions in the step size. In all stages, the criterion to accept a new point considers a nonmonotone decrease condition upon a merit function. Convergence results are established and the numerical performance is assessed through experiments in a set of problems collected from the literature. Both the theoretical and the experimental analysis support the feasibility of the proposed hybrid strategy.},
  archive      = {J_COAP},
  author       = {Begiato, Rodolfo G. and Custódio, Ana L. and Gomes-Ruggiero, Márcia A.},
  doi          = {10.1007/s10589-019-00149-y},
  journal      = {Computational Optimization and Applications},
  number       = {1},
  pages        = {93-112},
  shortjournal = {Comput. Optim. Appl.},
  title        = {A global hybrid derivative-free method for high-dimensional systems of nonlinear equations},
  volume       = {75},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Restarting the accelerated coordinate descent method with a
rough strong convexity estimate. <em>COAP</em>, <em>75</em>(1), 63–91.
(<a href="https://doi.org/10.1007/s10589-019-00137-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose new restarting strategies for the accelerated coordinate descent method. Our main contribution is to show that for a well chosen sequence of restarting times, the restarted method has a nearly geometric rate of convergence. A major feature of the method is that it can take profit of the local quadratic error bound of the objective function without knowing the actual value of the error bound. We also show that under the more restrictive assumption that the objective function is strongly convex, any fixed restart period leads to a geometric rate of convergence. Finally, we illustrate the properties of the algorithm on a regularized logistic regression problem and on a Lasso problem.},
  archive      = {J_COAP},
  author       = {Fercoq, Olivier and Qu, Zheng},
  doi          = {10.1007/s10589-019-00137-2},
  journal      = {Computational Optimization and Applications},
  number       = {1},
  pages        = {63-91},
  shortjournal = {Comput. Optim. Appl.},
  title        = {Restarting the accelerated coordinate descent method with a rough strong convexity estimate},
  volume       = {75},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Markov chain block coordinate descent. <em>COAP</em>,
<em>75</em>(1), 35–61. (<a
href="https://doi.org/10.1007/s10589-019-00140-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The method of block coordinate gradient descent (BCD) has been a powerful method for large-scale optimization. This paper considers the BCD method that successively updates a series of blocks selected according to a Markov chain. This kind of block selection is neither i.i.d. random nor cyclic. On the other hand, it is a natural choice for some applications in distributed optimization and Markov decision process, where i.i.d. random and cyclic selections are either infeasible or very expensive. By applying mixing-time properties of a Markov chain, we prove convergence of Markov chain BCD for minimizing Lipschitz differentiable functions, which can be nonconvex. When the functions are convex and strongly convex, we establish both sublinear and linear convergence rates, respectively. We also present a method of Markov chain inertial BCD. Finally, we discuss potential applications.},
  archive      = {J_COAP},
  author       = {Sun, Tao and Sun, Yuejiao and Xu, Yangyang and Yin, Wotao},
  doi          = {10.1007/s10589-019-00140-7},
  journal      = {Computational Optimization and Applications},
  number       = {1},
  pages        = {35-61},
  shortjournal = {Comput. Optim. Appl.},
  title        = {Markov chain block coordinate descent},
  volume       = {75},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Multiscale stochastic optimization: Modeling aspects and
scenario generation. <em>COAP</em>, <em>75</em>(1), 1–34. (<a
href="https://doi.org/10.1007/s10589-019-00135-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Real-world multistage stochastic optimization problems are often characterized by the fact that the decision maker may take actions only at specific points in time, even if relevant data can be observed much more frequently. In such a case there are not only multiple decision stages present but also several observation periods between consecutive decisions, where profits/costs occur contingent on the stochastic evolution of some uncertainty factors. We refer to such multistage decision problems with encapsulated multiperiod random costs, as multiscale stochastic optimization problems. In this article, we present a tailor-made modeling framework for such problems, which allows for a computational solution. We first establish new results related to the generation of scenario lattices and then incorporate the multiscale feature by leveraging the theory of stochastic bridge processes. All necessary ingredients to our proposed modeling framework are elaborated explicitly for various popular examples, including both diffusion and jump models.},
  archive      = {J_COAP},
  author       = {Glanzer, Martin and Pflug, Georg Ch.},
  doi          = {10.1007/s10589-019-00135-4},
  journal      = {Computational Optimization and Applications},
  number       = {1},
  pages        = {1-34},
  shortjournal = {Comput. Optim. Appl.},
  title        = {Multiscale stochastic optimization: Modeling aspects and scenario generation},
  volume       = {75},
  year         = {2020},
}
</textarea>
</details></li>
</ul>

</body>
</html>
