<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>DMKD_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="dmkd---60">DMKD - 60</h2>
<ul>
<li><details>
<summary>
(2020). Correction to: A unified view of density-based methods for
semi-supervised clustering and classification. <em>DMKD</em>,
<em>34</em>(6), 1984–1985. (<a
href="https://doi.org/10.1007/s10618-020-00707-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The article, A unified view of density-based methods for semi-supervised.},
  archive      = {J_DMKD},
  author       = {Castro Gertrudes, Jadson and Zimek, Arthur and Sander, Jörg and Campello, Ricardo J. G. B.},
  doi          = {10.1007/s10618-020-00707-7},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {11},
  number       = {6},
  pages        = {1984-1985},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Correction to: A unified view of density-based methods for semi-supervised clustering and classification},
  volume       = {34},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). DeepTable: A permutation invariant neural network for table
orientation classification. <em>DMKD</em>, <em>34</em>(6), 1963–1983.
(<a href="https://doi.org/10.1007/s10618-020-00711-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Tables are a common way to present information in an intuitive and concise manner. They are used extensively in media such as scientific articles or web pages. Automatically analyzing the content of tables bears special challenges. One of the most basic tasks is determination of the orientation of a table: In column tables, columns represent one entity with the different attribute values present in the different rows; row tables are vice versa, and matrix tables give information on pairs of entities. In this paper, we address the problem of classifying a given table into one of the three layouts horizontal (for row tables), vertical (for column tables), and matrix. We describe DeepTable, a novel method based on deep neural networks designed for learning from sets. Contrary to previous state-of-the-art methods, this basis makes DeepTable invariant to the permutation of rows or columns, which is a highly desirable property as in most tables the order of rows and columns does not carry specific information. We evaluate our method using a silver standard corpus of 5500 tables extracted from biomedical articles where the layout was determined heuristically. DeepTable outperforms previous methods in both precision and recall on our corpus. In a second evaluation, we manually labeled a corpus of 300 tables and were able to confirm DeepTable to reach superior performance in the table layout classification task. The codes and resources introduced here are available at https://github.com/Marhabibi/DeepTable .},
  archive      = {J_DMKD},
  author       = {Habibi, Maryam and Starlinger, Johannes and Leser, Ulf},
  doi          = {10.1007/s10618-020-00711-x},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {11},
  number       = {6},
  pages        = {1963-1983},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {DeepTable: A permutation invariant neural network for table orientation classification},
  volume       = {34},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). InceptionTime: Finding AlexNet for time series
classification. <em>DMKD</em>, <em>34</em>(6), 1936–1962. (<a
href="https://doi.org/10.1007/s10618-020-00710-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper brings deep learning at the forefront of research into time series classification (TSC). TSC is the area of machine learning tasked with the categorization (or labelling) of time series. The last few decades of work in this area have led to significant progress in the accuracy of classifiers, with the state of the art now represented by the HIVE-COTE algorithm. While extremely accurate, HIVE-COTE cannot be applied to many real-world datasets because of its high training time complexity in $$O(N^2\cdot T^4)$$ for a dataset with N time series of length T. For example, it takes HIVE-COTE more than 8 days to learn from a small dataset with $$N=1500$$ time series of short length $$T=46$$ . Meanwhile deep learning has received enormous attention because of its high accuracy and scalability. Recent approaches to deep learning for TSC have been scalable, but less accurate than HIVE-COTE. We introduce InceptionTime—an ensemble of deep Convolutional Neural Network models, inspired by the Inception-v4 architecture. Our experiments show that InceptionTime is on par with HIVE-COTE in terms of accuracy while being much more scalable: not only can it learn from 1500 time series in one hour but it can also learn from 8M time series in 13 h, a quantity of data that is fully out of reach of HIVE-COTE.},
  archive      = {J_DMKD},
  author       = {Ismail Fawaz, Hassan and Lucas, Benjamin and Forestier, Germain and Pelletier, Charlotte and Schmidt, Daniel F. and Weber, Jonathan and Webb, Geoffrey I. and Idoumghar, Lhassane and Muller, Pierre-Alain and Petitjean, François},
  doi          = {10.1007/s10618-020-00710-y},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {11},
  number       = {6},
  pages        = {1936-1962},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {InceptionTime: Finding AlexNet for time series classification},
  volume       = {34},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Bayesian mean-parameterized nonnegative binary matrix
factorization. <em>DMKD</em>, <em>34</em>(6), 1898–1935. (<a
href="https://doi.org/10.1007/s10618-020-00712-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Binary data matrices can represent many types of data such as social networks, votes, or gene expression. In some cases, the analysis of binary matrices can be tackled with nonnegative matrix factorization (NMF), where the observed data matrix is approximated by the product of two smaller nonnegative matrices. In this context, probabilistic NMF assumes a generative model where the data is usually Bernoulli-distributed. Often, a link function is used to map the factorization to the [0, 1] range, ensuring a valid Bernoulli mean parameter. However, link functions have the potential disadvantage to lead to uninterpretable models. Mean-parameterized NMF, on the contrary, overcomes this problem. We propose a unified framework for Bayesian mean-parameterized nonnegative binary matrix factorization models (NBMF). We analyze three models which correspond to three possible constraints that respect the mean-parameterization without the need for link functions. Furthermore, we derive a novel collapsed Gibbs sampler and a collapsed variational algorithm to infer the posterior distribution of the factors. Next, we extend the proposed models to a nonparametric setting where the number of used latent dimensions is automatically driven by the observed data. We analyze the performance of our NBMF methods in multiple datasets for different tasks such as dictionary learning and prediction of missing data. Experiments show that our methods provide similar or superior results than the state of the art, while automatically detecting the number of relevant components.},
  archive      = {J_DMKD},
  author       = {Lumbreras, Alberto and Filstroff, Louis and Févotte, Cédric},
  doi          = {10.1007/s10618-020-00712-w},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {11},
  number       = {6},
  pages        = {1898-1935},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Bayesian mean-parameterized nonnegative binary matrix factorization},
  volume       = {34},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). MIDIA: Exploring denoising autoencoders for missing data
imputation. <em>DMKD</em>, <em>34</em>(6), 1859–1897. (<a
href="https://doi.org/10.1007/s10618-020-00706-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to the ubiquitous presence of missing values (MVs) in real-world datasets, the MV imputation problem, aiming to recover MVs, is an important and fundamental data preprocessing step for various data analytics and mining tasks to effectively achieve good performance. To impute MVs, a typical idea is to explore the correlations amongst the attributes of the data. However, those correlations are usually complex and thus difficult to identify. Accordingly, we develop a new deep learning model called MIssing Data Imputation denoising Autoencoder (MIDIA) that effectively imputes the MVs in a given dataset by exploring non-linear correlations between missing values and non-missing values. Additionally, by considering various data missing patterns, we propose two effective MV imputation approaches based on the proposed MIDIA model, namely MIDIA-Sequential and MIDIA-Batch. MIDIA-Sequential imputes the MVs attribute-by-attribute sequentially by training an independent MIDIA model for each incomplete attribute. By contrast, MIDIA-Batch imputes the MVs in one batch by training a uniform MIDIA model. Finally, we evaluate the proposed approaches by experimentation in comparison with existing MV imputation algorithms. The experimental results demonstrate that both MIDIA-Sequential and MIDIA-Batch achieve significantly higher imputation accuracy compared with existing solutions, and the proposed approaches are capable of handling various data missing patterns and data types. Specifically, MIDIA-Sequential performs better than MIDIA-Batch for data with monotone missing pattern, while MIDIA-Batch performs better than MIDIA-Sequential for data with general missing pattern.},
  archive      = {J_DMKD},
  author       = {Ma, Qian and Lee, Wang-Chien and Fu, Tao-Yang and Gu, Yu and Yu, Ge},
  doi          = {10.1007/s10618-020-00706-8},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {11},
  number       = {6},
  pages        = {1859-1897},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {MIDIA: Exploring denoising autoencoders for missing data imputation},
  volume       = {34},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Challenges in benchmarking stream learning algorithms with
real-world data. <em>DMKD</em>, <em>34</em>(6), 1805–1858. (<a
href="https://doi.org/10.1007/s10618-020-00698-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Streaming data are increasingly present in real-world applications such as sensor measurements, satellite data feed, stock market, and financial data. The main characteristics of these applications are the online arrival of data observations at high speed and the susceptibility to changes in the data distributions due to the dynamic nature of real environments. The data stream mining community still faces some primary challenges and difficulties related to the comparison and evaluation of new proposals, mainly due to the lack of publicly available high quality non-stationary real-world datasets. The comparison of stream algorithms proposed in the literature is not an easy task, as authors do not always follow the same recommendations, experimental evaluation procedures, datasets, and assumptions. In this paper, we mitigate problems related to the choice of datasets in the experimental evaluation of stream classifiers and drift detectors. To that end, we propose a new public data repository for benchmarking stream algorithms with real-world data. This repository contains the most popular datasets from literature and new datasets related to a highly relevant public health problem that involves the recognition of disease vector insects using optical sensors. The main advantage of these new datasets is the prior knowledge of their characteristics and patterns of changes to adequately evaluate new adaptive algorithms. We also present an in-depth discussion about the characteristics, reasons, and issues that lead to different types of changes in data distribution, as well as a critical review of common problems concerning the current benchmark datasets available in the literature.},
  archive      = {J_DMKD},
  author       = {Souza, Vinicius M. A. and dos Reis, Denis M. and Maletzke, André G. and Batista, Gustavo E. A. P. A.},
  doi          = {10.1007/s10618-020-00698-5},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {11},
  number       = {6},
  pages        = {1805-1858},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Challenges in benchmarking stream learning algorithms with real-world data},
  volume       = {34},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Visualizing image content to explain novel image discovery.
<em>DMKD</em>, <em>34</em>(6), 1777–1804. (<a
href="https://doi.org/10.1007/s10618-020-00700-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The initial analysis of any large data set can be divided into two phases: (1) the identification of common trends or patterns and (2) the identification of anomalies or outliers that deviate from those trends. We focus on the goal of detecting observations with novel content, which can alert us to artifacts in the data set or, potentially, the discovery of previously unknown phenomena. To aid in interpreting and diagnosing the novel aspect of these selected observations, we recommend the use of novelty detection methods that generate explanations. In the context of large image data sets, these explanations should highlight what aspect of a given image is new (color, shape, texture, content) in a human-comprehensible form. We propose DEMUD-VIS, the first method for providing visual explanations of novel image content by employing a convolutional neural network (CNN) to extract image features, a method that uses reconstruction error to detect novel content, and an up-convolutional network to convert CNN feature representations back into image space. We demonstrate this approach on diverse images from ImageNet, freshwater streams, and the surface of Mars. Finally, we evaluate the utility of the visual explanations with a user study.},
  archive      = {J_DMKD},
  author       = {Lee, Jake H. and Wagstaff, Kiri L.},
  doi          = {10.1007/s10618-020-00700-0},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {11},
  number       = {6},
  pages        = {1777-1804},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Visualizing image content to explain novel image discovery},
  volume       = {34},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Credible seed identification for large-scale structural
network alignment. <em>DMKD</em>, <em>34</em>(6), 1744–1776. (<a
href="https://doi.org/10.1007/s10618-020-00699-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Structural network alignment utilizes the topological structure information to find correspondences between nodes of two networks. Researchers have proposed a line of useful algorithms which usually require a prior mapping of seeds acting as landmark points to align the rest nodes. Several seed-free algorithms are developed to solve the cold-start problem. However, existing approaches suffer high computational cost and low reliability, limiting their applications to large-scale network alignment. Moreover, there is a lack of useful metrics to quantify the credibility of seed mappings. To address these issues, we propose a credible seed identification framework and develop a metric to assess the reliability of a mapping. To tackle the cold-start problem, we employ graph embedding techniques to represent nodes by structural feature vectors in a latent space. We then leverage point set registration algorithms to match nodes algebraically and obtain an initial mapping of nodes. Besides, we propose a heuristic algorithm to improve the credibility of the initial mapping by filtering out mismatched node pairs. To tackle the computational problem in large-scale network alignment, we propose a divide-and-conquer scheme to divide large networks into smaller ones and then match them individually. It significantly improves the recall of mapping results. Finally, we conduct extensive experiments to evaluate the effectiveness and efficiency of our new approach. The results illustrate that the proposed method outperforms the state-of-the-art approaches in terms of both effectiveness and efficiency.},
  archive      = {J_DMKD},
  author       = {Wang, Chenxu and Wang, Yang and Zhao, Zhiyuan and Qin, Dong and Luo, Xiapu and Qin, Tao},
  doi          = {10.1007/s10618-020-00699-4},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {11},
  number       = {6},
  pages        = {1744-1776},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Credible seed identification for large-scale structural network alignment},
  volume       = {34},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Introducing time series snippets: A new primitive for
summarizing long time series. <em>DMKD</em>, <em>34</em>(6), 1713–1743.
(<a href="https://doi.org/10.1007/s10618-020-00702-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The first question a data analyst asks when confronting a new dataset is often, “Show me some representative/typical data.” Answering this question is simple in many domains, with random samples or aggregate statistics of some kind. Surprisingly, it is difficult for large time series datasets. The major difficulty is not time or space complexity, but defining what it means to be representative data for this data type. In this work, we show that the obvious candidate definitions: motifs, shapelets, cluster centers, random samples etc., are all poor choices. We introduce time series snippets, a novel representation of typical time series subsequences. Informally, time series snippets can be seen as the answer to the following question. If a user, which could be a human or a higher-level algorithm, only has resources (including human time) to inspect k subsequences of a long time series, which k subsequences should be chosen? Beyond their utility for visualizing and summarizing massive time series collections, we show that time series snippets have utility for high-level comparison of large time series collections.},
  archive      = {J_DMKD},
  author       = {Imani, Shima and Madrid, Frank and Ding, Wei and Crouter, Scott E. and Keogh, Eamonn},
  doi          = {10.1007/s10618-020-00702-y},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {11},
  number       = {6},
  pages        = {1713-1743},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Introducing time series snippets: A new primitive for summarizing long time series},
  volume       = {34},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Gaussian bandwidth selection for manifold learning and
classification. <em>DMKD</em>, <em>34</em>(6), 1676–1712. (<a
href="https://doi.org/10.1007/s10618-020-00692-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Kernel methods play a critical role in many machine learning algorithms. They are useful in manifold learning, classification, clustering and other data analysis tasks. Setting the kernel’s scale parameter, also referred to as the kernel’s bandwidth, highly affects the performance of the task in hand. We propose to set a scale parameter that is tailored to one of two types of tasks: classification and manifold learning. For manifold learning, we seek a scale which is best at capturing the manifold’s intrinsic dimension. For classification, we propose three methods for estimating the scale, which optimize the classification results in different senses. The proposed frameworks are simulated on artificial and on real datasets. The results show a high correlation between optimal classification rates and the estimated scales. Finally, we demonstrate the approach on a seismic event classification task.},
  archive      = {J_DMKD},
  author       = {Lindenbaum, Ofir and Salhov, Moshe and Yeredor, Arie and Averbuch, Amir},
  doi          = {10.1007/s10618-020-00692-x},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {11},
  number       = {6},
  pages        = {1676-1712},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Gaussian bandwidth selection for manifold learning and classification},
  volume       = {34},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Comparison of novelty detection methods for multispectral
images in rover-based planetary exploration missions. <em>DMKD</em>,
<em>34</em>(6), 1642–1675. (<a
href="https://doi.org/10.1007/s10618-020-00697-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Science teams for rover-based planetary exploration missions like the Mars Science Laboratory Curiosity rover have limited time for analyzing new data before making decisions about follow-up observations. There is a need for systems that can rapidly and intelligently extract information from planetary instrument datasets and focus attention on the most promising or novel observations. Several novelty detection methods have been explored in prior work for three-channel color images and non-image datasets, but few have considered multispectral or hyperspectral image datasets for the purpose of scientific discovery. We compared the performance of four novelty detection methods—Reed Xiaoli (RX) detectors, principal component analysis (PCA), autoencoders, and generative adversarial networks (GANs)—and the ability of each method to provide explanatory visualizations to help scientists understand and trust predictions made by the system. We show that pixel-wise RX and autoencoders trained with structural similarity (SSIM) loss can detect morphological novelties that are not detected by PCA, GANs, and mean squared error autoencoders, but that the latter methods are better suited for detecting spectral novelties—i.e., the best method for a given setting depends on the type of novelties that are sought. Additionally, we find that autoencoders provide the most useful explanatory visualizations for enabling users to understand and trust model detections, and that existing GAN approaches to novelty detection may be limited in this respect.},
  archive      = {J_DMKD},
  author       = {Kerner, Hannah R. and Wagstaff, Kiri L. and Bue, Brian D. and Wellington, Danika F. and Jacob, Samantha and Horton, Paul and Bell, James F. and Kwan, Chiman and Ben Amor, Heni},
  doi          = {10.1007/s10618-020-00697-6},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {11},
  number       = {6},
  pages        = {1642-1675},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Comparison of novelty detection methods for multispectral images in rover-based planetary exploration missions},
  volume       = {34},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). ColluEagle: Collusive review spammer detection using markov
random fields. <em>DMKD</em>, <em>34</em>(6), 1621–1641. (<a
href="https://doi.org/10.1007/s10618-020-00693-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Product reviews are extremely valuable for online shoppers in providing purchase decisions. Driven by immense profit incentives, fraudsters deliberately fabricate untruthful reviews to distort the reputation of online products. As online reviews become more and more important, group spamming, i.e., a team of fraudsters working collaboratively to attack a set of target products, becomes a new fashion. Previous works use review network effects, i.e. the relationships among reviewers, reviews, and products, to detect fake reviews or review spammers, but ignore time effects, which are critical in characterizing group spamming. In this paper, we propose a novel Markov random field (MRF)-based method (ColluEagle) to detect collusive review spammers, as well as review spam campaigns, considering both network effects and time effects. First we identify co-review pairs, a review phenomenon that happens between two reviewers who review a common product in a similar way, and then model reviewers and their co-review pairs as a pairwise-MRF, and use loopy belief propagation to evaluate the suspiciousness of reviewers. We further design a high quality yet easy-to-compute node prior for ColluEagle, through which the review spammer groups can also be subsequently identified. Experiments show that ColluEagle can not only detect collusive spammers with high precision, significantly outperforming state-of-the-art baselines—FraudEagle and SpEagle, but also identify highly suspicious review spammer campaigns.},
  archive      = {J_DMKD},
  author       = {Wang, Zhuo and Hu, Runlong and Chen, Qian and Gao, Pei and Xu, Xiaowei},
  doi          = {10.1007/s10618-020-00693-w},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {11},
  number       = {6},
  pages        = {1621-1641},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {ColluEagle: Collusive review spammer detection using markov random fields},
  volume       = {34},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). CrawlSN: Community-aware data acquisition with maximum
willingness in online social networks. <em>DMKD</em>, <em>34</em>(5),
1589–1620. (<a
href="https://doi.org/10.1007/s10618-020-00709-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Real social network datasets with community structures are critical for evaluating various algorithms in Online Social Networks (OSNs). However, obtaining such community data from OSNs has recently become increasingly challenging due to privacy issues and government regulations. In this paper, we thus make our first attempt to address two important factors, i.e., user willingness and existence of community structure, to obtain more complete OSN data. We formulate a new research problem, namely Community-aware Data Acquisition with Maximum Willingness in Online Social Networks (CrawlSN), to identify a group of users from an OSN, such that the group is a socially tight community and the users’ willingness to contribute data is maximized. We prove that CrawlSN is NP-hard and inapproximable within any factor unless, and propose an effective algorithm, named Community-aware Group Identification with Maximum Willingness (CIW) with various processing strategies. We conduct an evaluation study with 1093 volunteers to validate our problem formulation and demonstrate that CrawlSN outperforms the other alternatives. We also perform extensive experiments on 7 real datasets and show that the proposed CIW outperforms the other baselines in both solution quality and efficiency.},
  archive      = {J_DMKD},
  author       = {Hsu, Bay-Yuan and Tu, Chia-Lin and Chang, Ming-Yi and Shen, Chih-Ya},
  doi          = {10.1007/s10618-020-00709-5},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {9},
  number       = {5},
  pages        = {1589-1620},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {CrawlSN: Community-aware data acquisition with maximum willingness in online social networks},
  volume       = {34},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Simple and effective neural-free soft-cluster embeddings for
item cold-start recommendations. <em>DMKD</em>, <em>34</em>(5),
1560–1588. (<a
href="https://doi.org/10.1007/s10618-020-00708-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recommender systems are widely used in online platforms for easy exploration of personalized content. The best available recommendation algorithms are based on using the observed preference information among collaborating entities. A significant challenge in recommender system continues to be item cold-start recommendation: how to effectively recommend items with no observed or past preference information. Here we propose a two-stage algorithm based on soft clustering to provide an efficient solution to this problem. The crux of our approach lies in representing the items as soft-cluster embeddings in the space spanned by the side-information associated with the items. Though many item embedding approaches have been proposed for item cold-start recommendations in the past—and simple as they might appear—to the best of our knowledge, the approach based on soft-cluster embeddings has not been proposed in the research literature. Our experimental results on four benchmark datasets conclusively demonstrate that the proposed algorithm makes accurate recommendations in item cold-start settings compared to the state-of-the-art algorithms according to commonly used ranking metrics like Normalized Discounted Cumulative Gain (NDCG) and Mean Average Precision (MAP). The performance of our proposed algorithm on the MovieLens 20M dataset clearly demonstrates the scalability aspect of our algorithm compared to other popular algorithms. We also propose the metric Cold Items Precision (CIP) to quantify the ability of a system to recommend cold-start items. CIP can be used in conjunction with relevance ranking metrics like NDCG and MAP to measure the effectiveness of the cold-start recommendation algorithm.},
  archive      = {J_DMKD},
  author       = {Puthiya Parambath, Shameem A. and Chawla, Sanjay},
  doi          = {10.1007/s10618-020-00708-6},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {9},
  number       = {5},
  pages        = {1560-1588},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Simple and effective neural-free soft-cluster embeddings for item cold-start recommendations},
  volume       = {34},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Deep soccer analytics: Learning an action-value function for
evaluating soccer players. <em>DMKD</em>, <em>34</em>(5), 1531–1559. (<a
href="https://doi.org/10.1007/s10618-020-00705-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Given the large pitch, numerous players, limited player turnovers, and sparse scoring, soccer is arguably the most challenging to analyze of all the major team sports. In this work, we develop a new approach to evaluating all types of soccer actions from play-by-play event data. Our approach utilizes a Deep Reinforcement Learning (DRL) model to learn an action-value Q-function. To our knowledge, this is the first action-value function based on DRL methods for a comprehensive set of soccer actions. Our neural architecture fits continuous game context signals and sequential features within a play with two stacked LSTM towers, one for the home team and one for the away team separately. To validate the model performance, we illustrate both temporal and spatial projections of the learned Q-function, and conduct a calibration experiment to study the data fit under different game contexts. Our novel soccer Goal Impact Metric (GIM) applies values from the learned Q-function, to measure a player’s overall performance by the aggregate impact values of his actions over all the games in a season. To interpret the impact values, a mimic regression tree is built to find the game features that influence the values most. As an application of our GIM metric, we conduct a case study to rank players in the English Football League Championship. Empirical evaluation indicates GIM is a temporally stable metric, and its correlations with standard measures of soccer success are higher than that computed with other state-of-the-art soccer metrics.},
  archive      = {J_DMKD},
  author       = {Liu, Guiliang and Luo, Yudong and Schulte, Oliver and Kharrat, Tarak},
  doi          = {10.1007/s10618-020-00705-9},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {9},
  number       = {5},
  pages        = {1531-1559},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Deep soccer analytics: Learning an action-value function for evaluating soccer players},
  volume       = {34},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Active learning for hierarchical multi-label classification.
<em>DMKD</em>, <em>34</em>(5), 1496–1530. (<a
href="https://doi.org/10.1007/s10618-020-00704-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to technological advances, a massive amount of data is produced daily, presenting challenges for application areas where data needs to be labelled by a domain specialist or by expensive procedures, in order to be useful for supervised machine learning purposes. In order to select which data points will provide more information when labelled, one can make use of active learning methods. Active learning (AL) is a subfield of machine learning which addresses methods to build models with fewer, but more representative instances. Even though AL has been vastly studied, it has not been thoroughly investigated in hierarchical multi-label classification, a learning task where multiple class labels can be assigned to an instance and these labels are hierarchically structured. In this work, we provide a public framework containing baseline and state-of-the-art algorithms suitable for this task. Additionally, we also propose a new algorithm, namely Hierarchical Query-By-Committee (H-QBC), which is validated on datasets from different domains. Our results show that H-QBC is capable of providing superior predictive performance results compared to its competitors, while being computationally efficient and parameter free.},
  archive      = {J_DMKD},
  author       = {Nakano, Felipe Kenji and Cerri, Ricardo and Vens, Celine},
  doi          = {10.1007/s10618-020-00704-w},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {9},
  number       = {5},
  pages        = {1496-1530},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Active learning for hierarchical multi-label classification},
  volume       = {34},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). ROCKET: Exceptionally fast and accurate time series
classification using random convolutional kernels. <em>DMKD</em>,
<em>34</em>(5), 1454–1495. (<a
href="https://doi.org/10.1007/s10618-020-00701-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most methods for time series classification that attain state-of-the-art accuracy have high computational complexity, requiring significant training time even for smaller datasets, and are intractable for larger datasets. Additionally, many existing methods focus on a single type of feature such as shape or frequency. Building on the recent success of convolutional neural networks for time series classification, we show that simple linear classifiers using random convolutional kernels achieve state-of-the-art accuracy with a fraction of the computational expense of existing methods. Using this method, it is possible to train and test a classifier on all 85 ‘bake off’ datasets in the UCR archive in $$&lt;\,2\,\hbox {h}$$ , and it is possible to train a classifier on a large dataset of more than one million time series in approximately 1 h.},
  archive      = {J_DMKD},
  author       = {Dempster, Angus and Petitjean, François and Webb, Geoffrey I.},
  doi          = {10.1007/s10618-020-00701-z},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {9},
  number       = {5},
  pages        = {1454-1495},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {ROCKET: Exceptionally fast and accurate time series classification using random convolutional kernels},
  volume       = {34},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Large-scale network motif analysis using compression.
<em>DMKD</em>, <em>34</em>(5), 1421–1453. (<a
href="https://doi.org/10.1007/s10618-020-00691-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce a new method for finding network motifs. Subgraphs are motifs when their frequency in the data is high compared to the expected frequency under a null model. To compute this expectation, a full or approximate count of the occurrences of a motif is normally repeated on as many as 1000 random graphs sampled from the null model; a prohibitively expensive step. We use ideas from the minimum description length literature to define a new measure of motif relevance. With our method, samples from the null model are not required. Instead we compute the probability of the data under the null model and compare this to the probability under a specially designed alternative model. With this new relevance test, we can search for motifs by random sampling, rather than requiring an accurate count of all instances of a motif. This allows motif analysis to scale to networks with billions of links.},
  archive      = {J_DMKD},
  author       = {Bloem, Peter and de Rooij, Steven},
  doi          = {10.1007/s10618-020-00691-y},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {9},
  number       = {5},
  pages        = {1421-1453},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Large-scale network motif analysis using compression},
  volume       = {34},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Treant: Training evasion-aware decision trees.
<em>DMKD</em>, <em>34</em>(5), 1390–1420. (<a
href="https://doi.org/10.1007/s10618-020-00694-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Despite its success and popularity, machine learning is now recognized as vulnerable to evasion attacks, i.e., carefully crafted perturbations of test inputs designed to force prediction errors. In this paper we focus on evasion attacks against decision tree ensembles, which are among the most successful predictive models for dealing with non-perceptual problems. Even though they are powerful and interpretable, decision tree ensembles have received only limited attention by the security and machine learning communities so far, leading to a sub-optimal state of the art for adversarial learning techniques. We thus propose Treant, a novel decision tree learning algorithm that, on the basis of a formal threat model, minimizes an evasion-aware loss function at each step of the tree construction. Treant is based on two key technical ingredients: robust splitting and attack invariance, which jointly guarantee the soundness of the learning process. Experimental results on publicly available datasets show that Treant is able to generate decision tree ensembles that are at the same time accurate and nearly insensitive to evasion attacks, outperforming state-of-the-art adversarial learning techniques.},
  archive      = {J_DMKD},
  author       = {Calzavara, Stefano and Lucchese, Claudio and Tolomei, Gabriele and Abebe, Seyum Assefa and Orlando, Salvatore},
  doi          = {10.1007/s10618-020-00694-9},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {9},
  number       = {5},
  pages        = {1390-1420},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Treant: Training evasion-aware decision trees},
  volume       = {34},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Scalable attack on graph data by injecting vicious nodes.
<em>DMKD</em>, <em>34</em>(5), 1363–1389. (<a
href="https://doi.org/10.1007/s10618-020-00696-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent studies have shown that graph convolution networks (GCNs) are vulnerable to carefully designed attacks, which aim to cause misclassification of a specific node on the graph with unnoticeable perturbations. However, a vast majority of existing works cannot handle large-scale graphs because of their high time complexity. Additionally, existing works mainly focus on manipulating existing nodes on the graph, while in practice, attackers usually do not have the privilege to modify information of existing nodes. In this paper, we develop a more scalable framework named Approximate Fast Gradient Sign Method which considers a more practical attack scenario where adversaries can only inject new vicious nodes to the graph while having no control over the original graph. Methodologically, we provide an approximation strategy to linearize the model we attack and then derive an approximate closed-from solution with a lower time cost. To have a fair comparison with existing attack methods that manipulate the original graph, we adapt them to the new attack scenario by injecting vicious nodes. Empirical experimental results show that our proposed attack method can significantly reduce the classification accuracy of GCNs and is much faster than existing methods without jeopardizing the attack performance. We have open-sourced the code of our method https://github.com/wangjhgithub/AFGSM .},
  archive      = {J_DMKD},
  author       = {Wang, Jihong and Luo, Minnan and Suya, Fnu and Li, Jundong and Yang, Zijiang and Zheng, Qinghua},
  doi          = {10.1007/s10618-020-00696-7},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {9},
  number       = {5},
  pages        = {1363-1389},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Scalable attack on graph data by injecting vicious nodes},
  volume       = {34},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). TEASER: Early and accurate time series classification.
<em>DMKD</em>, <em>34</em>(5), 1336–1362. (<a
href="https://doi.org/10.1007/s10618-020-00690-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Early time series classification (eTSC) is the problem of classifying a time series after as few measurements as possible with the highest possible accuracy. The most critical issue of any eTSC method is to decide when enough data of a time series has been seen to take a decision: Waiting for more data points usually makes the classification problem easier but delays the time in which a classification is made; in contrast, earlier classification has to cope with less input data, often leading to inferior accuracy. The state-of-the-art eTSC methods compute a fixed optimal decision time assuming that every times series has the same defined start time (like turning on a machine). However, in many real-life applications measurements start at arbitrary times (like measuring heartbeats of a patient), implying that the best time for taking a decision varies widely between time series. We present TEASER, a novel algorithm that models eTSC as a two-tier classification problem: In the first tier, a classifier periodically assesses the incoming time series to compute class probabilities. However, these class probabilities are only used as output label if a second-tier classifier decides that the predicted label is reliable enough, which can happen after a different number of measurements. In an evaluation using 45 benchmark datasets, TEASER is two to three times earlier at predictions than its competitors while reaching the same or an even higher classification accuracy. We further show TEASER’s superior performance using real-life use cases, namely energy monitoring, and gait detection.},
  archive      = {J_DMKD},
  author       = {Schäfer, Patrick and Leser, Ulf},
  doi          = {10.1007/s10618-020-00690-z},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {9},
  number       = {5},
  pages        = {1336-1362},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {TEASER: Early and accurate time series classification},
  volume       = {34},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Fair-by-design matching. <em>DMKD</em>, <em>34</em>(5),
1291–1335. (<a
href="https://doi.org/10.1007/s10618-020-00675-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Matching algorithms are used routinely to match donors to recipients for solid organs transplantation, for the assignment of medical residents to hospitals, record linkage in databases, scheduling jobs on machines, network switching, online advertising, and image recognition, among others. Although many optimal solutions may exist to a given matching problem, when the elements that shall or not be included in a solution correspond to individuals, it becomes of paramount importance that the solution is selected fairly. In this paper we study individual fairness in matching problems. Given that many maximum matchings may exist, each one satisfying a different set of individuals, the only way to guarantee fairness is through randomization. Hence we introduce the distributional maxmin fairness framework which provides, for any given input instance, the strongest guarantee possible simultaneously for all individuals in terms of satisfaction probability (the probability of being matched in the solution). Specifically, a probability distribution over feasible solutions is maxmin-fair if it is not possible to improve the satisfaction probability of any individual without decreasing it for some other individual which is no better off. Our main contribution is a polynomial-time algorithm building on techniques from minimum cuts, and edge-coloring algorithms for regular bipartite graphs, and transversal theory. In the special case of bipartite matching, our algorithm runs in $$O((|V|^2 + |E| |V|^{2/3})\cdot (\log |V|)^2)$$ expected time. An experimental evaluation of our fair-matching algorithm shows its ability to scale to graphs with tens of millions of vertices and hundreds of millions of edges, taking only a few minutes on a simple architecture. To the best of our knowledge, this yields the first large-scale implementation of the egalitarian mechanism of Bogomolnaia and Moulin (Econometrica 72(1):257–279, 2004). Our analysis confirms that our method provides stronger satisfaction probability guarantees than non-trivial baselines.},
  archive      = {J_DMKD},
  author       = {García-Soriano, David and Bonchi, Francesco},
  doi          = {10.1007/s10618-020-00675-y},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {9},
  number       = {5},
  pages        = {1291-1335},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Fair-by-design matching},
  volume       = {34},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Exceptional spatio-temporal behavior mining through bayesian
non-parametric modeling. <em>DMKD</em>, <em>34</em>(5), 1267–1290. (<a
href="https://doi.org/10.1007/s10618-020-00674-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Collective social media provides a vast amount of geo-tagged social posts, which contain various records on spatio-temporal behavior. Modeling spatio-temporal behavior on collective social media is an important task for applications like tourism recommendation, location prediction and urban planning. Properly accomplishing this task requires a model that allows for diverse behavioral patterns on each of the three aspects: spatial location, time, and text. In this paper, we address the following question: how to find representative subgroups of social posts, for which the spatio-temporal behavioral patterns are substantially different from the behavioral patterns in the whole dataset? Selection and evaluation are the two challenging problems for finding the exceptional subgroups. To address these problems, we propose BNPM: a Bayesian non-parametric model, to model spatio-temporal behavior and infer the exceptionality of social posts in subgroups. By training BNPM on a large amount of randomly sampled subgroups, we can get the global distribution of behavioral patterns. For each given subgroup of social posts, its posterior distribution can be inferred by BNPM. By comparing the posterior distribution with the global distribution, we can quantify the exceptionality of each given subgroup. The exceptionality scores are used to guide the search process within the exceptional model mining framework to automatically discover the exceptional subgroups. Various experiments are conducted to evaluate the effectiveness and efficiency of our method. On four real-world datasets our method discovers subgroups coinciding with events, subgroups distinguishing professionals from tourists, and subgroups whose consistent exceptionality can only be truly appreciated by combining exceptional spatio-temporal and exceptional textual behavior.},
  archive      = {J_DMKD},
  author       = {Du, Xin and Pei, Yulong and Duivesteijn, Wouter and Pechenizkiy, Mykola},
  doi          = {10.1007/s10618-020-00674-z},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {9},
  number       = {5},
  pages        = {1267-1290},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Exceptional spatio-temporal behavior mining through bayesian non-parametric modeling},
  volume       = {34},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Delayed labelling evaluation for data streams.
<em>DMKD</em>, <em>34</em>(5), 1237–1266. (<a
href="https://doi.org/10.1007/s10618-019-00654-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A large portion of the stream mining studies on classification rely on the availability of true labels immediately after making predictions. This approach is well exemplified by the test-then-train evaluation, where predictions immediately precede true label arrival. However, in many real scenarios, labels arrive with non-negligible latency. This raises the question of how to evaluate classifiers trained in such circumstances. This question is of particular importance when stream mining models are expected to refine their predictions between acquiring instance data and receiving its true label. In this work, we propose a novel evaluation methodology for data streams when verification latency takes place, namely continuous re-evaluation. It is applied to reference data streams and it is used to differentiate between stream mining techniques in terms of their ability to refine predictions based on newly arriving instances. Our study points out, discusses and shows empirically the importance of considering the delay of instance labels when evaluating classifiers for data streams.},
  archive      = {J_DMKD},
  author       = {Grzenda, Maciej and Gomes, Heitor Murilo and Bifet, Albert},
  doi          = {10.1007/s10618-019-00654-y},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {9},
  number       = {5},
  pages        = {1237-1266},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Delayed labelling evaluation for data streams},
  volume       = {34},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Introduction to the special issue of the ECML PKDD 2020
journal track. <em>DMKD</em>, <em>34</em>(5), 1235–1236. (<a
href="https://doi.org/10.1007/s10618-020-00713-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_DMKD},
  author       = {Assent, Ira and Domeniconi, Carlotta and Gionis, Aristides and Hüllermeier, Eyke},
  doi          = {10.1007/s10618-020-00713-9},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {9},
  number       = {5},
  pages        = {1235-1236},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Introduction to the special issue of the ECML PKDD 2020 journal track},
  volume       = {34},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Efficient mining of the most significant patterns with
permutation testing. <em>DMKD</em>, <em>34</em>(4), 1201–1234. (<a
href="https://doi.org/10.1007/s10618-020-00687-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The extraction of patterns displaying significant association with a class label is a key data mining task with wide application in many domains. We introduce and study a variant of the problem that requires to mine the top-k statistically significant patterns, thus providing tight control on the number of patterns reported in output. We develop TopKWY, the first algorithm to mine the top-k significant patterns while rigorously controlling the family-wise error rate of the output, and provide theoretical evidence of its effectiveness. TopKWY crucially relies on a novel strategy to explore statistically significant patterns and on several key implementation choices, which may be of independent interest. Our extensive experimental evaluation shows that TopKWY enables the extraction of the most significant patterns from large datasets which could not be analyzed by the state-of-the-art. In addition, TopKWY improves over the state-of-the-art even for the extraction of all significant patterns.},
  archive      = {J_DMKD},
  author       = {Pellegrina, Leonardo and Vandin, Fabio},
  doi          = {10.1007/s10618-020-00687-8},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {7},
  number       = {4},
  pages        = {1201-1234},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Efficient mining of the most significant patterns with permutation testing},
  volume       = {34},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). ABBA: Adaptive brownian bridge-based symbolic aggregation of
time series. <em>DMKD</em>, <em>34</em>(4), 1175–1200. (<a
href="https://doi.org/10.1007/s10618-020-00689-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A new symbolic representation of time series, called ABBA, is introduced. It is based on an adaptive polygonal chain approximation of the time series into a sequence of tuples, followed by a mean-based clustering to obtain the symbolic representation. We show that the reconstruction error of this representation can be modelled as a random walk with pinned start and end points, a so-called Brownian bridge. This insight allows us to make ABBA essentially parameter-free, except for the approximation tolerance which must be chosen. Extensive comparisons with the SAX and 1d-SAX representations are included in the form of performance profiles, showing that ABBA is often able to better preserve the essential shape information of time series compared to other approaches, in particular when time warping measures are used. Advantages and applications of ABBA are discussed, including its in-built differencing property and use for anomaly detection, and Python implementations provided.},
  archive      = {J_DMKD},
  author       = {Elsworth, Steven and Güttel, Stefan},
  doi          = {10.1007/s10618-020-00689-6},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {7},
  number       = {4},
  pages        = {1175-1200},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {ABBA: Adaptive brownian bridge-based symbolic aggregation of time series},
  volume       = {34},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). TEAGS: Time-aware text embedding approach to generate
subgraphs. <em>DMKD</em>, <em>34</em>(4), 1136–1174. (<a
href="https://doi.org/10.1007/s10618-020-00688-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Contagions (e.g. virus and gossip) spread over the nodes in propagation graphs. We can use temporal-textual contents of nodes to compute the edge weights and generate subgraphs with highly relevant nodes. This is beneficial to many applications. Yet, challenges abound. First, the propagation pattern between each pair of nodes may change by time. Second, not always the same contagion propagates. Hence, current text mining approaches including topic-modeling cannot effectively compute the edge weights. Third, since the propagation is affected by time, the word–word co-occurrence patterns may differ in various temporal dimensions which adversely impacts the performance of word embedding approaches. We argue that multi-aspect temporal dimensions (hour, day, etc) should be considered to better calculate the correlation weights between the nodes. In this work, we devise a novel framework that on the one hand, integrates a time-aware word embedding component to construct the word vectors through multiple temporal facets, and on the other hand, uses a time-only multi-facet generative model to compute the weights. Subsequently, we propose a Max-Heap Graph cutting algorithm to generate subgraphs. We validate our model through experiments on real-world datasets. The results show that our model can generate the subgraphs more effective than other rivals and temporal dynamics must be adhered in the modeling of the dynamical processes.},
  archive      = {J_DMKD},
  author       = {Hosseini, Saeid and Najafipour, Saeed and Cheung, Ngai-Man and Yin, Hongzhi and Kangavari, Mohammad Reza and Zhou, Xiaofang},
  doi          = {10.1007/s10618-020-00688-7},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {7},
  number       = {4},
  pages        = {1136-1174},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {TEAGS: Time-aware text embedding approach to generate subgraphs},
  volume       = {34},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). An ultra-fast time series distance measure to allow data
mining in more complex real-world deployments. <em>DMKD</em>,
<em>34</em>(4), 1104–1135. (<a
href="https://doi.org/10.1007/s10618-020-00695-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {At their core, many time series data mining algorithms reduce to reasoning about the shapes of time series subsequences. This requires an effective distance measure, and for last two decades most algorithms use Euclidean distance or DTW as their core subroutine. We argue that these distance measures are not as robust as the community seems to believe. The undue faith in these measures perhaps derives from an overreliance on the benchmark datasets and self-selection bias. The community is simply reluctant to address more difficult domains, for which current distance measures are ill-suited. In this work, we introduce a novel distance measure MPdist. We show that our proposed distance measure is much more robust than current distance measures. For example, it can handle data with missing values or spurious regions. Furthermore, it allows us to successfully mine datasets that would defeat any Euclidean or DTW distance-based algorithm. Additionally, we show that our distance measure can be computed so efficiently as to allow analytics on very fast arriving streams.},
  archive      = {J_DMKD},
  author       = {Gharghabi, Shaghayegh and Imani, Shima and Bagnall, Anthony and Darvishzadeh, Amirali and Keogh, Eamonn},
  doi          = {10.1007/s10618-020-00695-8},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {7},
  number       = {4},
  pages        = {1104-1135},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {An ultra-fast time series distance measure to allow data mining in more complex real-world deployments},
  volume       = {34},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Struc2gauss: Structural role preserving network embedding
via gaussian embedding. <em>DMKD</em>, <em>34</em>(4), 1072–1103. (<a
href="https://doi.org/10.1007/s10618-020-00684-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Network embedding (NE) is playing a principal role in network mining, due to its ability to map nodes into efficient low-dimensional embedding vectors. However, two major limitations exist in state-of-the-art NE methods: role preservation and uncertainty modeling. Almost all previous methods represent a node into a point in space and focus on local structural information, i.e., neighborhood information. However, neighborhood information does not capture global structural information and point vector representation fails in modeling the uncertainty of node representations. In this paper, we propose a new NE framework, struc2gauss, which learns node representations in the space of Gaussian distributions and performs network embedding based on global structural information. struc2gauss first employs a given node similarity metric to measure the global structural information, then generates structural context for nodes and finally learns node representations via Gaussian embedding. Different structural similarity measures of networks and energy functions of Gaussian embedding are investigated. Experiments conducted on real-world networks demonstrate that struc2gauss effectively captures global structural information while state-of-the-art network embedding methods fail to, outperforms other methods on the structure-based clustering and classification task and provides more information on uncertainties of node representations.},
  archive      = {J_DMKD},
  author       = {Pei, Yulong and Du, Xin and Zhang, Jianpeng and Fletcher, George and Pechenizkiy, Mykola},
  doi          = {10.1007/s10618-020-00684-x},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {7},
  number       = {4},
  pages        = {1072-1103},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Struc2gauss: Structural role preserving network embedding via gaussian embedding},
  volume       = {34},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Matrix profile goes MAD: Variable-length motif and discord
discovery in data series. <em>DMKD</em>, <em>34</em>(4), 1022–1071. (<a
href="https://doi.org/10.1007/s10618-020-00685-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the last 15 years, data series motif and discord discovery have emerged as two useful and well-used primitives for data series mining, with applications to many domains, including robotics, entomology, seismology, medicine, and climatology. Nevertheless, the state-of-the-art motif and discord discovery tools still require the user to provide the relative length. Yet, in several cases, the choice of length is critical and unforgiving. Unfortunately, the obvious brute-force solution, which tests all lengths within a given range, is computationally untenable. In this work, we introduce a new framework, which provides an exact and scalable motif and discord discovery algorithm that efficiently finds all motifs and discords in a given range of lengths. We evaluate our approach with five diverse real datasets, and demonstrate that it is up to 20 times faster than the state-of-the-art. Our results also show that removing the unrealistic assumption that the user knows the correct length, can often produce more intuitive and actionable results, which could have otherwise been missed.},
  archive      = {J_DMKD},
  author       = {Linardi, Michele and Zhu, Yan and Palpanas, Themis and Keogh, Eamonn},
  doi          = {10.1007/s10618-020-00685-w},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {7},
  number       = {4},
  pages        = {1022-1071},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Matrix profile goes MAD: Variable-length motif and discord discovery in data series},
  volume       = {34},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Counting frequent patterns in large labeled graphs: A
hypergraph-based approach. <em>DMKD</em>, <em>34</em>(4), 980–1021. (<a
href="https://doi.org/10.1007/s10618-020-00686-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, the popularity of graph databases has grown rapidly. This paper focuses on single-graph as an effective model to represent information and its related graph mining techniques. In frequent pattern mining in a single-graph setting, there are two main problems: support measure and search scheme. In this paper, we propose a novel framework for designing support measures that brings together existing minimum-image-based and overlap-graph-based support measures. Our framework is built on the concept of occurrence/instance hypergraphs. Based on such, we are able to design a series of new support measures: minimum instance (MI) measure, and minimum vertex cover (MVC) measure, that combine the advantages of existing measures. More importantly, we show that the existing minimum-image-based support measure is an upper bound of the MI measure, which is also linear-time computable and results in counts that are close to number of instances of a pattern. We show that not only most major existing support measures and new measures proposed in this paper can be mapped into the new framework, but also they occupy different locations of the frequency spectrum. By taking advantage of the new framework, we discover that MVC can be approximated to a constant factor (in terms of number of pattern nodes) in polynomial time. In contrast to common belief, we demonstrate that the state-of-the-art overlap-graph-based maximum independent set (MIS) measure also has constant approximation algorithms. We further show that using standard linear programming and semidefinite programming techniques, polynomial-time relaxations for both MVC and MIS measures can be developed and their counts stand between MVC and MIS. In addition, we point out that MVC, MIS, and their relaxations are bounded within constant factor. In summary, all major support measures are unified in the new hypergraph-based framework which helps reveal their bounding relations and hardness properties.},
  archive      = {J_DMKD},
  author       = {Meng, Jinghan and Pitaksirianan, Napath and Tu, Yi-Cheng},
  doi          = {10.1007/s10618-020-00686-9},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {7},
  number       = {4},
  pages        = {980-1021},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Counting frequent patterns in large labeled graphs: A hypergraph-based approach},
  volume       = {34},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). The swiss army knife of time series data mining: Ten useful
things you can do with the matrix profile and ten lines of code.
<em>DMKD</em>, <em>34</em>(4), 949–979. (<a
href="https://doi.org/10.1007/s10618-019-00668-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The recently introduced data structure, the Matrix Profile, annotates a time series by recording the location of and distance to the nearest neighbor of every subsequence. This information trivially provides answers to queries for both time series motifs and time series discords, perhaps two of the most frequently used primitives in time series data mining. One attractive feature of the Matrix Profile is that it completely divorces the high-level details of the analytics performed, from the computational “heavy lifting.” The Matrix Profile can be computed using the appropriate computational paradigm for the task at hand: CPU, GPU, FPGA, distributed computing, anytime computation, incremental computation, and so forth. However, all the details of such computation can be hidden from the analyst who only needs to think about her analytical need. In this work, we expand on this philosophy and ask the following question: If we assume that we get the Matrix Profile for free, what interesting analytics can we do, writing at most ten lines of code? As we will show, the answer is surprisingly large and diverse. Our aim here is not to establish or compete with state-of-the-art results, but merely to show that we can both reproduce the results of many existing algorithms and find novel regularities in time series data collections with very little effort.},
  archive      = {J_DMKD},
  author       = {Zhu, Yan and Gharghabi, Shaghayegh and Silva, Diego Furtado and Dau, Hoang Anh and Yeh, Chin-Chia Michael and Shakibay Senobari, Nader and Almaslukh, Abdulaziz and Kamgar, Kaveh and Zimmerman, Zachary and Funning, Gareth and Mueen, Abdullah and Keogh, Eamonn},
  doi          = {10.1007/s10618-019-00668-6},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {7},
  number       = {4},
  pages        = {949-979},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {The swiss army knife of time series data mining: Ten useful things you can do with the matrix profile and ten lines of code},
  volume       = {34},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Guided sampling for large graphs. <em>DMKD</em>,
<em>34</em>(4), 905–948. (<a
href="https://doi.org/10.1007/s10618-020-00683-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Large real-world graphs claim lots of resources in terms of memory and computational power to study them and this makes their full analysis extremely challenging. In order to understand the structure and properties of these graphs, we intend to extract a small representative subgraph from a big graph while preserving its topology and characteristics. In this work, we aim at producing good samples with sample size as low as 0.1% while maintaining the structure and some of the key properties of a network. We exploit the fact that average values of degree and clustering coefficient of a graph can be estimated accurately and efficiently. We use the estimated values to guide the sampling process and extract tiny samples that preserve the properties of the graph and closely approximate their distributions in the original graph. The distinguishing feature of our work is that we apply traversal based sampling that utilizes only the local information of nodes as opposed to the global information of the network and this makes our approach a practical choice for crawling online networks. We evaluate the effectiveness of our sampling technique using real-world datasets and show that it surpasses the existing methods.},
  archive      = {J_DMKD},
  author       = {Yousuf, Muhammad Irfan and Kim, Suhyun},
  doi          = {10.1007/s10618-020-00683-y},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {7},
  number       = {4},
  pages        = {905-948},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Guided sampling for large graphs},
  volume       = {34},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Ptype: Probabilistic type inference. <em>DMKD</em>,
<em>34</em>(3), 870–904. (<a
href="https://doi.org/10.1007/s10618-020-00680-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Type inference refers to the task of inferring the data type of a given column of data. Current approaches often fail when data contains missing data and anomalies, which are found commonly in real-world data sets. In this paper, we propose ptype, a probabilistic robust type inference method that allows us to detect such entries, and infer data types. We further show that the proposed method outperforms existing methods.},
  archive      = {J_DMKD},
  author       = {Ceritli, Taha and Williams, Christopher K. I. and Geddes, James},
  doi          = {10.1007/s10618-020-00680-1},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {5},
  number       = {3},
  pages        = {870-904},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Ptype: Probabilistic type inference},
  volume       = {34},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Computing exact p-values for community detection.
<em>DMKD</em>, <em>34</em>(3), 833–869. (<a
href="https://doi.org/10.1007/s10618-020-00681-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Community detection is one of the most important issues in modern network science. Although numerous community detection algorithms have been proposed during the past decades, how to assess the statistical significance of one single community analytically and exactly still remains an open problem. In this paper, we present an analytical solution to calculate the exact p-value of a single community with the Erdös–Rényi model. Meanwhile, we propose a local search method for finding statistically significant communities based on the p-value minimization. Experimental results on both real networks and simulated networks demonstrate that our method is able to effectively detect true communities from different types of networks.},
  archive      = {J_DMKD},
  author       = {He, Zengyou and Liang, Hao and Chen, Zheng and Zhao, Can and Liu, Yan},
  doi          = {10.1007/s10618-020-00681-0},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {5},
  number       = {3},
  pages        = {833-869},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Computing exact P-values for community detection},
  volume       = {34},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Discrete-time survival forests with hellinger distance
decision trees. <em>DMKD</em>, <em>34</em>(3), 812–832. (<a
href="https://doi.org/10.1007/s10618-020-00682-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Random survival forests (RSF) are a powerful nonparametric method for building prediction models with a time-to-event outcome. RSF do not rely on the proportional hazards assumption and can be readily applied to both low- and higher-dimensional data. A remaining limitation of RSF, however, arises from the fact that the method is almost entirely focussed on continuously measured event times. This issue may become problematic in studies where time is measured on a discrete scale $$t = 1, 2, ...$$ , referring to time intervals $$[0,a_1), [a_1,a_2), \ldots $$ . In this situation, the application of methods designed for continuous time-to-event data may lead to biased estimators and inaccurate predictions if discreteness is ignored. To address this issue, we develop a RSF algorithm that is specifically designed for the analysis of (possibly right-censored) discrete event times. The algorithm is based on an ensemble of discrete-time survival trees that operate on transformed versions of the original time-to-event data using tree methods for binary classification. As the outcome variable in these trees is typically highly imbalanced, our algorithm implements a node splitting strategy based on Hellinger’s distance, which is a skew-insensitive alternative to classical split criteria such as the Gini impurity. The new algorithm thus provides flexible nonparametric predictions of individual-specific discrete hazard and survival functions. Our numerical results suggest that node splitting by Hellinger’s distance improves predictive performance when compared to the Gini impurity. Furthermore, discrete-time RSF improve prediction accuracy when compared to RSF approaches treating discrete event times as continuous in situations where the number of time intervals is small.},
  archive      = {J_DMKD},
  author       = {Schmid, Matthias and Welchowski, Thomas and Wright, Marvin N. and Berger, Moritz},
  doi          = {10.1007/s10618-020-00682-z},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {5},
  number       = {3},
  pages        = {812-832},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Discrete-time survival forests with hellinger distance decision trees},
  volume       = {34},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). An efficient k-means clustering algorithm for tall data.
<em>DMKD</em>, <em>34</em>(3), 776–811. (<a
href="https://doi.org/10.1007/s10618-020-00678-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The analysis of continously larger datasets is a task of major importance in a wide variety of scientific fields. Therefore, the development of efficient and parallel algorithms to perform such an analysis is a a crucial topic in unsupervised learning. Cluster analysis algorithms are a key element of exploratory data analysis and, among them, the K-means algorithm stands out as the most popular approach due to its easiness in the implementation, straightforward parallelizability and relatively low computational cost. Unfortunately, the K-means algorithm also has some drawbacks that have been extensively studied, such as its high dependency on the initial conditions, as well as to the fact that it might not scale well on massive datasets. In this article, we propose a recursive and parallel approximation to the K-means algorithm that scales well on the number of instances of the problem, without affecting the quality of the approximation. In order to achieve this, instead of analyzing the entire dataset, we work on small weighted sets of representative points that are distributed in such a way that more importance is given to those regions where it is harder to determine the correct cluster assignment of the original instances. In addition to different theoretical properties, which explain the reasoning behind the algorithm, experimental results indicate that our method outperforms the state-of-the-art in terms of the trade-off between number of distance computations and the quality of the solution obtained.},
  archive      = {J_DMKD},
  author       = {Capó, Marco and Pérez, Aritz and Lozano, Jose A.},
  doi          = {10.1007/s10618-020-00678-9},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {5},
  number       = {3},
  pages        = {776-811},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {An efficient K-means clustering algorithm for tall data},
  volume       = {34},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). TS-CHIEF: A scalable and accurate forest algorithm for time
series classification. <em>DMKD</em>, <em>34</em>(3), 742–775. (<a
href="https://doi.org/10.1007/s10618-020-00679-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Time Series Classification (TSC) has seen enormous progress over the last two decades. HIVE-COTE (Hierarchical Vote Collective of Transformation-based Ensembles) is the current state of the art in terms of classification accuracy. HIVE-COTE recognizes that time series data are a specific data type for which the traditional attribute-value representation, used predominantly in machine learning, fails to provide a relevant representation. HIVE-COTE combines multiple types of classifiers: each extracting information about a specific aspect of a time series, be it in the time domain, frequency domain or summarization of intervals within the series. However, HIVE-COTE (and its predecessor, FLAT-COTE) is often infeasible to run on even modest amounts of data. For instance, training HIVE-COTE on a dataset with only 1500 time series can require 8 days of CPU time. It has polynomial runtime with respect to the training set size, so this problem compounds as data quantity increases. We propose a novel TSC algorithm, TS-CHIEF (Time Series Combination of Heterogeneous and Integrated Embedding Forest), which rivals HIVE-COTE in accuracy but requires only a fraction of the runtime. TS-CHIEF constructs an ensemble classifier that integrates the most effective embeddings of time series that research has developed in the last decade. It uses tree-structured classifiers to do so efficiently. We assess TS-CHIEF on 85 datasets of the University of California Riverside (UCR) archive, where it achieves state-of-the-art accuracy with scalability and efficiency. We demonstrate that TS-CHIEF can be trained on 130 k time series in 2 days, a data quantity that is beyond the reach of any TSC algorithm with comparable accuracy.},
  archive      = {J_DMKD},
  author       = {Shifaz, Ahmed and Pelletier, Charlotte and Petitjean, François and Webb, Geoffrey I.},
  doi          = {10.1007/s10618-020-00679-8},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {5},
  number       = {3},
  pages        = {742-775},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {TS-CHIEF: A scalable and accurate forest algorithm for time series classification},
  volume       = {34},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Robust and sparse multigroup classification by the optimal
scoring approach. <em>DMKD</em>, <em>34</em>(3), 723–741. (<a
href="https://doi.org/10.1007/s10618-019-00666-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a robust and sparse classification method based on the optimal scoring approach. It is also applicable if the number of variables exceeds the number of observations. The data are first projected into a low dimensional subspace according to an optimal scoring criterion. The projection only includes a subset of the original variables (sparse modeling) and is not distorted by outliers (robust modeling). In this low dimensional subspace classification is performed by minimizing a robust Mahalanobis distance to the group centers. The low dimensional representation of the data is also useful for visualization purposes. We discuss the algorithm for the proposed method in detail. A simulation study illustrates the properties of robust and sparse classification by optimal scoring compared to the non-robust and/or non-sparse alternative methods. Three real data applications are given.},
  archive      = {J_DMKD},
  author       = {Ortner, Irene and Filzmoser, Peter and Croux, Christophe},
  doi          = {10.1007/s10618-019-00666-8},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {5},
  number       = {3},
  pages        = {723-741},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Robust and sparse multigroup classification by the optimal scoring approach},
  volume       = {34},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Model-based exception mining for object-relational data.
<em>DMKD</em>, <em>34</em>(3), 681–722. (<a
href="https://doi.org/10.1007/s10618-020-00677-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper develops model-based exception mining and outlier detection for the case of object-relational data. Object-relational data represent a complex heterogeneous network, which comprises objects of different types, links among these objects, also of different types, and attributes of these links. We follow the well-established exceptional model mining (EMM) framework, which has been previously applied for subgroup discovery in propositional data; our novel contribution is to develop EMM for relational data. EMM leverages machine learning models for exception mining: An object is exceptional to the extent that a model learned for the object data differs from a model learned for the general population. In relational data, EMM can therefore be used for detecting single outlier or exceptional objects. We combine EMM with state-of-the-art statistical-relational model discovery methods for constructing a graphical model (Bayesian network), that compactly represents probabilistic associations in the data. We investigate several outlierness metrics, based on the learned object-relational model, that quantify the extent to which the association pattern of a potential outlier object deviates from that of the whole population. Our method is validated on synthetic data sets and on real-world data sets about soccer and hockey matches, IMDb movies and mutagenic compounds. Compared to baseline methods, the EMM approach achieved the best detection accuracy when combined with a novel outlinerness metric. An empirical evaluation on soccer and movie data shows a strong correlation between our novel outlierness metric and success metrics: Individuals that our metric marks out as unusual tend to have unusual success.},
  archive      = {J_DMKD},
  author       = {Riahi, Fatemeh and Schulte, Oliver},
  doi          = {10.1007/s10618-020-00677-w},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {5},
  number       = {3},
  pages        = {681-722},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Model-based exception mining for object-relational data},
  volume       = {34},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). MasterMovelets: Discovering heterogeneous movelets for
multiple aspect trajectory classification. <em>DMKD</em>,
<em>34</em>(3), 652–680. (<a
href="https://doi.org/10.1007/s10618-020-00676-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the last few years trajectory classification has been applied to many real problems, basically considering the dimensions of space and time or attributes inferred from these dimensions. However, with the explosion of social media data and the advances in the semantic enrichment of mobility data, a new type of trajectory data has emerged, and the trajectory spatio-temporal points have now multiple and heterogeneous semantic dimensions. By semantic dimensions we mean any type of information that is neither spatial nor temporal. As a consequence, new classification methods are needed to deal with this new type of data. The main challenge is how to automatically select and combine the data dimensions and to discover the subtrajectories that better discriminate the class. In this paper we propose MasterMovelets, a new parameter-free method for trajectory classification which finds the best trajectory partition and dimension combination for robust high dimensional trajectory classification. Experimental results show that our approach outperforms state-of-the-art methods by reducing the classification error up to $$63\%$$ , indicating that our proposal is very promising for multidimensional sequence data classification.},
  archive      = {J_DMKD},
  author       = {Ferrero, Carlos Andres and Petry, Lucas May and Alvares, Luis Otavio and da Silva, Camila Leite and Zalewski, Willian and Bogorny, Vania},
  doi          = {10.1007/s10618-020-00676-x},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {5},
  number       = {3},
  pages        = {652-680},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {MasterMovelets: Discovering heterogeneous movelets for multiple aspect trajectory classification},
  volume       = {34},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Relaxing the strong triadic closure problem for edge
strength inference. <em>DMKD</em>, <em>34</em>(3), 611–651. (<a
href="https://doi.org/10.1007/s10618-020-00673-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Social networks often provide only a binary perspective on social ties: two individuals are either connected or not. While sometimes external information can be used to infer the strength of social ties, access to such information may be restricted or impractical to obtain. Sintos and Tsaparas (KDD 2014) first suggested to infer the strength of social ties from the topology of the network alone, by leveraging the Strong Triadic Closure (STC) property. The STC property states that if person A has strong social ties with persons B and C, B and C must be connected to each other as well (whether with a weak or strong tie). They exploited this property to formulate the inference of the strength of social ties as a NP-hard maximization problem, and proposed two approximation algorithms. We refine and improve this line of work, by developing a sequence of linear relaxations of the problem, which can be solved exactly in polynomial time. Usefully, these relaxations infer more fine-grained levels of tie strength (beyond strong and weak), which also allows one to avoid making arbitrary strong/weak strength assignments when the network topology provides inconclusive evidence. Moreover, these relaxations allow us to easily change the objective function to more sensible alternatives, instead of simply maximizing the number of strong edges. An extensive theoretical analysis leads to two efficient algorithmic approaches. Finally, our experimental results elucidate the strengths of the proposed approach, while at the same time questioning the validity of leveraging the STC property for edge strength inference in practice.},
  archive      = {J_DMKD},
  author       = {Adriaens, Florian and De Bie, Tijl and Gionis, Aristides and Lijffijt, Jefrey and Matakos, Antonis and Rozenshtein, Polina},
  doi          = {10.1007/s10618-020-00673-0},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {5},
  number       = {3},
  pages        = {611-651},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Relaxing the strong triadic closure problem for edge strength inference},
  volume       = {34},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). NegPSpan: Efficient extraction of negative sequential
patterns with embedding constraints. <em>DMKD</em>, <em>34</em>(2),
563–609. (<a href="https://doi.org/10.1007/s10618-019-00672-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sequential pattern mining is concerned with the extraction of frequent or recurrent behaviors, modeled as subsequences, from a sequence dataset. Such patterns inform about which events are frequently observed in sequences, i.e. events that really happen. Sometimes, knowing that some specific event does not happen is more informative than extracting observed events. Negative sequential patterns (NSPs) capture recurrent behaviors by patterns having the form of sequences mentioning both observed events and absence of events. Few approaches have been proposed to mine such NSPs. In addition, the syntax and semantics of NSPs differ in the different methods which makes it difficult to compare them. This article provides a unified framework for the formulation of the syntax and the semantics of NSPs. Then, we introduce a new algorithm, NegPSpan, that extracts NSPs using a prefix-based depth-first scheme, enabling maxgap constraints that other approaches do not take into account. The formal framework highlights the differences between the proposed approach and methods from the literature, especially against the state of the art approach eNSP. Intensive experiments on synthetic and real datasets show that NegPSpan can extract meaningful NSPs and that it can process bigger datasets than eNSP thanks to significantly lower memory requirements and better computation times.},
  archive      = {J_DMKD},
  author       = {Guyet, Thomas and Quiniou, René},
  doi          = {10.1007/s10618-019-00672-w},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {3},
  number       = {2},
  pages        = {563-609},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {NegPSpan: Efficient extraction of negative sequential patterns with embedding constraints},
  volume       = {34},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Integer programming ensemble of temporal relations
classifiers. <em>DMKD</em>, <em>34</em>(2), 533–562. (<a
href="https://doi.org/10.1007/s10618-019-00671-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The extraction of temporal events from text and the classification of temporal relations among both temporal events and time expressions are major challenges for the interface of data mining and natural language processing. We present an ensemble method, which reconciles the outputs of multiple heterogenous classifiers of temporal expressions. We use integer programming, a constrained optimisation technique, to improve on the best result of any individual classifier by choosing consistent temporal relations from among those recommended by multiple classifiers. Our ensemble method is conceptually simple and empirically powerful. It allows us to encode knowledge about the structure of valid temporal expressions as a set of constraints. It obtains new state-of-the-art results on two recent natural language processing challenges, SemEval-2013 TempEval-3 (Temporal Annotation) and SemEval-2016 Task 12 (Clinical TempEval), with F1 scores of 0.3915 and 0.595 respectively.},
  archive      = {J_DMKD},
  author       = {Kerr, Catherine and Hoare, Terri and Carroll, Paula and Mareček, Jakub},
  doi          = {10.1007/s10618-019-00671-x},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {3},
  number       = {2},
  pages        = {533-562},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Integer programming ensemble of temporal relations classifiers},
  volume       = {34},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Parameterized low-rank binary matrix approximation.
<em>DMKD</em>, <em>34</em>(2), 478–532. (<a
href="https://doi.org/10.1007/s10618-019-00669-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Low-rank binary matrix approximation is a generic problem where one seeks a good approximation of a binary matrix by another binary matrix with some specific properties. A good approximation means that the difference between the two matrices in some matrix norm is small. The properties of the approximation binary matrix could be: a small number of different columns, a small binary rank or a small Boolean rank. Unfortunately, most variants of these problems are NP-hard. Due to this, we initiate the systematic algorithmic study of low-rank binary matrix approximation from the perspective of parameterized complexity. We show in which cases and under what conditions the problem is fixed-parameter tractable, admits a polynomial kernel and can be solved in parameterized subexponential time.},
  archive      = {J_DMKD},
  author       = {Fomin, Fedor V. and Golovach, Petr A. and Panolan, Fahad},
  doi          = {10.1007/s10618-019-00669-5},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {3},
  number       = {2},
  pages        = {478-532},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Parameterized low-rank binary matrix approximation},
  volume       = {34},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Mining relaxed functional dependencies from data.
<em>DMKD</em>, <em>34</em>(2), 443–477. (<a
href="https://doi.org/10.1007/s10618-019-00667-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Relaxed functional dependencies (rfds) are properties expressing important relationships among data. Thanks to the introduction of approximations in data comparison and/or validity, they can capture constraints useful for several purposes, such as the identification of data inconsistencies or patterns of semantically related data. Nevertheless, rfds can provide benefits only if they can be automatically discovered from data. In this paper we present an rfd discovery algorithm relying on a lattice structured search space, previously used for fd discovery, new pruning strategies, and a new candidate rfd validation method. An experimental evaluation demonstrates the discovery performances of the proposed algorithm on real datasets, also providing a comparison with other algorithms.},
  archive      = {J_DMKD},
  author       = {Caruccio, Loredana and Deufemia, Vincenzo and Polese, Giuseppe},
  doi          = {10.1007/s10618-019-00667-7},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {3},
  number       = {2},
  pages        = {443-477},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Mining relaxed functional dependencies from data},
  volume       = {34},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Identifying exceptional (dis)agreement between groups.
<em>DMKD</em>, <em>34</em>(2), 394–442. (<a
href="https://doi.org/10.1007/s10618-019-00665-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Under the term behavioral data, we consider any type of data featuring individuals performing observable actions on entities. For instance, voting data depict parliamentarians who express their votes w.r.t. legislative procedures. In this work, we address the problem of discovering exceptional (dis)agreement patterns in such data, i.e., groups of individuals that exhibit an unexpected (dis)agreement under specific contexts compared to what is observed in overall terms. To tackle this problem, we design a generic approach, rooted in the Subgroup Discovery/Exceptional Model Mining framework, which enables the discovery of such patterns in two different ways. A branch-and-bound algorithm ensures an efficient exhaustive search of the underlying search space by leveraging closure operators and optimistic estimates on the interestingness measures. A second algorithm abandons the completeness by using a sampling paradigm which provides an alternative when an exhaustive search approach becomes unfeasible. To illustrate the usefulness of discovering exceptional (dis)agreement patterns, we report a comprehensive experimental study on four real-world datasets relevant to three different application domains: political analysis, rating data analysis and healthcare surveillance.},
  archive      = {J_DMKD},
  author       = {Belfodil, Adnene and Cazalens, Sylvie and Lamarre, Philippe and Plantevit, Marc},
  doi          = {10.1007/s10618-019-00665-9},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {3},
  number       = {2},
  pages        = {394-442},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Identifying exceptional (dis)agreement between groups},
  volume       = {34},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). SIAS-miner: Mining subjectively interesting attributed
subgraphs. <em>DMKD</em>, <em>34</em>(2), 355–393. (<a
href="https://doi.org/10.1007/s10618-019-00664-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data clustering, local pattern mining, and community detection in graphs are three mature areas of data mining and machine learning. In recent years, attributed subgraph mining has emerged as a new powerful data mining task in the intersection of these areas. Given a graph and a set of attributes for each vertex, attributed subgraph mining aims to find cohesive subgraphs for which (some of) the attribute values have exceptional values. The principled integration of graph and attribute data poses two challenges: (1) the definition of a pattern syntax (the abstract form of patterns) that is intuitive and lends itself to efficient search, and (2) the formalization of the interestingness of such patterns. We propose an integrated solution to both of these challenges. The proposed pattern syntax improves upon prior work in being both highly flexible and intuitive. Plus, we define an effective and principled algorithm to enumerate patterns of this syntax. The proposed approach for quantifying interestingness of these patterns is rooted in information theory, and is able to account for background knowledge on the data. While prior work quantified the interestingness for the cohesion of the subgraph and for the exceptionality of its attributes separately, then combining these in a parameterized trade-off, we instead handle this trade-off implicitly in a principled, parameter-free manner. Empirical results confirm we can efficiently find highly interesting subgraphs.},
  archive      = {J_DMKD},
  author       = {Bendimerad, Anes and Mel, Ahmad and Lijffijt, Jefrey and Plantevit, Marc and Robardet, Céline and De Bie, Tijl},
  doi          = {10.1007/s10618-019-00664-w},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {3},
  number       = {2},
  pages        = {355-393},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {SIAS-miner: Mining subjectively interesting attributed subgraphs},
  volume       = {34},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). On normalization and algorithm selection for unsupervised
outlier detection. <em>DMKD</em>, <em>34</em>(2), 309–354. (<a
href="https://doi.org/10.1007/s10618-019-00661-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper demonstrates that the performance of various outlier detection methods is sensitive to both the characteristics of the dataset, and the data normalization scheme employed. To understand these dependencies, we formally prove that normalization affects the nearest neighbor structure, and density of the dataset; hence, affecting which observations could be considered outliers. Then, we perform an instance space analysis of combinations of normalization and detection methods. Such analysis enables the visualization of the strengths and weaknesses of these combinations. Moreover, we gain insights into which method combination might obtain the best performance for a given dataset.},
  archive      = {J_DMKD},
  author       = {Kandanaarachchi, Sevvandi and Muñoz, Mario A. and Hyndman, Rob J. and Smith-Miles, Kate},
  doi          = {10.1007/s10618-019-00661-z},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {3},
  number       = {2},
  pages        = {309-354},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {On normalization and algorithm selection for unsupervised outlier detection},
  volume       = {34},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A survey and benchmarking study of multitreatment uplift
modeling. <em>DMKD</em>, <em>34</em>(2), 273–308. (<a
href="https://doi.org/10.1007/s10618-019-00670-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Uplift modeling is an instrument used to estimate the change in outcome due to a treatment at the individual entity level. Uplift models assist decision-makers in optimally allocating scarce resources. This allows the selection of the subset of entities for which the effect of a treatment will be largest and, as such, the maximization of the overall returns. The literature on uplift modeling mostly focuses on queries concerning the effect of a single treatment and rarely considers situations where more than one treatment alternative is utilized. This article surveys the current literature on multitreatment uplift modeling and proposes two novel techniques: the naive uplift approach and the multitreatment modified outcome approach. Moreover, a benchmarking experiment is performed to contrast the performances of different multitreatment uplift modeling techniques across eight data sets from various domains. We verify and, if needed, correct the imbalance among the pretreatment characteristics of the treatment groups by means of optimal propensity score matching, which ensures a correct interpretation of the estimated uplift. Conventional and recently proposed evaluation metrics are adapted to the multitreatment scenario to assess performance. None of the evaluated techniques consistently outperforms other techniques. Hence, it is concluded that performance largely depends on the context and problem characteristics. The newly proposed techniques are found to offer similar performances compared to state-of-the-art approaches.},
  archive      = {J_DMKD},
  author       = {Olaya, Diego and Coussement, Kristof and Verbeke, Wouter},
  doi          = {10.1007/s10618-019-00670-y},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {3},
  number       = {2},
  pages        = {273-308},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {A survey and benchmarking study of multitreatment uplift modeling},
  volume       = {34},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). FastEE: Fast ensembles of elastic distances for time series
classification. <em>DMKD</em>, <em>34</em>(1), 231–272. (<a
href="https://doi.org/10.1007/s10618-019-00663-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, many new ensemble-based time series classification (TSC) algorithms have been proposed. Each of them is significantly more accurate than their predecessors. The Hierarchical Vote Collective of Transformation-based Ensembles (HIVE-COTE) is currently the most accurate TSC algorithm when assessed on the UCR repository. It is a meta-ensemble of 5 state-of-the-art ensemble-based classifiers. The time complexity of HIVE-COTE—particularly for training—is prohibitive for most datasets. There is thus a critical need to speed up the classifiers that compose HIVE-COTE. This paper focuses on speeding up one of its components: Ensembles of Elastic Distances (EE), which is the classifier that leverages on the decades of research into the development of time-dedicated measures. Training EE can be prohibitive for many datasets. For example, it takes a month on the ElectricDevices dataset with 9000 instances. This is because EE needs to cross-validate the hyper-parameters used for the 11 similarity measures it encompasses. In this work, Fast Ensembles of Elastic Distances is proposed to train EE faster. There are two versions to it. The exact version makes it possible to train EE 10 times faster. The approximate version is 40 times faster than EE without significantly impacting the classification accuracy. This translates to being able to train EE on ElectricDevices in 13 h.},
  archive      = {J_DMKD},
  author       = {Tan, Chang Wei and Petitjean, François and Webb, Geoffrey I.},
  doi          = {10.1007/s10618-019-00663-x},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {1},
  number       = {1},
  pages        = {231-272},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {FastEE: Fast ensembles of elastic distances for time series classification},
  volume       = {34},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Deep multi-task learning for individuals origin–destination
matrices estimation from census data. <em>DMKD</em>, <em>34</em>(1),
201–230. (<a href="https://doi.org/10.1007/s10618-019-00662-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Rapid urbanization has made the estimation of the human mobility flows a substantial task for transportation and urban planners. Worker and student mobility flows are among the most weekly regular displacements and consequently generate road congestion issues. With urge of demands on efficient transport planning policies, estimating their commuting facilitates the decision-making processes for local authorities. Worker and student censuses often contain home location, work places and educational institutions. This paper proposes a novel approach to estimate individuals origin–destination matrices from census datasets. We use a multi-task neural network to learn a generic model providing the spatio-temporal estimations of commuters dynamic mobility flows on daily basis from static censuses. Multi-task learning aims at leveraging functional information incorporated in multiple tasks, which allows ameliorating the generalization performance within all the tasks. We first aggregate individuals household travel surveys and census databases with working and studying trips. The model learns the temporal distribution of displacements from these static sources and then it is applied on scholar and worker mobility sources to predict the temporal characteristics of commuters’ displacements (i.e. origin–destination matrices). Our method yields substantially more stable predictions in terms of accuracy and results in a significant error rate control in comparison to single task learning.},
  archive      = {J_DMKD},
  author       = {Katranji, Mehdi and Kraiem, Sami and Moalic, Laurent and Sanmarty, Guilhem and Khodabandelou, Ghazaleh and Caminada, Alexandre and Hadj Selem, Fouad},
  doi          = {10.1007/s10618-019-00662-y},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {1},
  number       = {1},
  pages        = {201-230},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Deep multi-task learning for individuals origin–destination matrices estimation from census data},
  volume       = {34},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Matching code and law: Achieving algorithmic fairness with
optimal transport. <em>DMKD</em>, <em>34</em>(1), 163–200. (<a
href="https://doi.org/10.1007/s10618-019-00658-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Increasingly, discrimination by algorithms is perceived as a societal and legal problem. As a response, a number of criteria for implementing algorithmic fairness in machine learning have been developed in the literature. This paper proposes the continuous fairness algorithm $$(\hbox {CFA}\theta )$$ which enables a continuous interpolation between different fairness definitions. More specifically, we make three main contributions to the existing literature. First, our approach allows the decision maker to continuously vary between specific concepts of individual and group fairness. As a consequence, the algorithm enables the decision maker to adopt intermediate “worldviews” on the degree of discrimination encoded in algorithmic processes, adding nuance to the extreme cases of “we’re all equal” and “what you see is what you get” proposed so far in the literature. Second, we use optimal transport theory, and specifically the concept of the barycenter, to maximize decision maker utility under the chosen fairness constraints. Third, the algorithm is able to handle cases of intersectionality, i.e., of multi-dimensional discrimination of certain groups on grounds of several criteria. We discuss three main examples (credit applications; college admissions; insurance contracts) and map out the legal and policy implications of our approach. The explicit formalization of the trade-off between individual and group fairness allows this post-processing approach to be tailored to different situational contexts in which one or the other fairness criterion may take precedence. Finally, we evaluate our model experimentally.},
  archive      = {J_DMKD},
  author       = {Zehlike, Meike and Hacker, Philipp and Wiedemann, Emil},
  doi          = {10.1007/s10618-019-00658-8},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {1},
  number       = {1},
  pages        = {163-200},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Matching code and law: Achieving algorithmic fairness with optimal transport},
  volume       = {34},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A comparative study of data-dependent approaches without
learning in measuring similarities of data objects. <em>DMKD</em>,
<em>34</em>(1), 124–162. (<a
href="https://doi.org/10.1007/s10618-019-00660-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Conventional general-purpose distance-based similarity measures, such as Minkowski distance (also known as $$\ell _p$$-norm with $$p&gt;0$$), are data-independent and sensitive to units or scales of measurement. There are existing general-purpose data-dependent measures, such as rank difference, Lin’s probabilistic measure and $$m_p$$-dissimilarity ($$p&gt;0$$), which are not sensitive to units or scales of measurement. Although they have been shown to be more effective than the traditional distance measures, their characteristics and relative performances have not been investigated. In this paper, we study the characteristics and relationships of different general-purpose data-dependent measures. We generalise $$m_p$$-dissimilarity where $$p\ge 0$$ by introducing $$m_0$$-dissimilarity and show that it is a generic data-dependent measure with data-dependent self-similarity, of which rank difference and Lin’s measure are special cases with data-independent self-similarity. We evaluate the effectiveness of a wide range of general-purpose data-dependent and data-independent measures in the content-based information retrieval and kNN classification tasks. Our findings show that the fully data-dependent measure of $$m_p$$-dissimilarity is a more effective alternative to other data-dependent and commonly-used distance-based similarity measures as its task-specific performance is more consistent across a wide range of datasets.},
  archive      = {J_DMKD},
  author       = {Aryal, Sunil and Ting, Kai Ming and Washio, Takashi and Haffari, Gholamreza},
  doi          = {10.1007/s10618-019-00660-0},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {1},
  number       = {1},
  pages        = {124-162},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {A comparative study of data-dependent approaches without learning in measuring similarities of data objects},
  volume       = {34},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Grafting for combinatorial binary model using frequent
itemset mining. <em>DMKD</em>, <em>34</em>(1), 101–123. (<a
href="https://doi.org/10.1007/s10618-019-00657-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider the class of linear predictors over all logical conjunctions of binary attributes, which we refer to as the class of combinatorial binary models (CBMs) in this paper. CBMs are of high knowledge interpretability but naïve learning of them from labeled data requires exponentially high computational cost with respect to the length of the conjunctions. On the other hand, in the case of large-scale datasets, long conjunctions are effective for learning predictors. To overcome this computational difficulty, we propose an algorithm, GRAfting for Binary datasets (GRAB), which efficiently learns CBMs within the $$L_1$$-regularized loss minimization framework. The key idea of GRAB is to adopt weighted frequent itemset mining for the most time-consuming step in the grafting algorithm, which is designed to solve large-scale $$L_1$$-RERM problems by an iterative approach. Furthermore, we experimentally showed that linear predictors of CBMs are effective in terms of prediction accuracy and knowledge discovery.},
  archive      = {J_DMKD},
  author       = {Lee, Taito and Matsushima, Shin and Yamanishi, Kenji},
  doi          = {10.1007/s10618-019-00657-9},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {1},
  number       = {1},
  pages        = {101-123},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Grafting for combinatorial binary model using frequent itemset mining},
  volume       = {34},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Topical network embedding. <em>DMKD</em>, <em>34</em>(1),
75–100. (<a href="https://doi.org/10.1007/s10618-019-00659-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Networked data involve complex information from multifaceted channels, including topology structures, node content, and/or node labels etc., where structure and content are often correlated but are not always consistent. A typical scenario is the citation relationships in scholarly publications where a paper is cited by others not because they have the same content, but because they share one or multiple subject matters. To date, while many network embedding methods exist to take the node content into consideration, they all consider node content as simple flat word/attribute set and nodes sharing connections are assumed to have dependency with respect to all words or attributes. In this paper, we argue that considering topic-level semantic interactions between nodes is crucial to learn discriminative node embedding vectors. In order to model pairwise topic relevance between linked text nodes, we propose topical network embedding, where interactions between nodes are built on the shared latent topics. Accordingly, we propose a unified optimization framework to simultaneously learn topic and node representations from the network text contents and structures, respectively. Meanwhile, the structure modeling takes the learned topic representations as conditional context under the principle that two nodes can infer each other contingent on the shared latent topics. Experiments on three real-world datasets demonstrate that our approach can learn significantly better network representations, i.e., 4.1% improvement over the state-of-the-art methods in terms of Micro-F1 on Cora dataset. (The source code of the proposed method is available through the github link: https://github.com/codeshareabc/TopicalNE.)},
  archive      = {J_DMKD},
  author       = {Shi, Min and Tang, Yufei and Zhu, Xingquan and Liu, Jianxun and He, Haibo},
  doi          = {10.1007/s10618-019-00659-7},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {1},
  number       = {1},
  pages        = {75-100},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Topical network embedding},
  volume       = {34},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A drift detection method based on dynamic classifier
selection. <em>DMKD</em>, <em>34</em>(1), 50–74. (<a
href="https://doi.org/10.1007/s10618-019-00656-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Machine learning algorithms can be applied to several practical problems, such as spam, fraud and intrusion detection, and customer preferences, among others. In most of these problems, data come in streams, which mean that data distribution may change over time, leading to concept drift. The literature is abundant on providing supervised methods based on error monitoring for explicit drift detection. However, these methods may become infeasible in some real-world applications—where there is no fully labeled data available, and may depend on a significant decrease in accuracy to be able to detect drifts. There are also methods based on blind approaches, where the decision model is updated constantly. However, this may lead to unnecessary system updates. In order to overcome these drawbacks, we propose in this paper a semi-supervised drift detector that uses an ensemble of classifiers based on self-training online learning and dynamic classifier selection. For each unknown sample, a dynamic selection strategy is used to choose among the ensemble’s component members, the classifier most likely to be the correct one for classifying it. The prediction assigned by the chosen classifier is used to compute an estimate of the error produced by the ensemble members. The proposed method monitors such a pseudo-error in order to detect drifts and to update the decision model only after drift detection. The achievement of this method is relevant in that it allows drift detection and reaction and is applicable in several practical problems. The experiments conducted indicate that the proposed method attains high performance and detection rates, while reducing the amount of labeled data used to detect drift.},
  archive      = {J_DMKD},
  author       = {Pinagé, Felipe and dos Santos, Eulanda M. and Gama, João},
  doi          = {10.1007/s10618-019-00656-w},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {1},
  number       = {1},
  pages        = {50-74},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {A drift detection method based on dynamic classifier selection},
  volume       = {34},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Interactive visual data exploration with subjective
feedback: An information-theoretic approach. <em>DMKD</em>,
<em>34</em>(1), 21–49. (<a
href="https://doi.org/10.1007/s10618-019-00655-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Visual exploration of high-dimensional real-valued datasets is a fundamental task in exploratory data analysis (EDA). Existing projection methods for data visualization use predefined criteria to choose the representation of data. There is a lack of methods that (i) use information on what the user has learned from the data and (ii) show patterns that she does not know yet. We construct a theoretical model where identified patterns can be input as knowledge to the system. The knowledge syntax here is intuitive, such as “this set of points forms a cluster”, and requires no knowledge of maths. This background knowledge is used to find a maximum entropy distribution of the data, after which the user is provided with data projections for which the data and the maximum entropy distribution differ the most, hence showing the user aspects of data that are maximally informative given the background knowledge. We study the computational performance of our model and present use cases on synthetic and real data. We find that the model allows the user to learn information efficiently from various data sources and works sufficiently fast in practice. In addition, we provide an open source EDA demonstrator system implementing our model with tailored interactive visualizations. We conclude that the information theoretic approach to EDA where patterns observed by a user are formalized as constraints provides a principled, intuitive, and efficient basis for constructing an EDA system.},
  archive      = {J_DMKD},
  author       = {Puolamäki, Kai and Oikarinen, Emilia and Kang, Bo and Lijffijt, Jefrey and De Bie, Tijl},
  doi          = {10.1007/s10618-019-00655-x},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {1},
  number       = {1},
  pages        = {21-49},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {Interactive visual data exploration with subjective feedback: An information-theoretic approach},
  volume       = {34},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A semi-supervised model for knowledge graph embedding.
<em>DMKD</em>, <em>34</em>(1), 1–20. (<a
href="https://doi.org/10.1007/s10618-019-00653-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Knowledge graphs have shown increasing importance in broad applications such as question answering, web search, and recommendation systems. The objective of knowledge graph embedding is to encode both entities and relations of knowledge graphs into continuous low-dimensional vector spaces to perform various machine learning tasks. Most of the existing works only focused on the local structure of knowledge graphs when utilizing structural information of entities, which may not sincerely preserve the global structure of knowledge graphs.In this paper, we propose a semi-supervised model by adopting graph convolutional networks to utilize both local and global structural information of entities. Specifically, our model takes textual information of each entity into consideration as entity attributes in the process of learning. We show the effectiveness of our model by applying it to two traditional tasks for knowledge graph: entity classification and link prediction. Experimental results on two well-known corpora reveal the advantages of this model compared to state-of-the-art methods on both tasks. Moreover, the results show that even with only 1% labeled data to train, our model can still achieve good performance.},
  archive      = {J_DMKD},
  author       = {Zhu, Jia and Zheng, Zetao and Yang, Min and Fung, Gabriel Pui Cheong and Tang, Yong},
  doi          = {10.1007/s10618-019-00653-z},
  journal      = {Data Mining and Knowledge Discovery},
  month        = {1},
  number       = {1},
  pages        = {1-20},
  shortjournal = {Data Mining Knowl. Discov.},
  title        = {A semi-supervised model for knowledge graph embedding},
  volume       = {34},
  year         = {2020},
}
</textarea>
</details></li>
</ul>

</body>
</html>
