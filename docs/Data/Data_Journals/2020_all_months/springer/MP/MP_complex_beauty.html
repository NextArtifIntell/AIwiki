<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>MP_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="mp---103">MP - 103</h2>
<ul>
<li><details>
<summary>
(2020). Certifiably optimal sparse inverse covariance estimation.
<em>MP</em>, <em>184</em>(1), 491–530. (<a
href="https://doi.org/10.1007/s10107-019-01419-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider the maximum likelihood estimation of sparse inverse covariance matrices. We demonstrate that current heuristic approaches primarily encourage robustness, instead of the desired sparsity. We give a novel approach that solves the cardinality constrained likelihood problem to certifiable optimality. The approach uses techniques from mixed-integer optimization and convex optimization, and provides a high-quality solution with a guarantee on its suboptimality, even if the algorithm is terminated early. Using a variety of synthetic and real datasets, we demonstrate that our approach can solve problems where the dimension of the inverse covariance matrix is up to 1000 s. We also demonstrate that our approach produces significantly sparser solutions than Glasso and other popular learning procedures, makes less false discoveries, while still maintaining state-of-the-art accuracy.},
  archive      = {J_MP},
  author       = {Bertsimas, Dimitris and Lamperski, Jourdain and Pauphilet, Jean},
  doi          = {10.1007/s10107-019-01419-7},
  journal      = {Mathematical Programming},
  number       = {1},
  pages        = {491-530},
  shortjournal = {Math. Program.},
  title        = {Certifiably optimal sparse inverse covariance estimation},
  volume       = {184},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Primal-dual optimization algorithms over riemannian
manifolds: An iteration complexity analysis. <em>MP</em>,
<em>184</em>(1), 445–490. (<a
href="https://doi.org/10.1007/s10107-019-01418-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper we study nonconvex and nonsmooth multi-block optimization over Euclidean embedded (smooth) Riemannian submanifolds with coupled linear constraints. Such optimization problems naturally arise from machine learning, statistical learning, compressive sensing, image processing, and tensor PCA, among others. By utilizing the embedding structure, we develop an ADMM-like primal-dual approach based on decoupled solvable subroutines such as linearized proximal mappings, where the duality is with respect to the embedded Euclidean spaces. First, we introduce the optimality conditions for the afore-mentioned optimization models. Then, the notion of $$\epsilon $$ -stationary solutions is introduced as a result. The main part of the paper is to show that the proposed algorithms possess an iteration complexity of $$O(1/\epsilon ^2)$$ to reach an $$\epsilon $$ -stationary solution. For prohibitively large-size tensor or machine learning models, we present a sampling-based stochastic algorithm with the same iteration complexity bound in expectation. In case the subproblems are not analytically solvable, a feasible curvilinear line-search variant of the algorithm based on retraction operators is proposed. Finally, we show specifically how the algorithms can be implemented to solve a variety of practical problems such as the NP-hard maximum bisection problem, the $$\ell _q$$ regularized sparse tensor principal component analysis and the community detection problem. Our preliminary numerical results show great potentials of the proposed methods.},
  archive      = {J_MP},
  author       = {Zhang, Junyu and Ma, Shiqian and Zhang, Shuzhong},
  doi          = {10.1007/s10107-019-01418-8},
  journal      = {Mathematical Programming},
  number       = {1},
  pages        = {445-490},
  shortjournal = {Math. Program.},
  title        = {Primal-dual optimization algorithms over riemannian manifolds: An iteration complexity analysis},
  volume       = {184},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020a). A tight <span class="math display">$$\sqrt{2}$$</span>
-approximation for linear 3-cut. <em>MP</em>, <em>184</em>(1), 411–443.
(<a href="https://doi.org/10.1007/s10107-019-01417-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We investigate the approximability of the linear 3-cut problem in directed graphs. The input here is a directed graph $$D=(V,E)$$ with node weights and three specified terminal nodes $$s,r,t\in V$$ , and the goal is to find a minimum weight subset of non-terminal nodes whose removal ensures that s cannot reach r and t, and r cannot reach t. The precise approximability of linear 3-cut has been wide open until now: the best known lower bound under the unique games conjecture (UGC) was 4 / 3, while the best known upper bound was 2 using a trivial algorithm. In this work we completely close this gap: we present a $$\sqrt{2}$$ -approximation algorithm and show that this factor is tight under UGC. Our contributions are twofold: (1) we analyze a natural two-step deterministic rounding scheme through the lens of a single-step randomized rounding scheme with non-trivial distributions, and (2) we construct integrality gap instances that meet the upper bound of $$\sqrt{2}$$ . Our gap instances can be viewed as a weighted graph sequence converging to a “graph limit structure”. We complement our results by showing connections between the linear 3-cut problem and other fundamental cut problems in directed graphs.},
  archive      = {J_MP},
  author       = {Bérczi, Kristóf and Chandrasekaran, Karthekeyan and Király, Tamás and Madan, Vivek},
  doi          = {10.1007/s10107-019-01417-9},
  journal      = {Mathematical Programming},
  number       = {1},
  pages        = {411-443},
  shortjournal = {Math. Program.},
  title        = {A tight $$\sqrt{2}$$ -approximation for linear 3-cut},
  volume       = {184},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Golden ratio algorithms for variational inequalities.
<em>MP</em>, <em>184</em>(1), 383–410. (<a
href="https://doi.org/10.1007/s10107-019-01416-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The paper presents a fully adaptive algorithm for monotone variational inequalities. In each iteration the method uses two previous iterates for an approximation of the local Lipschitz constant without running a linesearch. Thus, every iteration of the method requires only one evaluation of a monotone operator F and a proximal mapping g. The operator F need not be Lipschitz continuous, which also makes the algorithm interesting in the area of composite minimization. The method exhibits an ergodic O(1 / k) convergence rate and R-linear rate under an error bound condition. We discuss possible applications of the method to fixed point problems as well as its different generalizations.},
  archive      = {J_MP},
  author       = {Malitsky, Yura},
  doi          = {10.1007/s10107-019-01416-w},
  journal      = {Mathematical Programming},
  number       = {1},
  pages        = {383-410},
  shortjournal = {Math. Program.},
  title        = {Golden ratio algorithms for variational inequalities},
  volume       = {184},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Applying FISTA to optimization problems (with or) without
minimizers. <em>MP</em>, <em>184</em>(1), 349–381. (<a
href="https://doi.org/10.1007/s10107-019-01415-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Beck and Teboulle’s FISTA method for finding a minimizer of the sum of two convex functions, one of which has a Lipschitz continuous gradient whereas the other may be nonsmooth, is arguably the most important optimization algorithm of the past decade. While research activity on FISTA has exploded ever since, the mathematically challenging case when the original optimization problem has no minimizer has found only limited attention. In this work, we systematically study FISTA and its variants. We present general results that are applicable, regardless of the existence of minimizers.},
  archive      = {J_MP},
  author       = {Bauschke, Heinz H. and Bui, Minh N. and Wang, Xianfu},
  doi          = {10.1007/s10107-019-01415-x},
  journal      = {Mathematical Programming},
  number       = {1},
  pages        = {349-381},
  shortjournal = {Math. Program.},
  title        = {Applying FISTA to optimization problems (with or) without minimizers},
  volume       = {184},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Asynchronous level bundle methods. <em>MP</em>,
<em>184</em>(1), 319–348. (<a
href="https://doi.org/10.1007/s10107-019-01414-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we consider nonsmooth convex optimization problems with additive structure featuring independent oracles (black-boxes) working in parallel. Existing methods for solving these distributed problems in a general form are synchronous, in the sense that they wait for the responses of all the oracles before performing a new iteration. In this paper, we propose level bundle methods handling asynchronous oracles. These methods require original upper-bounds (using upper-models or scarce coordinations) to deal with asynchronicity. We prove their convergence using variational-analysis techniques and illustrate their practical performance on a Lagrangian decomposition problem.},
  archive      = {J_MP},
  author       = {Iutzeler, Franck and Malick, Jérôme and de Oliveira, Welington},
  doi          = {10.1007/s10107-019-01414-y},
  journal      = {Mathematical Programming},
  number       = {1},
  pages        = {319-348},
  shortjournal = {Math. Program.},
  title        = {Asynchronous level bundle methods},
  volume       = {184},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Approximations of semicontinuous functions with applications
to stochastic optimization and statistical estimation. <em>MP</em>,
<em>184</em>(1), 289–318. (<a
href="https://doi.org/10.1007/s10107-019-01413-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Upper semicontinuous (usc) functions arise in the analysis of maximization problems, distributionally robust optimization, and function identification, which includes many problems of nonparametric statistics. We establish that every usc function is the limit of a hypo-converging sequence of piecewise affine functions of the difference-of-max type and illustrate resulting algorithmic possibilities in the context of approximate solution of infinite-dimensional optimization problems. In an effort to quantify the ease with which classes of usc functions can be approximated by finite collections, we provide upper and lower bounds on covering numbers for bounded sets of usc functions under the Attouch-Wets distance. The result is applied in the context of stochastic optimization problems defined over spaces of usc functions. We establish confidence regions for optimal solutions based on sample average approximations and examine the accompanying rates of convergence. Examples from nonparametric statistics illustrate the results.},
  archive      = {J_MP},
  author       = {Royset, Johannes O.},
  doi          = {10.1007/s10107-019-01413-z},
  journal      = {Mathematical Programming},
  number       = {1},
  pages        = {289-318},
  shortjournal = {Math. Program.},
  title        = {Approximations of semicontinuous functions with applications to stochastic optimization and statistical estimation},
  volume       = {184},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Convergence of a relaxed inertial proximal algorithm for
maximally monotone operators. <em>MP</em>, <em>184</em>(1), 243–287. (<a
href="https://doi.org/10.1007/s10107-019-01412-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In a Hilbert space $${\mathcal {H}}$$ , given $$A{:}\;{\mathcal {H}}\rightarrow 2^{\mathcal {H}}$$ a maximally monotone operator, we study the convergence properties of a general class of relaxed inertial proximal algorithms. This study aims to extend to the case of the general monotone inclusion $$Ax \ni 0$$ the acceleration techniques initially introduced by Nesterov in the case of convex minimization. The relaxed form of the proximal algorithms plays a central role. It comes naturally with the regularization of the operator A by its Yosida approximation with a variable parameter, a technique recently introduced by Attouch–Peypouquet (Math Program Ser B, 2018. https://doi.org/10.1007/s10107-018-1252-x ) for a particular class of inertial proximal algorithms. Our study provides an algorithmic version of the convergence results obtained by Attouch–Cabot (J Differ Equ 264:7138–7182, 2018) in the case of continuous dynamical systems.},
  archive      = {J_MP},
  author       = {Attouch, Hedy and Cabot, Alexandre},
  doi          = {10.1007/s10107-019-01412-0},
  journal      = {Mathematical Programming},
  number       = {1},
  pages        = {243-287},
  shortjournal = {Math. Program.},
  title        = {Convergence of a relaxed inertial proximal algorithm for maximally monotone operators},
  volume       = {184},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). On the complexity of testing attainment of the optimal value
in nonlinear optimization. <em>MP</em>, <em>184</em>(1), 221–241. (<a
href="https://doi.org/10.1007/s10107-019-01411-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We prove that unless $$\hbox {P}=\hbox {NP}$$ , there exists no polynomial time (or even pseudo-polynomial time) algorithm that can test whether the optimal value of a nonlinear optimization problem where the objective and constraints are given by low-degree polynomials is attained. If the degrees of these polynomials are fixed, our results along with previously-known “Frank–Wolfe type” theorems imply that exactly one of two cases can occur: either the optimal value is attained on every instance, or it is strongly NP-hard to distinguish attainment from non-attainment. We also show that testing for some well-known sufficient conditions for attainment of the optimal value, such as coercivity of the objective function and closedness and boundedness of the feasible set, is strongly NP-hard. As a byproduct, our proofs imply that testing the Archimedean property of a quadratic module is strongly NP-hard, a property that is of independent interest to the convergence of the Lasserre hierarchy. Finally, we give semidefinite programming (SDP)-based sufficient conditions for attainment of the optimal value, in particular a new characterization of coercive polynomials that lends itself to an SDP hierarchy.},
  archive      = {J_MP},
  author       = {Ahmadi, Amir Ali and Zhang, Jeffrey},
  doi          = {10.1007/s10107-019-01411-1},
  journal      = {Mathematical Programming},
  number       = {1},
  pages        = {221-241},
  shortjournal = {Math. Program.},
  title        = {On the complexity of testing attainment of the optimal value in nonlinear optimization},
  volume       = {184},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Efficient first-order methods for convex minimization: A
constructive approach. <em>MP</em>, <em>184</em>(1), 183–220. (<a
href="https://doi.org/10.1007/s10107-019-01410-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We describe a novel constructive technique for devising efficient first-order methods for a wide range of large-scale convex minimization settings, including smooth, non-smooth, and strongly convex minimization. The technique builds upon a certain variant of the conjugate gradient method to construct a family of methods such that (a) all methods in the family share the same worst-case guarantee as the base conjugate gradient method, and (b) the family includes a fixed-step first-order method. We demonstrate the effectiveness of the approach by deriving optimal methods for the smooth and non-smooth cases, including new methods that forego knowledge of the problem parameters at the cost of a one-dimensional line search per iteration, and a universal method for the union of these classes that requires a three-dimensional search per iteration. In the strongly convex case, we show how numerical tools can be used to perform the construction, and show that the resulting method offers an improved worst-case bound compared to Nesterov’s celebrated fast gradient method.},
  archive      = {J_MP},
  author       = {Drori, Yoel and Taylor, Adrien B.},
  doi          = {10.1007/s10107-019-01410-2},
  journal      = {Mathematical Programming},
  number       = {1},
  pages        = {183-220},
  shortjournal = {Math. Program.},
  title        = {Efficient first-order methods for convex minimization: A constructive approach},
  volume       = {184},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Online covering with <span
class="math display"><em>ℓ</em><sub><em>q</em></sub></span> -norm
objectives and applications to network design. <em>MP</em>,
<em>184</em>(1), 155–182. (<a
href="https://doi.org/10.1007/s10107-019-01409-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider fractional online covering problems with $$\ell _q$$ -norm objectives as well as its dual packing problems. The problem of interest is of the form $$\min { f(x) \,:\, Ax\ge 1, x\ge 0}$$ where $$f(x)=\sum _{e} c_e \Vert x(S_e)\Vert _{q_e} $$ is the weighted sum of $$\ell _q$$ -norms and A is a non-negative matrix. The rows of A (i.e. covering constraints) arrive online over time. We provide an online $$O(\log d+\log \rho )$$ -competitive algorithm where $$\rho = \frac{ a_{\max }}{ a_{\min }}$$ and d is the maximum of the row sparsity of A and $$\max |S_e|$$ . This is based on the online primal-dual framework where we use the dual of the above convex program. Our result is nearly tight (even in the linear special case), and it expands the class of convex programs that admit online algorithms. We also provide two applications where such convex programs arise as relaxations of discrete optimization problems, for which our result leads to good online algorithms. In particular, we obtain an improved online algorithm (by two logarithmic factors) for non-uniform buy-at-bulk network design and a poly-logarithmic competitive ratio for throughput maximization under $$\ell _p$$ -norm capacities.},
  archive      = {J_MP},
  author       = {Shen, Xiangkun and Nagarajan, Viswanath},
  doi          = {10.1007/s10107-019-01409-9},
  journal      = {Mathematical Programming},
  number       = {1},
  pages        = {155-182},
  shortjournal = {Math. Program.},
  title        = {Online covering with $$\ell _q$$ -norm objectives and applications to network design},
  volume       = {184},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Asynchronous parallel algorithms for nonconvex optimization.
<em>MP</em>, <em>184</em>(1), 121–154. (<a
href="https://doi.org/10.1007/s10107-019-01408-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a new asynchronous parallel block-descent algorithmic framework for the minimization of the sum of a smooth nonconvex function and a nonsmooth convex one, subject to both convex and nonconvex constraints. The proposed framework hinges on successive convex approximation techniques and a novel probabilistic model that captures key elements of modern computational architectures and asynchronous implementations in a more faithful way than current state-of-the-art models. Other key features of the framework are: (1) it covers in a unified way several specific solution methods; (2) it accommodates a variety of possible parallel computing architectures; and (3) it can deal with nonconvex constraints. Almost sure convergence to stationary solutions is proved, and theoretical complexity results are provided, showing nearly ideal linear speedup when the number of workers is not too large.},
  archive      = {J_MP},
  author       = {Cannelli, Loris and Facchinei, Francisco and Kungurtsev, Vyacheslav and Scutari, Gesualdo},
  doi          = {10.1007/s10107-019-01408-w},
  journal      = {Mathematical Programming},
  number       = {1},
  pages        = {121-154},
  shortjournal = {Math. Program.},
  title        = {Asynchronous parallel algorithms for nonconvex optimization},
  volume       = {184},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Lower bounds for finding stationary points i. <em>MP</em>,
<em>184</em>(1), 71–120. (<a
href="https://doi.org/10.1007/s10107-019-01406-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We prove lower bounds on the complexity of finding $$\epsilon $$ -stationary points (points x such that $$\Vert \nabla f(x)\Vert \le \epsilon $$ ) of smooth, high-dimensional, and potentially non-convex functions f. We consider oracle-based complexity measures, where an algorithm is given access to the value and all derivatives of f at a query point x. We show that for any (potentially randomized) algorithm $$\mathsf {A}$$ , there exists a function f with Lipschitz pth order derivatives such that $$\mathsf {A}$$ requires at least $$\epsilon ^{-(p+1)/p}$$ queries to find an $$\epsilon $$ -stationary point. Our lower bounds are sharp to within constants, and they show that gradient descent, cubic-regularized Newton’s method, and generalized pth order regularization are worst-case optimal within their natural function classes.},
  archive      = {J_MP},
  author       = {Carmon, Yair and Duchi, John C. and Hinder, Oliver and Sidford, Aaron},
  doi          = {10.1007/s10107-019-01406-y},
  journal      = {Mathematical Programming},
  number       = {1},
  pages        = {71-120},
  shortjournal = {Math. Program.},
  title        = {Lower bounds for finding stationary points i},
  volume       = {184},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Newton-type methods for non-convex optimization under
inexact hessian information. <em>MP</em>, <em>184</em>(1), 35–70. (<a
href="https://doi.org/10.1007/s10107-019-01405-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider variants of trust-region and adaptive cubic regularization methods for non-convex optimization, in which the Hessian matrix is approximated. Under certain condition on the inexact Hessian, and using approximate solution of the corresponding sub-problems, we provide iteration complexity to achieve $$\varepsilon $$ -approximate second-order optimality which have been shown to be tight. Our Hessian approximation condition offers a range of advantages as compared with the prior works and allows for direct construction of the approximate Hessian with a priori guarantees through various techniques, including randomized sampling methods. In this light, we consider the canonical problem of finite-sum minimization, provide appropriate uniform and non-uniform sub-sampling strategies to construct such Hessian approximations, and obtain optimal iteration complexity for the corresponding sub-sampled trust-region and adaptive cubic regularization methods.},
  archive      = {J_MP},
  author       = {Xu, Peng and Roosta, Fred and Mahoney, Michael W.},
  doi          = {10.1007/s10107-019-01405-z},
  journal      = {Mathematical Programming},
  number       = {1},
  pages        = {35-70},
  shortjournal = {Math. Program.},
  title        = {Newton-type methods for non-convex optimization under inexact hessian information},
  volume       = {184},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Combinatorial n-fold integer programming and applications.
<em>MP</em>, <em>184</em>(1), 1–34. (<a
href="https://doi.org/10.1007/s10107-019-01402-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many fundamental $$\mathsf {NP}$$ -hard problems can be formulated as integer linear programs (ILPs). A famous algorithm by Lenstra solves ILPs in time that is exponential only in the dimension of the program, and polynomial in the size of the ILP. That algorithm became a ubiquitous tool in the design of fixed-parameter algorithms for $$\mathsf {NP}$$ -hard problems, where one wishes to isolate the hardness of a problem by some parameter. However, in many cases using Lenstra’s algorithm has two drawbacks: First, the run time of the resulting algorithms is often double-exponential in the parameter, and second, an ILP formulation in small dimension cannot easily express problems involving many different costs. Inspired by the work of Hemmecke et al. (Math Program 137(1–2, Ser. A):325–341, 2013), we develop a single-exponential algorithm for so-called combinatorial n-fold integer programs, which are remarkably similar to prior ILP formulations for various problems, but unlike them, also allow variable dimension. We then apply our algorithm to many relevant problems problems like Closest String, Swap Bribery, Weighted Set Multicover, and several others, and obtain exponential speedups in the dependence on the respective parameters, the input size, or both. Unlike Lenstra’s algorithm, which is essentially a bounded search tree algorithm, our result uses the technique of augmenting steps. At its heart is a deep result stating that in combinatorial n-fold IPs, existence of an augmenting step implies existence of a “local” augmenting step, which can be found using dynamic programming. Our results provide an important insight into many problems by showing that they exhibit this phenomenon, and highlights the importance of augmentation techniques.},
  archive      = {J_MP},
  author       = {Knop, Dušan and Koutecký, Martin and Mnich, Matthias},
  doi          = {10.1007/s10107-019-01402-2},
  journal      = {Mathematical Programming},
  number       = {1},
  pages        = {1-34},
  shortjournal = {Math. Program.},
  title        = {Combinatorial n-fold integer programming and applications},
  volume       = {184},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Random projections for quadratic programs. <em>MP</em>,
<em>183</em>(1), 619–647. (<a
href="https://doi.org/10.1007/s10107-020-01517-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Random projections map a set of points in a high dimensional space to a lower dimensional one while approximately preserving all pairwise Euclidean distances. Although random projections are usually applied to numerical data, we show in this paper that they can be successfully applied to quadratic programming formulations over a set of linear inequality constraints. Instead of solving the higher-dimensional original problem, we solve the projected problem more efficiently. This yields a feasible solution of the original problem. We prove lower and upper bounds of this feasible solution w.r.t. the optimal objective function value of the original problem. We then discuss some computational results on randomly generated instances, as well as a variant of Markowitz’ portfolio problem. It turns out that our method can find good feasible solutions of very large instances.},
  archive      = {J_MP},
  author       = {D’Ambrosio, Claudia and Liberti, Leo and Poirion, Pierre-Louis and Vu, Ky},
  doi          = {10.1007/s10107-020-01517-x},
  journal      = {Mathematical Programming},
  number       = {1},
  pages        = {619-647},
  shortjournal = {Math. Program.},
  title        = {Random projections for quadratic programs},
  volume       = {183},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Breaking symmetries to rescue sum of squares in the case of
makespan scheduling. <em>MP</em>, <em>183</em>(1), 583–618. (<a
href="https://doi.org/10.1007/s10107-020-01511-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The sum of squares (SoS) hierarchy gives an automatized technique to create a family of increasingly tight convex relaxations for binary programs. There are several problems for which a constant number of rounds of this hierarchy give integrality gaps matching the best known approximation algorithms. For many other problems, however, ad-hoc techniques give better approximation ratios than SoS in the worst case, as shown by corresponding lower bound instances. Notably, in many cases these instances are invariant under the action of a large permutation group. This yields the question how symmetries in a formulation degrade the performance of the relaxation obtained by the SoS hierarchy. In this paper, we study this for the case of the minimum makespan problem on identical machines. Our first result is to show that $$\varOmega (n)$$ rounds of SoS applied over the configuration linear program yields an integrality gap of at least 1.0009, where n is the number of jobs. This improves on the recent work by Kurpisz et al. (Math Program 172(1–2):231–248, 2018) that shows an analogous result for the weaker $$\hbox {LS}_{+}$$ and SA hierarchies. Our result is based on tools from representation theory of symmetric groups. Then, we consider the weaker assignment linear program and add a well chosen set of symmetry breaking inequalities that removes a subset of the machine permutation symmetries. We show that applying $$2^{\tilde{O}(1/\varepsilon ^2)}$$ rounds of the $$\text {SA}$$ hierarchy to this stronger linear program reduces the integrality gap to $$1+\varepsilon $$ , which yields a linear programming based polynomial time approximation scheme. Our results suggest that for this classical problem, symmetries were the main barrier preventing the $$\text {SoS}/\text {SA}$$ hierarchies to give relaxations of polynomial complexity with an integrality gap of $$1+\varepsilon $$ . We leave as an open question whether this phenomenon occurs for other symmetric problems.},
  archive      = {J_MP},
  author       = {Verdugo, Victor and Verschae, José and Wiese, Andreas},
  doi          = {10.1007/s10107-020-01511-3},
  journal      = {Mathematical Programming},
  number       = {1},
  pages        = {583-618},
  shortjournal = {Math. Program.},
  title        = {Breaking symmetries to rescue sum of squares in the case of makespan scheduling},
  volume       = {183},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Computing the nucleolus of weighted cooperative matching
games in polynomial time. <em>MP</em>, <em>183</em>(1), 555–581. (<a
href="https://doi.org/10.1007/s10107-020-01483-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We provide an efficient algorithm for computing the nucleolus for an instance of a weighted cooperative matching game. This resolves a long-standing open question posed in Faigle (Math Programm, 83: 555–569, 1998).},
  archive      = {J_MP},
  author       = {Könemann, Jochen and Pashkovich, Kanstantsin and Toth, Justin},
  doi          = {10.1007/s10107-020-01483-4},
  journal      = {Mathematical Programming},
  number       = {1},
  pages        = {555-581},
  shortjournal = {Math. Program.},
  title        = {Computing the nucleolus of weighted cooperative matching games in polynomial time},
  volume       = {183},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Linear programming using limited-precision oracles.
<em>MP</em>, <em>183</em>(1), 525–554. (<a
href="https://doi.org/10.1007/s10107-019-01444-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Since the elimination algorithm of Fourier and Motzkin, many different methods have been developed for solving linear programs. When analyzing the time complexity of LP algorithms, it is typically either assumed that calculations are performed exactly and bounds are derived on the number of elementary arithmetic operations necessary, or the cost of all arithmetic operations is considered through a bit-complexity analysis. Yet in practice, implementations typically use limited-precision arithmetic. In this paper we introduce the idea of a limited-precision LP oracle and study how such an oracle could be used within a larger framework to compute exact precision solutions to LPs. Under mild assumptions, it is shown that a polynomial number of calls to such an oracle and a polynomial number of bit operations, is sufficient to compute an exact solution to an LP. This work provides a foundation for understanding and analyzing the behavior of the methods that are currently most effective in practice for solving LPs exactly.},
  archive      = {J_MP},
  author       = {Gleixner, Ambros and Steffy, Daniel E.},
  doi          = {10.1007/s10107-019-01444-6},
  journal      = {Mathematical Programming},
  number       = {1},
  pages        = {525-554},
  shortjournal = {Math. Program.},
  title        = {Linear programming using limited-precision oracles},
  volume       = {183},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A generic exact solver for vehicle routing and related
problems. <em>MP</em>, <em>183</em>(1), 483–523. (<a
href="https://doi.org/10.1007/s10107-020-01523-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Major advances were recently obtained in the exact solution of vehicle routing problems (VRPs). Sophisticated branch-cut-and-price (BCP) algorithms for some of the most classical VRP variants now solve many instances with up to a few hundreds of customers. However, adapting and reimplementing those successful algorithms for other variants can be a very demanding task. This work proposes a BCP solver for a generic model that encompasses a wide class of VRPs. It incorporates the key elements found in the best existing VRP algorithms: ng-path relaxation, rank-1 cuts with limited memory, path enumeration, and rounded capacity cuts; all generalized through the new concepts of “packing set” and “elementarity set”. The concepts are also used to derive a branching rule based on accumulated resource consumption and to generalize the Ryan and Foster branching rule. Extensive experiments on several variants show that the generic solver has an excellent overall performance, in many problems being better than the best specific algorithms. Even some non-VRPs, like bin packing, vector packing and generalized assignment, can be modeled and effectively solved.},
  archive      = {J_MP},
  author       = {Pessoa, Artur and Sadykov, Ruslan and Uchoa, Eduardo and Vanderbeck, François},
  doi          = {10.1007/s10107-020-01523-z},
  journal      = {Mathematical Programming},
  number       = {1},
  pages        = {483-523},
  shortjournal = {Math. Program.},
  title        = {A generic exact solver for vehicle routing and related problems},
  volume       = {183},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A new contraction technique with applications to
congruency-constrained cuts. <em>MP</em>, <em>183</em>(1), 455–481. (<a
href="https://doi.org/10.1007/s10107-020-01498-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Minimum cut problems are among the most classical problems in Combinatorial Optimization and are used in a wide set of applications. Some of the best-known efficiently solvable variants include global mininmum cuts, minimum s–t cuts, and minimum odd cuts in undirected graphs. We study a problem class that can be seen to generalize the above variants, namely finding congruency-constrained minimum cuts, i.e., we consider cuts whose number of vertices is congruent to r modulo m, for some integers r and m. Apart from being a natural generalization of odd cuts, congruency-constrained minimum cuts exhibit an interesting link to a long-standing open problem in Integer Programming, namely whether integer programs described by an integer constraint matrix with bounded subdeterminants can be solved efficiently. We develop a new contraction technique inspired by Karger’s celebrated contraction algorithm for minimum cuts, which, together with further insights, leads to a polynomial time randomized approximation scheme for congruency-constrained minimum cuts for any constant modulus m. Instead of contracting edges of the original graph, we use splitting-off techniques to create an auxiliary graph on a smaller vertex set, which is used for performing random edge contractions. This way, a well-structured distribution of candidate pairs of vertices to be contracted is obtained, where the involved pairs are generally not connected by an edge. As a byproduct, our technique reveals new structural insights into near-minimum odd cuts, and, more generally, near-minimum congruency-constrained cuts.},
  archive      = {J_MP},
  author       = {Nägele, Martin and Zenklusen, Rico},
  doi          = {10.1007/s10107-020-01498-x},
  journal      = {Mathematical Programming},
  number       = {1},
  pages        = {455-481},
  shortjournal = {Math. Program.},
  title        = {A new contraction technique with applications to congruency-constrained cuts},
  volume       = {183},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). An exact algorithm for robust influence maximization.
<em>MP</em>, <em>183</em>(1), 419–453. (<a
href="https://doi.org/10.1007/s10107-020-01507-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a Branch-and-Cut algorithm for the robust influence maximization problem. The influence maximization problem aims to identify, in a social network, a set of given cardinality comprising actors that are able to influence the maximum number of other actors. We assume that the social network is given in the form of a graph with node thresholds to indicate the resistance of an actor to influence, and arc weights to represent the strength of the influence between two actors. In the robust version of the problem that we study, the node thresholds and arc weights are affected by uncertainty and we optimize over a worst-case scenario within given robustness budgets. We study properties of the robust solution and show that even computing the worst-case scenario for given robustness budgets is NP-hard. We implement an exact Branch-and-Cut as well as a heuristic Branch-Cut-and-Price. Numerical experiments show that we are able to solve to optimality instances of size comparable to other exact approaches in the literature for the non-robust problem, and we can tackle the robust version with similar performance. On larger instances ( $$\ge 2000$$ nodes), our heuristic Branch-Cut-and-Price significantly outperforms a 2-opt heuristic. An extended abstract of this paper appeared in the proceedings of IPCO 2019.},
  archive      = {J_MP},
  author       = {Nannicini, Giacomo and Sartor, Giorgio and Traversi, Emiliano and Wolfler Calvo, Roberto},
  doi          = {10.1007/s10107-020-01507-z},
  journal      = {Mathematical Programming},
  number       = {1},
  pages        = {419-453},
  shortjournal = {Math. Program.},
  title        = {An exact algorithm for robust influence maximization},
  volume       = {183},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Approximate multi-matroid intersection via iterative
refinement. <em>MP</em>, <em>183</em>(1), 397–418. (<a
href="https://doi.org/10.1007/s10107-020-01524-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce a new iterative rounding technique to round a point in a matroid polytope subject to further matroid constraints. This technique returns an independent set in one matroid with limited violations of the constraints of the other matroids. In addition to the classical steps of iterative relaxation approaches, we iteratively refine involved matroid constraints. This leads to more restrictive constraint systems whose structure can be exploited to prove the existence of constraints that can be dropped. Hence, throughout the iterations, we both tighten constraints and later relax them by dropping constraints under certain conditions. Due to the refinement step, we can deal with considerably more general constraint classes than existing iterative relaxation and rounding methods, which typically involve a single matroid polytope with additional simple cardinality constraints that do not overlap too much. We show that our rounding method, combined with an application of a matroid intersection algorithm, yields the first 2-approximation for finding a maximum-weight common independent set in 3 matroids. Moreover, our 2-approximation is LP-based and settles the integrality gap for the natural relaxation of the problem. Prior to our work, no upper bound better than 3 was known for the integrality gap, which followed from the greedy algorithm. We also discuss various other applications of our techniques, including an extension that allows us to handle a mixture of matroid and knapsack constraints.},
  archive      = {J_MP},
  author       = {Linhares, André and Olver, Neil and Swamy, Chaitanya and Zenklusen, Rico},
  doi          = {10.1007/s10107-020-01524-y},
  journal      = {Mathematical Programming},
  number       = {1},
  pages        = {397-418},
  shortjournal = {Math. Program.},
  title        = {Approximate multi-matroid intersection via iterative refinement},
  volume       = {183},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). The asymmetric traveling salesman path LP has constant
integrality ratio. <em>MP</em>, <em>183</em>(1), 379–395. (<a
href="https://doi.org/10.1007/s10107-019-01450-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We show that the classical LP relaxation of the asymmetric traveling salesman path problem (ATSPP) has constant integrality ratio. If $$\rho _{\text {ATSP}}$$ and $$\rho _{\text {ATSPP}}$$ denote the integrality ratios for the asymmetric TSP and its path version, then $$\rho _{\text {ATSPP}}\le 4\rho _{\text {ATSP}}-3$$ . We prove an even better bound for node-weighted instances: if the integrality ratio for ATSP on node-weighted instances is $$\rho _{\text {ATSP}}^{\text{ N }W}$$ , then the integrality ratio for ATSPP on node-weighted instances is at most $$2\rho _{\text {ATSP}}^{\text{ N }W}-1$$ . Moreover, we show that for ATSP node-weighted instances and unweighted digraph instances are almost equivalent. From this we deduce a lower bound of 2 on the integrality ratio of unweighted digraph instances.},
  archive      = {J_MP},
  author       = {Köhne, Anna and Traub, Vera and Vygen, Jens},
  doi          = {10.1007/s10107-019-01450-8},
  journal      = {Mathematical Programming},
  number       = {1},
  pages        = {379-395},
  shortjournal = {Math. Program.},
  title        = {The asymmetric traveling salesman path LP has constant integrality ratio},
  volume       = {183},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). An efficient characterization of submodular spanning tree
games. <em>MP</em>, <em>183</em>(1), 359–377. (<a
href="https://doi.org/10.1007/s10107-020-01499-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cooperative games form an important class of problems in game theory, where a key goal is to distribute a value among a set of players who are allowed to cooperate by forming coalitions. An outcome of the game is given by an allocation vector that assigns a value share to each player. A crucial aspect of such games is submodularity (or convexity). Indeed, convex instances of cooperative games exhibit several nice properties, e.g. regarding the existence and computation of allocations realizing some of the most important solution concepts proposed in the literature. For this reason, a relevant question is whether one can give a polynomial-time characterization of submodular instances, for prominent cooperative games that are in general non-convex. In this paper, we focus on a fundamental and widely studied cooperative game, namely the spanning tree game. An efficient recognition of submodular instances of this game was not known so far, and explicitly mentioned as an open question in the literature. We here settle this open problem by giving a polynomial-time characterization of submodular spanning tree games.},
  archive      = {J_MP},
  author       = {Koh, Zhuan Khye and Sanità, Laura},
  doi          = {10.1007/s10107-020-01499-w},
  journal      = {Mathematical Programming},
  number       = {1},
  pages        = {359-377},
  shortjournal = {Math. Program.},
  title        = {An efficient characterization of submodular spanning tree games},
  volume       = {183},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). On compact representations of voronoi cells of lattices.
<em>MP</em>, <em>183</em>(1), 337–358. (<a
href="https://doi.org/10.1007/s10107-019-01463-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In a seminal work, Micciancio and Voulgaris (SIAM J Comput 42(3):1364–1391, 2013) described a deterministic single-exponential time algorithm for the closest vector problem (CVP) on lattices. It is based on the computation of the Voronoi cell of the given lattice and thus may need exponential space as well. We address the major open question whether there exists such an algorithm that requires only polynomial space. To this end, we define a lattice basis to be c-compact if every facet normal of the Voronoi cell is a linear combination of the basis vectors using coefficients that are bounded by c in absolute value. Given such a basis, we get a polynomial space algorithm for CVP whose running time naturally depends on c. Thus, our main focus is the behavior of the smallest possible value of c, with the following results: there always exist c-compact bases, where c is bounded by $$n^2$$ for an n-dimensional lattice; there are lattices not admitting a c-compact basis with c growing sublinearly with the dimension; and every lattice with a zonotopal Voronoi cell has a 1-compact basis.},
  archive      = {J_MP},
  author       = {Hunkenschröder, Christoph and Reuland, Gina and Schymura, Matthias},
  doi          = {10.1007/s10107-019-01463-3},
  journal      = {Mathematical Programming},
  number       = {1},
  pages        = {337-358},
  shortjournal = {Math. Program.},
  title        = {On compact representations of voronoi cells of lattices},
  volume       = {183},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Dynamic flows with adaptive route choice. <em>MP</em>,
<em>183</em>(1), 309–335. (<a
href="https://doi.org/10.1007/s10107-020-01504-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study dynamic network flows and introduce a notion of instantaneous dynamic equilibrium (IDE) requiring that for any positive inflow into an edge, this edge must lie on a currently shortest path towards the respective sink. We measure current shortest path length by current waiting times in queues plus physical travel times. As our main results, we show:},
  archive      = {J_MP},
  author       = {Graf, Lukas and Harks, Tobias and Sering, Leon},
  doi          = {10.1007/s10107-020-01504-2},
  journal      = {Mathematical Programming},
  number       = {1},
  pages        = {309-335},
  shortjournal = {Math. Program.},
  title        = {Dynamic flows with adaptive route choice},
  volume       = {183},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A computational study of exact subgraph based SDP bounds for
max-cut, stable set and coloring. <em>MP</em>, <em>183</em>(1), 283–308.
(<a href="https://doi.org/10.1007/s10107-020-01512-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The “exact subgraph” approach was recently introduced as a hierarchical scheme to get increasingly tight semidefinite programming relaxations of several NP-hard graph optimization problems. Solving these relaxations is a computational challenge because of the potentially large number of violated subgraph constraints. We introduce a computational framework for these relaxations designed to cope with these difficulties. We suggest a partial Lagrangian dual, and exploit the fact that its evaluation decomposes into several independent subproblems. This opens the way to use the bundle method from non-smooth optimization to minimize the dual function. Finally computational experiments on the Max-Cut, stable set and coloring problem show the excellent quality of the bounds obtained with this approach.},
  archive      = {J_MP},
  author       = {Gaar, Elisabeth and Rendl, Franz},
  doi          = {10.1007/s10107-020-01512-2},
  journal      = {Mathematical Programming},
  number       = {1},
  pages        = {283-308},
  shortjournal = {Math. Program.},
  title        = {A computational study of exact subgraph based SDP bounds for max-cut, stable set and coloring},
  volume       = {183},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). An exact approach for the bilevel knapsack problem with
interdiction constraints and extensions. <em>MP</em>, <em>183</em>(1),
249–281. (<a href="https://doi.org/10.1007/s10107-020-01482-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider the bilevel knapsack problem with interdiction constraints, an extension of the classic 0–1 knapsack problem formulated as a Stackelberg game with two agents, a leader and a follower, that choose items from a common set and hold their own private knapsacks. First, the leader selects some items to be interdicted for the follower while satisfying a capacity constraint. Then the follower packs a set of the remaining items according to his knapsack constraint in order to maximize the profits. The goal of the leader is to minimize the follower’s total profit. We derive effective lower bounds for the bilevel knapsack problem and present an exact method that exploits the structure of the induced follower’s problem. The approach strongly outperforms the current state-of-the-art algorithms designed for the problem. We extend the same algorithmic framework to the interval min–max regret knapsack problem after providing a novel bilevel programming reformulation. Also for this problem, the proposed approach outperforms the exact algorithms available in the literature.},
  archive      = {J_MP},
  author       = {Della Croce, Federico and Scatamacchia, Rosario},
  doi          = {10.1007/s10107-020-01482-5},
  journal      = {Mathematical Programming},
  number       = {1},
  pages        = {249-281},
  shortjournal = {Math. Program.},
  title        = {An exact approach for the bilevel knapsack problem with interdiction constraints and extensions},
  volume       = {183},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A general framework for handling commitment in online
throughput maximization. <em>MP</em>, <em>183</em>(1), 215–247. (<a
href="https://doi.org/10.1007/s10107-020-01469-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study a fundamental online job admission problem where jobs with deadlines arrive online over time at their release dates, and the task is to determine a preemptive single-server schedule which maximizes the number of jobs that complete on time. To circumvent known impossibility results, we make a standard slackness assumption by which the feasible time window for scheduling a job is at least $$1+\varepsilon $$ times its processing time, for some $$\varepsilon &gt;0$$ . We quantify the impact that different provider commitment requirements have on the performance of online algorithms. Our main contribution is one universal algorithmic framework for online job admission both with and without commitments. Without commitment, our algorithm with a competitive ratio of $$\mathcal {O}(1/\varepsilon )$$ is the best possible (deterministic) for this problem. For commitment models, we give the first non-trivial performance bounds. If the commitment decisions must be made before a job’s slack becomes less than a $$\delta $$ -fraction of its size, we prove a competitive ratio of $$\mathcal {O}(\varepsilon /((\varepsilon -\delta )\delta ^2))$$ , for $$0&lt;\delta &lt;\varepsilon $$ . When a provider must commit upon starting a job, our bound is $$\mathcal {O}(1/\varepsilon ^2)$$ . Finally, we observe that for scheduling with commitment the restriction to the “unweighted” throughput model is essential; if jobs have individual weights, we rule out competitive deterministic algorithms.},
  archive      = {J_MP},
  author       = {Chen, Lin and Eberle, Franziska and Megow, Nicole and Schewior, Kevin and Stein, Cliff},
  doi          = {10.1007/s10107-020-01469-2},
  journal      = {Mathematical Programming},
  number       = {1},
  pages        = {215-247},
  shortjournal = {Math. Program.},
  title        = {A general framework for handling commitment in online throughput maximization},
  volume       = {183},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). <span class="math display"><em>ℓ</em><sub>1</sub></span>
-sparsity approximation bounds for packing integer programs.
<em>MP</em>, <em>183</em>(1), 195–214. (<a
href="https://doi.org/10.1007/s10107-020-01472-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider approximation algorithms for packing integer programs (PIPs) of the form $$\max {\langle c, x\rangle : Ax \le b, x \in {0,1}^n}$$ where A, b and c are nonnegative. We let $$W = \min _{i,j} b_i / A_{i,j}$$ denote the width of A which is at least 1. Previous work by Bansal et al. (Theory Comput 8(24):533–565, 2012) obtained an $$\varOmega (\frac{1}{\varDelta _0^{1/\lfloor W \rfloor }})$$ -approximation ratio where $$\varDelta _0$$ is the maximum number of nonzeroes in any column of A (in other words the $$\ell _0$$ -column sparsity of A). They raised the question of obtaining approximation ratios based on the $$\ell _1$$ -column sparsity of A (denoted by $$\varDelta _1$$ ) which can be much smaller than $$\varDelta _0$$ . Motivated by recent work on covering integer programs (Chekuri and Quanrud, in: Proceedings of the Thirtieth Annual ACM-SIAM Symposium on Discrete Algorithms, pp 1596–1615. SIAM, 2019; Chen et al., in: Proceedings of the Twenty-seventh Annual ACM-SIAM Symposium on Discrete Algorithms, pp 1984–2003. SIAM, 2016) we show that simple algorithms based on randomized rounding followed by alteration, similar to those of Bansal et al. (Theory Comput 8(24):533–565, 2012) (but with a twist), yield approximation ratios for PIPs based on $$\varDelta _1$$ . First, following an integrality gap example from (Theory Comput 8(24):533–565, 2012), we observe that the case of $$W=1$$ is as hard as maximum independent set even when $$\varDelta _1 \le 2$$ . In sharp contrast to this negative result, as soon as width is strictly larger than one, we obtain positive results via the natural LP relaxation. For PIPs with width $$W = 1 + \epsilon $$ where $$\epsilon \in (0,1]$$ , we obtain an $$\varOmega (\epsilon ^2/\varDelta _1)$$ -approximation. In the large width regime, when $$W \ge 2$$ , we obtain an $$\varOmega ((\frac{1}{1 + \varDelta _1/W})^{1/(W-1)})$$ -approximation. We also obtain a $$(1-\epsilon )$$ -approximation when $$W = \varOmega (\frac{\log (\varDelta _1/\epsilon )}{\epsilon ^2})$$ . Viewing the rounding algorithms as contention resolution schemes, we obtain approximation algorithms in the more general setting when the objective is a non-negative submodular function.},
  archive      = {J_MP},
  author       = {Chekuri, Chandra and Quanrud, Kent and Torres, Manuel R.},
  doi          = {10.1007/s10107-020-01472-7},
  journal      = {Mathematical Programming},
  number       = {1},
  pages        = {195-214},
  shortjournal = {Math. Program.},
  title        = {$$\ell _1$$ -sparsity approximation bounds for packing integer programs},
  volume       = {183},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020b). Improving the integrality gap for multiway cut.
<em>MP</em>, <em>183</em>(1), 171–193. (<a
href="https://doi.org/10.1007/s10107-020-01485-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the multiway cut problem, we are given an undirected graph with non-negative edge weights and a collection of k terminal nodes, and the goal is to partition the node set of the graph into k non-empty parts each containing exactly one terminal, so that the total weight of the edges crossing the partition is minimized. The multiway cut problem for $$k\ge 3$$ is APX-hard. For arbitrary k, the best-known approximation factor is 1.2965 due to Sharma and Vondrák (Proceedings of the forty-sixth annual ACM symposium on theory of computing, STOC, 2014) while the best known inapproximability result due to Angelidakis et al. (Integer programming and combinatorial optimization, IPCO, 2017) rules out efficient algorithms to achieve an approximation factor less than 1.2 under the unique games conjecture (UGC). In this work, we improve the lower bound to $$1.20016$$ under UGC by constructing an integrality gap instance for the CKR relaxation. The CKR relaxation embeds the graph into a simplex and it is known that its integrality gap translates to inapproximability under UGC. A technical challenge in improving the integrality gap has been the lack of geometric tools to understand higher-dimensional simplices. Our instance is a non-trivial 3-dimensional instance that overcomes this technical challenge. We analyze the gap of the instance by viewing it as a convex combination of 2-dimensional instances and a uniform 3-dimensional instance. We believe that this technique could be exploited further to construct instances with larger integrality gap. One of the ingredients of our proof technique is a generalization of a result on Sperner admissible labelings due to Mirzakhani and Vondrák (Proceedings of the twenty-sixth annual ACM-SIAM symposium on discrete algorithms, SODA, 2015) that might be of independent combinatorial interest.},
  archive      = {J_MP},
  author       = {Bérczi, Kristóf and Chandrasekaran, Karthekeyan and Király, Tamás and Madan, Vivek},
  doi          = {10.1007/s10107-020-01485-2},
  journal      = {Mathematical Programming},
  number       = {1},
  pages        = {171-193},
  shortjournal = {Math. Program.},
  title        = {Improving the integrality gap for multiway cut},
  volume       = {183},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Online submodular maximization: Beating 1/2 made simple.
<em>MP</em>, <em>183</em>(1), 149–169. (<a
href="https://doi.org/10.1007/s10107-019-01459-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Submodular Welfare Maximization problem (SWM) captures an important subclass of combinatorial auctions and has been studied extensively. In particular, it has been studied in a natural online setting in which items arrive one-by-one and should be allocated irrevocably upon arrival. For this setting, Korula et al. (SIAM J Comput 47(3):1056–1086, 2018) were able to show that the greedy algorithm is 0.5052-competitive when the items arrive in a uniformly random order. Unfortunately, however, their proof is very long and involved. In this work, we present an (arguably) much simpler analysis of the same algorithm that provides a slightly better guarantee of 0.5096-competitiveness. Moreover, this analysis applies also to a generalization of online SWM in which the sets defining a (simple) partition matroid arrive online in a uniformly random order, and we would like to maximize a monotone submodular function subject to this matroid. Furthermore, for this more general problem, we prove an upper bound of 0.574 on the competitive ratio of the greedy algorithm, ruling out the possibility that the competitiveness of this natural algorithm matches the optimal offline approximation ratio of $$1-1/e$$ .},
  archive      = {J_MP},
  author       = {Buchbinder, Niv and Feldman, Moran and Filmus, Yuval and Garg, Mohit},
  doi          = {10.1007/s10107-019-01459-z},
  journal      = {Mathematical Programming},
  number       = {1},
  pages        = {149-169},
  shortjournal = {Math. Program.},
  title        = {Online submodular maximization: Beating 1/2 made simple},
  volume       = {183},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Outer-product-free sets for polynomial optimization and
oracle-based cuts. <em>MP</em>, <em>183</em>(1), 105–148. (<a
href="https://doi.org/10.1007/s10107-020-01484-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper introduces cutting planes that involve minimal structural assumptions, enabling the generation of strong polyhedral relaxations for a broad class of problems. We consider valid inequalities for the set $$S\cap P$$ , where S is a closed set, and P is a polyhedron. Given an oracle that provides the distance from a point to S, we construct a pure cutting plane algorithm which is shown to converge if the initial relaxation is a polyhedron. These cuts are generated from convex forbidden zones, or S-free sets, derived from the oracle. We also consider the special case of polynomial optimization. Accordingly we develop a theory of outer-product-free sets, where S is the set of real, symmetric matrices of the form $$xx^T$$ . All maximal outer-product-free sets of full dimension are shown to be convex cones and we identify several families of such sets. These families are used to generate strengthened intersection cuts that can separate any infeasible extreme point of a linear programming relaxation efficiently. Computational experiments demonstrate the promise of our approach.},
  archive      = {J_MP},
  author       = {Bienstock, Daniel and Chen, Chen and Muñoz, Gonzalo},
  doi          = {10.1007/s10107-020-01484-3},
  journal      = {Mathematical Programming},
  number       = {1},
  pages        = {105-148},
  shortjournal = {Math. Program.},
  title        = {Outer-product-free sets for polynomial optimization and oracle-based cuts},
  volume       = {183},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Symmetry-breaking inequalities for ILP with structured
sub-symmetry. <em>MP</em>, <em>183</em>(1), 61–103. (<a
href="https://doi.org/10.1007/s10107-020-01491-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider integer linear programs whose solutions are binary matrices and whose (sub-)symmetry groups are symmetric groups acting on (sub-)columns. Such structured sub-symmetry groups arise in important classes of combinatorial problems, e.g. graph coloring or unit commitment. For a priori known (sub-)symmetries, we propose a framework to build (sub-)symmetry-breaking inequalities for such problems, by introducing one additional variable per considered sub-symmetry group. The derived inequalities are full-symmetry-breaking and in polynomial number w.r.t. the number of sub-symmetry groups considered. The proposed framework is applied to derive such inequalities when the symmetry group is the symmetric group acting on the columns. It is also applied to derive sub-symmetry-breaking inequalities for the graph coloring problem. Experimental results give insight into how to select the right inequality subset in order to efficiently break sub-symmetries. Finally, the framework is applied to derive (sub-)symmetry breaking inequalities for Min-up/min-down Unit Commitment Problem with or without ramp constraints. We show the effectiveness of the approach by presenting an experimental comparison with state-of-the-art symmetry-breaking formulations.},
  archive      = {J_MP},
  author       = {Bendotti, Pascale and Fouilhoux, Pierre and Rottner, Cécile},
  doi          = {10.1007/s10107-020-01491-4},
  journal      = {Mathematical Programming},
  number       = {1},
  pages        = {61-103},
  shortjournal = {Math. Program.},
  title        = {Symmetry-breaking inequalities for ILP with structured sub-symmetry},
  volume       = {183},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Extended formulations from communication protocols in
output-efficient time. <em>MP</em>, <em>183</em>(1), 41–59. (<a
href="https://doi.org/10.1007/s10107-020-01535-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deterministic protocols are well-known tools to obtain extended formulations, with many applications to polytopes arising in combinatorial optimization. Although constructive, those tools are not output-efficient, since the time needed to produce the extended formulation also depends on the number of rows of the slack matrix (hence, on the exact description in the original space). We give general sufficient conditions under which those tools can be implemented as to be output-efficient, showing applications to e.g. Yannakakis’ extended formulation for the stable set polytope of perfect graphs, for which, to the best of our knowledge, an efficient construction was previously not known. For specific classes of polytopes, we give also a direct, efficient construction of extended formulations arising from protocols. Finally, we deal with extended formulations coming from unambiguous non-deterministic protocols.},
  archive      = {J_MP},
  author       = {Aprile, Manuel and Faenza, Yuri},
  doi          = {10.1007/s10107-020-01535-9},
  journal      = {Mathematical Programming},
  number       = {1},
  pages        = {41-59},
  shortjournal = {Math. Program.},
  title        = {Extended formulations from communication protocols in output-efficient time},
  volume       = {183},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Strong mixed-integer programming formulations for trained
neural networks. <em>MP</em>, <em>183</em>(1), 3–39. (<a
href="https://doi.org/10.1007/s10107-020-01474-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present strong mixed-integer programming (MIP) formulations for high-dimensional piecewise linear functions that correspond to trained neural networks. These formulations can be used for a number of important tasks, such as verifying that an image classification network is robust to adversarial inputs, or solving decision problems where the objective function is a machine learning model. We present a generic framework, which may be of independent interest, that provides a way to construct sharp or ideal formulations for the maximum of d affine functions over arbitrary polyhedral input domains. We apply this result to derive MIP formulations for a number of the most popular nonlinear operations (e.g. ReLU and max pooling) that are strictly stronger than other approaches from the literature. We corroborate this computationally, showing that our formulations are able to offer substantial improvements in solve time on verification tasks for image classification networks.},
  archive      = {J_MP},
  author       = {Anderson, Ross and Huchette, Joey and Ma, Will and Tjandraatmadja, Christian and Vielma, Juan Pablo},
  doi          = {10.1007/s10107-020-01474-5},
  journal      = {Mathematical Programming},
  number       = {1},
  pages        = {3-39},
  shortjournal = {Math. Program.},
  title        = {Strong mixed-integer programming formulations for trained neural networks},
  volume       = {183},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Preface. <em>MP</em>, <em>183</em>(1), 1. (<a
href="https://doi.org/10.1007/s10107-020-01545-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_MP},
  doi          = {10.1007/s10107-020-01545-7},
  journal      = {Mathematical Programming},
  number       = {1},
  pages        = {1},
  shortjournal = {Math. Program.},
  title        = {Preface},
  volume       = {183},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A filtered bucket-clustering method for projection onto the
simplex and the <span class="math display"><em>ℓ</em><sub>1</sub></span>
ball. <em>MP</em>, <em>182</em>(1), 445–464. (<a
href="https://doi.org/10.1007/s10107-019-01401-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose in this paper a new method processing the projection of an arbitrary size vector onto the probabilistic simplex or the $$\ell _1$$ ball. Our method merges two principles. The first one is an original search of the projection using a bucket algorithm. The second one is a filtering, on the fly, of the values that cannot be part of the projection. The combination of these two principles offers a simple and efficient algorithm whose worst-case complexity is linear with respect to the vector size. Furthermore, the proposed algorithm exploits the representation of numeric values in digital computers to define the number of buckets and to accelerate the filtering.},
  archive      = {J_MP},
  author       = {Perez, Guillaume and Barlaud, Michel and Fillatre, Lionel and Régin, Jean-Charles},
  doi          = {10.1007/s10107-019-01401-3},
  journal      = {Mathematical Programming},
  number       = {1},
  pages        = {445-464},
  shortjournal = {Math. Program.},
  title        = {A filtered bucket-clustering method for projection onto the simplex and the $$\ell _1$$ ball},
  volume       = {182},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). On the complexity of detecting convexity over a box.
<em>MP</em>, <em>182</em>(1), 429–443. (<a
href="https://doi.org/10.1007/s10107-019-01396-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {It has recently been shown that the problem of testing global convexity of polynomials of degree four is strongly NP-hard, answering an open question of N.Z. Shor. This result is minimal in the degree of the polynomial when global convexity is of concern. In a number of applications however, one is interested in testing convexity only over a compact region, most commonly a box (i.e., a hyper-rectangle). In this paper, we show that this problem is also strongly NP-hard, in fact for polynomials of degree as low as three. This result is minimal in the degree of the polynomial and in some sense justifies why convexity detection in nonlinear optimization solvers is limited to quadratic functions or functions with special structure. As a byproduct, our proof shows that the problem of testing whether all matrices in an interval family are positive semidefinite is strongly NP-hard. This problem, which was previously shown to be (weakly) NP-hard by Nemirovski, is of independent interest in the theory of robust control.},
  archive      = {J_MP},
  author       = {Ahmadi, Amir Ali and Hall, Georgina},
  doi          = {10.1007/s10107-019-01396-x},
  journal      = {Mathematical Programming},
  number       = {1},
  pages        = {429-443},
  shortjournal = {Math. Program.},
  title        = {On the complexity of detecting convexity over a box},
  volume       = {182},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). The geometry of SDP-exactness in quadratic optimization.
<em>MP</em>, <em>182</em>(1), 399–428. (<a
href="https://doi.org/10.1007/s10107-019-01399-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Consider the problem of minimizing a quadratic objective subject to quadratic equations. We study the semialgebraic region of objective functions for which this problem is solved by its semidefinite relaxation. For the Euclidean distance problem, this is a bundle of spectrahedral shadows surrounding the given variety. We characterize the algebraic boundary of this region and we derive a formula for its degree.},
  archive      = {J_MP},
  author       = {Cifuentes, Diego and Harris, Corey and Sturmfels, Bernd},
  doi          = {10.1007/s10107-019-01399-8},
  journal      = {Mathematical Programming},
  number       = {1},
  pages        = {399-428},
  shortjournal = {Math. Program.},
  title        = {The geometry of SDP-exactness in quadratic optimization},
  volume       = {182},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Sum-of-squares hierarchy lower bounds for symmetric
formulations. <em>MP</em>, <em>182</em>(1), 369–397. (<a
href="https://doi.org/10.1007/s10107-019-01398-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce a method for proving Sum-of-Squares (SoS)/Lasserre hierarchy lower bounds when the initial problem formulation exhibits a high degree of symmetry. Our main technical theorem allows us to reduce the study of the positive semidefiniteness to the analysis of “well-behaved” univariate polynomial inequalities. We illustrate the technique on two problems, one unconstrained and the other with constraints. More precisely, we give a short elementary proof of Grigoriev/Laurent lower bound for finding the integer cut polytope of the complete graph. We also show that the SoS hierarchy requires a non-constant number of rounds to improve the initial integrality gap of 2 for the Min-Knapsack linear program strengthened with cover inequalities.},
  archive      = {J_MP},
  author       = {Kurpisz, Adam and Leppänen, Samuli and Mastrolilli, Monaldo},
  doi          = {10.1007/s10107-019-01398-9},
  journal      = {Mathematical Programming},
  number       = {1},
  pages        = {369-397},
  shortjournal = {Math. Program.},
  title        = {Sum-of-squares hierarchy lower bounds for symmetric formulations},
  volume       = {182},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Improved approximation algorithms for hitting 3-vertex
paths. <em>MP</em>, <em>182</em>(1), 355–367. (<a
href="https://doi.org/10.1007/s10107-019-01395-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study the problem of deleting a minimum cost set of vertices from a given vertex-weighted graph in such a way that the resulting graph has no induced path on three vertices. This problem is often called cluster vertex deletion in the literature and admits a straightforward 3-approximation algorithm since it is a special case of the vertex cover problem on a 3-uniform hypergraph. Recently, You, Wang, and Cao described an efficient 5/2-approximation algorithm for the unweighted version of the problem. Our main result is a 9/4-approximation algorithm for arbitrary weights, using the local ratio technique. We further conjecture that the problem admits a 2-approximation algorithm and give some support for the conjecture. This is in sharp contrast with the fact that the similar problem of deleting vertices to eliminate all triangles in a graph is known to be UGC-hard to approximate to within a ratio better than 3, as proved by Guruswami and Lee.},
  archive      = {J_MP},
  author       = {Fiorini, Samuel and Joret, Gwenaël and Schaudt, Oliver},
  doi          = {10.1007/s10107-019-01395-y},
  journal      = {Mathematical Programming},
  number       = {1},
  pages        = {355-367},
  shortjournal = {Math. Program.},
  title        = {Improved approximation algorithms for hitting 3-vertex paths},
  volume       = {182},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). The matching augmentation problem: A <span
class="math display">$$\frac{7}{4}$$</span>-approximation algorithm.
<em>MP</em>, <em>182</em>(1), 315–354. (<a
href="https://doi.org/10.1007/s10107-019-01394-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a $$\frac{7}{4}$$ approximation algorithm for the matching augmentation problem (MAP): given a multi-graph with edges of cost either zero or one such that the edges of cost zero form a matching, find a 2-edge connected spanning subgraph (2-ECSS) of minimum cost. We first present a reduction of any given MAP instance to a collection of well-structured MAP instances such that the approximation guarantee is preserved. Then we present a $$\frac{7}{4}$$ approximation algorithm for a well-structured MAP instance. The algorithm starts with a min-cost 2-edge cover and then applies ear-augmentation steps. We analyze the cost of the ear-augmentations using an approach similar to the one proposed by Vempala and Vetta for the (unweighted) min-size 2-ECSS problem (in: Jansen and Khuller (eds.) Approximation Algorithms for Combinatorial Optimization, Third International Workshop, APPROX 2000, Proceedings, LNCS 1913, pp.262–273, Springer, Berlin, 2000).},
  archive      = {J_MP},
  author       = {Cheriyan, J. and Dippel, J. and Grandoni, F. and Khan, A. and Narayan, V. V.},
  doi          = {10.1007/s10107-019-01394-z},
  journal      = {Mathematical Programming},
  number       = {1},
  pages        = {315-354},
  shortjournal = {Math. Program.},
  title        = {The matching augmentation problem: A $$\frac{7}{4}$$-approximation algorithm},
  volume       = {182},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Vector coloring the categorical product of graphs.
<em>MP</em>, <em>182</em>(1), 275–314. (<a
href="https://doi.org/10.1007/s10107-019-01393-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A vector t-coloring of a graph is an assignment of real vectors $$p_1, \ldots , p_n$$ to its vertices such that $$p_i^Tp_i = t-1,$$ for all $$i=1, \ldots , n$$ and $$p_i^Tp_j \le -1$$, whenever i and j are adjacent. The vector chromatic number of G is the smallest number $$t \ge 1$$ for which a vector t-coloring of G exists. For a graph H and a vector t-coloring $$p_1,\ldots ,p_n$$ of G, the map taking $$(i,\ell )\in V(G)\times V(H)$$ to $$p_i$$ is a vector t-coloring of the categorical product $$G \times H$$. It follows that the vector chromatic number of $$G \times H$$ is at most the minimum of the vector chromatic numbers of the factors. We prove that equality always holds, constituting a vector coloring analog of the famous Hedetniemi Conjecture from graph coloring. Furthermore, we prove necessary and sufficient conditions under which all optimal vector colorings of $$G\times H$$ are induced by optimal vector colorings of the factors. Our proofs rely on various semidefinite programming formulations of the vector chromatic number and a theory of optimal vector colorings we develop along the way, which is of independent interest.},
  archive      = {J_MP},
  author       = {Godsil, Chris and Roberson, David E. and Rooney, Brendan and Šámal, Robert and Varvitsiotis, Antonios},
  doi          = {10.1007/s10107-019-01393-0},
  journal      = {Mathematical Programming},
  number       = {1},
  pages        = {275-314},
  shortjournal = {Math. Program.},
  title        = {Vector coloring the categorical product of graphs},
  volume       = {182},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Uniqueness of DRS as the 2 operator resolvent-splitting and
impossibility of 3 operator resolvent-splitting. <em>MP</em>,
<em>182</em>(1), 233–273. (<a
href="https://doi.org/10.1007/s10107-019-01403-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Given the success of Douglas–Rachford splitting (DRS), it is natural to ask whether DRS can be generalized. Are there other 2 operator resolvent-splittings sharing the favorable properties of DRS? Can DRS be generalized to 3 operators? This work presents the answers: no and no. In a certain sense, DRS is the unique 2 operator resolvent-splitting, and generalizing DRS to 3 operators is impossible without lifting, where lifting roughly corresponds to enlarging the problem size. The impossibility result further raises a question. How much lifting is necessary to generalize DRS to 3 operators? This work presents the answer by providing a novel 3 operator resolvent-splitting with provably minimal lifting that directly generalizes DRS.},
  archive      = {J_MP},
  author       = {Ryu, Ernest K.},
  doi          = {10.1007/s10107-019-01403-1},
  journal      = {Mathematical Programming},
  number       = {1},
  pages        = {233-273},
  shortjournal = {Math. Program.},
  title        = {Uniqueness of DRS as the 2 operator resolvent-splitting and impossibility of 3 operator resolvent-splitting},
  volume       = {182},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Compact representation of near-optimal integer programming
solutions. <em>MP</em>, <em>182</em>(1), 199–232. (<a
href="https://doi.org/10.1007/s10107-019-01390-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {It is often useful in practice to explore near-optimal solutions of an integer programming problem. We show how all solutions within a given tolerance of the optimal value can be efficiently and compactly represented in a weighted decision diagram. The structure of the diagram facilitates rapid processing of a wide range of queries about the near-optimal solution space, as well as reoptimization after changes in the objective function. We also exploit the paradoxical fact that the diagram can be reduced in size if it is allowed to represent additional solutions. We show that a “sound reduction” operation, applied repeatedly, yields the smallest such diagram that is suitable for postoptimality analysis, and one that is typically far smaller than a tree that represents the same set of near-optimal solutions. We conclude that postoptimality analysis based on sound-reduced diagrams has the potential to extract significantly more useful information from an integer programming model than was previously feasible.},
  archive      = {J_MP},
  author       = {Serra, Thiago and Hooker, J. N.},
  doi          = {10.1007/s10107-019-01390-3},
  journal      = {Mathematical Programming},
  number       = {1},
  pages        = {199-232},
  shortjournal = {Math. Program.},
  title        = {Compact representation of near-optimal integer programming solutions},
  volume       = {182},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Distances to lattice points in knapsack polyhedra.
<em>MP</em>, <em>182</em>(1), 175–198. (<a
href="https://doi.org/10.1007/s10107-019-01392-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We give an optimal upper bound for the $$\ell _{\infty }$$-distance from a vertex of a knapsack polyhedron to its nearest feasible lattice point. In a randomised setting, we show that the upper bound can be significantly improved on average. As a corollary, we obtain an optimal upper bound for the additive integrality gap of integer knapsack problems and show that the integrality gap of a “typical” knapsack problem is drastically smaller than the integrality gap that occurs in a worst case scenario. We also prove that, in a generic case, the integer programming gap admits a natural optimal lower bound.},
  archive      = {J_MP},
  author       = {Aliev, Iskander and Henk, Martin and Oertel, Timm},
  doi          = {10.1007/s10107-019-01392-1},
  journal      = {Mathematical Programming},
  number       = {1},
  pages        = {175-198},
  shortjournal = {Math. Program.},
  title        = {Distances to lattice points in knapsack polyhedra},
  volume       = {182},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Stochastic packing integer programs with few queries.
<em>MP</em>, <em>182</em>(1), 141–174. (<a
href="https://doi.org/10.1007/s10107-019-01388-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider a stochastic variant of the packing-type integer linear programming problem, which contains random variables in the objective vector. We are allowed to reveal each entry of the objective vector by conducting a query, and the task is to find a good solution by conducting a small number of queries. We propose a general framework of adaptive and non-adaptive algorithms for this problem, and provide a unified methodology for analyzing the performance of those algorithms. We also demonstrate our framework by applying it to a variety of stochastic combinatorial optimization problems such as matching, matroid, and stable set problems.},
  archive      = {J_MP},
  author       = {Maehara, Takanori and Yamaguchi, Yutaro},
  doi          = {10.1007/s10107-019-01388-x},
  journal      = {Mathematical Programming},
  number       = {1},
  pages        = {141-174},
  shortjournal = {Math. Program.},
  title        = {Stochastic packing integer programs with few queries},
  volume       = {182},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Goal scoring, coherent loss and applications to machine
learning. <em>MP</em>, <em>182</em>(1), 103–140. (<a
href="https://doi.org/10.1007/s10107-019-01387-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Motivated by the binary classification problem in machine learning, we study in this paper a class of decision problems where the decision maker has a list of goals, from which he aims to attain the maximal possible number of goals. In binary classification, this essentially means seeking a prediction rule to achieve the lowest probability of misclassification, and computationally it involves minimizing a (difficult) non-convex, 0–1 loss function. To address the intractability, previous methods consider minimizing the cumulative loss—the sum of convex surrogates of the 0–1 loss of each goal. We revisit this paradigm and develop instead an axiomatic framework by proposing a set of salient properties on functions for goal scoring and then propose the coherent loss approach, which is a tractable upper-bound of the loss over the entire set of goals. We show that the proposed approach yields a strictly tighter approximation to the total loss (i.e., the number of missed goals) than any convex cumulative loss approach while preserving the convexity of the underlying optimization problem. Moreover, this approach, applied to for binary classification, also has a robustness interpretation which builds a connection to robust SVMs.},
  archive      = {J_MP},
  author       = {Yang, Wenzhuo and Sim, Melvyn and Xu, Huan},
  doi          = {10.1007/s10107-019-01387-y},
  journal      = {Mathematical Programming},
  number       = {1},
  pages        = {103-140},
  shortjournal = {Math. Program.},
  title        = {Goal scoring, coherent loss and applications to machine learning},
  volume       = {182},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A tractable approach for designing piecewise affine policies
in two-stage adjustable robust optimization. <em>MP</em>,
<em>182</em>(1), 57–102. (<a
href="https://doi.org/10.1007/s10107-019-01385-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider the problem of designing piecewise affine policies for two-stage adjustable robust linear optimization problems under right-hand side uncertainty. It is well known that a piecewise affine policy is optimal although the number of pieces can be exponentially large. A significant challenge in designing a practical piecewise affine policy is constructing good pieces of the uncertainty set. Here we address this challenge by introducing a new framework in which the uncertainty set is “approximated” by a “dominating” simplex. The corresponding policy is then based on a mapping from the uncertainty set to the simplex. Although our piecewise affine policy has exponentially many pieces, it can be computed efficiently by solving a compact linear program given the dominating simplex. Furthermore, we can find the dominating simplex in a closed form if the uncertainty set satisfies some symmetries and can be computed using a MIP in general. We would like to remark that our policy is an approximate piecewise-affine policy and is not necessarily a generalization of the class of affine policies. Nevertheless, the performance of our policy is significantly better than the affine policy for many important uncertainty sets, such as ellipsoids and norm-balls, both theoretically and numerically. For instance, for hypersphere uncertainty set, our piecewise affine policy can be computed by an LP and gives a $$O(m^{1/4})$$ -approximation whereas the affine policy requires us to solve a second order cone program and has a worst-case performance bound of $$O(\sqrt{m})$$ .},
  archive      = {J_MP},
  author       = {Ben-Tal, Aharon and El Housni, Omar and Goyal, Vineet},
  doi          = {10.1007/s10107-019-01385-0},
  journal      = {Mathematical Programming},
  number       = {1},
  pages        = {57-102},
  shortjournal = {Math. Program.},
  title        = {A tractable approach for designing piecewise affine policies in two-stage adjustable robust optimization},
  volume       = {182},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). On the cost of solving augmented lagrangian subproblems.
<em>MP</em>, <em>182</em>(1), 37–55. (<a
href="https://doi.org/10.1007/s10107-019-01384-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {At each iteration of the augmented Lagrangian algorithm, a nonlinear subproblem is being solved. The number of inner iterations (of some/any method) needed to obtain a solution of the subproblem, or even a suitable approximate stationary point, is in principle unknown. In this paper we show that to compute an approximate stationary point sufficient to guarantee local superlinear convergence of the augmented Lagrangian iterations, it is enough to solve two quadratic programming problems (or two linear systems in the equality-constrained case). In other words, two inner Newtonian iterations are sufficient. To the best of our knowledge, such results are not available even under the strongest assumptions (of second-order sufficiency, strict complementarity, and the linear independence constraint qualification). Our analysis is performed under second-order sufficiency only, which is the weakest assumption for obtaining local convergence and rate of convergence of outer iterations of the augmented Lagrangian algorithm. The structure of the quadratic problems in question is related to the stabilized sequential quadratic programming and to second-order corrections.},
  archive      = {J_MP},
  author       = {Fernández, Damián and Solodov, Mikhail},
  doi          = {10.1007/s10107-019-01384-1},
  journal      = {Mathematical Programming},
  number       = {1},
  pages        = {37-55},
  shortjournal = {Math. Program.},
  title        = {On the cost of solving augmented lagrangian subproblems},
  volume       = {182},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). The multiproximal linearization method for convex composite
problems. <em>MP</em>, <em>182</em>(1), 1–36. (<a
href="https://doi.org/10.1007/s10107-019-01382-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Composite minimization involves a collection of smooth functions which are aggregated in a nonsmooth manner. In the convex setting, we design an algorithm by linearizing each smooth component in accordance with its main curvature. The resulting method, called the Multiprox method, consists in solving successively simple problems (e.g., constrained quadratic problems) which can also feature some proximal operators. To study the complexity and the convergence of this method, we are led to prove a new type of qualification condition and to understand the impact of multipliers on the complexity bounds. We obtain explicit complexity results of the form $$O(\frac{1}{k})$$ involving new types of constant terms. A distinctive feature of our approach is to be able to cope with oracles involving moving constraints. Our method is flexible enough to include the moving balls method, the proximal Gauss–Newton’s method, or the forward–backward splitting, for which we recover known complexity results or establish new ones. We show through several numerical experiments how the use of multiple proximal terms can be decisive for problems with complex geometries.},
  archive      = {J_MP},
  author       = {Bolte, Jérôme and Chen, Zheng and Pauwels, Edouard},
  doi          = {10.1007/s10107-019-01382-3},
  journal      = {Mathematical Programming},
  number       = {1},
  pages        = {1-36},
  shortjournal = {Math. Program.},
  title        = {The multiproximal linearization method for convex composite problems},
  volume       = {182},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Risk minimization, regret minimization and progressive
hedging algorithms. <em>MP</em>, <em>181</em>(2), 509–530. (<a
href="https://doi.org/10.1007/s10107-020-01471-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper begins with a study on the dual representations of risk and regret measures and their impact on modeling multistage decision making under uncertainty. A relationship between risk envelopes and regret envelopes is established by using the Lagrangian duality theory. Such a relationship opens a door to a decomposition scheme, called progressive hedging, for solving multistage risk minimization and regret minimization problems. In particular, the classical progressive hedging algorithm is modified in order to handle a new class of linkage constraints that arises from reformulations and other applications of risk and regret minimization problems. Numerical results are provided to show the efficiency of the progressive hedging algorithms.},
  archive      = {J_MP},
  author       = {Sun, Jie and Yang, Xinmin and Yao, Qiang and Zhang, Min},
  doi          = {10.1007/s10107-020-01471-8},
  journal      = {Mathematical Programming},
  number       = {2},
  pages        = {509-530},
  shortjournal = {Math. Program.},
  title        = {Risk minimization, regret minimization and progressive hedging algorithms},
  volume       = {181},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Convex approximations for two-stage mixed-integer mean-risk
recourse models with conditional value-at-risk. <em>MP</em>,
<em>181</em>(2), 473–507. (<a
href="https://doi.org/10.1007/s10107-019-01428-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In traditional two-stage mixed-integer recourse models, the expected value of the total costs is minimized. In order to address risk-averse attitudes of decision makers, we consider a weighted mean-risk objective instead. Conditional value-at-risk is used as our risk measure. Integrality conditions on decision variables make the model non-convex and hence, hard to solve. To tackle this problem, we derive convex approximation models and corresponding error bounds, that depend on the total variations of the density functions of the random right-hand side variables in the model. We show that the error bounds converge to zero if these total variations go to zero. In addition, for the special cases of totally unimodular and simple integer recourse models we derive sharper error bounds.},
  archive      = {J_MP},
  author       = {van Beesten, E. Ruben and Romeijnders, Ward},
  doi          = {10.1007/s10107-019-01428-6},
  journal      = {Mathematical Programming},
  number       = {2},
  pages        = {473-507},
  shortjournal = {Math. Program.},
  title        = {Convex approximations for two-stage mixed-integer mean-risk recourse models with conditional value-at-risk},
  volume       = {181},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Minimizing buffered probability of exceedance by progressive
hedging. <em>MP</em>, <em>181</em>(2), 453–472. (<a
href="https://doi.org/10.1007/s10107-019-01462-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Stochastic programming problems have for a long time been posed in terms of minimizing the expected value of a random variable influenced by decision variables, but alternative objectives can also be considered, such as minimizing a measure of risk. Here something different is introduced: minimizing the buffered probability of exceedance for a specified loss threshold. The buffered version of the traditional concept of probability of exceedance has recently been developed with many attractive properties that are conducive to successful optimization, in contrast to the usual concept, which is often posed simply as the probability of failure. The main contribution here is to demonstrate that in minimizing buffered probability of exceedance the underlying convexities in a stochastic programming problem can be maintained and the progressive hedging algorithm can be employed to compute a solution.},
  archive      = {J_MP},
  author       = {Rockafellar, R. Tyrrell and Uryasev, Stan},
  doi          = {10.1007/s10107-019-01462-4},
  journal      = {Mathematical Programming},
  number       = {2},
  pages        = {453-472},
  shortjournal = {Math. Program.},
  title        = {Minimizing buffered probability of exceedance by progressive hedging},
  volume       = {181},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Solving lagrangian variational inequalities with
applications to stochastic programming. <em>MP</em>, <em>181</em>(2),
435–451. (<a href="https://doi.org/10.1007/s10107-019-01458-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Lagrangian variational inequalities feature both primal and dual elements in expressing first-order conditions for optimality in a wide variety of settings where “multipliers” in a very general sense need to be brought in. Their stochastic version relates to problems of stochastic programming and covers not only classical formats with inequality constraints but also composite models with nonsmooth objectives. The progressive hedging algorithm, as a means of solving stochastic programming problems, has however focused so far only on optimality conditions that correspond to variational inequalities in primal variables alone. Here that limitation is removed by appealing to a recent extension of progressive hedging to multistage stochastic variational inequalities in general.},
  archive      = {J_MP},
  author       = {Rockafellar, R. Tyrrell and Sun, Jie},
  doi          = {10.1007/s10107-019-01458-0},
  journal      = {Mathematical Programming},
  number       = {2},
  pages        = {435-451},
  shortjournal = {Math. Program.},
  title        = {Solving lagrangian variational inequalities with applications to stochastic programming},
  volume       = {181},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Theoretical and empirical analysis of trading activity.
<em>MP</em>, <em>181</em>(2), 405–434. (<a
href="https://doi.org/10.1007/s10107-018-1341-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Understanding the structure of financial markets deals with suitably determining the functional relation between financial variables. In this respect, important variables are the trading activity, defined here as the number of trades N, the traded volume V, the asset price P, the squared volatility $$\sigma ^2$$ , the bid-ask spread S and the cost of trading C. Different reasonings result in simple proportionality relations (“scaling laws”) between these variables. A basic proportionality is established between the trading activity and the squared volatility, i.e., $$N \sim \sigma ^2$$ . More sophisticated relations are the so called 3/2-law $$N^{3/2} \sim \sigma P V /C$$ and the intriguing scaling $$N \sim (\sigma P/S)^2$$ . We prove that these “scaling laws” are the only possible relations for considered sets of variables by means of a well-known argument from physics: dimensional analysis. Moreover, we provide empirical evidence based on data from the NASDAQ stock exchange showing that the sophisticated relations hold with a certain degree of universality. Finally, we discuss the time scaling of the volatility $$\sigma $$ , which turns out to be more subtle than one might naively expect.},
  archive      = {J_MP},
  author       = {Pohl, Mathias and Ristig, Alexander and Schachermayer, Walter and Tangpi, Ludovic},
  doi          = {10.1007/s10107-018-1341-x},
  journal      = {Mathematical Programming},
  number       = {2},
  pages        = {405-434},
  shortjournal = {Math. Program.},
  title        = {Theoretical and empirical analysis of trading activity},
  volume       = {181},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Martingale characterizations of risk-averse stochastic
optimization problems. <em>MP</em>, <em>181</em>(2), 377–403. (<a
href="https://doi.org/10.1007/s10107-019-01391-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper addresses risk awareness of stochastic optimization problems. Nested risk measures appear naturally in this context, as they allow beneficial reformulations for algorithmic treatments. The reformulations presented extend usual dynamic equations by involving risk awareness in the problem formulation. Nested risk measures are built on risk measures, which originate by conditioning on the history of a stochastic process. We derive martingale properties of these risk measures and use them to prove continuity. It is demonstrated that stochastic optimization problems, which incorporate risk awareness via nesting risk measures, are continuous with respect to the natural distance governing these optimization problems, the nested distance.},
  archive      = {J_MP},
  author       = {Pichler, Alois and Schlotter, Ruben},
  doi          = {10.1007/s10107-019-01391-2},
  journal      = {Mathematical Programming},
  number       = {2},
  pages        = {377-403},
  shortjournal = {Math. Program.},
  title        = {Martingale characterizations of risk-averse stochastic optimization problems},
  volume       = {181},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Randomness and permutations in coordinate descent methods.
<em>MP</em>, <em>181</em>(2), 349–376. (<a
href="https://doi.org/10.1007/s10107-019-01438-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider coordinate descent (CD) methods with exact line search on convex quadratic problems. Our main focus is to study the performance of the CD method that use random permutations in each epoch and compare it to the performance of the CD methods that use deterministic orders and random sampling with replacement. We focus on a class of convex quadratic problems with a diagonally dominant Hessian matrix, for which we show that using random permutations instead of random with-replacement sampling improves the performance of the CD method in the worst-case. Furthermore, we prove that as the Hessian matrix becomes more diagonally dominant, the performance improvement attained by using random permutations increases. We also show that for this problem class, using any fixed deterministic order yields a superior performance than using random permutations. We present detailed theoretical analyses with respect to three different convergence criteria that are used in the literature and support our theoretical results with numerical experiments.},
  archive      = {J_MP},
  author       = {Gürbüzbalaban, Mert and Ozdaglar, Asuman and Vanli, Nuri Denizcan and Wright, Stephen J.},
  doi          = {10.1007/s10107-019-01438-4},
  journal      = {Mathematical Programming},
  number       = {2},
  pages        = {349-376},
  shortjournal = {Math. Program.},
  title        = {Randomness and permutations in coordinate descent methods},
  volume       = {181},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Quantile-based risk sharing with heterogeneous beliefs.
<em>MP</em>, <em>181</em>(2), 319–347. (<a
href="https://doi.org/10.1007/s10107-018-1313-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study risk sharing problems with quantile-based risk measures and heterogeneous beliefs, motivated by the use of internal models in finance and insurance. Explicit forms of Pareto-optimal allocations and competitive equilibria are obtained by solving various optimization problems. For Expected Shortfall (ES) agents, Pareto-optimal allocations are shown to be equivalent to equilibrium allocations, and the equilibrium pricing measure is unique. For Value-at-Risk (VaR) agents or mixed VaR and ES agents, a competitive equilibrium does not exist. Our results generalize existing ones on risk sharing problems with risk measures and belief homogeneity, and draw an interesting connection to early work on optimization properties of ES and VaR.},
  archive      = {J_MP},
  author       = {Embrechts, Paul and Liu, Haiyan and Mao, Tiantian and Wang, Ruodu},
  doi          = {10.1007/s10107-018-1313-1},
  journal      = {Mathematical Programming},
  number       = {2},
  pages        = {319-347},
  shortjournal = {Math. Program.},
  title        = {Quantile-based risk sharing with heterogeneous beliefs},
  volume       = {181},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Risk forms: Representation, disintegration, and application
to partially observable two-stage systems. <em>MP</em>, <em>181</em>(2),
297–317. (<a href="https://doi.org/10.1007/s10107-019-01376-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce the concept of a risk form, which is a real functional of two arguments: a measurable function on a Polish space and a measure on that space. We generalize the duality theory and the Kusuoka representation to this setting. For a risk form acting on a product of Polish spaces, we define marginal and conditional forms and we prove a disintegration formula, which represents a risk form as a composition of its marginal and conditional forms. We apply the proposed approach to two-stage stochastic programming problems with partial information and decision-dependent observation distribution.},
  archive      = {J_MP},
  author       = {Dentcheva, Darinka and Ruszczyński, Andrzej},
  doi          = {10.1007/s10107-019-01376-1},
  journal      = {Mathematical Programming},
  number       = {2},
  pages        = {297-317},
  shortjournal = {Math. Program.},
  title        = {Risk forms: Representation, disintegration, and application to partially observable two-stage systems},
  volume       = {181},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Distributionally robust optimization with polynomial
densities: Theory, models and algorithms. <em>MP</em>, <em>181</em>(2),
265–296. (<a href="https://doi.org/10.1007/s10107-019-01429-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In distributionally robust optimization the probability distribution of the uncertain problem parameters is itself uncertain, and a fictitious adversary, e.g., nature, chooses the worst distribution from within a known ambiguity set. A common shortcoming of most existing distributionally robust optimization models is that their ambiguity sets contain pathological discrete distributions that give nature too much freedom to inflict damage. We thus introduce a new class of ambiguity sets that contain only distributions with sum-of-squares (SOS) polynomial density functions of known degrees. We show that these ambiguity sets are highly expressive as they conveniently accommodate distributional information about higher-order moments, conditional probabilities, conditional moments or marginal distributions. Exploiting the theoretical properties of a measure-based hierarchy for polynomial optimization due to Lasserre (SIAM J Optim 21(3):864–885, 2011), we prove that certain worst-case expectation constraints are polynomial-time solvable under these new ambiguity sets. We also show how SOS densities can be used to approximately solve the general problem of moments. We showcase the applicability of the proposed approach in the context of a stylized portfolio optimization problem and a risk aggregation problem of an insurance company.},
  archive      = {J_MP},
  author       = {de Klerk, Etienne and Kuhn, Daniel and Postek, Krzysztof},
  doi          = {10.1007/s10107-019-01429-5},
  journal      = {Mathematical Programming},
  number       = {2},
  pages        = {265-296},
  shortjournal = {Math. Program.},
  title        = {Distributionally robust optimization with polynomial densities: Theory, models and algorithms},
  volume       = {181},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). The subdifferential of measurable composite max integrands
and smoothing approximation. <em>MP</em>, <em>181</em>(2), 229–264. (<a
href="https://doi.org/10.1007/s10107-019-01441-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The subdifferential calculus for the expectation of nonsmooth random integrands involves many fundamental and challenging problems in stochastic optimization. It is known that for Clarke regular integrands, the Clarke subdifferential of the expectation equals the expectation of their Clarke subdifferential. In particular, this holds for convex integrands. However, little is known about the calculation of Clarke subgradients for the expectation of non-regular integrands. The focus of this contribution is to approximate Clarke subgradients for the expectation of random integrands by smoothing methods applied to the integrand. A framework for how to proceed along this path is developed and then applied to a class of measurable composite max integrands. This class contains non-regular integrands from stochastic complementarity problems as well as stochastic optimization problems arising in statistical learning.},
  archive      = {J_MP},
  author       = {Burke, James V. and Chen, Xiaojun and Sun, Hailin},
  doi          = {10.1007/s10107-019-01441-9},
  journal      = {Mathematical Programming},
  number       = {2},
  pages        = {229-264},
  shortjournal = {Math. Program.},
  title        = {The subdifferential of measurable composite max integrands and smoothing approximation},
  volume       = {181},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Special issue: On the interface between optimization and
probability. <em>MP</em>, <em>181</em>(2), 225–228. (<a
href="https://doi.org/10.1007/s10107-020-01521-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_MP},
  author       = {Kovacevic, Raimund and Wets, Roger J-B and Wozabal, David},
  doi          = {10.1007/s10107-020-01521-1},
  journal      = {Mathematical Programming},
  number       = {2},
  pages        = {225-228},
  shortjournal = {Math. Program.},
  title        = {Special issue: On the interface between optimization and probability},
  volume       = {181},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). An alternative to EM for gaussian mixture models: Batch and
stochastic riemannian optimization. <em>MP</em>, <em>181</em>(1),
187–223. (<a href="https://doi.org/10.1007/s10107-019-01381-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider maximum likelihood estimation for Gaussian Mixture Models (Gmm s). This task is almost invariably solved (in theory and practice) via the Expectation Maximization (EM) algorithm. EM owes its success to various factors, of which is its ability to fulfill positive definiteness constraints in closed form is of key importance. We propose an alternative to EM grounded in the Riemannian geometry of positive definite matrices, using which we cast Gmm parameter estimation as a Riemannian optimization problem. Surprisingly, such an out-of-the-box Riemannian formulation completely fails and proves much inferior to EM. This motivates us to take a closer look at the problem geometry, and derive a better formulation that is much more amenable to Riemannian optimization. We then develop Riemannian batch and stochastic gradient algorithms that outperform EM, often substantially. We provide a non-asymptotic convergence analysis for our stochastic method, which is also the first (to our knowledge) such global analysis for Riemannian stochastic gradient. Numerous empirical results are included to demonstrate the effectiveness of our methods.},
  archive      = {J_MP},
  author       = {Hosseini, Reshad and Sra, Suvrit},
  doi          = {10.1007/s10107-019-01381-4},
  journal      = {Mathematical Programming},
  number       = {1},
  pages        = {187-223},
  shortjournal = {Math. Program.},
  title        = {An alternative to EM for gaussian mixture models: Batch and stochastic riemannian optimization},
  volume       = {181},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Stationarity conditions and constraint qualifications for
mathematical programs with switching constraints. <em>MP</em>,
<em>181</em>(1), 149–186. (<a
href="https://doi.org/10.1007/s10107-019-01380-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In optimal control, switching structures demanding at most one control to be active at any time instance appear frequently. Discretizing such problems, a so-called mathematical program with switching constraints is obtained. Although these problems are related to other types of disjunctive programs like optimization problems with complementarity or vanishing constraints, their inherent structure makes a separate consideration necessary. Since standard constraint qualifications are likely to fail at the feasible points of switching-constrained optimization problems, stationarity notions which are weaker than the associated Karush–Kuhn–Tucker conditions need to be investigated in order to find applicable necessary optimality conditions. Furthermore, appropriately tailored constraint qualifications need to be formulated. In this paper, we introduce suitable notions of weak, Mordukhovich-, and strong stationarity for mathematical programs with switching constraints and present some associated constraint qualifications. Our findings are exploited to state necessary optimality conditions for (discretized) optimal control problems with switching constraints. Furthermore, we apply our results to optimization problems with either-or-constraints. First, a novel reformulation of such problems using switching constraints is presented. Second, the derived surrogate problem is exploited to obtain necessary optimality conditions for the original program.},
  archive      = {J_MP},
  author       = {Mehlitz, Patrick},
  doi          = {10.1007/s10107-019-01380-5},
  journal      = {Mathematical Programming},
  number       = {1},
  pages        = {149-186},
  shortjournal = {Math. Program.},
  title        = {Stationarity conditions and constraint qualifications for mathematical programs with switching constraints},
  volume       = {181},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Lattice closures of polyhedra. <em>MP</em>, <em>181</em>(1),
119–147. (<a href="https://doi.org/10.1007/s10107-019-01379-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Given $$P\subset {\mathbb {R}}^n$$, a mixed-integer set $$P^I=P\cap ({\mathbb {Z}}^{t}\times {\mathbb {R}}^{n-t}$$), and a k-tuple of n-dimensional integral vectors $$(\pi _1, \ldots , \pi _k)$$ where the last $$n-t$$ entries of each vector is zero, we consider the relaxation of $$P^I$$ obtained by taking the convex hull of points x in P for which $$ \pi _1^Tx,\ldots ,\pi ^T_kx$$ are integral. We then define the k-dimensional lattice closure of $$P^I$$ to be the intersection of all such relaxations obtained from k-tuples of n-dimensional vectors. When P is a rational polyhedron, we show that given any collection of such k-tuples, there is a finite subcollection that gives the same closure; more generally, we show that any k-tuple is dominated by another k-tuple coming from the finite subcollection. The k-dimensional lattice closure contains the convex hull of $$P^I$$ and is equal to the split closure when $$k=1$$. Therefore, a result of Cook et al. (Math Program 47:155–174, 1990) implies that when P is a rational polyhedron, the k-dimensional lattice closure is a polyhedron for $$k=1$$ and our finiteness result extends this to all $$k\ge 2$$. We also construct a polyhedral mixed-integer set with n integer variables and one continuous variable such that for any $$k &lt; n$$, finitely many iterations of the k-dimensional lattice closure do not give the convex hull of the set. Our result implies that t-branch split cuts cannot give the convex hull of the set, nor can valid inequalities from unbounded, full-dimensional, convex lattice-free sets.},
  archive      = {J_MP},
  author       = {Dash, Sanjeeb and Günlük, Oktay and Morán R., Diego A.},
  doi          = {10.1007/s10107-019-01379-y},
  journal      = {Mathematical Programming},
  number       = {1},
  pages        = {119-147},
  shortjournal = {Math. Program.},
  title        = {Lattice closures of polyhedra},
  volume       = {181},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Packing of arborescences with matroid constraints via
matroid intersection. <em>MP</em>, <em>181</em>(1), 85–117. (<a
href="https://doi.org/10.1007/s10107-019-01377-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Edmonds’ arborescence packing theorem characterizes directed graphs that have arc-disjoint spanning arborescences in terms of connectivity. Later he also observed a characterization in terms of matroid intersection. Since these fundamental results, intensive research has been done for understanding and extending these results. In this paper we shall extend the second characterization to the setting of reachability-based packing of arborescences. The reachability-based packing problem was introduced by Cs. Király as a common generalization of two different extensions of the spanning arborescence packing problem, one is due to Kamiyama, Katoh, and Takizawa, and the other is due to Durand de Gevigney, Nguyen, and Szigeti. Our new characterization of the arc sets of reachability-based packing in terms of matroid intersection gives an efficient algorithm for the minimum weight reachability-based packing problem, and it also enables us to unify further arborescence packing theorems and Edmonds’ matroid intersection theorem. For the proof, we also show how a new class of matroids can be defined by extending an earlier construction of matroids from intersecting submodular functions to bi-set functions based on an idea of Frank.},
  archive      = {J_MP},
  author       = {Király, Csaba and Szigeti, Zoltán and Tanigawa, Shin-ichi},
  doi          = {10.1007/s10107-019-01377-0},
  journal      = {Mathematical Programming},
  number       = {1},
  pages        = {85-117},
  shortjournal = {Math. Program.},
  title        = {Packing of arborescences with matroid constraints via matroid intersection},
  volume       = {181},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Dimension reduction for semidefinite programs via jordan
algebras. <em>MP</em>, <em>181</em>(1), 51–84. (<a
href="https://doi.org/10.1007/s10107-019-01372-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a new method for simplifying semidefinite programs (SDP) inspired by symmetry reduction. Specifically, we show if an orthogonal projection map satisfies certain invariance conditions, restricting to its range yields an equivalent primal–dual pair over a lower-dimensional symmetric cone—namely, the cone-of-squares of a Jordan subalgebra of symmetric matrices. We present a simple algorithm for minimizing the rank of this projection and hence the dimension of this subalgebra. We also show that minimizing rank optimizes the direct-sum decomposition of the algebra into simple ideals, yielding an optimal “block-diagonalization” of the SDP. Finally, we give combinatorial versions of our algorithm that execute at reduced computational cost and illustrate effectiveness of an implementation on examples. Through the theory of Jordan algebras, the proposed method easily extends to linear and second-order-cone programming and, more generally, symmetric cone optimization.},
  archive      = {J_MP},
  author       = {Permenter, Frank and Parrilo, Pablo A.},
  doi          = {10.1007/s10107-019-01372-5},
  journal      = {Mathematical Programming},
  number       = {1},
  pages        = {51-84},
  shortjournal = {Math. Program.},
  title        = {Dimension reduction for semidefinite programs via jordan algebras},
  volume       = {181},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). MIDAS: A mixed integer dynamic approximation scheme.
<em>MP</em>, <em>181</em>(1), 19–50. (<a
href="https://doi.org/10.1007/s10107-019-01368-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mixed integer dynamic approximation scheme (MIDAS) is a new sampling-based algorithm for solving finite-horizon stochastic dynamic programs with monotonic Bellman functions. MIDAS approximates these value functions using step functions, leading to stage problems that are mixed integer programs. We provide a general description of MIDAS, and prove its almost-sure convergence to a $$2T\varepsilon $$-optimal policy for problems with T stages when the Bellman functions are known to be monotonic, and the sampling process satisfies standard assumptions.},
  archive      = {J_MP},
  author       = {Philpott, A. B. and Wahid, F. and Bonnans, J. F.},
  doi          = {10.1007/s10107-019-01368-1},
  journal      = {Mathematical Programming},
  number       = {1},
  pages        = {19-50},
  shortjournal = {Math. Program.},
  title        = {MIDAS: A mixed integer dynamic approximation scheme},
  volume       = {181},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Exact semidefinite formulations for a class of (random and
non-random) nonconvex quadratic programs. <em>MP</em>, <em>181</em>(1),
1–17. (<a href="https://doi.org/10.1007/s10107-019-01367-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study a class of quadratically constrained quadratic programs (QCQPs), called diagonal QCQPs, which contain no off-diagonal terms $$x_j x_k$$ for $$j \ne k$$ , and we provide a sufficient condition on the problem data guaranteeing that the basic Shor semidefinite relaxation is exact. Our condition complements and refines those already present in the literature and can be checked in polynomial time. We then extend our analysis from diagonal QCQPs to general QCQPs, i.e., ones with no particular structure. By reformulating a general QCQP into diagonal form, we establish new, polynomial-time-checkable sufficient conditions for the semidefinite relaxations of general QCQPs to be exact. Finally, these ideas are extended to show that a class of random general QCQPs has exact semidefinite relaxations with high probability as long as the number of constraints grows no faster than a fixed polynomial in the number of variables. To the best of our knowledge, this is the first result establishing the exactness of the semidefinite relaxation for random general QCQPs.},
  archive      = {J_MP},
  author       = {Burer, Samuel and Ye, Yinyu},
  doi          = {10.1007/s10107-019-01367-2},
  journal      = {Mathematical Programming},
  number       = {1},
  pages        = {1-17},
  shortjournal = {Math. Program.},
  title        = {Exact semidefinite formulations for a class of (random and non-random) nonconvex quadratic programs},
  volume       = {181},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Chordal decomposition in operator-splitting methods for
sparse semidefinite programs. <em>MP</em>, <em>180</em>(1), 489–532. (<a
href="https://doi.org/10.1007/s10107-019-01366-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We employ chordal decomposition to reformulate a large and sparse semidefinite program (SDP), either in primal or dual standard form, into an equivalent SDP with smaller positive semidefinite (PSD) constraints. In contrast to previous approaches, the decomposed SDP is suitable for the application of first-order operator-splitting methods, enabling the development of efficient and scalable algorithms. In particular, we apply the alternating direction method of multipliers (ADMM) to solve decomposed primal- and dual-standard-form SDPs. Each iteration of such ADMM algorithms requires a projection onto an affine subspace, and a set of projections onto small PSD cones that can be computed in parallel. We also formulate the homogeneous self-dual embedding (HSDE) of a primal-dual pair of decomposed SDPs, and extend a recent ADMM-based algorithm to exploit the structure of our HSDE. The resulting HSDE algorithm has the same leading-order computational cost as those for the primal or dual problems only, with the advantage of being able to identify infeasible problems and produce an infeasibility certificate. All algorithms are implemented in the open-source MATLAB solver CDCS. Numerical experiments on a range of large-scale SDPs demonstrate the computational advantages of the proposed methods compared to common state-of-the-art solvers.},
  archive      = {J_MP},
  author       = {Zheng, Yang and Fantuzzi, Giovanni and Papachristodoulou, Antonis and Goulart, Paul and Wynn, Andrew},
  doi          = {10.1007/s10107-019-01366-3},
  journal      = {Mathematical Programming},
  number       = {1},
  pages        = {489-532},
  shortjournal = {Math. Program.},
  title        = {Chordal decomposition in operator-splitting methods for sparse semidefinite programs},
  volume       = {180},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A newton-CG algorithm with complexity guarantees for smooth
unconstrained optimization. <em>MP</em>, <em>180</em>(1), 451–488. (<a
href="https://doi.org/10.1007/s10107-019-01362-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider minimization of a smooth nonconvex objective function using an iterative algorithm based on Newton’s method and the linear conjugate gradient algorithm, with explicit detection and use of negative curvature directions for the Hessian of the objective function. The algorithm tracks Newton-conjugate gradient procedures developed in the 1980s closely, but includes enhancements that allow worst-case complexity results to be proved for convergence to points that satisfy approximate first-order and second-order optimality conditions. The complexity results match the best known results in the literature for second-order methods.},
  archive      = {J_MP},
  author       = {Royer, Clément W. and O’Neill, Michael and Wright, Stephen J.},
  doi          = {10.1007/s10107-019-01362-7},
  journal      = {Mathematical Programming},
  number       = {1},
  pages        = {451-488},
  shortjournal = {Math. Program.},
  title        = {A newton-CG algorithm with complexity guarantees for smooth unconstrained optimization},
  volume       = {180},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Faster subgradient methods for functions with hölderian
growth. <em>MP</em>, <em>180</em>(1), 417–450. (<a
href="https://doi.org/10.1007/s10107-018-01361-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The purpose of this manuscript is to derive new convergence results for several subgradient methods applied to minimizing nonsmooth convex functions with Hölderian growth. The growth condition is satisfied in many applications and includes functions with quadratic growth and weakly sharp minima as special cases. To this end there are three main contributions. First, for a constant and sufficiently small stepsize, we show that the subgradient method achieves linear convergence up to a certain region including the optimal set, with error of the order of the stepsize. Second, if appropriate problem parameters are known, we derive a decaying stepsize which obtains a much faster convergence rate than is suggested by the classical $$O(1/\sqrt{k})$$ result for the subgradient method. Thirdly we develop a novel “descending stairs” stepsize which obtains this faster convergence rate and also obtains linear convergence for the special case of weakly sharp functions. We also develop an adaptive variant of the “descending stairs” stepsize which achieves the same convergence rate without requiring an error bound constant which is difficult to estimate in practice.},
  archive      = {J_MP},
  author       = {Johnstone, Patrick R. and Moulin, Pierre},
  doi          = {10.1007/s10107-018-01361-0},
  journal      = {Mathematical Programming},
  number       = {1},
  pages        = {417-450},
  shortjournal = {Math. Program.},
  title        = {Faster subgradient methods for functions with hölderian growth},
  volume       = {180},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). New analysis of linear convergence of gradient-type methods
via unifying error bound conditions. <em>MP</em>, <em>180</em>(1),
371–416. (<a href="https://doi.org/10.1007/s10107-018-01360-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper reveals that a common and central role, played in many error bound (EB) conditions and a variety of gradient-type methods, is a residual measure operator. On one hand, by linking this operator with other optimality measures, we define a group of abstract EB conditions, and then analyze the interplay between them; on the other hand, by using this operator as an ascent direction, we propose an abstract gradient-type method, and then derive EB conditions that are necessary and sufficient for its linear convergence. The former provides a unified framework that not only allows us to find new connections between many existing EB conditions, but also paves a way to construct new ones. The latter allows us to claim the weakest conditions guaranteeing linear convergence for a number of fundamental algorithms, including the gradient method, the proximal point algorithm, and the forward–backward splitting algorithm. In addition, we show linear convergence for the proximal alternating linearized minimization algorithm under a group of equivalent EB conditions, which are strictly weaker than the traditional strongly convex condition. Moreover, by defining a new EB condition, we show Q-linear convergence of Nesterov’s accelerated forward–backward algorithm without strong convexity. Finally, we verify EB conditions for a class of dual objective functions.},
  archive      = {J_MP},
  author       = {Zhang, Hui},
  doi          = {10.1007/s10107-018-01360-1},
  journal      = {Mathematical Programming},
  number       = {1},
  pages        = {371-416},
  shortjournal = {Math. Program.},
  title        = {New analysis of linear convergence of gradient-type methods via unifying error bound conditions},
  volume       = {180},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Varying confidence levels for CVaR risk measures and minimax
limits. <em>MP</em>, <em>180</em>(1), 327–370. (<a
href="https://doi.org/10.1007/s10107-018-01359-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Conditional value at risk (CVaR) has been widely studied as a risk measure. In this paper we add to this work by focusing on the choice of confidence level and its impact on optimization problems with CVaR appearing in the objective and also the constraints. We start by considering a problem in which CVaR is minimized and investigate the way in which it approximates the minimax robust optimization problem as the confidence level is driven to one. We make use of a consistent tail condition which ensures that the CVaR of a random function will converge uniformly to its supremum as the confidence level increases, and establish an error bound for the CVaR optimal solution under second order growth conditions. The results are extended to a minimization problem with a constraint on the CVaR value which in the limit as the confidence level approaches one coincides with a problem having semi-infinite constraints. We study the sample average approximation scheme for the CVaR constraints and establish an exponential rate of convergence for the sample averaged optimal solution. We propose a procedure to explore the possibility of varying the confidence level to a lower value which can give an advantage when there is a need to find good solutions to CVaR-constrained problems out of sample. Our numerical results demonstrate that using the optimal solution to an adjusted problem with lower confidence level can lead to better overall performance.},
  archive      = {J_MP},
  author       = {Anderson, Edward and Xu, Huifu and Zhang, Dali},
  doi          = {10.1007/s10107-018-01359-8},
  journal      = {Mathematical Programming},
  number       = {1},
  pages        = {327-370},
  shortjournal = {Math. Program.},
  title        = {Varying confidence levels for CVaR risk measures and minimax limits},
  volume       = {180},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Balas formulation for the union of polytopes is optimal.
<em>MP</em>, <em>180</em>(1), 311–326. (<a
href="https://doi.org/10.1007/s10107-018-01358-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A celebrated theorem of Balas gives a linear mixed-integer formulation for the union of two nonempty polytopes whose relaxation gives the convex hull of this union. The number of inequalities in Balas formulation is linear in the number of inequalities that describe the two polytopes and the number of variables is doubled. In this paper we show that this is best possible: in every dimension there exist two nonempty polytopes such that if a formulation for the convex hull of their union has a number of inequalities that is polynomial in the number of inequalities that describe the two polytopes, then the number of additional variables is at least linear in the dimension of the polytopes. We then show that this result essentially carries over if one wants to approximate the convex hull of the union of two polytopes and also in the more restrictive setting of lift-and-project.},
  archive      = {J_MP},
  author       = {Conforti, Michele and Di Summa, Marco and Faenza, Yuri},
  doi          = {10.1007/s10107-018-01358-9},
  journal      = {Mathematical Programming},
  number       = {1},
  pages        = {311-326},
  shortjournal = {Math. Program.},
  title        = {Balas formulation for the union of polytopes is optimal},
  volume       = {180},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Using regularization and second order information in outer
approximation for convex MINLP. <em>MP</em>, <em>180</em>(1), 285–310.
(<a href="https://doi.org/10.1007/s10107-018-1356-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we present two new methods for solving convex mixed-integer nonlinear programming problems based on the outer approximation method. The first method is inspired by the level method and uses a regularization technique to reduce the step size when choosing new integer combinations. The second method combines ideas from both the level method and the sequential quadratic programming technique and uses a second order approximation of the Lagrangean when choosing the new integer combinations. The main idea behind the methods is to choose the integer combination more carefully at each iteration, in order to obtain the optimal solution in fewer iterations compared to the original outer approximation method. We prove rigorously that both methods will find and verify the optimal solution in a finite number of iterations. Furthermore, we present a numerical comparison of the methods based on 109 test problems to illustrate their advantages.},
  archive      = {J_MP},
  author       = {Kronqvist, Jan and Bernal, David E. and Grossmann, Ignacio E.},
  doi          = {10.1007/s10107-018-1356-3},
  journal      = {Mathematical Programming},
  number       = {1},
  pages        = {285-310},
  shortjournal = {Math. Program.},
  title        = {Using regularization and second order information in outer approximation for convex MINLP},
  volume       = {180},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Communication-efficient algorithms for decentralized and
stochastic optimization. <em>MP</em>, <em>180</em>(1), 237–284. (<a
href="https://doi.org/10.1007/s10107-018-1355-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a new class of decentralized first-order methods for nonsmooth and stochastic optimization problems defined over multiagent networks. Considering that communication is a major bottleneck in decentralized optimization, our main goal in this paper is to develop algorithmic frameworks which can significantly reduce the number of inter-node communications. Our major contribution is to present a new class of decentralized primal–dual type algorithms, namely the decentralized communication sliding (DCS) methods, which can skip the inter-node communications while agents solve the primal subproblems iteratively through linearizations of their local objective functions. By employing DCS, agents can find an $$\epsilon $$-solution both in terms of functional optimality gap and feasibility residual in $${{\mathcal {O}}}(1/\epsilon )$$ (resp., $${{\mathcal {O}}}(1/\sqrt{\epsilon })$$) communication rounds for general convex functions (resp., strongly convex functions), while maintaining the $${{\mathcal {O}}}(1/\epsilon ^2)$$ (resp., $$\mathcal{O}(1/\epsilon )$$) bound on the total number of intra-node subgradient evaluations. We also present a stochastic counterpart for these algorithms, denoted by SDCS, for solving stochastic optimization problems whose objective function cannot be evaluated exactly. In comparison with existing results for decentralized nonsmooth and stochastic optimization, we can reduce the total number of inter-node communication rounds by orders of magnitude while still maintaining the optimal complexity bounds on intra-node stochastic subgradient evaluations. The bounds on the (stochastic) subgradient evaluations are actually comparable to those required for centralized nonsmooth and stochastic optimization under certain conditions on the target accuracy.},
  archive      = {J_MP},
  author       = {Lan, Guanghui and Lee, Soomin and Zhou, Yi},
  doi          = {10.1007/s10107-018-1355-4},
  journal      = {Mathematical Programming},
  number       = {1},
  pages        = {237-284},
  shortjournal = {Math. Program.},
  title        = {Communication-efficient algorithms for decentralized and stochastic optimization},
  volume       = {180},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Optimality conditions and global convergence for nonlinear
semidefinite programming. <em>MP</em>, <em>180</em>(1), 203–235. (<a
href="https://doi.org/10.1007/s10107-018-1354-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sequential optimality conditions have played a major role in unifying and extending global convergence results for several classes of algorithms for general nonlinear optimization. In this paper, we extend theses concepts for nonlinear semidefinite programming. We define two sequential optimality conditions for nonlinear semidefinite programming. The first is a natural extension of the so-called Approximate-Karush–Kuhn–Tucker (AKKT), well known in nonlinear optimization. The second one, called Trace-AKKT, is more natural in the context of semidefinite programming as the computation of eigenvalues is avoided. We propose an augmented Lagrangian algorithm that generates these types of sequences and new constraint qualifications are proposed, weaker than previously considered ones, which are sufficient for the global convergence of the algorithm to a stationary point.},
  archive      = {J_MP},
  author       = {Andreani, Roberto and Haeser, Gabriel and Viana, Daiana S.},
  doi          = {10.1007/s10107-018-1354-5},
  journal      = {Mathematical Programming},
  number       = {1},
  pages        = {203-235},
  shortjournal = {Math. Program.},
  title        = {Optimality conditions and global convergence for nonlinear semidefinite programming},
  volume       = {180},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A simplex-type algorithm for continuous linear programs with
constant coefficients. <em>MP</em>, <em>180</em>(1), 157–201. (<a
href="https://doi.org/10.1007/s10107-018-1353-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider continuous linear programs over a continuous finite time horizon T, with a constant coefficient matrix, linear right hand side functions and linear cost coefficient functions. Specifically, we search for optimal solutions in the space of measures or of functions of bounded variation. These models generalize the separated continuous linear programming models and their various duals, as formulated in the past by Anderson, by Pullan, and by Weiss. In previous papers we formulated a symmetric dual and have shown strong duality. We also have presented a detailed description of optimal solutions and have defined a combinatorial analogue to basic solutions of standard LP. In this paper we present an algorithm which solves this class of problems in a finite bounded number of steps, using an analogue of the simplex method, in the space of measures.},
  archive      = {J_MP},
  author       = {Shindin, Evgeny and Weiss, Gideon},
  doi          = {10.1007/s10107-018-1353-6},
  journal      = {Mathematical Programming},
  number       = {1},
  pages        = {157-201},
  shortjournal = {Math. Program.},
  title        = {A simplex-type algorithm for continuous linear programs with constant coefficients},
  volume       = {180},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Convergence rate of inertial forward–backward algorithm
beyond nesterov’s rule. <em>MP</em>, <em>180</em>(1), 137–156. (<a
href="https://doi.org/10.1007/s10107-018-1350-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper we study the convergence of an Inertial Forward–Backward algorithm, with a particular choice of an over-relaxation term. In particular we show that for a sequence of over-relaxation parameters, that do not satisfy Nesterov’s rule, one can still expect some relatively fast convergence properties for the objective function. In addition we complement this work by studying the convergence of the algorithm in the case where the proximal operator is inexactly computed with the presence of some errors and we give sufficient conditions over these errors in order to obtain some convergence properties for the objective function.},
  archive      = {J_MP},
  author       = {Apidopoulos, Vassilis and Aujol, Jean-François and Dossal, Charles},
  doi          = {10.1007/s10107-018-1350-9},
  journal      = {Mathematical Programming},
  number       = {1},
  pages        = {137-156},
  shortjournal = {Math. Program.},
  title        = {Convergence rate of inertial Forward–Backward algorithm beyond nesterov’s rule},
  volume       = {180},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Network strength games: The core and the nucleolus.
<em>MP</em>, <em>180</em>(1), 117–136. (<a
href="https://doi.org/10.1007/s10107-018-1348-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The maximum number of edge-disjoint spanning trees in a network has been used as a measure of the strength of a network. It gives the number of disjoint ways that the network can be fully connected. This suggests a game theoretic analysis that shows the relative importance of the different links to form a strong network. We introduce the Network strength game as a cooperative game defined on a graph $$G=(V,E)$$. The player set is the edge-set E and the value of a coalition $$S \subseteq E$$ is the maximum number of disjoint spanning trees included in S. We study the core of this game, and we give a polynomial combinatorial algorithm to compute the nucleolus when the core is non-empty.},
  archive      = {J_MP},
  author       = {Baïou, Mourad and Barahona, Francisco},
  doi          = {10.1007/s10107-018-1348-3},
  journal      = {Mathematical Programming},
  number       = {1},
  pages        = {117-136},
  shortjournal = {Math. Program.},
  title        = {Network strength games: The core and the nucleolus},
  volume       = {180},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Second-order variational analysis in second-order cone
programming. <em>MP</em>, <em>180</em>(1), 75–116. (<a
href="https://doi.org/10.1007/s10107-018-1345-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The paper conducts a second-order variational analysis for an important class of nonpolyhedral conic programs generated by the so-called second-order/Lorentz/ice-cream cone $${\mathcal {Q}}$$. From one hand, we prove that the indicator function of $${\mathcal {Q}}$$ is always twice epi-differentiable and apply this result to characterizing the uniqueness of Lagrange multipliers together with an error bound estimate in the general second-order cone programming setting involving twice differentiable data. On the other hand, we precisely calculate the graphical derivative of the normal cone mapping to $${\mathcal {Q}}$$ under the metric subregularity constraint qualification and then give an application of the latter result to a complete characterization of isolated calmness for perturbed variational systems associated with second-order cone programs. The obtained results seem to be the first in the literature in these directions for nonpolyhedral problems without imposing any nondegeneracy assumptions.},
  archive      = {J_MP},
  author       = {Hang, Nguyen T. V. and Mordukhovich, Boris S. and Sarabi, M. Ebrahim},
  doi          = {10.1007/s10107-018-1345-6},
  journal      = {Mathematical Programming},
  number       = {1},
  pages        = {75-116},
  shortjournal = {Math. Program.},
  title        = {Second-order variational analysis in second-order cone programming},
  volume       = {180},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Piecewise affine parameterized value-function based bilevel
non-cooperative games. <em>MP</em>, <em>180</em>(1), 33–73. (<a
href="https://doi.org/10.1007/s10107-018-1344-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Generalizing certain network interdiction games communicated to us by Andrew Liu and his collaborators, this paper studies a bilevel, non-cooperative game wherein the objective function of each player’s optimization problem contains a value function of a second-level linear program parameterized by the first-level variables in a non-convex manner. In the applied network interdiction games, this parameterization is through a piecewise linear function that upper bounds the second-level decision variable. In order to give a unified treatment to the overall two-level game where the second-level problems may be minimization or maximization, we formulate it as a one-level game of a particular kind. Namely, each player’s objective function is the sum of a first-level objective function ± a value function of a second-level maximization problem whose objective function involves a difference-of-convex (dc), specifically piecewise affine, parameterization by the first-level variables. This non-convex parameterization is a major difference from the family of games with min-max objectives discussed in Facchinei et al. (Comput Optim Appl 59(1):85–112, 2014) wherein the convexity of the overall games is preserved. In contrast, the piecewise affine (dc) parameterization of the second-level objective functions to be maximized renders the players’ combined first-level objective functions non-convex and non-differentiable. We investigate the existence of a first-order stationary solution of such a game, which we call a quasi-Nash equilibrium, and study the computation of such a solution in the linear-quadratic case by Lemke’s method using a linear complementarity formulation.},
  archive      = {J_MP},
  author       = {Hao, Tianyu and Pang, Jong-Shi},
  doi          = {10.1007/s10107-018-1344-7},
  journal      = {Mathematical Programming},
  number       = {1},
  pages        = {33-73},
  shortjournal = {Math. Program.},
  title        = {Piecewise affine parameterized value-function based bilevel non-cooperative games},
  volume       = {180},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Necessary conditions for linear convergence of iterated
expansive, set-valued mappings. <em>MP</em>, <em>180</em>(1), 1–31. (<a
href="https://doi.org/10.1007/s10107-018-1343-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present necessary conditions for monotonicity of fixed point iterations of mappings that may violate the usual nonexpansive property. Notions of linear-type monotonicity of fixed point sequences—weaker than Fejér monotonicity—are shown to imply metric subregularity. This, together with the almost averaging property recently introduced by Luke et al. (Math Oper Res, 2018. https://doi.org/10.1287/moor.2017.0898), guarantees linear convergence of the sequence to a fixed point. We specialize these results to the alternating projections iteration where the metric subregularity property takes on a distinct geometric characterization of sets at points of intersection called subtransversality. Subtransversality is shown to be necessary for linear convergence of alternating projections for consistent feasibility.},
  archive      = {J_MP},
  author       = {Luke, D. Russell and Teboulle, Marc and Thao, Nguyen H.},
  doi          = {10.1007/s10107-018-1343-8},
  journal      = {Mathematical Programming},
  number       = {1},
  pages        = {1-31},
  shortjournal = {Math. Program.},
  title        = {Necessary conditions for linear convergence of iterated expansive, set-valued mappings},
  volume       = {180},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Correction to: Approximations and generalized newton
methods. <em>MP</em>, <em>179</em>(1), 469–471. (<a
href="https://doi.org/10.1007/s10107-019-01436-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In Subsection 3.2 of the paper [5]},
  archive      = {J_MP},
  author       = {Klatte, Diethard and Kummer, Bernd},
  doi          = {10.1007/s10107-019-01436-6},
  journal      = {Mathematical Programming},
  number       = {1},
  pages        = {469-471},
  shortjournal = {Math. Program.},
  title        = {Correction to: Approximations and generalized newton methods},
  volume       = {179},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Distances between optimal solutions of mixed-integer
programs. <em>MP</em>, <em>179</em>(1), 455–468. (<a
href="https://doi.org/10.1007/s10107-018-1323-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A classic result of Cook et al. (Math. Program. 34:251–264, 1986) bounds the distances between optimal solutions of mixed-integer linear programs and optimal solutions of the corresponding linear relaxations. Their bound is given in terms of the number of variables and a parameter $$ \varDelta $$, which quantifies sub-determinants of the underlying linear inequalities. We show that this distance can be bounded in terms of $$ \varDelta $$ and the number of integer variables rather than the total number of variables. To this end, we make use of a result by Olson (J. Number Theory 1:8–10, 1969) in additive combinatorics and demonstrate how it implies feasibility of certain mixed-integer linear programs. We conjecture that our bound can be improved to a function that only depends on $$ \varDelta $$, in general.},
  archive      = {J_MP},
  author       = {Paat, Joseph and Weismantel, Robert and Weltge, Stefan},
  doi          = {10.1007/s10107-018-1323-z},
  journal      = {Mathematical Programming},
  number       = {1},
  pages        = {455-468},
  shortjournal = {Math. Program.},
  title        = {Distances between optimal solutions of mixed-integer programs},
  volume       = {179},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). An extreme function which is nonnegative and discontinuous
everywhere. <em>MP</em>, <em>179</em>(1), 447–453. (<a
href="https://doi.org/10.1007/s10107-018-1322-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider Gomory and Johnson’s infinite group model with a single row. Valid inequalities for this model are expressed by valid functions and it has been recently shown that any valid function is dominated by some nonnegative valid function, modulo the affine hull of the model. Within the set of nonnegative valid functions, extreme functions are the ones that cannot be expressed as convex combinations of two distinct valid functions. In this paper we construct an extreme function $$\pi :\mathbb {R}\rightarrow [0,1]$$ whose graph is dense in $$\mathbb {R}\times [0,1]$$. Therefore $$\pi $$ is discontinuous everywhere.},
  archive      = {J_MP},
  author       = {Basu, Amitabh and Conforti, Michele and Di Summa, Marco},
  doi          = {10.1007/s10107-018-1322-0},
  journal      = {Mathematical Programming},
  number       = {1},
  pages        = {447-453},
  shortjournal = {Math. Program.},
  title        = {An extreme function which is nonnegative and discontinuous everywhere},
  volume       = {179},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). On the efficient computation of a generalized jacobian of
the projector over the birkhoff polytope. <em>MP</em>, <em>179</em>(1),
419–446. (<a href="https://doi.org/10.1007/s10107-018-1342-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We derive an explicit formula, as well as an efficient procedure, for constructing a generalized Jacobian for the projector of a given square matrix onto the Birkhoff polytope, i.e., the set of doubly stochastic matrices. To guarantee the high efficiency of our procedure, a semismooth Newton method for solving the dual of the projection problem is proposed and efficiently implemented. Extensive numerical experiments are presented to demonstrate the merits and effectiveness of our method by comparing its performance against other powerful solvers such as the commercial software Gurobi and the academic code PPROJ (Hager and Zhang in SIAM J Optim 26:1773–1798, 2016). In particular, our algorithm is able to solve the projection problem with over one billion variables and nonnegative constraints to a very high accuracy in less than 15 min on a modest desktop computer. More importantly, based on our efficient computation of the projections and their generalized Jacobians, we can design a highly efficient augmented Lagrangian method (ALM) for solving a class of convex quadratic programming (QP) problems constrained by the Birkhoff polytope. The resulted ALM is demonstrated to be much more efficient than Gurobi in solving a collection of QP problems arising from the relaxation of quadratic assignment problems.},
  archive      = {J_MP},
  author       = {Li, Xudong and Sun, Defeng and Toh, Kim-Chuan},
  doi          = {10.1007/s10107-018-1342-9},
  journal      = {Mathematical Programming},
  number       = {1},
  pages        = {419-446},
  shortjournal = {Math. Program.},
  title        = {On the efficient computation of a generalized jacobian of the projector over the birkhoff polytope},
  volume       = {179},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Faster algorithms for extensive-form game solving via
improved smoothing functions. <em>MP</em>, <em>179</em>(1), 385–417. (<a
href="https://doi.org/10.1007/s10107-018-1336-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sparse iterative methods, in particular first-order methods, are known to be among the most effective in solving large-scale two-player zero-sum extensive-form games. The convergence rates of these methods depend heavily on the properties of the distance-generating function that they are based on. We investigate both the theoretical and practical performance improvement of first-order methods (FOMs) for solving extensive-form games through better design of the dilated entropy function—a class of distance-generating functions related to the domains associated with the extensive-form games. By introducing a new weighting scheme for the dilated entropy function, we develop the first distance-generating function for the strategy spaces of sequential games that has only a logarithmic dependence on the branching factor of the player. This result improves the overall convergence rate of several FOMs working with dilated entropy function by a factor of $$\Omega (b^dd)$$, where b is the branching factor of the player, and d is the depth of the game tree. Thus far, counterfactual regret minimization methods have been faster in practice, and more popular, than FOMs despite their theoretically inferior convergence rates. Using our new weighting scheme and a practical parameter tuning procedure we show that, for the first time, the excessive gap technique, a classical FOM, can be made faster than the counterfactual regret minimization algorithm in practice for large games, and that the aggressive stepsize scheme of CFR+ is the only reason that the algorithm is faster in practice.},
  archive      = {J_MP},
  author       = {Kroer, Christian and Waugh, Kevin and Kılınç-Karzan, Fatma and Sandholm, Tuomas},
  doi          = {10.1007/s10107-018-1336-7},
  journal      = {Mathematical Programming},
  number       = {1},
  pages        = {385-417},
  shortjournal = {Math. Program.},
  title        = {Faster algorithms for extensive-form game solving via improved smoothing functions},
  volume       = {179},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Computing walrasian equilibria: Fast algorithms and
structural properties. <em>MP</em>, <em>179</em>(1), 343–384. (<a
href="https://doi.org/10.1007/s10107-018-1334-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present the first polynomial time algorithm for computing Walrasian equilibrium in an economy with indivisible goods and general buyer valuations having only access to an aggregate demand oracle, i.e., an oracle that given prices on all goods, returns the aggregated demand over the entire population of buyers. For the important special case of gross substitute valuations, our algorithm queries the aggregate demand oracle $${\widetilde{O}}(n)$$ times and takes $${\widetilde{O}}(n^3)$$ time, where n is the number of goods and the $${\widetilde{O}}(\cdot )$$ notation denotes an asymptotic bound up to polylogarithmic factors. Both algorithms are randomized. At the heart of our solution is a method for exactly minimizing certain convex functions which cannot be evaluated but for which the subgradients can be computed. We also give the fastest known algorithm for computing Walrasian equilibrium for gross substitute valuations in the value oracle model. Our algorithm has running time $${\widetilde{O}}((mn + n^3) T_V)$$ where $$T_V$$ is the cost of querying the value oracle. A key technical ingredient is to regularize a convex programming formulation of the problem in a way that subgradients are cheap to compute. En route, we give necessary and sufficient conditions for the existence of robust Walrasian prices, i.e., prices for which each agent has a unique demanded bundle and the demanded bundles clear the market. When such prices exist, the market can be perfectly coordinated by solely using prices.},
  archive      = {J_MP},
  author       = {Paes Leme, Renato and Wong, Sam Chiu-wai},
  doi          = {10.1007/s10107-018-1334-9},
  journal      = {Mathematical Programming},
  number       = {1},
  pages        = {343-384},
  shortjournal = {Math. Program.},
  title        = {Computing walrasian equilibria: Fast algorithms and structural properties},
  volume       = {179},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). When do birds of a feather flock together? K-means,
proximity, and conic programming. <em>MP</em>, <em>179</em>(1), 295–341.
(<a href="https://doi.org/10.1007/s10107-018-1333-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Given a set of data, one central goal is to group them into clusters based on some notion of similarity between the individual objects. One of the most popular and widely-used approaches is k-means despite the computational hardness to find its global minimum. We study and compare the properties of different convex relaxations by relating them to corresponding proximity conditions, an idea originally introduced by Kumar and Kannan. Using conic duality theory, we present an improved proximity condition under which the Peng–Wei relaxation of k-means recovers the underlying clusters exactly. Our proximity condition improves upon Kumar and Kannan and is comparable to that of Awashti and Sheffet, where proximity conditions are established for projective k-means. In addition, we provide a necessary proximity condition for the exactness of the Peng–Wei relaxation. For the special case of equal cluster sizes, we establish a different and completely localized proximity condition under which the Amini–Levina relaxation yields exact clustering, thereby having addressed an open problem by Awasthi and Sheffet in the balanced case. Our framework is not only deterministic and model-free but also comes with a clear geometric meaning which allows for further analysis and generalization. Moreover, it can be conveniently applied to analyzing various data generative models such as the stochastic ball models and Gaussian mixture models. With this method, we improve the current minimum separation bound for the stochastic ball models and achieve the state-of-the-art results of learning Gaussian mixture models.},
  archive      = {J_MP},
  author       = {Li, Xiaodong and Li, Yang and Ling, Shuyang and Strohmer, Thomas and Wei, Ke},
  doi          = {10.1007/s10107-018-1333-x},
  journal      = {Mathematical Programming},
  number       = {1},
  pages        = {295-341},
  shortjournal = {Math. Program.},
  title        = {When do birds of a feather flock together? k-means, proximity, and conic programming},
  volume       = {179},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Piecewise smooth extreme functions are piecewise linear.
<em>MP</em>, <em>179</em>(1), 265–293. (<a
href="https://doi.org/10.1007/s10107-018-1330-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The infinite relaxations in integer programming were introduced by Gomory and Johnson to provide a general framework for the theory of cutting planes: the so-called valid functions, and in particular the minimal and extreme functions, can be seen as automatic rules for the generation of cuts. However, while many extreme functions are piecewise linear and therefore easy to describe, the set of extreme functions turns out to have a very complicated mathematical structure, as several extreme functions are known that exhibit a somewhat pathological behavior. In this paper we show that if some smoothness assumption is imposed on an extreme function $$\pi $$, then $$\pi $$ is necessarily piecewise linear. More precisely, we show that if a continuous extreme function for the Gomory–Johnson one-dimensional infinite group relaxation is a piecewise $${\mathcal {C}}^2$$ function, then it is a piecewise linear function.},
  archive      = {J_MP},
  author       = {Di Summa, Marco},
  doi          = {10.1007/s10107-018-1330-0},
  journal      = {Mathematical Programming},
  number       = {1},
  pages        = {265-293},
  shortjournal = {Math. Program.},
  title        = {Piecewise smooth extreme functions are piecewise linear},
  volume       = {179},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). An efficient hessian based algorithm for solving large-scale
sparse group lasso problems. <em>MP</em>, <em>179</em>(1), 223–263. (<a
href="https://doi.org/10.1007/s10107-018-1329-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The sparse group Lasso is a widely used statistical model which encourages the sparsity both on a group and within the group level. In this paper, we develop an efficient augmented Lagrangian method for large-scale non-overlapping sparse group Lasso problems with each subproblem being solved by a superlinearly convergent inexact semismooth Newton method. Theoretically, we prove that, if the penalty parameter is chosen sufficiently large, the augmented Lagrangian method converges globally at an arbitrarily fast linear rate for the primal iterative sequence, the dual infeasibility, and the duality gap of the primal and dual objective functions. Computationally, we derive explicitly the generalized Jacobian of the proximal mapping associated with the sparse group Lasso regularizer and exploit fully the underlying second order sparsity through the semismooth Newton method. The efficiency and robustness of our proposed algorithm are demonstrated by numerical experiments on both the synthetic and real data sets.},
  archive      = {J_MP},
  author       = {Zhang, Yangjing and Zhang, Ning and Sun, Defeng and Toh, Kim-Chuan},
  doi          = {10.1007/s10107-018-1329-6},
  journal      = {Mathematical Programming},
  number       = {1},
  pages        = {223-263},
  shortjournal = {Math. Program.},
  title        = {An efficient hessian based algorithm for solving large-scale sparse group lasso problems},
  volume       = {179},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A decoupled first/second-order steps technique for nonconvex
nonlinear unconstrained optimization with improved complexity bounds.
<em>MP</em>, <em>179</em>(1), 195–222. (<a
href="https://doi.org/10.1007/s10107-018-1328-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In order to be provably convergent towards a second-order stationary point, optimization methods applied to nonconvex problems must necessarily exploit both first and second-order information. However, as revealed by recent complexity analyses of some of these methods, the overall effort to reach second-order points is significantly larger when compared to the one of approaching first-order ones. On the other hand, there are other algorithmic schemes, initially designed with first-order convergence in mind, that do not appear to maintain the same first-order performance when modified to take second-order information into account. In this paper, we propose a technique that separately computes first and second-order steps, and that globally converges to second-order stationary points: it consists in better connecting the steps to be taken and the stationarity criteria, potentially guaranteeing larger steps and decreases in the objective. Our approach is shown to lead to an improvement of the corresponding complexity bound with respect to the first-order optimality tolerance, while having a positive impact on the practical behavior. Although the applicability of our ideas is wider, we focus the presentation on trust-region methods with and without derivatives, and motivate in both cases the interest of our strategy.},
  archive      = {J_MP},
  author       = {Gratton, S. and Royer, C. W. and Vicente, L. N.},
  doi          = {10.1007/s10107-018-1328-7},
  journal      = {Mathematical Programming},
  number       = {1},
  pages        = {195-222},
  shortjournal = {Math. Program.},
  title        = {A decoupled first/second-order steps technique for nonconvex nonlinear unconstrained optimization with improved complexity bounds},
  volume       = {179},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Derivative-free robust optimization by outer approximations.
<em>MP</em>, <em>179</em>(1), 157–193. (<a
href="https://doi.org/10.1007/s10107-018-1326-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We develop an algorithm for minimax problems that arise in robust optimization in the absence of objective function derivatives. The algorithm utilizes an extension of methods for inexact outer approximation in sampling a potentially infinite-cardinality uncertainty set. Clarke stationarity of the algorithm output is established alongside desirable features of the model-based trust-region subproblems encountered. We demonstrate the practical benefits of the algorithm on a new class of test problems.},
  archive      = {J_MP},
  author       = {Menickelly, Matt and Wild, Stefan M.},
  doi          = {10.1007/s10107-018-1326-9},
  journal      = {Mathematical Programming},
  number       = {1},
  pages        = {157-193},
  shortjournal = {Math. Program.},
  title        = {Derivative-free robust optimization by outer approximations},
  volume       = {179},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Fully dynamic bin packing revisited. <em>MP</em>,
<em>179</em>(1), 109–155. (<a
href="https://doi.org/10.1007/s10107-018-1325-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider the fully dynamic bin packing problem, where items arrive and depart in an online fashion and repacking of previously packed items is allowed. The goal is, of course, to minimize both the number of bins used as well as the amount of repacking. A recently introduced way of measuring the repacking costs at each timestep is the migration factor, defined as the total size of repacked items divided by the size of an arriving or departing item. Concerning the trade-off between number of bins and migration factor, if we wish to achieve an asymptotic competitive ratio of $$1 + \epsilon $$ for the number of bins, a relatively simple argument proves a lower bound of $$\Omega ({1}/{\epsilon })$$ for the migration factor. We establish a nearly matching upper bound of $$O({1}/{\epsilon }^4 \log {1}/{\epsilon })$$ using a new dynamic rounding technique and new ideas to handle small items in a dynamic setting such that no amortization is needed. The running time of our algorithm is polynomial in the number of items nand in $${1}/{\epsilon }$$. The previous best trade-off was for an asymptotic competitive ratio of $${5}/{4}$$ for the bins (rather than $$1+\epsilon $$) and needed an amortized number of $$O(\log n)$$ repackings (while in our scheme the number of repackings is independent of n and non-amortized).},
  archive      = {J_MP},
  author       = {Berndt, Sebastian and Jansen, Klaus and Klein, Kim-Manuel},
  doi          = {10.1007/s10107-018-1325-x},
  journal      = {Mathematical Programming},
  number       = {1},
  pages        = {109-155},
  shortjournal = {Math. Program.},
  title        = {Fully dynamic bin packing revisited},
  volume       = {179},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). On the equivalence of the primal-dual hybrid gradient method
and douglas–rachford splitting. <em>MP</em>, <em>179</em>(1), 85–108.
(<a href="https://doi.org/10.1007/s10107-018-1321-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The primal-dual hybrid gradient (PDHG) algorithm proposed by Esser, Zhang, and Chan, and by Pock, Cremers, Bischof, and Chambolle is known to include as a special case the Douglas–Rachford splitting algorithm for minimizing the sum of two convex functions. We show that, conversely, the PDHG algorithm can be viewed as a special case of the Douglas–Rachford splitting algorithm.},
  archive      = {J_MP},
  author       = {O’Connor, Daniel and Vandenberghe, Lieven},
  doi          = {10.1007/s10107-018-1321-1},
  journal      = {Mathematical Programming},
  number       = {1},
  pages        = {85-108},
  shortjournal = {Math. Program.},
  title        = {On the equivalence of the primal-dual hybrid gradient method and Douglas–Rachford splitting},
  volume       = {179},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Regularized nonlinear acceleration. <em>MP</em>,
<em>179</em>(1), 47–83. (<a
href="https://doi.org/10.1007/s10107-018-1319-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We describe a convergence acceleration technique for unconstrained optimization problems. Our scheme computes estimates of the optimum from a nonlinear average of the iterates produced by any optimization method. The weights in this average are computed via a simple linear system, whose solution can be updated online. This acceleration scheme runs in parallel to the base algorithm, providing improved estimates of the solution on the fly, while the original optimization method is running. Numerical experiments are detailed on classical classification problems.},
  archive      = {J_MP},
  author       = {Scieur, Damien and d’Aspremont, Alexandre and Bach, Francis},
  doi          = {10.1007/s10107-018-1319-8},
  journal      = {Mathematical Programming},
  number       = {1},
  pages        = {47-83},
  shortjournal = {Math. Program.},
  title        = {Regularized nonlinear acceleration},
  volume       = {179},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). On the rational polytopes with chvátal rank 1. <em>MP</em>,
<em>179</em>(1), 21–46. (<a
href="https://doi.org/10.1007/s10107-018-1317-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study the following problem: given a rational polytope with Chvátal rank 1, does it contain an integer point? Boyd and Pulleyblank observed that this problem is in the complexity class NP $$\cap $$ co-NP, an indication that it is probably not NP-complete. It is open whether there is a polynomial time algorithm to solve the problem, and we give several special classes where this is indeed the case. We show that any compact convex set whose Chvátal closure is empty has an integer width of at most n, and we give an example showing that this bound is tight within an additive constant of 1. This determines the time complexity of a Lenstra-type algorithm. However, the promise that a polytope has Chvátal rank 1 seems hard to verify. We prove that deciding emptiness of the Chvátal closure of a rational polytope given by its linear description is NP-complete, even when the polytope is contained in the unit hypercube or is a rational simplex and it does not contain any integer point.},
  archive      = {J_MP},
  author       = {Cornuéjols, Gérard and Lee, Dabeen and Li, Yanjun},
  doi          = {10.1007/s10107-018-1317-x},
  journal      = {Mathematical Programming},
  number       = {1},
  pages        = {21-46},
  shortjournal = {Math. Program.},
  title        = {On the rational polytopes with chvátal rank 1},
  volume       = {179},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A strategy of global convergence for the affine scaling
algorithm for convex semidefinite programming. <em>MP</em>,
<em>179</em>(1), 1–19. (<a
href="https://doi.org/10.1007/s10107-018-1314-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The affine scaling algorithm is one of the earliest interior point methods developed for linear programming. This algorithm is simple and elegant in terms of its geometric interpretation, but it is notoriously difficult to prove its convergence. It often requires additional restrictive conditions such as nondegeneracy, specific initial solutions, and/or small step length to guarantee its global convergence. This situation is made worse when it comes to applying the affine scaling idea to the solution of semidefinite optimization problems or more general convex optimization problems. In (Math Program 83(1–3):393–406, 1998), Muramatsu presented an example of linear semidefinite programming, for which the affine scaling algorithm with either short or long step converges to a non-optimal point. This paper aims at developing a strategy that guarantees the global convergence for the affine scaling algorithm in the context of linearly constrained convex semidefinite optimization in a least restrictive manner. We propose a new rule of step size, which is similar to the Armijo rule, and prove that such an affine scaling algorithm is globally convergent in the sense that each accumulation point of the sequence generated by the algorithm is an optimal solution as long as the optimal solution set is nonempty and bounded. The algorithm is least restrictive in the sense that it allows the problem to be degenerate and it may start from any interior feasible point.},
  archive      = {J_MP},
  author       = {Qian, Xun and Liao, Li-Zhi and Sun, Jie},
  doi          = {10.1007/s10107-018-1314-0},
  journal      = {Mathematical Programming},
  number       = {1},
  pages        = {1-19},
  shortjournal = {Math. Program.},
  title        = {A strategy of global convergence for the affine scaling algorithm for convex semidefinite programming},
  volume       = {179},
  year         = {2020},
}
</textarea>
</details></li>
</ul>

</body>
</html>
