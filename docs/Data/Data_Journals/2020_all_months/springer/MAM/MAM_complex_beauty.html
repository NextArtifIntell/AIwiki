<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>MAM_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="mam---31">MAM - 31</h2>
<ul>
<li><details>
<summary>
(2020). GPT-3: Its nature, scope, limits, and consequences.
<em>MAM</em>, <em>30</em>(4), 681–694. (<a
href="https://doi.org/10.1007/s11023-020-09548-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this commentary, we discuss the nature of reversible and irreversible questions, that is, questions that may enable one to identify the nature of the source of their answers. We then introduce GPT-3, a third-generation, autoregressive language model that uses deep learning to produce human-like texts, and use the previous distinction to analyse it. We expand the analysis to present three tests based on mathematical, semantic (that is, the Turing Test), and ethical questions and show that GPT-3 is not designed to pass any of them. This is a reminder that GPT-3 does not do what it is not supposed to do, and that any interpretation of GPT-3 as the beginning of the emergence of a general form of artificial intelligence is merely uninformed science fiction. We conclude by outlining some of the significant consequences of the industrialisation of automatic and cheap production of good, semantic artefacts.},
  archive      = {J_MAM},
  author       = {Floridi, Luciano and Chiriatti, Massimo},
  doi          = {10.1007/s11023-020-09548-1},
  journal      = {Minds and Machines},
  month        = {12},
  number       = {4},
  pages        = {681-694},
  shortjournal = {Minds Mach.},
  title        = {GPT-3: Its nature, scope, limits, and consequences},
  volume       = {30},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). In defence of a reciprocal turing test. <em>MAM</em>,
<em>30</em>(4), 659–680. (<a
href="https://doi.org/10.1007/s11023-020-09552-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The traditional Turing test appeals to an interrogator&#39;s judgement to determine whether or not their interlocutor is an intelligent agent. This paper argues that this kind of asymmetric experimental set-up is inappropriate for tracking a property such as intelligence because intelligence is grounded in part by symmetric relations of recognition between agents. In place, it proposes a reciprocal test which takes into account the judgments of both interrogators and competitors to determine if an agent is intelligent. This form of social interaction better tracks both the evolution of natural intelligence and how the concept of intelligence is actually used within our society. This new test is defended against the criticisms that a proof of intelligence requires a demonstration of self-consciousness and that semantic externalism entails that a non-embodied Turing test is inadequate.},
  archive      = {J_MAM},
  author       = {Mallory, Fintan},
  doi          = {10.1007/s11023-020-09552-5},
  journal      = {Minds and Machines},
  month        = {12},
  number       = {4},
  pages        = {659-680},
  shortjournal = {Minds Mach.},
  title        = {In defence of a reciprocal turing test},
  volume       = {30},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Imitation game: Threshold or watershed? <em>MAM</em>,
<em>30</em>(4), 637–657. (<a
href="https://doi.org/10.1007/s11023-020-09544-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Showing remarkable insight into the relationship between language and thought, Alan Turing in 1950 proposed the Imitation Game as a proxy for the question “Can machines think?” and its meaning and practicality have been debated hotly ever since. The Imitation Game has come under criticism within the Computer Science and Artificial Intelligence communities with leading scientists proposing alternatives, revisions, or even that the Game be abandoned entirely. Yet Turing’s imagined conversational fragments between human and machine are rich with complex instances of inference of implied information, reasoning from generalizations, and meta-reasoning, challenges AI practitioners have wrestled with since at least 1980 and continue to study. We argue that the very fact the Imitation Game is so difficult may be the very reason it shouldn’t be changed or abandoned. The semi-decidability of the game at this point hints at the possibility of a hard limit to the powers of technology.},
  archive      = {J_MAM},
  author       = {Neufeld, Eric and Finnestad, Sonje},
  doi          = {10.1007/s11023-020-09544-5},
  journal      = {Minds and Machines},
  month        = {12},
  number       = {4},
  pages        = {637-657},
  shortjournal = {Minds Mach.},
  title        = {Imitation game: Threshold or watershed?},
  volume       = {30},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Preferential engagement and what can we learn from online
chess? <em>MAM</em>, <em>30</em>(4), 617–636. (<a
href="https://doi.org/10.1007/s11023-020-09550-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {An online game of chess against a human opponent appears to be indistinguishable from a game against a machine: both happen on the screen. Yet, people prefer to play chess against other people despite the fact that machines surpass people in skill. When the philosophers of 1970’s and 1980’s argued that computers will never surpass us in chess, perhaps their intuitions were rather saying “Computers will never be favored as opponents”? In this paper we analyse through the introduced concepts of psychological affordances and psychological interplay, what are the mechanisms that make a human-human (HH) interaction more meaningful than a human-computer (HC) interaction. We claim that an HH chess game consists of two intertwined, but independent simultaneous games—only one of which is retained in the HC game. To help with the analysis we introduce the thought experiment of a Preferential Engagement Test (PET) which is inspired by, but non-equivalent to, the Standard Turing Test. We also explore how the PET can illuminate, and be illuminated by, various philosophies of mind reading: Theory Theory, Simulation Theory and Mind Minding. We propose that our analysis along with the concept of PET could illuminate in a new way the conditions and challenges a machine (or its designers) must face before it can replace humans in a given occupation.},
  archive      = {J_MAM},
  author       = {Kulikov, Vadim},
  doi          = {10.1007/s11023-020-09550-7},
  journal      = {Minds and Machines},
  month        = {12},
  number       = {4},
  pages        = {617-636},
  shortjournal = {Minds Mach.},
  title        = {Preferential engagement and what can we learn from online chess?},
  volume       = {30},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Building thinking machines by solving animal cognition
tasks. <em>MAM</em>, <em>30</em>(4), 589–615. (<a
href="https://doi.org/10.1007/s11023-020-09535-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In ‘Computing Machinery and Intelligence’, Turing, sceptical of the question ‘Can machines think?’, quickly replaces it with an experimentally verifiable test: the imitation game. I suggest that for such a move to be successful the test needs to be relevant, expansive, solvable by exemplars, unpredictable, and lead to actionable research. The Imitation Game is only partially successful in this regard and its reliance on language, whilst insightful for partially solving the problem, has put AI progress on the wrong foot, prescribing a top-down approach for building thinking machines. I argue that to fix shortcomings with modern AI systems a nonverbal operationalisation is required. This is provided by the recent Animal-AI Testbed, which translates animal cognition tests for AI and provides a bottom-up research pathway for building thinking machines that create predictive models of their environment from sensory input.},
  archive      = {J_MAM},
  author       = {Crosby, Matthew},
  doi          = {10.1007/s11023-020-09535-6},
  journal      = {Minds and Machines},
  month        = {12},
  number       = {4},
  pages        = {589-615},
  shortjournal = {Minds Mach.},
  title        = {Building thinking machines by solving animal cognition tasks},
  volume       = {30},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). The questioning turing test. <em>MAM</em>, <em>30</em>(4),
563–587. (<a href="https://doi.org/10.1007/s11023-020-09551-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Turing Test (TT) is best regarded as a model to test for intelligence, where an entity’s intelligence is inferred from its ability to be attributed with ‘human-likeness’ during a text-based conversation. The problem with this model, however, is that it does not care if or how well an entity produces a meaningful conversation, as long as its interactions are humanlike enough. As a consequence, the TT attracts projects that concentrate on how best to fool the judges. In light of this, I propose a new version of the TT: the Questioning Turing Test (QTT). Here, the entity has to produce an enquiry rather than a conversation; and it is parametrised along two further dimensions in addition to ‘human-likeness’: ‘correctness’, evaluating if the entity accomplishes the enquiry; and ‘strategicness’, evaluating how well the entity accomplishes the enquiry, in terms of the number of questions asked (the fewer, the better).},
  archive      = {J_MAM},
  author       = {Damassino, Nicola},
  doi          = {10.1007/s11023-020-09551-6},
  journal      = {Minds and Machines},
  month        = {12},
  number       = {4},
  pages        = {563-587},
  shortjournal = {Minds Mach.},
  title        = {The questioning turing test},
  volume       = {30},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Twenty years beyond the turing test: Moving beyond the human
judges too. <em>MAM</em>, <em>30</em>(4), 533–562. (<a
href="https://doi.org/10.1007/s11023-020-09549-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the last 20 years the Turing test has been left further behind by new developments in artificial intelligence. At the same time, however, these developments have revived some key elements of the Turing test: imitation and adversarialness. On the one hand, many generative models, such as generative adversarial networks (GAN), build imitators under an adversarial setting that strongly resembles the Turing test (with the judge being a learnt discriminative model). The term “Turing learning” has been used for this kind of setting. On the other hand, AI benchmarks are suffering an adversarial situation too, with a ‘challenge-solve-and-replace’ evaluation dynamics whenever human performance is ‘imitated’. The particular AI community rushes to replace the old benchmark by a more challenging benchmark, one for which human performance would still be beyond AI. These two phenomena related to the Turing test are sufficiently distinctive, important and general for a detailed analysis. This is the main goal of this paper. After recognising the abyss that appears beyond superhuman performance, we build on Turing learning to identify two different evaluation schemas: Turing testing and adversarial testing. We revisit some of the key questions surrounding the Turing test, such as ‘understanding’, commonsense reasoning and extracting meaning from the world, and explore how the new testing paradigms should work to unmask the limitations of current and future AI. Finally, we discuss how behavioural similarity metrics could be used to create taxonomies for artificial and natural intelligence. Both testing schemas should complete a transition in which humans should give way to machines—not only as references to be imitated but also as judges—when pursuing and measuring machine intelligence.},
  archive      = {J_MAM},
  author       = {Hernández-Orallo, José},
  doi          = {10.1007/s11023-020-09549-0},
  journal      = {Minds and Machines},
  month        = {12},
  number       = {4},
  pages        = {533-562},
  shortjournal = {Minds Mach.},
  title        = {Twenty years beyond the turing test: Moving beyond the human judges too},
  volume       = {30},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Deceptive appearances: The turing test, response-dependence,
and intelligence as an emotional concept. <em>MAM</em>, <em>30</em>(4),
513–532. (<a href="https://doi.org/10.1007/s11023-020-09533-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Turing Test is routinely understood as a behaviourist test for machine intelligence. Diane Proudfoot (Rethinking Turing’s Test, Journal of Philosophy, 2013) has argued for an alternative interpretation. According to Proudfoot, Turing’s claim that intelligence is what he calls ‘an emotional concept’ indicates that he conceived of intelligence in response-dependence terms. As she puts it: ‘Turing’s criterion for “thinking” is…: x is intelligent (or thinks) if in the actual world, in an unrestricted computer-imitates-human game, x appears intelligent to an average interrogator’. The role of the famous test is thus to provide the conditions in which to examine the average interrogator’s responses. I shall argue that Proudfoot’s analysis falls short. The philosophical literature contains two main models of response-dependence, what I shall call the transparency model and the reference-fixing model. Proudfoot resists the thought that Turing might have endorsed one of these models to the exclusion of the other. But the details of her own analysis indicate that she is, in fact, committed to the claim that Turing’s account of intelligence is grounded in a transparency model, rather than a reference-fixing one. By contrast, I shall argue that while Turing did indeed conceive of intelligence in response-dependence terms, his account is grounded in a reference-fixing model, rather than a transparency one. This is fortunate (for Turing), because, as an account of intelligence, the transparency model is arguably problematic in a way that the reference-fixing model isn’t.},
  archive      = {J_MAM},
  author       = {Wheeler, Michael},
  doi          = {10.1007/s11023-020-09533-8},
  journal      = {Minds and Machines},
  month        = {12},
  number       = {4},
  pages        = {513-532},
  shortjournal = {Minds Mach.},
  title        = {Deceptive appearances: The turing test, response-dependence, and intelligence as an emotional concept},
  volume       = {30},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Rethinking turing’s test and the philosophical implications.
<em>MAM</em>, <em>30</em>(4), 487–512. (<a
href="https://doi.org/10.1007/s11023-020-09534-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the 70 years since Alan Turing’s ‘Computing Machinery and Intelligence’ appeared in Mind, there have been two widely-accepted interpretations of the Turing test: the canonical behaviourist interpretation and the rival inductive or epistemic interpretation. These readings are based on Turing’s Mind paper; few seem aware that Turing described two other versions of the imitation game. I have argued that both readings are inconsistent with Turing’s 1948 and 1952 statements about intelligence, and fail to explain the design of his game. I argue instead for a response-dependence interpretation (Proudfoot 2013). This interpretation has implications for Turing’s view of free will: I argue that Turing’s writings suggest a new form of free will compatibilism, which I call response-dependence compatibilism (Proudfoot 2017a). The philosophical implications of rethinking Turing’s test go yet further. It is assumed by numerous theorists that Turing anticipated the computational theory of mind. On the contrary, I argue, his remarks on intelligence and free will lead to a new objection to computationalism.},
  archive      = {J_MAM},
  author       = {Proudfoot, Diane},
  doi          = {10.1007/s11023-020-09534-7},
  journal      = {Minds and Machines},
  month        = {12},
  number       = {4},
  pages        = {487-512},
  shortjournal = {Minds Mach.},
  title        = {Rethinking turing’s test and the philosophical implications},
  volume       = {30},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). The genius of the ’original imitation game’ test.
<em>MAM</em>, <em>30</em>(4), 469–486. (<a
href="https://doi.org/10.1007/s11023-020-09543-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Twenty years ago in &quot;Turing&#39;s Two Tests for Intelligence&quot; I distinguished two distinct tests to be found in Alan Turing&#39;s 1950 paper &quot;Computing Machinery and Intelligence&quot;: one by then very well-known, the other neglected. I also explained the significance of the neglected test. This paper revisits some of the points in that paper and explains why they are even more relevant today. It also discusses the value of tests for machine intelligence based on games humans play, giving an analysis of some twentieth century TV game shows and how they relate to the tests for machine intelligence in Turing&#39;s paper and in some other tests for machine intelligence that have been proposed since. Their value in distinguishing between &#39;wise&#39; and simply ‘clever’ AI is discussed.},
  archive      = {J_MAM},
  author       = {Sterrett, S. G.},
  doi          = {10.1007/s11023-020-09543-6},
  journal      = {Minds and Machines},
  month        = {12},
  number       = {4},
  pages        = {469-486},
  shortjournal = {Minds Mach.},
  title        = {The genius of the &#39;Original imitation game&#39; test},
  volume       = {30},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Rethinking, reworking and revolutionising the turing test.
<em>MAM</em>, <em>30</em>(4), 463–468. (<a
href="https://doi.org/10.1007/s11023-020-09553-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_MAM},
  author       = {Damassino, Nicola and Novelli, Nicholas},
  doi          = {10.1007/s11023-020-09553-4},
  journal      = {Minds and Machines},
  month        = {12},
  number       = {4},
  pages        = {463-468},
  shortjournal = {Minds Mach.},
  title        = {Rethinking, reworking and revolutionising the turing test},
  volume       = {30},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020a). Publisher correction to: The ethics of AI ethics: An
evaluation of guidelines. <em>MAM</em>, <em>30</em>(3), 457–461. (<a
href="https://doi.org/10.1007/s11023-020-09526-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the original publication of this article, the Table 1 has been published in a low resolution. Now a larger version of Table 1 is published in this correction. The publisher apologizes for the error made during production.},
  archive      = {J_MAM},
  author       = {Hagendorff, Thilo},
  doi          = {10.1007/s11023-020-09526-7},
  journal      = {Minds and Machines},
  month        = {9},
  number       = {3},
  pages        = {457-461},
  shortjournal = {Minds Mach.},
  title        = {Publisher correction to: the ethics of AI ethics: an evaluation of guidelines},
  volume       = {30},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). The governance of unmanned aircraft systems (UAS): Aviation
law, human rights, and the free movement of data in the EU.
<em>MAM</em>, <em>30</em>(3), 439–455. (<a
href="https://doi.org/10.1007/s11023-020-09541-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The paper deals with the governance of Unmanned Aircraft Systems (UAS) in European law. Three different kinds of balance have been struck between multiple regulatory systems, in accordance with the sector of the governance of UAS which is taken into account. The first model regards the field of civil aviation law and its European Union (EU)’s regulation: the model looks like a traditional mix of top-down regulation and soft law. The second model concerns the EU general data protection law, the GDPR, which has set up a co-regulatory framework summed up with the principle of accountability also, but not only, in the field of drones. The third model of governance has been adopted by the EU through methods of legal experimentation and coordination mechanisms for UAS. The overall aim of the paper is to elucidate the ways in which such three models interact, insisting on differences and similarities with other technologies (e.g. self-driving cars), and further legal systems (e.g. the US).},
  archive      = {J_MAM},
  author       = {Pagallo, Ugo and Bassi, Eleonora},
  doi          = {10.1007/s11023-020-09541-8},
  journal      = {Minds and Machines},
  month        = {9},
  number       = {3},
  pages        = {439-455},
  shortjournal = {Minds Mach.},
  title        = {The governance of unmanned aircraft systems (UAS): Aviation law, human rights, and the free movement of data in the EU},
  volume       = {30},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Artificial intelligence, values, and alignment.
<em>MAM</em>, <em>30</em>(3), 411–437. (<a
href="https://doi.org/10.1007/s11023-020-09539-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper looks at philosophical questions that arise in the context of AI alignment. It defends three propositions. First, normative and technical aspects of the AI alignment problem are interrelated, creating space for productive engagement between people working in both domains. Second, it is important to be clear about the goal of alignment. There are significant differences between AI that aligns with instructions, intentions, revealed preferences, ideal preferences, interests and values. A principle-based approach to AI alignment, which combines these elements in a systematic way, has considerable advantages in this context. Third, the central challenge for theorists is not to identify ‘true’ moral principles for AI; rather, it is to identify fair principles for alignment that receive reflective endorsement despite widespread variation in people’s moral beliefs. The final part of the paper explores three ways in which fair principles for AI alignment could potentially be identified.},
  archive      = {J_MAM},
  author       = {Gabriel, Iason},
  doi          = {10.1007/s11023-020-09539-2},
  journal      = {Minds and Machines},
  month        = {9},
  number       = {3},
  pages        = {411-437},
  shortjournal = {Minds Mach.},
  title        = {Artificial intelligence, values, and alignment},
  volume       = {30},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Embedding values in artificial intelligence (AI) systems.
<em>MAM</em>, <em>30</em>(3), 385–409. (<a
href="https://doi.org/10.1007/s11023-020-09537-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Organizations such as the EU High-Level Expert Group on AI and the IEEE have recently formulated ethical principles and (moral) values that should be adhered to in the design and deployment of artificial intelligence (AI). These include respect for autonomy, non-maleficence, fairness, transparency, explainability, and accountability. But how can we ensure and verify that an AI system actually respects these values? To help answer this question, I propose an account for determining when an AI system can be said to embody certain values. This account understands embodied values as the result of design activities intended to embed those values in such systems. AI systems are here understood as a special kind of sociotechnical system that, like traditional sociotechnical systems, are composed of technical artifacts, human agents, and institutions but—in addition—contain artificial agents and certain technical norms that regulate interactions between artificial agents and other elements of the system. The specific challenges and opportunities of embedding values in AI systems are discussed, and some lessons for better embedding values in AI systems are drawn.},
  archive      = {J_MAM},
  author       = {van de Poel, Ibo},
  doi          = {10.1007/s11023-020-09537-4},
  journal      = {Minds and Machines},
  month        = {9},
  number       = {3},
  pages        = {385-409},
  shortjournal = {Minds Mach.},
  title        = {Embedding values in artificial intelligence (AI) systems},
  volume       = {30},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). The translator’s extended mind. <em>MAM</em>,
<em>30</em>(3), 349–383. (<a
href="https://doi.org/10.1007/s11023-020-09536-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The rapid development of natural language processing in the last three decades has drastically changed the way professional translators do their work. Nowadays most of them use computer-assisted translation (CAT) or translation memory (TM) tools whose evolution has been overshadowed by the much more sensational development of machine translation (MT) systems, with which TM tools are sometimes confused. These two language technologies now interact in mutually enhancing ways, and their increasing role in human translation has become a subject of behavioral studies. Philosophers and linguists, however, have been slow in coming to grips with these important developments. The present paper seeks to fill in this lacuna. I focus on the semantic aspects of the highly distributed human–computer interaction in the CAT process which presents an interesting case of an extended cognitive system involving a human translator, a TM tool, an MT engine, and sometimes other human translators or editors. Considered as a whole, such a system is engaged in representing the linguistic meaning of the source document in the target language. But the roles played by its various components, natural as well as artificial, are far from trivial, and the division of linguistic labor between them throws new light on the familiar notions that were initially inspired by rather different phenomena in the philosophy of language, mind, and cognitive science.},
  archive      = {J_MAM},
  author       = {Balashov, Yuri},
  doi          = {10.1007/s11023-020-09536-5},
  journal      = {Minds and Machines},
  month        = {9},
  number       = {3},
  pages        = {349-383},
  shortjournal = {Minds Mach.},
  title        = {The translator’s extended mind},
  volume       = {30},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). The state space of artificial intelligence. <em>MAM</em>,
<em>30</em>(3), 325–347. (<a
href="https://doi.org/10.1007/s11023-020-09538-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The goal of the paper is to develop and propose a general model of the state space of AI. Given the breathtaking progress in AI research and technologies in recent years, such conceptual work is of substantial theoretical interest. The present AI hype is mainly driven by the triumph of deep learning neural networks. As the distinguishing feature of such networks is the ability to self-learn, self-learning is identified as one important dimension of the AI state space. Another dimension is recognized as generalization, the possibility to go over from specific to more general types of problems. A third dimension is semantic grounding. Our overall analysis connects to a number of known foundational issues in the philosophy of mind and cognition: the blockhead objection, the Turing test, the symbol grounding problem, the Chinese room argument, and use theories of meaning. It shall finally be argued that the dimension of grounding decomposes into three sub-dimensions. And the dimension of self-learning turns out as only one of a whole range of “self-x-capacities” (based on ideas of organic computing) that span the self-x-subspace of the full AI state space.},
  archive      = {J_MAM},
  author       = {Lyre, Holger},
  doi          = {10.1007/s11023-020-09538-3},
  journal      = {Minds and Machines},
  month        = {9},
  number       = {3},
  pages        = {325-347},
  shortjournal = {Minds Mach.},
  title        = {The state space of artificial intelligence},
  volume       = {30},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). What is a simulation model? <em>MAM</em>, <em>30</em>(3),
301–323. (<a href="https://doi.org/10.1007/s11023-020-09520-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many philosophical accounts of scientific models fail to distinguish between a simulation model and other forms of models. This failure is unfortunate because there are important differences pertaining to their methodology and epistemology that favor their philosophical understanding. The core claim presented here is that simulation models are rich and complex units of analysis in their own right, that they depart from known forms of scientific models in significant ways, and that a proper understanding of the type of model simulations are fundamental for their philosophical assessment. I argue that simulation models can be distinguished from other forms of models by the many algorithmic structures, representation relations, and new semantic connections involved in their architecture. In this article, I reconstruct a general architecture for a simulation model, one that faithfully captures the complexities involved in most scientific research with computer simulations. Furthermore, I submit that a new methodology capable of conforming such architecture into a fully functional, computationally tractable computer simulation must be in place. I discuss this methodology—what I call recasting—and argue for its philosophical novelty. If these efforts are heading towards the right interpretation of simulation models, then one can show that computer simulations shed new light on the philosophy of science. To illustrate the potential of my interpretation of simulation models, I briefly discuss simulation-based explanations as a novel approach to questions about scientific explanation.},
  archive      = {J_MAM},
  author       = {Durán, Juan M.},
  doi          = {10.1007/s11023-020-09520-z},
  journal      = {Minds and Machines},
  month        = {9},
  number       = {3},
  pages        = {301-323},
  shortjournal = {Minds Mach.},
  title        = {What is a simulation model?},
  volume       = {30},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). The abstraction/representation account of computation and
subjective experience. <em>MAM</em>, <em>30</em>(2), 259–299. (<a
href="https://doi.org/10.1007/s11023-020-09522-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {I examine the abstraction/representation theory of computation put forward by Horsman et al., connecting it to the broader notion of modeling, and in particular, model-based explanation, as considered by Rosen. I argue that the ‘representational entities’ it depends on cannot themselves be computational, and that, in particular, their representational capacities cannot be realized by computational means, and must remain explanatorily opaque to them. I then propose that representation might be realized by subjective experience (qualia), through being the bearer of the structure of abstract objects that are represented.},
  archive      = {J_MAM},
  author       = {Szangolies, Jochen},
  doi          = {10.1007/s11023-020-09522-x},
  journal      = {Minds and Machines},
  month        = {6},
  number       = {2},
  pages        = {259-299},
  shortjournal = {Minds Mach.},
  title        = {The Abstraction/Representation account of computation and subjective experience},
  volume       = {30},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Towards the ethical publication of country of origin
information (COI) in the asylum process. <em>MAM</em>, <em>30</em>(2),
247–257. (<a href="https://doi.org/10.1007/s11023-020-09523-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article addresses the question of how ‘Country of Origin Information’ (COI) reports—that is, research developed and used to support decision-making in the asylum process—can be published in an ethical manner. The article focuses on the risk that published COI reports could be misused and thereby harm the subjects of the reports and/or those involved in their development. It supports a situational approach to assessing data ethics when publishing COI reports, whereby COI service providers must weigh up the benefits and harms of publication based, inter alia, on the foreseeability and probability of harm due to potential misuse of the research, the public good nature of the research, and the need to balance the rights and duties of the various actors in the asylum process, including asylum seekers themselves. Although this article focuses on the specific question of ‘how to publish COI reports in an ethical manner’, it also intends to promote further research on data ethics in the asylum process, particularly in relation to refugees, where more foundational issues should be considered.},
  archive      = {J_MAM},
  author       = {Aggarwal, Nikita and Floridi, Luciano},
  doi          = {10.1007/s11023-020-09523-w},
  journal      = {Minds and Machines},
  month        = {6},
  number       = {2},
  pages        = {247-257},
  shortjournal = {Minds Mach.},
  title        = {Towards the ethical publication of country of origin information (COI) in the asylum process},
  volume       = {30},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Moral gridworlds: A theoretical proposal for modeling
artificial moral cognition. <em>MAM</em>, <em>30</em>(2), 219–246. (<a
href="https://doi.org/10.1007/s11023-020-09524-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {I describe a suite of reinforcement learning environments in which artificial agents learn to value and respond to moral content and contexts. I illustrate the core principles of the framework by characterizing one such environment, or “gridworld,” in which an agent learns to trade-off between monetary profit and fair dealing, as applied in a standard behavioral economic paradigm. I then highlight the core technical and philosophical advantages of the learning approach for modeling moral cognition, and for addressing the so-called value alignment problem in AI.},
  archive      = {J_MAM},
  author       = {Haas, Julia},
  doi          = {10.1007/s11023-020-09524-9},
  journal      = {Minds and Machines},
  month        = {6},
  number       = {2},
  pages        = {219-246},
  shortjournal = {Minds Mach.},
  title        = {Moral gridworlds: A theoretical proposal for modeling artificial moral cognition},
  volume       = {30},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A normative approach to artificial moral agency.
<em>MAM</em>, <em>30</em>(2), 195–218. (<a
href="https://doi.org/10.1007/s11023-020-09525-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes a methodological redirection of the philosophical debate on artificial moral agency (AMA) in view of increasingly pressing practical needs due to technological development. This “normative approach” suggests abandoning theoretical discussions about what conditions may hold for moral agency and to what extent these may be met by artificial entities such as AI systems and robots. Instead, the debate should focus on how and to what extent such entities should be included in human practices normally assuming moral agency and responsibility of participants. The proposal is backed up by an analysis of the AMA debate, which is found to be overly caught in the opposition between so-called standard and functionalist conceptions of moral agency, conceptually confused and practically inert. Additionally, we outline some main themes of research in need of attention in light of the suggested normative approach to AMA.},
  archive      = {J_MAM},
  author       = {Behdadi, Dorna and Munthe, Christian},
  doi          = {10.1007/s11023-020-09525-8},
  journal      = {Minds and Machines},
  month        = {6},
  number       = {2},
  pages        = {195-218},
  shortjournal = {Minds Mach.},
  title        = {A normative approach to artificial moral agency},
  volume       = {30},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Analysing the combined health, social and economic impacts
of the corovanvirus pandemic using agent-based social simulation.
<em>MAM</em>, <em>30</em>(2), 177–194. (<a
href="https://doi.org/10.1007/s11023-020-09527-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {During the COVID-19 crisis there have been many difficult decisions governments and other decision makers had to make. E.g. do we go for a total lock down or keep schools open? How many people and which people should be tested? Although there are many good models from e.g. epidemiologists on the spread of the virus under certain conditions, these models do not directly translate into the interventions that can be taken by government. Neither can these models contribute to understand the economic and/or social consequences of the interventions. However, effective and sustainable solutions need to take into account this combination of factors. In this paper, we propose an agent-based social simulation tool, ASSOCC, that supports decision makers understand possible consequences of policy interventions, but exploring the combined social, health and economic consequences of these interventions.},
  archive      = {J_MAM},
  author       = {Dignum, Frank and Dignum, Virginia and Davidsson, Paul and Ghorbani, Amineh and van der Hurk, Mijke and Jensen, Maarten and Kammler, Christian and Lorig, Fabian and Ludescher, Luis Gustavo and Melchior, Alexander and Mellema, René and Pastrav, Cezara and Vanhee, Loïs and Verhagen, Harko},
  doi          = {10.1007/s11023-020-09527-6},
  journal      = {Minds and Machines},
  month        = {6},
  number       = {2},
  pages        = {177-194},
  shortjournal = {Minds Mach.},
  title        = {Analysing the combined health, social and economic impacts of the corovanvirus pandemic using agent-based social simulation},
  volume       = {30},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). The ethical governance of the digital during and after the
COVID-19 pandemic. <em>MAM</em>, <em>30</em>(2), 171–176. (<a
href="https://doi.org/10.1007/s11023-020-09528-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_MAM},
  author       = {Taddeo, Mariarosaria},
  doi          = {10.1007/s11023-020-09528-5},
  journal      = {Minds and Machines},
  month        = {6},
  number       = {2},
  pages        = {171-176},
  shortjournal = {Minds Mach.},
  title        = {The ethical governance of the digital during and after the COVID-19 pandemic},
  volume       = {30},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Ethical implications of closed loop brain device: 10-year
review. <em>MAM</em>, <em>30</em>(1), 145–170. (<a
href="https://doi.org/10.1007/s11023-020-09518-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Closed Loop medical devices such as Closed Loop Deep Brain Stimulation (CL-DBS) and Brain Computer Interface (BCI) are some of the emerging neurotechnologies. New generations of implantable brain–computer interfaces have recently gained success in human clinical trials. These implants detect specific neuronal patterns and provide the subject with information to respond to these patterns. Further, Closed Loop brain devices give control to the subject so that he can respond and decide on a therapeutic goal. Although the implants have improved subjects’ quality of life, their use has raised varied ethical concerns. The aim of this study is to provide an overview of the ethical implications specific to the therapeutic goals of closed loop brain implants. The authors analyze the related work qualitatively and quantitatively so as to help readers in understanding the various ethical implications of closed loop brain implants.},
  archive      = {J_MAM},
  author       = {Aggarwal, Swati and Chugh, Nupur},
  doi          = {10.1007/s11023-020-09518-7},
  journal      = {Minds and Machines},
  month        = {3},
  number       = {1},
  pages        = {145-170},
  shortjournal = {Minds Mach.},
  title        = {Ethical implications of closed loop brain device: 10-year review},
  volume       = {30},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). “Oh, dignity too?” Said the robot: Human dignity as the
basis for the governance of robotics. <em>MAM</em>, <em>30</em>(1),
121–143. (<a href="https://doi.org/10.1007/s11023-019-09514-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Healthcare robots enable practices that seemed far-fetched in the past. Robots might be the solution to bridge the loneliness that the elderly often experience; they may help wheelchair users walk again, or may help navigate the blind. European Institutions, however, acknowledge that human contact is an essential aspect of personal care and that the insertion of robots could dehumanize caring practices. Such instances of human–robot interactions raise the question to what extent the use and development of robots for healthcare applications can challenge the dignity of users. In this article, therefore, we explore how different robot applications in the healthcare domain support individuals in achieving ‘dignity’ or pressure it. We argue that since healthcare robot applications are novel, their associated risks and impacts may be unprecedented and unknown, thus triggering the need for a conceptual instrument that is binding and remains flexible at the same time. In this respect, as safety rules and data protection are often criticized to lack flexibility, and technology ethics to lack enforceability, we suggest human dignity as the overarching governance instrument for robotics, which is the inviolable value upon which all fundamental rights are grounded.},
  archive      = {J_MAM},
  author       = {Zardiashvili, Lexo and Fosch-Villaronga, Eduard},
  doi          = {10.1007/s11023-019-09514-6},
  journal      = {Minds and Machines},
  month        = {3},
  number       = {1},
  pages        = {121-143},
  shortjournal = {Minds Mach.},
  title        = {“Oh, dignity too?” said the robot: Human dignity as the basis for the governance of robotics},
  volume       = {30},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020b). The ethics of AI ethics: An evaluation of guidelines.
<em>MAM</em>, <em>30</em>(1), 99–120. (<a
href="https://doi.org/10.1007/s11023-020-09517-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Current advances in research, development and application of artificial intelligence (AI) systems have yielded a far-reaching discourse on AI ethics. In consequence, a number of ethics guidelines have been released in recent years. These guidelines comprise normative principles and recommendations aimed to harness the “disruptive” potentials of new AI technologies. Designed as a semi-systematic evaluation, this paper analyzes and compares 22 guidelines, highlighting overlaps but also omissions. As a result, I give a detailed overview of the field of AI ethics. Finally, I also examine to what extent the respective ethical principles and values are implemented in the practice of research, development and application of AI systems—and how the effectiveness in the demands of AI ethics can be improved.},
  archive      = {J_MAM},
  author       = {Hagendorff, Thilo},
  doi          = {10.1007/s11023-020-09517-8},
  journal      = {Minds and Machines},
  month        = {3},
  number       = {1},
  pages        = {99-120},
  shortjournal = {Minds Mach.},
  title        = {The ethics of AI ethics: An evaluation of guidelines},
  volume       = {30},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Ethical foresight analysis: What it is and why it is needed?
<em>MAM</em>, <em>30</em>(1), 77–97. (<a
href="https://doi.org/10.1007/s11023-020-09521-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {An increasing number of technology firms are implementing processes to identify and evaluate the ethical risks of their systems and products. A key part of these review processes is to foresee potential impacts of these technologies on different groups of users. In this article, we use the expression Ethical Foresight Analysis (EFA) to refer to a variety of analytical strategies for anticipating or predicting the ethical issues that new technological artefacts, services, and applications may raise. This article examines several existing EFA methodologies currently in use. It identifies the purposes of ethical foresight, the kinds of methods that current methodologies employ, and the strengths and weaknesses of each of these current approaches. The conclusion is that a new kind of foresight analysis on the ethics of emerging technologies is both feasible and urgently needed.},
  archive      = {J_MAM},
  author       = {Floridi, Luciano and Strait, Andrew},
  doi          = {10.1007/s11023-020-09521-y},
  journal      = {Minds and Machines},
  month        = {3},
  number       = {1},
  pages        = {77-97},
  shortjournal = {Minds Mach.},
  title        = {Ethical foresight analysis: What it is and why it is needed?},
  volume       = {30},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A puzzle concerning compositionality in machines.
<em>MAM</em>, <em>30</em>(1), 47–75. (<a
href="https://doi.org/10.1007/s11023-020-09519-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper attempts to describe and address a specific puzzle related to compositionality in artificial networks such as Deep Neural Networks and machine learning in general. The puzzle identified here touches on a larger debate in Artificial Intelligence related to epistemic opacity but specifically focuses on computational applications of human level linguistic abilities or properties and a special difficulty with relation to these. Thus, the resulting issue is both general and unique. A partial solution is suggested.},
  archive      = {J_MAM},
  author       = {Nefdt, Ryan M.},
  doi          = {10.1007/s11023-020-09519-6},
  journal      = {Minds and Machines},
  month        = {3},
  number       = {1},
  pages        = {47-75},
  shortjournal = {Minds Mach.},
  title        = {A puzzle concerning compositionality in machines},
  volume       = {30},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). The physical mandate for belief-goal psychology.
<em>MAM</em>, <em>30</em>(1), 23–45. (<a
href="https://doi.org/10.1007/s11023-020-09515-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article describes a heuristic argument for understanding certain physical systems in terms of properties that resemble the beliefs and goals of folk psychology. The argument rests on very simple assumptions. The core of the argument is that predictions about certain events can legitimately be based on assumptions about later events, resembling Aristotelian ‘final causation’; however, more nuanced causal entities (resembling fallible beliefs) must be introduced into these types of explanation in order for them to remain consistent with a causally local Universe.},
  archive      = {J_MAM},
  author       = {McGregor, Simon and Chrisley, Ron},
  doi          = {10.1007/s11023-020-09515-w},
  journal      = {Minds and Machines},
  month        = {3},
  number       = {1},
  pages        = {23-45},
  shortjournal = {Minds Mach.},
  title        = {The physical mandate for belief-goal psychology},
  volume       = {30},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Dynamical emergence theory (DET): A computational account of
phenomenal consciousness. <em>MAM</em>, <em>30</em>(1), 1–21. (<a
href="https://doi.org/10.1007/s11023-020-09516-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Scientific theories of consciousness identify its contents with the spatiotemporal structure of neural population activity. We follow up on this approach by stating and motivating Dynamical Emergence Theory (DET), which defines the amount and structure of experience in terms of the intrinsic topology and geometry of a physical system’s collective dynamics. Specifically, we posit that distinct perceptual states correspond to coarse-grained macrostates reflecting an optimal partitioning of the system’s state space—a notion that aligns with several ideas and results from computational neuroscience and cognitive psychology. We relate DET to existing work, offer predictions for empirical studies, and outline future research directions.},
  archive      = {J_MAM},
  author       = {Moyal, Roy and Fekete, Tomer and Edelman, Shimon},
  doi          = {10.1007/s11023-020-09516-9},
  journal      = {Minds and Machines},
  month        = {3},
  number       = {1},
  pages        = {1-21},
  shortjournal = {Minds Mach.},
  title        = {Dynamical emergence theory (DET): A computational account of phenomenal consciousness},
  volume       = {30},
  year         = {2020},
}
</textarea>
</details></li>
</ul>

</body>
</html>
