<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>MVA_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="mva---74">MVA - 74</h2>
<ul>
<li><details>
<summary>
(2020). An efficient, dense and long-range marker system for the
guidance of the visually impaired. <em>MVA</em>, <em>31</em>(7), 1–10.
(<a href="https://doi.org/10.1007/s00138-020-01097-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we address the problem of making a mobile/smartphone camera sensitive to distant fiducial markers. To this end, we carefully design a novel visual marker that is both dense and readable from large distances. The main novelty of the proposed marker is the combination of a quaternary color-based coding system with robust methods for reading the color patterns included in each frame once it is detected. These patterns include a CRC whose length grows linearly, whereas that of the message grows quadratically. Our experiments show that the proposed bundle marker-vision algorithm outperforms the alternatives in terms of distance and angle and also that it is very robust to changes in lighting conditions, thus making it a good intelligent system for guiding people with visual impairments in their day to day use of public transportation systems.},
  archive      = {J_MVA},
  author       = {Sáez, Juan Manuel and Lozano, Miguel Angel and Escolano, Francisco and Pita Lozano, Javier},
  doi          = {10.1007/s00138-020-01097-y},
  journal      = {Machine Vision and Applications},
  month        = {11},
  number       = {7},
  pages        = {1-10},
  shortjournal = {Mach. Vis. Appl.},
  title        = {An efficient, dense and long-range marker system for the guidance of the visually impaired},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Enhancing feature fusion for human pose estimation.
<em>MVA</em>, <em>31</em>(7), 1–9. (<a
href="https://doi.org/10.1007/s00138-020-01104-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Current human pose estimation methods mainly rely on designing efficient Convolutional Neural Networks (CNN) frameworks. These CNN architectures typically consist of high-to-low resolution sub-networks to learn semantic information, and then followed by low-to-high sub-networks to raise the resolution to locate the keypoints. Because low-level features have high resolution but less semantic information, while high-level features have rich semantic information but less high resolution details, so it is important to fuse different level features to improve the final performance. However, most existing models implement feature fusion by simply concatenate low-level and high-level features without considering the gap between spatial resolution and semantic levels. In this paper, we propose a new feature fusion method for human pose estimation. We introduce high level semantic information into low-level features to enhance feature fusion. Further, to keep both the high-level semantic information and high-resolution location details, we use Global Convolutional Network blocks to bridge the gap between low-level and high-level features. Experiments on MPII and LSP human pose estimation datasets demonstrate that efficient feature fusion can significantly improve the performance. The code is available at: https://github.com/tongjiangwei/FeatureFusion .},
  archive      = {J_MVA},
  author       = {Wang, Rui and Tong, Jiangwei and Wang, Xiangyang},
  doi          = {10.1007/s00138-020-01104-2},
  journal      = {Machine Vision and Applications},
  month        = {11},
  number       = {7},
  pages        = {1-9},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Enhancing feature fusion for human pose estimation},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Time-efficient spliced image analysis using higher-order
statistics. <em>MVA</em>, <em>31</em>(7), 1–20. (<a
href="https://doi.org/10.1007/s00138-020-01107-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image forgery is gaining huge momentum as changing the content is no longer arduous. One of the leading techniques of this category is image splicing. This technique generates a composite image formed by combining regions of images. Once the image is forged, it becomes nearly impossible for the human expert to substantiate. Hence, for detecting and localizing the spliced region in the forged image, a tool is to be developed which has become the need of the hour. Articles have been reported that one of the key ingredients for such a tool is noise inconsistency, among others. The spliced region contains the non-homogeneous distribution of noise which acts as a feature to localize it. State-of-the-art techniques based on inconsistent noise are suffering from challenges like the requirement of prior knowledge about the image, localization of spliced region and estimation of inconsistent non-gaussian noise. In this paper, a blind local noise estimation technique has been introduced using a fourth-order central moment to localize the spliced region. This paper tries to overcome the challenges of state-of-the-art techniques. Experimental analysis has been done on images of three publicly available datasets. The results are evaluated on pixel level using confusion matrix and some other performance measures. The result of the given approach is compared with previously reported techniques and found better than them.},
  archive      = {J_MVA},
  author       = {Jaiswal, Ankit Kumar and Srivastava, Rajeev},
  doi          = {10.1007/s00138-020-01107-z},
  journal      = {Machine Vision and Applications},
  month        = {11},
  number       = {7},
  pages        = {1-20},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Time-efficient spliced image analysis using higher-order statistics},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). GraspFusionNet: A two-stage multi-parameter grasp detection
network based on RGB–XYZ fusion in dense clutter. <em>MVA</em>,
<em>31</em>(7), 1–19. (<a
href="https://doi.org/10.1007/s00138-020-01108-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Robotic grasping of diverse range of novel objects is a great challenge in dense clutter, which is also critical to many applications. However, current methods are vulnerable to perception uncertainty for dense stacked objects, resulting in limited accuracy of multi-parameter grasp prediction. In this paper, we propose a two-stage grasp detection pipeline including sampling and predicting stages. The first sampling stage applies fully convolutional network to generate grasp proposal regions, which contain potential graspable objects. Among grasp proposal region, the second prediction stage predicts complete grasp parameters based on fusion of RGB–XYZ heightmaps, which are converted from color and depth images. To perceive essential structures of stable grasping, 2D CNN and 3D CNN are used to learn color and geometric features to predict multi-parameter grasp, respectively. The direct mapping from heightmaps to grasp parameters is realized based on a multi-task loss. Experiments on a self-built dataset and an open dataset are conducted to analyze the network performance. The results indicate that the proposed two-stage method achieves the best performance among other grasp detection algorithms. Robotic experiments demonstrate generalization ability and robustness in dense clutter for novel objects, and the proposed method achieves average grasp success rate of 82.4%, which is also better than other state-of-the-art methods. Our self-built dataset and robotic grasping video are available at https://github.com/liuwenhai/toteGrasping.git .},
  archive      = {J_MVA},
  author       = {Wang, Weiming and Liu, Wenhai and Hu, Jie and Fang, Yi and Shao, Quanquan and Qi, Jin},
  doi          = {10.1007/s00138-020-01108-y},
  journal      = {Machine Vision and Applications},
  month        = {11},
  number       = {7},
  pages        = {1-19},
  shortjournal = {Mach. Vis. Appl.},
  title        = {GraspFusionNet: A two-stage multi-parameter grasp detection network based on RGB–XYZ fusion in dense clutter},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Autocalibration method for scanning electron microscope
using affine camera model. <em>MVA</em>, <em>31</em>(7), 1–15. (<a
href="https://doi.org/10.1007/s00138-020-01109-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper deals with the task of autocalibration of scanning electron microscope (SEM), which is a technique allowing to compute camera motion and intrinsic parameters. In contrast to classical calibration, which implies the use of a calibration object and is known to be a tedious and rigid operation, auto- or selfcalibration is performed directly on the images acquired for the visual task. As autocalibration represents an optimization problem, all the steps contributing to the success of the algorithm are presented: formulation of the cost function incorporating metric constraints, definition of bounds, regularization, and optimization algorithm. The presented method allows full estimation of camera matrices for all views in the sequence. It was validated on virtual images as well as on real SEM images (pollen grains, cutting tools, etc.). The results show a good convergence range and low execution time, notably compared to classical methods, and even more in the context of the calibration of SEM.},
  archive      = {J_MVA},
  author       = {Kudryavtsev, Andrey V. and Guelpa, Valérian and Rougeot, Patrick and Lehmann, Olivier and Dembélé, Sounkalo and Sturm, Peter and Le Fort-Piat, Nadine},
  doi          = {10.1007/s00138-020-01109-x},
  journal      = {Machine Vision and Applications},
  month        = {11},
  number       = {7},
  pages        = {1-15},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Autocalibration method for scanning electron microscope using affine camera model},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Finding hard faces with better proposals and classifier.
<em>MVA</em>, <em>31</em>(7), 1–15. (<a
href="https://doi.org/10.1007/s00138-020-01110-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent studies witnessed that deep CNNs significantly improve the performance of face detection in the wild. However, detecting faces with small scales, large pose variations, and occlusions is still challenging. In this paper, to detect challenging faces, we present a boosted faster RCNN (F-RCN) version with an enhanced region proposal network (eRPN) module and newly introduced hard example mining strategies. The eRPN module generates better proposals than traditional RPN by integarating semantic information into the input feature maps. Two hard example mining strategies, i.e., online hard proposal mining (OHPM) and offline hard image mining (OHIM), are proposed to train better classifier. The OHPM can effectively sample quality and diversity of hard positive examples, which is important for detecting hard faces like tiny faces. The OHIM further boosts the classifier to detect hard faces via an auxiliary fine-tuning on a small proportion of training data. Experimental results on the FDDB, WIDER FACE, Pascal Faces, and AFW datasets show that our method significantly improves the faster-RCNN face detector and achieves performance superior or comparable to the state-of-the-art face detectors.},
  archive      = {J_MVA},
  author       = {Zeng, Xiaoxing and Peng, Xiaojiang and Wang, Yali and Qiao, Yu},
  doi          = {10.1007/s00138-020-01110-4},
  journal      = {Machine Vision and Applications},
  month        = {11},
  number       = {7},
  pages        = {1-15},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Finding hard faces with better proposals and classifier},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Real-time and accurate abnormal behavior detection in
videos. <em>MVA</em>, <em>31</em>(7), 1–13. (<a
href="https://doi.org/10.1007/s00138-020-01111-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abnormal crowd behavior detection is a hot research topic in the field of computer vision. In order to solve the problems of high computational cost and the imbalance between positive and negative samples, we propose an efficient algorithm that can detect and locate anomalies in videos. In order to solve the problem of less negative samples, the algorithm uses the spatiotemporal autoencoder to identify and extract the negative samples (contain abnormal behaviors) in the dataset in an unsupervised learning method. On this basis, a spatiotemporal convolutional neural network (CNN) is constructed with simple structure and low computational complexity. The supervised training method is used to train the spatiotemporal CNN with positive and negative samples to generate the detection model. Experiments are conducted on the UCSD and UMN datasets. The experiment results show that the proposed algorithm can detect and locate abnormal behaviors in real time (using only CPU), and the accuracy of the algorithm exceeds those of the existing algorithms at both the pixel level and frame level.},
  archive      = {J_MVA},
  author       = {Fan, Zheyi and Yin, Jianyuan and Song, Yu and Liu, Zhiwen},
  doi          = {10.1007/s00138-020-01111-3},
  journal      = {Machine Vision and Applications},
  month        = {11},
  number       = {7},
  pages        = {1-13},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Real-time and accurate abnormal behavior detection in videos},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Head and camera rotation invariant eye tracking algorithm
based on segmented group method of data handling. <em>MVA</em>,
<em>31</em>(7), 1–10. (<a
href="https://doi.org/10.1007/s00138-020-01112-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Eye-gaze tracking through camera is commonly used in a number of areas, such as computer user interface systems, sports science, psychology, and biometrics. The robustness of the head and camera rotation tracking algorithm has been a critical problem in recent years. In this paper, Haar-like features and a modified version of the group method of data handling, as well as segmented regression, are used together to find the base points of the eyes in a facial image. Then, a geometric transformation is applied to detect precise eye-gaze direction. The proposed algorithm is tested on GI4E and Columbia Gaze datasets and compared to other algorithms. The results show adequate accuracy, especially when the head/camera is rotated.},
  archive      = {J_MVA},
  author       = {Mohebbian, Mohammad Reza and Rasti, Javad},
  doi          = {10.1007/s00138-020-01112-2},
  journal      = {Machine Vision and Applications},
  month        = {11},
  number       = {7},
  pages        = {1-10},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Head and camera rotation invariant eye tracking algorithm based on segmented group method of data handling},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A real-time and precise ellipse detector via edge screening
and aggregation. <em>MVA</em>, <em>31</em>(7), 1–23. (<a
href="https://doi.org/10.1007/s00138-020-01113-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A fast and precise method for ellipse detection is proposed in this paper. The method aims at clearly removing the lines and curves which are not ellipse edges to improve the ellipse fitting. In arc extraction, the arcs are divided into four categories according to the gradient, and the size constraint is exploited to remove the interference lines. Then, the arc relative position constraints and the tangent lines constraint are employed to exactly group the arcs that belong to the same ellipse into a set. Finally, a post-processing approach is developed to remove the invalid ellipses. Due to the effective removal of the interference edges and the designed geometric multi-constraint, the computational costs of arc grouping and parameter estimation are dramatically reduced, and the fitting results are finely agreeable to the actual ellipse contours. The performance is evaluated with 3600 synthetic images and 1517 real images, and the experimental results demonstrate that the proposed method runs much faster than the current speed leading methods with the comparable or higher F-measure.},
  archive      = {J_MVA},
  author       = {Liu, Zhenyu and Liu, Xia and Duan, Guifang and Tan, Jianrong},
  doi          = {10.1007/s00138-020-01113-1},
  journal      = {Machine Vision and Applications},
  month        = {11},
  number       = {7},
  pages        = {1-23},
  shortjournal = {Mach. Vis. Appl.},
  title        = {A real-time and precise ellipse detector via edge screening and aggregation},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A neural network ensemble method for effective crack
segmentation using fully convolutional networks and multi-scale
structured forests. <em>MVA</em>, <em>31</em>(7), 1–18. (<a
href="https://doi.org/10.1007/s00138-020-01114-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Crack image segmentation has recently become a major research topic in nondestructive inspection. However, the image segmentation methods are not robust to variations such as illumination, weather, noise and the segmentation accuracy which cannot meet the requirements of practical applications. Therefore, a neural network ensemble method is proposed for effective crack segmentation in this paper, which consists of fully convolution networks (FCN) and multi-scale structured forests for edge detection (SFD). In order to improve the accuracy of crack segmentation and reduce the error mark under complex background, a new network model based on FCN model is proposed to address the problems that lose local information and the capacity of partial refinement, which are frequently encountered in FCN model in the crack segmentation. In addition, SFD is combined with the half-reconstruction method of anti-symmetrical bi-orthogonal wavelet to overcome the limitation of crack edge detection. Finally, the result of the two maps is merged after resizing to the original image dimensions. Qualitative and quantitative evaluations of the proposed methods are performed, showing that they can obtain better results than certain existing methods for crack segmentation.},
  archive      = {J_MVA},
  author       = {Wang, Sen and Wu, Xing and Zhang, Yinghui and Liu, Xiaoqin and Zhao, Lun},
  doi          = {10.1007/s00138-020-01114-0},
  journal      = {Machine Vision and Applications},
  month        = {11},
  number       = {7},
  pages        = {1-18},
  shortjournal = {Mach. Vis. Appl.},
  title        = {A neural network ensemble method for effective crack segmentation using fully convolutional networks and multi-scale structured forests},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A study of deep learning approaches for classification and
detection chromosomes in metaphase images. <em>MVA</em>, <em>31</em>(7),
1–18. (<a href="https://doi.org/10.1007/s00138-020-01115-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Chromosome analysis is an important approach to detecting genetic diseases. However, the process of identifying chromosomes in metaphase images can be challenging and time-consuming. Therefore, it is important to use automatic methods for detecting chromosomes to aid diagnosis. This work proposes a study of deep learning approaches for classification and detection of chromosome in metaphase images. Furthermore, we propose a method for detecting chromosomes, which includes new stages for preprocessing and reducing false positives and false negatives. The proposed method is evaluated using 74 chromosome images in the metaphase stage, which were obtained from the CRCN-NE database, resulting in 2174 chromosome regions. We undertake three types of evaluation: segmentation; classification of cropped regions of chromosomes; and detection of chromosomes in the original images. For the segmentation analysis, we evaluated the Otsu, adaptive, fuzzy and fuzzy-adaptive methods. For classification and detection, we evaluated the following state-of-the-art algorithms: VGG16, VGG19, Inception v3, MobileNet, Xception, Sharma and MiniVGG. The classification results showed that the proposed approach, using segmented images, obtained better results than using RGB images. Furthermore, when analyzing deep learning approaches, the VGG16 algorithm obtained the best results, using fine tuning, with a sensitivity of 0.98, specificity of 0.99 and AUC of 0.955. The results also showed that the proposed negative reduction method increased sensitivity by 18%, while maintaining the specificity value. Deep learning methods have been proved to be efficient at detecting chromosomes, but preprocessing and post-processing are important to avoid false negatives. Therefore, using binary images and adding stages for reducing false positives and false negatives are necessary in order to increase the quality of the images of the chromosomes detected.},
  archive      = {J_MVA},
  author       = {Andrade, Maria F. S. and Dias, Lucas V. and Macario, Valmir and Lima, Fabiana F. and Hwang, Suy F. and Silva, Júlio C. G. and Cordeiro, Filipe R.},
  doi          = {10.1007/s00138-020-01115-z},
  journal      = {Machine Vision and Applications},
  month        = {11},
  number       = {7},
  pages        = {1-18},
  shortjournal = {Mach. Vis. Appl.},
  title        = {A study of deep learning approaches for classification and detection chromosomes in metaphase images},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Detection of 3D bounding boxes of vehicles using perspective
transformation for accurate speed measurement. <em>MVA</em>,
<em>31</em>(7), 1–15. (<a
href="https://doi.org/10.1007/s00138-020-01117-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Detection and tracking of vehicles captured by traffic surveillance cameras is a key component of intelligent transportation systems. We present an improved version of our algorithm for detection of 3D bounding boxes of vehicles, their tracking and subsequent speed estimation. Our algorithm utilizes the known geometry of vanishing points in the surveilled scene to construct a perspective transformation. The transformation enables an intuitive simplification of the problem of detecting 3D bounding boxes to detection of 2D bounding boxes with one additional parameter using a standard 2D object detector. Main contribution of this paper is an improved construction of the perspective transformation which is more robust and fully automatic and an extended experimental evaluation of speed estimation. We test our algorithm on the speed estimation task of the BrnoCompSpeed dataset. We evaluate our approach with different configurations to gauge the relationship between accuracy and computational costs and benefits of 3D bounding box detection over 2D detection. All of the tested configurations run in real time and are fully automatic. Compared to other published state-of-the-art fully automatic results, our algorithm reduces the mean absolute speed measurement error by 32% (1.10 km/h to 0.75 km/h) and the absolute median error by 40% (0.97 km/h to 0.58 km/h).},
  archive      = {J_MVA},
  author       = {Kocur, Viktor and Ftáčnik, Milan},
  doi          = {10.1007/s00138-020-01117-x},
  journal      = {Machine Vision and Applications},
  month        = {11},
  number       = {7},
  pages        = {1-15},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Detection of 3D bounding boxes of vehicles using perspective transformation for accurate speed measurement},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Deep understanding of shopper behaviours and interactions
using RGB-d vision. <em>MVA</em>, <em>31</em>(7), 1–21. (<a
href="https://doi.org/10.1007/s00138-020-01118-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In retail environments, understanding how shoppers move about in a store’s spaces and interact with products is very valuable. While the retail environment has several favourable characteristics that support computer vision, such as reasonable lighting, the large number and diversity of products sold, as well as the potential ambiguity of shoppers’ movements, mean that accurately measuring shopper behaviour is still challenging. Over the past years, machine-learning and feature-based tools for people counting as well as interactions analytic and re-identification were developed with the aim of learning shopper skills based on occlusion-free RGB-D cameras in a top-view configuration. However, after moving into the era of multimedia big data, machine-learning approaches evolved into deep learning approaches, which are a more powerful and efficient way of dealing with the complexities of human behaviour. In this paper, a novel VRAI deep learning application that uses three convolutional neural networks to count the number of people passing or stopping in the camera area, perform top-view re-identification and measure shopper–shelf interactions from a single RGB-D video flow with near real-time performances has been introduced. The framework is evaluated on the following three new datasets that are publicly available: TVHeads for people counting, HaDa for shopper–shelf interactions and TVPR2 for people re-identification. The experimental results show that the proposed methods significantly outperform all competitive state-of-the-art methods (accuracy of 99.5% on people counting, 92.6% on interaction classification and 74.5% on re-id), bringing to different and significative insights for implicit and extensive shopper behaviour analysis for marketing applications.},
  archive      = {J_MVA},
  author       = {Paolanti, Marina and Pietrini, Rocco and Mancini, Adriano and Frontoni, Emanuele and Zingaretti, Primo},
  doi          = {10.1007/s00138-020-01118-w},
  journal      = {Machine Vision and Applications},
  month        = {11},
  number       = {7},
  pages        = {1-21},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Deep understanding of shopper behaviours and interactions using RGB-D vision},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A new feature extraction process based on SFTA and DWT to
enhance classification of ceramic tiles quality. <em>MVA</em>,
<em>31</em>(7), 1–15. (<a
href="https://doi.org/10.1007/s00138-020-01121-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a combination of image processing methods to detect ceramic tiles defects automatically. The primary goal is to identify faults in ceramic tiles, with or without texture. The process consists of four steps: preprocessing, feature extraction, optimization, and classification. In the second step, gray-level co-occurrence matrix, segmentation-based fractal texture analysis, discrete wavelet transform, local binary pattern, and a novel method composed of segmentation-based fractal texture analysis and discrete wavelet transform are applied. The genetic algorithm was used to optimize the parameters. In the classification step, k-nearest neighbor, support vector machine, multilayer perceptron, probabilistic neural network, and radial basis function network were assessed. Two datasets were used to validate the proposed process, totaling 782 ceramic tiles. In comparison with the other feature extraction methods commonly used, we demonstrate that the use of SFTA with DWT had a remarkable increase in the overall accuracy, without compromising computational time. The proposed method can be executed in real time on actual production lines and reaches a defect detection accuracy of 99.01% for smooth tiles and 97.89% for textured ones.},
  archive      = {J_MVA},
  author       = {Casagrande, Luan and Macarini, Luiz Antonio Buschetto and Bitencourt, Daniel and Fröhlich, Antônio Augusto and de Araujo, Gustavo Medeiros},
  doi          = {10.1007/s00138-020-01121-1},
  journal      = {Machine Vision and Applications},
  month        = {11},
  number       = {7},
  pages        = {1-15},
  shortjournal = {Mach. Vis. Appl.},
  title        = {A new feature extraction process based on SFTA and DWT to enhance classification of ceramic tiles quality},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Deep learning based breast cancer detection and
classification using fuzzy merging techniques. <em>MVA</em>,
<em>31</em>(7), 1–18. (<a
href="https://doi.org/10.1007/s00138-020-01122-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Automatic identification of abnormal and normal cells is a critical step in computer-assisted pathology, owing to certain heterogeneous characteristics of cancer cells. However, automated nuclei detection is problematic in unevenly shaped, overlapping and touching nuclei. It is, consequently, essential to detect single and overlapping nuclei and distinguish them from single ones for a reasonable quantitative analysis. Diagnosis is improved by introducing a computer-aided diagnosis system to automatically detect breast cancer tissue nuclei from whole slide images of hematoxylin and eosin stains. We propose a method for the automatic cell nuclei detection, segmentation, and classification of breast cancer using a deep convolutional neural network (Deep-CNN) approach. The main contribution of this work is the detection of nuclei using anisotropic diffusion in a filter and applying a novel multilevel saliency nuclei detection model in ductal carcinoma of breast cancer tissue. The detected nuclei are classified into benign and malignant cells by applying the new Deep-CNN model. Finally, the novel multilevel saliency nuclei detection technique is integrated with the Deep-CNN to produce an nMSDeep-CNN model that turns out to be the most accurate results with very less computation time. The accuracy, sensitivity and specificity of the proposed system are 98.62%, 0.947 and 0.964, respectively. The classification for benign and malignant cells is evaluated by applying 10 fold cross-validation. Thus, the system can be clinically used for an objective, accurate, and rapid diagnosis of abnormal tissue. The effectiveness of the suggested framework is demonstrated through experiments on several datasets.},
  archive      = {J_MVA},
  author       = {Krithiga, R. and Geetha, P.},
  doi          = {10.1007/s00138-020-01122-0},
  journal      = {Machine Vision and Applications},
  month        = {11},
  number       = {7},
  pages        = {1-18},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Deep learning based breast cancer detection and classification using fuzzy merging techniques},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Benchmarking deep network architectures for ethnicity
recognition using a new large face dataset. <em>MVA</em>,
<em>31</em>(7), 1–13. (<a
href="https://doi.org/10.1007/s00138-020-01123-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Although in recent years we have witnessed an explosion of the scientific research in the recognition of facial soft biometrics such as gender, age and expression with deep neural networks, the recognition of ethnicity has not received the same attention from the scientific community. The growth of this field is hindered by two related factors: on the one hand, the absence of a dataset sufficiently large and representative does not allow an effective training of convolutional neural networks for the recognition of ethnicity; on the other hand, the collection of new ethnicity datasets is far from simple and must be carried out manually by humans trained to recognize the basic ethnicity groups using the somatic facial features. To fill this gap in the facial soft biometrics analysis, we propose the VGGFace2 Mivia Ethnicity Recognition (VMER) dataset, composed by more than 3,000,000 face images annotated with 4 ethnicity categories, namely African American, East Asian, Caucasian Latin and Asian Indian. The final annotations are obtained with a protocol which requires the opinion of three people belonging to different ethnicities, in order to avoid the bias introduced by the well-known other race effect. In addition, we carry out a comprehensive performance analysis of popular deep network architectures, namely VGG-16, VGG-Face, ResNet-50 and MobileNet v2. Finally, we perform a cross-dataset evaluation to demonstrate that the deep network architectures trained with VMER generalize on different test sets better than the same models trained on the largest ethnicity dataset available so far. The ethnicity labels of the VMER dataset and the code used for the experiments are available upon request at https://mivia.unisa.it .},
  archive      = {J_MVA},
  author       = {Greco, Antonio and Percannella, Gennaro and Vento, Mario and Vigilante, Vincenzo},
  doi          = {10.1007/s00138-020-01123-z},
  journal      = {Machine Vision and Applications},
  month        = {11},
  number       = {7},
  pages        = {1-13},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Benchmarking deep network architectures for ethnicity recognition using a new large face dataset},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Impurities detection in edible bird’s nest using optical
segmentation and image fusion. <em>MVA</em>, <em>31</em>(7), 1–8. (<a
href="https://doi.org/10.1007/s00138-020-01124-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The cleanliness of edible bird’s nest (EBN) is among the determinative factors for market acceptance. As it is meant for human consumption, EBN should be free of any impurities or matter which are foreign to it, such as bird feathers, egg fragments and droppings. However, natural variations in composition, density and thickness impose inconsistency to the level of translucency and colour of EBN, resulting in intensity inhomogeneity in EBN images that substantially reduce the accuracy of the segmentation and detection of impurities. Consequently, the segmentation and detection of impurities, which are essential to visual automation in the cleaning and inspection processes, remain unsolved. This work proposes a novel optical segmentation method to segment impurities from the EBN, in order to facilitate the detection of impurities. EBN images captured under two different lighting scenarios, namely, low-angle blue-diffused lighting and red-diffused backlighting, were used to prepare the fused image for background-EBN-impurities segmentation. The applicability of the method was demonstrated by comparing the detection results with those of human inspectors. With a simple thresholding operation performed on fused images, the impurities detection algorithm recorded a true positive/recall rate of 93.39%, a precision of 71.17% and a false-negative detection rate of 4.8%. Despite the high misclassification rate of 32.25%, the algorithm was able to detect more than 93% of the impurities, compared to human inspectors, who required a second examination on the EBNs.},
  archive      = {J_MVA},
  author       = {Yee, Cong Kai and Yeo, Ying Heng and Cheng, Lai Hoong and Yen, Kin Sam},
  doi          = {10.1007/s00138-020-01124-y},
  journal      = {Machine Vision and Applications},
  month        = {11},
  number       = {7},
  pages        = {1-8},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Impurities detection in edible bird’s nest using optical segmentation and image fusion},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Road obstacles positional and dynamic features extraction
combining object detection, stereo disparity maps and optical flow data.
<em>MVA</em>, <em>31</em>(7), 1–11. (<a
href="https://doi.org/10.1007/s00138-020-01126-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {One of the most relevant tasks in an intelligent vehicle navigation system is the detection of obstacles. It is important that a visual perception system for navigation purposes identifies obstacles, and it is also important that this system can extract essential information that may influence the vehicle’s behavior, whether it will be generating an alert for a human driver or guide an autonomous vehicle in order to be able to make its driving decisions. In this paper we present an approach for the identification of obstacles and extraction of class, position, depth and motion information from these objects that employs data gained exclusively from passive vision. We use a convolutional neural network for the obstacles detection, optical flow for the analysis of movement of the detected obstacles, both in relation to the direction and in relation to the intensity of the movement, and also stereo vision for the analysis of distance of obstacles in relation to the vehicle. We performed our experiments on two different datasets, and the results obtained showed a good efficacy from the use of depth and motion patterns to assess the obstacles’ potential threat status.},
  archive      = {J_MVA},
  author       = {Rateke, Thiago and Wangenheim, Aldo von},
  doi          = {10.1007/s00138-020-01126-w},
  journal      = {Machine Vision and Applications},
  month        = {11},
  number       = {7},
  pages        = {1-11},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Road obstacles positional and dynamic features extraction combining object detection, stereo disparity maps and optical flow data},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Rapid self-localization of robot based on omnidirectional
vision technology. <em>MVA</em>, <em>31</em>(7), 1–20. (<a
href="https://doi.org/10.1007/s00138-020-01129-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose a self-localization method for a soccer robot using an omnidirectional camera. Based on the projective geometry of the omnidirectional visual system, the image distortion from the original omnidirectional image can be completely corrected, so the robot can quickly localize itself on the playing field. First, we transform the distorted omnidirectional image to a distortion-free unwrapped image of the soccer field by projective geometry. The obtained image makes the sequent field recognizable and the self-localization of the robot more convenient and accurate. Then, by geometric invariants, the correspondence between the unwrapped image and the model of the playing field is constructed. Next, the homography theory is applied to get the precise location and orientation of the robot. The simulation and experimental results show that the proposed method can quickly and accurately determine the position and azimuth of the soccer robot and the distance between two objects on the playing field.},
  archive      = {J_MVA},
  author       = {Chia, Tsorng-Lin and Chiang, Shu-Yin and Hsieh, Chaur-Heh},
  doi          = {10.1007/s00138-020-01129-7},
  journal      = {Machine Vision and Applications},
  month        = {11},
  number       = {7},
  pages        = {1-20},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Rapid self-localization of robot based on omnidirectional vision technology},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Root identification in minirhizotron imagery with multiple
instance learning. <em>MVA</em>, <em>31</em>(6), 1–13. (<a
href="https://doi.org/10.1007/s00138-020-01088-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, multiple instance learning (MIL) algorithms to automatically perform root detection and segmentation in minirhizotron imagery using only image-level labels are proposed. Root and soil characteristics vary from location to location, and thus, supervised machine learning approaches that are trained with local data provide the best ability to identify and segment roots in minirhizotron imagery. However, labeling roots for training data (or otherwise) is an extremely tedious and time-consuming task. This paper aims to address this problem by labeling data at the image level (rather than the individual root or root pixel level) and train algorithms to perform individual root pixel level segmentation using MIL strategies. Three MIL methods (multiple instance adaptive cosine coherence estimator, multiple instance support vector machine, multiple instance learning with randomized trees) were applied to root detection and compared to non-MIL approaches. The results show that MIL methods improve root segmentation in challenging minirhizotron imagery and reduce the labeling burden. In our results, multiple instance support vector machine outperformed other methods. The multiple instance adaptive cosine coherence estimator algorithm was a close second with an added advantage that it learned an interpretable root signature which identified the traits used to distinguish roots from soil and did not require parameter selection.},
  archive      = {J_MVA},
  author       = {Yu, Guohao and Zare, Alina and Sheng, Hudanyun and Matamala, Roser and Reyes-Cabrera, Joel and Fritschi, Felix B. and Juenger, Thomas E.},
  doi          = {10.1007/s00138-020-01088-z},
  journal      = {Machine Vision and Applications},
  month        = {9},
  number       = {6},
  pages        = {1-13},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Root identification in minirhizotron imagery with multiple instance learning},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). WatchNet++: Efficient and accurate depth-based network for
detecting people attacks and intrusion. <em>MVA</em>, <em>31</em>(6),
1–16. (<a href="https://doi.org/10.1007/s00138-020-01089-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present an efficient and accurate people detection approach based on deep learning to detect people attacks and intrusion in video surveillance scenarios Unlike other approaches using background segmentation and pre-processing techniques, which are not able to distinguish people from other elements in the scene, we propose WatchNet++ that is a depth-based and sequential network that localizes people in top-view depth images by predicting human body joints and pairwise connections (links) such as head and shoulders. WatchNet++ comprises a set of prediction stages and up-sampling operations that progressively refine the predictions of joints and links, leading to more accurate localization results. In order to train the network with varied and abundant data, we also present a large synthetic dataset of depth images with human models that is used to pre-train the network model. Subsequently, domain adaptation to real data is done via fine-tuning using a real dataset of depth images with people performing attacks and intrusion. An extensive evaluation of the proposed approach is conducted for the detection of attacks in airlocks and the counting of people in indoors and outdoors, showing high detection scores and efficiency. The network runs at 10 and 28 FPS using CPU and GPU, respectively.},
  archive      = {J_MVA},
  author       = {Villamizar, M. and Martínez-González, A. and Canévet, O. and Odobez, J.-M.},
  doi          = {10.1007/s00138-020-01089-y},
  journal      = {Machine Vision and Applications},
  month        = {9},
  number       = {6},
  pages        = {1-16},
  shortjournal = {Mach. Vis. Appl.},
  title        = {WatchNet++: Efficient and accurate depth-based network for detecting people attacks and intrusion},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Boosting binary masks for multi-domain learning through
affine transformations. <em>MVA</em>, <em>31</em>(6), 1–14. (<a
href="https://doi.org/10.1007/s00138-020-01090-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this work, we present a new, algorithm for multi-domain learning. Given a pretrained architecture and a set of visual domains received sequentially, the goal of multi-domain learning is to produce a single model performing a task in all the domains together. Recent works showed how we can address this problem by masking the internal weights of a given original convnet through learned binary variables. In this work, we provide a general formulation of binary mask-based models for multi-domain learning by affine transformations of the original network parameters. Our formulation obtains significantly higher levels of adaptation to new domains, achieving performances comparable to domain-specific models while requiring slightly more than 1 bit per network parameter per additional domain. Experiments on two popular benchmarks showcase the power of our approach, achieving performances close to state-of-the-art methods on the Visual Decathlon Challenge.},
  archive      = {J_MVA},
  author       = {Mancini, Massimiliano and Ricci, Elisa and Caputo, Barbara and Rota Bulò, Samuel},
  doi          = {10.1007/s00138-020-01090-5},
  journal      = {Machine Vision and Applications},
  month        = {9},
  number       = {6},
  pages        = {1-14},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Boosting binary masks for multi-domain learning through affine transformations},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Multi-source domain adaptation for image classification.
<em>MVA</em>, <em>31</em>(6), 1–19. (<a
href="https://doi.org/10.1007/s00138-020-01093-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, domain adaptation and transfer learning are known as promising techniques with admirable performance to deal with problems with distribution difference between the training (source domain) and test (target domain) data. In this paper, a novel unsupervised multi-source transductive transfer learning approach, referred to as multi-source domain adaptation for image classification (MDA), is proposed, to transfer knowledge across the selected samples of multiple-source domains and samples of target domain into a shared low-dimensional subspace with maximum decision regions. MDA extends maximum mean discrepancy criteria across multiple-source domains to find an optimal projection subspace and constructs embedded condensed domain-invariant clusters. Furthermore, MDA minimizes empirical risk and maximizes the rate of consistency between manifold and prediction function via learning an optimal classification. Extensive evaluations on two types of visual benchmark datasets under different difficulties illustrate that MDA significantly outperforms other baseline and state-of-the-art methods in both multiple- and single-source tasks. Our source code is available at https://github.com/jtahmores/MDA .},
  archive      = {J_MVA},
  author       = {Karimpour, Morvarid and Noori Saray, Shiva and Tahmoresnezhad, Jafar and Pourmahmood Aghababa, Mohammad},
  doi          = {10.1007/s00138-020-01093-2},
  journal      = {Machine Vision and Applications},
  month        = {9},
  number       = {6},
  pages        = {1-19},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Multi-source domain adaptation for image classification},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A comparative study of breast cancer tumor classification by
classical machine learning methods and deep learning method.
<em>MVA</em>, <em>31</em>(6), 1–10. (<a
href="https://doi.org/10.1007/s00138-020-01094-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In contemporary times, machine learning is being used in almost every field due to its better performance. Here, we consider different machine learning methods such as logistic regression, random forest, support vector classifier (SVC), AdaBoost classifier, bagging classifier, voting classifier, and Xception model to classify the breast cancer tumor and evaluate their performances. We used a standard dataset, i.e., breast Histopathology images, that has more than two lakhs color patches, each patch of size $$50\times 50$$ scanned at the resolution of 40 $$\times $$ . We use 60% of the above-mentioned dataset for training, 20% for validation, and 20% testing to all above-mentioned classifiers. The logistic regression classifier provides the scores of each precision, recall, and F1 measure as 0.72. The random forest method provides the score of each precision, recall, and F1 score as 0.80. The bagging and voting classifiers both have the values of each precision, recalls, and F1 scores as 0.81. In this case, both SVC and AdaBoost classifiers have the score of each precision, recall, and F1 score as 0.82, whereas in the case of the deep learning method, Xception model is used to have the score of each precision, recall, and F1 measure as 0.90 in the same condition. Thus, the Xception method performs the best among all mentioned methods in terms of each of the performance measures, i.e., precision, recall, and F1 score for the classification of breast cancer tumors. Thus, the importance of this research work is that we can classify tumors more accurately in less time. It may increase awareness of people toward breast cancer and decrease fears of tumors.},
  archive      = {J_MVA},
  author       = {Yadavendra and Chand, Satish},
  doi          = {10.1007/s00138-020-01094-1},
  journal      = {Machine Vision and Applications},
  month        = {9},
  number       = {6},
  pages        = {1-10},
  shortjournal = {Mach. Vis. Appl.},
  title        = {A comparative study of breast cancer tumor classification by classical machine learning methods and deep learning method},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Efficient use of recent progresses for real-time semantic
segmentation. <em>MVA</em>, <em>31</em>(6), 1–9. (<a
href="https://doi.org/10.1007/s00138-020-01095-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Different approaches were proposed to design deep CNNs for semantic segmentation. Usually, they are built upon an encoder–decoder architecture and require computationally expensive operations on high-resolution activation maps. Since for real-time segmentation the costs are critical, efficient approaches compromise spatial information to achieve real-time segmentation but with a considerable drop in accuracy. We introduce a new module based on depthwise separable, shuffled and grouped convolutions that optimize up-sampling operations by using a sizeable receptive field and preserving spatial information. Then, we designed an efficient network based on dense connectivity to achieve a remarkable trade-off accuracy and speed. We show through set of experiments that even by up-sampling with a lightweight decoder, our applied architecture scores on Cityscape 69.5% Mean IoU with $$1024\times 512$$ inputs and 95.2 FPS on the test set.},
  archive      = {J_MVA},
  author       = {El Houfi, Safae and Majda, Aicha},
  doi          = {10.1007/s00138-020-01095-0},
  journal      = {Machine Vision and Applications},
  month        = {9},
  number       = {6},
  pages        = {1-9},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Efficient use of recent progresses for real-time semantic segmentation},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Semi-supervised learning using adversarial training with
good and bad samples. <em>MVA</em>, <em>31</em>(6), 1–11. (<a
href="https://doi.org/10.1007/s00138-020-01096-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this work, we investigate semi-supervised learning (SSL) for image classification using adversarial training. Previous results have illustrated that generative adversarial networks (GANs) can be used for multiple purposes in SSL . Triple-GAN, which aims to jointly optimize model components by incorporating three players, generates suitable image-label pairs to compensate for the lack of labeled data in SSL with improved benchmark performance. Conversely, Bad (or complementary) GAN optimizes generation to produce complementary data-label pairs and force a classifier’s decision boundary to lie between data manifolds. Although it generally outperforms Triple-GAN, Bad GAN is highly sensitive to the amount of labeled data used for training. Unifying these two approaches, we present unified-GAN (UGAN), a novel framework that enables a classifier to simultaneously learn from both good and bad samples through adversarial training. We perform extensive experiments on various datasets and demonstrate that UGAN: (1) achieves competitive performance among other GAN-based models, and (2) is robust to variations in the amount of labeled data used for training.},
  archive      = {J_MVA},
  author       = {Li, Wenyuan and Wang, Zichen and Yue, Yuguang and Li, Jiayun and Speier, William and Zhou, Mingyuan and Arnold, Corey},
  doi          = {10.1007/s00138-020-01096-z},
  journal      = {Machine Vision and Applications},
  month        = {9},
  number       = {6},
  pages        = {1-11},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Semi-supervised learning using adversarial training with good and bad samples},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). ARF-crack: Rotation invariant deep fully convolutional
network for pixel-level crack detection. <em>MVA</em>, <em>31</em>(6),
1–12. (<a href="https://doi.org/10.1007/s00138-020-01098-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Autonomous detection of structural defect from images is a promising, but also challenging task to replace manual inspection. With the development of deep learning algorithms, several studies have adopted deep convolutional neural networks (CNN) or fully convolutional networks (FCN) to detect cracks in pixel-level. However, a fundamental property of cracks, that they are rotation invariant, has never been exploited. Although the rotation-invariant property can be implicitly learned by data augmentation, the network needs more parameters to learn features of different orientations and thus tend to overfit the training data. In this study, a rotation-invariant FCN called ARF-Crack is proposed that utilizes the rotation-invariant property of cracks explicitly. The architecture of a state-of-the-art FCN called DeepCrack for pixel-level crack detection is adopted and revised where active rotating filters (ARFs) are used to encode the rotation-invariant property into the network. The proposed ARF-Crack is evaluated on several benchmark datasets including concrete cracks, pavement cracks and corrosion images. The experimental results show that the proposed ARF-Crack requires less number of network parameters and achieves the highest average precision values for all the benchmark datasets compared to other approaches. The proposed ARF-Crack has the potential of detecting other rotation-invariant defects.},
  archive      = {J_MVA},
  author       = {Chen, Fu-Chen and Jahanshahi, Mohammad R.},
  doi          = {10.1007/s00138-020-01098-x},
  journal      = {Machine Vision and Applications},
  month        = {9},
  number       = {6},
  pages        = {1-12},
  shortjournal = {Mach. Vis. Appl.},
  title        = {ARF-crack: Rotation invariant deep fully convolutional network for pixel-level crack detection},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). 3D video semantic segmentation for wildfire smoke.
<em>MVA</em>, <em>31</em>(6), 1–10. (<a
href="https://doi.org/10.1007/s00138-020-01099-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Wildfires are a serious threat to ecosystems and human life. Usually, smoke is generated before the flame, and due to the diffusing nature of the smoke, we can detect smoke from a distance, so wildfire smoke detection is especially important for early warning systems. In this paper, we propose a 3D convolution-based encoder–decoder network architecture for video semantic segmentation in wildfire smoke scenes. In the encoder stage, we use 3D residual blocks to extract the spatiotemporal features of smoke. The downsampling feature from the encoder is upsampled by the decoder three times in succession. Then, three smoke map prediction modules are, respectively, passed, the output smoke prediction map is supervised by the binary image label, and finally, the final prediction is obtained by feature map fusion. Our model can achieve end-to-end training without pretraining from scratch. In addition, a dataset including 90 smoke videos is tested and trained in this paper. The experimental results of the smoke video show that our model quickly and accurately segmented the smoke area and produced few false positives.},
  archive      = {J_MVA},
  author       = {Zhu, Guodong and Chen, Zhenxue and Liu, Chengyun and Rong, Xuewen and He, Weikai},
  doi          = {10.1007/s00138-020-01099-w},
  journal      = {Machine Vision and Applications},
  month        = {9},
  number       = {6},
  pages        = {1-10},
  shortjournal = {Mach. Vis. Appl.},
  title        = {3D video semantic segmentation for wildfire smoke},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). An efficient and globally optimal method for camera pose
estimation using line features. <em>MVA</em>, <em>31</em>(6), 1–11. (<a
href="https://doi.org/10.1007/s00138-020-01100-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The accurate estimation of camera pose using numerous line correspondences in real time is a challenging task. This paper presents a non-iterative approach to solve the Perspective-n-Line (PnL) problem. The method can provide high speed and global optimality, as well as linear complexity. A nonlinear least squares (non-LLS) objective function is first formulated by parameterizing the rotation matrix with Cayley representation. A system of three third-order equations is then derived from its optimality conditions, and then, it is solved directly based on the Gröbner basis technique. Finally, the camera pose can be easily obtained by back-substitution. A major advantage of the proposed method lies in scalability, as the size of the elimination template used in the Gröbner basis technique is independent to the number of line correspondences. Extensive and detailed experiments on synthetic data and real images are conducted, demonstrating that the proposed method achieves an accuracy that is equivalent or superior to the leading methods, but with reduced computational requirements. The source code is available at https://github.com/dannyshin1/danny/tree/master/OPnL1 .},
  archive      = {J_MVA},
  author       = {Yu, Qida and Xu, Guili and Cheng, Yuehua},
  doi          = {10.1007/s00138-020-01100-6},
  journal      = {Machine Vision and Applications},
  month        = {9},
  number       = {6},
  pages        = {1-11},
  shortjournal = {Mach. Vis. Appl.},
  title        = {An efficient and globally optimal method for camera pose estimation using line features},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Deep learning applications in pulmonary medical imaging:
Recent updates and insights on COVID-19. <em>MVA</em>, <em>31</em>(6),
1–42. (<a href="https://doi.org/10.1007/s00138-020-01101-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Shortly after deep learning algorithms were applied to Image Analysis, and more importantly to medical imaging, their applications increased significantly to become a trend. Likewise, deep learning applications (DL) on pulmonary medical images emerged to achieve remarkable advances leading to promising clinical trials. Yet, coronavirus can be the real trigger to open the route for fast integration of DL in hospitals and medical centers. This paper reviews the development of deep learning applications in medical image analysis targeting pulmonary imaging and giving insights of contributions to COVID-19. It covers more than 160 contributions and surveys in this field, all issued between February 2017 and May 2020 inclusively, highlighting various deep learning tasks such as classification, segmentation, and detection, as well as different pulmonary pathologies like airway diseases, lung cancer, COVID-19 and other infections. It summarizes and discusses the current state-of-the-art approaches in this research domain, highlighting the challenges, especially with COVID-19 pandemic current situation.},
  archive      = {J_MVA},
  author       = {Farhat, Hanan and Sakr, George E. and Kilany, Rima},
  doi          = {10.1007/s00138-020-01101-5},
  journal      = {Machine Vision and Applications},
  month        = {9},
  number       = {6},
  pages        = {1-42},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Deep learning applications in pulmonary medical imaging: Recent updates and insights on COVID-19},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). OCLU-NET for occlusal classification of 3D dental models.
<em>MVA</em>, <em>31</em>(6), 1–12. (<a
href="https://doi.org/10.1007/s00138-020-01102-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the emergence in modern dentistry, the study of dental occlusion has been a subject of major interest. The aim of the present study is to investigate the capabilities of deep learning for the classification of dental occlusion using 3D images that has an exciting impact in several fields of dental anatomy. In present work, the 3D stereolithography (STL) files depicting the dental structures are converted to 2D histograms, using Absolute Angle Shape Distribution (AAD) technique, which are used as an input to deep or machine learning models for classification of dental structures based on the similarity of their shape features. To the best of the authors’ knowledge, no solution has been proposed for classification of dental occlusion using deep learning. Thus, an attempt has been made to propose a classification technique for dental occlusion. Based on the experimental analysis, it has been revealed that the deep learning-based convolutional neural network along with AAD performs better as compared to other existing machine learning techniques, with maximum accuracy of 78.95% for occlusion classification. However, the presented study is preliminary, but the experimental outcomes have demonstrated that deep learning is helpful in classifying dental occlusion and it has great application potential in the computer-assisted orthodontic treatment diagnosis.},
  archive      = {J_MVA},
  author       = {Juneja, Mamta and Singla, Ridhima and Saini, Sumindar Kaur and Kaur, Ravinder and Bajaj, Divya and Jindal, Prashant},
  doi          = {10.1007/s00138-020-01102-4},
  journal      = {Machine Vision and Applications},
  month        = {9},
  number       = {6},
  pages        = {1-12},
  shortjournal = {Mach. Vis. Appl.},
  title        = {OCLU-NET for occlusal classification of 3D dental models},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Fast and efficient difference of block means code for
palmprint recognition. <em>MVA</em>, <em>31</em>(6), 1–10. (<a
href="https://doi.org/10.1007/s00138-020-01103-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Over the past two decades, researchers in the field of biometrics have presented a wide variety of coding-based palmprint recognition methods. These approaches mainly rely on extracting the texture features, e.g. line orientations, and phase information, using different filters. In this paper, we propose a new efficient palmprint recognition method based on the Different of Block Means. In the proposed scheme, only basic operations (i.e. mainly additions and subtractions) are used, thus involving a much lower computational cost when compared with existing systems. This makes the system suitable for online palmprint identification and verification. Furthermore, the technique has been shown to deliver superior performance over related systems.},
  archive      = {J_MVA},
  author       = {Almaghtuf, Jumma and Khelifi, Fouad and Bouridane, Ahmed},
  doi          = {10.1007/s00138-020-01103-3},
  journal      = {Machine Vision and Applications},
  month        = {9},
  number       = {6},
  pages        = {1-10},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Fast and efficient difference of block means code for palmprint recognition},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Pedestrian detection using multi-scale
squeeze-and-excitation module. <em>MVA</em>, <em>31</em>(6), 1–9. (<a
href="https://doi.org/10.1007/s00138-020-01105-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Computer vision systems are major research items for autonomous vehicles. However, it is often challenging to understand the road scene, especially when objects are small and overlapping. To address these problems, this paper proposes a deep learning-based pedestrian detection method for small and overlapping objects. The proposed method adopts a parallel feature pyramid network with multi-scale feature layers, and the multi-scale squeeze-and-excitation (MSSE) module is proposed for better selection of multi-scale features. The proposed MSSE module helps to detect small objects by increasing the final feature resolution. In addition, channel-wise feature representation emphasizes important channels with reduced influence of weakly related features. Finally, the object’s proposals are regressed using soft non-maximum suppression to differentiate the overlapped objects. The experiments show significant performance enhancement with the proposed method in an ablation study.},
  archive      = {J_MVA},
  author       = {Lee, Yongwoo and Hwang, Hyekyoung and Shin, Jitae and Oh, Byung Tae},
  doi          = {10.1007/s00138-020-01105-1},
  journal      = {Machine Vision and Applications},
  month        = {9},
  number       = {6},
  pages        = {1-9},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Pedestrian detection using multi-scale squeeze-and-excitation module},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Detection and pose estimation of auto-rickshaws from traffic
images. <em>MVA</em>, <em>31</em>(6), 1–14. (<a
href="https://doi.org/10.1007/s00138-020-01106-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In intelligent transport systems, detection and identification of vehicle types enact a substantial role. In this context, this paper addresses the detection and pose classification of a specific vehicle type: auto-rickshaws which have been heavily neglected by the publicly available vehicle datasets, but remains the most commonly used and cheap form of transportation in south Asian countries. Here, we introduce a dataset for auto-rickshaws which consists of instances of varying shape, orientation, size, scale, colour, viewpoint and many more. Further, we carry out a detailed analysis on the performance of state-of-the-art detection algorithms based on both hand-designed and deep features on the proposed dataset. The introduction of pose classification along with the detection eventually results in better understanding of road scenes involving auto-rickshaws. As a matter of fact, we came up with revisions for the currently employed detection algorithms to achieve a low miss rate on the validation sets. It is evident that the findings of this study are tangible and enormously consequential to the road scene understanding and intelligent transportation of developing countries where auto-rickshaws play a pivotal role in public transportation.},
  archive      = {J_MVA},
  author       = {Bastian, Blossom Treesa and Charangatt Victor, Jiji},
  doi          = {10.1007/s00138-020-01106-0},
  journal      = {Machine Vision and Applications},
  month        = {9},
  number       = {6},
  pages        = {1-14},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Detection and pose estimation of auto-rickshaws from traffic images},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Hyper-parameter optimization of deep learning model for
prediction of parkinson’s disease. <em>MVA</em>, <em>31</em>(5), 1–15.
(<a href="https://doi.org/10.1007/s00138-020-01078-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Neurodegenerative disorder such as Parkinson’s disease (PD) is among the severe health problems in our aging society. It is a neural disorder that affects people socially as well as economically. It occurs due to the failure of the brain’s dopamine-producing cells to produce enough dopamine to enable the motor movement of the body. This disease primarily affects vision, speech, movement problems, and excretion activity, followed by depression, nervousness, sleeping problems, and panic attacks. The onset of Parkinson’s disease is diagnosed with the help of speech disorders, which are the earliest symptoms of it. The essential goal of this paper is to build up a viable clinical decision-making system that helps the doctor in diagnosing the PD influenced patients. In this paper, a specific framework based on grid search optimization is proposed to develop an optimized deep learning Model to predict the early onset of Parkinson’s disease whereby multiple hyperparameters are to be set and tuned for evaluation of the deep learning model. The grid search optimization consists of three main stages, i.e., the optimization of the deep learning model topology, the hyperparameters, and its performance. An evaluation of the proposed approach is done on the speech samples of PD patients and healthy individuals. The results of the approach proposed are finally analyzed, which shows that the fine-tuning of the deep learning model parameters result in the overall test accuracy of 89.23% and the average classification accuracy of 91.69%.},
  archive      = {J_MVA},
  author       = {Kaur, Sukhpal and Aggarwal, Himanshu and Rani, Rinkle},
  doi          = {10.1007/s00138-020-01078-1},
  journal      = {Machine Vision and Applications},
  month        = {7},
  number       = {5},
  pages        = {1-15},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Hyper-parameter optimization of deep learning model for prediction of parkinson’s disease},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). I-ME: Iterative model evolution for learning from weakly
labeled images and videos. <em>MVA</em>, <em>31</em>(5), 1–20. (<a
href="https://doi.org/10.1007/s00138-020-01079-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A significant bottleneck in building large-scale systems for image and video categorization is the requirement of labeled data. Manual labeling effort could be overcome by using the massive amount of web data. However, this type of data is collected through searching on the category names and is likely to inherit noise. In this study, (1) the primary objective is to improve utilizing weakly labeled data without any manual intervention. To this end, (2) we introduce a simple but effective method called “Iterative Model Evolution (I-ME)” where the goal is to discover representative instances by eliminating the irrelevant items so that the purified set can be directly used in training a model. In I-ME, (3) the elimination is done by leveraging the scores of two logistic regressors where the models are learned through iterations. We first apply our method for (4) recognizing complex human activities in images and videos and then a large-scale noisy web dataset, Clothing1M. (5) Our results are comparable to or better than the presented baselines on benchmark video datasets UCF-101, ActivityNet, FCVID and image dataset Action40. Through purifying with I-ME, we come up with only 40% of the noisy Clothing1M and we train the DNN with less but more representative training data without changing the network structure. (6) The success of I-ME over utilizing deep features supports that there is still room for improvement in exploiting large-scale weakly labeled data through mining to discover a smaller but more distinctive subset without increasing the complexity of the process.},
  archive      = {J_MVA},
  author       = {Yalcinkaya, Ozge and Golge, Eren and Duygulu, Pinar},
  doi          = {10.1007/s00138-020-01079-0},
  journal      = {Machine Vision and Applications},
  month        = {7},
  number       = {5},
  pages        = {1-20},
  shortjournal = {Mach. Vis. Appl.},
  title        = {I-ME: Iterative model evolution for learning from weakly labeled images and videos},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Detection of tomato organs based on convolutional neural
network under the overlap and occlusion backgrounds. <em>MVA</em>,
<em>31</em>(5), 1–13. (<a
href="https://doi.org/10.1007/s00138-020-01081-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traditional detection methods are not sensitive to small-sized tomato organs (flowers and fruits), because the immature green tomatoes are highly similar to the background color. The overlap among fruits and the occlusion of stems and leaves on tomato organs can lead to false and missing detection, which decreases the accuracy and generalization ability of the model. Therefore, a tomato organ recognition method based on improved Feature Pyramid Network was proposed in this paper. To begin with, multi-scale feature fusion was used to fuse the detailed bottom features and high-level semantic features to detect small-sized tomato organs to improve recognition rate. And then repulsion loss was used to take place of the original smooth L1 loss function. Besides, Soft-NMS (Soft non-maximum suppression) was adopted to replace non-maximum suppression to screen the bounding boxes of tomato organs to construct a recognition model of tomato key organ. Finally, the network was trained and verified on the collected image data set. The results showed that compared with the traditional Faster R-CNN model, the performance was greatly improved (mean average precision was improved from 90.7 to 99.5%). Subsequently, the training model can be compressed so that it can be embedded into the microcontroller to develop further precise pesticide targeting application system of tomato organs and the automatic picking device.},
  archive      = {J_MVA},
  author       = {Sun, Jun and He, Xiaofei and Wu, Minmin and Wu, Xiaohong and Shen, Jifeng and Lu, Bing},
  doi          = {10.1007/s00138-020-01081-6},
  journal      = {Machine Vision and Applications},
  month        = {7},
  number       = {5},
  pages        = {1-13},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Detection of tomato organs based on convolutional neural network under the overlap and occlusion backgrounds},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Photo-realistic dehazing via contextual generative
adversarial networks. <em>MVA</em>, <em>31</em>(5), 1–12. (<a
href="https://doi.org/10.1007/s00138-020-01082-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Single image dehazing is a challenging task due to its ambiguous nature. In this paper we present a new model based on generative adversarial networks (GANs) for single image dehazing, called as dehazing GAN. In contrast to estimating the transmission map and the atmospheric light separately as most existing deep learning methods, dehazing GAN restores the corresponding hazy-free image directly from a hazy image via a generative adversarial network. Extensive experimental results on both synthetic dataset and real-world images show our model outperforms the state-of-the-art algorithms.},
  archive      = {J_MVA},
  author       = {Zhang, Shengdong and He, Fazhi and Ren, Wenqi},
  doi          = {10.1007/s00138-020-01082-5},
  journal      = {Machine Vision and Applications},
  month        = {7},
  number       = {5},
  pages        = {1-12},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Photo-realistic dehazing via contextual generative adversarial networks},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Analysis of microtomographic images in automatic defect
localization and detection. <em>MVA</em>, <em>31</em>(5), 1–19. (<a
href="https://doi.org/10.1007/s00138-020-01084-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The paper presents a fast method of fully automatic localization and classification of defects in aluminium castings based on computed microtomography images. In the light of current research and based on available publications, where such analysis is made on the basis of images obtained from standard radiography (x-ray), this is a new approach which uses microtomographic images ($$\mu $$-CT). In addition, the above-mentioned solutions most often analyze a pre-separated portion of an image, which requires the initial operator interference. The authors’ own pre-processing methods, which allow to separate the element area and potential defect areas from $$\mu $$-CT images, and methods of extraction of selected features describing these areas have been proposed in the solution discussed here. A neural network trained using the Levenberg–Marquardt method with error backpropagation has been used as a classifier. The optimal network structure 20–4–1 and a set of 20 features describing the analysed areas have been determined as a result of performed tests. The applied solutions have provided 89% correct detection for any defect size and 96.73% for large defects, which is comparable to the results obtained from methods using x-ray images. This has confirmed that it is possible to use $$\mu $$-CT images in automatic defect localization in 3D. Thanks to this method, quantitative analysis of aluminium castings can be carried out without user interaction and fully automated.},
  archive      = {J_MVA},
  author       = {Marzec, Mariusz and Duda, Piotr and Wróbel, Zygmunt},
  doi          = {10.1007/s00138-020-01084-3},
  journal      = {Machine Vision and Applications},
  month        = {7},
  number       = {5},
  pages        = {1-19},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Analysis of microtomographic images in automatic defect localization and detection},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). DC-gnet for detection of glaucoma in retinal fundus imaging.
<em>MVA</em>, <em>31</em>(5), 1–14. (<a
href="https://doi.org/10.1007/s00138-020-01085-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Glaucoma is a retinal disease caused due to increased intraocular pressure in the eyes. It is the second most dominant cause of irreversible blindness after cataract, and if this remains undiagnosed, it may become the first common cause. Ophthalmologists use different comprehensive retinal examinations such as ophthalmoscopy, tonometry, perimetry, gonioscopy and pachymetry to diagnose glaucoma. But all these approaches are manual and time-consuming. Thus, a computer-aided diagnosis system may aid as an assistive measure for the initial screening of glaucoma for diagnosis purposes, thereby reducing the computational complexity. This paper presents a deep learning-based disc cup segmentation glaucoma network (DC-Gnet) for the extraction of structural features namely cup-to-disc ratio, disc damage likelihood scale and inferior superior nasal temporal regions for diagnosis of glaucoma. The proposed approach of segmentation has been tested on RIM-One and Drishti-GS dataset. Further, based on experimental analysis, the DC-Gnet is found to outperform U-net, Gnet and Deep-lab architectures.},
  archive      = {J_MVA},
  author       = {Juneja, Mamta and Thakur, Sarthak and Wani, Anuj and Uniyal, Archit and Thakur, Niharika and Jindal, Prashant},
  doi          = {10.1007/s00138-020-01085-2},
  journal      = {Machine Vision and Applications},
  month        = {7},
  number       = {5},
  pages        = {1-14},
  shortjournal = {Mach. Vis. Appl.},
  title        = {DC-gnet for detection of glaucoma in retinal fundus imaging},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Two-stream FCNs to balance content and style for style
transfer. <em>MVA</em>, <em>31</em>(5), 1–18. (<a
href="https://doi.org/10.1007/s00138-020-01086-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Style transfer is to render given image contents in given styles, and it has an important role in both computer vision fundamental research and industrial applications. Following the success of deep learning-based approaches, this problem has been re-launched recently, but still remains a difficult task because of trade-off between preserving contents and faithful rendering of styles. Indeed, how well-balanced content and style are is crucial in evaluating the quality of stylized images. In this paper, we propose an end-to-end two-stream fully convolutional networks (FCNs) aiming at balancing the contributions of the content and the style in rendered images. Our proposed network consists of the encoder and decoder parts. The encoder part utilizes a FCN for content and a FCN for style where the two FCNs have feature injections and are independently trained to preserve the semantic content and to learn the faithful style representation in each. The semantic content feature and the style representation feature are then concatenated adaptively and fed into the decoder to generate style-transferred (stylized) images. In order to train our proposed network, we employ a loss network, the pre-trained VGG-16, to compute content loss and style loss, both of which are efficiently used for the feature injection as well as the feature concatenation. Our intensive experiments show that our proposed model generates more balanced stylized images in content and style than state-of-the-art methods. Moreover, our proposed network achieves efficiency in speed.},
  archive      = {J_MVA},
  author       = {Vo, Duc Minh and Sugimoto, Akihiro},
  doi          = {10.1007/s00138-020-01086-1},
  journal      = {Machine Vision and Applications},
  month        = {7},
  number       = {5},
  pages        = {1-18},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Two-stream FCNs to balance content and style for style transfer},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Using CNN with bayesian optimization to identify cerebral
micro-bleeds. <em>MVA</em>, <em>31</em>(5), 1–14. (<a
href="https://doi.org/10.1007/s00138-020-01087-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article studies the problem of detecting cerebral micro-bleeds (CMBs) using a convolutional neural network (CNN). Cerebral micro-bleeds (CMBs) are increasingly recognized neuroimaging findings, occurring with cerebrovascular diseases, dementia, and normal aging. Naturally enough, it becomes necessary to detect CMBs in the early stages of life. The focus of this article is to infuse new techniques like Bayesian optimization to find the optimum set of hyper-parameters efficiently, making even the simplest of CNN architectures perform well on the problem. Experimentally, we observe our CNN (five layers, i.e., two convolution, two pooling, and one fully connected) achieves accuracy = 98.97%, sensitivity = 99.66%, specificity = 98.14%, and precision = 98.54% on the test set (hold-out validation) when calculated over an average of ten runs. The proposed model outperformed state-of-the-art methods.},
  archive      = {J_MVA},
  author       = {Doke, Piyush and Shrivastava, Dhiraj and Pan, Chichun and Zhou, Qinghua and Zhang, Yu-Dong},
  doi          = {10.1007/s00138-020-01087-0},
  journal      = {Machine Vision and Applications},
  month        = {7},
  number       = {5},
  pages        = {1-14},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Using CNN with bayesian optimization to identify cerebral micro-bleeds},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). GC-NET for classification of glaucoma in the retinal fundus
image. <em>MVA</em>, <em>31</em>(5), 1–18. (<a
href="https://doi.org/10.1007/s00138-020-01091-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Glaucoma is the second-most dominant cause for irreversible blindness, resulting in damage to the optic nerve. Ophthalmologist diagnoses this disease using a retinal examination of the dilated pupil. Since the diagnosis is a manual and laborious procedure, an automated approach for faster diagnosis is desirable. Convolutional neural networks (CNN) could allow automation of the diagnosis procedure due to their self-learning capabilities. This paper presents a deep learning-based glaucoma classification network (GC-NET) for classifying a retinal image as glaucomatous or non-glaucomatous. The proposed GC-NET has been tested on RIM-One and Drishti datasets. Our experimental results showed that GC-NET achieves accuracy of 97.51%, sensitivity of 98.78% and specificity of 96.20% with 0.81 true positive (Tp), 0.03 false positive (Fp), 0.76 true negative (Tn) and 0.01 false negative (Fn) which outperforms state of the art. Thus, the proposed approach could be very useful for initial screening of glaucoma patients.},
  archive      = {J_MVA},
  author       = {Juneja, Mamta and Thakur, Niharika and Thakur, Sarthak and Uniyal, Archit and Wani, Anuj and Jindal, Prashant},
  doi          = {10.1007/s00138-020-01091-4},
  journal      = {Machine Vision and Applications},
  month        = {7},
  number       = {5},
  pages        = {1-18},
  shortjournal = {Mach. Vis. Appl.},
  title        = {GC-NET for classification of glaucoma in the retinal fundus image},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Graph-based topic models for trajectory clustering in crowd
videos. <em>MVA</em>, <em>31</em>(5), 1–13. (<a
href="https://doi.org/10.1007/s00138-020-01092-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Probabilistic topic modelings, such as latent Dirichlet allocation (LDA) and correlated topic models (CTM), have recently emerged as powerful statistical tools for processing video content. They share an important property, i.e., using a common set of topics to model all data. However, such property can be too restrictive for modeling complex visual data such as crowd scenes where multiple fields of heterogeneous data jointly provide rich information about objects and events. This paper proposes graph-based extensions of LDA and CTM, referred to as GLDA and GCTM, to learn and analyze motion patterns by trajectory clustering in a highly cluttered and crowded environment. Unlike previous works that relied on a scene prior, we apply a spatio-temporal graph to uncover the spatial and temporal coherence between the trajectories of crowd motion during the learning process. The presented models advance the conventional approaches by integrating a manifold-based clustering as initialization and iterative statistical inference as optimization. The output of GLDA and GCTM are mid-level features that represent the motion patterns used later to generate trajectory clusters. Experiments on three different datasets show the effectiveness of the approaches in trajectory clustering and crowd motion modeling.},
  archive      = {J_MVA},
  author       = {Al Ghamdi, Manal and Gotoh, Yoshihiko},
  doi          = {10.1007/s00138-020-01092-3},
  journal      = {Machine Vision and Applications},
  month        = {7},
  number       = {5},
  pages        = {1-13},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Graph-based topic models for trajectory clustering in crowd videos},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Robust face tracking using multiple appearance models and
graph relational learning. <em>MVA</em>, <em>31</em>(4), 1–17. (<a
href="https://doi.org/10.1007/s00138-020-01071-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper addresses the problem of appearance matching across different challenges while doing visual face tracking in real-world scenarios. In this paper, FaceTrack is proposed that utilizes multiple appearance models with its long-term and short-term appearance memory for efficient face tracking. It demonstrates robustness to deformation, in-plane and out-of-plane rotation, scale, distractors and background clutter. It integrates on the advantages of the tracking-by-detection by using a face detector that tackles the drastic scale appearance change of a face. A weighted score-level fusion strategy is proposed to obtain the face tracking output having the highest fusion score by generating candidates around possible face locations. FaceTrack showcases impressive performance when initiated automatically by outperforming several state-of-the-art trackers, except Struck by a very minute margin: 0.001 in precision and 0.017 in success, respectively.},
  archive      = {J_MVA},
  author       = {Chakravorty, Tanushri and Bilodeau, Guillaume-Alexandre and Granger, Éric},
  doi          = {10.1007/s00138-020-01071-8},
  journal      = {Machine Vision and Applications},
  month        = {5},
  number       = {4},
  pages        = {1-17},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Robust face tracking using multiple appearance models and graph relational learning},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Coupling cell detection and tracking by temporal feedback.
<em>MVA</em>, <em>31</em>(4), 1–18. (<a
href="https://doi.org/10.1007/s00138-020-01072-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The tracking-by-detection strategy is the backbone of many methods for tracking living cells in time-lapse microscopy. An object detector is first applied to the input images, and the resulting detection candidates are then linked by a data association module. The performance of such methods strongly depends on the quality of the detector because detection errors propagate to the linking step. To tackle this issue, we propose a joint model for segmentation, detection and tracking. The model is defined implicitly as limiting distribution of a Markov chain Monte Carlo algorithm and contains a temporal feedback, which allows to dynamically alter detector parameters using hints given by neighboring frames and, in this way, correct detection errors. The proposed method can integrate any detector and is therefore not restricted to a specific domain. The parameters of the model are learned using an objective based on empirical risk minimization. We use our method to conduct large-scale experiments for confluent cultures of endothelial cells and evaluate its performance in the ISBI Cell Tracking Challenge, where it consistently scored among the best three methods.},
  archive      = {J_MVA},
  author       = {Sixta, Tomáš and Cao, Jiahui and Seebach, Jochen and Schnittler, Hans and Flach, Boris},
  doi          = {10.1007/s00138-020-01072-7},
  journal      = {Machine Vision and Applications},
  month        = {5},
  number       = {4},
  pages        = {1-18},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Coupling cell detection and tracking by temporal feedback},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Improved face super-resolution generative adversarial
networks. <em>MVA</em>, <em>31</em>(4), 1–12. (<a
href="https://doi.org/10.1007/s00138-020-01073-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The face super-resolution method is used for generating high-resolution images from low-resolution ones for better visualization. The super-resolution generative adversarial network (SRGAN) can generate a single super-resolution image with realistic textures, which is a groundbreaking work. Based on SRGAN, we proposed improved face super-resolution generative adversarial networks. The super-resolution image details generated by SRGAN usually have undesirable artifacts. To further improve visual quality, we delve into the key components of the SRGAN network architecture and improve each part to achieve a more powerful SRGAN. First, the SRGAN employs residual blocks as the core of the very deep generator network G. In this paper, we decide to employ dense convolutional network blocks (dense blocks), which connect each layer to every other layer in a feed-forward fashion as our very deep generator networks. Moreover, in the past few years, generative adversarial networks (GANs) have been applied to solve various problems. Despite its superior performance, it is difficult to train. A simple and effective regularization method called spectral normalization GAN is used to solve this problem. We have experimentally confirmed that our proposed method is superior to the other existing method in training stability and visual improvements.},
  archive      = {J_MVA},
  author       = {Wang, Mengxue and Chen, Zhenxue and Wu, Q. M. Jonathan and Jian, Muwei},
  doi          = {10.1007/s00138-020-01073-6},
  journal      = {Machine Vision and Applications},
  month        = {5},
  number       = {4},
  pages        = {1-12},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Improved face super-resolution generative adversarial networks},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Utilization of a convolutional method for alzheimer disease
diagnosis. <em>MVA</em>, <em>31</em>(4), 1–19. (<a
href="https://doi.org/10.1007/s00138-020-01074-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the increasing number of cases as well as care costs, Alzheimer’s disease has gained more interest in several scientific communities especially medical and computer science. Clinical and analytical tests are widely accepted techniques for detecting Alzheimer cases. However, early detection can help prevent damage to brain tissue and heal it with proper treatment. Interpreting brain images is considered as a time-consuming task with a high error-prone. Recently, advanced machine learning methods have successfully proved high performance in various fields including brain image analysis. These existing techniques, which become more used for clinical disease detection, present challenging wrongness sensibility to detect aberrant values or areas in the human brain. We conducted our work to automate the detection of the damaged areas and diagnose Alzheimer’s disease. Our method can segment MRI images, identify brain lesions and the different stages of Alzheimer’s disease. We evaluated our method using ample cases form public databases to demonstrate that our proposition performed reliable and effective results. Our proposal achieved an accuracy of 94.73%, a recall rate of 93.82%, and an F1-score of 92.8%. Also, the detection precision reached 91.76% with a sensitivity of 92.48% and a specificity rate of 90.64%. Our method creates an important way to optimize the imaging process via an automated computer-assisted diagnosis using potential deep learning methods to increase the consistency and accuracy of Alzheimer’s disease diagnosis worldwide.},
  archive      = {J_MVA},
  author       = {Allioui, Hanane and Sadgal, Mohamed and Elfazziki, Aziz},
  doi          = {10.1007/s00138-020-01074-5},
  journal      = {Machine Vision and Applications},
  month        = {5},
  number       = {4},
  pages        = {1-19},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Utilization of a convolutional method for alzheimer disease diagnosis},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Multi-scale crowd feature detection using vision sensing and
statistical mechanics principles. <em>MVA</em>, <em>31</em>(4), 1–16.
(<a href="https://doi.org/10.1007/s00138-020-01075-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Crowd behaviour analysis using vision has been subject to many different approaches. Multi-purpose crowd descriptors are one of the more recent approaches. These descriptors provide an opportunity to compare and categorize various types of crowds as well as classify their respective behaviours. Nevertheless, the automated calculation of descriptors which are expressed as measurements with accurate interpretation is a challenging problem. In this paper, analogies between human crowds and molecular thermodynamics systems are drawn for the measurement of crowd behaviour. Specifically, a novel descriptor is defined and measured for crowd behaviour at multiple scales. This descriptor uses the concept of Entropy for evaluating the state of crowd disorder. By results, the descriptor Entropy does indeed appear to capture the desired outcome for crowd entropy while utilizing easily detectable image features. Our new approach for machine understanding of crowd behaviour is promising, while it offers new complementary capabilities to the existing crowd descriptors, for example, as will be demonstrated, in the case of spectator crowds. The scope and performance of this descriptor are further discussed in detail in this paper.},
  archive      = {J_MVA},
  author       = {Arbab-Zavar, Banafshe and Sabeur, Zoheir A.},
  doi          = {10.1007/s00138-020-01075-4},
  journal      = {Machine Vision and Applications},
  month        = {5},
  number       = {4},
  pages        = {1-16},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Multi-scale crowd feature detection using vision sensing and statistical mechanics principles},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Parameter selection framework for stereo correspondence.
<em>MVA</em>, <em>31</em>(4), 1–15. (<a
href="https://doi.org/10.1007/s00138-020-01076-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose a method to select parameter values for stereo matching methods. The proposed method was trained in a supervised manner, and an evolutionary algorithm is used to select optimized parameter values for a given domain and a cost function constructed to measure the goodness level of candidate parameter values. Performance of the proposed method is compared to that of five current stereo matching methods, including the efficient large-scale stereo matching, belief propagation, semi-global block matching, stereo matching by training a convolutional neural network to compare image patches, and the efficient deep learning for stereo matching, for KITTI 2012, KITTI 2015, Middlebury, and EISAT datasets. The optimized parameters improve accuracy for all stereo matching methods considered, with some cases of improvement reaching up to $$24\%$$. Source code and experimental results are available online.},
  archive      = {J_MVA},
  author       = {Nguyen, Phuc Hong and Ahn, Chang Wook},
  doi          = {10.1007/s00138-020-01076-3},
  journal      = {Machine Vision and Applications},
  month        = {5},
  number       = {4},
  pages        = {1-15},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Parameter selection framework for stereo correspondence},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). On evolutionary computation techniques for multi-view
triangulation. <em>MVA</em>, <em>31</em>(4), 1–13. (<a
href="https://doi.org/10.1007/s00138-020-01077-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-view triangulation is an essential step in recovering three-dimensional structure from a set of images. It is a well-studied problem in computer vision with many suboptimal and optimal methods based on different optimality criteria. In this paper, we assess the ability of evolutionary computation (EC) methods in finding highly accurate solutions to this problem. We use an overlaying Luus–Jaakola optimizer to find good parameter configurations and determine appropriate computational budget for the EC methods. Empirical results on synthetic and real data demonstrate the superior performance of EC methods over existing triangulation methods.},
  archive      = {J_MVA},
  author       = {Nair, Nirmal S. and Nair, Madhu S.},
  doi          = {10.1007/s00138-020-01077-2},
  journal      = {Machine Vision and Applications},
  month        = {5},
  number       = {4},
  pages        = {1-13},
  shortjournal = {Mach. Vis. Appl.},
  title        = {On evolutionary computation techniques for multi-view triangulation},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Automated facial video-based recognition of depression and
anxiety symptom severity: Cross-corpus validation. <em>MVA</em>,
<em>31</em>(4), 1–19. (<a
href="https://doi.org/10.1007/s00138-020-01080-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {There is a growing interest in computational approaches permitting accurate detection of nonverbal signs of depression and related symptoms (i.e., anxiety and distress) that may serve as minimally intrusive means of monitoring illness progression. The aim of the present work was to develop a methodology for detecting such signs and to evaluate its generalizability and clinical specificity for detecting signs of depression and anxiety. Our approach focused on dynamic descriptors of facial expressions, employing motion history image, combined with appearance-based feature extraction algorithms (local binary patterns, histogram of oriented gradients), and visual geometry group features derived using deep learning networks through transfer learning. The relative performance of various alternative feature description and extraction techniques was first evaluated on a novel dataset comprising patients with a clinical diagnosis of depression ($$n=20$$) and healthy volunteers ($$n=45$$). Among various schemes involving depression measures as outcomes, best performance was obtained for continuous assessment of depression severity (as opposed to binary classification of patients and healthy volunteers). Comparable performance was achieved on a benchmark dataset, the audio/visual emotion challenge (AVEC’14). Regarding clinical specificity, results indicated that the proposed methodology was more accurate in detecting visual signs associated with self-reported anxiety symptoms. Findings are discussed in relation to clinical and technical limitations and future improvements.},
  archive      = {J_MVA},
  author       = {Pampouchidou, A. and Pediaditis, M. and Kazantzaki, E. and Sfakianakis, S. and Apostolaki, I. A. and Argyraki, K. and Manousos, D. and Meriaudeau, F. and Marias, K. and Yang, F. and Tsiknakis, M. and Basta, M. and Vgontzas, A. N. and Simos, P.},
  doi          = {10.1007/s00138-020-01080-7},
  journal      = {Machine Vision and Applications},
  month        = {5},
  number       = {4},
  pages        = {1-19},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Automated facial video-based recognition of depression and anxiety symptom severity: Cross-corpus validation},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A morphological approach to piecewise constant active
contour model incorporated with the geodesic edge term. <em>MVA</em>,
<em>31</em>(4), 1–25. (<a
href="https://doi.org/10.1007/s00138-020-01083-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traditional level set-based image segmentation method has to solve the level set evolution equation which is the Euler–Lagrange equation of the energy functional defined on the image domain. Solving level set evolution equation is very time-consuming, and reinitialization is usually needed. The level set evolution equation can also be solved by mathematical morphology. The morphological implementation is very simple, fast and stable. The piecewise constant active contour model incorporated with the geodesic edge term is a hybrid active contour model which combines two active contour models which are active contour model without edges and geodesic active contour model. In this paper, the mathematical morphology-based level set evolution method is applied to the piecewise constant active contour model incorporated with the geodesic edge term. The curvature morphological operator is also improved. Experimental results show that, compared with the original piecewise constant active contour model incorporated the geodesic edge term, the new mathematical morphology-based model can segment images more accurately and there are significant gains in simplicity, speed and stability. The new mathematical morphology-based model is also compared to morphological piecewise constant active contour model, morphological geodesic active contour model, traditional piecewise constant active contour model and two other active contour models. Results show that the proposed method gets the segmentation result with faster speed and higher accuracy.},
  archive      = {J_MVA},
  author       = {Yu, Song and Yiquan, Wu},
  doi          = {10.1007/s00138-020-01083-4},
  journal      = {Machine Vision and Applications},
  month        = {5},
  number       = {4},
  pages        = {1-25},
  shortjournal = {Mach. Vis. Appl.},
  title        = {A morphological approach to piecewise constant active contour model incorporated with the geodesic edge term},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Special issue on advanced machine vision. <em>MVA</em>,
<em>31</em>(3), 1–3. (<a
href="https://doi.org/10.1007/s00138-020-01061-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This preface acts as an introduction to the special issue on Advanced Machine Vision. It highlights the goal of this special issue on Advanced Machine Vision as well as discusses the reviewing process. On top of that, it highlights the selected submissions and describes them briefly.},
  archive      = {J_MVA},
  author       = {Puttemans, Steven and Goedemé, Toon and Mian, Ajmal and Moeslund, Thomas B. and Gade, Rikke},
  doi          = {10.1007/s00138-020-01061-w},
  journal      = {Machine Vision and Applications},
  month        = {3},
  number       = {3},
  pages        = {1-3},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Special issue on advanced machine vision},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). ColpoNet for automated cervical cancer screening using
colposcopy images. <em>MVA</em>, <em>31</em>(3), 1–15. (<a
href="https://doi.org/10.1007/s00138-020-01063-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cervical cancer is one among the trivial forms of cancer that counts for 6.6% of all females cancers with an estimated 570,000 new cases in 2018. The mortality rate due to cervical cancer is approximately 90% in low or middle income countries due to lack of suitable pre-screening procedures and experienced medical staff. Colposcopy images or cervigrams, are the images that capture the cervical region, are considered as the gold standard by the medical experts for the identification and evaluation of cervical cancer. The visual assessment of cervigrams for recognizing cancer suffers from high inter- or intra-variations especially among less or unskilled medical experts. However, this method is dependent on colposcopists’ observation and it is more time consuming, tedious and laborious task which calls for development of computer-aided method for diagnosis of cervical cancer. With the technological advancements, deep learning has been commonly employed for providing automated solutions for disease diagnosis due to its self-learning capability. This paper presents a deep-learning-based method for cervix cancer classification using colposcopy images. The architecture of the proposed method namely, ColpoNet, has been motivated by the DenseNet model because it is computationally more efficient as compared to other models. Further, the method has been tested and validated on the dataset released by the National Cancer Institute and it has been compared with other deep-learning models namely AlexNet, VGG16, ResNet50, LeNet and GoogleNet to check scope of its applicability. The experimental analysis revealed that ColpoNet achieved an accuracy of 81.353% and shows the highest performance rate as compared to other state-of-the-art deep techniques. Such classification system can be deployed in clinics to enhance the early detection of cervical cancer in less developed countries.},
  archive      = {J_MVA},
  author       = {Saini, Sumindar Kaur and Bansal, Vasudha and Kaur, Ravinder and Juneja, Mamta},
  doi          = {10.1007/s00138-020-01063-8},
  journal      = {Machine Vision and Applications},
  month        = {3},
  number       = {3},
  pages        = {1-15},
  shortjournal = {Mach. Vis. Appl.},
  title        = {ColpoNet for automated cervical cancer screening using colposcopy images},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Real-time camera pose estimation for sports fields.
<em>MVA</em>, <em>31</em>(3), 1–13. (<a
href="https://doi.org/10.1007/s00138-020-01064-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Given an image sequence featuring a portion of a sports field filmed by a moving and uncalibrated camera, such as the one of the smartphones, our goal is to compute automatically in real time the focal length and extrinsic camera parameters for each image in the sequence without using a priori knowledges of the position and orientation of the camera.To this end, we propose a novel framework that combines accurate localization and robust identification of specific keypoints in the image by using a fully convolutional deep architecture.Our algorithm exploits both the field lines and the players’ image locations, assuming their ground plane positions to be given, to achieve accuracy and robustness that is beyond the current state of the art.We will demonstrate its effectiveness on challenging soccer, basketball, and volleyball benchmark datasets.},
  archive      = {J_MVA},
  author       = {Citraro, Leonardo and Márquez-Neila, Pablo and Savarè, Stefano and Jayaram, Vivek and Dubout, Charles and Renaut, Félix and Hasfura, Andrés and Ben Shitrit, Horesh and Fua, Pascal},
  doi          = {10.1007/s00138-020-01064-7},
  journal      = {Machine Vision and Applications},
  month        = {3},
  number       = {3},
  pages        = {1-13},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Real-time camera pose estimation for sports fields},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). User-interactive salient object detection using YOLOv2, lazy
snapping, and gabor filters. <em>MVA</em>, <em>31</em>(3), 1–7. (<a
href="https://doi.org/10.1007/s00138-020-01065-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Salient object detection is the process of locating prominent objects in an image. In this field, deep learning methods are providing outstanding results. One way of finding salient objects is to first obtain a bounding box for the prominent object in the image and then use the bounding box to form the actual shape of the salient object. In this work, we find an object bounding box using YOLOv2 network. Next, we apply boundary correction to the bounding box predicted by the deep network. In the third step, we segment the image using a set of Gabor filters. Then, we select the matching segment from the first-level boundary correction. On the matching segment, we apply second-level boundary correction. Usually, in salient object detection, the end-user plays no role in selecting the salient object. In this work, we provide the user with a choice to improvise on the salient object detected at the first level. If the user is not satisfied with first-level boundary correction, he/she can choose for second-level boundary correction. The method provides a benefit over the existing methods as most of the saliency map results are static, and pure deep learning methods have blurred edges. By using this procedure, neat object edges are obtained. The algorithm is tested on three datasets against four state-of-the-art methods. The algorithm is evaluated based on F-measure. The proposed model achieves 0.86, 0.7904, and 0.745 F-measure for ASD, ECSSD, and PASCAL-S dataset, respectively.},
  archive      = {J_MVA},
  author       = {Srivastava, Gargi and Srivastava, Rajeev},
  doi          = {10.1007/s00138-020-01065-6},
  journal      = {Machine Vision and Applications},
  month        = {3},
  number       = {3},
  pages        = {1-7},
  shortjournal = {Mach. Vis. Appl.},
  title        = {User-interactive salient object detection using YOLOv2, lazy snapping, and gabor filters},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Singular value decomposition-based virtual representation
for face recognition. <em>MVA</em>, <em>31</em>(3), 1–9. (<a
href="https://doi.org/10.1007/s00138-020-01067-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the field of face recognition, a key issue is whether there are a sufficient number of face training samples with valid information. Due to the complexity of human face images, face recognition is easy to be affected by the external environment such as light intensity, gesture expression, hairstyle, and occlusion. Therefore, it is difficult to obtain enough effective samples in practical applications. In this paper, we propose a new algorithm that generates virtual images by utilizing the information of the test sample via singular value decomposition. The virtual images not only extend the training sample set but also can better adapt to the test sample. In addition, we use the weighted score fusion scheme to calculate the ultimate result, which can better take advantages of data from different sources including original images and virtual images. Experimental results on the Extended Yale_B, AR, GT, ORL, and FERET face databases prove that our algorithm can obtain satisfactory performance.},
  archive      = {J_MVA},
  author       = {Liu, Shigang and Wang, Yuhong and Peng, Yali and Hou, Sujuan and Zhang, Keyou and Wu, Xiaojun},
  doi          = {10.1007/s00138-020-01067-4},
  journal      = {Machine Vision and Applications},
  month        = {3},
  number       = {3},
  pages        = {1-9},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Singular value decomposition-based virtual representation for face recognition},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Object detection based on semi-supervised domain adaptation
for imbalanced domain resources. <em>MVA</em>, <em>31</em>(3), 1–18. (<a
href="https://doi.org/10.1007/s00138-020-01068-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {On specified scenarios, models trained on specific datasets (source domain) can generalize well to novel scenes (target domain) via knowledge transfer. However, these source detectors might not be perfectly aligned with a low target resource due to the imbalanced and inconsistent domain shift involved. In this paper, we propose a semi-supervised detector that adapts the domain shifts on both appearance and semantic levels. Based on this, two components are introduced as appearance adaptation networks with instance and batch normalization, and semantic adaptation networks where an adversarial transferring procedure is embedded by re-weighting the discriminator loss to improve the feature alignments between the two domains with imbalanced scales. Furthermore, a self-paced training procedure is performed to re-train the detector by alternately generating pseudo-labels in the target domain from easy to hard. In our experiments, an empirical analysis of the proposed framework is conducted by evaluating performance in various datasets such as Cityscapes and VOC0712, and the results verify the higher accuracy and effectiveness of the proposed detector in comparison with state-of-the-art detectors.},
  archive      = {J_MVA},
  author       = {Li, Wei and Wang, Meng and Wang, Hongbin and Zhang, Yafei},
  doi          = {10.1007/s00138-020-01068-3},
  journal      = {Machine Vision and Applications},
  month        = {3},
  number       = {3},
  pages        = {1-18},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Object detection based on semi-supervised domain adaptation for imbalanced domain resources},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Deep convolutional neural networks with transfer learning
for automated brain image classification. <em>MVA</em>, <em>31</em>(3),
1–16. (<a href="https://doi.org/10.1007/s00138-020-01069-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {MR brain image categorization has been an active research domain from the last decade. Several techniques have been devised in the past for MR image categorization, starting from classical to the deep learning methods like convolutional neural networks (CNNs). Classical machine learning methods need handcrafted features to perform classification. The CNNs, on the other hand, perform classification by extracting image features directly from raw images via tuning the parameters of the convolutional and pooling layer. The features extracted by CNN strongly depend on the size of the training dataset. If the training dataset is small, CNN tends to overfit after several epochs. So, deep CNNs (DCNNs) with transfer learning have evolved. The prime objective of the present work is to explore the capability of different pre-trained DCNN models with transfer learning for pathological brain image classification. Various pre-trained DCNNs, namely Alexnet, Resnet50, GoogLeNet, VGG-16, Resnet101, VGG-19, Inceptionv3, and InceptionResNetV2, were used in the present study. The last few layers of these models were replaced to accommodate new image categories for our application. These models were extensively evaluated on data from Harvard, clinical, and benchmark Figshare repository. The dataset was then partitioned in the ratio 60:40 for training and testing. The validation on the test set reveals that the pre-trained Alexnet with transfer learning exhibited the best performance in less time compared to other proposed models. The proposed method is more generic as it does not need any handcrafted features and can achieve an accuracy value of 100%, 94%, and 95.92% for three datasets. Other performance measures used in the study include sensitivity, specificity, precision, false positive rate, error, F-score, Mathew correlation coefficient, and area under the curve. The results are compared with both the traditional machine learning methods and those using CNN.},
  archive      = {J_MVA},
  author       = {Kaur, Taranjit and Gandhi, Tapan Kumar},
  doi          = {10.1007/s00138-020-01069-2},
  journal      = {Machine Vision and Applications},
  month        = {3},
  number       = {3},
  pages        = {1-16},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Deep convolutional neural networks with transfer learning for automated brain image classification},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Gradient self-weighting linear collaborative discriminant
regression classification for human cognitive states classification.
<em>MVA</em>, <em>31</em>(3), 1–16. (<a
href="https://doi.org/10.1007/s00138-020-01070-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent decades, huge volumes of data are available to inspect human brain activities for disease detection. Specifically, the functional magnetic resonance imaging (fMRI) is a powerful tool to enquire the brain functions. In fMRI, identifying the active patterns of the specific cognitive state is one of the emerging concerns for neuroscientists. The high-dimensional features make fMRI data difficult for mining and classification, because if the volume of the data space increases, then the acquired data become sparse, which leads to the “curse of dimensionality” problem. To address this concern, a new feature selection and classification methodology was proposed for classifying the human cognitive states from fMRI data. Initially, the fMRI data were collected from the StarPlus and Haxby datasets. Then, k-nearest neighbors algorithm (k-NN)-based genetic algorithm was developed to choose the optimal voxels from the active region of interests. The proposed approach selects the data to feature subsets based on k-NN algorithm, so the data volume was effectively reduced and the voxel information was maintained significantly. The most informative voxels were given as the input for gradient self-weighting that produces an optimal weight value. Respective weight value was added to the projection matrix of linear collaborative discriminant regression classification for identifying the future projection matrix that reduces the error between two individual voxels in subspace. The experimental outcome shows that the proposed methodology improved the accuracy in fMRI data classification up to 0.7–23% compared to the existing methods.},
  archive      = {J_MVA},
  author       = {Gupta, K. O. and Chatur, P. N.},
  doi          = {10.1007/s00138-020-01070-9},
  journal      = {Machine Vision and Applications},
  month        = {3},
  number       = {3},
  pages        = {1-16},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Gradient self-weighting linear collaborative discriminant regression classification for human cognitive states classification},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Absolute time encoding for temporal super-resolution using
de bruijn coded exposures. <em>MVA</em>, <em>31</em>(1), 1–15. (<a
href="https://doi.org/10.1007/s00138-019-01050-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many target tracking tasks require high spatial and temporal precision. High frame rate imaging at high spatial resolution is commonly used in these applications, but this approach is expensive and generates large amounts of data which can complicate implementation. When tracking a single object in motion, almost all of this information is unused. A technique has been developed to exploit this sparsity and track motion with a long exposure where absolute timing is encoded by modulating the exposure over time according to a de Bruijn sequence. This technique has been implemented in the Desert Fireball Network to track bright meteors entering the Earth’s atmosphere for orbit determination and successful meteorite recovery. An alternate proof of concept implementation was also developed demonstrating tracking at 36 megapixels and 1000 Hz using a consumer camera with an inexpensive modulated light source and retroreflective target. The technique could be applied to other tracking problems requiring high temporal and spatial precision such as particle image velocimetry and space surveillance and tracking.},
  archive      = {J_MVA},
  author       = {Howie, Robert M. and Paxman, Jonathan and Bland, Philip A. and Towner, Martin C.},
  doi          = {10.1007/s00138-019-01050-8},
  journal      = {Machine Vision and Applications},
  month        = {2},
  number       = {1},
  pages        = {1-15},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Absolute time encoding for temporal super-resolution using de bruijn coded exposures},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Towards infield, live plant phenotyping using a
reduced-parameter CNN. <em>MVA</em>, <em>31</em>(1), 1–14. (<a
href="https://doi.org/10.1007/s00138-019-01051-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {There is an increase in consumption of agricultural produce as a result of the rapidly growing human population, particularly in developing nations. This has triggered high-quality plant phenotyping research to help with the breeding of high-yielding plants that can adapt to our continuously changing climate. Novel, low-cost, fully automated plant phenotyping systems, capable of infield deployment, are required to help identify quantitative plant phenotypes. The identification of quantitative plant phenotypes is a key challenge which relies heavily on the precise segmentation of plant images. Recently, the plant phenotyping community has started to use very deep convolutional neural networks (CNNs) to help tackle this fundamental problem. However, these very deep CNNs rely on some millions of model parameters and generate very large weight matrices, thus making them difficult to deploy infield on low-cost, resource-limited devices. We explore how to compress existing very deep CNNs for plant image segmentation, thus making them easily deployable infield and on mobile devices. In particular, we focus on applying these models to the pixel-wise segmentation of plants into multiple classes including background, a challenging problem in the plant phenotyping community. We combined two approaches (separable convolutions and SVD) to reduce model parameter numbers and weight matrices of these very deep CNN-based models. Using our combined method (separable convolution and SVD) reduced the weight matrix by up to 95% without affecting pixel-wise accuracy. These methods have been evaluated on two public plant datasets and one non-plant dataset to illustrate generality. We have successfully tested our models on a mobile device.},
  archive      = {J_MVA},
  author       = {Atanbori, John and French, Andrew P. and Pridmore, Tony P.},
  doi          = {10.1007/s00138-019-01051-7},
  journal      = {Machine Vision and Applications},
  month        = {2},
  number       = {1},
  pages        = {1-14},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Towards infield, live plant phenotyping using a reduced-parameter CNN},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Aerial-DEM geolocalization for GPS-denied UAS navigation.
<em>MVA</em>, <em>31</em>(1), 1–12. (<a
href="https://doi.org/10.1007/s00138-019-01052-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accelerated by the proliferation of small, affordable, and lightweight electronically scanning radar systems as well as advances in Unmanned Aircraft System (UAS) technology, Geo-Registered Radar Returns data are becoming an incredible source for geolocalization in GPS-denied UAS navigation. Most existing approaches match aerial images to pre-stored Digital Elevation Models (DEMs) through 3D terrain reconstruction or GPU-based terrain rendering techniques. However, these reconstruction or rendering processes are themselves error-prone and time-consuming, which further decrease UAS navigation accuracy. In this work, we propose a novel geolocalization approach by directly matching aerial images to DEMs. Inspired by success of deep learning in face recognition/verification, we develop a triplet-ranking network to embed aerial images and DEMs into the same low-dimensional feature space, where matching Aerial-DEM are near one another and mismatched Aerial-DEM are far apart. To create large-scale training dataset, we design an efficient terrain generation approach using per-pixel displacement mapping technique. This approach augments aerial datasets by simulating visual appearances of terrain under different lighting conditions. Experiments are conducted to show the effectiveness of our deep network in finding matches between aerial images and DEMs.},
  archive      = {J_MVA},
  author       = {Wang, Teng and Somani, Arun K.},
  doi          = {10.1007/s00138-019-01052-6},
  journal      = {Machine Vision and Applications},
  month        = {2},
  number       = {1},
  pages        = {1-12},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Aerial-DEM geolocalization for GPS-denied UAS navigation},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Convolutional networks for appearance-based recommendation
and visualisation of mascara products. <em>MVA</em>, <em>31</em>(1),
1–13. (<a href="https://doi.org/10.1007/s00138-019-01053-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this work, we explore the problems of recommending and visualising makeup products based on images of customers. Focusing on mascara, we propose a two-stage approach that first recommends products to a new customer based on the preferences of other customers with similar visual appearance and then visualises how the recommended products might look on the customer. For the initial product recommendation, we train a Siamese convolutional neural network, using our own dataset of cropped eye regions from images of 91 female subjects, such that it learns to output feature vectors that place images of the same subject close together in high-dimensional space. We evaluate the trained network based on its ability to correctly identify existing subjects from unseen images, and then assess its capability to identify visually similar subjects when an image of a new subject is used as input. For product visualisation, we train per-product generative adversarial networks to map the appearance of a specific product onto an image of a customer with no makeup. We train models to generate images of two mascara formulations and assess their capability to generate realistic mascara lashes while changing as little as possible within non-lash image regions and simulating the different effects of the two products used.},
  archive      = {J_MVA},
  author       = {Holder, Christopher J. and Ricketts, Stephen and Obara, Boguslaw},
  doi          = {10.1007/s00138-019-01053-5},
  journal      = {Machine Vision and Applications},
  month        = {2},
  number       = {1},
  pages        = {1-13},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Convolutional networks for appearance-based recommendation and visualisation of mascara products},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Anthropometric clothing measurements from 3D body scans.
<em>MVA</em>, <em>31</em>(1), 1–11. (<a
href="https://doi.org/10.1007/s00138-019-01054-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a full processing pipeline to acquire anthropometric measurements from 3D measurements. The first stage of our pipeline is a commercial point cloud scanner. In the second stage, a pre-defined body model is fitted to the captured point cloud. We have generated one male and one female model from the SMPL library. The fitting process is based on non-rigid iterative closest point algorithm that minimizes overall energy of point distance and local stiffness energy terms. In the third stage, we measure multiple circumference paths on the fitted model surface and use a nonlinear regressor to provide the final estimates of anthropometric measurements. We scanned 194 male and 181 female subjects, and the proposed pipeline provides mean absolute errors from 2.5 to 16.0 mm depending on the anthropometric measurement.},
  archive      = {J_MVA},
  author       = {Yan, Song and Wirta, Johan and Kämäräinen, Joni-Kristian},
  doi          = {10.1007/s00138-019-01054-4},
  journal      = {Machine Vision and Applications},
  month        = {2},
  number       = {1},
  pages        = {1-11},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Anthropometric clothing measurements from 3D body scans},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Detection of difficult airway using deep learning.
<em>MVA</em>, <em>31</em>(1), 1–11. (<a
href="https://doi.org/10.1007/s00138-019-01055-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Whenever a patient needs to enter the operating room, in case the surgery requires general anesthesia, he/she must be intubated, and an anesthesiologist has to make a previous check to the patient in order to evaluate his/her airway. This process should be done to the patient to anticipate any problem, such as a difficult airway at the time of being anesthetized. In fact, the inadequate detection of a difficult airway can cause serious complications, even death. This research work proposes a mobile app that uses a convolutional neural network to detect a difficult airway. This model classifies two classes of the Mallampati score, namely Mallampati 1–2 (with low risk of difficult airway) and Mallampati 3–4 (with higher risk of difficult airway). The average accuracy of the predictive model is 88.5% for classifying pictures. A total of 240 pictures were used for training the model. The results of sensitivity and specificity were 90% in average.},
  archive      = {J_MVA},
  author       = {Aguilar, Kevin and Alférez, Germán H. and Aguilar, Christian},
  doi          = {10.1007/s00138-019-01055-3},
  journal      = {Machine Vision and Applications},
  month        = {2},
  number       = {1},
  pages        = {1-11},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Detection of difficult airway using deep learning},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020b). Rosette plant segmentation with leaf count using orthogonal
transform and deep convolutional neural network. <em>MVA</em>,
<em>31</em>(1), 1–14. (<a
href="https://doi.org/10.1007/s00138-019-01056-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Plant image analysis plays an important role in agriculture. It is used to record the morphological plant traits regularly and accurately. The plant growth is one of the key traits to be analyzed, which relies on leaf area (i.e., leaf region or plant region) and leaf count. One of the ways to find the leaf count is counting the leaves using segmented plant region. In this paper, a new plant region segmentation scheme is proposed in the orthogonal transform domain based on orthogonal transform coefficients. Initially, an analysis of orthogonal transform coefficients is carried out in terms of the response of orthogonal basis vectors to extract the plant region. After extracting the plant region, the L*a*b and CMYK color spaces are used for noise removal in the segmentation scheme. Finally, the leaves are counted using fine-tuned deep convolutional neural network models. The proposed scheme is experimented on CVPPP benchmark datasets and also tested with the images taken from mobile phone to ensure its reliability and cross-platform applicability. The experiment results on CVPPP benchmark datasets are promising.},
  archive      = {J_MVA},
  author       = {Praveen Kumar, J. and Domnic, S.},
  doi          = {10.1007/s00138-019-01056-2},
  journal      = {Machine Vision and Applications},
  month        = {2},
  number       = {1},
  pages        = {1-14},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Rosette plant segmentation with leaf count using orthogonal transform and deep convolutional neural network},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Cross-spectral registration of natural images with SIPCFE.
<em>MVA</em>, <em>31</em>(1), 1–18. (<a
href="https://doi.org/10.1007/s00138-020-01057-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image registration is a viable task in the field of computer vision with many applications. When images are captured under different spectrum conditions, a challenge is imposed on the task of registration. Researchers carefully handcraft a local module insensitive to illumination changes across cross-spectral image pairs to tackle this challenge. We, in this paper, develop an optimized feature-based approach Single Instance Phase Congruency Feature Extractor (SIPCFE) to tackle the problem of natural cross-spectral image registration. SIPCFE uses the phase information of an image pair to quickly identify and describe reliable keypoints that are insensitive to illumination. It then employs a sequence of outlier removal processes to find the matching feature points accurately and the Direct Linear Transformation to estimate the geometric transformation to align the image pair. We extensively study the proposed approach for every module in the system to give more insights into the challenges. We benchmark our proposed method and other state-of-the-art feature-based methods developed for cross-spectral imagery on three datasets with various settings and image contents. The comprehensive analysis of cross-spectral registration results of natural images demonstrates that SIPCFE achieves up to 47.24%, 14.29%, and 12.45% accuracy improvement on the first, second, and third dataset, respectively, over the second best registration method in the benchmark.},
  archive      = {J_MVA},
  author       = {Farzaneh, Amir Hossein and Qi, Xiaojun},
  doi          = {10.1007/s00138-020-01057-6},
  journal      = {Machine Vision and Applications},
  month        = {2},
  number       = {1},
  pages        = {1-18},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Cross-spectral registration of natural images with SIPCFE},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Transfer learning privileged information fuels CAD diagnosis
of breast cancer. <em>MVA</em>, <em>31</em>(1), 1–23. (<a
href="https://doi.org/10.1007/s00138-020-01058-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The efficiency in breast cancer from imaging-based computer-aided diagnosis (CAD) has been revealed in recent years. As a fact, the methods grounded on a single modality constantly lack behind multimodal CAD imaging. However, owing to the restrictions of imaging devices, expressly in rural hospitals, single-modal imaging becomes a favorite in clinical practice for diagnosis. A fresh learning model trending nowadays known as learning using privileged information (LUPI) adopts additional privileged information (PI) modality to help during the training stage, but PI does not contribute in the testing stage. Meanwhile, the link exists between PI and training samples; the same is then reassigned to the learned model. We propose a LUPI-based CAD framework for breast cancer using privileged information in this work. The work offers both a classifier- or feature-level LUPI, in which the information is shifted from the additional PI modality to the diagnosis modality. A thorough comparison has been made among six classifier-level algorithms and six feature-level LUPI algorithms. The experimental results on both the acquired primary datasets show that all classifier-level and deep learning-based feature-level LUPI algorithms can enhance the performance of a single-modal imaging-based CAD for breast cancer by relocating PI.},
  archive      = {J_MVA},
  author       = {Shaikh, Tawseef Ayoub and Ali, Rashid and Beg, M. M. Sufyan},
  doi          = {10.1007/s00138-020-01058-5},
  journal      = {Machine Vision and Applications},
  month        = {2},
  number       = {1},
  pages        = {1-23},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Transfer learning privileged information fuels CAD diagnosis of breast cancer},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Graph laplacian for image anomaly detection. <em>MVA</em>,
<em>31</em>(1), 1–16. (<a
href="https://doi.org/10.1007/s00138-020-01059-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Reed–Xiaoli detector (RXD) is recognized as the benchmark algorithm for image anomaly detection; however, it presents known limitations, namely the dependence over the image following a multivariate Gaussian model, the estimation and inversion of a high-dimensional covariance matrix, and the inability to effectively include spatial awareness in its evaluation. In this work, a novel graph-based solution to the image anomaly detection problem is proposed; leveraging the graph Fourier transform, we are able to overcome some of RXD’s limitations while reducing computational cost at the same time. Tests over both hyperspectral and medical images, using both synthetic and real anomalies, prove the proposed technique is able to obtain significant gains over performance by other algorithms in the state of the art.},
  archive      = {J_MVA},
  author       = {Verdoja, Francesco and Grangetto, Marco},
  doi          = {10.1007/s00138-020-01059-4},
  journal      = {Machine Vision and Applications},
  month        = {2},
  number       = {1},
  pages        = {1-16},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Graph laplacian for image anomaly detection},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Deep learning in medical image registration: A survey.
<em>MVA</em>, <em>31</em>(1), 1–18. (<a
href="https://doi.org/10.1007/s00138-020-01060-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The establishment of image correspondence through robust image registration is critical to many clinical tasks such as image fusion, organ atlas creation, and tumor growth monitoring and is a very challenging problem. Since the beginning of the recent deep learning renaissance, the medical imaging research community has developed deep learning-based approaches and achieved the state-of-the-art in many applications, including image registration. The rapid adoption of deep learning for image registration applications over the past few years necessitates a comprehensive summary and outlook, which is the main scope of this survey. This requires placing a focus on the different research areas as well as highlighting challenges that practitioners face. This survey, therefore, outlines the evolution of deep learning-based medical image registration in the context of both research challenges and relevant innovations in the past few years. Further, this survey highlights future research directions to show how this field may be possibly moved forward to the next level.},
  archive      = {J_MVA},
  author       = {Haskins, Grant and Kruger, Uwe and Yan, Pingkun},
  doi          = {10.1007/s00138-020-01060-x},
  journal      = {Machine Vision and Applications},
  month        = {2},
  number       = {1},
  pages        = {1-18},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Deep learning in medical image registration: A survey},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020a). Correction to: Rosette plant segmentation with leaf count
using orthogonal transform and deep convolutional neural network.
<em>MVA</em>, <em>31</em>(1), 1. (<a
href="https://doi.org/10.1007/s00138-020-01062-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unfortunately, the co-author name was incorrectly spelled.},
  archive      = {J_MVA},
  author       = {Praveen Kumar, J. and Domnic, S.},
  doi          = {10.1007/s00138-020-01062-9},
  journal      = {Machine Vision and Applications},
  month        = {2},
  number       = {1},
  pages        = {1},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Correction to: Rosette plant segmentation with leaf count using orthogonal transform and deep convolutional neural network},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Publisher correction to: 10.1007/s00138-020-01059-4;
10.1007/s00138-020-01057-6; 10.1007/s00138-020-01058-5;
10.1007/s00138-020-01060-x; 10.1007/s00138-019-01054-4;
10.1007/s00138-019-01056-2; 10.1007/s00138-019-01053-5;
10.1007/s00138-019-01055-3; 10.1007/s00138-019-01052-6;
10.1007/s00138-019-01051-7; 10.1007/s00138-019-01050-8. <em>MVA</em>,
<em>31</em>(1), 1. (<a
href="https://doi.org/10.1007/s00138-020-01066-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The articles listed below were published in Issue January 2020, Issue 1, instead of Issue February 2020, Issues 1–2.},
  archive      = {J_MVA},
  doi          = {10.1007/s00138-020-01066-5},
  journal      = {Machine Vision and Applications},
  month        = {2},
  number       = {1},
  pages        = {1},
  shortjournal = {Mach. Vis. Appl.},
  title        = {Publisher correction to: 10.1007/s00138-020-01059-4; 10.1007/s00138-020-01057-6; 10.1007/s00138-020-01058-5; 10.1007/s00138-020-01060-x; 10.1007/s00138-019-01054-4; 10.1007/s00138-019-01056-2; 10.1007/s00138-019-01053-5; 10.1007/s00138-019-01055-3; 10.1007/s00138-019-01052-6; 10.1007/s00138-019-01051-7; 10.1007/s00138-019-01050-8},
  volume       = {31},
  year         = {2020},
}
</textarea>
</details></li>
</ul>

</body>
</html>
