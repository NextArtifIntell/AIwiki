<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>APIN_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="apin---284">APIN - 284</h2>
<ul>
<li><details>
<summary>
(2020). Focused random walk with probability distribution for SAT
with long clauses. <em>APIN</em>, <em>50</em>(12), 4732–4753. (<a
href="https://doi.org/10.1007/s10489-020-01768-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Focused random walk (FRW) is one of the most influential paradigm of stochastic local search (SLS) algorithms for the propositional satisfiability (SAT) problem. Recently, an interesting probability distribution (PD) strategy for variable selection was proposed and has been successfully used to improve SLS algorithms, resulting in state-of-the-art solvers. However, most solvers based on the PD strategy only use polynomial function (PoF) to handle the exponential decay and are still unsatisfactory in dealing with medium and huge k-SAT instances at and near the phase transition. The present paper is focused on handling all k-SAT instances with long clauses. Firstly, an extensive empirical study of one state-of-the-art FRW solver WalkSATlm on a wide range of SAT problems is presented with the focus given on fitting the distribution of the break value of variable selected in each step, which turns out to be a Boltzmann function. Using theses case studies as a basis, we propose a pseudo normal function (PNF) to fit the distribution of the break value of variable selected, which is actually a variation of the Boltzmann function. In addition, a new tie-breaking flipping (TBF) strategy is proposed to prevent the same variable from being flipped in consecutive steps. The PNF based PD strategy combined with the TBF strategy lead to a new variable selection heuristic named PNF-TBF. The PNF-TBF heuristic along with a variable allocation value (Vav) function are used to significantly improve ProbSAT, a state-of-the-art SLS solver, leads to a new FRW algorithm dubbed PNFSat, which achieves the state-of-the-art performance on a broad range of huge random 7-SAT instance near the phase transition as demonstrated via the extensive experimental studies. Some further improved versions on top of PNFSat are presented respectively, including PNFSat_alt, which achieves the state-of-the-art performance on the medium 7-SAT instances at the phase transition; PN&amp;PoFSat, which achieves the state-of-the-art performance on a broad range of random 5-SAT benchmarks; as well as an integrated version of these three algorithms, named PDSat, which achieves the state-of-the-art performances on all huge and medium random k-SAT instances with long clauses as demonstrated via the comparative studies using different benchmarks.},
  archive      = {J_APIN},
  author       = {Fu, Huimin and Liu, Jun and Xu, Yang},
  doi          = {10.1007/s10489-020-01768-3},
  journal      = {Applied Intelligence},
  month        = {12},
  number       = {12},
  pages        = {4732-4753},
  shortjournal = {Appl. Intell.},
  title        = {Focused random walk with probability distribution for SAT with long clauses},
  volume       = {50},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Enhancing recommendation accuracy of item-based
collaborative filtering using bhattacharyya coefficient and most similar
item. <em>APIN</em>, <em>50</em>(12), 4708–4731. (<a
href="https://doi.org/10.1007/s10489-020-01775-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The item-based collaborative filtering technique recommends an item to the user from the rating of k-nearest items. Generally, a random value of k is considered to find nearest neighbor from item-item similarity matrix. However, consideration of a random value for k intuitively is not a rational approach, as different items may have different value of k nearest neighbor. Sparsity in the data set is another challenge in collaborative filtering, as number of co-rated items’ may be few or zero. Due to the above two reasons, collaborative filtering provides inaccurate recommendations, because the predicted rating may tend towards the Mean. The objective of the proposed work is to improve the accuracy by mitigating the above issues. Instead of using a random value of k, we use the most similar neighbor for each target item so as to predict the target item, since finding k for different target item is computationally expensive. Bhattacharyya Coefficient is used as a similarity measure to handle sparsity in the dataset. The performance of the proposed algorithm is tested the datasets of MovieLens and Film Trust, and experimental results reveal better prediction accuracy than the best of the prevalent prediction approaches exist in literature.},
  archive      = {J_APIN},
  author       = {Singh, Pradeep Kumar and Sinha, Madhabendra and Das, Suvrojit and Choudhury, Prasenjit},
  doi          = {10.1007/s10489-020-01775-4},
  journal      = {Applied Intelligence},
  month        = {12},
  number       = {12},
  pages        = {4708-4731},
  shortjournal = {Appl. Intell.},
  title        = {Enhancing recommendation accuracy of item-based collaborative filtering using bhattacharyya coefficient and most similar item},
  volume       = {50},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Fast and accurate cable detection using CNN. <em>APIN</em>,
<em>50</em>(12), 4688–4707. (<a
href="https://doi.org/10.1007/s10489-020-01746-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, unmanned aerial vehicle (UAV) based vision inspections have been widely applied in electricity systems for both efficiency improvement and labor cost saving. Cable detection is essential for both navigation and flight safety of aerial vehicles. However, power cable detection is widely regarded as a challenging task since the targets are very weak and very easy to be confused with cluttered backgrounds. Traditional line and edge detectors are lack of robustness to scene variations. Recent deep learning based methods also can not support fast and stable power cable detection well for onboard applications . In this paper, a new convolutional neural network (CNN) based cable detection method is proposed. First of all, we encode cables by groups of evenly distributed key points, which reduce the complexities of detection tasks. By this approach, the proposed model detect grouped key points of cables from aerial images directly and the detailed pixels of cables can be restore with the curve equations which are implicitly behind those grouped key points. Subsequently, new methods of data labeling and augmentation, sample matching, post clustering, and performance evaluation for cable key points detection are presented. Finally, comprehensive experimental results demonstrate the efficiency and accuracy of our proposed cable detection method.},
  archive      = {J_APIN},
  author       = {Dai, Zhiyong and Yi, Jianjun and Zhang, Yajun and Zhou, Bo and He, Liang},
  doi          = {10.1007/s10489-020-01746-9},
  journal      = {Applied Intelligence},
  month        = {12},
  number       = {12},
  pages        = {4688-4707},
  shortjournal = {Appl. Intell.},
  title        = {Fast and accurate cable detection using CNN},
  volume       = {50},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Light-YOLOv3: Fast method for detecting green mangoes in
complex scenes using picking robots. <em>APIN</em>, <em>50</em>(12),
4670–4687. (<a
href="https://doi.org/10.1007/s10489-020-01818-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {When a robot picks green fruit under natural light, the color of the fruit is similar to the background; uneven lighting and fruit and leaf occlusion often affect the performance of the detection method. We take green mangoes as an experimental object. A lightweight green mangoes detection method based on YOLOv3 is proposed here. To improve the detection speed of the method, we first combine the color, texture, and shape features of green mango to design a lightweight network unit to replace the residual units in YOLOv3. Second, the improved Multiscale context aggregation (MSCA) module is used to concatenate multilayer features and make predictions, solving the problem of insufficient position information and semantic information on the prediction feature map in YOLOv3; this approach effectively improves the detection effect for the green mangoes. To address the overlap of green mangoes, soft non-maximum suppression (Soft-NMS) is used to replace non-maximum suppression (NMS), thereby reducing the missing of predicted boxes due to green mango overlaps. Finally, an auxiliary inspection green mango image enhancement algorithm (CLAHE-Mango) is proposed, is suitable for low-brightness detection environments and improves the accuracy of the green mango detection method. The experimental results show that the F1% of Light-YOLOv3 in the test set is 97.7%. To verify the performance of Light-YOLOv3 under the embedded platform, we embed one-stage methods into the Adreno 640 and Mali-G76 platforms. Compared with YOLOv3, the F1% of Light-YOLOv3 is increased by 4.5%, and the running speed is increased by 5 times, which can meet the real-time running requirements for picking robots. Through three sets of comparative experiments, we could determine that our method has the best detection results in terms of dense, backlit, direct light, night, long distance, and special angle scenes under complex lighting.},
  archive      = {J_APIN},
  author       = {Xu, Zhi-Feng and Jia, Rui-Sheng and Sun, Hong-Mei and Liu, Qing-Ming and Cui, Zhe},
  doi          = {10.1007/s10489-020-01818-w},
  journal      = {Applied Intelligence},
  month        = {12},
  number       = {12},
  pages        = {4670-4687},
  shortjournal = {Appl. Intell.},
  title        = {Light-YOLOv3: Fast method for detecting green mangoes in complex scenes using picking robots},
  volume       = {50},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Effective constructive heuristic and metaheuristic for the
distributed assembly blocking flow-shop scheduling problem.
<em>APIN</em>, <em>50</em>(12), 4647–4669. (<a
href="https://doi.org/10.1007/s10489-020-01809-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Scheduling in distributed production system has become an active research field in recent years. This paper investigates the distributed assembly blocking flow-shop scheduling problem (DABFSP), which consists of two stages: production and assembly. The first stage is processing jobs in several identical factories. Each factory has a series of machines no intermediate buffers existing between adjacent ones. The second stage assembles the processed jobs into the final products through a single machine. The objective is to minimize the maximum completion time or makespan of all products. To address this problem, a constructive heuristic is proposed based on a new assignment rule of jobs and a product-based insertion procedure. Afterwards, an iterated local search (ILS) is presented, which integrates an integrated encoding scheme, a multi-type perturbation procedure containing four kinds of perturbed operators based on problem-specific knowledge and a critical-job-based variable neighborhood search. Finally, a comprehensive computational experiment and comparisons with the closely related and well performing methods in the literature are carried out. The experimental and comparison results show that the proposed constructive heuristic and ILS can solve the DABFSP effectively and efficiently.},
  archive      = {J_APIN},
  author       = {Shao, Zhongshi and Shao, Weishi and Pi, Dechang},
  doi          = {10.1007/s10489-020-01809-x},
  journal      = {Applied Intelligence},
  month        = {12},
  number       = {12},
  pages        = {4647-4669},
  shortjournal = {Appl. Intell.},
  title        = {Effective constructive heuristic and metaheuristic for the distributed assembly blocking flow-shop scheduling problem},
  volume       = {50},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Parallel dual networks for visual object tracking.
<em>APIN</em>, <em>50</em>(12), 4631–4646. (<a
href="https://doi.org/10.1007/s10489-020-01783-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Visual Object Tracking plays an essential role in solving many basic problems in computer vision. In order to improve the tracking accuracy, the previous methods have prevented tracking failures from occurring by improving the ability to describe the target. However, few of them consider ways to relocate and track the target after a tracking failure. In this paper, we propose the use of a parallel dual network for visual object tracking. This is constructed from two networks and an adjustment module to enable judgement of tracking failures, as well as target relocation and tracking. Firstly, we employ the Siamese matching method and correlation filter method to build tracking network and inspecting network. Both networks track the target simultaneously to obtain two tracking results. Secondly, an adjustment module is constructed, which compares the overlap ratio of the two tracking results with a set threshold, then fuses them or selects the best one. Finally, the fusion or selection result is output and the tracker is updated. We perform comprehensive experiments on five benchmarks: VOT2016, UAV123, Temple Color-128, OTB-100 and OTB-50. The results demonstrate that, compared with other state-of-the-art algorithms, the proposed tracking method improves tracking precision while maintaining real-time performance.},
  archive      = {J_APIN},
  author       = {Li, Tian and Wu, Peihan and Ding, Feifei and Yang, Wenyuan},
  doi          = {10.1007/s10489-020-01783-4},
  journal      = {Applied Intelligence},
  month        = {12},
  number       = {12},
  pages        = {4631-4646},
  shortjournal = {Appl. Intell.},
  title        = {Parallel dual networks for visual object tracking},
  volume       = {50},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Multi-level knowledge-based approach for implicit aspect
identification. <em>APIN</em>, <em>50</em>(12), 4616–4630. (<a
href="https://doi.org/10.1007/s10489-020-01817-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sentiment analysis or opinion mining is the area of research in Natural Language Processing (NLP) and text mining which deals with the systematic identification of subjective information from user generated text. At a more fine-grained level, aspect-based sentiment analysis focuses on the targets of users’ opinions and determining sentiment orientation of these opinions. Among different tasks of aspect-based sentiment analysis, aspect extraction is the key task which includes extraction of both explicit and implicit aspects. Due to the complexity of implicit aspects, not much effort has been put forward to solve the problem while explicit aspects have been studied extensively in the recent past. Existing approaches for implicit aspect extraction have focused on specific type of aspects and have neglected the actual problem. Therefore, in this paper, we have proposed a multi-level approach which identifies implicit aspects using co-occurrence and similarity-based techniques. This research focuses on the extraction of clues for implicit targets of users’ opinions and identification of true targets of users’ opinions with the help of implicit aspect clues. The proposed approach is divided into two phases: first, several rules are crafted to identify clues for implicit aspects in a review sentence. Secondly, aspects are assigned on the basis of extracted clues using proposed multi-level approach. The proposed model can extract not only implicit aspect clues associated with opinion words but also allocate clues to opinion words where no association is identified. This helps to identify implicit aspects with or without co-occurrences of opinion words with explicit aspects. Experimental evaluation elaborates the importance of implicit aspect clues in the identification of targets of users’ opinions. The proposed approach shows better results as compared with the state-of-the-art approaches for the identification of implicit targets of users’ opinions on a dataset of different product reviews.},
  archive      = {J_APIN},
  author       = {Rana, Toqir A. and Cheah, Yu-N and Rana, Tauseef},
  doi          = {10.1007/s10489-020-01817-x},
  journal      = {Applied Intelligence},
  month        = {12},
  number       = {12},
  pages        = {4616-4630},
  shortjournal = {Appl. Intell.},
  title        = {Multi-level knowledge-based approach for implicit aspect identification},
  volume       = {50},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Twin labeled LDA: A supervised topic model for document
classification. <em>APIN</em>, <em>50</em>(12), 4602–4615. (<a
href="https://doi.org/10.1007/s10489-020-01798-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, some statistic topic modeling approaches, e.g., Latent Dirichlet allocation (LDA), have been widely applied in the field of document classification. However, standard LDA is a completely unsupervised algorithm, and then there is growing interest in incorporating prior information into the topic modeling procedure. Some effective approaches have been developed to model different kinds of prior information, for example, observed labels, hidden labels, the correlation among labels, label frequencies; however, these methods often need heavy computing because of model complexity. In this paper, we propose a new supervised topic model for document classification problems, Twin Labeled LDA (TL-LDA), which has two sets of parallel topic modeling processes, one incorporates the prior label information by hierarchical Dirichlet distributions, the other models the grouping tags, which have prior knowledge about the label correlation; the two processes are independent from each other, so the TL-LDA can be trained efficiently by multi-thread parallel computing. Quantitative experimental results compared with state-of-the-art approaches demonstrate our model gets the best scores on both rank-based and binary prediction metrics in solving single-label classification, and gets the best scores on three metrics, i.e., One Error, Micro-F1, and Macro-F1 while multi-label classification, including non power-law and power-law datasets. The results show benefit from modeling fully prior knowledge, our model has outstanding performance and generalizability on document classification. Further comparisons with recent works also indicate the proposed model is competitive with state-of-the-art approaches.},
  archive      = {J_APIN},
  author       = {Wang, Wei and Guo, Bing and Shen, Yan and Yang, Han and Chen, Yaosen and Suo, Xinhua},
  doi          = {10.1007/s10489-020-01798-x},
  journal      = {Applied Intelligence},
  month        = {12},
  number       = {12},
  pages        = {4602-4615},
  shortjournal = {Appl. Intell.},
  title        = {Twin labeled LDA: A supervised topic model for document classification},
  volume       = {50},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). DBN based SD-ARX model for nonlinear time series prediction
and analysis. <em>APIN</em>, <em>50</em>(12), 4586–4601. (<a
href="https://doi.org/10.1007/s10489-020-01804-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {One of the main purposes of nonlinear system modeling is to design model-based controllers such as model predictive control (MPC). A group of deep belief networks (DBNs) are used to approximate the function type coefficients of a state dependent autoregressive model with exogenous variables (SD-ARX), which can represent nonlinear dynamics, and thus a DBN-based state-dependent ARX (DBN-ARX) model is obtained in this paper. The DBN-ARX model has the function approximation ability of single DBN model and the nonlinear description advantage of SD-ARX model. All parameters of the DBN-ARX model are estimated by the pre-training and fine-tuning strategies and the stability condition of the model are also discussed. The proposed DBN-ARX model is a pseudo-linear ARX model identified offline, and its function type coefficients are composed of the operating-point dependent DBNs. The usefulness of the DBN-ARX model is illustrated by modeling a continuously stirred tank reactor (CSTR) time series, Box and Jenkins data, a nonlinear process and a water tank system. The four experimental results show that the one-step-ahead and multi-step-ahead prediction accuracy of the proposed DBN-ARX model is improved comparing with the modeling results of several existing models.},
  archive      = {J_APIN},
  author       = {Xu, Wenquan and Peng, Hui and Tian, Xiaoying and Peng, Xiaoyan},
  doi          = {10.1007/s10489-020-01804-2},
  journal      = {Applied Intelligence},
  month        = {12},
  number       = {12},
  pages        = {4586-4601},
  shortjournal = {Appl. Intell.},
  title        = {DBN based SD-ARX model for nonlinear time series prediction and analysis},
  volume       = {50},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Exploiting relay nodes for maximizing wireless underground
sensor network lifetime. <em>APIN</em>, <em>50</em>(12), 4568–4585. (<a
href="https://doi.org/10.1007/s10489-020-01735-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A major challenge in wireless underground sensor networks is the signal attenuation originated from multi-environment transmission between underground sensor nodes and the above-ground base station. To overcome this issue, an efficient approach is deploying a set of relay nodes aboveground, thereby reducing transmission loss by shortening transmitting distance. However, this introduces several new challenges, including load balancing and transmission loss minimization. This paper tackles the problem of deploying relay nodes to reduce transmission loss under a load balancing constraint by proposing two approximation algorithms. The first algorithm is inspired by Beam Search, combined with a new selection scheme based on Boltzmann distribution. The second algorithm aims to further improve the solutions obtained by the former by reducing the transmission loss. We observe that we can find an optimal assignment between sensor nodes and a set of the chosen relay in polynomial time by reformulating the part of the problem as a bipartite matching problem with minimum cost. Experimental results indicate that the proposed methods perform better than the other existing ones in most of our test instances while reducing the execution time.},
  archive      = {J_APIN},
  author       = {Tam, Nguyen Thi and Dung, Dinh Anh and Hung, Tran Huy and Binh, Huynh Thi Thanh and Yu, Shui},
  doi          = {10.1007/s10489-020-01735-y},
  journal      = {Applied Intelligence},
  month        = {12},
  number       = {12},
  pages        = {4568-4585},
  shortjournal = {Appl. Intell.},
  title        = {Exploiting relay nodes for maximizing wireless underground sensor network lifetime},
  volume       = {50},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Guided autoencoder for dimensionality reduction of
pedestrian features. <em>APIN</em>, <em>50</em>(12), 4557–4567. (<a
href="https://doi.org/10.1007/s10489-020-01813-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Autoencoder and other conventional dimensionality reduction algorithms have achieved great success in dimensionality reduction. In this paper, we present an improved autoencoder structure, which was applied it in the field of pedestrian feature dimensionality reduction. The novel method is also verified on Mnist dataset. High-dimensional deep pedestrian features outperform other descriptors while it is challenging for computing capability and memory in existing systems. The dimensionality reduction method we proposed takes advantages of autoencoder and principal component analysis to achieve high efficiency. A novel weight matrix initialization and an improved reconstruction of autoencoder are proposed. Furthermore, by fusing features labeled with the same pedestrian, the proposed structure minimizes the loss after dimensionality reduction. Experimental results demonstrate that our method outperforms traditional dimensionality reduction methods. In the experiment, the pedestrian features were generated by ResNet and Market-1501 data-set. Our method achieves up to 8.834% mAP increment compared to a principal component analysis, when 2048-dimension pedestrian features are reduced to 16-dimension features.},
  archive      = {J_APIN},
  author       = {Li, Xuan and Zhang, Tao and Zhao, Xin and Yi, Zhengming},
  doi          = {10.1007/s10489-020-01813-1},
  journal      = {Applied Intelligence},
  month        = {12},
  number       = {12},
  pages        = {4557-4567},
  shortjournal = {Appl. Intell.},
  title        = {Guided autoencoder for dimensionality reduction of pedestrian features},
  volume       = {50},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A parameter-free affinity based clustering. <em>APIN</em>,
<em>50</em>(12), 4543–4556. (<a
href="https://doi.org/10.1007/s10489-020-01812-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Several methods have been proposed to estimate the number of clusters in a dataset; the basic idea behind all of them has been to study an index that measures inter-cluster separation and intra-cluster cohesion over a range of cluster numbers and report the number which gives an optimum value of the index. In this paper, we propose a simple parameter-free approach that is more like human cognition of clusters, where closely lying points are easily identified to form a cluster and the total number of clusters is revealed. To identify closely lying points, the affinity of two points is defined as a function of distance and a threshold affinity is identified, above which two points in a dataset are likely to be in the same cluster. Well separated clusters are identified even in the presence of outliers, whereas for a not well-separated dataset, the final number of clusters is estimated from the detected clusters. And they are merged to produce the final clusters. Experiments performed with several large dimensional synthetic and real datasets show good results with robustness to noise and density variation within a dataset.},
  archive      = {J_APIN},
  author       = {Mukhoty, Bhaskar and Gupta, Ruchir and K., Lakshmanan and Kumar, Mayank},
  doi          = {10.1007/s10489-020-01812-2},
  journal      = {Applied Intelligence},
  month        = {12},
  number       = {12},
  pages        = {4543-4556},
  shortjournal = {Appl. Intell.},
  title        = {A parameter-free affinity based clustering},
  volume       = {50},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A novel ant colony optimization based on game for traveling
salesman problem. <em>APIN</em>, <em>50</em>(12), 4529–4542. (<a
href="https://doi.org/10.1007/s10489-020-01799-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ant Colony Optimization (ACO) algorithms tend to fall into local optimal and have insufficient astringency when applied to solve Traveling Salesman Problem (TSP). To address this issue, a novel game-based ACO (NACO) is proposed in this report. NACO consists of two ACOs: Ant Colony System (ACS) and Max-Min Ant System (MMAS). First, an entropy-weighted learning strategy is proposed. By improving diversity adaptively, the optimal solution precision can be optimized. Then, to improve the astringency, a nucleolus game strategy is set for ACS colonies. ACS colonies under cooperation share pheromone distribution and distribute cooperative profits through nucleolus. Finally, to jump out of the local optimum, mean filtering is introduced to process the pheromone distribution when the algorithm stalls. From the experimental results, it is demonstrated that NACO has well performance in terms of both the solution precision and the astringency.},
  archive      = {J_APIN},
  author       = {Yang, Kang and You, Xiaoming and Liu, Shen and Pan, Han},
  doi          = {10.1007/s10489-020-01799-w},
  journal      = {Applied Intelligence},
  month        = {12},
  number       = {12},
  pages        = {4529-4542},
  shortjournal = {Appl. Intell.},
  title        = {A novel ant colony optimization based on game for traveling salesman problem},
  volume       = {50},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Multi-split optimized bagging ensemble model selection for
multi-class educational data mining. <em>APIN</em>, <em>50</em>(12),
4506–4528. (<a
href="https://doi.org/10.1007/s10489-020-01776-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Predicting students’ academic performance has been a research area of interest in recent years, with many institutions focusing on improving the students’ performance and the education quality. The analysis and prediction of students’ performance can be achieved using various data mining techniques. Moreover, such techniques allow instructors to determine possible factors that may affect the students’ final marks. To that end, this work analyzes two different undergraduate datasets at two different universities. Furthermore, this work aims to predict the students’ performance at two stages of course delivery (20% and 50% respectively). This analysis allows for properly choosing the appropriate machine learning algorithms to use as well as optimize the algorithms’ parameters. Furthermore, this work adopts a systematic multi-split approach based on Gini index and p-value. This is done by optimizing a suitable bagging ensemble learner that is built from any combination of six potential base machine learning algorithms. It is shown through experimental results that the posited bagging ensemble models achieve high accuracy for the target group for both datasets.},
  archive      = {J_APIN},
  author       = {Injadat, MohammadNoor and Moubayed, Abdallah and Nassif, Ali Bou and Shami, Abdallah},
  doi          = {10.1007/s10489-020-01776-3},
  journal      = {Applied Intelligence},
  month        = {12},
  number       = {12},
  pages        = {4506-4528},
  shortjournal = {Appl. Intell.},
  title        = {Multi-split optimized bagging ensemble model selection for multi-class educational data mining},
  volume       = {50},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Multi-stream neural network fused with local information and
global information for HOI detection. <em>APIN</em>, <em>50</em>(12),
4495–4505. (<a
href="https://doi.org/10.1007/s10489-020-01794-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Human-Object Interaction (HOI) Detection is a new genre of human-centric visual relationship detection task, which is significant to deep understanding of visual scenes. Due to the complexity of the visual scene in the image, HOI detection is still a challenging task, the most critical part of which is feature extraction and representation. Some existing approaches rely solely on local region information for HOI detection without using global contextual information, but global contextual information contributes to this task in some HOI categories. Other approaches incorporate global contextual information for HOI detection while losing local region information. In this work, we propose a multi-stream neural network architecture composed of three special module that employs both local region information and global contextual information for HOI detection. This model can detect not only the HOI categories based on local region information but also on global contextual information. Our model more fully considers all HOI categories in the dataset. Compared with other existing approaches, the proposed model shows improved performance on V-COCO and HICO-DET benchmark datasets, especially when predicting rare HOI categories.},
  archive      = {J_APIN},
  author       = {Xia, Limin and Li, Rui},
  doi          = {10.1007/s10489-020-01794-1},
  journal      = {Applied Intelligence},
  month        = {12},
  number       = {12},
  pages        = {4495-4505},
  shortjournal = {Appl. Intell.},
  title        = {Multi-stream neural network fused with local information and global information for HOI detection},
  volume       = {50},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Generative image inpainting for link prediction.
<em>APIN</em>, <em>50</em>(12), 4482–4494. (<a
href="https://doi.org/10.1007/s10489-020-01648-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Link prediction is a fundamental task that predicts whether a link exists between two nodes based on the currently observed network. Existing approaches such as heuristic-based algorithms assume that two nodes are likely to have a link in a network. In fact, they limit algorithm effectiveness when the assumptions are not correct. Moreover, these link prediction algorithms lack generalization ability, which indicates that they have different effects on different types of networks. For example, the common neighbours algorithm works well on social networks, but it shows poor performance on electric power networks. Inspired by the image inpainting technology of generative adversarial networks and the adjacency matrix representation of networks, we propose a new framework for link prediction based on the image inpainting method, named the Generative Image Inpainting for Link Prediction algorithm (GIILP), to address these problems. The key idea of the GIILP is that the network can be converted into an image (the image is a form of the adjacency matrix (two dimensions)). Pixel values represent the likelihood of two nodes. Thus, the problem of predicting a possible link between nodes is converted into filling missing pixels in an image. The link information of the network can be expressed by the image information, which means that our algorithm does not need assumptions such as heuristics, and works regardless of the dataset type. The experimental results on multiple link prediction public datasets demonstrate that our algorithm has an advantage over other algorithms, including heuristic-based and deep learning methods.},
  archive      = {J_APIN},
  author       = {Qian, Fulan and Li, Jianhong and Du, Xiuquan and Chen, Xi and Zhao, Shu and Zhang, Yanping},
  doi          = {10.1007/s10489-020-01648-w},
  journal      = {Applied Intelligence},
  month        = {12},
  number       = {12},
  pages        = {4482-4494},
  shortjournal = {Appl. Intell.},
  title        = {Generative image inpainting for link prediction},
  volume       = {50},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Differential evolution with infeasible-guiding mutation
operators for constrained multi-objective optimization. <em>APIN</em>,
<em>50</em>(12), 4459–4481. (<a
href="https://doi.org/10.1007/s10489-020-01733-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Constrained multi-objective optimization problems (CMOPs) are common in engineering design fields. To solve such problems effectively, this paper proposes a new differential evolution variant named IMDE with infeasible-guiding mutation operators and a multistrategy technique. In IMDE, an infeasible solution with lower objective values is maintained for each individual in the main population, and this infeasible solution is then incorporated into some common differential evolution’s mutation operators to guide the search toward the region with promising objective values. Moreover, multiple mutation strategies and control parameters are adopted during the trial vector generation procedure to enhance both the convergence and the diversity of differential evolution. The superior performance of IMDE is validated via comparisons with some state-of-the-art constrained multi-objective evolutionary algorithms over 3 sets of artificial benchmarks and 4 widely used engineering design problems. The experiments show that IMDE outperforms other algorithms or obtains similar results. It is an effective approach for solving CMOPs, basically due to the use of infeasible-guiding mutation operators and multiple strategies.},
  archive      = {J_APIN},
  author       = {Xu, Bin and Duan, Wei and Zhang, Haifeng and Li, Zeqiu},
  doi          = {10.1007/s10489-020-01733-0},
  journal      = {Applied Intelligence},
  month        = {12},
  number       = {12},
  pages        = {4459-4481},
  shortjournal = {Appl. Intell.},
  title        = {Differential evolution with infeasible-guiding mutation operators for constrained multi-objective optimization},
  volume       = {50},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). An improved moth-flame optimization algorithm with
orthogonal opposition-based learning and modified position updating
mechanism of moths for global optimization problems. <em>APIN</em>,
<em>50</em>(12), 4434–4458. (<a
href="https://doi.org/10.1007/s10489-020-01793-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Moth-Flame Optimization (MFO) algorithm is a new population-based meta-heuristic algorithm for solving global optimization problems. Flames generation and spiral search are two key components that affect the performance of MFO. To improve the diversity of flames and the searching ability of moths, an improved Moth-Flame Optimization (IMFO) algorithm is proposed. The main features of the IMFO are: the flames are generated by orthogonal opposition-based learning (OOBL); the modified position updating mechanism of moths with linear search and mutation operator. To evaluate the performance of IMFO, the IMFO algorithm is compared with other 20 algorithms on 23 benchmark functions and IEEE (Institute of Electrical and Electronics Engineers) CEC (Congress on Evolutionary Computation) 2014 benchmark test set. The comparative results show that the IMFO is effective and has good performance in terms of jumping out of local optimum, balancing exploitation ability and exploration ability. Moreover, the IMFO is also used to solve three engineering optimization problems, and it is compared with other well-known algorithms. The comparison results show that the IMFO algorithm can improve the global search ability of MFO and effectively solve the practical engineering optimization problems.},
  archive      = {J_APIN},
  author       = {Zhao, Xiaodong and Fang, Yiming and Liu, Le and Li, Jianxiong and Xu, Miao},
  doi          = {10.1007/s10489-020-01793-2},
  journal      = {Applied Intelligence},
  month        = {12},
  number       = {12},
  pages        = {4434-4458},
  shortjournal = {Appl. Intell.},
  title        = {An improved moth-flame optimization algorithm with orthogonal opposition-based learning and modified position updating mechanism of moths for global optimization problems},
  volume       = {50},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Prioritizing positive feature values: A new hierarchical
feature selection method. <em>APIN</em>, <em>50</em>(12), 4412–4433. (<a
href="https://doi.org/10.1007/s10489-020-01782-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this work we address the problem of feature selection for the classification task in hierarchical and sparse feature spaces, which characterize many real-world applications nowadays. A binary feature space is deemed hierarchical when its binary features are related via generalization-specialization relationships, and is considered sparse when in general the instances contain much fewer “positive” than “negative” feature values. In any given instance, a feature value is deemed positive (negative) when the property associated with the feature has been (has not been) observed for that instance. Although there are many methods for the traditional feature selection problem in the literature, the proper treatment to hierarchical feature structures is still a challenge. Hence, we introduce a novel hierarchical feature selection method that follows the lazy learning paradigm—selecting a feature subset tailored for each instance in the test set. Our strategy prioritizes the selection of features with positive values, since they tend to be more informative—the presence of a relatively rare property is usually a piece of more relevant information than the absence of that property. Experiments on different application domains have shown that the proposed method outperforms previous hierarchical feature selection methods and also traditional methods in terms of predictive accuracy, selecting smaller feature subsets in general.},
  archive      = {J_APIN},
  author       = {Silva, Pablo Nascimento da and Plastino, Alexandre and Freitas, Alex A.},
  doi          = {10.1007/s10489-020-01782-5},
  journal      = {Applied Intelligence},
  month        = {12},
  number       = {12},
  pages        = {4412-4433},
  shortjournal = {Appl. Intell.},
  title        = {Prioritizing positive feature values: A new hierarchical feature selection method},
  volume       = {50},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Joint local structure preservation and redundancy
minimization for unsupervised feature selection. <em>APIN</em>,
<em>50</em>(12), 4394–4411. (<a
href="https://doi.org/10.1007/s10489-020-01800-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unsupervised feature selection is an indispensable pre-processing step in many data mining and pattern recognition tasks where the unlabeled high dimensional data are ubiquitous. Most of existing methods fail to explore the local geometric structure consistency (preservation) of the input data and minimize redundancy of selected features simultaneously. In this paper we propose a novel unsupervised feature selection method which jointly integrates the local geometric structure consistency and redundancy minimization (JLSPRM) into an unified framework. JLSPRM utilizes nonnegative spectral analysis to learn the cluster labels of the input data, then the local geometric structure consistency is developed to make the learned cluster labels more accurate, during which the feature selection operation is performed. To minimize the redundancy rate among selected features, the maximal information coefficient (MIC) is utilized to evaluate the correlation of the pairwise features. Besides, the ℓ2,1-norm is exerted on feature selection matrix which makes the framework decent for selecting features. An efficient iterative optimization algorithm is designed to obtain the solution of the unsupervised feature selection model. The superiority and effectiveness of our proposed approach over the state-of-the-art feature selection methods have also been validated through the extensive experiments on nine benchmark datasets.},
  archive      = {J_APIN},
  author       = {Li, Hao and Wang, Yongli and Li, Yanchao and Hu, Peng and Zhao, Ruxin},
  doi          = {10.1007/s10489-020-01800-6},
  journal      = {Applied Intelligence},
  month        = {12},
  number       = {12},
  pages        = {4394-4411},
  shortjournal = {Appl. Intell.},
  title        = {Joint local structure preservation and redundancy minimization for unsupervised feature selection},
  volume       = {50},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Novel coupled DP system for fuzzy c-means clustering and
image segmentation. <em>APIN</em>, <em>50</em>(12), 4378–4393. (<a
href="https://doi.org/10.1007/s10489-020-01784-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study proposed a novel fuzzy c-means clustering method which calculates the density of the data points based on the weighted mean distance (WMFCM). A novel coupled DNA-GA and P system (DP system) is introduced to realize the clustering process. The evolutional rules in the coupled DP system can help the WMFCM algorithm jump out of the local optimum and get the initial clustering centers. The performance of the coupled DP system in dealing with fuzzy clustering problems is measured by conducting experimental analysis on UCI datasets and BSDS300 image datasets. And the experimental results are compared with several popular algorithms. Experimental results show that our algorithm can perform better than other algorithms.},
  archive      = {J_APIN},
  author       = {Jiang, Zhenni and Liu, Xiyu},
  doi          = {10.1007/s10489-020-01784-3},
  journal      = {Applied Intelligence},
  month        = {12},
  number       = {12},
  pages        = {4378-4393},
  shortjournal = {Appl. Intell.},
  title        = {Novel coupled DP system for fuzzy C-means clustering and image segmentation},
  volume       = {50},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A prediction strategy based on special points and
multiregion knee points for evolutionary dynamic multiobjective
optimization. <em>APIN</em>, <em>50</em>(12), 4357–4377. (<a
href="https://doi.org/10.1007/s10489-020-01772-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dynamic multiobjective optimization problems exist widely in the real word and require the optimization algorithms to track the Pareto front (PF) over time. A prediction strategy based on special points and multi-region knee points (MRKPs) is proposed for solving dynamic multiobjective optimization problems. Whenever a change is detected, the prediction strategy reacts effectively to the change by generating four subpopulations based on four strategies. The first subpopulation is created by selecting the representative individuals using a special point strategy. The second subpopulation consists of a solution set using a multiregion knee point strategy. The third subpopulation is introduced to the nondominated set by a convergence strategy. The fourth subpopulation comprises diverse individuals from an adaptive diversity maintenance strategy. The four subpopulations merge into a new population to accurately predict the location and distribution of the PF after an environmental change. MRKP is compared with four popular evolutionary algorithms on standard instances with different changing dynamics. Finally, MRKP provides better results than other competitors in terms of Inverted Generational Distance and Hypervolume metrics. The results reveal that MRKP can quickly adapt to changing environments and provide good tracking ability when dealing with dynamic multiobjective optimization problems.},
  archive      = {J_APIN},
  author       = {Wei, Lixin and Guo, Zeyin and Fan, Rui and Sun, Hao and Zhao, Zhiwei},
  doi          = {10.1007/s10489-020-01772-7},
  journal      = {Applied Intelligence},
  month        = {12},
  number       = {12},
  pages        = {4357-4377},
  shortjournal = {Appl. Intell.},
  title        = {A prediction strategy based on special points and multiregion knee points for evolutionary dynamic multiobjective optimization},
  volume       = {50},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A novel additive consistency for intuitionistic fuzzy
preference relations in group decision making. <em>APIN</em>,
<em>50</em>(12), 4342–4356. (<a
href="https://doi.org/10.1007/s10489-020-01796-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deriving the priority vectors of the alternatives from preference relations is an interesting research topic for group decision making with preference information. This paper uses an example to show that the ranking or the optimal alternative could not always be derived from the existing additively consistent intuitionistic fuzzy preference relations. Thus, we provide novel additively consistent intuitionistic fuzzy preference relations and characterize them with Tanino’s normalized (T-normalized) intuitionistic fuzzy priority vectors. Then, we propose some methods to check and achieve the T-normalization, acceptably additive consistency and consensus of the intuitionistic fuzzy preference relations in group decision making using the local, individual and optimal collective intuitionistic fuzzy priority vectors, respectively. We also give some examples to show how the proposed models work and make comparisons with the existing methods to demonstrate the advantages of the proposed methods.},
  archive      = {J_APIN},
  author       = {Yang, Wei and Jhang, Seong Tae and Shi, Shao Guang and Xu, Ze Shui and Ma, Zhen Ming},
  doi          = {10.1007/s10489-020-01796-z},
  journal      = {Applied Intelligence},
  month        = {12},
  number       = {12},
  pages        = {4342-4356},
  shortjournal = {Appl. Intell.},
  title        = {A novel additive consistency for intuitionistic fuzzy preference relations in group decision making},
  volume       = {50},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Learning stacking regressors for single image
super-resolution. <em>APIN</em>, <em>50</em>(12), 4325–4341. (<a
href="https://doi.org/10.1007/s10489-020-01787-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Example learning-based single image super-resolution (SR) technique has been widely recognized for its effectiveness in restoring a high-resolution (HR) image with finer details from a given low-resolution (LR) input. However, most popular approaches only choose one type of image features to learn the mapping relationship between LR and HR images, making it difficult to fit into the diversity of different natural images. In this paper, we propose a novel stacking learning-based SR framework by extracting both the gradient features and the texture features of images simultaneously to train two complementary models. Since the gradient features are helpful to represent the edge structures while the texture features are beneficial to restore the texture details, the newly proposed method cleverly combines the merits of two complementary features and makes the resultant HR images more faithful to their original counterparts. Moreover, we enhance the SR capacity by using a residual cascaded scheme to further reduce the gap between the super-resolved images and the corresponding original images. Experimental results carried out on seven benchmark datasets indicate that the proposed SR framework performs better than other seven state-of-the-art SR methods in both quantitative and qualitative quality assessments.},
  archive      = {J_APIN},
  author       = {Zhang, Kaibing and Luo, Shuang and Li, Minqi and Jing, Junfeng and Lu, Jian and Xiong, Zenggang},
  doi          = {10.1007/s10489-020-01787-0},
  journal      = {Applied Intelligence},
  month        = {12},
  number       = {12},
  pages        = {4325-4341},
  shortjournal = {Appl. Intell.},
  title        = {Learning stacking regressors for single image super-resolution},
  volume       = {50},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Probabilistic active filtering with gaussian processes for
occluded object search in clutter. <em>APIN</em>, <em>50</em>(12),
4310–4324. (<a
href="https://doi.org/10.1007/s10489-020-01789-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes a Gaussian process model-based probabilistic active learning approach for occluded object search in clutter. Due to heavy occlusions, an agent must be able to gradually reduce uncertainty during the observations of objects in its workspace by systematically rearranging them. In this work, we apply a Gaussian process to capture the uncertainties of both system dynamics and observation function. Robot manipulation is optimized by mutual information that naturally indicates the potential of moving one object to search for new objects based on the predicted uncertainties of two models. An active learning framework updates the state belief based on sensor observations. We validated our proposed method in a simulation robot task. The results demonstrate that with samples generated by random actions, the proposed method can learn intelligent object search behaviors while iteratively converging its predicted state to the ground truth.},
  archive      = {J_APIN},
  author       = {Cui, Yunduan and Ooga, Jun’ichiro and Ogawa, Akihito and Matsubara, Takamitsu},
  doi          = {10.1007/s10489-020-01789-y},
  journal      = {Applied Intelligence},
  month        = {12},
  number       = {12},
  pages        = {4310-4324},
  shortjournal = {Appl. Intell.},
  title        = {Probabilistic active filtering with gaussian processes for occluded object search in clutter},
  volume       = {50},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A hybrid stock price index forecasting model based on
variational mode decomposition and LSTM network. <em>APIN</em>,
<em>50</em>(12), 4296–4309. (<a
href="https://doi.org/10.1007/s10489-020-01814-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Changes in the composite stock price index are a barometer of social and economic development. To improve the accuracy of stock price index prediction, this paper introduces a new hybrid model, VMD-LSTM, that combines variational mode decomposition (VMD) and a long short-term memory (LSTM) network. The proposed model is based on decomposition-and-ensemble framework. VMD is a data-processing technique through which the original complex series can be decomposed into a limited number of subseries with relatively simple modes of fluctuations. It can effectively overcome the shortcomings of mode mixing that sometimes exist in the empirical mode decomposition (EMD) method. LSTM is an improved version of recurrent neural networks (RNNs) that introduces a “gate” mechanism, and can effectively filter out the critical previous information, making it suitable for the financial time series forecasting. The capability of VMD-LSTM in stock price index forecasting is verified comprehensively by comparing with some single models and the EMD-based and other VMD-based hybrid models. Evaluated by level and directional prediction criteria, as well as a newly introduced statistic called the complexity-invariant distance (CID), the VMD-LSTM model shows an outstanding performance in stock price index forecasting. The hybrid models perform significantly better than the single models, and the forecasting accuracy of the VMD-based models is generally higher than that of the EMD-based models.},
  archive      = {J_APIN},
  author       = {Niu, Hongli and Xu, Kunliang and Wang, Weiqing},
  doi          = {10.1007/s10489-020-01814-0},
  journal      = {Applied Intelligence},
  month        = {12},
  number       = {12},
  pages        = {4296-4309},
  shortjournal = {Appl. Intell.},
  title        = {A hybrid stock price index forecasting model based on variational mode decomposition and LSTM network},
  volume       = {50},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Dynamic knowledge graph based fake-review detection.
<em>APIN</em>, <em>50</em>(12), 4281–4295. (<a
href="https://doi.org/10.1007/s10489-020-01761-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Online product reviews are an important driver of customers’ purchasing behavior. Fake reviews seriously mislead consumers, challenging the fairness of the online shopping environment. Although the detection of fake reviews has progressed, several problems remain. First, fake comment recognition ignores the correlation between time and the semantics of the comment texts, which is always hidden in the context of the reviews. Second, the impact of multi-source information on fake comment recognition is not considered, as it constitutes a complex, high-dimensional, heterogeneous relationship between reviewers, reviews, stores and commodities. To overcome these problems, the present paper proposes a dynamic knowledge graph-based method for fake-review detection. Based on the characteristics of online product reviews, it first extracts four types of entities using a developed neural network model called sentence vector/twin-word embedding conditioned bidirectional long short-term memory. Time series related features are then added to the knowledge graph construction process, forming dynamic graph networks. To enhance the fake-review detection, four indicators are newly defined for determining the relationships among the four types of nodes. In experimental evaluations, our method surpassed the state-of-the-art results.},
  archive      = {J_APIN},
  author       = {Fang, Youli and Wang, Hong and Zhao, Lili and Yu, Fengping and Wang, Caiyu},
  doi          = {10.1007/s10489-020-01761-w},
  journal      = {Applied Intelligence},
  month        = {12},
  number       = {12},
  pages        = {4281-4295},
  shortjournal = {Appl. Intell.},
  title        = {Dynamic knowledge graph based fake-review detection},
  volume       = {50},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Cycle representation-disentangling network: Learning to
completely disentangle spatial-temporal features in video.
<em>APIN</em>, <em>50</em>(12), 4261–4280. (<a
href="https://doi.org/10.1007/s10489-020-01750-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Video representation learning is a significant problem of video understanding. However, the complex entangled spatiotemporal information in frames makes video representation learning a very tough task. Many studies have been conducted on decomposing video representation into dynamic and static features; existing works use either a two-stream structure or a compression strategy to learn the static and motion features from videos. However, neither approach can guarantee that networks learn features with low coupling degrees. To address this problem, we propose the exchangeable property (EP), a constraint that encourages networks to learn disentangled features from videos. Based on the EP, we propose a novel network called the Cycle Representation-Disentangling Network (CRD-Net), which adopts the strategy of exchanging features and reconstructing videos to factorize videos into stationary and temporal varying components. CRD-Net adopts a new training paradigm, as it is trained on paired videos with different static features but similar dynamic features. In addition, we introduce the pair loss and cycle loss, which forces the motion encoder to abandon time-invariant features and the consistent loss, which forces the static encoder to abandon time-variant features in the whole video. In experiments, we show the advantages of CRD-Net in completely disentangling video features and obtain better results than the state of the art on several video understanding tasks.},
  archive      = {J_APIN},
  author       = {Sun, Pengfei and Su, Xin and Guo, Shangqi and Chen, Feng},
  doi          = {10.1007/s10489-020-01750-z},
  journal      = {Applied Intelligence},
  month        = {12},
  number       = {12},
  pages        = {4261-4280},
  shortjournal = {Appl. Intell.},
  title        = {Cycle representation-disentangling network: Learning to completely disentangle spatial-temporal features in video},
  volume       = {50},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). MR-OVnTSA: A heuristics based sensitive pattern hiding
approach for big data. <em>APIN</em>, <em>50</em>(12), 4241–4260. (<a
href="https://doi.org/10.1007/s10489-020-01749-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a novel ‘MapReduce Based Optimum Victim Item and Transaction Selection Approach (MR-OVnTSA)’ that provides a feasible and intelligent solution for protecting sensitive frequent itemsets present in big data. The approach advocates to resolve the captious challenges, existing knowledge hiding algorithms are encountering. The proposed solution optimally minimizes the side effect of hiding process on non-sensitive information, and maintains a balance between knowledge and privacy as well as handles the exponential growth in data volume efficiently. The algorithm plugs the most optimum item and transaction as victim, by intelligently analyzing their coverage value i.e. it chooses one with maximal impact on sensitive knowledge but minimal on non-sensitive information. Further, the MapReduce version of the proposed scheme resolves the issue of non-feasibility by processing large-scale data (big data) in a parallel fashion. Experiments have been demonstrated over real and synthetically generated large-scale datasets. Results evince that the proffered scheme is much more efficient and maintains the balance between the privacy preservation, data quality maintenance, and CPU time, when dealing with large voluminous big datasets compared to existing knowledge hiding techniques.},
  archive      = {J_APIN},
  author       = {Sharma, Shivani and Toshniwal, Durga},
  doi          = {10.1007/s10489-020-01749-6},
  journal      = {Applied Intelligence},
  month        = {12},
  number       = {12},
  pages        = {4241-4260},
  shortjournal = {Appl. Intell.},
  title        = {MR-OVnTSA: A heuristics based sensitive pattern hiding approach for big data},
  volume       = {50},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A multi-objective evolutionary approach for the nonlinear
scale-free level problem. <em>APIN</em>, <em>50</em>(12), 4223–4240. (<a
href="https://doi.org/10.1007/s10489-020-01788-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Procedural Content Generation in games aims to produce replayable levels that might adapt to the player or designer preferences. One of the most popular approaches adopted for search-based level generation is the use of Evolutionary Algorithms. This paper addresses the context of dungeon generation and divide the problem in two steps: the first consists of generating the physical level, while the second is to create the puzzle. Although there are several approaches in the related literature for generating levels and puzzles, few are successful at generating unpredictable or diverse levels. The objective of this work is to propose a Multi-objective Evolutionary approach for generating the level and creating the puzzle providing a wide range of unpredictable and diverse solutions with different level dimensions. To achieve this goal, the level is modeled as scale-free topology with a nonlinear resolution. This model avoids the generation of linear, repetitive and grid-like levels, giving the algorithm additional freedom to explore the search space for diverse solutions. Four classical well-known evolutionary algorithms (SPEA2, PAES, NSGA-II and MOCell) were applied to both sub-problems. To analyze the results, we consider a set of quality indicators covering both convergence and diversity. Our results indicated that, the proposed multi-objective formulation was able to provide a wide range of satisfactory and diverse solutions with different level dimensions, independently of the algorithm used. Thus, the main contribution of this work is that level designers can choose between different solutions, based on solution properties and/or designer priorities.},
  archive      = {J_APIN},
  author       = {Ruela, André Siqueira and Delgado, Karina Valdivia and Bernardes, João},
  doi          = {10.1007/s10489-020-01788-z},
  journal      = {Applied Intelligence},
  month        = {12},
  number       = {12},
  pages        = {4223-4240},
  shortjournal = {Appl. Intell.},
  title        = {A multi-objective evolutionary approach for the nonlinear scale-free level problem},
  volume       = {50},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Multi-objective decomposition optimization algorithm based
on adaptive weight vector and matching strategy. <em>APIN</em>,
<em>50</em>(12), 4206–4222. (<a
href="https://doi.org/10.1007/s10489-020-01771-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-objective evolutionary algorithm based on decomposition (MOEA/D) uses pre-set weight vector and random matching mechanism between sub-problems and individuals, which makes the algorithm simple and efficient. However, when solving the problem of discontinuous Pareto Front, this will lead to not only the decline of the diversity of population, but also the degradation of the performance of solution set. To solve these problems, a multi-objective decomposition optimization algorithm based on adaptive weight vector and matching strategy (MOEA/D-AVM) is proposed in the article. Firstly, the algorithm finds the invalid sub-problems in the discontinuous region, and then updates these sub-problems according to the current evolutionary stage. It can reduce the possibility that the invalid sub-problems mislead evolutionary process. Secondly, the matching mechanism is established according to the value of penalty-based boundary intersection (PBI) and the Euclidean distance between the sub-problems and individuals. This mechanism can enhance the relationship between individuals and sub-problems. Finally, individuals who perform well in the neighborhood replacement operation are saved in the external archive. It can improve the diversity of the optimal solution set obtained by the algorithm. The proposed algorithm is compared with other related algorithms in the standard test problem. The result shows that the solution set obtained by MOEA/D-AVM not only can better cover the Pareto Front, but also has a competitive performance in solving the problem of discontinuous Pareto Front.},
  archive      = {J_APIN},
  author       = {Li, Erchao and Chen, Ruiting},
  doi          = {10.1007/s10489-020-01771-8},
  journal      = {Applied Intelligence},
  month        = {12},
  number       = {12},
  pages        = {4206-4222},
  shortjournal = {Appl. Intell.},
  title        = {Multi-objective decomposition optimization algorithm based on adaptive weight vector and matching strategy},
  volume       = {50},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). GAMA: Graph attention multi-agent reinforcement learning
algorithm for cooperation. <em>APIN</em>, <em>50</em>(12), 4195–4205.
(<a href="https://doi.org/10.1007/s10489-020-01755-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-agent reinforcement learning (MARL) is an important way to realize multi-agent cooperation. But there are still many challenges, including the scalability and the uncertainty of the environment that limit its application. In this paper, we explored to solve those problems through the graph network and the attention mechanism. Finally we succeeded in extending the existing algorithm and obtaining a new algorithm called GAMA. Specifically through the graph network, we made the environment information shared among agents. Meanwhile, the unimportant information was filtered out with the help of the attention mechanism, which helped to improve the communication efficiency. As a result, GAMA obtained the highest mean episode rewards compared to the baselines as well as excellent scalability. The reason why we choose the graph network is that understanding the relationship among agents plays a key role in solving multi-agent problems. And the graph network is very suitable for relational induction bias. Through the integration with the attention mechanism, it was shown that agents could figure out their relationship and focus on the influential environment factors in our experiment.},
  archive      = {J_APIN},
  author       = {Chen, Haoqiang and Liu, Yadong and Zhou, Zongtan and Hu, Dewen and Zhang, Ming},
  doi          = {10.1007/s10489-020-01755-8},
  journal      = {Applied Intelligence},
  month        = {12},
  number       = {12},
  pages        = {4195-4205},
  shortjournal = {Appl. Intell.},
  title        = {GAMA: Graph attention multi-agent reinforcement learning algorithm for cooperation},
  volume       = {50},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A derived least square fast learning network model.
<em>APIN</em>, <em>50</em>(12), 4176–4194. (<a
href="https://doi.org/10.1007/s10489-020-01773-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The extreme learning machine (ELM) requires a large number of hidden layer nodes in the training process. Thus, random parameters will exponentially increase and affect network stability. Moreover, the single activation function affects the generalization capability of the network. This paper proposes a derived least square fast learning network (DLSFLN) to solve the aforementioned problems. DLSFLN uses the inheritance of some functions to obtain various activation functions through continuous differentiation of functions. The types of activation functions were increased and the mapping capability of hidden layer neurons was enhanced when the random parameter dimension was maintained. DLSFLN randomly generates the input weights and hidden layer thresholds and uses the least square method to determine the connection weights between the output and the input layers and that between the output and the input nodes. The regression and classification experiments show that DLSFLN has a faster training speed and better training accuracy, generalization capability, and stability compared with other neural network algorithms, such as fast learning network(FLN).},
  archive      = {J_APIN},
  author       = {Wang, Meiqi and Jia, Sixian and Chen, Enli and Yang, Shaopu and Liu, Pengfei and Qi, Zhuang},
  doi          = {10.1007/s10489-020-01773-6},
  journal      = {Applied Intelligence},
  month        = {12},
  number       = {12},
  pages        = {4176-4194},
  shortjournal = {Appl. Intell.},
  title        = {A derived least square fast learning network model},
  volume       = {50},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Optimal local dimming based on an improved greedy algorithm.
<em>APIN</em>, <em>50</em>(12), 4162–4175. (<a
href="https://doi.org/10.1007/s10489-020-01769-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As a new technology appeared in recent years, the local dimming can effectively reduce the power consumption of a display system and improve its display effect. A suitable local dimming algorithm should have efficient performance and can make the displayed images have higher visual quality. However, most of the existing local dimming methods can not have both of the above advantages. In this paper, the local dimming is taken as an optimization problem. On the basis of our previous work which focuses on reducing the image distortion and power consumption, image contrast ratio which is another important factor of visual quality is also considered. To improve the running efficiency of local dimming, the Greedy Algorithm (GRA) which is one of the simplest heuristic algorithms is used to design the local dimming algorithm. In order to improve the global optimization ability of the GRA, an Improved Greedy Algorithm(IGRA) based on the strategies of Taking out-Putting in and variable search step size is proposed. Experienced in four different types of images and compared with five parameter-based algorithms, the IGRA can obtain a higher visual quality under the same or lower power consumption. It is also proved that the IGRA has more powerful search ability and higher running efficiency by the comparisons with the Improved Shuffled Frog Leaping Algorithm (ISFLA) proposed in our previous work, and two recent algorithms including the Modified Genetic Algorithm (MGA) and the Improved Particle Swarm Optimization (IPSO).},
  archive      = {J_APIN},
  author       = {Zhang, Tao and Zeng, Qin and Zhao, Xin},
  doi          = {10.1007/s10489-020-01769-2},
  journal      = {Applied Intelligence},
  month        = {12},
  number       = {12},
  pages        = {4162-4175},
  shortjournal = {Appl. Intell.},
  title        = {Optimal local dimming based on an improved greedy algorithm},
  volume       = {50},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A modified brain storm optimization algorithm with a special
operator to solve constrained optimization problems. <em>APIN</em>,
<em>50</em>(12), 4145–4161. (<a
href="https://doi.org/10.1007/s10489-020-01763-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a novel approach based on the combination of the Modified Brain Storm Optimization algorithm (MBSO) with a simplified version of the Constraint Consensus method as special operator to solve constrained numerical optimization problems. Regarding the special operator, which aims to reach the feasible region of the search space, the consensus vector becomes the feasibility vector computed by the hardest constraint in turn for a current infeasible solution; then the operations to mix the other feasibility vectors are avoided. This new combined algorithm, named as MBSO-R+V, solves a suit of eighteen test problems in ten and thirty dimensions. From a set of experiments related to the location and frequency of application of the constraint consensus method within MBSO, a suitable design of the combined approach is presented. This proposal shows encouraging final results while being compared against state-of-the-art algorithms, showing that it is viable to add special operators to improve the capabilities of swarm-intelligence algorithms when dealing with continuous constrained search spaces.},
  archive      = {J_APIN},
  author       = {Cervantes-Castillo, Adriana and Mezura-Montes, Efrén},
  doi          = {10.1007/s10489-020-01763-8},
  journal      = {Applied Intelligence},
  month        = {12},
  number       = {12},
  pages        = {4145-4161},
  shortjournal = {Appl. Intell.},
  title        = {A modified brain storm optimization algorithm with a special operator to solve constrained optimization problems},
  volume       = {50},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Correction to: Facial expression recognition sensing the
complexity of testing samples. <em>APIN</em>, <em>50</em>(11),
4143–4144. (<a
href="https://doi.org/10.1007/s10489-020-01709-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The original version of this article unfortunately contained a mistake. Graphs c, d and e are missing in Figure 4. The correct and complete graphs of Figure 4 is shown here.},
  archive      = {J_APIN},
  author       = {Chang, Tianyuan and Li, Huihui and Wen, Guihua and Hu, Yang and Ma, Jiajiong},
  doi          = {10.1007/s10489-020-01709-0},
  journal      = {Applied Intelligence},
  month        = {11},
  number       = {11},
  pages        = {4143-4144},
  shortjournal = {Appl. Intell.},
  title        = {Correction to: Facial expression recognition sensing the complexity of testing samples},
  volume       = {50},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). An interpretable regression approach based on bi-sparse
optimization. <em>APIN</em>, <em>50</em>(11), 4117–4142. (<a
href="https://doi.org/10.1007/s10489-020-01687-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Given the increasing amounts of data and high feature dimensionalities in forecasting problems, it is challenging to build regression models that are both computationally efficient and highly accurate. Moreover, regression models commonly suffer from low interpretability when using a single kernel function or a composite of multi-kernel functions to address nonlinear fitting problems. In this paper, we propose a bi-sparse optimization-based regression (BSOR) model and corresponding algorithm with reconstructed row and column kernel matrices in the framework of support vector regression (SVR). The BSOR model can predict continuous output values for given input points while using the zero-norm regularization method to achieve sparse instance and feature sets. Experiments were run on 16 datasets to compare BSOR to SVR, linear programming SVR (LPSVR), least squares SVR (LSSVR), multi-kernel learning SVR (MKLSVR), least absolute shrinkage and selection operator regression (LASSOR), and relevance vector regression (RVR). BSOR significantly outperformed the other six regression models in predictive accuracy, identification of the fewest representative instances, selection of the fewest important features, and interpretability of results, apart from its slightly high runtime.},
  archive      = {J_APIN},
  author       = {Zhang, Zhiwang and Gao, Guangxia and Yao, Tao and He, Jing and Tian, Yingjie},
  doi          = {10.1007/s10489-020-01687-3},
  journal      = {Applied Intelligence},
  month        = {11},
  number       = {11},
  pages        = {4117-4142},
  shortjournal = {Appl. Intell.},
  title        = {An interpretable regression approach based on bi-sparse optimization},
  volume       = {50},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). NetDAP: (Δ, γ) −approximate pattern matching with length
constraints. <em>APIN</em>, <em>50</em>(11), 4094–4116. (<a
href="https://doi.org/10.1007/s10489-020-01778-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Pattern matching(PM) with gap constraints has been applied to compute the support of a pattern in a sequence, which is an essential task of the repetitive sequential pattern mining (or sequence pattern mining). Compared with exact PM, approximate PM allows data noise (differences) between the pattern and the matched subsequence. Therefore, more valuable patterns can be found. Approximate PM with gap constraints mainly adopts the Hamming distance to measure the approximation degree which only reflects the number of different characters between two sequences, but ignores the distance between different characters. Hence, this paper addresses (δ, γ) approximate PM with length constraints which employs local-global constraints to improve the accuracy of the PM, namely, the maximal distance between two corresponding characters is less or equal to the local threshold δ, and the sum of all the δ distances is also less or equal to the global threshold γ. To tackle the problem effectively, this paper proposes an effective online algorithm, named NetDAP, which employs a special designed data structure named approximate single-leaf Nettree. An approximate single-leaf Nettree can be created by adopting dynamic programming to determine the range of rootleaf, the minimal root, the maximal root, the range of nodes for each level, and the range of parents for each node. To improve the performance, two pruning strategies are proposed to prune the nodes and the parent-child relationships which do not satisfy the δ and γ distance constraints respectively. Finally, extensive experimental results on real protein data sets and time series verify the performance of the proposed algorithm.},
  archive      = {J_APIN},
  author       = {Wu, Youxi and Fan, Jinquan and Li, Yan and Guo, Lei and Wu, Xindong},
  doi          = {10.1007/s10489-020-01778-1},
  journal      = {Applied Intelligence},
  month        = {11},
  number       = {11},
  pages        = {4094-4116},
  shortjournal = {Appl. Intell.},
  title        = {NetDAP: (δ, γ) −approximate pattern matching with length constraints},
  volume       = {50},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Data-driven multi-attribute decision-making by combining
probability distributions based on compatibility and entropy.
<em>APIN</em>, <em>50</em>(11), 4081–4093. (<a
href="https://doi.org/10.1007/s10489-020-01738-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-attribute decision-making has many applications in different fields. How to make decisions objectively when there are many attributes is still an open issue. This paper proposes a data-driven multi-attribute decision-making method considering the compatibility and entropy. Mainly, data of different decision attributes are normalized to probability distributions. The compatibility weight and entropy weight are computed respectively and then combined to a final weight. The scores of decision objects are derived by combining weighted probability distributions. In order to verify the effectiveness of the proposed method, two examples are given to compare with the AHP method and an improved data envelopment analysis method respectively. The former results show that the proposed method can obtain more objective results and produce a low computation complexity. The latter demonstrate the proposed method focuses more on the overall performance of decision attributes while the improved data envelopment analysis emphasises more on the ecological performance.},
  archive      = {J_APIN},
  author       = {Zhang, Hengqi and Jiang, Wen and Deng, Xinyang},
  doi          = {10.1007/s10489-020-01738-9},
  journal      = {Applied Intelligence},
  month        = {11},
  number       = {11},
  pages        = {4081-4093},
  shortjournal = {Appl. Intell.},
  title        = {Data-driven multi-attribute decision-making by combining probability distributions based on compatibility and entropy},
  volume       = {50},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). An effective multi-level synchronization clustering method
based on a linear weighted vicsek model. <em>APIN</em>, <em>50</em>(11),
4063–4080. (<a
href="https://doi.org/10.1007/s10489-020-01767-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To conquer the shortcoming that general clustering methods cannot process big data in the main memory, this paper presents an effective multi-level synchronization clustering (MLSynC) method by using a framework of “divide and collect” and a linear weighted Vicsek model. We also introduce two concrete implementations of MLSynC method, a two-level framework algorithm and a recursive algorithm. MLSynC method has a different process with SynC algorithm, ESynC algorithm and SSynC algorithm. By the theoretic analysis, we find the time complexity of MLSynC method is less than SSynC. Simulation and experimental study on multi-kinds of data sets validate that MLSynC method not only gets better local synchronization effect but also needs less iterative times and time cost than SynC algorithm. Moreover, we observe that MLSynC method not only needs less time cost than ESynC and SSynC, but also almost gets the same local synchronization effect as ESynC and SSynC if the partition of the data set is proper. Further comparison experiments with some classical clustering algorithms demonstrate the clustering effect of MLSynC method.},
  archive      = {J_APIN},
  author       = {Chen, Xinquan and Qiu, Yirou},
  doi          = {10.1007/s10489-020-01767-4},
  journal      = {Applied Intelligence},
  month        = {11},
  number       = {11},
  pages        = {4063-4080},
  shortjournal = {Appl. Intell.},
  title        = {An effective multi-level synchronization clustering method based on a linear weighted vicsek model},
  volume       = {50},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Asynchronous framework with reptile+ algorithm to meta learn
partially observable markov decision process. <em>APIN</em>,
<em>50</em>(11), 4050–4062. (<a
href="https://doi.org/10.1007/s10489-020-01748-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Meta-learning has recently received much attention in a wide variety of deep reinforcement learning (DRL). In non-meta-learning, we have to train a deep neural network as a controller to learn a specific control task from scratch using a large amount of data. This way of training has shown many limitations in handling different related tasks. Therefore, meta-learning on control domains becomes a powerful tool for transfer learning on related tasks. However, it is widely known that meta-learning requires massive computation and training time. This paper will propose a novel DRL framework, which is called HCGF-R2-DDPG (Hybrid CPU/GPU Framework for Reptile+ and Recurrent Deep Deterministic Policy Gradient). HCGF-R2-DDPG will integrate meta-learning into a general asynchronous training architecture. The proposed framework will allow utilising both CPU and GPU to boost the training speed for the meta network initialisation. We will evaluate HCGF-R2-DDPG on various Partially Observable Markov Decision Process (POMDP) domains.},
  archive      = {J_APIN},
  author       = {Nguyen, Dang Quang and Vien, Ngo Anh and Dang, Viet-Hung and Chung, TaeChoong},
  doi          = {10.1007/s10489-020-01748-7},
  journal      = {Applied Intelligence},
  month        = {11},
  number       = {11},
  pages        = {4050-4062},
  shortjournal = {Appl. Intell.},
  title        = {Asynchronous framework with reptile+ algorithm to meta learn partially observable markov decision process},
  volume       = {50},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Joint label-specific features and label correlation for
multi-label learning with missing label. <em>APIN</em>, <em>50</em>(11),
4029–4049. (<a
href="https://doi.org/10.1007/s10489-020-01715-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing multi-label learning classification algorithms ignore that class labels may be determined by some features in the original feature space. And only a partial label of each instance can be obtained for some real applications. Therefore, we propose a novel algorithm named joint Label-Specific features and Label Correlation for multi-label learning with Missing Label (LSLC-ML) and its optimized version to solve the above-mentioned problems. First, a missing label can be recovered by the learned positive and negative label correlations from the incomplete training data sets, then the label-specific features can be selected, finally the multi-label classification task can be modeled by combining the labelspecific feature selections, missing labels and positive and negative label correlations. The experimental results show that our algorithm LSLC-ML has strong competitiveness compared with some state-of-the-art algorithms in evaluation matrices when tested on benchmark multi-label data sets.},
  archive      = {J_APIN},
  author       = {Cheng, Ziwei and Zeng, Ziwei},
  doi          = {10.1007/s10489-020-01715-2},
  journal      = {Applied Intelligence},
  month        = {11},
  number       = {11},
  pages        = {4029-4049},
  shortjournal = {Appl. Intell.},
  title        = {Joint label-specific features and label correlation for multi-label learning with missing label},
  volume       = {50},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Content-aware web robot detection. <em>APIN</em>,
<em>50</em>(11), 4017–4028. (<a
href="https://doi.org/10.1007/s10489-020-01754-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Web crawlers account for more than a third of the total web traffic and they are threatening the security, privacy and veracity of web applications and their users. Businesses in finance, ticketing, and publishing, as well as websites with rich and unique content are the ones mostly affected by their actions. To deal with this problem, we present a novel web robot detection approach that takes advantage of the content of a website based on the assumption that human web users are interested in specific topics, while web robots crawl the web randomly. Our approach extends the typical user session representation of log-based features with a novel set of features that capture the semantics of the content of the requested resources. In addition, we contribute a new real-world dataset, which we make publicly available, towards alleviating the scarcity of open data in this field. Empirical results on this dataset validate our assumption and show that our approach outranks state-of-the-art methods for web robot detection.},
  archive      = {J_APIN},
  author       = {Lagopoulos, Athanasios and Tsoumakas, Grigorios},
  doi          = {10.1007/s10489-020-01754-9},
  journal      = {Applied Intelligence},
  month        = {11},
  number       = {11},
  pages        = {4017-4028},
  shortjournal = {Appl. Intell.},
  title        = {Content-aware web robot detection},
  volume       = {50},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Economic data analytic AI technique on IoT edge devices for
health monitoring of agriculture machines. <em>APIN</em>,
<em>50</em>(11), 3990–4016. (<a
href="https://doi.org/10.1007/s10489-020-01744-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the era of Internet of things (IoT), network Connection of an enormous number of agriculture machines and service centers is an expectation. However, it will be with a generation of massive volume of data, thus overwhelming the network traffic and storage system especially when manufacturers give maintenance service typically by various data analytic applications on the cloud. The situation is more complex in the context of low latency applications such as health monitoring of agriculture machines, although require emergency responses. Performing the computational intelligence on edge devices is one of the best approaches in developing green communications and managing the blast of network traffic. Due to the increasing usage of smartphone applications, the edge computation on the smartphone can highly assist the network traffic management. In connection with the mentioned point, in the context of exploiting the limited computation power of smartphones, the design of an AI-based data analytic technique is a challenging task. On the other hand, the users’ need for economic technology makes it not to be easily pierced. This research work aims both targets by presenting a bi-level genetic algorithm approach of an optimized data analytic AI technique for monitoring the health of the agriculture vehicles which can be economically utilized on smartphone end-devices using the built-in microphones instead of expensive IoT sensors.},
  archive      = {J_APIN},
  author       = {Gupta, Neeraj and Khosravy, Mahdi and Patel, Nilesh and Dey, Nilanjan and Gupta, Saurabh and Darbari, Hemant and Crespo, Rubén González},
  doi          = {10.1007/s10489-020-01744-x},
  journal      = {Applied Intelligence},
  month        = {11},
  number       = {11},
  pages        = {3990-4016},
  shortjournal = {Appl. Intell.},
  title        = {Economic data analytic AI technique on IoT edge devices for health monitoring of agriculture machines},
  volume       = {50},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). FLGAI: A unified network embedding framework integrating
multi-scale network structures and node attribute information.
<em>APIN</em>, <em>50</em>(11), 3976–3989. (<a
href="https://doi.org/10.1007/s10489-020-01780-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Network embedding is an effective method aiming to learn the low-dimensional vector representation of nodes in networks, which has been widely used in various network analytic tasks such as node classification, node clustering, and link prediction. The objective of network embedding is to capture the structural information and inherent characteristics of the network as much as possible in the low-dimensional vector representation. However, the majority of the existing network embedding methods merely exploited the microscopic proximity of the network structure to learn the node representation, which tend to generate sub-optimal network representation. In this paper, we propose a novel nonnegative matrix factorization (NMF) based network representation learning framework called FLGAI, which jointly integrates the local network structure, global network structure, and attribute information to learn the network representation. First, we employ the first-order proximity and second-order proximity jointly to preserve the local network structure. Then, the community structure is introduced to preserve the global network structure. Third, we exploit the node attribute information to capture the node characteristics. To preserve the structural information and the network node attributes simultaneously, we formulate their consensus relationships and optimize them jointly in a unified NMF framework to derive the final network representation. To evaluate the effectiveness of our model, we conduct extensive experiments on six real-world datasets and the empirical results demonstrate the superior performance of the proposed method over the state-of-the-art approaches in both node classification and node clustering tasks.},
  archive      = {J_APIN},
  author       = {Pan, Yu and Hu, Guyu and Qiu, Junyang and Zhang, Yanyan and Wang, Shuaihui and Shao, Dongsheng and Pan, Zhisong},
  doi          = {10.1007/s10489-020-01780-7},
  journal      = {Applied Intelligence},
  month        = {11},
  number       = {11},
  pages        = {3976-3989},
  shortjournal = {Appl. Intell.},
  title        = {FLGAI: A unified network embedding framework integrating multi-scale network structures and node attribute information},
  volume       = {50},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). An adaptive GP-based memetic algorithm for symbolic
regression. <em>APIN</em>, <em>50</em>(11), 3961–3975. (<a
href="https://doi.org/10.1007/s10489-020-01745-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Symbolic regression is a process to find a mathematical expression that represents the relationship between a set of explanatory variables and a measured variable. It has become a best-known problem for GP (genetic programming), as GP can use the tree representation to represent solutions as expression trees. Since the success of memetic algorithms (MAs (Memetic algorithms (MAs) can be regarded as a class of methods that combine population-based global search and local search [6, 30])) has proved the importance of local search in augmenting the global search ability of GP, GP with local search is investigated to solve symbolic regression tasks in this work. An important design issue of MAs is the balance between the global exploration of GP and the local exploitation, which has a great influence on the performance and efficiency of MAs. This work proposes a GP-based memetic algorithm for symbolic regression, termed as aMeGP (a daptive Me metic GP), which can balance global exploration and local exploitation adaptively. Compared with GP, two improvements are made in aMeGP to invoke and stop local search adaptively during evolution. The proposed aMeGP is compared with GP-based and nonGP-based symbolic regression methods on both benchmark test functions and real-world applications. The results show that aMeGP is generally better than both GP-based and nonGP-based reference methods with its evolved solutions achieving lower root mean square error (RMSE) for most test cases. Moreover, aMeGP outperforms the reference GP-based methods in the convergence ability, which can converge to lower RMSE values with faster or similar speeds.},
  archive      = {J_APIN},
  author       = {Liang, Jiayu and Xue, Yu},
  doi          = {10.1007/s10489-020-01745-w},
  journal      = {Applied Intelligence},
  month        = {11},
  number       = {11},
  pages        = {3961-3975},
  shortjournal = {Appl. Intell.},
  title        = {An adaptive GP-based memetic algorithm for symbolic regression},
  volume       = {50},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Multi-objective traveling salesman problem: An ABC approach.
<em>APIN</em>, <em>50</em>(11), 3942–3960. (<a
href="https://doi.org/10.1007/s10489-020-01713-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Using the concept of swap operation and swap sequence on the sequence of paths of a Traveling Salesman Problem(TSP) Artificial Bee Colony (ABC) algorithm is modified to solve multi-objective TSP. The fitness of a solution is determined using a rule following the dominance property of a multi-objective optimization problem. This fitness is used for the selection process of the onlooker bee phase of the algorithm. A set of rules is used to improve the solutions in each phase of the algorithm. Rules are selected according to their performance using the roulette wheel selection process. At the end of each iteration, the parent solution set and the solution sets after each phase of the ABC algorithm are combined to select a new solution set for the next iteration. The combined solution set is divided into different non-dominated fronts and then a new solution set, having cardinality of parent solution set, is selected from the upper-level non-dominated fronts. When some solutions are required to select from a particular front then crowding distances between the solutions of the front are measured and the isolated solutions are selected for the preservation of diversity. Different standard performance metrics are used to test the performance of the proposed approach. Different sizes standard benchmark test problems from TSPLIB are used for the purpose. Test results show that the proposed approach is efficient enough to solve multi-objective TSP.},
  archive      = {J_APIN},
  author       = {Khan, Indadul and Maiti, Manas Kumar and Basuli, Krishnendu},
  doi          = {10.1007/s10489-020-01713-4},
  journal      = {Applied Intelligence},
  month        = {11},
  number       = {11},
  pages        = {3942-3960},
  shortjournal = {Appl. Intell.},
  title        = {Multi-objective traveling salesman problem: An ABC approach},
  volume       = {50},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Transient search optimization: A new meta-heuristic
optimization algorithm. <em>APIN</em>, <em>50</em>(11), 3926–3941. (<a
href="https://doi.org/10.1007/s10489-020-01727-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article offers a new physical-based meta-heuristic optimization algorithm, which is named Transient Search Optimization (TSO) algorithm. This algorithm is inspired by the transient behavior of switched electrical circuits that include storage elements such as inductance and capacitance. The exploration and exploitation of the TSO algorithm are verified by using twenty-three benchmark, where its statistical (average and standard deviation) results are compared with the most recent 15 optimization algorithms. Furthermore, the non-parametric sign test, p value test, execution time, and convergence curves proved the superiority of the TSO against other algorithms. Also, the TSO algorithm is applied for the optimal design of three well-known constrained engineering problems (coil spring, welded beam, and pressure vessel). In conclusion, the comparison revealed that the TSO is promising and very competitive algorithm for solving different engineering problems.},
  archive      = {J_APIN},
  author       = {Qais, Mohammed H. and Hasanien, Hany M. and Alghuwainem, Saad},
  doi          = {10.1007/s10489-020-01727-y},
  journal      = {Applied Intelligence},
  month        = {11},
  number       = {11},
  pages        = {3926-3941},
  shortjournal = {Appl. Intell.},
  title        = {Transient search optimization: A new meta-heuristic optimization algorithm},
  volume       = {50},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A review of mathematical modeling, artificial intelligence
and datasets used in the study, prediction and management of COVID-19.
<em>APIN</em>, <em>50</em>(11), 3913–3925. (<a
href="https://doi.org/10.1007/s10489-020-01770-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the past few months, several works were published in regards to the dynamics and early detection of COVID-19 via mathematical modeling and Artificial intelligence (AI). The aim of this work is to provide the research community with comprehensive overview of the methods used in these studies as well as a compendium of available open source datasets in regards to COVID-19. In all, 61 journal articles, reports, fact sheets, and websites dealing with COVID-19 were studied and reviewed. It was found that most mathematical modeling done were based on the Susceptible-Exposed-Infected-Removed (SEIR) and Susceptible-infected-recovered (SIR) models while most of the AI implementations were Convolutional Neural Network (CNN) on X-ray and CT images. In terms of available datasets, they include aggregated case reports, medical images, management strategies, healthcare workforce, demography, and mobility during the outbreak. Both Mathematical modeling and AI have both shown to be reliable tools in the fight against this pandemic. Several datasets concerning the COVID-19 have also been collected and shared open source. However, much work is needed to be done in the diversification of the datasets. Other AI and modeling applications in healthcare should be explored in regards to this COVID-19.},
  archive      = {J_APIN},
  author       = {Mohamadou, Youssoufa and Halidou, Aminou and Kapen, Pascalin Tiam},
  doi          = {10.1007/s10489-020-01770-9},
  journal      = {Applied Intelligence},
  month        = {11},
  number       = {11},
  pages        = {3913-3925},
  shortjournal = {Appl. Intell.},
  title        = {A review of mathematical modeling, artificial intelligence and datasets used in the study, prediction and management of COVID-19},
  volume       = {50},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Modeling learner’s dynamic knowledge construction procedure
and cognitive item difficulty for knowledge tracing. <em>APIN</em>,
<em>50</em>(11), 3894–3912. (<a
href="https://doi.org/10.1007/s10489-020-01756-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Knowledge tracing (KT) is essential for adaptive learning to obtain learners’ current states of knowledge for the purpose of providing adaptive service. Generally, the knowledge construction procedure is constantly evolving because students dynamically learn and forget over time. Unfortunately, to the best of our knowledge most existing approaches consider only a fragment of the information that relates to learning or forgetting, and the problem of making use of rich information during learners’ learning interactions to achieve more precise prediction of learner performance in KT remains under-explored. Moreover, existing work either neglects the problem difficulty or assumes that it is constant, and this is unrealistic in the actual learning process as problem difficulty affects performance undoubtedly and also varies overtime in terms of the cognitive challenge it presents to individual learners. To this end, we herein propose a novel model, KTM-DLF (Knowledge Tracing Machine by modeling cognitive item Difficulty and Learning and Forgetting), to trace the evolution of each learner’s knowledge acquisition during exercise activities by modeling his or her dynamic knowledge construction procedure and cognitive item difficulty. Specifically, we first specify the concept of cognitive item difficulty and propose a method to model the cognitive item difficulty adaptively based on learners’ learning histories. Then, based on two classical theories (the learning curve theory and the Ebbinghaus forgetting curve theory), we propose a method for modeling learners’ learning and forgetting over time. Finally, the KTM-DLF model is proposed to incorporate learners’ abilities, the cognitive item difficulty, and the two dynamic procedures (learning and forgetting) together. We then use the factorization machine framework to embed features in high dimensions and model pairwise interactions to increase the model’s accuracy. Extensive experiments have been conducted on three public real-world datasets, and the results confirm that our proposed model outperforms the other state-of-the-art educational data mining models.},
  archive      = {J_APIN},
  author       = {Gan, Wenbin and Sun, Yuan and Peng, Xian and Sun, Yi},
  doi          = {10.1007/s10489-020-01756-7},
  journal      = {Applied Intelligence},
  month        = {11},
  number       = {11},
  pages        = {3894-3912},
  shortjournal = {Appl. Intell.},
  title        = {Modeling learner’s dynamic knowledge construction procedure and cognitive item difficulty for knowledge tracing},
  volume       = {50},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Using prior knowledge in the inference of gene association
networks. <em>APIN</em>, <em>50</em>(11), 3882–3893. (<a
href="https://doi.org/10.1007/s10489-020-01705-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traditional computational techniques are recently being improved with the use of prior biological knowledge from open-access repositories in the area of gene expression data analysis. In this work, we propose the use of prior knowledge as heuristic in an inference method of gene-gene associations from gene expression profiles. In this paper, we use Gene Ontology, which is an open-access ontology where genes are annotated using their biological functionality, as a source of prior knowledge together with a gene pairwise Gene-Ontology-based measure. The performance of our proposal has been compared to other benchmark methods for the inference of gene networks, outperforming in some cases and obtaining similar and competitive results in others, but with the advantage of providing simple and interpretable models, which is a desired feature for the Artificial Intelligence Health related models as stated by the European Union.},
  archive      = {J_APIN},
  author       = {Nepomuceno-Chamorro, Isabel A. and Nepomuceno, Juan A. and Galván-Rojas, José Luis and Vega-Márquez, Belén and Rubio-Escudero, Cristina},
  doi          = {10.1007/s10489-020-01705-4},
  journal      = {Applied Intelligence},
  month        = {11},
  number       = {11},
  pages        = {3882-3893},
  shortjournal = {Appl. Intell.},
  title        = {Using prior knowledge in the inference of gene association networks},
  volume       = {50},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A deceptive detection model based on topic, sentiment, and
sentence structure information. <em>APIN</em>, <em>50</em>(11),
3868–3881. (<a
href="https://doi.org/10.1007/s10489-020-01779-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deceptive reviews on Web are a common phenomenon and how to detect them has a very important impact on products, services, and even business policies. In order to filter out deceptive reviews more accurately, a new model called Sentence Joint Topic Sentiment Model (SJTSM) is presented in this paper, which incorporates the sentence structure of reviews and the sentiment label information of words based on Latent Dirichlet Allocation (LDA) model to extract the review features. The proposed model employs Gibbs algorithm to estimate the maximum likelihood parameters and takes the vector of topic-sentiment distribution as the review features. Then a voting system of multiple-classifier, which takes the extracted review feature vector as its input is designed to realize the classification of deceptive review detection. The comparative experiments on different public datasets with other existing methods based on LDA model show that the new classifying system based on SJTSM model can achieve more satisfying classification results on deceptive review detection.},
  archive      = {J_APIN},
  author       = {Du, Xiaodong and Zhu, Ruiqi and Zhao, Fuqiang and Zhao, Fangzhou and Han, Ping and Zhu, Zhengyu},
  doi          = {10.1007/s10489-020-01779-0},
  journal      = {Applied Intelligence},
  month        = {11},
  number       = {11},
  pages        = {3868-3881},
  shortjournal = {Appl. Intell.},
  title        = {A deceptive detection model based on topic, sentiment, and sentence structure information},
  volume       = {50},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A hybrid two-stage financial stock forecasting algorithm
based on clustering and ensemble learning. <em>APIN</em>,
<em>50</em>(11), 3852–3867. (<a
href="https://doi.org/10.1007/s10489-020-01766-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper investigates the problem of the stock closing price forecasting for the stock market. Based on existing two-stage fusion models in the literature, two new prediction models based on clustering have been proposed, where k-means clustering method is adopted to cluster several common technical indicators. In addition, ensemble learning has also been applied to improve the prediction accuracy. Finally, a hybrid prediction model, which combines both the k-means clustering and ensemble learning, has been proposed. The experimental results on a number of Chinese stocks demonstrate that the hybrid prediction model obtains the best predicting accuracy of the stock price. The k-means clustering on the stock technical indicators can further enhance the prediction accuracy of the ensemble learning.},
  archive      = {J_APIN},
  author       = {Xu, Ying and Yang, Cuijuan and Peng, Shaoliang and Nojima, Yusuke},
  doi          = {10.1007/s10489-020-01766-5},
  journal      = {Applied Intelligence},
  month        = {11},
  number       = {11},
  pages        = {3852-3867},
  shortjournal = {Appl. Intell.},
  title        = {A hybrid two-stage financial stock forecasting algorithm based on clustering and ensemble learning},
  volume       = {50},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A dynamic multi-sensor data fusion approach based on
evidence theory and WOWA operator. <em>APIN</em>, <em>50</em>(11),
3837–3851. (<a
href="https://doi.org/10.1007/s10489-020-01739-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-sensor data fusion (MSDF) problems have attracted widespread attention recently. However, it is still an open issue about how to make the fusion process effectively even if the collected data conflict due to several unpredictable reasons. Moreover, most existing approaches mainly concentrated on the distinction of evidence sources, which cannot well consider the feature of individual belief degree and the associated preference of decision-makers. To address such an issue, a dynamic MSDF method based on evidence theory and weighted ordered weighted averaging (WOWA) operator is proposed in this study. A numerical example is analyzed to demonstrate its whole calculation procedure. Two simulation experiments, composed of a motor rotor fault diagnosis and an insulator string target recognition application, are also mentioned to illustrate its effectiveness and applied value. The results show that the proposed methodology can enhance the fusion accuracy in the constrained scenarios with the consideration of preference relation.},
  archive      = {J_APIN},
  author       = {Wang, Jiayi and Yu, Qiuze},
  doi          = {10.1007/s10489-020-01739-8},
  journal      = {Applied Intelligence},
  month        = {11},
  number       = {11},
  pages        = {3837-3851},
  shortjournal = {Appl. Intell.},
  title        = {A dynamic multi-sensor data fusion approach based on evidence theory and WOWA operator},
  volume       = {50},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Gene selection of non-small cell lung cancer data for
adjuvant chemotherapy decision using cell separation algorithm.
<em>APIN</em>, <em>50</em>(11), 3822–3836. (<a
href="https://doi.org/10.1007/s10489-020-01740-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Since recommended treatment for Non-small cell lung cancer (NSCLC) after surgery is chemotherapy, the prediction of effectiveness or futileness of adjuvant chemotherapy (ACT) in early stage is important for future decision. Classification of NSCLC in gene expression data is performed to predict effectiveness or futileness of ACT. Selection of genes highly correlated with the class attribute, affects the classification accuracy. In this paper, a new cell separation algorithm is proposed which it imitates the action of cell separation using differential centrifugation process involving multiple centrifugation steps and increasing the rotor speed in each step. The CSA uses the application of centrifugal force to separate the solutions based on their objective function in different steps while the velocity is increased in each step. The CSA contributes to automatic trade-off between exploration and exploitation by control of selection rate during the search process. To examine the CSA, 25 test functions were used first and then the CSA was applied to predict effectiveness or futileness of ACT. The number of genes in candidate subsets is handled by increasing the subset size if after a certain number of iterations there is no improvement in fitness of the subset. This contributes to less time consideration and memory usage. In this experiment, the NSCLC data contain 280 samples collected from four institutes are used. As results, the minimum number of five genes with dependency degree equal to one and classification accuracy of higher than 94% for SVM, KNN and MLP classifiers is obtained.},
  archive      = {J_APIN},
  author       = {Jaddi, Najmeh Sadat and Saniee Abadeh, Mohammad},
  doi          = {10.1007/s10489-020-01740-1},
  journal      = {Applied Intelligence},
  month        = {11},
  number       = {11},
  pages        = {3822-3836},
  shortjournal = {Appl. Intell.},
  title        = {Gene selection of non-small cell lung cancer data for adjuvant chemotherapy decision using cell separation algorithm},
  volume       = {50},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A novel multi-classifier based on a density-dependent
quantized binary tree LSSVM and the logistic global whale optimization
algorithm. <em>APIN</em>, <em>50</em>(11), 3808–3821. (<a
href="https://doi.org/10.1007/s10489-020-01736-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The least squares support vector machine (LSSVM) is a useful binary classifier, but its performance is limited due to the lack of sparseness. The density-dependent quantized LSSVM (DSM) with quantized input data can increase the sparseness to effectively accomplish binary classification. However, the DSM cannot be directly used in multi-classification applications for most practical data-classification problems. We propose a novel multi-classifier based on a density-dependent quantized binary tree LSSVM (DBSM) and the logistic global whale optimization algorithm (LWA) to improve multi-classification accuracy and computational efficiency. The DBSM consists of multiple DSM classifiers, which hierarchically divide data according to a modified binary tree architecture. The tree architecture is constructed quickly and correctly with the quantized data instead of the original input data. An appropriate initial population of DBSM parameters is generated by using a logistic map and an improved opposition-based learning strategy. Then, the DBSM parameters are optimized by the whale optimization algorithm integrated with the gbest-guided artificial bee colony algorithm. According to the experimental results, the DBSM solves multi-classification problems faster than the one-versus-one based support vector machine (OVO-SVM) and the one-versus-all based LSSVM without sacrificing accuracy. The LWA precisely finds the optimal DBSM parameters without a heavy computational burden, in contrast to recent optimization algorithms. The proposed classifier achieves a 3.39% higher accuracy and consumes 52.83% less time than the genetic algorithm-based OVO-SVM. These results prove that the LWA-DBSM can complete multi-class classification tasks precisely and quickly.},
  archive      = {J_APIN},
  author       = {Chen, Jiaoliao and Zhuo, Xingai and Xu, Fang and Wang, Jiacai and Zhang, Dan and Zhang, Libin},
  doi          = {10.1007/s10489-020-01736-x},
  journal      = {Applied Intelligence},
  month        = {11},
  number       = {11},
  pages        = {3808-3821},
  shortjournal = {Appl. Intell.},
  title        = {A novel multi-classifier based on a density-dependent quantized binary tree LSSVM and the logistic global whale optimization algorithm},
  volume       = {50},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Incrementally updating the high average-utility patterns
with pre-large concept. <em>APIN</em>, <em>50</em>(11), 3788–3807. (<a
href="https://doi.org/10.1007/s10489-020-01743-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {High-utility itemset mining (HUIM) is considered as an emerging approach to detect the high-utility patterns from databases. Most existing algorithms of HUIM only consider the itemset utility regardless of the length. This limitation raises the utility as a result of a growing itemset size. High average-utility itemset mining (HAUIM) considers the size of the itemset, thus providing a more balanced scale to measure the average-utility for decision-making. Several algorithms were presented to efficiently mine the set of high average-utility itemsets (HAUIs) but most of them focus on handling static databases. In the past, a fast-updated (FUP)-based algorithm was developed to efficiently handle the incremental problem but it still has to re-scan the database when the itemset in the original database is small but there is a high average-utility upper-bound itemset (HAUUBI) in the newly inserted transactions. In this paper, an efficient framework called PRE-HAUIMI for transaction insertion in dynamic databases is developed, which relies on the average-utility-list (AUL) structures. Moreover, we apply the pre-large concept on HAUIM. A pre-large concept is used to speed up the mining performance, which can ensure that if the total utility in the newly inserted transaction is within the safety bound, the small itemsets in the original database could not be the large ones after the database is updated. This, in turn, reduces the recurring database scans and obtains the correct HAUIs. Experiments demonstrate that the PRE-HAUIMI outperforms the state-of-the-art batch mode HAUI-Miner, and the state-of-the-art incremental IHAUPM and FUP-based algorithms in terms of runtime, memory, number of assessed patterns and scalability.},
  archive      = {J_APIN},
  author       = {Lin, Jerry Chun-Wei and Pirouz, Matin and Djenouri, Youcef and Cheng, Chien-Fu and Ahmed, Usman},
  doi          = {10.1007/s10489-020-01743-y},
  journal      = {Applied Intelligence},
  month        = {11},
  number       = {11},
  pages        = {3788-3807},
  shortjournal = {Appl. Intell.},
  title        = {Incrementally updating the high average-utility patterns with pre-large concept},
  volume       = {50},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Capped l1-norm distance metric-based fast robust twin
extreme learning machine. <em>APIN</em>, <em>50</em>(11), 3775–3787. (<a
href="https://doi.org/10.1007/s10489-020-01757-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose a new 0.00,0.00,1.00 fast robust twin extreme learning machine (FRTELM) based on the least squares sense and capped L1-norm distance metric. FRTELM first replaced the inequality constraints in TELM with equality constraints, and then introduced the capped L1-norm distance metric to replace the L2-norm distance metric in TELM. FRTELM not only retains the advantages of TELM, but also overcomes the shortcomings of TELM exaggeration of outliers based on squared L2-norm distance metrics. This improvement improves the robustness and learning efficiency of TELM in solving outlier problems. An efficient optimization algorithm is exploited to solve the nonconvex and nonsmooth challenging problem. In theory, we analyze and discuss the complexity of the algorithm in detail, and prove the convergence and local optimality of the algorithm. Extensive experiments conducted across multiple datasets demonstrates that the proposed method is competitive with state-of-the-art methods in terms of robustness and feasibility.},
  archive      = {J_APIN},
  author       = {MA, Jun},
  doi          = {10.1007/s10489-020-01757-6},
  journal      = {Applied Intelligence},
  month        = {11},
  number       = {11},
  pages        = {3775-3787},
  shortjournal = {Appl. Intell.},
  title        = {Capped l1-norm distance metric-based fast robust twin extreme learning machine},
  volume       = {50},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A multi-objective algorithm for multi-label filter feature
selection problem. <em>APIN</em>, <em>50</em>(11), 3748–3774. (<a
href="https://doi.org/10.1007/s10489-020-01785-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Feature selection is an important data preprocessing method before classification. Multi-objective optimization algorithms have been proved an effective way to solve feature selection problems. However, there are few studies on multi-objective optimization feature selection methods for multi-label data. In this paper, a multi-objective multi-label filter feature selection algorithm based on two particle swarms (MOMFS) is proposed. We use mutual information to measure the relevance between features and label sets, and the redundancy between features, which are taken as two objectives. In order to avoid Particle Swarm Optimization (PSO) from falling into the local optimum and obtaining a false Pareto front, we employ two swarms to optimize the two objectives separately and propose an improved hybrid topology based on particle’s fitness value. Furthermore, an archive maintenance strategy is introduced to maintain the distribution of archive. In order to study the effectiveness of the proposed algorithm, we select five multi-label evaluation criteria and perform experiments on seven multi-label data sets. MOMFS is compared with classic single-objective multi-label feature selection algorithms, multi-objective filter and wrapper feature selection algorithms. The experimental results show that MOMFS can effectively reduce the multi-label data dimension and perform better than other approaches on five evaluation criteria.},
  archive      = {J_APIN},
  author       = {Dong, Hongbin and Sun, Jing and Li, Tao and Ding, Rui and Sun, Xiaohang},
  doi          = {10.1007/s10489-020-01785-2},
  journal      = {Applied Intelligence},
  month        = {11},
  number       = {11},
  pages        = {3748-3774},
  shortjournal = {Appl. Intell.},
  title        = {A multi-objective algorithm for multi-label filter feature selection problem},
  volume       = {50},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Designing an interval type-2 fuzzy disturbance observer for
a class of nonlinear systems based on modified particle swarm
optimization. <em>APIN</em>, <em>50</em>(11), 3731–3747. (<a
href="https://doi.org/10.1007/s10489-020-01774-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a new interval type-2 fuzzy disturbance observer design for a class of nonlinear systems using modified particle swarm optimization. The design procedure has two main parts, including the selection of the initial structure of the type-2 fuzzy disturbance observer, and the optimization of the observer parameters using a modified particle swarm optimization algorithm. The modified particle swarm optimization algorithm has a better performance in terms of the accuracy and convergence rate compared with the standard particle swarm optimization and many other evolutionary algorithms. In this algorithm, the upper and lower bounds of the search space are defined for the parameters of each particle based on their values, and weaker particles are substituted with new particles. To accentuate the outstanding performance of the modified particle swarm optimization for the considered task, its performance is compared with five famous meta-heuristic optimization algorithms. In addition, utilizing interval type-2 fuzzy systems in the proposed observer provides more robustness compared with type-1 fuzzy systems. The effectiveness of the proposed fuzzy disturbance observer is shown through computer simulation and experimental results for the ball and beam system, while the system is subjected to sinusoid and square disturbances, and a comparison is drawn to indicate the superiority of the proposed fuzzy disturbance observer over the other observers.},
  archive      = {J_APIN},
  author       = {Naderi, Shokoufeh and Rezaie, Behrooz and Faramin, Mostafa},
  doi          = {10.1007/s10489-020-01774-5},
  journal      = {Applied Intelligence},
  month        = {11},
  number       = {11},
  pages        = {3731-3747},
  shortjournal = {Appl. Intell.},
  title        = {Designing an interval type-2 fuzzy disturbance observer for a class of nonlinear systems based on modified particle swarm optimization},
  volume       = {50},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A new parameter reduction algorithm for interval-valued
fuzzy soft sets based on pearson’s product moment coefficient.
<em>APIN</em>, <em>50</em>(11), 3718–3730. (<a
href="https://doi.org/10.1007/s10489-020-01708-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Interval-valued fuzzy soft set theory is a newly emerging mathematical tool for dealing with uncertain problems. Parameter reduction abandons redundant parameters meanwhile holds the powerful ability to support decision making Ma et al. (2014) expressed four diverse parameter reduction approaches which are appropriate for the different scenarios. Optimal choice considered parameter reduction approach is not effective to support the additive parameters. The other three methods provide the support for the newly additive parameters but possess very low rate of success. And the four methods are computationally complicated. In this paper, we propose Pearson’s product moment coefficient based parameter reduction algorithm for an interval-valued fuzzy soft sets. By comparison with four algorithms, this approach not only is carried out before getting the scores, give attention to the newly additive parameters, and has much higher probability to find parameter reduction, but also is not more computationally complicated. Therefore, this algorithm is the most efficient to support extension and combination of multiple evaluation systems based on interval-valued fuzzy soft set in the down-to-earth applications environment. The superiority and effectiveness of the proposed approach is demonstrated by this means of a suitable practical application case of on-line reservation for accommodation and twenty synthetic generated datasets.},
  archive      = {J_APIN},
  author       = {Ma, Xiuqin and Qin, Hongwu},
  doi          = {10.1007/s10489-020-01708-1},
  journal      = {Applied Intelligence},
  month        = {11},
  number       = {11},
  pages        = {3718-3730},
  shortjournal = {Appl. Intell.},
  title        = {A new parameter reduction algorithm for interval-valued fuzzy soft sets based on pearson’s product moment coefficient},
  volume       = {50},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Solving artificial ant problem using two artificial bee
colony programming versions. <em>APIN</em>, <em>50</em>(11), 3695–3717.
(<a href="https://doi.org/10.1007/s10489-020-01741-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Artificialant problem is considered as a sub-problem of robotic path planning. In this study, it is solved using two different methods: artificial bee colony programming and a new version of it called shrinking artificial bee colony programming. The former is a novel evolutionary computation based automatic programming method based on artificial bee colony algorithm and it was previously applied to this problem by the researchers. However, in this study, more comprehensive analyses and comparison study are provided. The shrinking artificial bee colony programming was developed in this study and its basic idea is to reduce the number of food sources, periodically, instead of a constant number used in the artificial bee colony programming. First, some parameter tuning studies were carried out for the shrinking artificial bee colony programming. Then, performances of the artificial bee colony programming, shrinking artificial bee colony programming and some other evolutionary computation based automatic programming methods were compared on Santa Fe and Los Altos Hills trails. Simulation results and the comparison study show that both of the algorithms can be used to solve the artificial ant problem effectively. Furthermore, the periodically decreasing population size property added to the artificial bee colony programming improves the performance of the algorithm on the artificial ant problem. While the proposed approach shows one of the superior performances among the considered methods, the results of the artificial bee colony programming are competitive to the methods in the comparison study.},
  archive      = {J_APIN},
  author       = {Boudardara, Fateh and Gorkemli, Beyza},
  doi          = {10.1007/s10489-020-01741-0},
  journal      = {Applied Intelligence},
  month        = {11},
  number       = {11},
  pages        = {3695-3717},
  shortjournal = {Appl. Intell.},
  title        = {Solving artificial ant problem using two artificial bee colony programming versions},
  volume       = {50},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Large margin classifiers to generate synthetic data for
imbalanced datasets. <em>APIN</em>, <em>50</em>(11), 3678–3694. (<a
href="https://doi.org/10.1007/s10489-020-01719-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper we propose the development of an approach capable of improving the results obtained by classification algorithms when applied to imbalanced datasets. The method, called Incremental Synthetic Balancing Algorithm (ISBA), performs an iterative procedure based on large margin classifiers, aiming to generate synthetic samples in order to reduce the level of imbalance. In the process, we use the support vectors as the reference for the generation of new instances, allowing them to be positioned in regions with greater representativeness. Furthermore, the new samples can exceed the limits of the ones used for their generation, which enables extrapolation of the boundaries of the minority class, achieving more significant recognition of this class of interest. We present comparative experiments with other techniques, among them the SMOTE, which provide strong evidence of the applicability of the proposed approach.},
  archive      = {J_APIN},
  author       = {Ladeira Marques, Marcelo and Moraes Villela, Saulo and Hasenclever Borges, Carlos Cristiano},
  doi          = {10.1007/s10489-020-01719-y},
  journal      = {Applied Intelligence},
  month        = {11},
  number       = {11},
  pages        = {3678-3694},
  shortjournal = {Appl. Intell.},
  title        = {Large margin classifiers to generate synthetic data for imbalanced datasets},
  volume       = {50},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Dynamic feature selection method with minimum redundancy
information for linear data. <em>APIN</em>, <em>50</em>(11), 3660–3677.
(<a href="https://doi.org/10.1007/s10489-020-01726-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Feature selection plays a fundamental role in many data mining and machine learning tasks. In this paper, we proposed a novel feature selection method, namely, Dynamic Feature Selection Method with Minimum Redundancy Information (MRIDFS). In MRIDFS, the conditional mutual information is used to calculate the relevance and the redundancy among multiple features, and a new concept, the feature-dependent redundancy ratio, was introduced. Such ratio can represent redundancy more accurately. To evaluate our method, MRIDFS is tested and compared with seven popular methods on 16 benchmark data sets. Experimental results show that MRIDFS outperforms in terms of average classification accuracy.},
  archive      = {J_APIN},
  author       = {Zhou, HongFang and Wen, Jing},
  doi          = {10.1007/s10489-020-01726-z},
  journal      = {Applied Intelligence},
  month        = {11},
  number       = {11},
  pages        = {3660-3677},
  shortjournal = {Appl. Intell.},
  title        = {Dynamic feature selection method with minimum redundancy information for linear data},
  volume       = {50},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Uniform distribution driven adaptive differential evolution.
<em>APIN</em>, <em>50</em>(11), 3638–3659. (<a
href="https://doi.org/10.1007/s10489-020-01707-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Evolutionary algorithms are popular optimization tools for real-world applications due to their numerous advantages such as capability of parallel search along multiple directions by maintaining a population of candidates, invariance to certain mathematical properties (convexity, continuity and hardness) of fitness landscape and ability to handle black-box problems. However, most of the current evolutionary algorithms are loosely based on heuristics inspired by nature and lack the crucial theoretical background. Motivated by the overwhelming advantages of such optimization algorithms and the necessity for theoretical foundation, this paper presents a new evolutionary algorithm - UDE (Uniform Differential Evolution) for solving single- objective optimization problems along with a theoretical analysis of the proposed UDE algorithm. Thus, this paper formally gives insights about the features and properties of the various optimization strategies used. This method is different from traditional Differential Evolution variants as it employs a uniform probability distribution for generating new candidate solutions. UDE is further developed to obtain an adaptive evolutionary algorithm - Adaptive UDE (AUDE), which has shown to obtain significant improvements in the performance and convergence speeds compared to other algorithms on a benchmark set of 19 test problems. The source codes are available at http://worksupplements.droppages.com/ude_aude .},
  archive      = {J_APIN},
  author       = {Sengupta, Raunak and Pal, Monalisa and Saha, Sriparna and Bandyopadhyay, Sanghamitra},
  doi          = {10.1007/s10489-020-01707-2},
  journal      = {Applied Intelligence},
  month        = {11},
  number       = {11},
  pages        = {3638-3659},
  shortjournal = {Appl. Intell.},
  title        = {Uniform distribution driven adaptive differential evolution},
  volume       = {50},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Anchor-free multi-orientation text detection in natural
scene images. <em>APIN</em>, <em>50</em>(11), 3623–3637. (<a
href="https://doi.org/10.1007/s10489-020-01742-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Text detection in natural scene images is a key prerequisite for computer vision tasks such as image search, blind navigation, autopilot, and multi-language translation. Existing text detection methods only detect partial region of large-scale texts and are difficult to detect small-scale texts. Aiming at this problem, an anchor-free multi-orientation text detection method is proposed. Firstly, Feature Pyramid Network (FPN) is used to combine the multiple feature layers of Convolutional Neural Network (CNN) to predict the geometric properties of text, which can be used to expand the receptive field of each pixel and thus help to detect more large-scale texts. Secondly, a new loss function independent of the scale of text is designed, which enables the pixels in the small-scale text to have a larger calculation weight, thereby facilitating the detection of small-scale texts. Finally, the results of pixel-level semantic segmentation are used to filter obviously unreasonable candidate text boxes, and at the same time improve the accuracy and recall rate of text detection. The experimental results on ICDAR 2015 and MSRA-TD500 prove the good performance of our method.},
  archive      = {J_APIN},
  author       = {Lu, Liqiong and Wu, Dong and Wu, Tao and Huang, Faliang and Yi, Yaohua},
  doi          = {10.1007/s10489-020-01742-z},
  journal      = {Applied Intelligence},
  month        = {11},
  number       = {11},
  pages        = {3623-3637},
  shortjournal = {Appl. Intell.},
  title        = {Anchor-free multi-orientation text detection in natural scene images},
  volume       = {50},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). In-depth exploration of attribute information for person
re-identification. <em>APIN</em>, <em>50</em>(11), 3607–3622. (<a
href="https://doi.org/10.1007/s10489-020-01752-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Pedestrian’s attribute information plays an important role in person re-identification (re-ID) for its complementary to pedestrian’s identity labels. However, there are few methods to utilize attribute information, which limits the development of re-ID community. In this paper, we analyze the effect of attribute information on re-ID to obtain both qualitative and quantitative results, indicating the potential for in-depth exploration of attribute information. On this basis, we propose an Identity Recognition Network (IRN) and an Attribute Recognition Network (ARN). IRN enhances the attention to pedestrian’s local information while identifying pedestrians’ identity. ARN calculates the attribute similarity among pedestrians accurately to promote the identification of IRN. The combination of them makes deep exploration of attribute information and is easy to implement. The experimental results on two large-scale re-ID benchmarks demonstrate the effectiveness of our method, which is on par with the state-of-the-art. In the DukeMTMC-reID dataset, mAP (rank-1) accuracy is improved from 58.4 (78.3) % to 66.4 (82.7) % for ResNet-50. In the Market1501 dataset, mAP (rank-1) accuracy is improved from 75.8 (90.5) % to 79.5 (92.8) % for ResNet-50.},
  archive      = {J_APIN},
  author       = {Yin, Jianyuan and Fan, Zheyi and Chen, Shuni and Wang, Yilin},
  doi          = {10.1007/s10489-020-01752-x},
  journal      = {Applied Intelligence},
  month        = {11},
  number       = {11},
  pages        = {3607-3622},
  shortjournal = {Appl. Intell.},
  title        = {In-depth exploration of attribute information for person re-identification},
  volume       = {50},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Reinforcement learning algorithm for non-stationary
environments. <em>APIN</em>, <em>50</em>(11), 3590–3606. (<a
href="https://doi.org/10.1007/s10489-020-01758-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Reinforcement learning (RL) methods learn optimal decisions in the presence of a stationary environment. However, the stationary assumption on the environment is very restrictive. In many real world problems like traffic signal control, robotic applications, etc., one often encounters situations with non-stationary environments, and in these scenarios, RL methods yield sub-optimal decisions. In this paper, we thus consider the problem of developing RL methods that obtain optimal decisions in a non-stationary environment. The goal of this problem is to maximize the long-term discounted reward accrued when the underlying model of the environment changes over time. To achieve this, we first adapt a change point algorithm to detect change in the statistics of the environment and then develop an RL algorithm that maximizes the long-run reward accrued. We illustrate that our change point method detects change in the model of the environment effectively and thus facilitates the RL algorithm in maximizing the long-run reward. We further validate the effectiveness of the proposed solution on non-stationary random Markov decision processes, a sensor energy management problem, and a traffic signal control problem.},
  archive      = {J_APIN},
  author       = {Padakandla, Sindhu and K. J., Prabuchandran and Bhatnagar, Shalabh},
  doi          = {10.1007/s10489-020-01758-5},
  journal      = {Applied Intelligence},
  month        = {11},
  number       = {11},
  pages        = {3590-3606},
  shortjournal = {Appl. Intell.},
  title        = {Reinforcement learning algorithm for non-stationary environments},
  volume       = {50},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Boundary-connection deletion strategy based method for
community detection in complex networks. <em>APIN</em>, <em>50</em>(11),
3570–3589. (<a
href="https://doi.org/10.1007/s10489-020-01762-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Community detection in complex networks is a difficult problem. Up to now, there is no very effective method to solve it. Recently, many community detection algorithms based on edge removal have been proposed. However, these edge removal methods often delete many key connections within communities and weaken (or destroy) the community structure of the network. This will make the network communities more difficult to be identified and reduce the accuracy and stability of the algorithm. This paper proposed a boundary connection deletion based community detection algorithm. Different from other algorithms, our algorithm focuses on identifying and removing the boundary connections between network modules. This can enhance the network community structure and get high quality network modules. With high performance, our algorithm can detect the optimal and hierarchical community structure in weighted networks simultaneously. In order to verify the effectiveness of our algorithm, the stability and robustness of our algorithm were firstly analyzed. Then a series of experiments had been done on the real-world and synthetic networks. The real-world networks include Zachary’s karate club network, dolphin social network, American college foot-ball network, PolBooks network, Les Misérables character network, and the coauthorship network of scientists; The synthetic networks include GN benchmark and LFR benckmark. Two indices NMI and Modularity Q were used to compare our algorithm with the recently proposed algorithms, including meta-LPAm+, Srinivas and Rajendran’s model, IDPM, CFCDs, EDCD, CNM, and CDASS. Experimental results show that our algorithm has better performance than these algorithms.},
  archive      = {J_APIN},
  author       = {Yuan, Chao and Rong, Chuitian and Yao, Qingshuang},
  doi          = {10.1007/s10489-020-01762-9},
  journal      = {Applied Intelligence},
  month        = {11},
  number       = {11},
  pages        = {3570-3589},
  shortjournal = {Appl. Intell.},
  title        = {Boundary-connection deletion strategy based method for community detection in complex networks},
  volume       = {50},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Identifying ground truth in opinion spam: An empirical
survey based on review psychology. <em>APIN</em>, <em>50</em>(11),
3554–3569. (<a
href="https://doi.org/10.1007/s10489-020-01764-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Because it is very harmful, opinion spam, especially that involving untruthful reviews, has attracted much attention in the last decade. However, the lack of annotations, i.e., the ground truth problem, still serves as the key challenge. It is difficult because spammers always deliberately forge their reviews, which cannot be distinguished even by field experts. Considering the obvious intention of spammers, i.e., to promote or demote an items reputation, the opportunity exists to label them by considering crowd psychology. To date, several studies have applied, verified, and presented helpful evidence, including prior, empirical, heuristic, and simulative pseudo truths. In this paper, after investigating both authentic and deceptive reviewers’ diverse motives, we survey state-of-the-art truth by considering two classical roles, e.g., crowdsourcing and expert spammers. For each role, several topics related to spam attacks either with or without disguising and possible outliers are highlighted. Comparison analyses led to some interesting conclusions: 1) data on professional spammers are more challenging to collect and less reliable than data on crowdsourcing spammers; 2) most linguistic evidences are less reliable than behavioral footprints; 3) abnormal activities are as trustworthy as spamming objectives, while they hardly need any extra support, such as the user profile; and 4) the top reliable facts requiring acceptable effort are deviation, burstiness, grouped spamming, deviation over the threshold, review distribution, opinion proportion and spam cost. Moreover, we introduce several promising directions for future research. In general, this survey may shed light on new angles that can be used to understand review spam and to improve the performance of any anti-spam platforms.},
  archive      = {J_APIN},
  author       = {Li, Jiandun and Wang, Xiaogang and Yang, Liu and Zhang, Pengpeng and Yang, Dingyu},
  doi          = {10.1007/s10489-020-01764-7},
  journal      = {Applied Intelligence},
  month        = {11},
  number       = {11},
  pages        = {3554-3569},
  shortjournal = {Appl. Intell.},
  title        = {Identifying ground truth in opinion spam: An empirical survey based on review psychology},
  volume       = {50},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A boosting self-training framework based on instance
generation with natural neighbors for k nearest neighbor. <em>APIN</em>,
<em>50</em>(11), 3535–3553. (<a
href="https://doi.org/10.1007/s10489-020-01732-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The semi-supervised self-training method is one of the successful methodologies of semi-supervised classification. The mislabeling is the most challenging issue in self-training methods and the ensemble learning is one of the common techniques for dealing with the mislabeling. Specifically, the ensemble learning can solve or alleviate the mislabeling by constructing an ensemble classifier to improve prediction accuracy in the self-training process. However, most ensemble learning methods may not perform well in self-training methods because it is difficult for ensemble learning methods to train an effective ensemble classifier with a small number of labeled data. Inspired by the successful boosting methods, we introduce a new boosting self-training framework based on instance generation with natural neighbors (BoostSTIG) in this paper. BoostSTIG is compatible with most boosting methods and self-training methods. It can use most boosting methods to solve or alleviate the mislabeling of existing self-training methods by improving the prediction accuracy in the self-training process. Besides, an instance generation with natural neighbors is proposed to enlarge initial labeled data in BoostSTIG, which makes boosting methods more suitable for self-training methods. In experiments, we apply the BoostSTIG framework to 2 self-training methods and 4 boosting methods, and then validate BoostSTIG by comparing some state-of-the-art technologies on real data sets. Intensive experiments show that BoostSTIG can improve the performance of tested self-training methods and train an effective k nearest neighbor.},
  archive      = {J_APIN},
  author       = {Li, Junnan and Zhu, Qingsheng},
  doi          = {10.1007/s10489-020-01732-1},
  journal      = {Applied Intelligence},
  month        = {11},
  number       = {11},
  pages        = {3535-3553},
  shortjournal = {Appl. Intell.},
  title        = {A boosting self-training framework based on instance generation with natural neighbors for k nearest neighbor},
  volume       = {50},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). S3D-CNN: Skeleton-based 3D consecutive-low-pooling neural
network for fall detection. <em>APIN</em>, <em>50</em>(10), 3521–3534.
(<a href="https://doi.org/10.1007/s10489-020-01751-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most existing deep-learning-based fall detection methods use either 2D neural network without considering movement representation sequences, or whole sequences instead of only those in the fall period. These characteristics result in inaccurate extraction of human action features and failure to detect falls due to background interferences or activity representation beyond the fall period. To alleviate these problems, a skeleton-based 3D consecutive-low-pooling neural network (S3D-CNN) for fall detection is proposed in this paper. In the S3D-CNN, an activity feature clustering selector is designed to extract the skeleton representation in depth videos using pose estimation algorithm and form optimized skeleton sequence of fall period. A 3D consecutive-low-pooling (3D-CLP) neural network is proposed to process these representation sequences by improving network in terms of layer number, pooling kernel size, and single input frame number. The proposed method is evaluated on public and self-collected datasets respectively, outperforming the existing methods.},
  archive      = {J_APIN},
  author       = {Xiong, Xin and Min, Weidong and Zheng, Wei-Shi and Liao, Pin and Yang, Hao and Wang, Shuai},
  doi          = {10.1007/s10489-020-01751-y},
  journal      = {Applied Intelligence},
  month        = {10},
  number       = {10},
  pages        = {3521-3534},
  shortjournal = {Appl. Intell.},
  title        = {S3D-CNN: Skeleton-based 3D consecutive-low-pooling neural network for fall detection},
  volume       = {50},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Two-dimensional discrete feature based spatial attention
CapsNet for sEMG signal recognition. <em>APIN</em>, <em>50</em>(10),
3503–3520. (<a
href="https://doi.org/10.1007/s10489-020-01725-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning frameworks(such as deep convolutional networks) require data to have a regular shape. However, discrete features extracted from heterogeneous data cannot be collected in a regular shape to convolute. In this article, a Two-Dimensional Discrete Feature Based Spatial Attention CapsNet(TDACAPS) is proposed to convert one-dimensional discrete features into two-dimensional structured data through Cartesian Product for surface electromyogram(sEMG) signal recognition. sEMG signal varies from person to person is the main signal source of prosthetic control. Our model transforms multi-angle discrete features into structured data to find the inherent law of sEMG signal. Due to uneven information distribution of structured data, this model combines capsule network with attention mechanism to place emphasis on abundant information regions and reduce ancillary information loss. Extensive experiments show our model yields an improvement for sEMG signal recognition of almost 3% than capsule network and other neural networks under different conditions. Our attention mechanism that employs overlapping pooling to search feature map weight is preferable to the squeeze-and-excitation module, convolutional block attention module and others. Moreover, we validate that our model has great expansibility on Wine Quality Dataset and Breast Cancer Wisconsin.},
  archive      = {J_APIN},
  author       = {Chen, Guoqi and Wang, Wanliang and Wang, Zheng and Liu, Honghai and Zang, Zelin and Li, Weikun},
  doi          = {10.1007/s10489-020-01725-0},
  journal      = {Applied Intelligence},
  month        = {10},
  number       = {10},
  pages        = {3503-3520},
  shortjournal = {Appl. Intell.},
  title        = {Two-dimensional discrete feature based spatial attention CapsNet for sEMG signal recognition},
  volume       = {50},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Maximum likelihood-based influence maximization in social
networks. <em>APIN</em>, <em>50</em>(10), 3487–3502. (<a
href="https://doi.org/10.1007/s10489-020-01747-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Influence Maximization (IM) is an important issue in network analyzing which widely occurs in social networks. The IM problem aims to detect the top-k influential seed nodes that can maximize the influence spread. Although a lot of studies have been performed, a novel algorithm with a better balance between time-consumption and guaranteed performance is still needed. In this work, we present a novel algorithm called MLIM for the IM problem, which adopts maximum likelihood-based scheme under the Independent Cascade(IC) model. We construct thumbnails of the social network and calculate the L-value for each vertex using the maximum likelihood criterion. A greedy algorithm is proposed to sequentially choose the seeds with the smallest L-value. Empirical results on real-world networks have proved that the proposed method can provide a wider influence spreading while obtaining lower time consumption.},
  archive      = {J_APIN},
  author       = {Liu, Wei and Li, Yun and Chen, Xin and He, Jie},
  doi          = {10.1007/s10489-020-01747-8},
  journal      = {Applied Intelligence},
  month        = {10},
  number       = {10},
  pages        = {3487-3502},
  shortjournal = {Appl. Intell.},
  title        = {Maximum likelihood-based influence maximization in social networks},
  volume       = {50},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A new hybrid stability measure for feature selection.
<em>APIN</em>, <em>50</em>(10), 3471–3486. (<a
href="https://doi.org/10.1007/s10489-020-01731-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Feature Selection (FS) algorithms are applied in bioinformatics applications to identify the disease causing genes. Performance of such algorithms is measured in terms of accuracy of the model and stability of FS algorithms. Stability evaluates the identical replication of feature sets obtained after every execution. Recently research has shown that a stability measure must satisfy set of properties like, fully defined, monotonicity, boundedness, deterministic maximum stability, and correction for chance. Among the existing stability measures, only Nogueira’s frequency based stability measure satisfies all the required properties. However, frequency based stability measures fail to discriminate among the cases when overall frequency of features are same. In order to address this issue, the paper proposes a hybrid similarity based stability measure which satisfies all the desirable properties, as mentioned earlier. The proposed stability measure is unique as it is the first similarity based stability measure that satisfies all the required properties. Also, all these essential properties are mathematically established. Further, the paper also proposes a combination of frequency based and similarity based measure which preserves all the aspects of both the approaches. The work presented also analyzes the stability performance of LASSO and Elastic Net, using synthetic and microarray gene expression datasets. Elastic Net depicts higher stability and selection of relevant features.},
  archive      = {J_APIN},
  author       = {Naik, Akshata K. and Kuppili, Venkatanareshbabu and Edla, Damodar Reddy},
  doi          = {10.1007/s10489-020-01731-2},
  journal      = {Applied Intelligence},
  month        = {10},
  number       = {10},
  pages        = {3471-3486},
  shortjournal = {Appl. Intell.},
  title        = {A new hybrid stability measure for feature selection},
  volume       = {50},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). UWFP-outlier: An efficient frequent-pattern-based outlier
detection method for uncertain weighted data streams. <em>APIN</em>,
<em>50</em>(10), 3452–3470. (<a
href="https://doi.org/10.1007/s10489-020-01718-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose an efficient frequent-pattern-based outlier detection method, namely, UWFP-Outlier, for identifying the implicit outliers from uncertain weighted data streams. For reducing the time cost of the UWFP-Outlier method, in the weighted frequent pattern mining phase, we introduce the concepts of the maximal weight and maximal probability to form a compact anti-monotonic property, thereby reducing the scale of potential extensible patterns. For accurately detecting the outliers, in the outlier detection phase, we design two deviation indices to measure the deviation degree of each transaction in the uncertain weighted data streams by considering more factors that may influence its deviation degree; then, the transactions which have large deviation degrees are judged as outliers. The experimental results indicate that the proposed UWFP-Outlier method can accurately detect the outliers from uncertain weighted data streams with a lower time cost.},
  archive      = {J_APIN},
  author       = {Cai, Saihua and Li, Li and Li, Qian and Li, Sicong and Hao, Shangbo and Sun, Ruizhi},
  doi          = {10.1007/s10489-020-01718-z},
  journal      = {Applied Intelligence},
  month        = {10},
  number       = {10},
  pages        = {3452-3470},
  shortjournal = {Appl. Intell.},
  title        = {UWFP-outlier: An efficient frequent-pattern-based outlier detection method for uncertain weighted data streams},
  volume       = {50},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Hierarchical graph attention networks for semi-supervised
node classification. <em>APIN</em>, <em>50</em>(10), 3441–3451. (<a
href="https://doi.org/10.1007/s10489-020-01729-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, there has been a promising tendency to generalize convolutional neural networks (CNNs) to graph domain. However, most of the methods cannot obtain adequate global information due to their shallow structures. In this paper, we address this challenge by proposing a hierarchical graph attention network (HGAT) for semi-supervised node classification. This network employs a hierarchical mechanism for the learning of node features. Thus, more information can be effectively obtained of the node features by iteratively using coarsening and refining operations on different hierarchical levels. Moreover, HGAT combines with the attention mechanism in the input and prediction layer. It can assign different weights to different nodes in a neighborhood, which helps to improve accuracy. Experiment results demonstrate that state-of-the-art performance was achieved by our method, not only on Cora, Citeseer, and Pubmed citation datasets, but also on the simplified NELL knowledge graph dataset. The sensitive analysis further verifies that HGAT can capture global structure information by increasing the receptive field, as well as the effective transfer of node features.},
  archive      = {J_APIN},
  author       = {Li, Kangjie and Feng, Yixiong and Gao, Yicong and Qiu, Jian},
  doi          = {10.1007/s10489-020-01729-w},
  journal      = {Applied Intelligence},
  month        = {10},
  number       = {10},
  pages        = {3441-3451},
  shortjournal = {Appl. Intell.},
  title        = {Hierarchical graph attention networks for semi-supervised node classification},
  volume       = {50},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Incremental small sphere and large margin for online
recognition of communication jamming. <em>APIN</em>, <em>50</em>(10),
3429–3440. (<a
href="https://doi.org/10.1007/s10489-020-01717-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the anti-jamming field of radio communication, the problem of online and multiclass jamming recognition is fundamental to implement reasonable anti-jamming measures. The incremental small sphere and large margin (IncSSLM) is proposed, this model can learn the compact boundary for own communication signals and known jamming, which relieves the open-set problem of radio data. Meanwhile it can also update the model of classifier in real time, which avoids the large memory requirement for vast jamming data and saving much time for training. The core of proposed method is the small sphere and large margin (SSLM) approach, which makes the spherical area as compact as possible, like support vector data description (SVDD), and also makes the margin between them as far as possible, like support vector machine (SVM). In other words, it can minimize intra-class divergence and maximize inter-class space. Therefore, there is a significant enhancement of recognition performance when compared with open classifiers such as SVM, and considerable superiority of training efficiency when compared with the canonical SSLM algorithm. Numerical experiments based on synthetic data, practical complex feature data of high-resolution range profile (HRRP), and jamming data of radio communication demonstrate that IncSSLM is efficient and promising for multiple and online recognition of vase and open-set radio jamming.},
  archive      = {J_APIN},
  author       = {Guo, Yu and Meng, Jin and Li, Yaxing and Ge, Songhu and Xing, Jinling and Wu, Hao},
  doi          = {10.1007/s10489-020-01717-0},
  journal      = {Applied Intelligence},
  month        = {10},
  number       = {10},
  pages        = {3429-3440},
  shortjournal = {Appl. Intell.},
  title        = {Incremental small sphere and large margin for online recognition of communication jamming},
  volume       = {50},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A survey on automatic image annotation. <em>APIN</em>,
<em>50</em>(10), 3412–3428. (<a
href="https://doi.org/10.1007/s10489-020-01696-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Automatic image annotation is a crucial area in computer vision, which plays a significant role in image retrieval, image description, and so on. Along with the internet technique developing, there are numerous images posted on the web, resulting in the fact that it is a challenge to annotate images only by humans. Hence, many computer vision researchers are interested in automatic image annotation and make a great effort in optimizing its performance. Automatic image annotation is a task that assigns several tags in a limited vocabulary to describe an image. There are many algorithms proposed to tackle this problem and all achieve great performance. In this paper, we review seven algorithms for automatic image annotation and evaluate these algorithms leveraging different image features, such as color histograms and Gist descriptor. Our goal is to provide insights into the automatic image annotation. A lot of comprehensive experiments, which are based on Corel5K, IAPR TC-12, and ESP Game datasets, are designed to compare the performance of these algorithms. We also compare the performance of traditional algorithms employing deep learning features. Considering that not all associated labels are annotated by human annotators, we leverage the DIA metrics on IAPR TC-12 and ESP Game datasets.},
  archive      = {J_APIN},
  author       = {Chen, Yilu and Zeng, Xiaojun and Chen, Xing and Guo, Wenzhong},
  doi          = {10.1007/s10489-020-01696-2},
  journal      = {Applied Intelligence},
  month        = {10},
  number       = {10},
  pages        = {3412-3428},
  shortjournal = {Appl. Intell.},
  title        = {A survey on automatic image annotation},
  volume       = {50},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Query set centered sparse projection learning for set based
image classification. <em>APIN</em>, <em>50</em>(10), 3400–3411. (<a
href="https://doi.org/10.1007/s10489-020-01730-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Set based image classification technology has been developed successfully in recent decades. Previous approaches dispose set based image classification by employing all the gallery sets to learn metrics or construct the model using a typical number of parameters. However, they are based on the assumption that the global structure is consistent with the local structure, which is rigid in real applications. Additionally, the participation of all gallery sets increases the influence of outliers. This paper conducts this task via sparse projection learning by employing ℓ2,1 norm from the perspective of the query set. Instead of involving all the image sets, this work devotes to searching for a local region, which is centered with a query set and constructed by the candidates selected from different classes in the gallery sets. By maximizing the inter-class while minimizing the intra-class of the candidates from the gallery sets from the query set, this work can learn a discriminate and sparse projection for image set feature extraction. In order to learn the projection, an alternative updating algorithm to solve the optimization problem is proposed and the convergence and complexity are analyzed. Finally, the distance is measured in the discriminate low-dimensional space using Euclidean distance between the central data point of the query set and the central one of images from the same class. The proposed approach learns the projection in the local set centered with the query set with ℓ2,1 norm, which contributes to more discriminative feature. Compared with the existing algorithms, the experiments on the challenging databases demonstrate that the proposed simple yet effective approach obtains the best classification accuracy with comparable time cost.},
  archive      = {J_APIN},
  author       = {Zhu, Wenjie and Peng, Bo and Wu, Han and Wang, Binhao},
  doi          = {10.1007/s10489-020-01730-3},
  journal      = {Applied Intelligence},
  month        = {10},
  number       = {10},
  pages        = {3400-3411},
  shortjournal = {Appl. Intell.},
  title        = {Query set centered sparse projection learning for set based image classification},
  volume       = {50},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Short term electric load forecasting using hybrid algorithm
for smart cities. <em>APIN</em>, <em>50</em>(10), 3379–3399. (<a
href="https://doi.org/10.1007/s10489-020-01728-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many day-to-day operation decisions in a smart city need short term load forecasting (STLF) of its customers. STLF is a challenging task because the forecasting accuracy is affected by external factors whose relationships are usually complex and nonlinear. In this paper, a novel hybrid forecasting algorithm is proposed. The proposed hybrid forecasting method is based on locally weighted support vector regression (LWSVR) and the modified grasshopper optimization algorithm (MGOA). Obtaining the appropriate values of LWSVR parameters is vital to achieving satisfactory forecasting accuracy. Therefore, the MGOA is proposed in this paper to optimally select the LWSVR’s parameters. The proposed MGOA can be derived by presenting two modifications on the conventional GOA in which the chaotic initialization and the sigmoid decreasing criterion are employed to treat the drawbacks of the conventional GOA. Then the hybrid LWSVR-MGOA method is used to solve the STLF problem. The performance of the proposed LWSVR-MGOA method is assessed using six different real-world datasets. The results reveal that the proposed forecasting method gives a much better forecasting performance in comparison with some published forecasting methods in all cases.},
  archive      = {J_APIN},
  author       = {Elattar, Ehab E. and Sabiha, Nehmdoh A. and Alsharef, Mohammad and Metwaly, Mohamed K. and Abd-Elhady, Amr M. and Taha, Ibrahim B. M.},
  doi          = {10.1007/s10489-020-01728-x},
  journal      = {Applied Intelligence},
  month        = {10},
  number       = {10},
  pages        = {3379-3399},
  shortjournal = {Appl. Intell.},
  title        = {Short term electric load forecasting using hybrid algorithm for smart cities},
  volume       = {50},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Is position important? Deep multi-task learning for
aspect-based sentiment analysis. <em>APIN</em>, <em>50</em>(10),
3367–3378. (<a
href="https://doi.org/10.1007/s10489-020-01760-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The position information of aspect is essential and useful for aspect-based sentiment analysis, while how to model the position of the aspect effectively during aspect-based sentiment analysis has not been well studied. Inspired by the intuition that the position prediction can help boost the performance of aspect-based sentiment analysis, we propose a D eep M ulti-T ask L earning (DMTL) model, which handles sentiment prediction (SP) and position prediction (PP) simultaneously. In particular, we first use a shared layer to learn the common features of the two tasks. Then, two task-specific layers are utilized to learn the features specific to the tasks and perform position prediction and sentiment prediction in parallel. Inspired by autoencoder structure, we design a position-aware attention and a deep bi-directional LSTM (DBi-LSTM) model for sentiment prediction and position prediction respectively to capture the position information better. Extensive experiments on four benchmark datasets show that our approach can effectively improve the performance of aspect-based sentiment analysis compared with the strong baselines.},
  archive      = {J_APIN},
  author       = {Zhou, Jie and Huang, Jimmy Xiangji and Hu, Qinmin Vivian and He, Liang},
  doi          = {10.1007/s10489-020-01760-x},
  journal      = {Applied Intelligence},
  month        = {10},
  number       = {10},
  pages        = {3367-3378},
  shortjournal = {Appl. Intell.},
  title        = {Is position important? deep multi-task learning for aspect-based sentiment analysis},
  volume       = {50},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A novel discrete whale optimization algorithm for solving
knapsack problems. <em>APIN</em>, <em>50</em>(10), 3350–3366. (<a
href="https://doi.org/10.1007/s10489-020-01722-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Whale optimization algorithm (WOA) is a recently proposed meta-heuristic algorithm which imitates the hunting behavior of humpback whales. Due to its characteristic advantages, it has found its place in the mature population-based methods in many scientific and engineering fields. Because WOA was proposed for continuous optimization, it cannot be directly used to solve discrete optimization problems. For this purpose, we first give a new V -shaped function by drawing lesson from the existing discretization methods, which transfer a real vector to an integer vector. On this basis, we propose a novel discrete whale optimization algorithm (DWOA). DWOA uses the new proposed V -shaped function to generate an integer vector, and it can be used to solve discrete optimization problems with solution space {0,1,…,m1}×{0,1,…,m2}×… ×{0,1,…,mn}. To verify effectiveness of DWOA for the 0-1 knapsack problem and the discount {0-1} knapsack problem, we solve their benchmark instances from published literature and compare with the state-of-the-art algorithms. The comparison results show that the DWOA has more superiority than existing algorithms for the two kinds of knapsack problems.},
  archive      = {J_APIN},
  author       = {Li, Ya and He, Yichao and Liu, Xuejing and Guo, Xiaohu and Li, Zewen},
  doi          = {10.1007/s10489-020-01722-3},
  journal      = {Applied Intelligence},
  month        = {10},
  number       = {10},
  pages        = {3350-3366},
  shortjournal = {Appl. Intell.},
  title        = {A novel discrete whale optimization algorithm for solving knapsack problems},
  volume       = {50},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Modeling of complex internal logic for knowledge base
completion. <em>APIN</em>, <em>50</em>(10), 3336–3349. (<a
href="https://doi.org/10.1007/s10489-020-01734-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Knowledge base completion has been an active research topic for knowledge graph. However, existing methods are of low learning and generalization abilities, and neglect the rich internal logic between entities and relationships. To solve the above problems, this paper proposes the modeling of complex internal logic for knowledge base completion. This method first integrates the semantic information into the knowledge representation model and strengthens the credibility scores of the positive and negative triples with the semantic gap, which not only makes the model converge faster, but also can obtain the knowledge representation of the fusion semantic information; and then we put forward the concept of knowledge subgraph, through the memory network and multi-hop attention mechanism, the knowledge information in the knowledge subgraph and the to-be-complemented triple are merged. In the process of model training, we have different training methods from the classical memory network, and added reinforcement learning. The reciprocal of the correct reasoning knowledge information in the model output is used as the reward value, and the final training model complements the triple information. The high computing capability of knowledge representation, the high learning and generalization abilities of the memory network and the multi-hop attention mechanism are also utilized in the method. The experimental results on data sets FB15k and WN18 show that the present method performs well in knowledge base completion and can effectively improve Hits@10 and MRR values. We also verified the practicability of the proposed method in the recommendation system and question answering system base on knowledge base, and have achieved good results.},
  archive      = {J_APIN},
  author       = {Wang, Hongbin and Jiang, Shengchen and Yu, Zhengtao},
  doi          = {10.1007/s10489-020-01734-z},
  journal      = {Applied Intelligence},
  month        = {10},
  number       = {10},
  pages        = {3336-3349},
  shortjournal = {Appl. Intell.},
  title        = {Modeling of complex internal logic for knowledge base completion},
  volume       = {50},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A MCMEIF-LT model for risk assessment based on linguistic
terms and risk attitudes. <em>APIN</em>, <em>50</em>(10), 3318–3335. (<a
href="https://doi.org/10.1007/s10489-020-01737-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to the limitation of assessors’ knowledge and the uncertainty of risks, risk assessment data reasonably are given in the form of linguistic terms, safety risk assessment in petrochemical industry is often a multi-criterion and multi-expert information fusion based on linguistic terms(MCMEIF-LT) problem. A novel model dealing with the MCMEIF-LT problem is presented in this paper. Firstly, the individual linguistic assessment distributions are fused to collective distributions and multiple criteria are fused to a comprehensive criterion. In the fusion process, the objective weights of assessment experts are calculated with using the credibilities of assessment data and the attitudes of decision makers are considered. Secondly, a Fuzzy Number Weighted Ordered Weighted Aggregation(FN-WOWA) operator which can transform a fuzzy number into a crisp value is proposed. In the FN-WOWA operator, the utility function can incorporate the assessors’ loss-based risk attitudes and the membership function can reflect the importance of the values in the integrated fuzzy number. Based on crisp values, a risk matrix is constructed. Finally, a real application is demonstrated to show the flexibility and practicality of the MCMEIF-LT model.},
  archive      = {J_APIN},
  author       = {Tian, Donghong and Min, Chao and Li, Lingna and Gao, Jie},
  doi          = {10.1007/s10489-020-01737-w},
  journal      = {Applied Intelligence},
  month        = {10},
  number       = {10},
  pages        = {3318-3335},
  shortjournal = {Appl. Intell.},
  title        = {A MCMEIF-LT model for risk assessment based on linguistic terms and risk attitudes},
  volume       = {50},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Combining a gradient-based method and an evolution strategy
for multi-objective reinforcement learning. <em>APIN</em>,
<em>50</em>(10), 3301–3317. (<a
href="https://doi.org/10.1007/s10489-020-01702-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-objective reinforcement learning (MORL) algorithms aim to approximate the Pareto frontier uniformly in multi-objective decision making problems. In the scenario of deep reinforcement learning (RL), gradient-based methods are often adopted to learn deep policies/value functions due to the fast convergence speed, while pure gradient-based methods can not guarantee a uniformly approximated Pareto frontier. On the other side, evolution strategies straightly manipulate in the solution space to achieve a well-distributed Pareto frontier, but applying evolution strategies to optimize deep networks is still a challenging topic. To leverage the advantages of both kinds of methods, we propose a two-stage MORL framework combining a gradient-based method and an evolution strategy. First, an efficient multi-policy soft actor-critic algorithm is proposed to learn multiple policies collaboratively. The lower layers of all policy networks are shared. The first-stage learning can be regarded as representation learning. Secondly, the multi-objective covariance matrix adaptation evolution strategy (MO-CMA-ES) is applied to fine-tune policy-independent parameters to approach a dense and uniform estimation of the Pareto frontier. Experimental results on three benchmarks (Deep Sea Treasure, Adaptive Streaming, and Super Mario Bros) show the superiority of the proposed method.},
  archive      = {J_APIN},
  author       = {Chen, Diqi and Wang, Yizhou and Gao, Wen},
  doi          = {10.1007/s10489-020-01702-7},
  journal      = {Applied Intelligence},
  month        = {10},
  number       = {10},
  pages        = {3301-3317},
  shortjournal = {Appl. Intell.},
  title        = {Combining a gradient-based method and an evolution strategy for multi-objective reinforcement learning},
  volume       = {50},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Improving graph-based label propagation algorithm with group
partition for fraud detection. <em>APIN</em>, <em>50</em>(10),
3291–3300. (<a
href="https://doi.org/10.1007/s10489-020-01724-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fraudulent user detection is a crucial issue in financial risk management. Due to the lack of labeled data and the reliability of labeling, label propagation algorithms (LPA) are effective solutions in this scenario. Most existing models only propagate the risk probabilities for individual users through feature level, while ignoring the real-world graph structure and the characteristics of gang crime. This paper improves the graph-based LPA through group partition, which can be directly implemented on the graph at hand with full consideration of the group information. The exhaustive experimental results testify the performance of our proposed model KGLPA over other off-the-shelf models and amend the insufficiency of feature-based LPA with higher reliability and stability to improve the detection of fraudulent users and secure the marketing budgets.},
  archive      = {J_APIN},
  author       = {Wang, Jiahui and Guo, Yi and Wen, Xinxiu and Wang, Zhihong and Li, Zhen and Tang, Minwei},
  doi          = {10.1007/s10489-020-01724-1},
  journal      = {Applied Intelligence},
  month        = {10},
  number       = {10},
  pages        = {3291-3300},
  shortjournal = {Appl. Intell.},
  title        = {Improving graph-based label propagation algorithm with group partition for fraud detection},
  volume       = {50},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). An improved CapsNet applied to recognition of 3D vertebral
images. <em>APIN</em>, <em>50</em>(10), 3276–3290. (<a
href="https://doi.org/10.1007/s10489-020-01695-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning is currently widely applied in medical image processing and has achieved good results. However, recognizing vertebrae via image processing remains a challenging problem due to their complex spatial structures. CapsNet is a newly proposed network whose characteristics compensate for some shortcomings of traditional CNNs, and it has been shown to perform well on many tasks, including medical image recognition. In this paper, we applied a modified CapsNet to recognise 3D vertebral images by introducing an RNN module into CapsNet to further enhance its learning ability. This new network is called RNNinCaps, and it achieves the highest recognition performance on 3D vertebral images (the average accuracy of RNNinCaps exceeds the accuracy of the original CapsNet by 46.2% and that of a traditional CNN by 12.6%). RNNinCaps also performs better than several mainstream networks. RNNinCaps can promotes CapsNet’s application in the field of 3D medical image recognition.},
  archive      = {J_APIN},
  author       = {Wang, Hao and Shao, Kun and Huo, Xing},
  doi          = {10.1007/s10489-020-01695-3},
  journal      = {Applied Intelligence},
  month        = {10},
  number       = {10},
  pages        = {3276-3290},
  shortjournal = {Appl. Intell.},
  title        = {An improved CapsNet applied to recognition of 3D vertebral images},
  volume       = {50},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Generalization of dempster–shafer theory: A complex mass
function. <em>APIN</em>, <em>50</em>(10), 3266–3275. (<a
href="https://doi.org/10.1007/s10489-019-01617-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dempster–Shafer evidence theory has been widely used in various fields of applications, because of the flexibility and effectiveness in modeling uncertainties without prior information. However, the existing evidence theory is insufficient to consider the situations where it has no capability to express the fluctuations of data at a given phase of time during their execution, and the uncertainty and imprecision which are inevitably involved in the data occur concurrently with changes to the phase or periodicity of the data. In this paper, therefore, a generalized Dempster–Shafer evidence theory is proposed. To be specific, a mass function in the generalized Dempster–Shafer evidence theory is modeled by a complex number, called as a complex basic belief assignment, which has more powerful ability to express uncertain information. Based on that, a generalized Dempster’s combination rule is exploited. In contrast to the classical Dempster’s combination rule, the condition in terms of the conflict coefficient between the evidences is released in the generalized Dempster’s combination rule. Hence, it is more general and applicable than the classical Dempster’s combination rule. When the complex mass function is degenerated from complex numbers to real numbers, the generalized Dempster’s combination rule degenerates to the classical evidence theory under the condition that the conflict coefficient between the evidences is less than 1. In a word, this generalized Dempster–Shafer evidence theory provides a promising way to model and handle more uncertain information. Thanks to this advantage, an algorithm for decision-making is devised based on the generalized Dempster–Shafer evidence theory. Finally, an application in a medical diagnosis illustrates the efficiency and practicability of the proposed algorithm.},
  archive      = {J_APIN},
  author       = {Xiao, Fuyuan},
  doi          = {10.1007/s10489-019-01617-y},
  journal      = {Applied Intelligence},
  month        = {10},
  number       = {10},
  pages        = {3266-3275},
  shortjournal = {Appl. Intell.},
  title        = {Generalization of Dempster–Shafer theory: A complex mass function},
  volume       = {50},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A recurrent neural network for urban long-term traffic flow
forecasting. <em>APIN</em>, <em>50</em>(10), 3252–3265. (<a
href="https://doi.org/10.1007/s10489-020-01716-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper investigates the use of recurrent neural network to predict urban long-term traffic flows. A representation of the long-term flows with related weather and contextual information is first introduced. A recurrent neural network approach, named RNN-LF, is then proposed to predict the long-term of flows from multiple data sources. Moreover, a parallel implementation on GPU of the proposed solution is developed (GRNN-LF), which allows to boost the performance of RNN-LF. Several experiments have been carried out on real traffic flow including a small city (Odense, Denmark) and a very big city (Beijing). The results reveal that the sequential version (RNN-LF) is capable of dealing effectively with traffic of small cities. They also confirm the scalability of GRNN-LF compared to the most competitive GPU-based software tools when dealing with big traffic flow such as Beijing urban data.},
  archive      = {J_APIN},
  author       = {Belhadi, Asma and Djenouri, Youcef and Djenouri, Djamel and Lin, Jerry Chun-Wei},
  doi          = {10.1007/s10489-020-01716-1},
  journal      = {Applied Intelligence},
  month        = {10},
  number       = {10},
  pages        = {3252-3265},
  shortjournal = {Appl. Intell.},
  title        = {A recurrent neural network for urban long-term traffic flow forecasting},
  volume       = {50},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Effective node selection technique towards sparse learning.
<em>APIN</em>, <em>50</em>(10), 3239–3251. (<a
href="https://doi.org/10.1007/s10489-020-01720-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Neural networks are getting wider and deeper to achieve state-of-the-art results in various machine learning domains. Such networks result in complex structures, high model size, and computational costs. Moreover, these networks are failing to adapt to new data due to their isolation in the specific domain-target space. To tackle these issues, we propose a sparse learning method to train the existing network on new classes by selecting non-crucial parameters from the network. Sparse learning also manages to keep the performance of existing classes with no additional network structure and memory costs by employing an effective node selection technique, which analyzes and selects unimportant parameters by using information theory in the neuron distribution of the fully connected layers. Our method could learn up to 40% novel classes without notable loss in the accuracy of existing classes. Through experiments, we show how a sparse learning method competes with state-of-the-art methods in terms of accuracy and even surpasses the performance of related algorithms in terms of efficiency in memory, processing speed, and overall training time. Importantly, our method can be implemented in both small and large applications, and we justify this by using well-known networks such as LeNet, AlexNet, and VGG-16.},
  archive      = {J_APIN},
  author       = {Ibrokhimov, Bunyodbek and Hur, Cheonghwan and Kang, Sanggil},
  doi          = {10.1007/s10489-020-01720-5},
  journal      = {Applied Intelligence},
  month        = {10},
  number       = {10},
  pages        = {3239-3251},
  shortjournal = {Appl. Intell.},
  title        = {Effective node selection technique towards sparse learning},
  volume       = {50},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Predictive intelligence of reliable analytics in distributed
computing environments. <em>APIN</em>, <em>50</em>(10), 3219–3238. (<a
href="https://doi.org/10.1007/s10489-020-01712-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Lack of knowledge in the underlying data distribution in distributed large-scale data can be an obstacle when issuing analytics &amp; predictive modelling queries. Analysts find themselves having a hard time finding analytics/exploration queries that satisfy their needs. In this paper, we study how exploration query results can be predicted in order to avoid the execution of ‘bad’/non-informative queries that waste network, storage, financial resources, and time in a distributed computing environment. The proposed methodology involves clustering of a training set of exploration queries along with the cardinality of the results (score) they retrieved and then using query-centroid representatives to proceed with predictions. After the training phase, we propose a novel refinement process to increase the reliability of predicting the score of new unseen queries based on the refined query representatives. Comprehensive experimentation with real datasets shows that more reliable predictions are acquired after the proposed refinement method, which increases the reliability of the closest centroid and improves predictability under the right circumstances.},
  archive      = {J_APIN},
  author       = {Kathidjiotis, Yiannis and Kolomvatsos, Kostas and Anagnostopoulos, Christos},
  doi          = {10.1007/s10489-020-01712-5},
  journal      = {Applied Intelligence},
  month        = {10},
  number       = {10},
  pages        = {3219-3238},
  shortjournal = {Appl. Intell.},
  title        = {Predictive intelligence of reliable analytics in distributed computing environments},
  volume       = {50},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A novel approach for multi-cue feature fusion for robust
object tracking. <em>APIN</em>, <em>50</em>(10), 3201–3218. (<a
href="https://doi.org/10.1007/s10489-020-01649-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Object tracking is a significant problem of computer vision due to challenging environmental variations. Single cue appearance model is not sufficient to handle the variations. To this end, we propose a multi-cue tracking framework in which complementary cues namely, LBP and HOG were exploited to develop a robust appearance model. The proposed feature fusion captures the high-level relationship between the features and diminishes the low-level relationship. Transductive reliability is also integrated at each frame to make tracker adaptive with the changing environment. In addition, K-Means based classifier creates clear and concise boundary between positive and negative fragments which are further used to update the reference dictionary. This adaptation strategy prevents the erroneous updation of the proposed tracker during background clutters, occlusion, and fast motion. Qualitative and quantitative analysis on challenging video sequences from OTB-100 dataset, VOT dataset and UAV123 reveal that the proposed tracker performs favorably against 13 others state-of-the-art trackers.},
  archive      = {J_APIN},
  author       = {Kumar, Ashish and Walia, Gurjit Singh and Sharma, Kapil},
  doi          = {10.1007/s10489-020-01649-9},
  journal      = {Applied Intelligence},
  month        = {10},
  number       = {10},
  pages        = {3201-3218},
  shortjournal = {Appl. Intell.},
  title        = {A novel approach for multi-cue feature fusion for robust object tracking},
  volume       = {50},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Detecting salient regions in a bi-temporal hyperspectral
scene by iterating clustering and classification. <em>APIN</em>,
<em>50</em>(10), 3179–3200. (<a
href="https://doi.org/10.1007/s10489-020-01701-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hyperspectral (HS) images captured from Earth by satellite and aircraft have become increasingly important in several environmental and ecological contexts (e.g. agriculture and urban areas). In the present study we propose an iterative learning methodology for the change detection of HS scenes taken at different times in the same areas. It cascades clustering and classification through iterative learning, in order to separate salient regions, where a change occurs in the scene from the unchanged background. The iterative learning is evaluated in both the clustering and the classification steps. The experiments performed with the proposed methodology provide encouraging results, also compared to several recent state-of-the-art competitors.},
  archive      = {J_APIN},
  author       = {Appice, Annalisa and Guccione, Pietro and Acciaro, Emilio and Malerba, Donato},
  doi          = {10.1007/s10489-020-01701-8},
  journal      = {Applied Intelligence},
  month        = {10},
  number       = {10},
  pages        = {3179-3200},
  shortjournal = {Appl. Intell.},
  title        = {Detecting salient regions in a bi-temporal hyperspectral scene by iterating clustering and classification},
  volume       = {50},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). An intrusion detection approach based on improved deep
belief network. <em>APIN</em>, <em>50</em>(10), 3162–3178. (<a
href="https://doi.org/10.1007/s10489-020-01694-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In today’s interconnected society, cyberattacks have become more frequent and sophisticated, and existing intrusion detection systems may not be adequate in the complex cyberthreat landscape. For instance, existing intrusion detection systems may have overfitting, low classification accuracy, and high false positive rate (FPR) when faced with significantly large volume and variety of network data. An intrusion detection approach based on improved deep belief network (DBN) is proposed in this paper to mitigate the above problems, where the dataset is processed by probabilistic mass function (PMF) encoding and Min-Max normalization method to simplify the data preprocessing. Furthermore, a combined sparsity penalty term based on Kullback-Leibler (KL) divergence and non-mean Gaussian distribution is introduced in the likelihood function of the unsupervised training phase of DBN, and sparse constraints retrieve the sparse distribution of the dataset, thus avoiding the problem of feature homogeneity and overfitting. Finally, simulation experiments are performed on the NSL-KDD and UNSW-NB15 public datasets. The proposed method achieves 96.17% and 86.49% accuracy, respectively. Experimental results show that compared with the state-of-the-art methods, the proposed method achieves significant improvement in classification accuracy and FPR.},
  archive      = {J_APIN},
  author       = {Tian, Qiuting and Han, Dezhi and Li, Kuan-Ching and Liu, Xingao and Duan, Letian and Castiglione, Arcangelo},
  doi          = {10.1007/s10489-020-01694-4},
  journal      = {Applied Intelligence},
  month        = {10},
  number       = {10},
  pages        = {3162-3178},
  shortjournal = {Appl. Intell.},
  title        = {An intrusion detection approach based on improved deep belief network},
  volume       = {50},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Brain storm optimization using a slight relaxation selection
and multi-population based creating ideas ensemble. <em>APIN</em>,
<em>50</em>(10), 3137–3161. (<a
href="https://doi.org/10.1007/s10489-020-01690-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Brain storm optimization is a swarm intelligence algorithm inspired by the brainstorming process in human beings. Many researchers have paid much more attention to it, and many attempts have been made to improve it’s performance. The search ability of brain storm optimization is maintained by the creating process of ideas, but it still suffers from sticking into stagnation during exploitation phase. This paper proposes a novel brain storm optimization variant, named RMBSO, in which a slight relaxation selection and multi-population based creating ideas ensemble are employed to improve the performance of brain storm optimization on global optimization problem with diverse landscapes. Firstly, the basic framework of original brain storm optimization is imbedded into multi-population based ensemble of heterogeneous but complementary creating ideas to make the algorithm jump out of stagnation with strong searching ability. Secondly, a new triangular mutation ruler and a simple partition of subpopulations are designed to better balance exploration and exploitation. Thirdly, a slight relaxation selection mechanism instead of greedy choice is first developed to keep the population’s diversity. Finally, extensive experiments on the suit of CEC 2015 benchmark functions and statistical comparisons are executed. Experimental results indicate that the proposed algorithm is significantly better than, or at least comparable to the state-of-the-art brain storm optimization variants and several improved differential evolution algorithms.},
  archive      = {J_APIN},
  author       = {Sun, Yuehong and Wei, Jianxiang and Wu, Tingting and Xiao, Kelian and Bao, Jianyang and Jin, Ye},
  doi          = {10.1007/s10489-020-01690-8},
  journal      = {Applied Intelligence},
  month        = {10},
  number       = {10},
  pages        = {3137-3161},
  shortjournal = {Appl. Intell.},
  title        = {Brain storm optimization using a slight relaxation selection and multi-population based creating ideas ensemble},
  volume       = {50},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Finding every car: A traffic surveillance multi-scale
vehicle object detection method. <em>APIN</em>, <em>50</em>(10),
3125–3136. (<a
href="https://doi.org/10.1007/s10489-020-01704-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {According to the problem that the multi-scale vehicle objects in traffic surveillance video are difficult to detect and the overlapping objects are prone to missed detection, an improved vehicle object detection method based on YOLOv3 was proposed. In order to extract feature more efficiently, we first use the inverted residuals technique to improve the convolutional layer of YOLOv3. To solve the multi-scale vehicle object detection problem, three spatial pyramid pooling(SPP) modules are added before each YOLO layer to obtain multi-scale information. In order to cope with the overlapping of vehicles in traffic videos, soft non maximum suppression (Soft-NMS) is used to replace non maximum suppression (NMS), thereby reducing the missing of predicted boxes due to vehicle overlaps. Our experiment results in the Car dataset and the KITTI dataset confirm that the proposed method achieves good detection results for vehicle objects of various scales in various scenes. Our method can meet the needs of practical applications better.},
  archive      = {J_APIN},
  author       = {Mao, Qi-Chao and Sun, Hong-Mei and Zuo, Ling-Qun and Jia, Rui-Sheng},
  doi          = {10.1007/s10489-020-01704-5},
  journal      = {Applied Intelligence},
  month        = {10},
  number       = {10},
  pages        = {3125-3136},
  shortjournal = {Appl. Intell.},
  title        = {Finding every car: A traffic surveillance multi-scale vehicle object detection method},
  volume       = {50},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Analysis of zig-zag scan based modified feedback convolution
algorithm against differential attacks and its application to image
encryption. <em>APIN</em>, <em>50</em>(10), 3101–3124. (<a
href="https://doi.org/10.1007/s10489-020-01697-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, a novel zig-zag scan-based feedback convolution algorithm for image encryption against differential attacks is proposed. The two measures Number of Pixel Change Rate (NPCR) and Unified Average Changed Intensity (UACI) are commonly utilized for analyzing the differential attacks. From the study of the existing papers, even though high Number of Pixel Change Rate and Unified Average Changed Intensity values are obtained, a few values lie in the critical range of α-level significance which in turn increase the possibility of differential attacks. To overcome differential attacks, two aspects of scanning with different test cases are analyzed and from these analyses, it is concluded that zig-zag scan based feedback convolution in forward and reverse direction achieves good Number of Pixel Change Rate and Unified Average Changed Intensity without critical values. Zig-zag scan based feedback convolution in forward and reverse direction is enforced for key sequence generation and applied in diffusion process to achieve high level of security. Moreover, plain image related initial seed is also generated to overcome the chosen/known plain text attacks. Both numerical and theoretical analyses are performed to prove that the proposed encryption method is resistant to differential attacks. General security measures are carried out for the proposed method to validate its security level. From the simulations, it is shown that the proposed methodology has good keyspace, high key sensitivity, good randomness, and uniform distribution of cipher image pixels.},
  archive      = {J_APIN},
  author       = {Vidhya, R. and Brindha, M. and Gounden, N. Ammasai},
  doi          = {10.1007/s10489-020-01697-1},
  journal      = {Applied Intelligence},
  month        = {10},
  number       = {10},
  pages        = {3101-3124},
  shortjournal = {Appl. Intell.},
  title        = {Analysis of zig-zag scan based modified feedback convolution algorithm against differential attacks and its application to image encryption},
  volume       = {50},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A new re-encoding ECOC using reject option. <em>APIN</em>,
<em>50</em>(10), 3090–3100. (<a
href="https://doi.org/10.1007/s10489-020-01642-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {When training base classifier by ternary Error Correcting Output Codes (ECOC), it is well know that some classes are ignored. On this account, a non-competent classifier emerges when it classify an instance whose real label does not belong to the meta-subclasses. Meanwhile, the classic ECOC dichotomizers can only produce binary outputs and have no capability of rejection for classification. To overcome the non-competence problem and better model the multi-class problem for reducing the classification cost, we embed reject option to ECOC and present a new variant of ECOC algorithm called as Reject-Option-based Re-encoding ECOC (ROECOC). The cost-sensitive classification model and cost-loss function based on Receiver Operating Characteristic (ROC) curve are built respectively. The optimal reject threshold values are obtained by combing the condition to be met for minimizing the loss function and the ROC convex hull. In so doing, reject option (t1, t2) provides a three-symbol output to make dichotomizers more competent and ROECOC more universal and practical for cost-sensitive classification issue. Experimental results on two kinds of datasets show that our scheme with low-degree freedom of initialized ECOC can effectively enhance accuracy and reduce cost.},
  archive      = {J_APIN},
  author       = {Lei, Lei and Song, Yafei and Luo, Xi},
  doi          = {10.1007/s10489-020-01642-2},
  journal      = {Applied Intelligence},
  month        = {10},
  number       = {10},
  pages        = {3090-3100},
  shortjournal = {Appl. Intell.},
  title        = {A new re-encoding ECOC using reject option},
  volume       = {50},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Particle swarm optimization based parameter selection
technique for unsupervised discriminant analysis in transfer learning
framework. <em>APIN</em>, <em>50</em>(10), 3071–3089. (<a
href="https://doi.org/10.1007/s10489-020-01710-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The purpose of transfer learning is to utilize the knowledge gained from the existing (source) domain to enhance the performance on a distinct but related (target) domain. Existing works on transfer learning are not capable of optimizing different quality measures (components) such as minimizing the marginal distribution, minimizing the conditional distribution, maximizing the target domain variance, modeling the manifold by utilizing the common geometric properties in the source as well as the target domain at the same time. Moreover, existing transfer learning methods use conventional approaches to determine the appropriate values of their parameters, which is very hectic and time-consuming. Therefore, in order to overcome the drawbacks of existing approaches, we propose a Particle Swarm Optimization based Parameter Selection Approach for Unsupervised Discriminant Analysis (UDATL-PSO) in transfer learning framework. In UDATL-PSO, all the quality measures are considered at the same time, as well as the PSO approach has been used to select the best values of their parameters. Extensive experiments on various transfer learning tasks show that the proposed method has a significant influence on state-of-the-art methods.},
  archive      = {J_APIN},
  author       = {Sanodiya, Rakesh Kumar and Mathew, Jimson and Saha, Sriparna and Tripathi, Piyush},
  doi          = {10.1007/s10489-020-01710-7},
  journal      = {Applied Intelligence},
  month        = {10},
  number       = {10},
  pages        = {3071-3089},
  shortjournal = {Appl. Intell.},
  title        = {Particle swarm optimization based parameter selection technique for unsupervised discriminant analysis in transfer learning framework},
  volume       = {50},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Deep spatial-temporal networks for crowd flows prediction by
dilated convolutions and region-shifting attention mechanism.
<em>APIN</em>, <em>50</em>(10), 3057–3070. (<a
href="https://doi.org/10.1007/s10489-020-01698-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Flow prediction at a citywide level is of great significance to traffic management and public safety. Since deep learning has achieved success to deal with complex nonlinear problems, it has drawn increasing attention on making crowd flows prediction through neural networks. Generally, convolutional neural network (CNN) and recurrent neural network (RNN) have been applied to model the spatial-temporal dependency of the city. However, there are still two major challenges in predicting flows. First, it is difficult to train the model with the ability to capture both the nearby and distant spatial dependency by deep local convolutions. Second, daily and weekly patterns in temporal dependency are not strictly periodic for their dynamic temporal shifting in each region. To address these issues, we propose a novel deep learning model which called Local-Dilated Region-Shifting Network (LDRSN). LDRSN combines local convolutions with dilated convolutions to learn the nearby and distant spatial dependency. Furthermore, a new region-level attention mechanism is proposed to model the temporal shifting which varies by region. In the experiments, we compare the proposed method with other state-of-the-art methods in two real-world crowd flows datasets. The Mean Absolute Error (MAE), Mean Absolute Percentage Error (MAPE), Root Mean Square Error (RMSE) were used as the evaluation indexes. The experiment results show the effectiveness of the proposed model.},
  archive      = {J_APIN},
  author       = {Tian, Chujie and Zhu, Xinning and Hu, Zheng and Ma, Jian},
  doi          = {10.1007/s10489-020-01698-0},
  journal      = {Applied Intelligence},
  month        = {10},
  number       = {10},
  pages        = {3057-3070},
  shortjournal = {Appl. Intell.},
  title        = {Deep spatial-temporal networks for crowd flows prediction by dilated convolutions and region-shifting attention mechanism},
  volume       = {50},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Projection multi-birth support vector machinea for
multi-classification. <em>APIN</em>, <em>50</em>(10), 3040–3056. (<a
href="https://doi.org/10.1007/s10489-020-01699-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As an important multi-classification learning tool, multi-birth support vector machine (MBSVM) has been widely studied and applied due to its low computational complexity and good generalization. In this paper, a new multi-birth support vector machine is proposed to handle multi-class classification problem, called projection multi-birth support vector machine (PMBSVM). Specifically, we intend to seek a projection direction wk for k-th class, so that the covariance of remaining samples (except the k-th class) is as small as possible, and the samples of k-th class are as far as possible from the mean of the remaining samples. The proposed PMBSVM not only inherits the advantages of MBSVM, but also can find a suitable projection direction for each class so that the sample is separable in the projection space. Additionally, a regularization term is introduced to maximize the margin of different classes in the projected space. Moreover, a recursive PMBSVM algorithm is proposed for generating multiple orthogonal projection directions for each class. Then we extend the proposed approaches to nonlinear situations through kernel technology. Simulation results on benchmark datasets show that the proposed algorithms improve the generalization in most cases.},
  archive      = {J_APIN},
  author       = {Wen, Yakun and Ma, Jun and Yuan, Chao and Yang, Liming},
  doi          = {10.1007/s10489-020-01699-z},
  journal      = {Applied Intelligence},
  month        = {10},
  number       = {10},
  pages        = {3040-3056},
  shortjournal = {Appl. Intell.},
  title        = {Projection multi-birth support vector machinea for multi-classification},
  volume       = {50},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Elective future: The influence factor mining of students’
graduation development based on hierarchical attention neural network
model with graph. <em>APIN</em>, <em>50</em>(10), 3023–3039. (<a
href="https://doi.org/10.1007/s10489-020-01692-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The graduation development such as employment and graduate school admission of college students are important tasks. However, mining the factors that can affect the development of graduation remains challenging, because the most important factor “course” is not independent and inequality, which are always ignored by previous researchers. Furthermore, traditional structured methods cannot handle the complex relationships between courses, and attention networks cannot distinguish the weights of compulsory and elective courses with different distributions. Therefore, we present a Graph-based Hierarchical Attention Neural Network Model with Elective Course (GHANN-EC) for the prediction of graduation development in this study. Specifically, we use graph embedding that captures the unstructured relationships between courses and hierarchical attention that assigns the importance of the courses to excavating course information that represent students’ independent interests, and can more accurately understand the relationship between graduation development and academic performance. Experimental results on the real-world datasets show that GHANN-EC outperforms the existing popular approach.},
  archive      = {J_APIN},
  author       = {Ouyang, Yong and Zeng, Yawen and Gao, Rong and Yu, Yonghong and Wang, Chunzhi},
  doi          = {10.1007/s10489-020-01692-6},
  journal      = {Applied Intelligence},
  month        = {10},
  number       = {10},
  pages        = {3023-3039},
  shortjournal = {Appl. Intell.},
  title        = {Elective future: The influence factor mining of students’ graduation development based on hierarchical attention neural network model with graph},
  volume       = {50},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Local keypoint-based faster r-CNN. <em>APIN</em>,
<em>50</em>(10), 3007–3022. (<a
href="https://doi.org/10.1007/s10489-020-01665-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Region-based Convolutional Neural Network (R-CNN) detectors have achieved state-of-the-art results on various challenging benchmarks. Although R-CNN has achieved high detection performance, the research of local information in producing candidates is insufficient. In this paper, we design a Keypoint-based Faster R-CNN (K-Faster) method for object detection. K-Faster incorporates local keypoints in Faster R-CNN to improve the detection performance. In detail, a sparse descriptor, which first detects the points of interest in a given image and then samples a local patch and describes its invariant features, is first employed to produce keypoints. All 2-combinations of the produced keypoints are second selected to generate keypoint anchors, which are helpful for object detection. The heterogeneously distributed anchors are then encoded in feature maps based on their areas and center coordinates. Finally, the keypoint anchors are coupled with the anchors produced by Faster R-CNN, and the coupled anchors are used for Region Proposal Network (RPN) training. Comparison experiments are implemented on PASCAL VOC 07/12 and MS COCO. The experimental results show that our K-Faster approach not only increases the mean Average Precision (mAP) performance but also improves the positioning precision of the detected boxes.},
  archive      = {J_APIN},
  author       = {Ding, Xintao and Li, Qingde and Cheng, Yongqiang and Wang, Jinbao and Bian, Weixin and Jie, Biao},
  doi          = {10.1007/s10489-020-01665-9},
  journal      = {Applied Intelligence},
  month        = {10},
  number       = {10},
  pages        = {3007-3022},
  shortjournal = {Appl. Intell.},
  title        = {Local keypoint-based faster R-CNN},
  volume       = {50},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Multi objective programming problem in the hesitant fuzzy
environment. <em>APIN</em>, <em>50</em>(10), 2991–3006. (<a
href="https://doi.org/10.1007/s10489-020-01682-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we introduce a hesitant fuzzy multi-objective programming problem, in which the evaluation information provided by the decision makers is expressed in a hesitant fuzzy environment. For this purpose a new solution concept, namely hesitant fuzzy Pareto optimal solution to the problem is introduced, and two methods are proposed to obtain it. Then it is shown that the optimal solutions of these methods are the hesitant fuzzy Pareto optimal solutions. Finally, these methods are implemented on some illustrative examples and comparative analysis of our methodology is taken with other extensions of fuzzy sets.},
  archive      = {J_APIN},
  author       = {Rouhbakhsh, F. F. and Ranjbar, M. and Effati, S. and Hassanpour, H.},
  doi          = {10.1007/s10489-020-01682-8},
  journal      = {Applied Intelligence},
  month        = {10},
  number       = {10},
  pages        = {2991-3006},
  shortjournal = {Appl. Intell.},
  title        = {Multi objective programming problem in the hesitant fuzzy environment},
  volume       = {50},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Salient object detection based on distribution-edge guidance
and iterative bayesian optimization. <em>APIN</em>, <em>50</em>(10),
2977–2990. (<a
href="https://doi.org/10.1007/s10489-020-01691-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Salient object detection has witnessed rapid progress, despite most existing methods still struggling in complex scenes, unfortunately. In this paper, we propose an efficient framework for salient object detection based on distribution-edge guidance and iterative Bayesian optimization. By considering color, spatial, and edge information, a discriminative metric is first constructed to measure the similarity between different regions. Next, boundary prior embedded with background scatter distribution is utilized to yield the boundary contrast map, and then a contour completeness map is derived through a wholly closed shape of the object. Finally, the above both maps are jointly integrated into an iterative Bayesian optimization framework to obtain the final saliency map. Results from an extensive number of experimentations demonstrate that the promising performance of the proposed algorithm against the state-of-the-art saliency detection methods in terms of different evaluation metrics on several benchmark datasets.},
  archive      = {J_APIN},
  author       = {Xia, Chenxing and Gao, Xiuju and Li, Kuan-Ching and Zhao, Qianjin and Zhang, Shunxiang},
  doi          = {10.1007/s10489-020-01691-7},
  journal      = {Applied Intelligence},
  month        = {10},
  number       = {10},
  pages        = {2977-2990},
  shortjournal = {Appl. Intell.},
  title        = {Salient object detection based on distribution-edge guidance and iterative bayesian optimization},
  volume       = {50},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A novel framework of fuzzy oblique decision tree
construction for pattern classification. <em>APIN</em>, <em>50</em>(9),
2959–2975. (<a
href="https://doi.org/10.1007/s10489-020-01675-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, some significant efforts on fuzzy oblique decision tree (FODT) have been done to improve classification accuracy and decrease tree size. Firstly, to eliminate data redundancy and improve classification efficiency, a forward greedy fast feature selection algorithm based on neighborhood rough set (NRS_FS_FAST) is introduced. Then, a new fuzzy rule generation algorithm (FRGA) is proposed to generate fuzzy rules. These fuzzy rules are used to construct leaf nodes for each class in each layer of the FODT. Different from the traditional axis-parallel decision trees and oblique decision trees, the FODT takes dynamic mining fuzzy rules as decision functions. Moreover, the parameter δ, which can control the size of the tree, is optimized by genetic algorithm. Finally, a series of comparative experiments are carried out with five traditional decision trees (C4.5, Best First Tree (BFT), amulti-class alternating decision tree (LAD), Simple Cart (SC), Naive Bayes Tree (NBT)), and recently proposed decision trees (FRDT, HHCART, and FMMDT-HB) on UCI machine learning datasets. The experimental results demonstrate that the FODT exhibits better performance on classification accuracy and tree size than the chosen benchmarks.},
  archive      = {J_APIN},
  author       = {Cai, Yuliang and Zhang, Huaguang and He, Qiang and Duan, Jie},
  doi          = {10.1007/s10489-020-01675-7},
  journal      = {Applied Intelligence},
  month        = {9},
  number       = {9},
  pages        = {2959-2975},
  shortjournal = {Appl. Intell.},
  title        = {A novel framework of fuzzy oblique decision tree construction for pattern classification},
  volume       = {50},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A mix-supervised unified framework for salient object
detection. <em>APIN</em>, <em>50</em>(9), 2945–2958. (<a
href="https://doi.org/10.1007/s10489-020-01700-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, although deep learning network has shown its advantages in supervised salient object detection, supervised models often require massive pixel-wise annotations and learnable parameters, which seriously manacle training and testing of models. In this paper, we present a mix-supervised unified framework for salient object detection to avoid the insufficient training labels and speed training and testing up, which is composed of a region-wise stream and a pixel-wise stream. In the region-wise stream, to avoid the requirement of expensive pixel-wise annotations, an improved energy equation based manifold learning algorithm is employed, by which accurate object location and prior knowledge are introduced by the unsupervised learning. In the pixel-wise stream, to alleviate the problem of time-consuming, a simplified bi-directional reuse network is introduced, which can obtain clear object contour and competitive performance with fewer parameters. To relieve the bottleneck pressure of parallel training and testing, each steam is directly connected to its pre-processed color feature and post-processing refinement. Extensive experiments demonstrate that each component contributes to the final results and complement each other perfectly.},
  archive      = {J_APIN},
  author       = {Jia, Fengwei and Guan, Jian and Qi, Shuhan and Li, Huale and Wang, Xuan},
  doi          = {10.1007/s10489-020-01700-9},
  journal      = {Applied Intelligence},
  month        = {9},
  number       = {9},
  pages        = {2945-2958},
  shortjournal = {Appl. Intell.},
  title        = {A mix-supervised unified framework for salient object detection},
  volume       = {50},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Single image deraining via nonlocal squeeze-and-excitation
enhancing network. <em>APIN</em>, <em>50</em>(9), 2932–2944. (<a
href="https://doi.org/10.1007/s10489-020-01693-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Raindrop blur or rain streaks can severely degrade the visual quality of the images, which causes many practical vision systems to fail to work, such as autonomous driving and video surveillance. Hence, it is important to address the problem of single image de-raining. In this paper, we propose a novel deep network for single image de-raining. The proposed network consists of three stages, including encoder stage, Dense Non-Local Residual Block (DNLRB) stage, and decoder stage. As spatial contextual information has been analyzed to be meaningful for image de-raining (Huang et al. ??), we adopt squeeze-and-excitation enhancing on feature maps in each convolution layer for capturing spatial contextual information. In addition, to better leverage spatial contextual information for extracting rain components, the non-local mean operation has been embed in DNLRB. Both quantitative and qualitative experimental results demonstrate the proposed method performs favorably against the state-of-the-art de-raining methods. The source codes will be available at https://supercong94.wixsite.com/supercong94 .},
  archive      = {J_APIN},
  author       = {Wang, Cong and Fan, Wanshu and Zhu, Honghe and Su, Zhixun},
  doi          = {10.1007/s10489-020-01693-5},
  journal      = {Applied Intelligence},
  month        = {9},
  number       = {9},
  pages        = {2932-2944},
  shortjournal = {Appl. Intell.},
  title        = {Single image deraining via nonlocal squeeze-and-excitation enhancing network},
  volume       = {50},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A team of pursuit learning automata for solving
deterministic optimization problems. <em>APIN</em>, <em>50</em>(9),
2916–2931. (<a
href="https://doi.org/10.1007/s10489-020-01657-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Learning Automata (LA) is a popular decision-making mechanism to “determine the optimal action out of a set of allowable actions” [1]. The distinguishing characteristic of automata-based learning is that the search for an optimal parameter (or decision) is conducted in the space of probability distributions defined over the parameter space, rather than in the parameter space itself [2]. In this paper, we propose a novel LA paradigm that can solve a large class of deterministic optimization problems. Although many LA algorithms have been devised in the literature, those LA schemes are not able to solve deterministic optimization problems as they suppose that the environment is stochastic. In this paper, our proposed scheme can be seen as the counterpart of the family of pursuit LA developed for stochastic environments [3]. While classical pursuit LAs can pursue the action with the highest reward estimate, our pursuit LA rather pursues the collection of actions that yield the highest performance by invoking a team of LA. The theoretical analysis of the pursuit scheme does not follow classical LA proofs, and can pave the way towards more schemes where LA can be applied to solve deterministic optimization problems. Furthermore, we analyze the scheme under both a constant learning parameter and a time-decaying learning parameter. We provide some experimental results that show how our Pursuit-LA scheme can be used to solve the Maximum Satisfiability (Max-SAT) problem. To avoid premature convergence and better explore the search space, we enhance our scheme with the concept of artificial barriers recently introduced in [4]. Interestingly, although our scheme is simple by design, we observe that it performs well compared to sophisticated state-of-the-art approaches.},
  archive      = {J_APIN},
  author       = {Yazidi, Anis and Bouhmala, Nourredine and Goodwin, Morten},
  doi          = {10.1007/s10489-020-01657-9},
  journal      = {Applied Intelligence},
  month        = {9},
  number       = {9},
  pages        = {2916-2931},
  shortjournal = {Appl. Intell.},
  title        = {A team of pursuit learning automata for solving deterministic optimization problems},
  volume       = {50},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Multi-view visual bayesian personalized ranking for
restaurant recommendation. <em>APIN</em>, <em>50</em>(9), 2901–2915. (<a
href="https://doi.org/10.1007/s10489-020-01703-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent recommendation systems, the image information of items is often used in conjunction with deep convolution network to directly learn the visual features of items. However, the existing approaches usually use only one image to represent an item. These approaches are inadequate for an item with multi-view related images. For a restaurant, it has visual information of food, drink, environment, and so on. Each view of an item can be represented by multiple images. In this paper, we propose a new factorization model that combines multi-view visual information with the implicit feedback data for restaurant prediction and ranking. The visual features (visual information) of images are extracted by using a deep convolution network and are integrated into a collaborative filtering framework. In order to conduct personalized recommendation better, the multi-view visual features are fused through user related weights. User related weights reflect the personalized visual preference for restaurants and the weights are different and independent between users. We applied this model to make personalized recommendations for users on two real-world restaurant review datasets. Experimental results show that our model with multi-view visual information achieves better performance than models without or with only single-view visual information.},
  archive      = {J_APIN},
  author       = {Zhang, Xiaoyan and Luo, Haihua and Chen, Bowei and Guo, Guibing},
  doi          = {10.1007/s10489-020-01703-6},
  journal      = {Applied Intelligence},
  month        = {9},
  number       = {9},
  pages        = {2901-2915},
  shortjournal = {Appl. Intell.},
  title        = {Multi-view visual bayesian personalized ranking for restaurant recommendation},
  volume       = {50},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A human resource allocation method for business processes
using team faultlines. <em>APIN</em>, <em>50</em>(9), 2887–2900. (<a
href="https://doi.org/10.1007/s10489-020-01686-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Equitable human resource allocation can maximize the efficiency of resources and optimize business performance. Despite numerous methods that have been suggested for solving the allocation problem, most of the existing methods focus on a single resource or task, and neglect the effects of team composition in business performance. In this paper, we introduce team faultlines to the human resource allocation problem. We first analyze resource characteristics from a demographic perspective and business process, then utilize the information value to select key characteristics and determine the corresponding weight. Second, we qualitatively identify team faultlines based on the clustering results of human resources and quantitatively measure the strength and distance of team faultlines. Multi-layer perceptron is utilized to build the base and ensemble performance prediction model. The allocation model and flow are designed subsequently. The reasonableness and effectiveness are evaluated with a real-world scenario, and the results show that our human resource allocation method using team faultlines can allocate human resources with high performance and optimize the business process.},
  archive      = {J_APIN},
  author       = {Zhao, Weidong and Pu, Shi and Jiang, Danni},
  doi          = {10.1007/s10489-020-01686-4},
  journal      = {Applied Intelligence},
  month        = {9},
  number       = {9},
  pages        = {2887-2900},
  shortjournal = {Appl. Intell.},
  title        = {A human resource allocation method for business processes using team faultlines},
  volume       = {50},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A similarity model based on reinforcement local maximum
connected same destination structure oriented to disordered fusion of
knowledge graphs. <em>APIN</em>, <em>50</em>(9), 2867–2886. (<a
href="https://doi.org/10.1007/s10489-020-01673-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The alignment and fusion of knowledge graphs have been at entity alignment and fusion which match and align knowledge graphs (KGs) by measuring similarity of the entities in KGs. Nevertheless, false fusion of completely different KGs can be easily caused if only considering entity similarity but ignoring entity relationship similarity. This paper focuses on entity relationship similarity model and KGs fusion method to achieve automatic construction of KGs. First, Same Destination Paths is developed based on Maximum Common Subgraph, which is used to build the Local Maximum Connected Same Destination Structure (LMCSDS) model to measure the entity relationship similarity of KGs. Then, a fusion method for similar fragmentation KGs (FKGs) is developed by analyzing the types of FKGs. Third, a Reinforcement Local Maximum Connected Same Destination Structure (RLMCSDS) similarity model is developed to ensure that the similarity between FKGs can still be measured correctly after fusion of FKGs. Meanwhile, the fusion results obtained by the developed RLMCSDS model and fusion method are theoretically proved to be independent with the fusion order of FKGs. Finally, experimental results on several datasets demonstrate the outstanding performance of the RLMCSDS model in comparison with some existing methods. Moreover, a complete KG can be built by the RLMCSDS model and the fusion method.},
  archive      = {J_APIN},
  author       = {Lin, Lin and Liu, Jie and Lv, Yancheng and Guo, Feng},
  doi          = {10.1007/s10489-020-01673-9},
  journal      = {Applied Intelligence},
  month        = {9},
  number       = {9},
  pages        = {2867-2886},
  shortjournal = {Appl. Intell.},
  title        = {A similarity model based on reinforcement local maximum connected same destination structure oriented to disordered fusion of knowledge graphs},
  volume       = {50},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Multi-instance iris remote authentication using private
multi-class perceptron on malicious cloud server. <em>APIN</em>,
<em>50</em>(9), 2848–2866. (<a
href="https://doi.org/10.1007/s10489-020-01681-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, biometric authentication system (BAS) has become the most promising and popular authentication system in identity management. Due to its capability to solve the limitations of unimodal systems, multi-biometric systems (MBS) have been extensively accepted in various fields. The main step in MBS is information fusion. On the other hand, directly storing the fused templates into a centralized server leads to privacy concerns. Recently, many BAS based on homomorphic encryption has been introduced to provide confidentiality for the fused templates. However, most of the existing solutions rely on an implication of the assumption that the server is “Honest-but-Curious”. As a result, the compromise of such server results into entire system vulnerability. To address this, we propose a novel P rivacy P reserving (PP) multi-instance iris remote authentication system to accord with attacks at the malicious server and over the transmission channel. Our scheme uses F ully H omomorphic E ncryption (FHE) to achieve the confidentiality of the fused iris templates and polynomial factorization algorithm to achieve the integrity of the matching result. We propose a PP iris authentication system using P rivate M ulti-C lass P erceptron (PMCP) by using the properties of FHE. Moreover, we propose C ontradistinguish S imilarity A nalysis (CSA), a feature level fusion technique that minimizes the between-class correlations and maximizes the pair-wise correlations. Our method has experimented on IITD and CASIA-V3-Interval iris databases to check the effectiveness and robustness. Experimental results show that our method provides improved accuracy, and eliminates the need to trust the cloud server when compared to the state-of-the-art approaches.},
  archive      = {J_APIN},
  author       = {Morampudi, Mahesh Kumar and Veldandi, Sowmya and Prasad, Munaga V. N. K. and Raju, U. S. N.},
  doi          = {10.1007/s10489-020-01681-9},
  journal      = {Applied Intelligence},
  month        = {9},
  number       = {9},
  pages        = {2848-2866},
  shortjournal = {Appl. Intell.},
  title        = {Multi-instance iris remote authentication using private multi-class perceptron on malicious cloud server},
  volume       = {50},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A novel classification method based on ICGOA-KELM for fault
diagnosis of rolling bearing. <em>APIN</em>, <em>50</em>(9), 2833–2847.
(<a href="https://doi.org/10.1007/s10489-020-01684-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A novel classification method based on ICGOA-KELM is presented in this paper. In ICGOA-KELM, an improved circle chaotic map with grasshopper optimization algorithm (ICGOA) is designed to optimize the parameters of Kernel extreme learning machine (KELM) to improve the stability and accuracy of fault classification for rolling bearing based on parameter modification of circle chaotic map. Grasshopper optimization algorithm (GOA) is a new heuristic optimization algorithm, which has strong global searching ability. However, it still may fall into local optimization in some cases. In this paper, the vibration signals of rolling bearing are preprocessed by using Variational Modal Decomposition (VMD). Then Multi-scale Permutation Entropy (MPE) is utilized to extracted features of intrinsic mode functions (IMFs) decomposed by VMD. In addition, KPCA is adopted to select the salient features with high contribution rates to remove redundant and irrelevant features. Finally, the salient features are fed into ICGOA-KELM to fulfill fault classification. Therefore, a new fault detection and classification method based on VMD, MPE, KPCA and ICGOA-KELM is proposed. This method is applied to the fault classification of rolling bearing and the identification of different damage fault degrees. Experiments verify that the proposed method is more effective than CGOA-KELM for fault diagnosis of rolling bearing.},
  archive      = {J_APIN},
  author       = {Chen, Peng and Zhao, Xiaoqiang and Zhu, Qixian},
  doi          = {10.1007/s10489-020-01684-6},
  journal      = {Applied Intelligence},
  month        = {9},
  number       = {9},
  pages        = {2833-2847},
  shortjournal = {Appl. Intell.},
  title        = {A novel classification method based on ICGOA-KELM for fault diagnosis of rolling bearing},
  volume       = {50},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A hybrid model of convolutional neural networks and deep
regression forests for crowd counting. <em>APIN</em>, <em>50</em>(9),
2818–2832. (<a
href="https://doi.org/10.1007/s10489-020-01688-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Real-time monitoring variation of crowd via video surveillance plays a significant role in the new generation of technology in a smart city. We propose a crowd counting algorithm based on deep regression forest, named CountForest. First of all, according to the correlation among frames, the crowd counting problem is transformed into a label-distribution-learning problem. Then we combine convolutional neural networks(CNN) and deep regression forest to make a hybrid model. CNN is introduced for the task of feature learning and deep decision forest is extended to address label distribution learning problem in crowd counting. Thereinto, the proposed network replaces its softmax layer with the aforementioned probabilistic decision forest in order to better establish a mapping relationship between image features and crowds’ number so as to implement an end-to-end hybrid model for crowd counting problem. Our method demonstrated in the final experiments not only attains the high accuracy in crowd counting but has comparable robustness and instantaneity in selected public datasets as well.},
  archive      = {J_APIN},
  author       = {Ji, Qingge and Zhu, Ting and Bao, Di},
  doi          = {10.1007/s10489-020-01688-2},
  journal      = {Applied Intelligence},
  month        = {9},
  number       = {9},
  pages        = {2818-2832},
  shortjournal = {Appl. Intell.},
  title        = {A hybrid model of convolutional neural networks and deep regression forests for crowd counting},
  volume       = {50},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Path planning of UAV for oilfield inspections in a
three-dimensional dynamic environment with moving obstacles based on an
improved pigeon-inspired optimization algorithm. <em>APIN</em>,
<em>50</em>(9), 2800–2817. (<a
href="https://doi.org/10.1007/s10489-020-01650-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, uninhabited aerial vehicles (UAV) have been used for oilfield inspections in many enterprises which can realize oilfield inspections by fewer workers. Path planning is one indispensable element in oilfield inspections through UAVs and it is also a complicated optimal problem. Now, although many researches have been focused on it, they are mainly discussed based on two-dimension planes. In practices, oilfields are complex three-dimensional spaces with many targeted points and moving obstacles between the starting and the ending point, which bring current methods some difficulties. In order to solve this problem, a three-dimensional environment model for oilfields is established for the first time, which includes: a static oil-well equipment, moving obstacles, and so on. Then, a cost function is defined to evaluate the best path, which includes: total length, average height, total time, and total electricity consumption. Finally, an improved pigeon-inspired optimization algorithm is proposed to solve problems about path planning in a three-dimensional dynamic environment of oilfields, which is named PIOFOA. In the PIOFOA, a pigeon-inspired optimization (PIO) algorithm is used to optimize the initial path and a fruit fly optimization algorithm (FOA) is used to continue local optimizations, so as to search the best path after movements of obstacles. Compared with some other methods, simulation results show that the proposed PIOFOA method is more effective.},
  archive      = {J_APIN},
  author       = {Ge, Fawei and Li, Kun and Han, Ying and Xu, Wensu and Wang, Yi’an},
  doi          = {10.1007/s10489-020-01650-2},
  journal      = {Applied Intelligence},
  month        = {9},
  number       = {9},
  pages        = {2800-2817},
  shortjournal = {Appl. Intell.},
  title        = {Path planning of UAV for oilfield inspections in a three-dimensional dynamic environment with moving obstacles based on an improved pigeon-inspired optimization algorithm},
  volume       = {50},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Comprehensive learning cuckoo search with chaos-lambda
method for solving economic dispatch problems. <em>APIN</em>,
<em>50</em>(9), 2779–2799. (<a
href="https://doi.org/10.1007/s10489-020-01654-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Economic dispatch (ED) is an important part in the economic operation of power systems. It is an NP-hard problem with multiple practical constraints. This paper proposes a novel approach that combines a swarm intelligence algorithm with a constraint-handling mechanism to solve the ED problem. First, we design a comprehensive learning cuckoo search algorithm with two strengthen strategies. A comprehensive learning strategy is designed to give the algorithm advanced learning ability in high-dimensional and multi-modal environment and thus enhance the search ability. A duplicate elimination strategy is utilized as an elite strategy to improve the evolving efficiency of the algorithm. Then, we propose a constraint-based population generation method named chaos-lambda method to reduce the searching complexity, and a solution repair method to repair unfeasible solutions that violate the constraints. The proposed approach is tested on 5 systems with different benchmarks and compared with the state-of-the-art algorithms. Our approach achieves the best performance on every test.},
  archive      = {J_APIN},
  author       = {Huang, Zhenyu and Zhao, Jian and Qi, Liang and Gao, Zhengzhong and Duan, Hua},
  doi          = {10.1007/s10489-020-01654-y},
  journal      = {Applied Intelligence},
  month        = {9},
  number       = {9},
  pages        = {2779-2799},
  shortjournal = {Appl. Intell.},
  title        = {Comprehensive learning cuckoo search with chaos-lambda method for solving economic dispatch problems},
  volume       = {50},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Semi-supervised learning method based on predefined
evenly-distributed class centroids. <em>APIN</em>, <em>50</em>(9),
2770–2778. (<a
href="https://doi.org/10.1007/s10489-020-01689-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Compared to supervised learning, semi-supervised learning reduces the dependence of deep learning on a large number of labeled samples. In this work, we use a small number of labeled samples and perform data augmentation on unlabeled samples to achieve image classification. Our method constrains all samples to the predefined evenly-distributed class centroids (PEDCC) by the corresponding loss function. Specifically, the PEDCC-Loss for labeled samples, and the maximum mean discrepancy loss for unlabeled samples are used to make the feature distribution closer to the distribution of PEDCC. Our method ensures that the inter-class distance is large and the intra-class distance is small enough to make the classification boundaries between different classes clearer. Meanwhile, for unlabeled samples, we also use KL divergence to constrain the consistency of the network predictions between unlabeled and augmented samples. Our semi-supervised learning method achieves the state-of-the-art results, with 4000 labeled samples on CIFAR10 and 1000 labeled samples on SVHN, and the accuracy is 95.10% and 97.58% respectively. Code is available in https://github.com/sweetTT/semi-supervised-method-based-on-PEDCC .},
  archive      = {J_APIN},
  author       = {Zhu, Qiu-yu and Li, Tian-tian},
  doi          = {10.1007/s10489-020-01689-1},
  journal      = {Applied Intelligence},
  month        = {9},
  number       = {9},
  pages        = {2770-2778},
  shortjournal = {Appl. Intell.},
  title        = {Semi-supervised learning method based on predefined evenly-distributed class centroids},
  volume       = {50},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Distributed learning for supervised multiview feature
selection. <em>APIN</em>, <em>50</em>(9), 2749–2769. (<a
href="https://doi.org/10.1007/s10489-020-01683-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multiview feature selection technique is specifically designed to reduce the dimensionality of multiview data and has received much attention. Most proposed multiview supervised feature selection methods suffer from the problem of efficiently handling the large-scale and high-dimensional data. To address this, this paper designs an efficient supervised multiview feature selection method for multiclass problems by combining the distributed optimization method in the Alternating Direction Method of Multipliers (ADMM). Specifically, the distributed strategy is reflected in two aspects. On the one hand, a sample-partition based distributed strategy is adopted, which calculates the loss term of each category individually. On the other hand, a view-partition based distributed strategy is used to explore the consistent and characteristic information of views. We adopt the individual regularization on each view and the common loss term which is obtained by fusing different views to jointly share the label matrix. Benefited from the distributed framework, the model can realize a distributed solution for the transformation matrix and reduce the complexity for multiview feature selection. Extensive experiments have demonstrated that the proposed method achieves a great improvement on training time, and the comparable or better performance compared to several state-of-the-art supervised feature selection algorithms.},
  archive      = {J_APIN},
  author       = {Men, Min and Zhong, Ping and Wang, Zhi and Lin, Qiang},
  doi          = {10.1007/s10489-020-01683-7},
  journal      = {Applied Intelligence},
  month        = {9},
  number       = {9},
  pages        = {2749-2769},
  shortjournal = {Appl. Intell.},
  title        = {Distributed learning for supervised multiview feature selection},
  volume       = {50},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A BOA-based adaptive strategy with multi-party perspective
for automated multilateral negotiations. <em>APIN</em>, <em>50</em>(9),
2718–2748. (<a
href="https://doi.org/10.1007/s10489-020-01646-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Determining an effective strategy for intelligent agents in multilateral negotiations is a more complicated problem than in bilateral negotiations. In order to achieve an optimal and beneficial agreement the agent needs to consider the behavior and desired utility of more than one opponent, determine a concession tactic based on a smaller agreement space, and use a computationally efficient mechanism for generating optimal offers. However, a mere extension of bilateral negotiation strategies cannot be effective in multilateral negotiations because the nature of most bilateral negotiation strategies is based on interaction with only one opponent and tracking a single behavior during the negotiation process. In this paper, we propose an adaptive approach based on a multi-party perspective to determine multilateral negotiation strategy. The proposed approach applies the BOA framework (Bidding, Opponent model, and Acceptance) and dynamically models the opponents’ preference profiles. In order to estimate the obtainable utility from opponents and help find a good offer, the agent uses an ensemble model made by individual frequency-based opponent models and a different level of attention to each party’s behavior. The proposed approach also implements a bidding strategy which applies the opponents’ desirable utility to adapt the agent’s concession tactic and produce appropriate offers. The results of experimental evaluations on various negotiation scenarios against the state of the art multilateral negotiation strategies show that our proposed strategy can provide superior performance in both individual utility and social welfare and lead to more optimal and fairer agreements.},
  archive      = {J_APIN},
  author       = {Amini, Mohammad and Fathian, Mohammad and Ghazanfari, Mehdi},
  doi          = {10.1007/s10489-020-01646-y},
  journal      = {Applied Intelligence},
  month        = {9},
  number       = {9},
  pages        = {2718-2748},
  shortjournal = {Appl. Intell.},
  title        = {A BOA-based adaptive strategy with multi-party perspective for automated multilateral negotiations},
  volume       = {50},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A multi-agent complex network algorithm for multi-objective
optimization. <em>APIN</em>, <em>50</em>(9), 2690–2717. (<a
href="https://doi.org/10.1007/s10489-020-01666-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To deal with the multi-objective optimization problems (MOPs), this study proposes a new Multi-Objective Multi-Agent Complex Network Optimization Algorithm called MOMCNA based on the idea of Cellular genetic algorithm (CGA) and the Multi-agent complex network. Compared with the traditional CGA for multi objective problem, the individuals in the population of MOMCNA have more features of intelligent agent, the new form of neighborhood for the population, private archive for individuals, the new strategy of “local-global” genetic operator and the chaotic mutation are proposed in the new algorithm to balance the convergence and diversity of the algorithm. Seventeen unconstrained multi-objective optimization problems and seven many-objective problems are introduced and tested to evaluate the new algorithm, in addition, the classical traffic assignment problem based on different system optimum principle is also established to evaluate the new algorithm. The comparison between MOMCNA and other classical algorithms shows that the proposed MOMCNA proves to be competitive in dealing with multi-objective and many-objective optimization problems and the structure of the complex network made up of population also has effect on algorithm’s performance.},
  archive      = {J_APIN},
  author       = {Li, Xueyan and Zhang, Hankun},
  doi          = {10.1007/s10489-020-01666-8},
  journal      = {Applied Intelligence},
  month        = {9},
  number       = {9},
  pages        = {2690-2717},
  shortjournal = {Appl. Intell.},
  title        = {A multi-agent complex network algorithm for multi-objective optimization},
  volume       = {50},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Semantic image segmentation with shared decomposition
convolution and boundary reinforcement structure. <em>APIN</em>,
<em>50</em>(9), 2676–2689. (<a
href="https://doi.org/10.1007/s10489-020-01671-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep convolutional neural networks (DCNNs) have shown excellent performances in the field of computer vision. In this paper, we propose a new semantic image segmentation model, and the two hallmarks of our architecture are the usage of shared decomposition convolution (SDC) operation and boundary reinforcement (BR) structure. SDC operation can extract dense features and increase correlation of features in the same group, which can relieve the grid artifact problem. BR structure combines the spatial information from different layers in DCNNs to enhance the spatial resolution and enrich target boundary position information simultaneously. The simulation results show that the proposed model can achieve 94.6% segmentation accuracy and 76.3% mIOU on PASCAL VOC 2012 database respectively, which verifies the effectiveness of the proposed model.},
  archive      = {J_APIN},
  author       = {Zhu, Hegui and Wang, Baoyu and Zhang, Xiangde and Liu, Jinhai},
  doi          = {10.1007/s10489-020-01671-x},
  journal      = {Applied Intelligence},
  month        = {9},
  number       = {9},
  pages        = {2676-2689},
  shortjournal = {Appl. Intell.},
  title        = {Semantic image segmentation with shared decomposition convolution and boundary reinforcement structure},
  volume       = {50},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Collaborative filtering recommendation algorithm based on
interval-valued fuzzy numbers. <em>APIN</em>, <em>50</em>(9), 2663–2675.
(<a href="https://doi.org/10.1007/s10489-020-01661-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most collaborative filtering recommendation algorithms use crisp ratings to represent the users’ preferences. However, users’ preferences are subjective and changeable, crisp ratings can’t measure the uncertainty of users’ preferences effectively. In order to solve this problem, this paper proposes the interval-valued triangular fuzzy rating model. This model replaces crisp ratings with interval-valued triangular fuzzy numbers on the basis of users’ rating statistics information, which can measure the users’ preferences in a more reasonable way. Based on this model, the collaborative filtering recommendation algorithm based on interval-valued fuzzy numbers is designed. The algorithm calculates the users’ similarity by the interval-valued triangular fuzzy numbers, and takes the ambiguity of ratings into consideration in the prediction stage. Our experiments prove that, compared with other fuzzy and traditional algorithms, our algorithm can increase the prediction precision and rank accuracy effectively with a little time cost, and has an obvious advantage when implemented in a sparse dataset which has more users than items. Thus our method has strong effectiveness and practicability.},
  archive      = {J_APIN},
  author       = {Wu, Yitao and ZHao, Yi and Wei, Shuai},
  doi          = {10.1007/s10489-020-01661-z},
  journal      = {Applied Intelligence},
  month        = {9},
  number       = {9},
  pages        = {2663-2675},
  shortjournal = {Appl. Intell.},
  title        = {Collaborative filtering recommendation algorithm based on interval-valued fuzzy numbers},
  volume       = {50},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A general-purpose distributed pattern mining system.
<em>APIN</em>, <em>50</em>(9), 2647–2662. (<a
href="https://doi.org/10.1007/s10489-020-01664-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper explores five pattern mining problems and proposes a new distributed framework called DT-DPM: Decomposition Transaction for Distributed Pattern Mining. DT-DPM addresses the limitations of the existing pattern mining problems by reducing the enumeration search space. Thus, it derives the relevant patterns by studying the different correlation among the transactions. It first decomposes the set of transactions into several clusters of different sizes, and then explores heterogeneous architectures, including MapReduce, single CPU, and multi CPU, based on the densities of each subset of transactions. To evaluate the DT-DPM framework, extensive experiments were carried out by solving five pattern mining problems (FIM: Frequent Itemset Mining, WIM: Weighted Itemset Mining, UIM: Uncertain Itemset Mining, HUIM: High Utility Itemset Mining, and SPM: Sequential Pattern Mining). Experimental results reveal that by using DT-DPM, the scalability of the pattern mining algorithms was improved on large databases. Results also reveal that DT-DPM outperforms the baseline parallel pattern mining algorithms on big databases.},
  archive      = {J_APIN},
  author       = {Belhadi, Asma and Djenouri, Youcef and Lin, Jerry Chun-Wei and Cano, Alberto},
  doi          = {10.1007/s10489-020-01664-w},
  journal      = {Applied Intelligence},
  month        = {9},
  number       = {9},
  pages        = {2647-2662},
  shortjournal = {Appl. Intell.},
  title        = {A general-purpose distributed pattern mining system},
  volume       = {50},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A hierarchical self-attentive neural extractive summarizer
via reinforcement learning (HSASRL). <em>APIN</em>, <em>50</em>(9),
2633–2646. (<a
href="https://doi.org/10.1007/s10489-020-01669-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, deep neural extractive-based summarization approaches have achieved vast popularity over conventional approaches. However, previously proposed neural extractive-based models have issues that limit their performance. One of these issues is related to the architecture of the used neural network that skips some details about the document hierarchical structure. Moreover, these models are optimized to maximize the probabilities of the training data ground truth labels rather than the evaluation metric that actually measures the quality of the summarization; this way of optimization might neglect important information related to sentence ranking. To address these issues, we combined reinforcement and supervised learning to train a hierarchical self-attentive reinforced neural network-based summarization model to rank sentences according to their significance by directly optimizing the ROUGE evaluation metric. The proposed model employs a hierarchical self-attention mechanism to generate document and sentence embeddings that reflect the hierarchical structure of the document and give better feature representation. While reinforcement learning enables direct optimization with respect to evaluation metrics, the attention mechanism adds an extra source of information to direct the summary extraction. The model was evaluated on the basis of three well-known datasets, namely, CNN, Daily Mail, and their combined version CNN/Daily Mail. Experimental results showed that the model achieved higher ROUGE scores than state-of-the-art models for extractive summarization on the three datasets.},
  archive      = {J_APIN},
  author       = {Mohsen, Farida and Wang, Jiayang and Al-Sabahi, Kamal},
  doi          = {10.1007/s10489-020-01669-5},
  journal      = {Applied Intelligence},
  month        = {9},
  number       = {9},
  pages        = {2633-2646},
  shortjournal = {Appl. Intell.},
  title        = {A hierarchical self-attentive neural extractive summarizer via reinforcement learning (HSASRL)},
  volume       = {50},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). K-PbC: An improved cluster center initialization for
categorical data clustering. <em>APIN</em>, <em>50</em>(8), 2610–2632.
(<a href="https://doi.org/10.1007/s10489-020-01677-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The performance of a partitional clustering algorithm is influenced by the initial random choice of cluster centers. Different runs of the clustering algorithm on the same data set often yield different results. This paper addresses that challenge by proposing an algorithm named k-PbC, which takes advantage of non-random initialization from the view of pattern mining to improve clustering quality. Specifically, k-PbC first performs a maximal frequent itemset mining approach to find a set of initial clusters. It then uses a kernel-based method to form cluster centers and an information-theoretic based dissimilarity measure to estimate the distance between cluster centers and data objects. An extensive experimental study was performed on various real categorical data sets to draw a comparison between k-PbC and state-of-the-art categorical clustering algorithms in terms of clustering quality. Comparative results have revealed that the proposed initialization method can enhance clustering results and k-PbC outperforms compared algorithms for both internal and external validation metrics.},
  archive      = {J_APIN},
  author       = {Dinh, Duy-Tai and Huynh, Van-Nam},
  doi          = {10.1007/s10489-020-01677-5},
  journal      = {Applied Intelligence},
  month        = {8},
  number       = {8},
  pages        = {2610-2632},
  shortjournal = {Appl. Intell.},
  title        = {K-PbC: An improved cluster center initialization for categorical data clustering},
  volume       = {50},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Rule-centred genetic programming (RCGP): An imperialist
competitive approach. <em>APIN</em>, <em>50</em>(8), 2589–2609. (<a
href="https://doi.org/10.1007/s10489-019-01601-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Automatic programming is one of the challenging fields of AI to generate solutions for high-level programming problems. There are variant methodologies attempting to introduce an efficient technique which address problems of this domain. In this paper, a novel Rule-Centred Genetic Programming (RCGP) is proposed. RCGP benefits from a series of evolutionary rules to help the algorithm choose intelligent alterations in the chromosome of individuals during the evolution yet preserves its stochastic evolutionary nature. Further, a modified search strategy based on Imperialist Competitive Algorithm (ICA) is employed in RCGP that shows to be significantly effective to deal with various problems which differ in degree of complexity. The proposed method features competitive convergence both in the case of speed and accuracy as well as a simpler mechanism than the several existing GP methods. RCGP is tested on nine benchmark problems which are synthesis and real world. The obtained results indicate that RCGP outperforms recent GP methods and is capable of hybridizing with other types of evolutionary algorithms. The method shows to be competent enough to enhance the quality of automatic programming solutions in both aspects of accuracy and efficiency compared to existing methods.},
  archive      = {J_APIN},
  author       = {Hosseini Amini, Seyed Mohammad Hossein and Abdollahi, Mohammad and Amir Haeri, Maryam},
  doi          = {10.1007/s10489-019-01601-6},
  journal      = {Applied Intelligence},
  month        = {8},
  number       = {8},
  pages        = {2589-2609},
  shortjournal = {Appl. Intell.},
  title        = {Rule-centred genetic programming (RCGP): An imperialist competitive approach},
  volume       = {50},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). SmartIX: A database indexing agent based on reinforcement
learning. <em>APIN</em>, <em>50</em>(8), 2575–2588. (<a
href="https://doi.org/10.1007/s10489-020-01674-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Configuring databases for efficient querying is a complex task, often carried out by a database administrator. Solving the problem of building indexes that truly optimize database access requires a substantial amount of database and domain knowledge, the lack of which often results in wasted space and memory for irrelevant indexes, possibly jeopardizing database performance for querying and certainly degrading performance for updating. In this paper, we develop the SmartIX architecture to solve the problem of automatically indexing a database by using reinforcement learning to optimize queries by indexing data throughout the lifetime of a database. We train and evaluate SmartIX performance using TPC-H, a standard, and scalable database benchmark. Our empirical evaluation shows that SmartIX converges to indexing configurations with superior performance compared to standard baselines we define and other reinforcement learning methods used in related work.},
  archive      = {J_APIN},
  author       = {Paludo Licks, Gabriel and Colleoni Couto, Julia and de Fátima Miehe, Priscilla and de Paris, Renata and Dubugras Ruiz, Duncan and Meneguzzi, Felipe},
  doi          = {10.1007/s10489-020-01674-8},
  journal      = {Applied Intelligence},
  month        = {8},
  number       = {8},
  pages        = {2575-2588},
  shortjournal = {Appl. Intell.},
  title        = {SmartIX: A database indexing agent based on reinforcement learning},
  volume       = {50},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Attribute susceptibility and entropy based data
anonymization to improve users community privacy and utility in
publishing data. <em>APIN</em>, <em>50</em>(8), 2555–2574. (<a
href="https://doi.org/10.1007/s10489-020-01656-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {User attributes affect community (i.e., a group of people with some common properties/attributes) privacy in users’ data publishing because some attributes may expose multiple users’ identities and their associated sensitive information during published data analysis. User attributes such as gender, age, and race, may allow an adversary to form users’ communities based on their values, and launch sensitive information inference attack subsequently. As a result, explicit disclosure of private information of a specific users’ community can occur from the privacy preserved published data. Each item of user attributes impacts users’ community privacy differently, and some types of attributes are highly susceptible. More susceptible types of attributes enable multiple users’ unique identifications and sensitive information inferences more easily, and their presence in published data increases users’ community privacy risks. Most of the existing privacy models ignore the impact of susceptible attributes on user’s community privacy and they mainly focus on preserving the individual privacy in the released data. This paper presents a novel data anonymization algorithm that significantly improves users’ community privacy without sacrificing the guarantees on anonymous data utility in publishing data. The proposed algorithm quantifies the susceptibility of each attribute present in user’s dataset to effectively preserve users’ community privacy. Data generalization is performed adaptively by considering both user attributes’ susceptibility and entropy simultaneously. The proposed algorithm controls over-generalization of the data to enhance anonymous data utility for the legitimate information consumers. Due to the widespread applications of social networks (SNs), we focused on the SN users’ community privacy preserved and utility enhanced anonymous data publishing. The simulation results obtained from extensive experiments, and comparisons with the existing algorithms show the effectiveness of the proposed algorithm and verify the aforementioned claims.},
  archive      = {J_APIN},
  author       = {Majeed, Abdul and Lee, Sungchang},
  doi          = {10.1007/s10489-020-01656-w},
  journal      = {Applied Intelligence},
  month        = {8},
  number       = {8},
  pages        = {2555-2574},
  shortjournal = {Appl. Intell.},
  title        = {Attribute susceptibility and entropy based data anonymization to improve users community privacy and utility in publishing data},
  volume       = {50},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). D-ANP: A multiple criteria decision making method for
supplier selection. <em>APIN</em>, <em>50</em>(8), 2537–2554. (<a
href="https://doi.org/10.1007/s10489-020-01639-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Supplier selection can be regarded as a classic multiple criteria decision making (MCDM) problem. To a great extent, experts’ evaluations play a decisive role in the decision-making process. There will inevitable exist a variety of indefinite factors, which result from imprecision, uncertainty, and fuzziness due to the subjective judgment of human beings. As an effective tool to express uncertain information, the theory of D numbers performs better in comparison to other existing methods. In addition to that, analytic network process (ANP) method is applied more broadly for its advantages of flexibility, rationality and creditability than analytic hierarchy process (AHP) method. In this study, the D-ANP methodology is proposed to apply in the field of supplier selection, which is the extension of the traditional ANP method using D numbers. The validity of the presented methodology is illustrated by an application for supplier selection.},
  archive      = {J_APIN},
  author       = {Fei, Liguo},
  doi          = {10.1007/s10489-020-01639-x},
  journal      = {Applied Intelligence},
  month        = {8},
  number       = {8},
  pages        = {2537-2554},
  shortjournal = {Appl. Intell.},
  title        = {D-ANP: A multiple criteria decision making method for supplier selection},
  volume       = {50},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Structured block diagonal representation for subspace
clustering. <em>APIN</em>, <em>50</em>(8), 2523–2536. (<a
href="https://doi.org/10.1007/s10489-020-01629-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The aim of the subspace clustering is to segment the high-dimensional data into the corresponding subspace. The structured sparse subspace clustering and the block diagonal representation clustering are quite advanced spectral-type subspace clustering algorithms when handling to the linear subspaces. In this paper, the respective advantages of these two algorithms are fully exploited, and the structured block diagonal representation (SBDR) subspace clustering is proposed. In many classical spectral-type subspace clustering algorithms, the affinity matrix which obeys the block diagonal property can not necessarily bring satisfying clustering results. However, the k-block diagonal regularizer of the SBDR algorithm directly pursues the block diagonal matrix, and this regularizer is obviously more effective. On the other hand, the general procedure of the spectral-type subspace clustering algorithm is to get the affinity matrix firstly and next perform the spectral clustering. The SBDR algorithm considers the intrinsic relationship of the two seemingly separate steps, the subspace structure matrix obtained by the spectral clustering is used iteratively to facilitate a better initialization for the representation matrix. The experimental results on the synthetic dataset and the real dataset have demonstrated the superior performance of the proposed algorithm over other prevalent subspace clustering algorithms.},
  archive      = {J_APIN},
  author       = {Liu, Maoshan and Wang, Yan and Sun, Jun and Ji, Zhicheng},
  doi          = {10.1007/s10489-020-01629-z},
  journal      = {Applied Intelligence},
  month        = {8},
  number       = {8},
  pages        = {2523-2536},
  shortjournal = {Appl. Intell.},
  title        = {Structured block diagonal representation for subspace clustering},
  volume       = {50},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A novel community detection method based on whale
optimization algorithm with evolutionary population. <em>APIN</em>,
<em>50</em>(8), 2503–2522. (<a
href="https://doi.org/10.1007/s10489-020-01659-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Community detection is the process of detecting communities in complex networks. Communities are important structures that can help us further study the properties of complex networks. In recent years, swarm intelligence algorithms have been applied to community detection and have achieved remarkable results. However, these existing algorithms have limited search ability and easily fall into the problem of local optima. In this paper, we propose a new community detection approach based on an improved whale optimization algorithm (WOA). The WOA is applied to a discrete symbol space in solving the community detection problem, therefore topology structure-based search strategies, adjustment and mergence policies, and evolutionary population method are designed to improve the efficiency and effectiveness of the method. Then, a whale optimization algorithm with evolutionary population for community detection (EP-WOCD) is proposed. Extensive experiments are conducted to compare the EP-WOCD with other state-of-the-art algorithms on both artificial and real-world social networks. Experimental results show that the EP-WOCD is effective and stable.},
  archive      = {J_APIN},
  author       = {Feng, Yunfei and Chen, Hongmei and Li, Tianrui and Luo, Chuan},
  doi          = {10.1007/s10489-020-01659-7},
  journal      = {Applied Intelligence},
  month        = {8},
  number       = {8},
  pages        = {2503-2522},
  shortjournal = {Appl. Intell.},
  title        = {A novel community detection method based on whale optimization algorithm with evolutionary population},
  volume       = {50},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Deep reinforcement learning for imbalanced classification.
<em>APIN</em>, <em>50</em>(8), 2488–2502. (<a
href="https://doi.org/10.1007/s10489-020-01637-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data in real-world application often exhibit skewed class distribution which poses an intense challenge for machine learning. Conventional classification algorithms are not effective in case of imbalanced data distribution, and may fail when the data distribution is highly imbalanced. To address this issue, we propose a general imbalanced classification model based on deep reinforcement learning, in which we formulate the classification problem as a sequential decision-making process and solve it by a deep Q-learning network. In our model, the agent performs a classification action on one sample in each time step, and the environment evaluates the classification action and returns a reward to the agent. The reward from the minority class sample is larger, so the agent is more sensitive to the minority class. The agent finally finds an optimal classification policy in imbalanced data under the guidance of the specific reward function and beneficial simulated environment. Experiments have shown that our proposed model outperforms other imbalanced classification algorithms, and identifies more minority samples with better classification performance.},
  archive      = {J_APIN},
  author       = {Lin, Enlu and Chen, Qiong and Qi, Xiaoming},
  doi          = {10.1007/s10489-020-01637-z},
  journal      = {Applied Intelligence},
  month        = {8},
  number       = {8},
  pages        = {2488-2502},
  shortjournal = {Appl. Intell.},
  title        = {Deep reinforcement learning for imbalanced classification},
  volume       = {50},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Oversampling technique based on fuzzy representativeness
difference for classifying imbalanced data. <em>APIN</em>,
<em>50</em>(8), 2465–2487. (<a
href="https://doi.org/10.1007/s10489-020-01644-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Class imbalance problem poses a difficulty to learning algorithms in pattern classification. Oversampling techniques is one of the most widely used techniques to solve these problems, but the majority of them use the sample size ratio as an imbalanced standard. This paper proposes a fuzzy representativeness difference-based oversampling technique, using affinity propagation and the chromosome theory of inheritance (FRDOAC). The fuzzy representativeness difference (FRD) is adopted as a new imbalance metric, which focuses on the importance of samples rather than the number. FRDOAC firstly finds the representative samples of each class according to affinity propagation. Secondly, fuzzy representativeness of every sample is calculated by the Mahalanobis distance. Finally, synthetic positive samples are generated by the chromosome theory of inheritance until the fuzzy representativeness difference of two classes is small. A thorough experimental study on 16 benchmark datasets was performed and the results show that our method is better than other advanced imbalanced classification algorithms in terms of various evaluation metrics.},
  archive      = {J_APIN},
  author       = {Ren, Ruonan and Yang, Youlong and Sun, Liqin},
  doi          = {10.1007/s10489-020-01644-0},
  journal      = {Applied Intelligence},
  month        = {8},
  number       = {8},
  pages        = {2465-2487},
  shortjournal = {Appl. Intell.},
  title        = {Oversampling technique based on fuzzy representativeness difference for classifying imbalanced data},
  volume       = {50},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Joint user mention behavior modeling for mentionee
recommendation. <em>APIN</em>, <em>50</em>(8), 2449–2464. (<a
href="https://doi.org/10.1007/s10489-020-01635-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As an emerging online interaction service in Twitter-like social media systems, mention serves to significantly improve both user interaction experience and information propagation. In recent years, the problem of mentionee recommendation, i.e., recommending mentionees (mentioned users) when mentioners (mentioning users) mention others, has received considerable attention. However, the extreme sparsity of mentioner-mentionee matrix creates a severe challenge. While an increasing line of work has exploited diverse effects such as the textual content and spatio-temporal context influences to cope with this challenge, there lacks a comprehensive study of the joint effect of all these influencing factors. In light of this, we propose a joint latent-class probabilistic model, named Joint Topic-Area Model (JTAM), to tackle the mentionee recommendation problem by simultaneously learning and modeling users’ semantic interests, the spatio-temporal mentioning patterns of mentioners, the geographical distribution of mentionees, and their joint effects on users’ mention behaviors in a unified way. Moreover, to facilitate online query performance, we design an efficient query answering approach that enables fast top-k mentionee recommendation. To evaluate the performance of our method, we conduct extensive experiments on a large real-world dataset. The results demonstrate the superiority of our method in recommending mentionees in terms of both effectiveness and efficiency compared with other state-of-the-art methods.},
  archive      = {J_APIN},
  author       = {Tang, Xiaoyue and Zhang, Cong and Meng, Weiyi and Wang, Kai},
  doi          = {10.1007/s10489-020-01635-1},
  journal      = {Applied Intelligence},
  month        = {8},
  number       = {8},
  pages        = {2449-2464},
  shortjournal = {Appl. Intell.},
  title        = {Joint user mention behavior modeling for mentionee recommendation},
  volume       = {50},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Minimum interpretation by autoencoder-based serial and
enhanced mutual information production. <em>APIN</em>, <em>50</em>(8),
2423–2448. (<a
href="https://doi.org/10.1007/s10489-019-01619-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The present paper aims to propose an information-theoretic method for interpreting the inference mechanism of neural networks. The new method aims to interpret the inference mechanism minimally by disentangling complex information into simpler and easily interpretable information. This disentanglement of complex information can be realized by maximizing mutual information between input patterns and the corresponding neurons. However, because the use of mutual information has faced difficulty in computation, we use the well-known autoencoder to increase mutual information by re-interpreting the sparsity constraint, which is considered a device to increase mutual information. The computational procedures to increase mutual information are decomposed into the serial operation of equal use of neurons and specific responses to input patterns. The specific responses are realized by enhancing the results by the equal use of neurons. The method was applied to three data sets: the glass, office equipment, and pulsar data sets. With all three data sets, we could observe that, when the number of neurons was forced to increase, mutual information could be increased. Then, collective weights, or average collectively treated weights, showed that the method could extract the simple and linear relations between inputs and targets, making it possible to interpret the inference mechanism minimally.},
  archive      = {J_APIN},
  author       = {Kamimura, Ryotaro},
  doi          = {10.1007/s10489-019-01619-w},
  journal      = {Applied Intelligence},
  month        = {8},
  number       = {8},
  pages        = {2423-2448},
  shortjournal = {Appl. Intell.},
  title        = {Minimum interpretation by autoencoder-based serial and enhanced mutual information production},
  volume       = {50},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Phase enhancement model based on supervised convolutional
neural network for coherent DOA estimation. <em>APIN</em>,
<em>50</em>(8), 2411–2422. (<a
href="https://doi.org/10.1007/s10489-020-01678-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {When the elevation of targets is smaller than beamwidth, the coherent multi-path signals will significantly degrade the direction of arrival (DOA) estimation accuracy of existing methods for a very-high-frequency (VHF) radar system. Through detailed theoretical analysis, we demonstrate that the phase distortion is the key factor of degrading the accuracy of DOA estimation. Hence, a novel phase enhancement model based on supervised convolutional neural network (CNN) for coherent DOA estimation is proposed to mitigate the phase distortion and improve estimation accuracy. The results of simulation experiments and real data have demonstrated the superiority of proposed method in DOA estimation accuracy and resolution compared to classic physics-driven methods. Moreover, the proposed scheme is suitable for the coherent DOA estimation compared with existing data-driven methods.},
  archive      = {J_APIN},
  author       = {Xiang, Houhong and Chen, Baixiao and Yang, Ting and Liu, Dong},
  doi          = {10.1007/s10489-020-01678-4},
  journal      = {Applied Intelligence},
  month        = {8},
  number       = {8},
  pages        = {2411-2422},
  shortjournal = {Appl. Intell.},
  title        = {Phase enhancement model based on supervised convolutional neural network for coherent DOA estimation},
  volume       = {50},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Reinforcement learning with convolutional reservoir
computing. <em>APIN</em>, <em>50</em>(8), 2400–2410. (<a
href="https://doi.org/10.1007/s10489-020-01679-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, reinforcement learning models have achieved great success, mastering complex tasks such as Go and other games with higher scores than human players. Many of these models store considerable data on the tasks and achieve high performance by extracting visual and time-series features using convolutional neural networks (CNNs) and recurrent neural networks respectively. However, these networks have very high computational costs because they need to be trained by repeatedly using the stored data. In this study, we propose a novel practical approach called reinforcement learning with a convolutional reservoir computing (RCRC) model. The RCRC model uses a fixed random-weight CNN and a reservoir computing model to extract visual and time-series features. Using these extracted features, it decides actions with an evolution strategy method. Thereby, the RCRC model has several desirable features: (1) there is no need to train the feature extractor, (2) there is no need to store training data, (3) it can take a wide range of actions, and (4) there is only a single task-dependent weight matrix to be trained. Furthermore, we show the RCRC model can solve multiple reinforcement learning tasks with a completely identical feature extractor.},
  archive      = {J_APIN},
  author       = {Chang, Hanten and Futagami, Katsuya},
  doi          = {10.1007/s10489-020-01679-3},
  journal      = {Applied Intelligence},
  month        = {8},
  number       = {8},
  pages        = {2400-2410},
  shortjournal = {Appl. Intell.},
  title        = {Reinforcement learning with convolutional reservoir computing},
  volume       = {50},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Targeted aspects oriented topic modeling for short texts.
<em>APIN</em>, <em>50</em>(8), 2384–2399. (<a
href="https://doi.org/10.1007/s10489-020-01672-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Topic modeling has demonstrated its value in short text topic discovery. For this task, a common way adopted by many topic models is to perform a full analysis to find all the possible topics. However, these topic models overlook the importance of deeper topics, leading to confusing topics discovered. In practice, people always tend to find more focused topics on some special aspects (or events), rather than a set of coarse topics. Therefore, in this paper, we propose a novel method, Targeted Aspects Oriented Topic Modeling (TATM), to discover more focused topics on specific aspects in short texts. Specifically, each short text is assigned to only one targeted aspect derived from an enhanced Dirichlet Multinomial Mixture process (E-DMM). This process helps group similar words as many as possible, which achieves topic homogeneity. In addition, TATM discovers the topics for each targeted aspect from as many angles as possible by performing target-level modeling, which achieves topic completeness. Thus, TATM can make a balance between the two conflicting properties without employing any additional information or pre-trained knowledge. The extensive experiments conducted on five real-world datasets demonstrate that our proposed model can effectively discover more focused and complete topics, and it outperforms the state-of-the-art baselines.},
  archive      = {J_APIN},
  author       = {He, Jin and Li, Lei and Wang, Yan and Wu, Xindong},
  doi          = {10.1007/s10489-020-01672-w},
  journal      = {Applied Intelligence},
  month        = {8},
  number       = {8},
  pages        = {2384-2399},
  shortjournal = {Appl. Intell.},
  title        = {Targeted aspects oriented topic modeling for short texts},
  volume       = {50},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Virtual machine placement based on multi-objective
reinforcement learning. <em>APIN</em>, <em>50</em>(8), 2370–2383. (<a
href="https://doi.org/10.1007/s10489-020-01633-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-objective virtual machine (VM) placement is a powerful tool, which can achieve different goals in data centers. It is an NP-hard problem, and various works have been proposed to solve it. However, almost all of them ignore the selection of weights. The selection of weights is difficult, but it is essential for multi-objective optimization. The inappropriate weights will cause the obtained solution set deviating from the Pareto optimal set. Fortunately, we find that this problem can be easily solved by using the Chebyshev scalarization function in multi-objective reinforcement learning (RL). In this paper, we propose a VM placement algorithm based on multi-objective RL (VMPMORL). VMPMORL is designed based on the Chebyshev scalarization function. We aim to find a Pareto approximate set to minimize energy consumption and resource wastage simultaneously. Compared with other multi-objective RL algorithms in the field of VM placement, VMPMORL not only uses the concept of the Pareto set but also solves the weight selection problem. Finally, VMPMORL is compared with some state-of-the-art algorithms in recent years. The results show that VMPMORL can achieve better performance than the approaches above.},
  archive      = {J_APIN},
  author       = {Qin, Yao and Wang, Hua and Yi, Shanwen and Li, Xiaole and Zhai, Linbo},
  doi          = {10.1007/s10489-020-01633-3},
  journal      = {Applied Intelligence},
  month        = {8},
  number       = {8},
  pages        = {2370-2383},
  shortjournal = {Appl. Intell.},
  title        = {Virtual machine placement based on multi-objective reinforcement learning},
  volume       = {50},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Multi-branch cross attention model for prediction of KRAS
mutation in rectal cancer with t2-weighted MRI. <em>APIN</em>,
<em>50</em>(8), 2352–2369. (<a
href="https://doi.org/10.1007/s10489-020-01658-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The accurate identification of KRAS mutation status on medical images is critical for doctors to specify treatment options for patients with rectal cancer. Deep learning methods have recently been successfully introduced to medical diagnosis and treatment problems, although substantial challenges remain in the computer-aided diagnosis (CAD) due to the lack of large training datasets. In this paper, we propose a multi-branch cross attention model (MBCAM) to separate KRAS mutation cases from wild type cases using limited T2-weighted MRI data. Our model is built on multiple different branches generated based on our existing MRI data, which can take full advantage of the information contained in small data sets. The cross attention block (CA block) is proposed to fuse formerly independent branches to ensure that the model can learn as many common features as possible for preventing the overfitting of the model due to the limited dataset. The inter-branch loss is proposed to constrain the learning range of the model, confirming that the model can learn more general features from multi-branch data. We tested our method on the collected dataset and compared it to four previous works and five popular deep learning models using transfer learning. Our result shows that the MBCAM achieved an accuracy of 88.92% for the prediction of KRAS mutations with an AUC of 95.75%. These results are a significant improvement over those existing methods (p &lt; 0.05).},
  archive      = {J_APIN},
  author       = {Wang, JiaWen and Cui, YanFen and Shi, GuoHua and Zhao, JuanJuan and Yang, XiaoTang and Qiang, Yan and Du, QianQian and Ma, Yue and Kazihise, Ntikurako Guy-Fernand},
  doi          = {10.1007/s10489-020-01658-8},
  journal      = {Applied Intelligence},
  month        = {8},
  number       = {8},
  pages        = {2352-2369},
  shortjournal = {Appl. Intell.},
  title        = {Multi-branch cross attention model for prediction of KRAS mutation in rectal cancer with t2-weighted MRI},
  volume       = {50},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A multi-label text classification method via dynamic
semantic representation model and deep neural network. <em>APIN</em>,
<em>50</em>(8), 2339–2351. (<a
href="https://doi.org/10.1007/s10489-020-01680-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The increment of new words and text categories requires more accurate and robust classification methods. In this paper, we propose a novel multi-label text classification method that combines dynamic semantic representation model and deep neural network (DSRM-DNN). DSRM-DNN first utilizes word embedding model and clustering algorithm to select semantic words. Then the selected words are designated as the elements of DSRM-DNN and quantified by the weighted combination of word attributes. Finally, we construct a text classifier by combining deep belief network and back-propagation neural network. During the classification process, the low-frequency words and new words are re-expressed by the existing semantic words under sparse constraint. We evaluate the performance of DSRM-DNN on RCV1-v2, Reuters-21578, EUR-Lex, and Bookmarks. Experimental results show that our method outperforms the state-of-the-art methods.},
  archive      = {J_APIN},
  author       = {Wang, Tianshi and Liu, Li and Liu, Naiwen and Zhang, Huaxiang and Zhang, Long and Feng, Shanshan},
  doi          = {10.1007/s10489-020-01680-w},
  journal      = {Applied Intelligence},
  month        = {8},
  number       = {8},
  pages        = {2339-2351},
  shortjournal = {Appl. Intell.},
  title        = {A multi-label text classification method via dynamic semantic representation model and deep neural network},
  volume       = {50},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Cost-sensitive hierarchical classification for imbalance
classes. <em>APIN</em>, <em>50</em>(8), 2328–2338. (<a
href="https://doi.org/10.1007/s10489-019-01624-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The hierarchical classification with an imbalance class problem is a challenge for in machine learning, and is caused by data with an uneven distribution. Learning from an imbalanced dataset can lead to performance degradation of the classifier. Cost-sensitive learning is a useful solution for handling the gap probability of majority and minority classes. This paper proposes a cost-sensitive hierarchical classification for imbalance classes (CSHCIC), constructing a cost-sensitive factor to balance the relationship between majority and minority classes. First, we divide a large hierarchical classification task into several small subclassification tasks by class hierarchy. Second, we establish a cost-sensitive factor by more precisely using the number of different samples of subclassifications. Then, we calculate the probability of every node using logistic regression. Lastly, we update the cost-sensitive factor using the flexibility factor and the number of samples. The experimental results show that the cost-sensitive hierarchical classification method achieves excellent performance on handling imbalance class datasets. The running time cost of the proposed method is smaller than most state-of-the-art methods.},
  archive      = {J_APIN},
  author       = {Zheng, Weijie and Zhao, Hong},
  doi          = {10.1007/s10489-019-01624-z},
  journal      = {Applied Intelligence},
  month        = {8},
  number       = {8},
  pages        = {2328-2338},
  shortjournal = {Appl. Intell.},
  title        = {Cost-sensitive hierarchical classification for imbalance classes},
  volume       = {50},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Twin support vector machine based on improved artificial
fish swarm algorithm with application to flame recognition.
<em>APIN</em>, <em>50</em>(8), 2312–2327. (<a
href="https://doi.org/10.1007/s10489-020-01676-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, a twin support vector machine (TWSVM) based on improved artificial fish swarm algorithm (IAFSA) for fire flame recognition is proposed in view of the large computation burden and slow classification speed of the traditional support vector machine (SVM). Twin support vector machine is a machine learning algorithm developing from standard support vector machine. However, twin support vector machine cannot deal with the parameter selection problem well. The difficulty of parameter selection may greatly restrict the application of TWSVM in flame recognition problem. So a novel artificial fish swarm algorithm (AFSA) is used to solve the parameter selection problem of TWSVM. In order to make up for the drawbacks of the basic AFSA, the chaotic transformation is first applied to initialize the position of artificial fish swarm since it may be non-uniformly initialized in the basic artificial fish swarm algorithm. Then, the Cauchy mutation is used to make the fish swarm jump out of the local optimal solution after continuously expanding the visual scope of the artificial fish during the foraging procedure. An adaptively step-size adjusting method is then developed to optimize the moving steps of the swarming and following behaviors in order to accelerate the convergence speed of the developed algorithm. Last, to further improve the efficiency and accuracy of the algorithm, an elimination and regeneration mechanism based on adaptive t-distribution mutation is utilized to update the artificial fish swarm at each iterative procedure. Experimental results show that the TWSVM algorithm based on improved artificial fish swarm algorithm is a more effective method to identify the flame and greatly improves the accuracy and real-time performance of the flame recognition compared with PSO-TWSVM, Grid-TWSVM, GA-TWSVM, FOA-TWSVM, GSO-TWSVM, AFSA-TWSVM and the traditional SVM.},
  archive      = {J_APIN},
  author       = {Gao, Yikai and Xie, Linbo and Zhang, Zhengdao and Fan, Qigao},
  doi          = {10.1007/s10489-020-01676-6},
  journal      = {Applied Intelligence},
  month        = {8},
  number       = {8},
  pages        = {2312-2327},
  shortjournal = {Appl. Intell.},
  title        = {Twin support vector machine based on improved artificial fish swarm algorithm with application to flame recognition},
  volume       = {50},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020b). Editor’s note: Forthcoming 30th anniversary of applied
intelligence. <em>APIN</em>, <em>50</em>(8), 2311. (<a
href="https://doi.org/10.1007/s10489-020-01781-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_APIN},
  doi          = {10.1007/s10489-020-01781-6},
  journal      = {Applied Intelligence},
  month        = {8},
  number       = {8},
  pages        = {2311},
  shortjournal = {Appl. Intell.},
  title        = {Editor’s note: Forthcoming 30th anniversary of applied intelligence},
  volume       = {50},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A relation based algorithm for solving direct current
circuit problems. <em>APIN</em>, <em>50</em>(7), 2293–2309. (<a
href="https://doi.org/10.1007/s10489-020-01667-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper addresses the challenging problem of developing the automatic algorithm for solving direct current circuit problem. Leveraging on the innovated methods it proposes a high-performance relation based algorithm, called RaDCC. The challenges of the problem lie in relation acquisition and relation inference presentation after adopting the newly-established relation principle of solving problems. A high-performance procedure is developed for the challenging task of relation acquisition by leveraging on three innovated methods. Three methods are an enhanced schematics understanding method that can understand complicated structures of schematics, an extended syntax-semantics model method and a unit-theorem inference method to acquire schematic relations, explicit text relations and implicit text relations, respectively. To address another challenging problem of readable solution generation an action-schema presentation method is proposed to convert relation inference actions into relation inference presentations. The experimental results show that the proposed algorithm is high-performance since it achieves an accuracy of over 83.2% for solving problems from textbooks and 70.6% for solving problems from examination papers on a dataset that contains 1012 direct current circuit problems collected from the authority sources, much higher than the performance of the baseline algorithm.},
  archive      = {J_APIN},
  author       = {He, Bin and Yu, Xinguo and Jian, Pengpeng and Zhang, Ting},
  doi          = {10.1007/s10489-020-01667-7},
  journal      = {Applied Intelligence},
  month        = {7},
  number       = {7},
  pages        = {2293-2309},
  shortjournal = {Appl. Intell.},
  title        = {A relation based algorithm for solving direct current circuit problems},
  volume       = {50},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Multiple birth support vector machine based on recurrent
neural networks. <em>APIN</em>, <em>50</em>(7), 2280–2292. (<a
href="https://doi.org/10.1007/s10489-020-01655-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multiple birth support vector machine (MBSVM) is a new classification algorithm, which includes the advantages of low complexity and high computing efficiency. However, the traditional MBSVM does not take into account the correlation sequence information among all dimensions of the samples when using the method to classify datasets, which limits the further improvement of the classification accuracy. Although some scholars have combined neural networks with support vector machine (SVM), these methods do not take into account the sequence correlation among different features. For the above problems, we present several variants of MBSVM algorithms to illustrate the validity and reliability of the theory: Multiple Birth Support Vector Machine based on Multilayer Perceptron (MLP-MBSVM), Multiple Birth Support Vector Machine based on Long-Short Term Memory Networks (LSTM-MBSVM), Multiple Birth Support Vector Machine based on Multilayer Perceptron and Long-Short Term Memory Networks(MLP-LSTM-MBSVM). After introducing multilayer perceptron and long-short term memory networks, these algorithms can take full account of the sequence correlation information between different features of samples. The experiments results show that the algorithms proposed in this paper are effective, and they can greatly improve the classification accuracy of multiple birth support vector machine.},
  archive      = {J_APIN},
  author       = {Ding, Shifei and Sun, Yuting and An, Yuexuan and Jia, Weikuan},
  doi          = {10.1007/s10489-020-01655-x},
  journal      = {Applied Intelligence},
  month        = {7},
  number       = {7},
  pages        = {2280-2292},
  shortjournal = {Appl. Intell.},
  title        = {Multiple birth support vector machine based on recurrent neural networks},
  volume       = {50},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A new multiple attribute decision making method for
selecting design schemes in sponge city construction with trapezoidal
interval type-2 fuzzy information. <em>APIN</em>, <em>50</em>(7),
2252–2279. (<a
href="https://doi.org/10.1007/s10489-019-01608-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Selecting the most suitable design scheme in sponge city construction can be seen as a multiple attribute decision-making (MADM) problem. To express the uncertain and fuzzy decision-making information, interval type-2 fuzzy sets (IT2FSs) are useful tools. The paper focuses on decision making with trapezoidal interval type-2 fuzzy numbers (TIT2FNs) and discusses the evaluation of municipal road design schemes in sponge city construction. To do these, several operations on TIT2FNs based on Hamacher t-norm and t-conorm are first defined, where both the operations on the membership degree and on eight non-negative real values are considered. Then, two (2-additive) generalized Shapley trapezoidal interval type-2 fuzzy Hamacher Choquet integral operators are presented, which globally reflect interactions among elements. Considering the case where the decision-making weighting information is incomplete known, Manhattan distance measure-based models for obtaining the optimal fuzzy measure and 2-additive measure are constructed, respectively. Furthermore, an approach for trapezoidal interval type-2 fuzzy MADM is developed. Finally, a practical example is provided to illustrate the utilization of the new method, and comparison analysis is provided.},
  archive      = {J_APIN},
  author       = {Meng, Fanyong and Li, Shutian},
  doi          = {10.1007/s10489-019-01608-z},
  journal      = {Applied Intelligence},
  month        = {7},
  number       = {7},
  pages        = {2252-2279},
  shortjournal = {Appl. Intell.},
  title        = {A new multiple attribute decision making method for selecting design schemes in sponge city construction with trapezoidal interval type-2 fuzzy information},
  volume       = {50},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Teach machine to learn: Hand-drawn multi-symbol sketch
recognition in one-shot. <em>APIN</em>, <em>50</em>(7), 2239–2251. (<a
href="https://doi.org/10.1007/s10489-019-01607-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The ability to sequentially learn from few examples and re-utilize previous knowledge is an important milestone on the path to artificial general intelligence. In this paper, we propose Teach Machine to Learn (TML), a few-shot learning model for hand-drawn multi-symbol sketch recognition. The model decomposes multi-symbol sketch into stroke primitives and then explains the observed sequences in a bayesian criterion. A Bidirectional Long Short Term Memory (BiLSTM) encoder is employed for stroke primitives encoding. Meanwhile, a probabilistic Hidden Markov Model (HMM) is constructed for complete sketch inference and recognition. The challenging task of hand-drawn multi-symbol sketch recognition is implemented on two public datasets. The comparative results indicate that the proposed method outperforms the currently booming image-based deep models in recognition accuracy. Furthermore, our method is capable to continuously learn new concepts even in one-shot. The codes are currently available in https://github.com/chongyupan/Teach-Machine-to-Learn.},
  archive      = {J_APIN},
  author       = {Pan, Chongyu and Huang, Jian and Gong, Jianxing and Chen, Cheng},
  doi          = {10.1007/s10489-019-01607-0},
  journal      = {Applied Intelligence},
  month        = {7},
  number       = {7},
  pages        = {2239-2251},
  shortjournal = {Appl. Intell.},
  title        = {Teach machine to learn: Hand-drawn multi-symbol sketch recognition in one-shot},
  volume       = {50},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Distributed learning automata-based scheme for
classification using novel pursuit scheme. <em>APIN</em>,
<em>50</em>(7), 2222–2238. (<a
href="https://doi.org/10.1007/s10489-019-01627-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Learning Automata (LA) is a popular decision making mechanism to “determine the optimal action out of a set of allowable actions” (Agache and Oommen, IEEE Trans Syst Man Cybern-Part B Cybern 2002(6): 738–749, 2002). The distinguishing characteristic of automata-based learning is that the search for the optimising parameter vector is conducted in the space of probability distributions defined over the parameter space, rather than in the parameter space itself (Thathachar and Sastry, IEEE Trans Syst Man Cybern-Part B Cybern 32(6): 711–722, 2002). Recently, Goodwin and Yazidi pioneered the use of Ant Colony Optimisation (ACO) for solving classification problems (Goodwin and Yazidi 2016). In this paper, we propose a novel classifier based on the theory of LA. The classification problem is formulated as a deterministic optimization problem involving a team of LA that operate collectively to optimize an objective function. Although many LA algorithms have been devised in the literature, those LA schemes are not able to solve deterministic optimization problems as they suppose that the environment is stochastic. In this paper, we develop a novel pursuit LA which can be seen as the counterpart of the family of pursuit LA developed for stochastic environments (Agache and Oommen, IEEE Trans Syst Man Cybern Part B Cybern 32(6): 738–749, 2002). While classical pursuit LA are able to pursue the action with the highest reward estimate, our pursuit LA rather pursues the collection of actions that yield the highest performance. The theoretical analysis of the pursuit scheme does not follow classical LA proofs and can pave the way towards more schemes where LA can be applied to solve deterministic optimization problems. When applied to classification, the essence of our scheme is to search for a separator in the feature space by imposing a LA based random walk in a grid system. To each node in the gird we attach an LA, whose actions are the choice of the edges forming the separator. The walk is self-enclosing, i.e., a new random walk is started whenever the walker returns to starting node forming a closed classification path yielding a multiedged polygon. In our approach, the different LA attached at the different nodes search for a polygon that best encircles and separates each class. Based on the obtained polygons, we perform classification by labelling items encircled by a polygon as part of a class using ray casting function. Seen from a methodological perspective, PolyPursuit-LA has appealing properties compared to SVM. In fact, unlike PolyPursuit-LA, the SVM performance is dependent on the right choice of kernel function (e.g. Linear Kernel, Gaussian Kernel)— which is considered a “black art”. PolyPursuit-LA can find arbitrarily complex separators in the feature space. Experimental results from both synthetic and real-life data show that our scheme is able to perfectly separate both simple and complex patterns outperforming existing classifiers, including polynomial and linear SVM, without the need of any “kernel trick”. We believe that the results are impressive given the simplicity of PolyPursuit-LA compared to other approaches such as SVM.},
  archive      = {J_APIN},
  author       = {Goodwin, Morten and Yazidi, Anis},
  doi          = {10.1007/s10489-019-01627-w},
  journal      = {Applied Intelligence},
  month        = {7},
  number       = {7},
  pages        = {2222-2238},
  shortjournal = {Appl. Intell.},
  title        = {Distributed learning automata-based scheme for classification using novel pursuit scheme},
  volume       = {50},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). FRWCAE: Joint faster-RCNN and wasserstein convolutional
auto-encoder for instance retrieval. <em>APIN</em>, <em>50</em>(7),
2208–2221. (<a
href="https://doi.org/10.1007/s10489-019-01625-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Based on the powerful feature extraction capability of deep convolutional neural networks, image-level retrieval methods have achieved superior performance compared to the hand-crafted features and indexing algorithms. However, people tend to focus on foreground objects of interest in images. Locating objects accurately and using object-level features for retrieval become the essential tasks of instance search. In this work, we propose a novel instance retrieval method FRWACE, which combines the Faster R-CNN framework for object-level feature extraction with a brand-new Wasserstein Convolutional Auto-encoder for dimensionality reduction. In addition, we propose a considerate category-first spatial re-rank strategy to improve instance-level retrieval accuracy. Extensive experiments on four large datasets Oxford 5K, Paris 6K, Oxford 105K and Paris 106K show that our approach has achieved significant performance compared to the state-of-the-arts.},
  archive      = {J_APIN},
  author       = {Zhang, Yi-yang and Feng, Yong and Liu, Da-jiang and Shang, Jia-xing and Qiang, Bao-hua},
  doi          = {10.1007/s10489-019-01625-y},
  journal      = {Applied Intelligence},
  month        = {7},
  number       = {7},
  pages        = {2208-2221},
  shortjournal = {Appl. Intell.},
  title        = {FRWCAE: Joint faster-RCNN and wasserstein convolutional auto-encoder for instance retrieval},
  volume       = {50},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). The classification and denoising of image noise based on
deep neural networks. <em>APIN</em>, <em>50</em>(7), 2194–2207. (<a
href="https://doi.org/10.1007/s10489-019-01623-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Currently, image denoising is a challenge in many applications of computer vision. The existing denoising methods depend on the information of noise types or levels, which are generally classified by experts. These methods have not applied computational methods to pre-classify the image noise types. Furthermore, some methods assume that the noise type of the image is a certain one like Gaussian noise, which limits the ability of the denoising in real applications. Different from the existing methods, this paper introduces a new method that can classify and denoise not only a certain type noise but also mixed types of noises for real demand. Our method utilizes two types of deep learning networks. One is used to classify the noise type of the images and the other one performs denoising based on the classification result of the first one. Our framework can automatically denoise single or mixed types of noises with these efforts. Our experimental results show that our classification network achieves higher accuracy, and our denoising network can ensure higher PSNR and SSIM values than the existing methods.},
  archive      = {J_APIN},
  author       = {Liu, Fan and Song, Qingzeng and Jin, Guanghao},
  doi          = {10.1007/s10489-019-01623-0},
  journal      = {Applied Intelligence},
  month        = {7},
  number       = {7},
  pages        = {2194-2207},
  shortjournal = {Appl. Intell.},
  title        = {The classification and denoising of image noise based on deep neural networks},
  volume       = {50},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Residual learning based densely connected deep dilated
network for joint deblocking and super resolution. <em>APIN</em>,
<em>50</em>(7), 2177–2193. (<a
href="https://doi.org/10.1007/s10489-020-01670-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In many practical situations, images are not only down sampled but also compressed for efficient transmission and storage. JPEG and MPEG-2 compressions often introduce blocking artifacts because they process the data as 8 × 8 blocks. Many of the existing super resolution (SR) methods assume low resolution images as a down sampled version of high resolution (HR) image, and neglect the degradation due to compression. This exacerbates artifacts in the SR image and reduces the user experience. To address the joint deblocking and SR (DbSR), a novel deep network with dense skip connections and dilated convolutions is proposed in this paper, and we name it as DenseDbSR. Recently, many researchers have proposed deeper networks and achieved improvement in the SR performance. However, training deeper networks is very challenging because of the problem of vanishing gradients. Simply increasing the depth of the network leads to cumbersome computational costs. To enlarge the field-of-view (FOV) without increasing the computational cost, the dilated convolution is used. The dilated convolution exponentially expands the FOV and helps to exploit the contextual information efficiently. Moreover, the dense skip connections create short paths for gradients to be back-propagated efficiently and alleviates the problem of vanishing gradients. Furthermore, the network is relieved from the training burden by learning residuals of the SR image instead of learning raw images. From the conducted extensive experimentation, the proposed DenseDbSR network produced better performance in terms of PSNR and SSIM than the compared state-of-the-art methods.},
  archive      = {J_APIN},
  author       = {Amaranageswarao, Gadipudi and Deivalakshmi, S. and Ko, Seok-Bum},
  doi          = {10.1007/s10489-020-01670-y},
  journal      = {Applied Intelligence},
  month        = {7},
  number       = {7},
  pages        = {2177-2193},
  shortjournal = {Appl. Intell.},
  title        = {Residual learning based densely connected deep dilated network for joint deblocking and super resolution},
  volume       = {50},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). P-GWO and MOFA: Two new algorithms for the MSRCPSP with the
deterioration effect and financial constraints (case study of a gas
treating company). <em>APIN</em>, <em>50</em>(7), 2151–2176. (<a
href="https://doi.org/10.1007/s10489-020-01663-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a bi-objective mathematical formulation for the multi-skill resource-constrained project scheduling problem (MSRCPSP) with the deterioration effect and financial constraints. The objectives are to optimize the makespan and cost of project, simultaneously. Due to the high NP-hardness of the proposed model, a Pareto-based Grey Wolf Optimizer (P-GWO) algorithm has been developed to solve the problem. A new procedure based on the Weighted Sum Method (WSM) has been designed for the P-GWO to rank the solutions of population in order to find the alpha, beta, delta, and omega wolves. The P-GWO also uses a new procedure based on the Data Envelopment Analysis (DEA) to keep the most efficient newly found solutions and update the archive of non-dominated solutions. Besides, a Multi-Objective Fibonacci-based Algorithm (MOFA) based on the characteristics of the Fibonacci sequence has been proposed to solve the problem. The MOFA utilizes a novel neighborhood operator to generate as many feasible solutions as required in each iteration. For the MOFA, new procedures for finding the best solution of each iteration, elitism and updating archive of non-dominated solutions have been developed as well. To evaluate the proposed algorithms, a series of numerical experiments have been conducted and the outputs of our proposed methods were compared with the Non-dominated Sorting Genetic Algorithm II (NSGA-II), Multi-Objective Imperialist Competitive Algorithm (MOICA), and Multi-Objective Fruit-Fly Optimization Algorithm (MOFFOA) in terms of several performance measures. Moreover, a real-life overhaul project in a gas treating company has been studied to demonstrate the practicality of the proposed model. The results of all numerical experiments demonstrate that the P-GWO outperforms other algorithms in terms of most of the metrics. The outputs imply that the MOFA can generate high quality solutions within a reasonable computation time.},
  archive      = {J_APIN},
  author       = {Hosseinian, Amir Hossein and Baradaran, Vahid},
  doi          = {10.1007/s10489-020-01663-x},
  journal      = {Applied Intelligence},
  month        = {7},
  number       = {7},
  pages        = {2151-2176},
  shortjournal = {Appl. Intell.},
  title        = {P-GWO and MOFA: Two new algorithms for the MSRCPSP with the deterioration effect and financial constraints (case study of a gas treating company)},
  volume       = {50},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A novel design method for dual-passband IIR digital filters.
<em>APIN</em>, <em>50</em>(7), 2132–2150. (<a
href="https://doi.org/10.1007/s10489-020-01631-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rapid development of wireless communication technology, digital filters are now key components in many modern digital systems. Dual-passband digital filter is an important module of digital filter and has attracted wide attention. This paper proposes a novel evolutionary method to design diversified structure digital filters. Our proposed method using an adaptive multiple-elites- guide composite differential evolution algorithm, coupled with a shift mechanism (AMECoDEs) doesn’t need to use known circuit structures. Structures and parameters are evolved by crossover, mutation, and selection. Thus, our proposed method can directly design the diversified dual-passband digital filter structure and can effectively balance exploration and exploitation to prevent individuals from premature convergence. In our experiment, the connection probability, the subsystem number of the filter structure, as well as the scale factor and the crossover rate of AMECoDEs are explored to determine the optimal configuration. Compared with exiting state-of-the-art evolutionary algorithms for the design of the symmetrical and asymmetrical dual-bandpass filters, our proposed method has the smallest average passband ripple and stopband attenuation with the fastest convergence.},
  archive      = {J_APIN},
  author       = {Chen, Lijia and Wang, Jingfei and Liu, Mingguo and Chen, Chung-Hao},
  doi          = {10.1007/s10489-020-01631-5},
  journal      = {Applied Intelligence},
  month        = {7},
  number       = {7},
  pages        = {2132-2150},
  shortjournal = {Appl. Intell.},
  title        = {A novel design method for dual-passband IIR digital filters},
  volume       = {50},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Similarity measure with indeterminate parameters regarding
cubic hesitant neutrosophic numbers and its risk grade assessment
approach for prostate cancer patients. <em>APIN</em>, <em>50</em>(7),
2120–2131. (<a
href="https://doi.org/10.1007/s10489-020-01653-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The prostate cancer (PC) of older men has become an important problem with the increment of aging degree in current society. Then, it is necessary to give the timely detection and reasonable risk assessment of a PC patient so as to provide a suitable treatment option for the PC patient. However, there may exist the hybrid information of an interval-valued fuzzy number (IVFN) and a hesitant indeterminate number/hesitant neutrosophic number (HNN) together in risk data of PC patients, while existing PC risk grade assessment approaches with the cubic hesitant fuzzy set (CHFS) composed of an IVFN and a hesitant fuzzy set cannot carry out such a risk grade assessment problem with the hybrid information of both IVFN and HNN. To solve the issue, for the first time this study proposes a cubic hesitant neutrosophic number (CHNN) to express the hybrid information of both IVFN and HNN and then introduces a similarity measure with an indeterminate parameter regarding CHNNs for the risk grade assessment of PC patients. In this paper, therefore, a concept of a CHNN set is firstly presented to suitably express the hybrid information of both IVFN and HNN as the generalization of CHFS. Then, the CHNN set is transformed into the parameterized CHFS (P-CHFS) for de-neutrosophication by means of an indeterminate parameter. Next, generalized distance and similarity measures between P-CHFSs are proposed based on the least common multiple cardinality/number (LCMC) extension method. Thus, a PC risk assessment approach is developed by using the similarity measure of P-CHFSs with the physician’s optimistic, moderate and pessimistic attitudes under CHNN environment. Finally, sixteen clinical cases are provided as risk assessment examples of PC patients to indicate the effectiveness and applicability of the proposed PC risk assessment approach in CHNN setting. However, the proposed assessment approach can effectively and flexibly deal with assessment problems of PC risk grades regarding the physicians’ optimistic, moderate and pessimistic attitudes in CHNN setting, which shows its advantage.},
  archive      = {J_APIN},
  author       = {Fu, Jing and Ye, Jun},
  doi          = {10.1007/s10489-020-01653-z},
  journal      = {Applied Intelligence},
  month        = {7},
  number       = {7},
  pages        = {2120-2131},
  shortjournal = {Appl. Intell.},
  title        = {Similarity measure with indeterminate parameters regarding cubic hesitant neutrosophic numbers and its risk grade assessment approach for prostate cancer patients},
  volume       = {50},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Intelligent detection of edge inconsistency for mechanical
workpiece by machine vision with deep learning and variable geometry
model. <em>APIN</em>, <em>50</em>(7), 2105–2119. (<a
href="https://doi.org/10.1007/s10489-020-01641-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Inconsistent edges of mechanical workpieces in the same batch are one of the main reasons that lead to different machining performance. A full recognition of the inconsistent features has significant impact on enhancement of their intelligent machining ability. An intelligent hybrid strategy is proposed for edge inconsistent feature detection by machine vision, in which deep learning is combined with variable geometric model together to conduct the function. A deep convolutional neural network based feature classification model is established with K-Means clustering tactic. Supported on the classification model, a variable geometric model for specific edge inconsistency is given with an inconsistency evaluation function to investigate the match degree between the geometric model and the actual detected edge, and then particle swarm optimization algorithm is applied to find the solution of this geometric model. Detection experiments are carried out on a domestic servo-driven vision measuring platform to verify the performance of the proposed approach. The results show that the combined scheme can classify the different type of geometric contour of edge features with 100% correctness, and better evaluation performance by dice similarity index and Hausdorff distance in comparisons with other recent candidate methodologies. It is also indicated that the presented method provides a good recognition of the geometrical shape with less than 0.06mm maximum error for workpiece with 142 × 119mm size in the visual field.},
  archive      = {J_APIN},
  author       = {Lin, Xiankun and Wang, Xin and Li, Li},
  doi          = {10.1007/s10489-020-01641-3},
  journal      = {Applied Intelligence},
  month        = {7},
  number       = {7},
  pages        = {2105-2119},
  shortjournal = {Appl. Intell.},
  title        = {Intelligent detection of edge inconsistency for mechanical workpiece by machine vision with deep learning and variable geometry model},
  volume       = {50},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A self controlled RDP approach for feature extraction in
online handwriting recognition using deep learning. <em>APIN</em>,
<em>50</em>(7), 2093–2104. (<a
href="https://doi.org/10.1007/s10489-020-01632-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The identification of accurate features is the initial task for benchmarked handwriting recognition. For handwriting recognition, the objective of feature computation is to find those characteristics of a handwritten stroke that depict the class of a stroke and make it separable from the rest of the stroke classes. The present study proposes a feature extraction technique for online handwritten strokes based on a self controlled Ramer-Douglas-Peucker (RDP) algorithm. This novel approach prepares a smaller length feature vector for different shaped online handwritten strokes without preprocessing and without any control parameter to RDP. Thus, it also overcomes the shortcomings of the traditional chain code based feature extraction approach that requires preprocessing of data, and the original RDP algorithm that requires a control parameter as an input to RDP. We further propose a deep learning network of 1-dimensional convolutional neural networks (Conv1Ds) for recognition, which trains in few minutes due to the smaller dimension of the convolution combined with smaller length feature vectors. The proposed approach can be applied to different scripts and different writing styles. The key aim of the present study is to provide a script independent feature extraction technique that is well suited for smaller devices. It improves the recognition over the best reported accuracy in the literature which was achieved using hidden Markov models with directional features, from 87.67% to 95.61% on a Gurmukhi dataset. For Unipen online handwriting datasets the results are at par with the literature.},
  archive      = {J_APIN},
  author       = {Singh, Sukhdeep and Chauhan, Vinod Kumar and Smith, Elisa H. Barney},
  doi          = {10.1007/s10489-020-01632-4},
  journal      = {Applied Intelligence},
  month        = {7},
  number       = {7},
  pages        = {2093-2104},
  shortjournal = {Appl. Intell.},
  title        = {A self controlled RDP approach for feature extraction in online handwriting recognition using deep learning},
  volume       = {50},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Superpixel via coarse-to-fine boundary shift. <em>APIN</em>,
<em>50</em>(7), 2079–2092. (<a
href="https://doi.org/10.1007/s10489-019-01595-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {K-means is used by numerous superpixel algorithms, such as SLIC, MSLIC and LSC, because of its simplicity and efficiency. Yet those k-means based algorithm failed to perform well on connectivity and accuracy. In this paper, we propose a coarse-to-fine boundary shift strategy (CFBS) as a replacement of k-means. The CFBS solves the superpixel segmentation problem by shifting boundries rather than clustering pixels. In other words, it can be defined as a special k-means algorithm optimized for superpixel segmentation. By replacing k-means with CFBS, SLIC and LSC are upgraded to NeoSLIC and NeoLSC. Experiments show that NeoSLIC and NeoLSC outperform SLIC and LSC in accuracy and efficiency respectively, and NeoSLIC and NeoLSC alleviate dis-connectivity. In addition, experiments also show that CFBS achieves great improvements on semantic segmentation, class segmentation and segmented flow.},
  archive      = {J_APIN},
  author       = {Wu, Xiang and Chen, Yufei and Liu, Xianhui and Shen, Jianan and Zhuo, Keqiang and Zhao, Weidong},
  doi          = {10.1007/s10489-019-01595-1},
  journal      = {Applied Intelligence},
  month        = {7},
  number       = {7},
  pages        = {2079-2092},
  shortjournal = {Appl. Intell.},
  title        = {Superpixel via coarse-to-fine boundary shift},
  volume       = {50},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Almost periodic solutions of quaternion-valued neutral type
high-order hopfield neural networks with state-dependent delays and
leakage delays. <em>APIN</em>, <em>50</em>(7), 2067–2078. (<a
href="https://doi.org/10.1007/s10489-020-01634-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we consider the existence and global exponential stability of almost periodic solutions for a class of quaternion-valued neutral type neural networks with state-dependent delays and leakage delays by a direct approach. That is, without decomposing the considered quaternion-valued systems into real-valued systems, by using the contraction mapping fixed point theorem, we obtain the existence of almost periodic solutions of the considered networks with state-dependent delays and leakage delays, and by the counter-evidence method, we obtain the global exponential stability of almost periodic solutions of the considered networks with discrete delays and leakage delays. When the time delays in the considered systems are proportional time delays, our existence results are still true. At the same time, when the considered systems degenerate into real-valued or complex-valued systems, our results are still new.},
  archive      = {J_APIN},
  author       = {Li, Yongkun and Xiang, Jianglian and Li, Bing},
  doi          = {10.1007/s10489-020-01634-2},
  journal      = {Applied Intelligence},
  month        = {7},
  number       = {7},
  pages        = {2067-2078},
  shortjournal = {Appl. Intell.},
  title        = {Almost periodic solutions of quaternion-valued neutral type high-order hopfield neural networks with state-dependent delays and leakage delays},
  volume       = {50},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Joint discriminative subspace and distribution adaptation
for unsupervised domain adaptation. <em>APIN</em>, <em>50</em>(7),
2050–2066. (<a
href="https://doi.org/10.1007/s10489-019-01610-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In traditional machine learning algorithms, the classification models are learned on the training data (source domain) to reuse for labelling the test data (target domain) where the training and test samples are from the same distributions. However in nowadays applications, the existence of distribution shift across the source and target doamins degrades the model performance, significantly. Domain adaptation methods have been proposed to compensate domain shift problem by aligning the distributions across the source and target domains under various adaptation strategies. This paper addresses the robust image classification problem for unsupervised domain adaptation. Specifically, following three methods are proposed: Discriminative Subspace Learning (DSL), Joint Geometrical and Statistical Distribution Adaptation (GSDA), and Joint Subspace and Distribution Adaptation (DSL-GSDA). DSL is a subspace centric method that aligns the specific and shared features across domains. Indeed, DSL finds two projections to map the source and target data into independent subspaces by aligning the discriminant and global structures of domains. GSDA trends to find an adaptive classifier through statistical and geometrical distribution alignment and minimizes the prediction error. DSL-GSDA, as a combination of DSL and GSDA, consists of two subspace and distribution adaptation levels. DSL-GSDA uses DSL to build two aligned subspaces of source and target domains. The distributions of source and target data in new subspaces is adapted via GSDA. The proposed methods are evaluated on benchmark visual datasets for object, digit and face recongnition tasks. Visual datasets consist of image domains that have been captured under various real-world conditions where the domain shift is unavoidable. The experiment results show that DSL, GSDA and DSL-GSDA outperform other state-of-the-art domain adaptation methods by 6.19%, 1.48% and 1.99% improvement, respectively. Our source code is available at https://github.com/jtahmores/DSLGSDA (https://github.com/jtahmores/DSLGSDA).},
  archive      = {J_APIN},
  author       = {Gholenji, Elahe and Tahmoresnezhad, Jafar},
  doi          = {10.1007/s10489-019-01610-5},
  journal      = {Applied Intelligence},
  month        = {7},
  number       = {7},
  pages        = {2050-2066},
  shortjournal = {Appl. Intell.},
  title        = {Joint discriminative subspace and distribution adaptation for unsupervised domain adaptation},
  volume       = {50},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Artificial intelligence snapchat: Visual conversation agent.
<em>APIN</em>, <em>50</em>(7), 2040–2049. (<a
href="https://doi.org/10.1007/s10489-019-01621-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Visual conversation is a dialog in which parties exchange visual information. The key novelty presented in this paper is an artificial intelligence-driven visual conversation automation method. We will present a state of the art Artificial Intelligence Snapchat Visual Conversation Agent (AISVCA). AISVCA uses our proposed artificial intelligence-driven visual conversation automation method to create received image caption and generate an appropriate reasonable visual response. These functionalities are achieved by using a combination of Convolutional Neural Network (CNN), Long Short-Term Memory Neural Network (LSTM) and, Latent Semantic Indexing method (LSI). CNN and LSTM are used to create image captions and, LSI is used to assess the semantic similarity between captions generated from personalized image dataset, and captions that are extracted from the received image content. We will show that AISVCA, using the proposed method can generate a visual response that is basically indistinguishable from a human visual response. To evaluate the proposed approach, we measured the accuracy of the proposed system and, conducted a user study to test communication quality. In the user study, we analyzed source credibility and interpersonal attraction of the AISVCA. The user study results showed that there are no significant differences in communication quality between a visual conversation with AISVCA and visual conversation with the human agent.},
  archive      = {J_APIN},
  author       = {Arsovski, Sasa and Cheok, Adrian David and Govindarajoo, Kirthana and Salehuddin, Nurizzaty and Vedadi, Somaiyeh},
  doi          = {10.1007/s10489-019-01621-2},
  journal      = {Applied Intelligence},
  month        = {7},
  number       = {7},
  pages        = {2040-2049},
  shortjournal = {Appl. Intell.},
  title        = {Artificial intelligence snapchat: Visual conversation agent},
  volume       = {50},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A sanitization approach for big data with improved data
utility. <em>APIN</em>, <em>50</em>(7), 2025–2039. (<a
href="https://doi.org/10.1007/s10489-020-01640-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The process of collaborative data mining may sometimes expose the sensitive patterns present inside the data which may be undesirable to the data owner. Sensitive Pattern Hiding (SPH) is a subfield of data mining that addresses this problem. However, most of the existing approaches used for hiding sensitive patterns cause high side-effect on non-sensitive patterns which in-turn reduces the utility of the sanitized dataset. Furthermore, most of them are sequential in nature and are not able to cope with massive amounts of data and often results in high execution time. To resolve these identified challenges of utility and non-feasibility, two parallelized approaches have been proposed named PGVIR and PHCR based on spark parallel computing framework which modifies the data such that no sensitive patterns can be extracted while maintaining the utility of the sanitized dataset. Experiments performed using benchmark dataset shows that PGVIR scales better and PHCR causes fewer side-effects to the data compared to the existing techniques.},
  archive      = {J_APIN},
  author       = {Sharma, Udit and Toshniwal, Durga and Sharma, Shivani},
  doi          = {10.1007/s10489-020-01640-4},
  journal      = {Applied Intelligence},
  month        = {7},
  number       = {7},
  pages        = {2025-2039},
  shortjournal = {Appl. Intell.},
  title        = {A sanitization approach for big data with improved data utility},
  volume       = {50},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Optimized PSO algorithm based on the simplicial algorithm of
fixed point theory. <em>APIN</em>, <em>50</em>(7), 2009–2024. (<a
href="https://doi.org/10.1007/s10489-020-01630-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Particle swarm optimization algorithm (PSO) has been optimized from various aspects since it was proposed. Optimization of PSO can be realized by optimizing its iterative process or the initial parameters and heuristic methods have been combined with the initial PSO algorithm to improve its performance. In this paper, we introduce the Simplicial Algorithm (SA) of fixed point theory into the optimization of PSO and proposed a FP-PSO (Fixed-point PSO) improved algorithm. In FP-PSO algorithm, the optimization of target function is converted into the problem of solving a fixed point equation set, and the solution set obtained by Simplicial Algorithm (SA) of fixed point theory is used as the initial population of PSO algorithm, then the remaining parameters can be obtained accordingly with classical PSO algorithm. Since the fixed point method has sound mathematical properties, the initial population obtained with FP-PSO include nearly all the approximate local extremes which maintain the diversity of population and can optimize the flight direction of particles, and shows their advantages on setting other initial parameters. We make an experimental study with five commonly used testing functions from UCI (University of California Irvine) which include two single-peak functions and three multi-peak functions. The results indicate that the convergence accuracy, stability, and robustness of FP-PSO algorithm are significantly superior to existing improve strategies which also optimize PSO algorithm by optimizing initial population, especially when dealing with complex situations. In addition, we nest the FP-PSO algorithm with four classical improved PSO algorithms that improve PSO by optimizing iterative processes, and carry out contrast experiments on three multi-peak functions under different conditions (rotating or non-rotating). The experimental results show that the performance of the improved algorithm using nested strategy are also significantly enhanced compared with these original algorithms.},
  archive      = {J_APIN},
  author       = {Ren, Minglun and Huang, Xiaodi and Zhu, Xiaoxi and Shao, Liangjia},
  doi          = {10.1007/s10489-020-01630-6},
  journal      = {Applied Intelligence},
  month        = {7},
  number       = {7},
  pages        = {2009-2024},
  shortjournal = {Appl. Intell.},
  title        = {Optimized PSO algorithm based on the simplicial algorithm of fixed point theory},
  volume       = {50},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A weighted SVM ensemble predictor based on AdaBoost for
blast furnace ironmaking process. <em>APIN</em>, <em>50</em>(7),
1997–2008. (<a
href="https://doi.org/10.1007/s10489-020-01662-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As one of the most complex industrial reactors, there remain some urgent issues for blast furnace (BF), such as BF automation, prediction of the inner thermal state, etc. In this work, the prediction of BF inner thermal state, which is represented by the silicon content in BF hot metal, is taken as an imbalanced binary classification problem and a Weighted Support Vector Machine (W-SVM) ensemble predictor based on AdaBoost is presented for the prediction task. Compared with the traditional W-SVM algorithm, the proposed predictor dynamically adjusts the weight distribution of training samples according to the performance of weak classifier, in this way to mine information lurked in the samples. The prediction can act as a guide to aid the operators for judging the thermal state of BF in time. Experiments results on five benchmark datasets and two real-world BFs datasets demonstrate the efficiency of the proposed W-SVM ensemble predictor.},
  archive      = {J_APIN},
  author       = {Luo, Shihua and Dai, Zian and Chen, Tianxin and Chen, Hongyi and Jian, Ling},
  doi          = {10.1007/s10489-020-01662-y},
  journal      = {Applied Intelligence},
  month        = {7},
  number       = {7},
  pages        = {1997-2008},
  shortjournal = {Appl. Intell.},
  title        = {A weighted SVM ensemble predictor based on AdaBoost for blast furnace ironmaking process},
  volume       = {50},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). An intuitionistic linguistic MCDM model based on
probabilistic exceedance method and evidence theory. <em>APIN</em>,
<em>50</em>(6), 1979–1995. (<a
href="https://doi.org/10.1007/s10489-020-01638-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The optimization in multi-criteria decision making under uncertain conditions has attracted more and more scholars in recent years. However, it is still an open issue that how to better evaluate the satisfaction with more complex objects. Since the great performance of intuitionistic fuzzy set on handling the uncertain information, in this paper, a new fuzzy linguistic model for non-scalar criteria satisfaction expressed via intuitionistic fuzzy sets is proposed, which makes experts evaluate more objectively. Moreover, a corresponding aggregation approach based on the Choquet probabilistic exceedance method is also proposed. After a series of calculation processes, the final aggregated results embodied by intuitionistic fuzzy sets (IFSs) can be obtained. Then by converting them into the belief intervals, the best alternative can be selected more objectively. In addition, two real-life applications are shown to demonstrate the practicality of proposed method.},
  archive      = {J_APIN},
  author       = {Liu, Zeyi and Xiao, Fuyuan},
  doi          = {10.1007/s10489-020-01638-y},
  journal      = {Applied Intelligence},
  month        = {6},
  number       = {6},
  pages        = {1979-1995},
  shortjournal = {Appl. Intell.},
  title        = {An intuitionistic linguistic MCDM model based on probabilistic exceedance method and evidence theory},
  volume       = {50},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Explicit feedback meet with implicit feedback in GPMF: A
generalized probabilistic matrix factorization model for recommendation.
<em>APIN</em>, <em>50</em>(6), 1955–1978. (<a
href="https://doi.org/10.1007/s10489-020-01643-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recommender Systems focus on implicit and explicit feedback or parameters of users for better rating prediction. Most of the existing recommender systems use only one type of feedback ignoring the other one. Based on the availability of resources, we may consider more number of feedback of both the types to predict user’s rating for a particular item more accurately. However to the best of our knowledge, there is no generalized model that is fitted for multiple parameters or feedback. In this paper, we have proposed a Generalized Probabilistic Matrix Factorization (GPMF) model which uses multiple parameters of both the types for recommendation. To build GPMF, first we develop three models focusing on users’ crucial side information. First model PMFE (P robabilistic M atrix F actorization with E xplicit_Feedback) is proposed based on an explicit feedback of users and second one is PMFI (P robabilistic M atrix F actorization with I mplicit_Feedback), where an implicit feedback is considered. The last one is PMFEI (P robabilistic M atrix F actorization with E xplicit and I mplicit_Feedback), where both explicit and implicit feedback are considered. Extensive experiments on real world datasets show that PMFEI performs better compare to baselines. PMFEI model also performs better compare to baselines for cold-start users and cold-start items also. In our experimental section, it is shown that GPMF performs better when we consider both explicit and implicit feedback. The effectiveness of each parameter is not same for recommendation. Using GPMF we can estimate the effectiveness of a parameter. Based on this effectiveness, we can add or remove more parameters for better rating prediction.},
  archive      = {J_APIN},
  author       = {Mandal, Supriyo and Maiti, Abyayananda},
  doi          = {10.1007/s10489-020-01643-1},
  journal      = {Applied Intelligence},
  month        = {6},
  number       = {6},
  pages        = {1955-1978},
  shortjournal = {Appl. Intell.},
  title        = {Explicit feedback meet with implicit feedback in GPMF: A generalized probabilistic matrix factorization model for recommendation},
  volume       = {50},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Clustering association rules to build beliefs and discover
unexpected patterns. <em>APIN</em>, <em>50</em>(6), 1943–1954. (<a
href="https://doi.org/10.1007/s10489-020-01651-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Interesting pattern discovery is an important topic in data mining research. Many different definitions have been proposed to describe whether a pattern is interesting. Among these many definitions, unexpectedness has shown to be a highly promising measure. Mining unexpected patterns allows one to identify a failing in prior knowledge and may suggest an aspect of the data that deserves further investigation. Unexpected patterns are typically mined using belief-driven methods, but these require an established belief system. Prior studies have manually built their own partial belief systems to apply their method, but these remain laborious to create. In this study, we propose a novel approach that is able to automatically detect beliefs from data, which can in turn be used to reveal unexpected patterns. Central to this approach is a clustering-based method in which clusters represent beliefs and outliers are potential unexpected patterns. We also propose a pattern representation that captures the semantic relation between patterns rather than the lexical difference. An experimental evaluation on different datasets and a comparison to some other methods demonstrate the effectiveness of the proposed method, as well as the relevance of the discovered patterns.},
  archive      = {J_APIN},
  author       = {Bui-Thi, Danh and Meysman, Pieter and Laukens, Kris},
  doi          = {10.1007/s10489-020-01651-1},
  journal      = {Applied Intelligence},
  month        = {6},
  number       = {6},
  pages        = {1943-1954},
  shortjournal = {Appl. Intell.},
  title        = {Clustering association rules to build beliefs and discover unexpected patterns},
  volume       = {50},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Modified dynamic fuzzy c-means clustering algorithm –
application in dynamic customer segmentation. <em>APIN</em>,
<em>50</em>(6), 1922–1942. (<a
href="https://doi.org/10.1007/s10489-019-01626-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The dynamic customer segmentation (DCS) is a useful tool for managers in implementing marketing strategies by observing dynamic changes that are happening in the customer segments over time. The Crespo’s dynamic fuzzy c-means (CDFCM) is one of the clustering algorithms introduced in the literature for DCS. We have suggested modifications to the CDFCM algorithm owing to certain shortcomings found in it, resulting in the modified dynamic fuzzy c-means (MDFCM) algorithm. To show the performance of the MDFCM algorithm, extensive experiments were carried out in comparison with the CDFCM algorithm using a retail supermarket dataset with eleven new data updates. To validate the results of the MDFCM algorithm, the fuzzy clustering evaluation measures such as Xie-Beni (XB) index, within sum of squared error (WSSE), root mean squared error (RMSE), Kwon index, and Tang index are utilized. The experimental results show that MDFCM is the most effective clustering algorithm for DCS, and the results are tested statistically to show its significance. The MDFCM algorithm is further compared with another successful algorithm available in the literature called Fathabadi’s dynamic fuzzy c-means (FDFCM). To show the usefulness of the MDFCM algorithm, a DCS framework is proposed and it has been demonstrated through a case study.},
  archive      = {J_APIN},
  author       = {Munusamy, Sivaguru and Murugesan, Punniyamoorthy},
  doi          = {10.1007/s10489-019-01626-x},
  journal      = {Applied Intelligence},
  month        = {6},
  number       = {6},
  pages        = {1922-1942},
  shortjournal = {Appl. Intell.},
  title        = {Modified dynamic fuzzy c-means clustering algorithm – application in dynamic customer segmentation},
  volume       = {50},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). End-to-end multitask siamese network with residual
hierarchical attention for real-time object tracking. <em>APIN</em>,
<em>50</em>(6), 1908–1921. (<a
href="https://doi.org/10.1007/s10489-019-01605-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Object tracking with deep networks has recently achieved substantial improvement in terms of tracking performance. In this paper, we propose a multitask Siamese neural network that uses a residual hierarchical attention mechanism to achieve high-performance object tracking. This network is trained offline in an end-to-end manner, and it is capable of real-time tracking. To produce more efficient and generative attention-aware features, we propose residual hierarchical attention learning using residual skip connections in the attention module to receive hierarchical attention. Moreover, we formulate a multitask correlation filter layer to exploit the missing link between context awareness and regression target adaptation, and we insert this differentiable layer into a neural network to improve the discriminatory capability of the network. The results of experimental analyses conducted on the OTB, VOT and TColor-128 datasets, which contain various tracking scenarios, demonstrate the efficiency of our proposed real-time object-tracking network.},
  archive      = {J_APIN},
  author       = {Huang, Wenhui and Gu, Jason and Ma, Xin and Li, Yibin},
  doi          = {10.1007/s10489-019-01605-2},
  journal      = {Applied Intelligence},
  month        = {6},
  number       = {6},
  pages        = {1908-1921},
  shortjournal = {Appl. Intell.},
  title        = {End-to-end multitask siamese network with residual hierarchical attention for real-time object tracking},
  volume       = {50},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Efficient meta-heuristic approaches in solving minimal
exposure path problem for heterogeneous wireless multimedia sensor
networks in internet of things. <em>APIN</em>, <em>50</em>(6),
1889–1907. (<a
href="https://doi.org/10.1007/s10489-019-01628-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {One of the well-known methods for evaluating Heterogeneous wireless multimedia sensor networks (HWMSNs) in Internet of Things have drawn attention of the research community because this type of networks possesses great advantages of both coverage and performance. One of the most fundamental issues in HWMSNs is the barrier coverage problem which evaluates the surveillance capability of the network systems, especially those designed for security purposes. Among multiple approaches to solve this issue, finding the minimal exposure path (MEP), which corresponds to the worst-case coverage of the network is the most popular and efficient way. However, the MEP problem in HWMSNs (hereinafter heterogeneous multimedia MEP or HM-MEP) is specifically complex and challenging with the unique features of the HWMSNs. Thus, the problem is then converted into numerical functional extreme with high dimension, non-differential and non-linearity. Adapting to these features, two efficient meta-heuristic algorithms, Hybrid Evolutionary Algorithm (HEA) and Gravitation Particle Swarm Optimization (GPSO) are proposed for solving the problem. The HEA is a hybrid evolutionary algorithm in combination with local search while the GPSO is a novel particle swarm optimization based on the gravity force theory. Experimental results on extensive instances indicate that the proposed algorithms are suitable for the HM-MEP problem and perform well in term of both solution accuracy and computation time compared to existing approaches.},
  archive      = {J_APIN},
  author       = {Binh, Nguyen Thi My and Binh, Huynh Thi Thanh and Van Linh, Nguyen and Yu, Shui},
  doi          = {10.1007/s10489-019-01628-9},
  journal      = {Applied Intelligence},
  month        = {6},
  number       = {6},
  pages        = {1889-1907},
  shortjournal = {Appl. Intell.},
  title        = {Efficient meta-heuristic approaches in solving minimal exposure path problem for heterogeneous wireless multimedia sensor networks in internet of things},
  volume       = {50},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). “Solving discounted 0-1 knapsack problems by a discrete
hybrid teaching-learning-based optimization algorithm.” <em>APIN</em>,
<em>50</em>(6), 1872–1888. (<a
href="https://doi.org/10.1007/s10489-020-01652-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The discounted {0–1} knapsack problem (D{0–1}KP) is a kind of knapsack problem with group structure and discount relationships among items. It is more challenging than the classical 0–1 knapsack problem. A more effective hybrid algorithm, the discrete hybrid teaching-learning-based optimization algorithm (HTLBO), is proposed to solve D{0–1}KP in this paper. HTLBO is based on the framework of the teaching-learning-based optimization (TLBO) algorithm. A two-tuple consisting of a quaternary vector and a real vector is used to represent an individual in HTLBO and that allows TLBO to effectively solve discrete optimization problems. We enhanced the optimization ability of HTLBO from three aspects. The learning strategy in the Learner phase is modified to extend the exploration capability of HTLBO. Inspired by the human learning process, self-learning factors are incorporated into the Teacher and Learner phases, which balances the exploitation and exploration of the algorithm. Two types of crossover operators are designed to enhance the global search capability of HTLBO. Finally, we conducted extensive experiments on eight sets of 80 instances using our proposed approach. The experiment results show that the new algorithm has higher accuracy and better stability than do previous methods. Overall, HTLBO is an excellent approach for solving the D{0–1}KP.},
  archive      = {J_APIN},
  author       = {Wu, Congcong and Zhao, Jianli and Feng, Yanhong and Lee, Malrey},
  doi          = {10.1007/s10489-020-01652-0},
  journal      = {Applied Intelligence},
  month        = {6},
  number       = {6},
  pages        = {1872-1888},
  shortjournal = {Appl. Intell.},
  title        = {“Solving discounted {0-1} knapsack problems by a discrete hybrid teaching-learning-based optimization algorithm”},
  volume       = {50},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Multi-layer manifold learning with feature selection.
<em>APIN</em>, <em>50</em>(6), 1859–1871. (<a
href="https://doi.org/10.1007/s10489-019-01563-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many fundamental problems in machine learning require some form of dimensionality reduction. To this end, two different strategies were used: Manifold Learning and Feature Selection. Manifold learning (or data embedding) attempts to compute a subspace from original data by feature recombination/transformation. Feature selection aims to select the most relevant features in the original space. In this paper, we propose a novel cooperative Manifold learning-Feature selection that goes beyond the simple concatenation of these two modules. Our basic idea is to transform a given shallow embedding to a deep variant by computing a cascade of embeddings in which each embedding undergoes feature selection and elimination. We use filter approaches in order to efficiently select irrelevant features at any stage of the process. For a case study, our proposed framework was used with two typical linear embedding algorithms: Local Discriminant Embedding (LDE) (a supervised technique) and Locality Preserving Projections (LPP) (unsupervised technique) on four challenging face databases and it has been conveniently compared with other cooperative schemes. Moreover, a comparison with several state-of-the-art manifold learning methods is provided. As it is exhibited by our experimental study, the proposed framework can achieve superior learning performance with respect to classic cooperative schemes and to many competing manifold learning methods.},
  archive      = {J_APIN},
  author       = {Dornaika, F.},
  doi          = {10.1007/s10489-019-01563-9},
  journal      = {Applied Intelligence},
  month        = {6},
  number       = {6},
  pages        = {1859-1871},
  shortjournal = {Appl. Intell.},
  title        = {Multi-layer manifold learning with feature selection},
  volume       = {50},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Dynamic hesitant fuzzy bayesian network and its application
in the optimal investment port decision making problem of “twenty-first
century maritime silk road.” <em>APIN</em>, <em>50</em>(6), 1846–1858.
(<a href="https://doi.org/10.1007/s10489-020-01647-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most traditional decision making methods cannot deal with intensive uncertain data that varies with time effectively, and they only derive the static risk analysis by different aggregation operators. There is little research of the dynamic decision making problems with massive uncertain information. To manage with different kinds of uncertain knowledge and hesitancy in the dynamic decision-making process, this paper combines the advantages of hesitant fuzzy sets (HFSs) in depicting information and Bayesian Network (BN) in uncertain reasoning. Considering the uncertain information of risk factors varies with time, the concept of Dynamic Hesitant Fuzzy Bayesian Network (DHFBN) is proposed to deal with dynamic decision making problems under the hesitant fuzzy environment. Then, an improved Particle Swarm Optimization (PSO) algorithm and the Expectation-Maximization (EM) algorithm are adopted for the structure learning and parameters learning of DHFBN respectively. Based on the learned optimal DHFBN, a dynamic reasoning and prediction method is developed. Furthermore, a case about the optimal port investment decision making problem of “21st Century Maritime Silk Road” is presented to illustrate the application of the proposed method. Finally, we also conduct a comparative experiment to testify the validity and advantages of the method in detail.},
  archive      = {J_APIN},
  author       = {Song, Chenyang and Xu, Zeshui and Zhang, Yixin and Wang, Xinxin},
  doi          = {10.1007/s10489-020-01647-x},
  journal      = {Applied Intelligence},
  month        = {6},
  number       = {6},
  pages        = {1846-1858},
  shortjournal = {Appl. Intell.},
  title        = {Dynamic hesitant fuzzy bayesian network and its application in the optimal investment port decision making problem of “twenty-first century maritime silk road”},
  volume       = {50},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). NetNPG: Nonoverlapping pattern matching with general gap
constraints. <em>APIN</em>, <em>50</em>(6), 1832–1845. (<a
href="https://doi.org/10.1007/s10489-019-01616-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Pattern matching (PM) with gap constraints (or flexible wildcards) is one of the essential tasks in repetitive sequential pattern mining (or sequence pattern mining), since it can compute the support of a pattern in a sequence. Nonoverlapping PM (or PM under nonoverlapping condition) which is a kind of PM with gap constraints methods allows the same position character in the sequence to be reused at different locations in the pattern, but is not allowed to be reused in the same position of the pattern. The researches on nonoverlapping are under non-negative gaps which are more restrictive on the order of each character occurring in the sequence. As we know that it is easy to obtain valuable patterns under the nonoverlapping condition in sequence pattern mining. This paper addresses a nonoverlapping PM problem with general gaps which means that the gap can be a negative value. We proposes an effective algorithm which employs Nettree structure to convert the problem into a general gap Nettree at first. In order to find the nonoverlapping occurrences, the algorithm employs a backtracking strategy to find the leftmost full path in each iteration. This paper also analyzes the time and space complexities of the proposed algorithm. Experimental results verify the proposed algorithm has better performance and demonstrate that the general gap is more flexible than the non-negative gap.},
  archive      = {J_APIN},
  author       = {Shi, Qiaoshuo and Shan, Jinsong and Yan, Wenjie and Wu, Youxi and Wu, Xindong},
  doi          = {10.1007/s10489-019-01616-z},
  journal      = {Applied Intelligence},
  month        = {6},
  number       = {6},
  pages        = {1832-1845},
  shortjournal = {Appl. Intell.},
  title        = {NetNPG: Nonoverlapping pattern matching with general gap constraints},
  volume       = {50},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Hidden data states-based complex terminology extraction from
textual web data model. <em>APIN</em>, <em>50</em>(6), 1813–1831. (<a
href="https://doi.org/10.1007/s10489-019-01568-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In order to respect the standards of the “semantic web” which allows the data to be shared and reused between several applications, it became necessary to model web text documents with a vision based on the concepts and exploit available linguistic resources. It’s evident that the extraction of semantic tokens ensures semantic modelling of web documents. Unfortunately, terminology extraction techniques from unstructured Web text remain unable to provide powerful results. Indeed, systems developed based on the classical techniques extract massively high amounts of candidate terms and leave the task of separation between relevant and irrelevant candidates for post-processing. In this paper, we introduce HMM-Extract a novel model for terminology retrieval based on Markov model. Our model integrates two modules that work in cascade: a module based on Hidden Markov Model (HMM) for complex term extraction and a module based on Markov Chain for filtering terms provided by the HMM. Thus, we try to focus on three main contributions: firstly, we provide a linguistic and statistical specification of relevant terms. Secondly, we show the possibility of using a HMM to extract relevant terms from unstructured textual documents. Finally, we prove the importance of integrating statistical knowledge in a Markov Chain and we show, experimentally, its contribution to the field of terminology extraction.},
  archive      = {J_APIN},
  author       = {Fkih, Fethi and Omri, Mohamed Nazih},
  doi          = {10.1007/s10489-019-01568-4},
  journal      = {Applied Intelligence},
  month        = {6},
  number       = {6},
  pages        = {1813-1831},
  shortjournal = {Appl. Intell.},
  title        = {Hidden data states-based complex terminology extraction from textual web data model},
  volume       = {50},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Reduced-order observer-based robust leader-following control
of heterogeneous discrete-time multi-agent systems with system
uncertainties. <em>APIN</em>, <em>50</em>(6), 1794–1812. (<a
href="https://doi.org/10.1007/s10489-019-01553-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, the leader-following control of heterogeneous discrete-time multi-agent systems (HD_MASs) in the presence of system uncertainties under directed topology is addressed. It aims to achieve reference tracking, disturbance rejection and robust control while the references and disturbances are generated by an autonomous exosystem. In practice, these agents are often different types of devices, thus they have different internal dynamics. Moreover, it is difficult to measure all states of each aircraft due to high cost or technical limitation. In this case, a novel leader-following output consensus problem is formulated and solved in this paper. Firstly, an appropriate linear transformation is proposed to divide the state information of each agent into measurable and unmeasurable parts. Then the reduced-order observer is designed only for unmeasurable parts. Based on the designed observer, the distributed feedback controller is proposed such that the outputs of all followers reach the same trajectory with the leader. In light of the internal model principle and discrete-time algebraic Riccati equation, the robust leader-following consensus of HD_MASs is achieved. Furthermore, this paper extends the results to continuous-time multi-agent systems. Finally, several simulation experiments are presented to verify the effectiveness of the theoretical results.},
  archive      = {J_APIN},
  author       = {Cai, Yuliang and Zhang, Huaguang and Liang, Yuling and Gao, Zhiyun},
  doi          = {10.1007/s10489-019-01553-x},
  journal      = {Applied Intelligence},
  month        = {6},
  number       = {6},
  pages        = {1794-1812},
  shortjournal = {Appl. Intell.},
  title        = {Reduced-order observer-based robust leader-following control of heterogeneous discrete-time multi-agent systems with system uncertainties},
  volume       = {50},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A hybrid quantum feature selection algorithm using a quantum
inspired graph theoretic approach. <em>APIN</em>, <em>50</em>(6),
1775–1793. (<a
href="https://doi.org/10.1007/s10489-019-01604-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Quantum machine learning bridges the gap between abstract developments in quantum computing and the applied research on machine learning. It generally exposes the synthesis of important machine learning algorithms in a quantum framework. Dimensionality reduction of a dataset with a suitable feature selection strategy is one of the most important tasks in knowledge discovery and data mining. The efficient feature selection strategy helps to improve the overall accuracy of a large dataset in terms of machine learning operations. In this paper, a quantum feature selection algorithm using a graph-theoretic approach has been proposed. The proposed algorithm has used the concept of correlation coefficient based graph-theoretic classical approach initially and then applied the quantum Oracle with CNOT operation to verify whether the dataset is suitable for dimensionality reduction or not. If it is suitable, then our algorithm can efficiently estimate their high correlation values by using quantum parallel amplitude estimation and amplitude amplification techniques. This paper also shows that our proposed algorithm substantially outperforms than some popular classical feature selection algorithms for supervised classification in terms of query complexity of $O(\frac {k\sqrt {N_{c}^{(k)}N_{f}^{(k)}}}{\epsilon })$, where N is the size of the feature vectors whose values are ⩾ THmin(minimum threshold), k is the number of iterations and where 𝜖 is the error for estimating those feature vectors. Compared with the classical counterpart, i.e. the performance of our quantum algorithm quadratically improves than others.},
  archive      = {J_APIN},
  author       = {Chakraborty, Sanjay and Shaikh, Soharab Hossain and Chakrabarti, Amlan and Ghosh, Ranjan},
  doi          = {10.1007/s10489-019-01604-3},
  journal      = {Applied Intelligence},
  month        = {6},
  number       = {6},
  pages        = {1775-1793},
  shortjournal = {Appl. Intell.},
  title        = {A hybrid quantum feature selection algorithm using a quantum inspired graph theoretic approach},
  volume       = {50},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). DC programming and DCA for parametric-margin ν-support
vector machine. <em>APIN</em>, <em>50</em>(6), 1763–1774. (<a
href="https://doi.org/10.1007/s10489-019-01618-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As a development of ν-support vector machine (ν-SVM), parametric-margin ν-support vector machine (Par-ν-SVM) can be useful in many cases, especially heteroscedastic noise classification problems. The present article proposes a novel and fast method to solve the primal problem of Par-ν-SVM (named as DC-Par-ν-SVM), while Par-ν-SVM maximizes the parametric-margin by solving a dual quadratic programming problem. In fact, the primal non-convex problem is converted into an unconstrained problem to express the objective function as the difference of convex functions (DC). The DC-Algorithm (DCA) based on generalized Newton’s method is proposed to solve the unconstrained problem cited. Numerical experiments performed on several artificial, real-life, UCI and NDC data sets showed the superiority of the DC-Par-ν-SVM in terms of both accuracy and learning speed.},
  archive      = {J_APIN},
  author       = {Bazikar, Fatemeh and Ketabchi, Saeed and Moosaei, Hossein},
  doi          = {10.1007/s10489-019-01618-x},
  journal      = {Applied Intelligence},
  month        = {6},
  number       = {6},
  pages        = {1763-1774},
  shortjournal = {Appl. Intell.},
  title        = {DC programming and DCA for parametric-margin ν-support vector machine},
  volume       = {50},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Rule-based aggregation driven by similar images for visual
saliency detection. <em>APIN</em>, <em>50</em>(6), 1745–1762. (<a
href="https://doi.org/10.1007/s10489-019-01582-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The visual saliency detection consists in determining the relevant visual information in a scene to segment it from the background. This paper proposes visual saliency detection in a rule-based approach using the image similarity cue to improve saliency detection performance. Our system induces rules for saliency detection, and for a given input image, determines the subset of rules to be used from a set of candidate rules. The proposed approach consists of two main stages: training and testing. Firstly, during the training stage, our system learns an ensemble of rough-set-based rules by combining knowledge extracted from outputs of four state-of-the-art saliency models. Secondly, our system determines the most suitable subset of induced rules for binary detection of pixels of a salient object in an image. The decision of the best subset of rules is based on the image similarity cue. The binary determination of saliency in the output image, exempts us from performing a post-processing stage as is needed in most saliency approaches. The proposed method is evaluated quantitatively on three challenging databases designed for the saliency detection task. The results obtained from the performed experiments indicate that the proposed method outperforms the state-of-the-art approaches used for comparison.},
  archive      = {J_APIN},
  author       = {Lopez-Alanis, Alberto and Lizarraga-Morales, Rocio A. and Contreras-Cruz, Marco A. and Ayala-Ramirez, Victor and Sanchez-Yanez, Raul E. and Trujillo-Romero, Felipe},
  doi          = {10.1007/s10489-019-01582-6},
  journal      = {Applied Intelligence},
  month        = {6},
  number       = {6},
  pages        = {1745-1762},
  shortjournal = {Appl. Intell.},
  title        = {Rule-based aggregation driven by similar images for visual saliency detection},
  volume       = {50},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Customized ranking for products through online reviews: A
method incorporating prospect theory with an improved VIKOR.
<em>APIN</em>, <em>50</em>(6), 1725–1744. (<a
href="https://doi.org/10.1007/s10489-019-01577-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Online product reviews are significant in modern e-business because they can influence consumers’ purchase decisions. However, with the dramatic increase in the number of product categories and reviews, it is impossible for consumers to read all online reviews. In this paper, we design a novel method to help customers rank products using online reviews. Our method can be divided into three stages: generating a list of related alternative products based on specific filter conditions, collecting online reviews, and processing and measuring customer satisfaction. This study offers three significant improvements over previous approaches. First, we incorporate prospect theory that reflects the greater impact of negative reviews on customers’ purchase decisions to measure customer satisfaction concerning each attribute more accurately. Second, we combine the collective attribute weights calculated by entropy weight method (EWM) and the individual attribute weights given by a customer to improve VIKOR method, which can adjust the proportion of the two types of weights according to the customer’s knowledge of the product attributes. Third, for processing online reviews, we develop a new sentiment analysis algorithm that factors in the degree of consumer sentiment. This technique is different from the procedures used by existing studies for ranking products. To validate our method, we conduct a case study of automobile ranking and make some comparisons, which together demonstrate that the proposed method not only saves time and effort but also helps consumers select the products they really want.},
  archive      = {J_APIN},
  author       = {Zhang, Chuan and Tian, Yu-xin and Fan, Ling-wei and Li, Ying-hui},
  doi          = {10.1007/s10489-019-01577-3},
  journal      = {Applied Intelligence},
  month        = {6},
  number       = {6},
  pages        = {1725-1744},
  shortjournal = {Appl. Intell.},
  title        = {Customized ranking for products through online reviews: A method incorporating prospect theory with an improved VIKOR},
  volume       = {50},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Transferring the semantic constraints in human manipulation
behaviors to robots. <em>APIN</em>, <em>50</em>(6), 1711–1724. (<a
href="https://doi.org/10.1007/s10489-019-01580-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this study, we aim to help robots manipulate objects with the guidance of semantic constraints (the grasp location, grasp type, approaching way, trajectory constraint, grasp force, and opening width) which are learnt from human manipulation behaviors. In order to transfer the complex and uncertain relationships between the attributes of object and task and semantic constraints in human behaviors to robots. We propose a representation method of human behaviors in machine-understandable semantics and a collaborative reasoning mechanism. With the suggested semantic constraints from human behaviors, the robot manipulation can be completed under “consciousness” and proper for both the object and task. The shareability of the object attributes, primitive actions and semantic constraints makes the proposed method be generalized to new objects and even new tasks. Additionally, we find that the object’s parts can be grasped sequentially, according to the object’s state and the action to be performed.},
  archive      = {J_APIN},
  author       = {Li, Cici and Tian, Guohui},
  doi          = {10.1007/s10489-019-01580-8},
  journal      = {Applied Intelligence},
  month        = {6},
  number       = {6},
  pages        = {1711-1724},
  shortjournal = {Appl. Intell.},
  title        = {Transferring the semantic constraints in human manipulation behaviors to robots},
  volume       = {50},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Hierarchical linear and nonlinear adaptive learning model
for system identification and prediction. <em>APIN</em>, <em>50</em>(6),
1699–1710. (<a
href="https://doi.org/10.1007/s10489-019-01615-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose a method to increase the model accuracy with linear and nonlinear sub-models. The linear sub-model applies the least square error (LSE) algorithm and the nonlinear sub-model uses neural networks (NN). The two sub-models are updated hierarchically using the Lyapunov function. The proposed method has two advantages: 1) The neural networks is a multi-parametric model. Using the proposed model, the weights of NN model can be summarized into the coefficients or parameters of auto-regressive eXogenous/auto-regressive moving average (ARX/ARMA) model structure, making it easier to establish control laws, 2) learning rate is updated to ensure the convergence of errors at each training epoch. One can improve the accuracy of model and the whole control system. We have demonstrated by the experimental studies that the proposed technique gives better results when compared to the existing studies.},
  archive      = {J_APIN},
  author       = {Jami’in, Mohammad Abu and Anam, Khairul and Rulaningtyas, Riries and Mudjiono, Urip and Adianto, Adianto and Wee, Hui-Ming},
  doi          = {10.1007/s10489-019-01615-0},
  journal      = {Applied Intelligence},
  month        = {6},
  number       = {6},
  pages        = {1699-1710},
  shortjournal = {Appl. Intell.},
  title        = {Hierarchical linear and nonlinear adaptive learning model for system identification and prediction},
  volume       = {50},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Improved image representation and sparse representation for
image classification. <em>APIN</em>, <em>50</em>(6), 1687–1698. (<a
href="https://doi.org/10.1007/s10489-019-01612-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {It seems that for multiple available images of the same object, the pixel values at the same image position are almost always different, which is especially obvious for the deformable object. This implies that it will be not easy to correctly classify the deformable object. In order to extract salient features of images and improve the performance of image classification, a novel image classification algorithm is proposed in this paper. The algorithm can effectively preserve the large-scale information and global features of the original image, reduce the difference in different images of the same object, and significantly improve the accuracy of image classification. Firstly, the virtual image is generated by the new image representation procedure. Secondly, the image classification algorithm is used to obtain the corresponding classification scores of the original image and the virtual image, respectively. Finally, the ultimate classification score is obtained by a simple and efficient score fusion scheme. A large number of experiments on three widely used image databases show that the proposed algorithm outperforms other state-of-the-art algorithms in classification accuracy. At the same time, the algorithm has the advantages of simple implementation and high computational efficiency.},
  archive      = {J_APIN},
  author       = {Zheng, Shijun and Zhang, Yongjun and Liu, Wenjie and Zou, Yongjie},
  doi          = {10.1007/s10489-019-01612-3},
  journal      = {Applied Intelligence},
  month        = {6},
  number       = {6},
  pages        = {1687-1698},
  shortjournal = {Appl. Intell.},
  title        = {Improved image representation and sparse representation for image classification},
  volume       = {50},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Online transferable representation with heterogeneous
sources. <em>APIN</em>, <em>50</em>(6), 1674–1686. (<a
href="https://doi.org/10.1007/s10489-019-01620-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Learning from streaming data has gained a lot of attention and interest in the past decades. These improvements have shown promising results when the models are trained and test on a single streaming source. However, the trained model often fail to produce the reliable results due to the difficulty of data shift and knowledge transfer with heterogeneous streaming domains. In this paper, we propose an architecture that is based on autoencoders. Specifically, we use online feature learning based on denoising autoencoder to learn more robust representations from streaming data. In order to tackle with data shift between source and target streaming data, we develop an ensemble weighted strategy, which can effectively handle the concept drifts of streaming data. Moreover, we develop the transfer mechanism, which is capable of transferring label information across heterogeneous domains. Finally, we combine online learning, data shift adaption and knowledge transfer with heterogeneous domains into a single process, which makes our proposed architecture powerful in learning and predicting for multistream classification problem. Experiments on heterogeneous datasets validate that the proposed algorithm can quickly and accurately classify instances on a stream together with a small number of labeled examples. Compared with a few related methods, our algorithm achieves some state-of-the-art results.},
  archive      = {J_APIN},
  author       = {Li, Yanchao and Li, Hao},
  doi          = {10.1007/s10489-019-01620-3},
  journal      = {Applied Intelligence},
  month        = {6},
  number       = {6},
  pages        = {1674-1686},
  shortjournal = {Appl. Intell.},
  title        = {Online transferable representation with heterogeneous sources},
  volume       = {50},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020a). Editor’s note: Applied intelligence and COVID-19 research.
<em>APIN</em>, <em>50</em>(6), 1673. (<a
href="https://doi.org/10.1007/s10489-020-01721-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_APIN},
  doi          = {10.1007/s10489-020-01721-4},
  journal      = {Applied Intelligence},
  month        = {6},
  number       = {6},
  pages        = {1673},
  shortjournal = {Appl. Intell.},
  title        = {Editor’s note: Applied intelligence and COVID-19 research},
  volume       = {50},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Nonlinear system modeling using self-organizing fuzzy neural
networks for industrial applications. <em>APIN</em>, <em>50</em>(5),
1657–1672. (<a
href="https://doi.org/10.1007/s10489-020-01645-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, a novel self-organizing fuzzy neural network with an adaptive learning algorithm (SOFNN-ALA) for nonlinear system modeling and identification in industrial processes is proposed. To efficiently enhance the generalization capability, the proposed SOFNN-ALA is designed by using both structure identification and parameter estimation simultaneously in the learning process. In the structure identification phase, the rule neuron with the highest neuronal activity will be split into two new rule neurons. Meanwhile, the redundant rule neurons with small singular values will be removed to simplify the network structure. In the parameter estimation phase, an adaptive learning algorithm (ALA), which is designed based on the widely used Levenberg-Marquardt (LM) optimization algorithm, is adopted to optimize the network parameters. The ALA-based learning algorithm can not only speed up the convergence speed but also enhance the modeling performance. Moreover, we carefully analyze the convergence of the proposed SOFNN-ALA to guarantee its successful practical application. Finally, the effectiveness and efficiency of the proposed SOFNN-ALA is validated by several examples. The experimental results demonstrate that the proposed SOFNN-ALA exhibits a better comprehensive performance than some other state-of-the-art SOFNNs for nonlinear system modeling in industrial applications. The source code can be downloaded from https://github.com/hyitzhb/SOFNN-ALA.git.},
  archive      = {J_APIN},
  author       = {Zhou, Hongbiao and Zhao, Huanyu and Zhang, Yu},
  doi          = {10.1007/s10489-020-01645-z},
  journal      = {Applied Intelligence},
  month        = {5},
  number       = {5},
  pages        = {1657-1672},
  shortjournal = {Appl. Intell.},
  title        = {Nonlinear system modeling using self-organizing fuzzy neural networks for industrial applications},
  volume       = {50},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Discrete selfish herd optimizer for solving graph coloring
problem. <em>APIN</em>, <em>50</em>(5), 1633–1656. (<a
href="https://doi.org/10.1007/s10489-020-01636-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Selfish herd optimizer is a new and effective continuous optimization algorithm, but the algorithm cannot solve the specific discrete problem. Therefore, we design a discrete selfish herd optimizer (DSHO) specifically for solving graph coloring problems. At present, many existing methods cannot effectively solve graph coloring problems. To deal with this difficult problem, in DSHO, we propose an effective step-size updating method. This step-size update method is not random, but is updated based on the conflict matrix of candidate solutions. Moreover, we also add a mechanism to deal with potential color conflict areas. By using these methods, DSHO can effectively reduce the number of conflict areas when calculating the conflict matrix of candidate solutions. To verify DSHO’s optimization performance, we use 83 test cases, they includes five simulated maps, six real maps (China, New York, America, Paris, France and Russia) and 72 test cases of minimum coloring number (They come from DIMACS Implementation Challenge). DSHO compare with the latest coloring algorithms, they are DABC, DFA, GETS, HACO, LGFPA, FROGSIM and SDGC, respectively. The experimental outcomes exhibit that DSHO has minimum number of conflict areas, smaller standard deviation, fewer iterations and higher coloring success rate. Discrete selfish herd optimizer is more competitive than other algorithms. Therefore, it is a new method for solving graph coloring problems effectively.},
  archive      = {J_APIN},
  author       = {Zhao, Ruxin and Wang, Yongli and Liu, Chang and Hu, Peng and Jelodar, Hamed and Rabbani, Mahdi and Li, Hao},
  doi          = {10.1007/s10489-020-01636-0},
  journal      = {Applied Intelligence},
  month        = {5},
  number       = {5},
  pages        = {1633-1656},
  shortjournal = {Appl. Intell.},
  title        = {Discrete selfish herd optimizer for solving graph coloring problem},
  volume       = {50},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Progressive residual networks for image super-resolution.
<em>APIN</em>, <em>50</em>(5), 1620–1632. (<a
href="https://doi.org/10.1007/s10489-019-01548-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The recent advances in deep convolutional neural networks (DCNNs) have convincingly demonstrated high-capability reconstruction for single image super-resolution (SR). However, it is a big challenge for most DCNNs-based SR models when the scaling factor increases. In this paper, we propose a novel Progressive Residual Network (PRNet) to integrate hierarchical and scale features for single image SR, which works well for both small and large scaling factors. Specifically, we introduce a Progressive Residual Module (PRM) to extract local multi-scale features through dense connected up-sampling convolution layers. Meanwhile, by embedding residual learning into each module, the relative information between high-resolution and low-resolution multi-scale features is fully exploited to boost reconstruction performance. Finally, the scale-specific features are fused to the reconstruction module for restoring the high-quality image. Extensive quantitative and qualitative evaluations on benchmark datasets illustrate that our PRNet achieves superior performance and in particular obtains new state-of-the-art results for large scaling factors such as 4 × and 8 ×.},
  archive      = {J_APIN},
  author       = {Wan, Jin and Yin, Hui and Chong, Ai-Xin and Liu, Zhi-Hao},
  doi          = {10.1007/s10489-019-01548-8},
  journal      = {Applied Intelligence},
  month        = {5},
  number       = {5},
  pages        = {1620-1632},
  shortjournal = {Appl. Intell.},
  title        = {Progressive residual networks for image super-resolution},
  volume       = {50},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A dirichlet process biterm-based mixture model for short
text stream clustering. <em>APIN</em>, <em>50</em>(5), 1609–1619. (<a
href="https://doi.org/10.1007/s10489-019-01606-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Short text stream clustering has become an important problem for mining textual data in diverse social media platforms (e.g., Twitter). However, most of the existing clustering methods (e.g., LDA and PLSA) are developed based on the assumption of a static corpus of long texts, while little attention has been given to short text streams. Different from the long texts, the clustering of short texts is more challenging since their word co-occurrence pattern easily suffers from a sparsity problem. In this paper, we propose a Dirichlet process biterm-based mixture model (DP-BMM), which can deal with the topic drift problem and the sparsity problem in short text stream clustering. The major advantages of DP-BMM include (1) DP-BMM explicitly exploits the word-pairs constructed from each document to enhance the word co-occurrence pattern in short texts; (2) DP-BMM can deal with the topic drift problem of short text streams naturally. Moreover, we further propose an improved algorithm of DP-BMM with forgetting property called DP-BMM-FP, which can efficiently delete biterms of outdated documents by deleting clusters of outdated batches. To perform inference, we adopt an online Gibbs sampling method for parameter estimation. Our extensive experimental results on real-world datasets show that DP-BMM and DP-BMM-FP can achieve a better performance than the state-of-the-art methods in terms of NMI metrics.},
  archive      = {J_APIN},
  author       = {Chen, Junyang and Gong, Zhiguo and Liu, Weiwen},
  doi          = {10.1007/s10489-019-01606-1},
  journal      = {Applied Intelligence},
  month        = {5},
  number       = {5},
  pages        = {1609-1619},
  shortjournal = {Appl. Intell.},
  title        = {A dirichlet process biterm-based mixture model for short text stream clustering},
  volume       = {50},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Neuroevolutionary learning in nonstationary environments.
<em>APIN</em>, <em>50</em>(5), 1590–1608. (<a
href="https://doi.org/10.1007/s10489-019-01591-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This work presents a new neuro-evolutionary model, called NEVE (Neuroevolutionary Ensemble), based on an ensemble of Multi-Layer Perceptron (MLP) neural networks for learning in nonstationary environments. NEVE makes use of quantum-inspired evolutionary models to automatically configure the ensemble members and combine their output. The quantum-inspired evolutionary models identify the most appropriate topology for each MLP network, select the most relevant input variables, determine the neural network weights and calculate the voting weight of each ensemble member. Four different approaches of NEVE are developed, varying the mechanism for detecting and treating concepts drifts, including proactive drift detection approaches. The proposed models were evaluated in real and artificial datasets, comparing the results obtained with other consolidated models in the literature. The results show that the accuracy of NEVE is higher in most cases and the best configurations are obtained using some mechanism for drift detection. These results reinforce that the neuroevolutionary ensemble approach is a robust choice for situations in which the datasets are subject to sudden changes in behaviour.},
  archive      = {J_APIN},
  author       = {Escovedo, Tatiana and Koshiyama, Adriano and da Cruz, Andre Abs and Vellasco, Marley},
  doi          = {10.1007/s10489-019-01591-5},
  journal      = {Applied Intelligence},
  month        = {5},
  number       = {5},
  pages        = {1590-1608},
  shortjournal = {Appl. Intell.},
  title        = {Neuroevolutionary learning in nonstationary environments},
  volume       = {50},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Multiple allocation p-hub location problem for content
placement in VoD services: A differential evolution based approach.
<em>APIN</em>, <em>50</em>(5), 1573–1589. (<a
href="https://doi.org/10.1007/s10489-019-01609-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In video-on-demand (VoD) services, large volumes of digital data are kept at hubs which are spatially distributed over large geographic areas and users are connected to these hubs based on their demands. In this article, we consider a large database of video files, that are pre-partitioned to multiple segments based on the demand patterns of users. These segments are restricted to be located only in hubs. Here, users are allowed to be allocated to multiple hubs and all hubs are assumed to be connected with each other. We jointly decide the location of hubs, the placement of segments to these hubs and then the assignment of users to these hubs as per their demand patterns and finally, we find the optimal paths to route the demands of users for different segments having the objective of minimizing the total routing cost. In this article, a differential evolution (DE) based method is proposed to solve the problem. The proposed DE-based method utilizes an efficient function to evaluate the objective value of a candidate solution to the proposed problem. It also incorporates two problem-specific solution refinement techniques for faster convergence. Instances of the problem are generated from the real world movie database and the proposed method is applied to these instances and the performance is evaluated against the benchmark results obtained from CPLEX.},
  archive      = {J_APIN},
  author       = {Atta, Soumen and Sen, Goutam},
  doi          = {10.1007/s10489-019-01609-y},
  journal      = {Applied Intelligence},
  month        = {5},
  number       = {5},
  pages        = {1573-1589},
  shortjournal = {Appl. Intell.},
  title        = {Multiple allocation p-hub location problem for content placement in VoD services: A differential evolution based approach},
  volume       = {50},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Faster horn diagnosis - a performance comparison of
abductive reasoning algorithms. <em>APIN</em>, <em>50</em>(5),
1558–1572. (<a
href="https://doi.org/10.1007/s10489-019-01575-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abductive inference derives explanations for encountered anomalies and thus embodies a natural approach for diagnostic reasoning. Yet its computational complexity, which is inherent to the expressiveness of the underlying theory, remains a disadvantage. Even when restricting the representation to Horn formulae the problem is NP-complete. Hence, finding procedures that can efficiently solve abductive diagnosis problems is of particular interest from a research as well as practical point of view. In this paper, we aim at providing guidance on choosing an algorithm or tool when confronted with the issue of computing explanations in propositional logic-based abduction. Our focus lies on Horn representations, which provide a suitable language to describe most diagnostic scenarios. We illustrate abduction via two contrasting problem formulations: direct proof methods and conflict-driven techniques. While the former is based on determining logical consequences, the later searches for suitable refutations involving possible causes. To reveal runtime performance trends we conducted a case study, in which we compared publicly available general purpose tools, established Horn reasoning engines, as well as new variations of known methods as a means for abduction.},
  archive      = {J_APIN},
  author       = {Koitz-Hristov, Roxane and Wotawa, Franz},
  doi          = {10.1007/s10489-019-01575-5},
  journal      = {Applied Intelligence},
  month        = {5},
  number       = {5},
  pages        = {1558-1572},
  shortjournal = {Appl. Intell.},
  title        = {Faster horn diagnosis - a performance comparison of abductive reasoning algorithms},
  volume       = {50},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Semigroup of fuzzy automata and its application for fast
accurate fault diagnosis on machine and anti-fatigue control.
<em>APIN</em>, <em>50</em>(5), 1542–1557. (<a
href="https://doi.org/10.1007/s10489-019-01611-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In order to carry out machine fault diagnosis earlier and more accurately such as the automatic detection for the ship’s level scale, and the existing literatures didn’t discuss these until now. However, for solving the problem, this paper presents a semigroup of fuzzy automata and its properties, constructs a fuzzy inference system on the semigroup of fuzzy automata, and discusses its application on machine fault diagnosis and the anti-fatigue driving reminder device. At the same time, the comparison between this inference model and the existing diagnosis methods is discussed. The experimental results show that the diagnosis speed and the average precision of the proposed inference model are faster and higher than those of traditional methods, which their maximum diagnosis precision is 95.98%.},
  archive      = {J_APIN},
  author       = {Wu, Qing E and Guang, Mengke and Chen, Hu and Sun, Lijun},
  doi          = {10.1007/s10489-019-01611-4},
  journal      = {Applied Intelligence},
  month        = {5},
  number       = {5},
  pages        = {1542-1557},
  shortjournal = {Appl. Intell.},
  title        = {Semigroup of fuzzy automata and its application for fast accurate fault diagnosis on machine and anti-fatigue control},
  volume       = {50},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A parameter-free hybrid instance selection algorithm based
on local sets with natural neighbors. <em>APIN</em>, <em>50</em>(5),
1527–1541. (<a
href="https://doi.org/10.1007/s10489-019-01598-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Instance selection aims to search for the best patterns in the training set and main instance selection methods include condensation methods, edition methods and hybrid methods. Hybrid methods combine advantages of both edition methods and condensation methods. Nevertheless, most of existing hybrid approaches heavily rely on parameters and are relatively time-consuming, resulting in the performance instability and application difficulty. Though several relatively fast and (or) parameter-free hybrid methods are proposed, they still have the difficulty in achieving both high accuracy and high reduction. In order to solve these problems, we present a new parameter-free hybrid instance selection algorithm based on local sets with natural neighbors (LSNaNIS). A new parameter-free definition for the local set is first proposed based on the fast search for natural neighbors. The new local set can fast and reasonably describe local characteristics of data. In LSNaNIS, we use the new local set to design an edition method (LSEdit) to remove harmful samples, a border method (LSBorder) to retain representative border samples and a core method (LSCore) to condense internal samples. Comparison experiments show that LSNaNIS is relatively fast and outperforms existing hybrid methods in improving the k-nearest neighbor in terms of both accuracy and reduction.},
  archive      = {J_APIN},
  author       = {Li, Junnan and Zhu, Qingsheng and Wu, Quanwang},
  doi          = {10.1007/s10489-019-01598-y},
  journal      = {Applied Intelligence},
  month        = {5},
  number       = {5},
  pages        = {1527-1541},
  shortjournal = {Appl. Intell.},
  title        = {A parameter-free hybrid instance selection algorithm based on local sets with natural neighbors},
  volume       = {50},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A multi-population differential evolution with best-random
mutation strategy for large-scale global optimization. <em>APIN</em>,
<em>50</em>(5), 1510–1526. (<a
href="https://doi.org/10.1007/s10489-019-01613-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Differential evolution (DE) is an efficient population-based search algorithm with good robustness, but it faces challenges in dealing with Large-Scale Global Optimization (LSGO). In this paper, we proposed an improved multi-population differential evolution with best-random mutation strategy (called mDE-brM). The population is divided into three sub-populations based on the fitness values, each sub-population uses different mutation strategies and control parameters, individuals share different mutation strategies and control parameters by migrating among sub-populations. A novel mutation strategy is proposed, which uses the best individual and a randomly selected individual to generate base vector. The performance of mDE-brM is evaluated on the CEC 2013 LSGO benchmark suite and compared with 5 state-of-the-art optimization techniques. The results show that, compared with other contestant algorithms, mDE-brM has a competitive performance and better efficiency in LSGO.},
  archive      = {J_APIN},
  author       = {Ma, Yongjie and Bai, Yulong},
  doi          = {10.1007/s10489-019-01613-2},
  journal      = {Applied Intelligence},
  month        = {5},
  number       = {5},
  pages        = {1510-1526},
  shortjournal = {Appl. Intell.},
  title        = {A multi-population differential evolution with best-random mutation strategy for large-scale global optimization},
  volume       = {50},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A dissimilarity measure for mixed nominal and ordinal
attribute data in k-modes algorithm. <em>APIN</em>, <em>50</em>(5),
1498–1509. (<a
href="https://doi.org/10.1007/s10489-019-01583-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Among the existing clustering algorithms, the k-Means algorithm is one of the most commonly used clustering methods. As an extension of the k-Means algorithm, the k-Modes algorithm has been widely applied to categorical data clustering by replacing means with modes. However, there are more mixed-type data containing categorical, ordinal and numerical attributes. Mixed-type data clustering problem has recently attracted much attention from the data mining research community, but most of them fail to notice the ordinal attributes and establish explicit metric similarity of ordinal attributes. In this paper, the limitations of some existing dissimilarity measure of k-Modes algorithm in mixed ordinal and nominal data are analyzed by using some illustrative examples. Based on the idea of mining ordinal information of ordinal attribute, a new dissimilarity measure for the k-Modes algorithm to cluster this type of data is proposed. The distinct characteristic of the new dissimilarity measure is to take account of the ordinal information of ordinal attribute. A convergence study and time complexity of the k-Modes algorithm based on this new dissimilarity measure indicates that it can be effectively used for large data sets. The results of comparative experiments on nine real data sets from UCI show the effectiveness of the new dissimilarity measure.},
  archive      = {J_APIN},
  author       = {Yuan, Fang and Yang, Youlong and Yuan, Tiantian},
  doi          = {10.1007/s10489-019-01583-5},
  journal      = {Applied Intelligence},
  month        = {5},
  number       = {5},
  pages        = {1498-1509},
  shortjournal = {Appl. Intell.},
  title        = {A dissimilarity measure for mixed nominal and ordinal attribute data in k-modes algorithm},
  volume       = {50},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Mining top-k frequent patterns from uncertain databases.
<em>APIN</em>, <em>50</em>(5), 1487–1497. (<a
href="https://doi.org/10.1007/s10489-019-01622-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mining uncertain frequent patterns (UFPs) from uncertain databases was recently introduced, and there are various approaches to solve this problem in the last decade. However, systems are often faced with the problem of too many UFPs being discovered by the traditional approaches to this issue, and thus will spend a lot of time and resources to rank and find the most promising patterns. Therefore, this paper introduces a task named mining top-k UFPs from uncertain databases. We then propose an efficient method named TUFP (mining Top-k UFPs) to carry this out. Effective threshold raising strategies are introduced to help the proposed algorithm reduce the number of generated candidates to enhance the performance in terms of the runtime as well as memory usage. Finally, several experiments on the number of generated candidates, mining time, memory usage and scalability of TUFP and two state-of-the-art approaches (CUFP-mine and LUNA) were conducted. The performance studies show that TUFP is efficient in terms of mining time, memory usage and scalability for mining top-k UFPs.},
  archive      = {J_APIN},
  author       = {Le, Tuong and Vo, Bay and Huynh, Van-Nam and Nguyen, Ngoc Thanh and Baik, Sung Wook},
  doi          = {10.1007/s10489-019-01622-1},
  journal      = {Applied Intelligence},
  month        = {5},
  number       = {5},
  pages        = {1487-1497},
  shortjournal = {Appl. Intell.},
  title        = {Mining top-k frequent patterns from uncertain databases},
  volume       = {50},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Multi-scale affined-HOF and dimension selection for
view-unconstrained action recognition. <em>APIN</em>, <em>50</em>(5),
1468–1486. (<a
href="https://doi.org/10.1007/s10489-019-01572-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper an action recognition method that can adaptively handle the problems of variations in camera viewpoint is introduced. Our contribution is three-fold. First, a space-sampling algorithm based on affine transform in multiple scales is proposed to yield a series of different viewpoints from a single one. A histogram of dense optical flow is then extracted over each fixed-size patch for a given generated viewpoint as a local feature descriptor. Second, a dimension selection procedure is also proposed to retain only the dimensions that have distinctive information and discard the unnecessary ones in the feature vector space. Third, to adapt to a situation in which video data in multiple viewpoints are used for training; an extended method with a voting algorithm is also introduced to increase the recognition accuracy. By conducting experiments using both simulated and realistic datasets (http://www.aislab.org/index.php/en/mvar-datasets), the proposed method is validated. The method is found to be accurate and capable of maintaining its accuracy under a wide range of viewpoint changes. In addition, the method is less sensitive to variations in subject scale, subject position, action speed, partial occlusion, and background. The method is also validated by comparing with state-of-the-art view-invariant action recognition methods using well-known i3DPost and MuHAVi public datasets.},
  archive      = {J_APIN},
  author       = {Tran, Dinh Tuan and Yamazoe, Hirotake and Lee, Joo-Ho},
  doi          = {10.1007/s10489-019-01572-8},
  journal      = {Applied Intelligence},
  month        = {5},
  number       = {5},
  pages        = {1468-1486},
  shortjournal = {Appl. Intell.},
  title        = {Multi-scale affined-HOF and dimension selection for view-unconstrained action recognition},
  volume       = {50},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). STDS: Self-training data streams for mining limited labeled
data in non-stationary environment. <em>APIN</em>, <em>50</em>(5),
1448–1467. (<a
href="https://doi.org/10.1007/s10489-019-01585-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Inthis article, wefocus on the classification problem to semi-supervised learning in non-stationary environment. Semi-supervised learning is a learning task from both labeled and unlabeled data points. There are several approaches to semi-supervised learning in stationary environment which are not applicable directly for data streams. We propose a novel semi-supervised learning algorithm, named STDS. The proposed approach uses labeled and unlabeled data and employs an approach to handle the concept drift in data streams. The main challenge in semi-supervised self-training for data streams is to find a proper selection metric in order to find a set of high-confidence predictions and a proper underlying base learner. We therefore propose an ensemble approach to find a set of high-confidence predictions based on clustering algorithms and classifier predictions. We then employ the Kullback-Leibler (KL) divergence approach to measure the distribution differences between sequential chunks in order to detect the concept drift. When drift is detected, a new classifier is updated from the new set of labeled data in the current chunk; otherwise, a percentage of high-confidence newly labeled data in the current chunk is added to the labeled data in the next chunk for updating the incremental classifier based on the proposed selection metric. The results of our experiments on a number of classification benchmark datasets show that STDS outperforms the supervised and the most of other semi-supervised learning methods.},
  archive      = {J_APIN},
  author       = {Khezri, Shirin and Tanha, Jafar and Ahmadi, Ali and Sharifi, Arash},
  doi          = {10.1007/s10489-019-01585-3},
  journal      = {Applied Intelligence},
  month        = {5},
  number       = {5},
  pages        = {1448-1467},
  shortjournal = {Appl. Intell.},
  title        = {STDS: Self-training data streams for mining limited labeled data in non-stationary environment},
  volume       = {50},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Single image deraining via deep pyramid network with spatial
contextual information aggregation. <em>APIN</em>, <em>50</em>(5),
1437–1447. (<a
href="https://doi.org/10.1007/s10489-019-01567-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Rain streaks usually give rise to visual degradation and cause many computer vision algorithms to fail. So it is necessary to develop an effective deraining algorithm as preprocess of high-level vision tasks. In this paper, we propose a novel deep learning based deraining method. Specifically, the multi-scale kernels and feature maps are both important for single image deraining. However, the previous works ignore the two multi-scale information or only consider the multi-scale kernels information. Instead, our method learns multi-scale information both from the perspectives of kernels and feature maps, respectively, by designing spatial contextual information aggregation module and pyramid network module. The former module can capture the rain streaks with different sizes and the latter module can extract rain streaks from different scales further. Moreover, we also employ squeeze-and-excitation and skip connections to enhance the correlation between channels and transmit the information from low-level to high-level, respectively. The experimental results show that the proposed method achieves significant improvements over the recent state-of-the-art methods in Rain100H, Rain100L, Rain1200 and Rain1400 datasets.},
  archive      = {J_APIN},
  author       = {Wang, Cong and Wu, Yutong and Cai, Yu and Yao, Guangle and Su, Zhixun and Wang, Hongyan},
  doi          = {10.1007/s10489-019-01567-5},
  journal      = {Applied Intelligence},
  month        = {5},
  number       = {5},
  pages        = {1437-1447},
  shortjournal = {Appl. Intell.},
  title        = {Single image deraining via deep pyramid network with spatial contextual information aggregation},
  volume       = {50},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Asymmetric response aggregation heuristics for rating
prediction and recommendation. <em>APIN</em>, <em>50</em>(5), 1416–1436.
(<a href="https://doi.org/10.1007/s10489-019-01594-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {User-based collaborative filtering is widely used in recommendation systems, which normally comprises three steps: (1) finding the nearest conceptual neighbors, (2) aggregating the neighbors’ ratings to predict the ratings of unrated items, and (3) generating recommendations based on the prediction. Existing algorithms mainly focus on steps 1 and 3 but neglect subtle treatments of aggregating neighbors’ suggestions in step 2. Based on the discovery of psychology that (i) users’ responses to positive and negative suggestions are different, and (ii) users may respond differently from one another, this paper proposes a Personal Asymmetry Response-based Suggestions Aggregation (PARSA) algorithm, which first uses a linear regression method to learn each user’s response to negative/positive suggestions from neighbors and then uses a gradient descent algorithm for optimizing them. In addition, this paper designs an Identical Asymmetry Response-based Suggestions Aggregation (IARSA) baseline algorithm, which assumes that all the users’ responses to suggestions are identical as references to verify the key contribution of the heuristics employed in our PARSA algorithm that user may responses differently to positive and negative suggestions. Three sets of experiments are designed and implemented over two real-life datasets (i.e., Eachmovie and Netflix) to evaluate the performance of our algorithms. Further, in order to eliminate the influence of different similarity measures, this paper selects three kinds of similarity measures to discover neighbors. Experimental results demonstrate that most people indeed pay more attention to negative suggestions and our algorithms achieve better prediction and recommendation performances than the compared algorithms under various similarity measures.},
  archive      = {J_APIN},
  author       = {Ji, Shujuan and Yang, Wei and Guo, Shenghui and Chiu, Dickson K.W. and Zhang, Chunjin and Yuan, Xinyue},
  doi          = {10.1007/s10489-019-01594-2},
  journal      = {Applied Intelligence},
  month        = {5},
  number       = {5},
  pages        = {1416-1436},
  shortjournal = {Appl. Intell.},
  title        = {Asymmetric response aggregation heuristics for rating prediction and recommendation},
  volume       = {50},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Reduction methods of type-2 fuzzy variables and their
applications to stackelberg game. <em>APIN</em>, <em>50</em>(5),
1398–1415. (<a
href="https://doi.org/10.1007/s10489-019-01578-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper is designed based on the mathematical models for bi-level programming in Stackelberg game under type-2 fuzzy environment. The parameters of the objective functions on both levels are considered as type-2 fuzzy numbers in the first case whereas the parameters of the objective functions and the constraints are chosen as type-2 fuzzy numbers in the second case. Critical value based reduction methods are applied to reduce type-2 fuzzy numbers to type-1 fuzzy numbers in the first case. After that, centroid method is used for completely defuzzifying type-2 fuzzy numbers. Besides this, the obtained results are compared with the help of LINGO iterative scheme and genetic algorithm. Coming to the second case, a chance constraint programming with the help of generalized credibility measure is utilized to convert the fuzzy problem to its equivalent crisp form. LINGO iterative scheme is used to solve the deterministic problem using fuzzy programming. The sensitivity analysis is shown to different credibility levels of right hand side of the constraints to find the value of objective function in each level. Finally, real-life based numerical problems are presented to show the performance of the proposed models and techniques. At last, conclusion about the findings and outlook are described.},
  archive      = {J_APIN},
  author       = {Roy, Sankar Kumar and Maiti, Sumit Kumar},
  doi          = {10.1007/s10489-019-01578-2},
  journal      = {Applied Intelligence},
  month        = {5},
  number       = {5},
  pages        = {1398-1415},
  shortjournal = {Appl. Intell.},
  title        = {Reduction methods of type-2 fuzzy variables and their applications to stackelberg game},
  volume       = {50},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Joint dictionary and graph learning for unsupervised feature
selection. <em>APIN</em>, <em>50</em>(5), 1379–1397. (<a
href="https://doi.org/10.1007/s10489-019-01561-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the explosion of unlabelled and high-dimensional data, unsupervised feature selection has become an critical and challenging problem in machine learning. Recently, data representation based model has been successfully deployed for unsupervised feature selection, which defines feature importance as the capability to represent original data via a reconstruction function. However, most existing algorithms conduct feature selection on original feature space, which will be affected by the noisy and redundant features of original feature space. In this paper, we investigate how to conduct feature selection on the dictionary basis space of the data, which can capture higher level and more abstract representation than original low-level representation. In addition, a similarity graph is learned simultaneously to preserve the local geometrical data structure which has been confirmed critical for unsupervised feature selection. In summary, we propose a model (referred to as DGL-UFS briefly) to integrate dictionary learning, similarity graph learning and feature selection into a uniform framework. Experiments on various types of real world datasets demonstrate the effectiveness of the proposed framework DGL-UFS.},
  archive      = {J_APIN},
  author       = {Ding, Deqiong and Xia, Fei and Yang, Xiaogao and Tang, Chang},
  doi          = {10.1007/s10489-019-01561-x},
  journal      = {Applied Intelligence},
  month        = {5},
  number       = {5},
  pages        = {1379-1397},
  shortjournal = {Appl. Intell.},
  title        = {Joint dictionary and graph learning for unsupervised feature selection},
  volume       = {50},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Sketch discriminatively regularized online gradient descent
classification. <em>APIN</em>, <em>50</em>(5), 1367–1378. (<a
href="https://doi.org/10.1007/s10489-019-01590-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Online learning represents an important family of efficient and scalable algorithms for large-scale classification problems. Many of them are linear with fast computational speed, but when faced with complex classification, they more likely have low accuracies. In order to improve accuracies, kernel trick is applied, however, it often brings high computational cost. In fact, discriminative information is vital in classification which is still not fully utilized in these algorithms. In this paper, we proposed a novel online linear method, called Sketch Discriminatively Regularized Online Gradient Descent Classification (SDROGD). In order to exploit inter-class separability and intra-class compactness, SDROGD utilizes a matrix to characterize the discriminative information and embeds it directly into a new regularization term. This matrix can be updated by the sketch technique in an online manner. After applying a simple but effective optimization, we show that SDROGD has a good time complexity bound, which is linear with the feature dimension or the number of samples. Experimental results on both toy and real-world datasets demonstrate that SDROGD has not only faster computational speed but also much better classification accuracies than some related kernelized algorithms.},
  archive      = {J_APIN},
  author       = {Xue, Hui and Ren, Zhen},
  doi          = {10.1007/s10489-019-01590-6},
  journal      = {Applied Intelligence},
  month        = {5},
  number       = {5},
  pages        = {1367-1378},
  shortjournal = {Appl. Intell.},
  title        = {Sketch discriminatively regularized online gradient descent classification},
  volume       = {50},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Two novel ELM-based stacking deep models focused on image
recognition. <em>APIN</em>, <em>50</em>(5), 1345–1366. (<a
href="https://doi.org/10.1007/s10489-019-01584-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Extreme learning machine (ELM) and its variants have been widely used in the field of object recognition and other complex classification tasks. Traditional deep learning architectures like Convolutional Neural Network (CNN) are capable of extracting high-level features, which are the key for the models to make right decisions. However, traditional deep architectures are confronted with solving a tough, non-convex optimization problem, which is a time-consuming process. In this paper, we propose two hierarchical models, i.e., Random Recursive Constrained ELM (R2CELM) and Random Recursive Local- Receptive-Fields-Based ELM (R2ELM-LRF), which are constructed by stacking with CELM or ELM-LRF, respectively. Besides, inspired by the stacking generalization philosophy, random projection and kernelization are incorporated as their constitutive elements. R2CELM and R2ELM-LRF not only fully inherit the merits of ELM, but also take advantage of the superiority of CELM and ELM-LRF in the field of image recognition, respectively. The essence of CELM is to constrain the weight vectors from the input layer to the hidden layer to be consistent with the directions from one class to another class, while ELM-LRF is adept at exploiting the local structures in images through many local receptive fields. In the empirical results, R2CELM and R2ELM-LRF demonstrate their better performance in testing accuracy on the six benchmark image recognition datasets, compared with their basic learners and other state-of-the-art algorithms. Moreover, the proposed two deep ELM models need less training time when compared with traditional Deep Neural Network (DNN) based models.},
  archive      = {J_APIN},
  author       = {Song, Gang and Dai, Qun and Han, Xiaomeng and Guo, Lin},
  doi          = {10.1007/s10489-019-01584-4},
  journal      = {Applied Intelligence},
  month        = {5},
  number       = {5},
  pages        = {1345-1366},
  shortjournal = {Appl. Intell.},
  title        = {Two novel ELM-based stacking deep models focused on image recognition},
  volume       = {50},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Unconstrained convex minimization based implicit lagrangian
twin extreme learning machine for classification (ULTELMC).
<em>APIN</em>, <em>50</em>(4), 1327–1344. (<a
href="https://doi.org/10.1007/s10489-019-01596-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The recently proposed twin extreme learning machine (TELM) requires solving two quadratic programming problems (QPPs) in order to find two non-parallel hypersurfaces in the feature that brings in the additional requirement of external optimization toolbox such as MOSEK. In this paper, we propose implicit Lagrangian TELM for classification via unconstrained convex minimization problem (ULTELMC) and further suggest iterative convergent schemes which eliminates the requirement of external optimization toolbox generally required in solving the quadratic programming problems (QPPs) of TELM. The solutions to the dual variables of the proposed ULTELMC are obtained using iterative schemes containing ‘plus’ function which is not differentiable. To overcome this shortcoming, the generalized derivative approach and smooth approximation approaches are suggested. Further, to test the performance of the proposed approaches, classification performances are compared with support vector machine (SVM), twin support vector machine (TWSVM), extreme learning machine (ELM), twin extreme learning machine (TELM) and Lagrangian extreme learning machine (LELM). Moreover, non-requirement to solve QPPs makes the iterative schemes find the solution faster as compared to the reported methods that finds the solution in dual space. Computational times required in finding the solutions are also presented for comparison.},
  archive      = {J_APIN},
  author       = {Borah, Parashjyoti and Gupta, Deepak},
  doi          = {10.1007/s10489-019-01596-0},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {4},
  pages        = {1327-1344},
  shortjournal = {Appl. Intell.},
  title        = {Unconstrained convex minimization based implicit lagrangian twin extreme learning machine for classification (ULTELMC)},
  volume       = {50},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Collision avoiding decentralized sorting of robotic swarm.
<em>APIN</em>, <em>50</em>(4), 1316–1326. (<a
href="https://doi.org/10.1007/s10489-019-01602-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sorting the swarm of robots is required when the robots are carrying different loads and they can not simply swap the loads. In 2016, Zhou et al. presented an interesting algorithm to sort a swarm of robots, wherein the authors made a main tree and a feedback tree to assign a topology to the robots, based on which the robots moved while arranging themselves in a sorted order. While the approach was very interesting and the results were critically analyzed by the authors, we see a critical problem that the approach did not account for collisions because of which the results can be very different. In this paper, we extend the work of Zhou et al. by enabling the robots to avoid collision by using a geometric approach called as “follow the gap” method. Together both the algorithms allow robot swarm to sort themselves in a straight line while avoiding collision simultaneously.},
  archive      = {J_APIN},
  author       = {Kumar, Utkarsh and Banerjee, Adrish and Kala, Rahul},
  doi          = {10.1007/s10489-019-01602-5},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {4},
  pages        = {1316-1326},
  shortjournal = {Appl. Intell.},
  title        = {Collision avoiding decentralized sorting of robotic swarm},
  volume       = {50},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Multi-strategy brain storm optimization algorithm with
dynamic parameters adjustment. <em>APIN</em>, <em>50</em>(4), 1289–1315.
(<a href="https://doi.org/10.1007/s10489-019-01600-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As a novel swarm intelligence optimization algorithm, brain storm optimization (BSO) has its own unique capabilities in solving optimization problems. However, the performance of traditional BSO strategy in balancing exploitation and exploration is inadequate, which reduces the convergence performance of BSO. To overcome these problems, a multi-strategy BSO with dynamic parameters adjustment (MSBSO) is presented in this paper. In MSBSO, four competitive strategies based on improved individual selection rules are designed to adapt to different search scopes, thus obtaining more diverse and effective individuals. In addition, a simple adaptive parameter that can dynamically regulate search scopes is designed as the basis for selecting strategies. The proposed MSBSO algorithm and other state-of-the-art algorithms are tested on CEC 2013 benchmark functions and CEC 2015 large scale global optimization (LSGO) benchmark functions, and the experimental results prove that the MSBSO algorithm is more competitive than other related algorithms.},
  archive      = {J_APIN},
  author       = {Liu, Jianan and Peng, Hu and Wu, Zhijian and Chen, Jianqiang and Deng, Changshou},
  doi          = {10.1007/s10489-019-01600-7},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {4},
  pages        = {1289-1315},
  shortjournal = {Appl. Intell.},
  title        = {Multi-strategy brain storm optimization algorithm with dynamic parameters adjustment},
  volume       = {50},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Feature redundancy term variation for mutual
information-based feature selection. <em>APIN</em>, <em>50</em>(4),
1272–1288. (<a
href="https://doi.org/10.1007/s10489-019-01597-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Feature selection plays a critical role in many applications that are relevant to machine learning, image processing and gene expression analysis. Traditional feature selection methods intend to maximize feature dependency while minimizing feature redundancy. In previous information-theoretical-based feature selection methods, feature redundancy term is measured by the mutual information between a candidate feature and each already-selected feature or the interaction information among a candidate feature, each already-selected feature and the class. However, the larger values of the traditional feature redundancy term do not indicate the worse a candidate feature because a candidate feature can obtain large redundant information, meanwhile offering large new classification information. To address this issue, we design a new feature redundancy term that considers the relevancy between a candidate feature and the class given each already-selected feature, and a novel feature selection method named min-redundancy and max-dependency (MRMD) is proposed. To verify the effectiveness of our method, MRMD is compared to eight competitive methods on an artificial example and fifteen real-world data sets respectively. The experimental results show that our method achieves the best classification performance with respect to multiple evaluation criteria.},
  archive      = {J_APIN},
  author       = {Gao, Wanfu and Hu, Liang and Zhang, Ping},
  doi          = {10.1007/s10489-019-01597-z},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {4},
  pages        = {1272-1288},
  shortjournal = {Appl. Intell.},
  title        = {Feature redundancy term variation for mutual information-based feature selection},
  volume       = {50},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Belief-peaks clustering based on fuzzy label propagation.
<em>APIN</em>, <em>50</em>(4), 1259–1271. (<a
href="https://doi.org/10.1007/s10489-019-01576-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For unsupervised learning, we propose a new clustering method which incorporates belief peaks into a linear label propagation strategy. The proposed method aims to reveal the data structure by finding out the exact number of clusters and deriving a fuzzy partition. Firstly, the cluster centers and outliers can be identified by the improved belief metric, which makes use of the whole data distribution information so as to correctly highlight the cluster centers without the limitation of massive neighbor points. Secondly, an informative initial fuzzy cluster assignment for each remaining point is created by considering the distances between its neighbors and each cluster center, then the fuzzy label of each point will be iteratively updated by absorbing its neighbors’ label information until the fuzzy partition is stable. The label propagation assignment strategy provides a valuable alternative technique with explicit convergence and linear complexity in the field of belief-peaks clustering. The effectiveness of the proposed method is tested on seven commonly used real-world datasets from the UCI Machine Learning Repository, and seven synthetic datasets in the domain of data clustering. Comparing with several state-of-the-art clustering methods, the experiments reveal that the proposed method enhanced the clustering results in terms of the exact numbers of clusters and the Adjusted Rand Index. Further, the parameter analysis experiments validate the robustness to the two tunable parameters in the proposed method.},
  archive      = {J_APIN},
  author       = {Meng, Jintao and Fu, Dongmei and Tang, Yongchuan},
  doi          = {10.1007/s10489-019-01576-4},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {4},
  pages        = {1259-1271},
  shortjournal = {Appl. Intell.},
  title        = {Belief-peaks clustering based on fuzzy label propagation},
  volume       = {50},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). An efficient strategy for using multifactorial optimization
to solve the clustered shortest path tree problem. <em>APIN</em>,
<em>50</em>(4), 1233–1258. (<a
href="https://doi.org/10.1007/s10489-019-01599-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Arising from the need of all time for optimization of irrigation systems, distribution network and cable network, Clustered Shortest-Path Tree Problem (CluSPT) has been attracting a lot of attention and interest from the research community. On the other hand, the Multifactorial Evolutionary Algorithm (MFEA) is one of the most recently exploited realms of Evolutionary Algorithms (EAs) and its performance in solving optimization problems has been very promising. Considering these characteristics, this paper describes a new approach using the MFEA for solving the CluSPT. The MFEA has two tasks: the goal of the first task is to determine the best tree (w.r.t. cost minimization) which envelops all vertices of the CluSPT while the goal of the second task is to find the fittest solution possible for the problem. The purpose of the second task is to find good materials for implicit genetic transfer process in MFEA to improve the quality of CluSPT. To apply this new algorithm, a decoding scheme for deriving individual solutions from the unified representation in the MFEA is also introduced in this paper. Furthermore, evolutionary operators such as population initialization, crossover and mutation operators are also proposed. These operators are applicable for constructing valid solution from both sparse and complete graph. Although the proposed algorithm is slightly complicated for implementation, it can enhance ability to explore and exploit the Unified Search Space (USS). To prove this increment in performance i.e, to assess the effectiveness of the proposed algorithm and methods, the authors implemented them on both Euclidean and Non-Euclidean instances. Experiment results show that the proposed MFEA outperformed existing heuristic algorithms in most of the test cases. The impact of the proposed MFEA was analyzed and a possible influential factor that may be useful for further study was also pointed out.},
  archive      = {J_APIN},
  author       = {Thanh, Pham Dinh and Binh, Huynh Thi Thanh and Trung, Tran Ba},
  doi          = {10.1007/s10489-019-01599-x},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {4},
  pages        = {1233-1258},
  shortjournal = {Appl. Intell.},
  title        = {An efficient strategy for using multifactorial optimization to solve the clustered shortest path tree problem},
  volume       = {50},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Semi-supervised dimensionality reduction via sparse locality
preserving projection. <em>APIN</em>, <em>50</em>(4), 1222–1232. (<a
href="https://doi.org/10.1007/s10489-019-01574-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The dimensionality reduction of the unbalanced semi-supervised problem is difficult because there are too few labeled samples. In this paper, we propose a new dimensionality reduction method for the unbalanced semi-supervised problem, called sparse locality preserving projection (SLPP for short). In the past work of solving the semi-supervised dimensionality reduction problems, they either abandon some unlabeled samples or do not utilize the implicit discriminant information of unlabeled samples. While, SLPP learns the optimal projection matrix with the full use of the discriminant information and the geometric structure of the unlabeled samples. Here, we preserve the geometric structure of the rest unlabeled samples and their k-nearest neighbors after increasing the number of labeled samples by label propagation. The optimization problem of SLPP can be easily solved by a generalized eigenvalue problem. Results on various data sets from UCI machine learning repository and two hyperspectral data sets demonstrate that SLPP is superior to other conventional reduction methods.},
  archive      = {J_APIN},
  author       = {Guo, Huijie and Zou, Hui and Tan, Junyan},
  doi          = {10.1007/s10489-019-01574-6},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {4},
  pages        = {1222-1232},
  shortjournal = {Appl. Intell.},
  title        = {Semi-supervised dimensionality reduction via sparse locality preserving projection},
  volume       = {50},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Data mining-based approach for ontology matching problem.
<em>APIN</em>, <em>50</em>(4), 1204–1221. (<a
href="https://doi.org/10.1007/s10489-019-01593-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ontology matching aims at identifying the correspondences between instances and data properties of different ontologies. The use of data mining approach in matching ontology problem is reviewed in this article. We propose DMOM (Data Mining for Ontology Matching based instances) framework to select data properties of instances efficiently. The framework exploits data mining techniques to select the most appropriate features to match ontologies. Moreover, three strategies have been investigated to select the relevant features for the matching process. The first one called exhaustive, explores the enumerate search tree randomly by generating at each iteration a subset of feature attributes, where each node is evaluated by running the matching process on its selected attributes. The second approach called statistical, it uses some statistical values to select the most relevant properties. The third one called FIM (Frequent Itemsets Mining), it explores the correlation between different properties and selects the most frequent properties describing the overall instances of the given ontology. To demonstrate the usefulness of DMOM framework, several experiments have been carried out on OAEI (Ontology Alignment Evaluation Initiative) and DBpedia ontology databases. The results show that the third strategy, FIM, outperforms the two other strategies (Exhaustive, and Statistical). The results also reveal that DMOM outperforms the state-of-the-art ontology matching approaches in terms of execution time and the quality of the matching process.},
  archive      = {J_APIN},
  author       = {Belhadi, Hiba and Akli-Astouati, Karima and Djenouri, Youcef and Lin, Jerry Chun-Wei},
  doi          = {10.1007/s10489-019-01593-3},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {4},
  pages        = {1204-1221},
  shortjournal = {Appl. Intell.},
  title        = {Data mining-based approach for ontology matching problem},
  volume       = {50},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A CTR prediction model based on user interest via attention
mechanism. <em>APIN</em>, <em>50</em>(4), 1192–1203. (<a
href="https://doi.org/10.1007/s10489-019-01571-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, click-through rate (CTR) prediction is a challenge problem in the aspect of online advertising. Some researchers have proposed deep learning-based models that follow a similar embedding and MLP paradigm. However, the corresponding approaches generally ignore the importance of capturing the latent user interest behind user behaviour data. In this paper, we present a novel attentive deep interest-based network model called ADIN. Specifically, we capture the interest sequence in the interest extractor layer, and the auxiliary losses are employed to produce the interest state with the deep supervision. First, we model the dependency between behaviours by using a bidirectional gated recurrent unit (Bi-GRU). Next, we extract the interest evolving process that is related to the target and propose an interest evolving layer. At the same time, attention mechanism is embedded into the sequential structure. Then, the model learns highly non-linear interactions of features based on stack autoencoders. An experiment has been done using four real-world datasets, the proposed model achieves superior performance than the existing state-of-the-art models.},
  archive      = {J_APIN},
  author       = {Li, Hao and Duan, Huichuan and Zheng, Yuanjie and Wang, Qianqian and Wang, Yu},
  doi          = {10.1007/s10489-019-01571-9},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {4},
  pages        = {1192-1203},
  shortjournal = {Appl. Intell.},
  title        = {A CTR prediction model based on user interest via attention mechanism},
  volume       = {50},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Evolutionary dataset optimisation: Learning algorithm
quality through evolution. <em>APIN</em>, <em>50</em>(4), 1172–1191. (<a
href="https://doi.org/10.1007/s10489-019-01592-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper we propose a novel method for learning how algorithms perform. Classically, algorithms are compared on a finite number of existing (or newly simulated) benchmark datasets based on some fixed metrics. The algorithm(s) with the smallest value of this metric are chosen to be the ‘best performing’. We offer a new approach to flip this paradigm. We instead aim to gain a richer picture of the performance of an algorithm by generating artificial data through genetic evolution, the purpose of which is to create populations of datasets for which a particular algorithm performs well on a given metric. These datasets can be studied so as to learn what attributes lead to a particular progression of a given algorithm. Following a detailed description of the algorithm as well as a brief description of an open source implementation, a case study in clustering is presented. This case study demonstrates the performance and nuances of the method which we call Evolutionary Dataset Optimisation. In this study, a number of known properties about preferable datasets for the clustering algorithms known as k-means and DBSCAN are realised in the generated datasets.},
  archive      = {J_APIN},
  author       = {Wilde, Henry and Knight, Vincent and Gillard, Jonathan},
  doi          = {10.1007/s10489-019-01592-4},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {4},
  pages        = {1172-1191},
  shortjournal = {Appl. Intell.},
  title        = {Evolutionary dataset optimisation: Learning algorithm quality through evolution},
  volume       = {50},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Unsupervised discriminative feature representation via
adversarial auto-encoder. <em>APIN</em>, <em>50</em>(4), 1155–1171. (<a
href="https://doi.org/10.1007/s10489-019-01581-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Feature representation is generally applied to reducing the dimensions of high-dimensional data to accelerate the process of data handling and enhance the performance of pattern recognition. However, the dimensionality of data nowadays appears to be a rapidly increasing trend. Existing unsupervised feature representation methods are susceptible to the rapidly increasing dimensionality of data, which may result in learning a meaningless feature that in turn affect their performance in other applications. In this paper, an unsupervised adversarial auto-encoder network is studied. This network is a probability model that combines generative adversarial networks and variational auto-encoder to perform variational inference and aims to generate reconstructed data similar to original data as much as possible. Due to its adversarial training, this model is relatively robust in feature learning compared with other methods. First, the architecture and training strategy of adversarial auto-encoder are presented. We attempt to learn a discriminative feature representation for high-dimensional image data via adversarial auto-encoder and take its advantage into image clustering, which has become a difficult computer vision task recently. Then amounts of comparative experiments are carried out. The comparison contains eight feature representation methods and two recently proposed deep clustering methods performed on eight different publicly available image data sets. Finally, to evaluate their performance, we utilize a K-means clustering on the low-dimensional feature learned from each feature representation algorithm, and select three evaluation metrics including clustering accuracy, adjusted rand index and normalized mutual information, to provide a comparison. Comprehensive experiments prove the usefulness of the learned discriminative feature via adversarial auto-encoder in the tested data sets.},
  archive      = {J_APIN},
  author       = {Guo, Wenzhong and Cai, Jinyu and Wang, Shiping},
  doi          = {10.1007/s10489-019-01581-7},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {4},
  pages        = {1155-1171},
  shortjournal = {Appl. Intell.},
  title        = {Unsupervised discriminative feature representation via adversarial auto-encoder},
  volume       = {50},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A reference points and intuitionistic fuzzy dominance based
particle swarm algorithm for multi/many-objective optimization.
<em>APIN</em>, <em>50</em>(4), 1133–1154. (<a
href="https://doi.org/10.1007/s10489-019-01569-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Intuitionistic Fuzzy Sets is one of the most influential extension and development of Zadeh’s fuzzy set theory. It has strong performance in dealing with uncertain information, while taking into account information on membership degree, non-membership degree and hesitation degree. In this paper, a new loose Pareto dominant relationship named intuitionistic fuzzy dominance is adopted to research multi/many-objective particle swarm optimization problems. Particle swarm optimization (PSO) with double search strategy is employed to update the population to enhance the exploitation and exploration capability of particle in the objective space, especially high-dimensional objective space. In addition, the uniformly distributed reference points are used to balance the convergence and diversity of the algorithm. The proposed algorithm has been compared with four recent multi-objective particle swarm optimization algorithms and four state-of-the-art many-objective evolutionary algorithms on 16 benchmark MOPs with 3, 5,8,10 and 15 objectives, respectively. The simulation results show that the proposed algorithm has better performance on most test problems.},
  archive      = {J_APIN},
  author       = {Yang, Wusi and Chen, Li and Wang, Yi and Zhang, Maosheng},
  doi          = {10.1007/s10489-019-01569-3},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {4},
  pages        = {1133-1154},
  shortjournal = {Appl. Intell.},
  title        = {A reference points and intuitionistic fuzzy dominance based particle swarm algorithm for multi/many-objective optimization},
  volume       = {50},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A hybrid fuzzy filtering - fuzzy thresholding technique for
region of interest detection in noisy images. <em>APIN</em>,
<em>50</em>(4), 1112–1132. (<a
href="https://doi.org/10.1007/s10489-019-01551-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Noise leads to the ambiguity in regions of interest detection by corrupting the pixel information and is a vital problem in image processing domain. A novel hybrid technique based on fuzzy filtering and fuzzy thresholding is proposed here to extract the object regions accurately in presence of Gaussian noises. The proposed method is automated, does not need any parameter tuning as well does not need prior knowledge of the image or noise. An asymmetrical triangular fuzzy filter with median center coupled with a thresholding based on fuzziness minimization technique are implemented for this purpose. The fuzzy thresholding technique helps to classify the pixels with low signal-to-noise ratio (SNR) caused either due to noise or by the application of noise removal process. The proposed technique is applied in benchmark images corrupted by noises and are compared with some of the popular algorithms of object detection. The results indicate that the proposed method has superior performance in terms of peak signal-to-noise ratio (PSNR) and mean square error (MSE) value for images corrupted with Gaussian noises with standard deviation upto 1.5.},
  archive      = {J_APIN},
  author       = {Bandyopadhyay, Sanmoy and Das, Saurabh and Datta, Abhirup},
  doi          = {10.1007/s10489-019-01551-z},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {4},
  pages        = {1112-1132},
  shortjournal = {Appl. Intell.},
  title        = {A hybrid fuzzy filtering - fuzzy thresholding technique for region of interest detection in noisy images},
  volume       = {50},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Driver action recognition using deformable and dilated
faster r-CNN with optimized region proposals. <em>APIN</em>,
<em>50</em>(4), 1100–1111. (<a
href="https://doi.org/10.1007/s10489-019-01603-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Distracted driver action is the main cause of road traffic crashes, which threatens the security of human life and public property. Based on the observation that cues (like the hand holding the cigarette) reveal what the driver is doing, a driver action recognition model is proposed, which is called deformable and dilated Faster R-CNN (DD-RCNN). Our approach utilizes the detection of motion-specific objects to classify driver actions exhibiting great intra-class differences and inter-class similarity. Firstly, deformable and dilated residual block are designed to extract features of action-specific RoIs that are small in size and irregular in shape (such as cigarettes and cell phones). Attention modules are embedded in the modified ResNet to reweight features in channel and spatial dimensions. Then, the region proposal optimization network (RPON) is presented to reduce the number of RoIs entering R-CNN and improves model efficiency. Lastly, the RoI pooling module is replaced with the deformable one, and the simplified R-CNN without regression layer is trained as the final classifier. Experiments show that DD-RCNN demonstrates state-of-the-art results on Kaggle-driving dataset and self-built dataset.},
  archive      = {J_APIN},
  author       = {Lu, Mingqi and Hu, Yaocong and Lu, Xiaobo},
  doi          = {10.1007/s10489-019-01603-4},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {4},
  pages        = {1100-1111},
  shortjournal = {Appl. Intell.},
  title        = {Driver action recognition using deformable and dilated faster R-CNN with optimized region proposals},
  volume       = {50},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Quality based adaptive score fusion approach for multimodal
biometric system. <em>APIN</em>, <em>50</em>(4), 1086–1099. (<a
href="https://doi.org/10.1007/s10489-019-01579-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multimodal Biometric Systems are extensively employed over unimodal counterparts for user authentication in the digital world. However, the application of multimodal systems to security-critical applications is limited mainly due to non-adaptiveness of these systems to the dynamic environment and inability to distinguish between spoofing attack and the noisy input image. In order to address these issues, a multimodal biometric system, which adaptively combines the scores from individual classifiers is proposed. For this, three modalities viz. face, finger, and iris are used to extract individual classifier scores. These classifier scores are adaptively fused considering that concurrent modalities are boosted and discordant modalities are suppressed. The conflicting belief among classifiers is resolved not only to achieve optimum fusion of classifier scores but also to cater dynamic environment. The proposed quality based score fusion also distinguish between spoofing attacks and noisy inputs as well. The performance of the proposed multimodal biometric system is experimentally validated using three chimeric multimodal databases. On an average, the proposed system achieves an accuracy of 99.5%, an EER of 0.5% and also outperforms state-of-the-art methods.},
  archive      = {J_APIN},
  author       = {Gupta, Keshav and Walia, Gurjit Singh and Sharma, Kapil},
  doi          = {10.1007/s10489-019-01579-1},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {4},
  pages        = {1086-1099},
  shortjournal = {Appl. Intell.},
  title        = {Quality based adaptive score fusion approach for multimodal biometric system},
  volume       = {50},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Generating high quality crowd density map based on
perceptual loss. <em>APIN</em>, <em>50</em>(4), 1073–1085. (<a
href="https://doi.org/10.1007/s10489-019-01573-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {High quality crowd density maps preserve a large amount of spatial information of crowd distribution, which provides significant priori information for the field of crowd behavior analysis and anomaly detection. Recent work on crowd density estimation pays more attention to the accuracy of crowd counting, ignoring the quality of crowd density map estimation. Hence, in this paper, we propose an end-to-end crowd density estimation network to generate high quality crowd density map. The original pixel-level Euclidean distance loss function in the Multi-column Convolutional Neural Network (MCNN) is replaced by the perceptual loss network. By optimizing the perceptual loss function that is defined as the differences between high-level semantic features generated by a pre-trained network, high-quality map estimation can be obtained. At the same time the accuracy of crowd counting and the sensitivity to the external environment can be improved. Extensive experiments conducted on challenging datasets validate the proposed method outperforms the state-of-the-art methods in both the crowd counting accuracy and the density estimation quality.},
  archive      = {J_APIN},
  author       = {Fan, Zheyi and Zhu, Yixuan and Song, Yu and Liu, Zhiwen},
  doi          = {10.1007/s10489-019-01573-7},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {4},
  pages        = {1073-1085},
  shortjournal = {Appl. Intell.},
  title        = {Generating high quality crowd density map based on perceptual loss},
  volume       = {50},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Detecting smoky vehicles from traffic surveillance videos
based on dynamic features. <em>APIN</em>, <em>50</em>(4), 1057–1072. (<a
href="https://doi.org/10.1007/s10489-019-01589-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing smoky vehicle detection methods are vulnerable to false alarms because of the continuous interferences from common passed vehicles and the complex characteristics of smoke. This paper presents a video smoky vehicle detection method based on dynamic features. Three groups of features, including Multi-Sequence Integral Projection (MS-IP), Center-Symmetric Local Binary Patterns on Three Orthogonal Planes (CSLBP-TOP) and Histograms of Oriented Optical Flow (HOOF), are proposed or employed to characterize dynamic features of successive Region of Interest (ROIs). More specifically, the MS-IP characterizes the diffusion and distribution information based on multiple-sequence analysis and integral projection. The CSLBP-TOP characterizes the spatiotemporal texture information by (1) combining the strengths of Shift-Invariant Feature Transform (SIFT) and LBP and (2) extending the spatial features to three-dimensional (3D) space based on three orthogonal planes (TOP). The HOOF characterizes the motion information by inducing a very characteristic optical flow profile to distinguish smoky objects and non-smoky objects in successive ROIs based on the fact that the smoke is ejected from vehicle exhaust port and then gradually spreads around. The above three groups of features are complementary, and we fuse them to increase algorithm robustness. Experiment results show that our method achieves better performances than existing methods.},
  archive      = {J_APIN},
  author       = {Tao, Huanjie},
  doi          = {10.1007/s10489-019-01589-z},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {4},
  pages        = {1057-1072},
  shortjournal = {Appl. Intell.},
  title        = {Detecting smoky vehicles from traffic surveillance videos based on dynamic features},
  volume       = {50},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). ADSCNet: Asymmetric depthwise separable convolution for
semantic segmentation in real-time. <em>APIN</em>, <em>50</em>(4),
1045–1056. (<a
href="https://doi.org/10.1007/s10489-019-01587-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Semantic segmentation can be considered as a per-pixel localization and classification problem, which gives a meaningful label to each pixel in an input image. Deep convolutional neural networks have made extremely successful in semantic segmentation in recent years. However, some challenges still exist. The first challenge task is that most current networks are complex and it is hard to deploy these models on mobile devices because of the limitation of computational cost and memory. Getting more contextual information from downsampled feature maps is another challenging task. To this end, we propose an asymmetric depthwise separable convolution network (ADSCNet) which is a lightweight neural network for real-time semantic segmentation. To facilitating information propagation, Dense Dilated Convolution Connections (DDCC), which connects a set of dilated convolutional layers in a dense way, is introduced in the network. Pooling operation is inserted before ADSCNet unit to cover more contextual information in prediction. Extensive experimental results validate the superior performance of our proposed method compared with other network architectures. Our approach achieves mean intersection over union (mIOU) of 67.5% on Cityscapes dataset at 76.9 frames per second.},
  archive      = {J_APIN},
  author       = {Wang, Jiawei and Xiong, Hongyun and Wang, Haibo and Nian, Xiaohong},
  doi          = {10.1007/s10489-019-01587-1},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {4},
  pages        = {1045-1056},
  shortjournal = {Appl. Intell.},
  title        = {ADSCNet: Asymmetric depthwise separable convolution for semantic segmentation in real-time},
  volume       = {50},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Automatic evolution of bi-clusters from microarray data
using self-organized multi-objective evolutionary algorithm.
<em>APIN</em>, <em>50</em>(4), 1027–1044. (<a
href="https://doi.org/10.1007/s10489-019-01554-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the current paper, a novel approach is proposed for bi-clustering of gene expression data using the fusion of differential evolution framework and self-organizing map (SOM), named as BiClustSMEA. Variable number of gene and condition cluster centers are encoded in different solutions of the population to determine the number of bi-clusters from a dataset in an automated way. The concept of SOM is utilized in designing new genetic operators for both gene and condition clusters to reach to the optimal solution in a faster way. In order to measure the goodness of a bi-clustering solution, three bi-cluster quality measures, mean squared error, row variance, and bi-cluster size, are optimized simultaneously using differential evolution as the underlying optimization strategy. The concept of polynomial mutation is incorporated in our framework to generate highly diverse solutions which in turn helps in faster convergence. The proposed approach is applied on two real-life microarray gene expression datasets and results are compared with various state-of-the-art techniques. Results obtained clearly illustrate that our approach extracts high-quality bi-clusters as compared to other methods and also it converges much faster than other competitors. Further, the obtained results are validated using statistical significance test and biological significance test.},
  archive      = {J_APIN},
  author       = {Saini, Naveen and Saha, Sriparna and Soni, Chirag and Bhattacharyya, Pushpak},
  doi          = {10.1007/s10489-019-01554-w},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {4},
  pages        = {1027-1044},
  shortjournal = {Appl. Intell.},
  title        = {Automatic evolution of bi-clusters from microarray data using self-organized multi-objective evolutionary algorithm},
  volume       = {50},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A novel hybrid sine cosine algorithm for global optimization
and its application to train multilayer perceptrons. <em>APIN</em>,
<em>50</em>(4), 993–1026. (<a
href="https://doi.org/10.1007/s10489-019-01570-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Sine Cosine Algorithm (SCA) is a recently developed efficient metaheuristic algorithm to find the solution of global optimization problems. However, in some circumstances, this algorithm suffers the problem of low exploitation, skipping of true solutions and insufficient balance between exploration and exploitation. Therefore, the present paper aims to alleviate these issues from SCA by proposing an improved variant of SCA called HSCA. The HSCA modifies the search mechanism of classical SCA by including the leading guidance and hybridizing with simulated quenching algorithm. The proposed HSCA is tested on classical benchmark set, standard and complex benchmarks sets IEEE CEC 2014 and CEC 2017 and four engineering optimization problems. In addition to these problems, the HSCA is also used to train multilayer perceptrons as a real-life application. The experimental results and analysis on benchmark problems and real-life application problems demonstrate the superiority of the HSCA as compared to other comparative optimization algorithms.},
  archive      = {J_APIN},
  author       = {Gupta, Shubham and Deep, Kusum},
  doi          = {10.1007/s10489-019-01570-w},
  journal      = {Applied Intelligence},
  month        = {4},
  number       = {4},
  pages        = {993-1026},
  shortjournal = {Appl. Intell.},
  title        = {A novel hybrid sine cosine algorithm for global optimization and its application to train multilayer perceptrons},
  volume       = {50},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A new modeling and inference approach for the belief rule
base with attribute reliability. <em>APIN</em>, <em>50</em>(3), 976–992.
(<a href="https://doi.org/10.1007/s10489-019-01586-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A belief rule-based (BRB) model with attribute reliability (BRB-r) has been developed recently, where the systematic uncertainty is regarded as attribute reliability by extending the traditional BRB model. The BRB-r model provides a framework to deal with the systematic uncertainty, but the drawbacks in modeling and inference reduces the accuracy of it. This paper proposed a new modeling and inference approach to improve the effectiveness of the BRB-r. This approach is constituted by two parts: data processing and BRB inference. In the data processing, the attribute reliability is calculated based on the auto regressive model, while the parameters of BRB-r are optimized using the differential evolution algorithm. In the BRB inference, a new attribute reliability fusion algorithm is proposed, which can effectively integrate attribute reliability into the BRB model and ensure the rationality in different situations. A benchmark case about pipeline leak detection and a practical case about condition monitoring are studied to demonstrate the rationality and feasibility of the proposed approach to the BRB-r model.},
  archive      = {J_APIN},
  author       = {You, Yaqian and Sun, Jianbin and Jiang, Jiang and Lu, Shuai},
  doi          = {10.1007/s10489-019-01586-2},
  journal      = {Applied Intelligence},
  month        = {3},
  number       = {3},
  pages        = {976-992},
  shortjournal = {Appl. Intell.},
  title        = {A new modeling and inference approach for the belief rule base with attribute reliability},
  volume       = {50},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Enhanced harmony search algorithm with circular region
perturbation for global optimization problems. <em>APIN</em>,
<em>50</em>(3), 951–975. (<a
href="https://doi.org/10.1007/s10489-019-01558-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To improve the searching effectiveness of the harmony search (HS) algorithm, an enhanced harmony search algorithm with circular region perturbation (EHS_CRP) is proposed in this paper. In the EHS_CRP algorithm, a global and local dimension selection strategy is designed to accelerate the search speed of the algorithm. A selection learning operator based on the global and local mean level is proposed to improve the balance between exploration and exploitation. Circular region perturbation is employed to avoid the algorithm stagnation and get a better exploration region. To assess performance, the proposed algorithm is compared with 10 state-of-the-art swarm intelligent approaches in a large set of global optimization problems. The simulation results confirm that EHS_CRP has a significant advantage in terms of accuracy, convergence speed, stability and robustness. Moreover, EHS_CRP performs better than other tested methods in engineering design optimization problems. Thus, the EHS_CRP algorithm is a viable and reliable alternative for some difficult and multidimensional real-world problems.},
  archive      = {J_APIN},
  author       = {Wu, Wenqiang and Ouyang, Haibin and Mohamed, Ali Wagdy and Zhang, Chunliang and Li, Steven},
  doi          = {10.1007/s10489-019-01558-6},
  journal      = {Applied Intelligence},
  month        = {3},
  number       = {3},
  pages        = {951-975},
  shortjournal = {Appl. Intell.},
  title        = {Enhanced harmony search algorithm with circular region perturbation for global optimization problems},
  volume       = {50},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Shared subspace least squares multi-label linear
discriminant analysis. <em>APIN</em>, <em>50</em>(3), 939–950. (<a
href="https://doi.org/10.1007/s10489-019-01559-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-label linear discriminant analysis (MLDA) has been explored for multi-label dimension reduction. However, MLDA involves dense matrices eigen-decomposition which is known to be computationally expensive for large-scale problems. In this paper, we show that the formulation of MLDA can be equivalently casted as a least squares problem so as to significantly reduce the computation burden and scale to the data collections with higher dimension. Further, it is also found that appealing regularization techniques can be incorporated into the least-squares model to boost generalization accuracy. Experimental results on several popular multi-label benchmarks not only verify the established equivalence relationship, but also demonstrate the effectiveness and efficiency of our proposed algorithms.},
  archive      = {J_APIN},
  author       = {Yu, Hongbin and Zhang, Tao and Jia, Wenjing},
  doi          = {10.1007/s10489-019-01559-5},
  journal      = {Applied Intelligence},
  month        = {3},
  number       = {3},
  pages        = {939-950},
  shortjournal = {Appl. Intell.},
  title        = {Shared subspace least squares multi-label linear discriminant analysis},
  volume       = {50},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Weighted multi-information constrained matrix factorization
for personalized travel location recommendation based on geo-tagged
photos. <em>APIN</em>, <em>50</em>(3), 924–938. (<a
href="https://doi.org/10.1007/s10489-019-01566-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Given travel history, travel location recommendation can automatically suggest users where to visit. Huge efforts have been devoted to introducing different additional information (e.g., sequential, textual, geographical, and visual information) for enhancing recommendation performance. However, existing methods only consider limited additional information and treat different information equally. In this paper, we present Weighted Multi-Information Constrained Matrix Factorization (WIND-MF) for personalized travel location recommendation based on geo-tagged photos. On one hand, photos (visual information), users’ visit sequences (sequential information), and textual tags (textual information) are leveraged to comprehensively profile users and travel locations. On the other hand, visual, sequential, and textual similarities as well as geographical distance based co-visit probabilities are assigned with different weights to constrain the factorization of the original user-travel location matrix. We experimented on a dataset of six cities in China, and the experiment results verify the superiority of the proposed method. The code and dataset is available at https://github.com/revaludo/WIND-MF.},
  archive      = {J_APIN},
  author       = {Lyu, Dandan and Chen, Ling and Xu, Zhenxing and Yu, Shanshan},
  doi          = {10.1007/s10489-019-01566-6},
  journal      = {Applied Intelligence},
  month        = {3},
  number       = {3},
  pages        = {924-938},
  shortjournal = {Appl. Intell.},
  title        = {Weighted multi-information constrained matrix factorization for personalized travel location recommendation based on geo-tagged photos},
  volume       = {50},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). An improved recurrent neural networks for 3d object
reconstruction. <em>APIN</em>, <em>50</em>(3), 905–923. (<a
href="https://doi.org/10.1007/s10489-019-01523-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {3D-R2N2 and other advanced 3D reconstruction neural networks have achieved impressive results, however most of them still suffer from training difficulties and detail losing, due to their weak feature extraction capability and improper loss function. This paper aims to overcome these shortcomings and defects by building a brand new model based on 3D-R2N2. The new model adopts densely connected structure as encoder, and utilizes Chamfer Distance as loss function. The aim is to enhance the learning ability of the network for complex data, meanwhile, make the focus of the whole network rest on the reconstruction of detail structures. In addition, we also made an improved decoder by building two parallel predictor branches to make better use of the feature information and boost the network’s performance on reconstruction task. Through extensive tests, the results show that our proposed model called 3D-R2N2-V2 is slightly slower than 3D-R2N2 in predicting speed, but it can be 20% to 30% faster than 3D-R2N2 in training speed and obtain 15% and 10% better voxel IoU results on both single- and multi-view reconstruction tasks, respectively. Compared with other recent state-of-the-art methods like OGN and DRC, the reconstruction effect of our approach is also competitive.},
  archive      = {J_APIN},
  author       = {Ma, Tingsong and Kuang, Ping and Tian, Wenhong},
  doi          = {10.1007/s10489-019-01523-3},
  journal      = {Applied Intelligence},
  month        = {3},
  number       = {3},
  pages        = {905-923},
  shortjournal = {Appl. Intell.},
  title        = {An improved recurrent neural networks for 3d object reconstruction},
  volume       = {50},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Evolutionary echo state network for long-term time series
prediction: On the edge of chaos. <em>APIN</em>, <em>50</em>(3),
893–904. (<a href="https://doi.org/10.1007/s10489-019-01546-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Quantitative analysis of neural networks is a critical issue to improve their performance. In this paper, we investigate a long-term time series prediction based on the echo state network operating at the edge of chaos. We also assess the eigenfunction of echo state networks and its criticality by the Hermite polynomials. A Hermite polynomial-based activation function design with fast convergence is proposed and the relation between long-term time dependence and edge-of-chaos criticality is given. A new particle swarm optimization-gravitational search algorithm is put forward to improve the parameters estimation that helps attain on the edge of chaos. The method was verified using a chaotic Lorenz system and a real health index data set. The experimental results indicate that evolution makes the reservoir great potential to run on the edge of chaos with rich expression.},
  archive      = {J_APIN},
  author       = {Zhang, Gege and Zhang, Chao and Zhang, WeiDong},
  doi          = {10.1007/s10489-019-01546-w},
  journal      = {Applied Intelligence},
  month        = {3},
  number       = {3},
  pages        = {893-904},
  shortjournal = {Appl. Intell.},
  title        = {Evolutionary echo state network for long-term time series prediction: On the edge of chaos},
  volume       = {50},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Mining clique frequent approximate subgraphs from
multi-graph collections. <em>APIN</em>, <em>50</em>(3), 878–892. (<a
href="https://doi.org/10.1007/s10489-019-01564-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, some algorithms have been reported for mining multi-graph Frequent Approximate Subgraphs (FASs). This kind of algorithm has a large applicability in social network analysis, image classification tasks and clustering, among others. However, all of them mine a large number of patterns. For this reason, some algorithms for mining a subset of FASs have been proposed. In this paper, we propose an efficient algorithm for mining a subset of FASs on multi-graph collections. Our proposed algorithm becomes an alternative for reducing the number of mined FASs by computing the clique FASs. It is important to highlight that, to the best of our knowledge; our proposal is the first algorithm for mining clique FASs on multi-graph collections. Our proposal is compared against other reported solutions and evaluated over several synthetic and real-world multi-graphs. In addition, we show the usefulness of the patterns mined by our proposal for image classification.},
  archive      = {J_APIN},
  author       = {Acosta-Mendoza, Niusvel and Carrasco-Ochoa, Jesús Ariel and Martínez-Trinidad, José Francisco and Gago-Alonso, Andrés and Medina-Pagola, José Eladio},
  doi          = {10.1007/s10489-019-01564-8},
  journal      = {Applied Intelligence},
  month        = {3},
  number       = {3},
  pages        = {878-892},
  shortjournal = {Appl. Intell.},
  title        = {Mining clique frequent approximate subgraphs from multi-graph collections},
  volume       = {50},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Data-driven missing data imputation in cluster monitoring
system based on deep neural network. <em>APIN</em>, <em>50</em>(3),
860–877. (<a href="https://doi.org/10.1007/s10489-019-01560-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to cluster instability, not in the cluster monitoring system. This paper focuses on the missing data imputation processing for the cluster monitoring application and proposes a new hybrid multiple imputation framework. This new imputation approach is different from the conventional multiple imputation technologies in the fact that it attempts to impute the missing data for an arbitrary missing pattern with a model-based and data-driven combination architecture. Essentially, the deep neural network, as the data model, extracts deep features from the data and deep features are further calculated then by a regression or data-driven strategies and used to create the estimation of missing data with the arbitrary missing pattern. This paper gives evidence that if we can train a deep neural network to construct the deep features of the data, imputation based on deep features is better than that directly on the original data. In the experiments, we compare the proposed method with other conventional multiple imputation approaches for varying missing data patterns, missing ratios, and different datasets including real cluster data. The result illustrates that when data encounters larger missing ratio and various missing patterns, the proposed algorithm has the ability to achieve more accurate and stable imputation performance.},
  archive      = {J_APIN},
  author       = {Lin, Jie and Li, NianHua and Alam, Md Ashraful and Ma, Yuqing},
  doi          = {10.1007/s10489-019-01560-y},
  journal      = {Applied Intelligence},
  month        = {3},
  number       = {3},
  pages        = {860-877},
  shortjournal = {Appl. Intell.},
  title        = {Data-driven missing data imputation in cluster monitoring system based on deep neural network},
  volume       = {50},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A novel parallel accelerated CRPF algorithm. <em>APIN</em>,
<em>50</em>(3), 849–859. (<a
href="https://doi.org/10.1007/s10489-019-01534-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Particle filtering is one of the most important algorithms for solving state estimation of nonlinear systems and has been widely studied in many fields. However, due to the unknown complex noise in the actual system, its estimation performance is degraded. Moreover, when the number of particles increase, the real-time performance of the algorithm is poor. For these two problems above, this paper proposed a parallel acceleration CRPF (cost-reference particle filter) algorithm based on CUDA (Compute Unified Device Architecture). CRPF does not need known noise statistics in nonlinear system state estimation, which can reduce the influence of unknown noise on state estimation accuracy. Combined with GPU’s (Graphics Processing Unit) multi-thread parallel computing capability, CRPF parallel acceleration can be realized. Since the data association can’t be parallel resampled, all the particles are evenly distributed to multiple blocks, and resampling process can be parallelized by block parallel computing, so as to improve the speed of the algorithm. At the same time, in order to reduce the global particle performance degradation caused by block resampling, the particles with low probability mass in each block are optimized by using a portion of global high-quality particles. Through two sets of simulation experiments, it is proved that the proposed method has improved in estimation accuracy and the real-time performance has been improved significantly, which can provide a new idea for the practical application of nonlinear filtering method.},
  archive      = {J_APIN},
  author       = {Wang, Jinhua and Cao, Jie and Li, Wei and Yu, Ping and Huang, Kaijie},
  doi          = {10.1007/s10489-019-01534-0},
  journal      = {Applied Intelligence},
  month        = {3},
  number       = {3},
  pages        = {849-859},
  shortjournal = {Appl. Intell.},
  title        = {A novel parallel accelerated CRPF algorithm},
  volume       = {50},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). CLR-based deep convolutional spiking neural network with
validation based stopping for time series classification. <em>APIN</em>,
<em>50</em>(3), 830–848. (<a
href="https://doi.org/10.1007/s10489-019-01552-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Huge amount of time series data over several domains such as engineering, biomedical and finance, demands the development of efficient methods for the problem of time series classification. The classification of univariate and multivariate time series together using a single architecture is a very difficult task. In this work, a bio-inspired convolutional spiking neural network (CSNN) is proposed for both univariate and multivariate time series. For this, first we develop a simple transformation to convert raw time series sequences into matrices. The CSNN is a three staged framework which include convolutional feature extraction, spike encoding using soft leaky integrate and fire (Soft-LIf) and classification. As spikes generated are differentiable, thus the learning algorithm for CSNN uses error-backpropagation with cyclical learning rates (CLR) and RMSprop optimizer. Additionally, validation based stopping rules are employed to overcome the overfitting which also provides a set of parameters associated with low validation set loss. Thereafter, to demonstrate the accuracy and robustness of proposed CSNN model, we have used University of California (UCR) univariate as well as University of East Anglia (UEA) multivariate datasets to perform the experiments. Moreover, we conduct comparative empirical performance evaluation with benchmark methods and also with recent deep networks proposed for time series classification. Our results reveal that proposed CSNN advances the baseline methods by achieving higher performance accuracy for both univariate and multivariate datasets. It is shown that the CLR with RMSprop optimizer is able to achieve faster convergence, however CLR and adaptive rates are considered competitive to each other. In addition, we also address the optimal model selection and study the effects of different factors on the performance of proposed CSNN.},
  archive      = {J_APIN},
  author       = {Gautam, Anjali and Singh, Vrijendra},
  doi          = {10.1007/s10489-019-01552-y},
  journal      = {Applied Intelligence},
  month        = {3},
  number       = {3},
  pages        = {830-848},
  shortjournal = {Appl. Intell.},
  title        = {CLR-based deep convolutional spiking neural network with validation based stopping for time series classification},
  volume       = {50},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Transfer learning based 3D fuzzy multivariable control for
an RTP system. <em>APIN</em>, <em>50</em>(3), 812–829. (<a
href="https://doi.org/10.1007/s10489-019-01557-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Rapid thermal processing (RTP) is an important process in the fabrication of semiconductor devices. It is difficult to achieve temperature uniformity control of the wafer in RTP since the system is a highly nonlinear process with strong spatial distribution. In this study, a transfer learning-based three-dimensional (3D) fuzzy multivariable control scheme is proposed for the temperature uniformity control of an RTP system. In difference to the traditional expert-knowledge based design, a two-level framework of transfer learning methodology is constructed to design the 3D fuzzy multivariable controller (3D FMC) with the help of a multi-output support vector regression (M-SVR). The 3D FMC defines a qualitative spatial fuzzy structure that will be transferred to the M-SVR. On the other hand, the structure parameters of the M-SVR will be learned from data and transferred to design quantitative parameters of the 3D FMC. Under the framework of transfer learning, the control laws (e.g. human control experience) hidden in spatio-temporal data can be extracted and formulated back into multi-output 3D fuzzy rules. The proposed method provides an effective integration of the spatial fuzzy inference and the transfer learning for 3D FLC design. The newly developed method is applied to the temperature uniformity control of a rapid thermal chemical vapor deposition (RTCVD) system at the set temperature 1000K, and the maximum non-uniformity along the wafer radius is close to 1K.},
  archive      = {J_APIN},
  author       = {Zhang, Xian-Xia and Li, Han-Xiong and Cheng, Chong and Ma, Shiwei},
  doi          = {10.1007/s10489-019-01557-7},
  journal      = {Applied Intelligence},
  month        = {3},
  number       = {3},
  pages        = {812-829},
  shortjournal = {Appl. Intell.},
  title        = {Transfer learning based 3D fuzzy multivariable control for an RTP system},
  volume       = {50},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Multi-channel biomimetic visual transformation for object
feature extraction and recognition of complex scenes. <em>APIN</em>,
<em>50</em>(3), 792–811. (<a
href="https://doi.org/10.1007/s10489-019-01550-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Object recognition occurs accurately with human visual neural mechanism despite in different complex background interference. For computer system, it is still a challenging work of object recognition and classification. Recently, many methods for object recognition based on human visual perception mechanism are presented. However, most methods cannot achieve a better recognition accuracy when object images are corrupted by some background interferences. Therefore, it is necessary to propose a method for object recognition of complex scene. Inspired by biomimetic visual mechanism and visual memory, a multi-channel biomimetic visual transformation (MCBVT) is proposed in this paper. MCBVT involves three channels. Firstly, some algorithms including orientation edge detection (OED), local spatial frequency detection (LSFD) and weighted centroid coordinate calculation are adopted for two stage’s visual memory maps creations during the first channel, where some visual memory points are stored in memory map. Secondly, an object hitting map (OHM) is built in the second channel and the OHM is an edge image without background interference. After that, the first stage’s visual memory hitting map is obtained through execute back-tracking second stage’s visual memory map. Furthermore, an OHM is constructed through back-tracking with common memory points in first stage’s visual memory map and first stage’s visual memory hitting map. Thirdly, the OED and LSFD algorithms are conducted to extract a feature map of OHM in the third channel. Consequently, the final feature map is reshaped into a feature vector, which is used for object recognition. Additionally, several image database experiments are implemented, the recognition accuracy for alphanumeric, MPEG-7 and GTSRB database are 93.33%, 91.33 and 90% respectively. Moreover, same object images in different backgrounds share with highly similar feature maps. On the contrary, different object images with complex backgrounds through MCBVT show different feature maps. The experiments reveal a better selectivity and invariance of MCBVT features. In summary, the proposed MCBVT provides a new framework of feature extraction. Background interference of object image is eliminated through the first and second channel, which is a new method for background noise reduction. Meanwhile, the results show that the proposed MCBVT method is better than other feature extraction methods. The contributions of this paper is significant in computational intelligence for the further work.},
  archive      = {J_APIN},
  author       = {Yu, Lingli and Jin, Mingyue and Zhou, Kaijun},
  doi          = {10.1007/s10489-019-01550-0},
  journal      = {Applied Intelligence},
  month        = {3},
  number       = {3},
  pages        = {792-811},
  shortjournal = {Appl. Intell.},
  title        = {Multi-channel biomimetic visual transformation for object feature extraction and recognition of complex scenes},
  volume       = {50},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Cluster-based kriging approximation algorithms for
complexity reduction. <em>APIN</em>, <em>50</em>(3), 778–791. (<a
href="https://doi.org/10.1007/s10489-019-01549-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Kriging or Gaussian Process Regression is applied in many fields as a non-linear regression model as well as a surrogate model in the field of evolutionary computation. However, the computational and space complexity of Kriging, that is cubic and quadratic in the number of data points respectively, becomes a major bottleneck with more and more data available nowadays. In this paper, we propose a general methodology for the complexity reduction, called cluster Kriging, where the whole data set is partitioned into smaller clusters and multiple Kriging models are built on top of them. In addition, four Kriging approximation algorithms are proposed as candidate algorithms within the new framework. Each of these algorithms can be applied to much larger data sets while maintaining the advantages and power of Kriging. The proposed algorithms are explained in detail and compared empirically against a broad set of existing state-of-the-art Kriging approximation methods on a well-defined testing framework. According to the empirical study, the proposed algorithms consistently outperform the existing algorithms. Moreover, some practical suggestions are provided for using the proposed algorithms.},
  archive      = {J_APIN},
  author       = {van Stein, Bas and Wang, Hao and Kowalczyk, Wojtek and Emmerich, Michael and Bäck, Thomas},
  doi          = {10.1007/s10489-019-01549-7},
  journal      = {Applied Intelligence},
  month        = {3},
  number       = {3},
  pages        = {778-791},
  shortjournal = {Appl. Intell.},
  title        = {Cluster-based kriging approximation algorithms for complexity reduction},
  volume       = {50},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Optimizing multicast routing tree on application layer via
an encoding-free non-dominated sorting genetic algorithm. <em>APIN</em>,
<em>50</em>(3), 759–777. (<a
href="https://doi.org/10.1007/s10489-019-01547-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The task of Application Layer Multicast (ALM) routing is to establish a proper routing tree on application layer network, which transmits data packages from a source to multiple destinations. ALM routing tree is instable due to the end-hosts’ departure or failure. In view of this, we formulated the ALM routing as a multi-objective optimization problem, where the aim is to optimize the ALM routing tree’s topological structure for minimizing its transmission delay and instability simultaneously. A novel encoding-free non-dominated sorting genetic algorithm is proposed for solving our formulated optimization problem. For achieving encoding-free, genotypes are directly represented as tree-like phenotypes in our proposed algorithm. Accordingly, the genetic operators acting on genotypes, like crossover and mutation, need to be redesigned to adapt the tree-like genotypes. The worst-case time complexity of the proposed algorithm is theoretically analyzed and exhaustive simulation results manifest that our proposed algorithm is capable of obtaining high-quality Pareto front. Besides, several interesting issues, such as how to select the final solution out of the obtained Pareto front and the reason why GP is not used, are also discussed.},
  archive      = {J_APIN},
  author       = {Liu, Qing and Tang, Rongjun and Ren, Haipeng and Pei, Yan},
  doi          = {10.1007/s10489-019-01547-9},
  journal      = {Applied Intelligence},
  month        = {3},
  number       = {3},
  pages        = {759-777},
  shortjournal = {Appl. Intell.},
  title        = {Optimizing multicast routing tree on application layer via an encoding-free non-dominated sorting genetic algorithm},
  volume       = {50},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Exploiting skew-adaptive delimitation mechanism for learning
expressive classification rules. <em>APIN</em>, <em>50</em>(3), 746–758.
(<a href="https://doi.org/10.1007/s10489-019-01533-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The expressivity of machine learning algorithms is considered to be critical in intelligent data analysis tasks for practical application. As an alternative set of classification rule learning algorithms to conventional decision tree, Prism family of algorithms induce modular rules concisely, thus exhibiting good expressiveness for human users. However, existing Prism rule induction techniques are limited by the assumption of Gaussian distribution for quantitative attributes, and may not be available for real life data analyzing, in which skewness is commonly observed. For this reason, we investigate a skew-adaptive mechanism for rule term boundary delimitation in Prism inductive learning. The propose algorithm, called P2-Prism, could learn expressive classification rules directly from quantitative data beyond Gaussian distribution. By employing statistical inference characteristics of Poisson process, our mechanism provides a significant contribution to classification rule inductive learning with adaption of skewed data distribution. The experimental evaluation of our algorithm demonstrates its skew-adaptive superiority on benchmark datasets, comparing with state-of-the-art algorithms. Furthermore, it is shown that P2-Prism is a robust classifier in the presence of various levels of noise, which further reveals its adaptability to the skewness of data distribution.},
  archive      = {J_APIN},
  author       = {Hao, Zhi-yong and Yang, Chen and Liu, Lei and Kustudic, Mijat and Niu, Ben},
  doi          = {10.1007/s10489-019-01533-1},
  journal      = {Applied Intelligence},
  month        = {3},
  number       = {3},
  pages        = {746-758},
  shortjournal = {Appl. Intell.},
  title        = {Exploiting skew-adaptive delimitation mechanism for learning expressive classification rules},
  volume       = {50},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). An effective distance based feature selection approach for
imbalanced data. <em>APIN</em>, <em>50</em>(3), 717–745. (<a
href="https://doi.org/10.1007/s10489-019-01543-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Class imbalance is one of the critical areas in classification. The challenges become more severe when the data set has a large number of features. Traditional classifiers generally favour the majority class because of skewed class distributions. In recent years, feature selection is being used to select the appropriate features for better classification of minority class. However, these studies are limited to imbalance that arise between the classes. In addition to between class imbalance, within class imbalance, along with large number of features, adds additional complexity and results in poor performance of the classifier. In the current study, we propose an effective distance based feature selection method (ED-Relief) that uses a sophisticated distance measure, in order to tackle simultaneous occurrence of between and within class imbalance. This method has been tested on a variety of simulated experiments and real life data sets and the results are compared with the traditional Relief method and some of the well known recent distance based feature selection methods. The results clearly show the superiority of the proposed effective distance based feature selection method.},
  archive      = {J_APIN},
  author       = {Shahee, Shaukat Ali and Ananthakumar, Usha},
  doi          = {10.1007/s10489-019-01543-z},
  journal      = {Applied Intelligence},
  month        = {3},
  number       = {3},
  pages        = {717-745},
  shortjournal = {Appl. Intell.},
  title        = {An effective distance based feature selection approach for imbalanced data},
  volume       = {50},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Triangular coil pattern of local radius of gyration face for
heterogeneous face recognition. <em>APIN</em>, <em>50</em>(3), 698–716.
(<a href="https://doi.org/10.1007/s10489-019-01545-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper puts forward a novel methodology for Heterogeneous Face Recognition (HFR), where we present a new-fangled image representation technique called the Local Radius of Gyration Face (LRGF), which has been theoretically proved to be invariant to changes in illumination, rotation and noise. Finally, a novel Local Triangular Coil Binary Pattern (LTCBP) is presented so as to apprehend the local variations of the LRGF attributes, and the method has been entitled as the Triangular Coil Pattern of Local Radius of Gyration Face (TCPLRGF). The proposed algorithm has been tested on a number of challenging databases to study the precision of the TCPLRGF method under varying condition of illumination, rotation, noise and also the recognition accuracy of sketch-photo and NIR-VIS image. The Rank-1 recognition accuracy of 98.27% on CMU-PIE Database, 98.09% on Extended Yale B Database, 96.35% on AR Face Database, 100% on CUHK Face Sketch (CUFS) Database, 89.01% on LFW Database and 98.74% on the CASIA-HFB NIR-VIS Database exhibits the supremacy of the proposed strategy in Heterogeneous Face Recognition (HFR) under various conditions, compared to other recent state-of-the-art methods. For reckoning the similarity measure between images, a hybridized approach amalgamating the Jaccard Similarity method and the standardized L1 norm approach has been taken into account.},
  archive      = {J_APIN},
  author       = {Kar, Arindam and Neogi, Pinaki Prasad Guha},
  doi          = {10.1007/s10489-019-01545-x},
  journal      = {Applied Intelligence},
  month        = {3},
  number       = {3},
  pages        = {698-716},
  shortjournal = {Appl. Intell.},
  title        = {Triangular coil pattern of local radius of gyration face for heterogeneous face recognition},
  volume       = {50},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Energy-based structural least squares MBSVM for
classification. <em>APIN</em>, <em>50</em>(3), 681–697. (<a
href="https://doi.org/10.1007/s10489-019-01536-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multiple birth support vector machine (MBSVM) is an extension of twin support vector machine on multi-class classification problem. In MBSVM, the size of each QP problem is restricted by the number of patterns in one of the K classes, so the computational complexity of MBSVM is much lower and the training speed of it is faster than the existing multi-class SVM. However, MBSVM neglects the structural information of data which may contain some significant prior knowledge for training classifiers. In this paper, we first present an improved version of structural least square twin support vector machine (S-LSTWSVM), called energy-based structural least square twin support vector machine (ES-LSTWSVM), which converts the constraints of the S-LSTWSVM into an energy-based model by introducing an energy for each hyperplane. Then we use the strategy of “rest-versus-one” in MBSVM to extend ES-LSTWSVM into the multi-class classification, called energy-based structural least squares MBSVM (ESLS-MBSVM). In order to prove the validity of ESLS-MBSVM, the experiment has been performed on UCI datasets. The experimental results show that our ESLS-MBSVM is effective and has good classification performance. In order to better illustrate the experimental results, we use Friedman test and ROC analysis for statistical comparisons.},
  archive      = {J_APIN},
  author       = {Shi, Songhui and Ding, Shifei and Zhang, Zichen and Jia, Weikuan},
  doi          = {10.1007/s10489-019-01536-y},
  journal      = {Applied Intelligence},
  month        = {3},
  number       = {3},
  pages        = {681-697},
  shortjournal = {Appl. Intell.},
  title        = {Energy-based structural least squares MBSVM for classification},
  volume       = {50},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Semantic and syntactic analysis in learning representation
based on a sentiment analysis model. <em>APIN</em>, <em>50</em>(3),
663–680. (<a href="https://doi.org/10.1007/s10489-019-01540-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The rapid development of e-commerce gives researchers confidence that customers will be willing to share more and more online data, which in turn, would allow for improved mining algorithms. Many companies also foresee vast profits in mining data from online interaction, behavior, and activity. Opinion mining, also known as sentiment analysis, means automatically detecting and understanding personal expressions about a product or service from customer textual reviews. Recently, aspect-based sentiment analysis has become widely interesting to researchers, particularly with respect to embedded words. Algorithms such as word2vec and GloVe perform well when it comes to capturing analogies and toward lexical semantics in general. However, more complex algorithms are needed to address this issue more precisely, using larger corpora and special kinds of data. This paper introduces a knowledge representation approach that centers on aspect rating and weighting. The study focuses on how to understand the nature of sentimental representation using a multilayer architecture. We present a model that uses a mixture of semantic and syntactic components to capture both semantic and sentimental information. This model shares its probability foundation with the words recognized by word2vec and builds on our prior work concerning opinion-aspect relation analysis. This new algorithm is designed specifically, however, to discover sentiment-enriched embedding rather than word similarities. Experiments were performed using a review dataset from the electronic domain. Results show that the model achieved both appropriate levels of detail and rich representation capabilities.},
  archive      = {J_APIN},
  author       = {Vo, Anh-Dung and Nguyen, Quang-Phuoc and Ock, Cheol-Young},
  doi          = {10.1007/s10489-019-01540-2},
  journal      = {Applied Intelligence},
  month        = {3},
  number       = {3},
  pages        = {663-680},
  shortjournal = {Appl. Intell.},
  title        = {Semantic and syntactic analysis in learning representation based on a sentiment analysis model},
  volume       = {50},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A fast and accurate explicit kernel map. <em>APIN</em>,
<em>50</em>(3), 647–662. (<a
href="https://doi.org/10.1007/s10489-019-01538-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Kernel functions are powerful techniques that have been used successfully in many machine learning algorithms. Explicit kernel maps have emerged as an alternative to standard kernel functions in order to overcome the latter’s scalability issues. An explicit kernel map such as Random Fourier Features (RFF) is a popular method for approximating shift invariant kernels. However, it requires large run time in order to achieve good accuracy. Faster and more accurate variants of it have also been proposed recently. All these methods are still approximations to a shift invariant kernel. Instead of an approximation, we propose a fast, exact and explicit kernel map called Explicit Cosine Map (ECM). The advantage of this exact map is manifested in the form of performance improvements in kernel based algorithms. Furthermore, its explicit nature enables it to be used in streaming applications. Another explicit kernel map called Euler kernel map is also proposed. The effectiveness of both kernel maps is evaluated in the application of streaming Anomaly Detection (AD). The AD results indicate that ECM based algorithm achieves better AD accuracy than previous algorithms, while being faster.},
  archive      = {J_APIN},
  author       = {Francis, Deena P. and Raimond, Kumudha},
  doi          = {10.1007/s10489-019-01538-w},
  journal      = {Applied Intelligence},
  month        = {3},
  number       = {3},
  pages        = {647-662},
  shortjournal = {Appl. Intell.},
  title        = {A fast and accurate explicit kernel map},
  volume       = {50},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). User correlation model for question recommendation in
community question answering. <em>APIN</em>, <em>50</em>(2), 634–645.
(<a href="https://doi.org/10.1007/s10489-019-01544-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we address the problem of question recommendation that automatically recommends a new question to suitable users to answer in community question answering (CQA). The major challenge of question recommendation is the accurate selection of suitable users to answer. Most of the existing approaches attempt to find suitable users in CQA by estimating user’s existing capability, user’s interest or blending both for the question. However, these methods ignore correlation among users (askers and answerers) in terms of topic preference. In this study, we propose a user correlation model (UCM) to effectively estimate degree of correlation among users in terms of topic preference. Furthermore, we present the UCM-based approach to question recommendation, which provides a mechanism to naturally integrate the correlation between answerer and asker in terms of topic preference with content relevance between the answerer and the question into a unified probabilistic framework. Experiments using real-world data from Stack Overflow show that our UCM-based approach consistently and significantly improves the performance of question recommendation. Hence, our approach can increase question recommendation accuracy in CQA according to utilize the correlation between answerer and asker in terms of topic preference.},
  archive      = {J_APIN},
  author       = {Fu, Chaogang},
  doi          = {10.1007/s10489-019-01544-y},
  journal      = {Applied Intelligence},
  month        = {2},
  number       = {2},
  pages        = {634-645},
  shortjournal = {Appl. Intell.},
  title        = {User correlation model for question recommendation in community question answering},
  volume       = {50},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Exploring duality on ontology debugging. <em>APIN</em>,
<em>50</em>(2), 620–633. (<a
href="https://doi.org/10.1007/s10489-019-01528-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ontology debugging is an important and intractable reasoning task. Minimal unsatisfiability-preserving subset (MUPS) which is a minimal subset of an ontology to debug an unsatisfiable concept, is an important concept in ontology debugging. Hitting set relation between conflict set and diagnosis is essential for computing all conflict sets. And there has been many successful explorations about hitting set duality between conflict set and diagnosis in many fields. So in this paper, we will explore the duality between MUPS and minimal correctness-preserving subset (MCPS) which denotes the minimal diagnosis of a concept to debug unsatisfiable concepts on ontology debugging domain. Then several methods for computing all MUPSes will be devised based on the duality between MUPS and MCPS, meanwhile parallel strategies are also applied to newly proposed methods. And we show, by an empirical evaluation, that performances of different methods on real world ontologies from different domains. And it is proved that it is meaningful to explore duality on ontology debugging, as it really boosts the efficiency when applied to complex ontologies compared to the previous methods.},
  archive      = {J_APIN},
  author       = {Gao, Jie and Ouyang, Dantong and Ye, Yuxin},
  doi          = {10.1007/s10489-019-01528-y},
  journal      = {Applied Intelligence},
  month        = {2},
  number       = {2},
  pages        = {620-633},
  shortjournal = {Appl. Intell.},
  title        = {Exploring duality on ontology debugging},
  volume       = {50},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Sandpiper optimization algorithm: A novel approach for
solving real-life engineering problems. <em>APIN</em>, <em>50</em>(2),
582–619. (<a href="https://doi.org/10.1007/s10489-019-01507-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a novel bio-inspired algorithm called Sandpiper Optimization Algorithm (SOA) and applies it to solve challenging real-life problems. The main inspiration behind this algorithm is the migration and attacking behaviour of sandpipers. These two steps are modeled and implemented computationally to emphasize intensification and diversification in the search space. The comparison of proposed SOA algorithm is performed with nine competing optimization algorithms over 44 benchmark functions. The analysis of computational complexity and convergence behaviors of the proposed algorithm have been evaluated. Further, SOA algorithm is hybridized with decision tree machine-learning algorithm to solve real-life applications. The experimental results demonstrated that the proposed algorithm is able to solve challenging constrained optimization problems and outperforms the other state-of-the-art optimization algorithms.},
  archive      = {J_APIN},
  author       = {Kaur, Amandeep and Jain, Sushma and Goel, Shivani},
  doi          = {10.1007/s10489-019-01507-3},
  journal      = {Applied Intelligence},
  month        = {2},
  number       = {2},
  pages        = {582-619},
  shortjournal = {Appl. Intell.},
  title        = {Sandpiper optimization algorithm: A novel approach for solving real-life engineering problems},
  volume       = {50},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Unsupervised representation learning based on the deep
multi-view ensemble learning. <em>APIN</em>, <em>50</em>(2), 562–581.
(<a href="https://doi.org/10.1007/s10489-019-01526-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep networks have recently achieved great success in feature learning problem on various computer vision applications. Among different approaches in deep learning, unsupervised methods have attracted a lot of attention particularly to problems with limited training data. However, compared with supervised methods, unsupervised deep learning methods usually suffer from lower accuracy and higher computational time. To deal with these problems, we aim to restrict the number of connections between successive layers while enhancing discriminatory power using a data-driven approach. To this end, we propose a novel deep multi-view ensemble model. The structure of each layer is composed of an ensemble of encoders or decoders and mask operations. The multi-view ensemble of encoders or decoders enable the network to benefit from local complementary information and preserve local characteristics in final generated features, while mask operations determine the connections between successive layers. The experimental results on popular datasets indicate the effectiveness and validity of the method in clustering and classification tasks while the processing time is reduced.},
  archive      = {J_APIN},
  author       = {Koohzadi, Maryam and Charkari, Nasrollah Moghadam and Ghaderi, Foad},
  doi          = {10.1007/s10489-019-01526-0},
  journal      = {Applied Intelligence},
  month        = {2},
  number       = {2},
  pages        = {562-581},
  shortjournal = {Appl. Intell.},
  title        = {Unsupervised representation learning based on the deep multi-view ensemble learning},
  volume       = {50},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Multi-criteria decision making in pythagorean fuzzy
environment. <em>APIN</em>, <em>50</em>(2), 537–561. (<a
href="https://doi.org/10.1007/s10489-019-01532-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Pythagorean fuzzy set, initially extended by Yager from intuitionistic fuzzy set, is capable of modeling information with more uncertainties in the process of multi-criteria decision making (MCDM), thus can be used on wider range of conditions. The fuzzy decision analysis of this paper is mainly built upon two expressions in Pythagorean fuzzy environment, named Pythagorean fuzzy number (PFN) and interval-valued Pythagorean fuzzy number (IVPFN), respectively. We initiate a novel axiomatic definition of Pythagorean fuzzy distance measurement, including PFNs and IVPFNs. After that, corresponding theorems are put forward and then proved. Based on the defined distance measurements, the closeness indexes are developed for both expressions, inspired by the idea of technique for order preference by similarity to ideal solution (TOPSIS) approach. After these basic definitions have been established, the hierarchical decision approach is presented to handle MCDM problems under Pythagorean fuzzy environment. To address hierarchical decision issues, the closeness index-based score function is defined to calculate the score of each permutation for the optimal alternative. To determine criterion weights, a new method based on the proposed similarity measure and aggregation operator of PFNs and IVPFNs is presented according to Pythagorean fuzzy information from decision matrix, rather than being provided in advance by decision makers, which can effectively reduce human subjectivity. An experimental case is then conducted to demonstrate the applicability and flexibility of the proposed decision approach. Finally, extension forms of Pythagorean fuzzy decision approach for heterogeneous information are briefly introduced to show its potentials on further applications in other processing fields with information uncertainties.},
  archive      = {J_APIN},
  author       = {Fei, Liguo and Deng, Yong},
  doi          = {10.1007/s10489-019-01532-2},
  journal      = {Applied Intelligence},
  month        = {2},
  number       = {2},
  pages        = {537-561},
  shortjournal = {Appl. Intell.},
  title        = {Multi-criteria decision making in pythagorean fuzzy environment},
  volume       = {50},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Optimizing shapelets quality measure for imbalanced time
series classification. <em>APIN</em>, <em>50</em>(2), 519–536. (<a
href="https://doi.org/10.1007/s10489-019-01535-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Time series classification has been considered as one of the most challenging problems in data mining and is widely used in a broad range of fields. A biased distribution leads to classification on minority time series objects more severe. A commonly taken approach is to extract or select the representative features to retain the structure of a time series object. However, when the data distribution is imbalanced, the traditional features cannot represent time series effectively, especially in multi-class environment. In this paper, Shapelets — a primitive time series mining technology — is applied to extract the most representative subsequences. Especially, we verify that IG (Information Gain) is unsuitable as a shapelet quality measure for imbalanced data sets. Nevertheless, we propose two quality measures for shapelets on imbalanced binary and multi-class problem respectively. Based on extracted shapelet features, we select the diversified top-k shapelets based on new quality measure to represent the top-k best features and achieve this procedure on map-reduce framework. Lastly, two oversampling methods based on shapelet features are proposed to re-balance the binary and multi-class time series data sets. We validated our methods on the benchmark data sets by comparing with the canonical classifiers and the state-of-the-art time series algorithms. It is verified that the proposed algorithms perform more competitive than the compared methods in statistical significance.},
  archive      = {J_APIN},
  author       = {Yan, Qiuyan and Cao, Yang},
  doi          = {10.1007/s10489-019-01535-z},
  journal      = {Applied Intelligence},
  month        = {2},
  number       = {2},
  pages        = {519-536},
  shortjournal = {Appl. Intell.},
  title        = {Optimizing shapelets quality measure for imbalanced time series classification},
  volume       = {50},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Skill based transfer learning with domain adaptation for
continuous reinforcement learning domains. <em>APIN</em>,
<em>50</em>(2), 502–518. (<a
href="https://doi.org/10.1007/s10489-019-01527-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Although reinforcement learning is known as an effective machine learning technique, it might perform poorly in complex problems, especially real-world problems, leading to a slow rate of convergence. This issue magnifies when facing continuous domains where the curse of dimensionality is inevitable, and generalization is mostly desired. Transfer learning is a successful technique to remedy such a problem which results in significant improvements in learning performance by providing generalization not only within a task but also across different but related or similar tasks. The critical issue in transfer learning is how to incorporate the knowledge acquired from learning in a different but related task in the past. Domain adaptation is an exciting paradigm that seeks to address this challenge. In this paper, we propose a novel skill based Transfer Learning with Domain Adaptation (TLDA) approach suitable for continuous RL problems. TLDA discovers and learns skills as high-level knowledge from source task and then uses domain adaptation technique to help agent discover state-action mapping as a relation between the source and target tasks. With such mapping, TLDA can adapt source skills and speed up learning on a new target task. The experimental results verify the achievement of an effective transfer learning method for continuous reinforcement learning problems.},
  archive      = {J_APIN},
  author       = {Shoeleh, Farzaneh and Asadpour, Masoud},
  doi          = {10.1007/s10489-019-01527-z},
  journal      = {Applied Intelligence},
  month        = {2},
  number       = {2},
  pages        = {502-518},
  shortjournal = {Appl. Intell.},
  title        = {Skill based transfer learning with domain adaptation for continuous reinforcement learning domains},
  volume       = {50},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Feature subset selection combining maximal information
entropy and maximal information coefficient. <em>APIN</em>,
<em>50</em>(2), 487–501. (<a
href="https://doi.org/10.1007/s10489-019-01537-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Feature subset selection is an efficient step to reduce the dimension of data, which remains an active research field in decades. In order to develop highly accurate and fast searching feature subset selection algorithms, a filter feature subset selection method combining maximal information entropy (MIE) and the maximal information coefficient (MIC) is proposed in this paper. First, a new metric mMIE-mMIC is defined to minimize the MIE among features while maximizing the MIC between the features and the class label. The mMIE-mMIC algorithm is designed to evaluate whether a candidate subset is valid for classification. Second, two searching strategies are adopted to identify a suitable solution in the candidate subset space, including the binary particle swarm optimization algorithm (BPSO) and sequential forward selection (SFS). Finally, classification is performed on UCI datasets to validate the performance of our work compared to 9 existing methods. Experimental results show that in most cases, the proposed method behaves equally or better than the other 9 methods in terms of classification accuracy and F1-score.},
  archive      = {J_APIN},
  author       = {Zheng, Kangfeng and Wang, Xiujuan and Wu, Bin and Wu, Tong},
  doi          = {10.1007/s10489-019-01537-x},
  journal      = {Applied Intelligence},
  month        = {2},
  number       = {2},
  pages        = {487-501},
  shortjournal = {Appl. Intell.},
  title        = {Feature subset selection combining maximal information entropy and maximal information coefficient},
  volume       = {50},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Novel grey wolf optimization based on modified differential
evolution for numerical function optimization. <em>APIN</em>,
<em>50</em>(2), 468–486. (<a
href="https://doi.org/10.1007/s10489-019-01521-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Grey wolf optimization algorithm (GWO) is a new swarm intelligence optimization algorithm proposed in recent years. Because of few control parameters and easy implementation, GWO is widely used in many fields. Compared with other common swarm optimization algorithms, it is more suitable for the global optimization problems. Nevertheless, the algorithm still has the shortcoming of low accuracy and slow convergence speed. In this paper, a novel hybrid optimization algorithm named MDE-GWO is proposed. Firstly, a JADE with opposition-based learning strategy algorithm (MDE) is embedded in GWO to enhance the ability of avoiding a local optimum. Notably, by introducing the opposition-based learning strategy, the search ability of the improved algorithm is increased greatly. Additionally, in order to balance the global and local search capabilities and speed up the convergence of the GWO algorithm, a leader moving-rate strategy is put forward. 28 typical benchmark functions are utilized to test the performance of the improved algorithm. The experimental results show that MDE-GWO has stronger advantages in search accuracy, stability and convergence speed in most cases.},
  archive      = {J_APIN},
  author       = {Luo, Jun and Liu, Zewei},
  doi          = {10.1007/s10489-019-01521-5},
  journal      = {Applied Intelligence},
  month        = {2},
  number       = {2},
  pages        = {468-486},
  shortjournal = {Appl. Intell.},
  title        = {Novel grey wolf optimization based on modified differential evolution for numerical function optimization},
  volume       = {50},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Extracting relations of crime rates through fuzzy
association rules mining. <em>APIN</em>, <em>50</em>(2), 448–467. (<a
href="https://doi.org/10.1007/s10489-019-01531-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data mining is an important technology to reveal the patterns from crime data. Although there are many researches about this topic, less work models the relations between rates of different kinds of crime. In this paper, an algorithm based on fuzzy association rules (AR) mining is proposed to discover these relations. Two datasets, which are crimes in Chicago from 2012 to 2017 and crimes in NSW from 2008 to 2012, are used for case studies. At first, crime data is preprocessed, where every kind of crime occurring in every district during every month is counted. For a crime in a combination of district and month, the membership function, which is based on hypothesis testing, is designed to evaluate the degree to which its rate is high, normal or low, and the fuzzy transactional dataset is formed. A bridge between fuzzy transactional dataset and binary AR mining algorithm is built, so those mature tools of binary AR mining can be applied to generate fuzzy ARs. In the results of case studies, the strong relations between rates of different crime can be found. There are many interesting and surprise rules, which are worthy to be further studied by domain experts.},
  archive      = {J_APIN},
  author       = {Zhang, Zhongjie and Huang, Jian and Hao, Jianguo and Gong, Jianxing and Chen, Hao},
  doi          = {10.1007/s10489-019-01531-3},
  journal      = {Applied Intelligence},
  month        = {2},
  number       = {2},
  pages        = {448-467},
  shortjournal = {Appl. Intell.},
  title        = {Extracting relations of crime rates through fuzzy association rules mining},
  volume       = {50},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Adaptive graph regularized nonnegative matrix factorization
for data representation. <em>APIN</em>, <em>50</em>(2), 438–447. (<a
href="https://doi.org/10.1007/s10489-019-01539-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As a classical data representation method, nonnegative matrix factorization (NMF) can well capture the global structure information of the observed data, and it has been successfully applied in many fields. It is generally known that the local manifold structures will have a better effect than the global structures in image recognition and clustering. The local structure information can well be preserved by the neighbor graph in the manifold learning methods. The traditional neighbor graph constructed relies heavily on the original observed data. However, the original data usually contain a lot of noise and outliers in practical application, which results in obtaining inaccurate neighbor graph, and ultimately leads to performance degradation. How to get the ideal local structure information becomes more and more important. By combing the manifold learning into NMF, we propose an adaptive graph regularized nonnegative matrix factorization (AGNMF). In AGNMF, the neighbor graph is obtained by adaptive iteration. Both the global information and the local manifold can be well captured in AGNMF, and the better data representation can be obtained. A large number of experiments on different data sets show that our AGNMF has good clustering ability.},
  archive      = {J_APIN},
  author       = {Zhang, Lin and Liu, Zhonghua and Pu, Jiexin and Song, Bin},
  doi          = {10.1007/s10489-019-01539-9},
  journal      = {Applied Intelligence},
  month        = {2},
  number       = {2},
  pages        = {438-447},
  shortjournal = {Appl. Intell.},
  title        = {Adaptive graph regularized nonnegative matrix factorization for data representation},
  volume       = {50},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). An algorithm for influence maximization in competitive
social networks with unwanted users. <em>APIN</em>, <em>50</em>(2),
417–437. (<a href="https://doi.org/10.1007/s10489-019-01506-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rapid development of online social networks, the problem of influence maximization (IM) has attracted tremendous attention motivated by widespread application scenarios. However, there is less research focusing on the information spreading with the existence of some unwanted users. Such problem has a wide range of applications since there always exist conflicts of interest between competing businesses. In this paper, we formally define the problem of influence maximization with limited unwanted users (IML) under the independent cascade model. In order to avoid the time-consuming process of simulating the propagation in the traditional method of influence maximization, we propose a propagation path based strategy to compute the activation probabilities between the node pairs. Based on the activation probability, we define a propagation increment function to avoid simulating the influence spreading process on the candidate seed nodes. To select the optimal seed set, we present a greedy algorithm to sequentially select the nodes which can maximize the influence increment to join the seed set. Experimental results in real social networks have shown that the algorithm proposed not only outperforms the existing methods but also consumes much less computation time.},
  archive      = {J_APIN},
  author       = {Liu, Wei and Chen, Ling and Chen, Xin and Chen, Bolun},
  doi          = {10.1007/s10489-019-01506-4},
  journal      = {Applied Intelligence},
  month        = {2},
  number       = {2},
  pages        = {417-437},
  shortjournal = {Appl. Intell.},
  title        = {An algorithm for influence maximization in competitive social networks with unwanted users},
  volume       = {50},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A novel method based on deep learning for aligned
fingerprints matching. <em>APIN</em>, <em>50</em>(2), 397–416. (<a
href="https://doi.org/10.1007/s10489-019-01530-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this study, a novel method based on deep learning for aligned fingerprints matching is proposed. According to the characteristics of fingerprint images, a convolutional network, Finger ConvNet, is designed. In addition, a new joint supervision signal is used to train Finger ConvNet to obtain deep features. Experimental studies are performed on public fingerprint datasets, the ID Card fingerprint dataset and the Ten-Finger Fingerprint Card fingerprint dataset. Furthermore, four performance indicators, the false matching rate (FMR), false non-matching rate (FNMR), equal error rate (EER) and receiver operating characteristic (ROC) curve, are measured. The experimental results demonstrate the effectiveness of the proposed method, which achieved a competitive effect in comparison with conventional fingerprint matching algorithms in fingerprint verification tasks using the FVC2000, FVC2002, and FVC2004 datasets. Moreover, the matching speed of the proposed method was almost 5 times faster than the fastest conventional fingerprint matching algorithms. In addition, it can be used as a fast matching method to filter out many templates with low scores by setting a threshold according to the matching scores and thus accelerate the process in identification tasks.},
  archive      = {J_APIN},
  author       = {Liu, Yonghong and Zhou, Baicun and Han, Congying and Guo, Tiande and Qin, Jin},
  doi          = {10.1007/s10489-019-01530-4},
  journal      = {Applied Intelligence},
  month        = {2},
  number       = {2},
  pages        = {397-416},
  shortjournal = {Appl. Intell.},
  title        = {A novel method based on deep learning for aligned fingerprints matching},
  volume       = {50},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Plan merging by reuse for multi-agent planning.
<em>APIN</em>, <em>50</em>(2), 365–396. (<a
href="https://doi.org/10.1007/s10489-019-01429-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-Agent Planning deals with the task of generating a plan for/by a set of agents that jointly solve a planning problem. One of the biggest challenges is how to handle interactions arising from agents’ actions. The first contribution of the paper is Plan Merging by Reuse, pmr, an algorithm that automatically adjusts its behaviour to the level of interaction. Given a multi-agent planning task, pmr assigns goals to specific agents. The chosen agents solve their individual planning tasks and the resulting plans are merged. Since merged plans are not always valid, pmr performs planning by reuse to generate a valid plan. The second contribution of the paper is rrpt-plan, a stochastic plan-reuse planner that combines plan reuse, standard search and sampling. We have performed extensive sets of experiments in order to analyze the performance of pmr in relation to state of the art multi-agent planning techniques.},
  archive      = {J_APIN},
  author       = {Luis, Nerea and Fernández, Susana and Borrajo, Daniel},
  doi          = {10.1007/s10489-019-01429-0},
  journal      = {Applied Intelligence},
  month        = {2},
  number       = {2},
  pages        = {365-396},
  shortjournal = {Appl. Intell.},
  title        = {Plan merging by reuse for multi-agent planning},
  volume       = {50},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Community-based influence maximization in attributed
networks. <em>APIN</em>, <em>50</em>(2), 354–364. (<a
href="https://doi.org/10.1007/s10489-019-01529-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Influence Maximization, aiming at selecting a small set of seed users in a social network to maximize the spread of influence, has attracted considerable attention recently. Most existing influence maximization algorithms focus on pure networks, while in many real-world social networks, nodes are often associated with a rich set of attributes or features, aka attributed networks. Moreover, most of existing influence maximization methods suffer from the problems of high computational cost and no performance guarantee, as these methods heavily depend on analysis and exploitation of network structure. In this paper, we propose a new algorithm to solve community-based influence maximization problem in attributed networks, which consists of three steps: community detection, candidate community generation and seed node selection. Specifically, we first propose the candidate community generation process, which utilizes information of community structure as well as node attribute to narrow down possible community candidates. We then propose a model to predict influence strength between nodes in attributed network, which takes advantage of topology structure similarity and attribute similarity between nodes in addition to social interaction strength, thus improve the prediction accuracy comparing to the existing methods significantly. Finally, we select seed nodes by proposing the computation method of influence set, through which the marginal influence gain of nodes can be calculated directly, avoiding tens of thousands of Monte Carlo simulations and ultimately making the algorithm more efficient. Experiments on four real social network datasets demonstrate that our proposed algorithm outperforms state-of-the-art influence maximization algorithms in both influence spread and running time.},
  archive      = {J_APIN},
  author       = {Huang, Huimin and Shen, Hong and Meng, Zaiqiao},
  doi          = {10.1007/s10489-019-01529-x},
  journal      = {Applied Intelligence},
  month        = {2},
  number       = {2},
  pages        = {354-364},
  shortjournal = {Appl. Intell.},
  title        = {Community-based influence maximization in attributed networks},
  volume       = {50},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Using deep learning to preserve data confidentiality.
<em>APIN</em>, <em>50</em>(2), 341–353. (<a
href="https://doi.org/10.1007/s10489-019-01515-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Preserving data confidentiality is crucial when releasing microdata for public-use. There are a variety of proposed approaches; many of them are based on traditional probability theory and statistics. These approaches mainly focus on masking the original data. In practice, these masking techniques, despite covering part of the data, risk leaving sensitive data open to release. In this paper, we approach this problem using a deep learning-based generative model which generates simulation data to mask the original data. Generating simulation data that holds the same statistical characteristics as the raw data becomes the key idea and also the main challenge in this study. In particular, we explore the statistical similarities between the raw data and the generated data, given that the generated data and raw data are not obviously distinguishable. Two statistical evaluation metrics, Absolute Relative Residual Values and Hellinger Distance, are the evaluation methods we have decided upon to evaluate our results. We also conduct extensive experiments to validate our idea with two real-world datasets: the Census Dataset and the Environmental Dataset.},
  archive      = {J_APIN},
  author       = {Li, Wei and Meng, Pengqiu and Hong, Yi and Cui, Xiaohui},
  doi          = {10.1007/s10489-019-01515-3},
  journal      = {Applied Intelligence},
  month        = {2},
  number       = {2},
  pages        = {341-353},
  shortjournal = {Appl. Intell.},
  title        = {Using deep learning to preserve data confidentiality},
  volume       = {50},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A local community detection algorithm based on internal
force between nodes. <em>APIN</em>, <em>50</em>(2), 328–340. (<a
href="https://doi.org/10.1007/s10489-019-01541-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Community structure is an important characteristic of complex networks. Uncovering communities in complex networks is currently a hot research topic in the field of network analysis. Local community detection algorithms based on seed-extension are widely used for addressing this problem because they excel in efficiency and effectiveness. Compared with global community detection methods, local methods can uncover communities without the integral structural information of complex networks. However, they still have quality and stability deficiencies in overlapping community detection. For this reason, a local community detection algorithm based on internal force between nodes is proposed. First, local degree central nodes and Jaccard coefficient are used to detect core members of communities as seeds in the network, thus guaranteeing that the selected seeds are central nodes of communities. Second, the node with maximum degree among seeds is pre-extended by the fitness function every time. Finally, the top k nodes with the best performance in pre-extension process are extended by the fitness function with internal force between nodes to obtain high-quality communities in the network. Experimental results on both real and artificial networks show that the proposed algorithm can uncover communities more accurately than all the comparison algorithms.},
  archive      = {J_APIN},
  author       = {Guo, Kun and He, Ling and Chen, Yuzhong and Guo, Wenzhong and Zheng, Jianning},
  doi          = {10.1007/s10489-019-01541-1},
  journal      = {Applied Intelligence},
  month        = {2},
  number       = {2},
  pages        = {328-340},
  shortjournal = {Appl. Intell.},
  title        = {A local community detection algorithm based on internal force between nodes},
  volume       = {50},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Learning adaptive trust strength with user roles of truster
and trustee for trust-aware recommender systems. <em>APIN</em>,
<em>50</em>(2), 314–327. (<a
href="https://doi.org/10.1007/s10489-019-01542-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {There are two key characteristics of users in trust relationships that have been well studied: (1) users trust their friends with different trust strengths and (2) users play multiple roles of trusters and trustees in trust relationships. However, few studies have considered both of these factors. Indeed, it is quite common for someone to respond to his/her friend that they trusted him/her, which indicates that there exist two kinds of information between each pair of users: the trust influence of trustee on truster and the feedback influence of truster on trustee. Considering this problem, we propose a novel adaptive method to learn the trust influence between users with multiple roles of truster and trustee for recommendation. First, we propose to introduce the concept of latent trust strength to learn adaptive role-based trust strength with limited values for each trust relationship between users. Second, because there is only one training example to learn each parameter of latent trust strength, we further propose two regularization methods by building relations between latent trust strength and user preferences to guide the training process of latent trust strength. After that, we develop a new recommendation method, RoleTS, by integrating the role-based trust strength into a previous recommendation model, TrustSVD, which considers both explicit and implicit information of trust and ratings. We also conduct a series of experiments to study the performance of the proposed method. Experimental results on two public real datasets demonstrate that the proposed method performs better than several state-of-the-art algorithms.},
  archive      = {J_APIN},
  author       = {Pan, Yiteng and He, Fazhi and Yu, Haiping and Li, Haoran},
  doi          = {10.1007/s10489-019-01542-0},
  journal      = {Applied Intelligence},
  month        = {2},
  number       = {2},
  pages        = {314-327},
  shortjournal = {Appl. Intell.},
  title        = {Learning adaptive trust strength with user roles of truster and trustee for trust-aware recommender systems},
  volume       = {50},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Robust expected model change for active learning in
regression. <em>APIN</em>, <em>50</em>(2), 296–313. (<a
href="https://doi.org/10.1007/s10489-019-01519-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Active learning methods have been introduced to reduce the expense of acquiring labeled data. To solve regression problems with active learning, several expected model change maximization strategies have been developed to select the samples that are likely to greatly affect the current model. However, some of the selected samples may be outliers, which can result in poor estimation performance. To address this limitation, this study proposes an active learning framework that adopts an expected model change that is robust for both linear and nonlinear regression problems. By embedding local outlier probability, the learning framework aims to avoid outliers when selecting the samples that result in the greatest change to the current model. Experiments are conducted on synthetic and benchmark data to compare the performance of the proposed method with that of existing methods. The experimental results demonstrate that the proposed active learning algorithm outperforms its counterparts.},
  archive      = {J_APIN},
  author       = {Park, Sung Ho and Kim, Seoung Bum},
  doi          = {10.1007/s10489-019-01519-z},
  journal      = {Applied Intelligence},
  month        = {2},
  number       = {2},
  pages        = {296-313},
  shortjournal = {Appl. Intell.},
  title        = {Robust expected model change for active learning in regression},
  volume       = {50},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). DCADE: Divide and conquer alignment with dynamic encoding
for full page data extraction. <em>APIN</em>, <em>50</em>(2), 271–295.
(<a href="https://doi.org/10.1007/s10489-019-01499-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we consider the problem of full schema induction from either multiple list pages or singleton pages with the same template. Existing approaches do not work well for this problem because they use fixed abstraction schemes that are suitable for data-rich detection, but they are not appropriate for small records and complex data found in other sections. We propose an unsupervised full schema web data extraction via Divide-and-Conquer Alignment with Dynamic Encoding (DCADE for short). We define the Content Equivalence Class (CEC) and Typeset Equivalence Class (TEC) based on leaf node content. We then combine HTML attributes (i.e., id and class) in the paths for various levels of encoding, so that the proposed algorithm can align leaf nodes by exploring patterns at various levels from specific to general. We conducted experiments on 49 real-world websites used in TEX and ExAlg. The proposed DCADE achieved a 0.962 F1 measure for non-recordset data extraction (denoted by FD), and a 0.936 F1 measure for recordset data extraction (denoted by FS), which outperformed other page-level web data extraction methods, i.e., DCA (FD= 0.660), TEX (FD= 0.454 and FS= 0.549), RoadRunner (FD= 0.396 and FS= 0.330), and UWIDE (FD= 0.260 and FS= 0.081).},
  archive      = {J_APIN},
  author       = {Yuliana, Oviliani Yenty and Chang, Chia-Hui},
  doi          = {10.1007/s10489-019-01499-0},
  journal      = {Applied Intelligence},
  month        = {2},
  number       = {2},
  pages        = {271-295},
  shortjournal = {Appl. Intell.},
  title        = {DCADE: Divide and conquer alignment with dynamic encoding for full page data extraction},
  volume       = {50},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Multi-objective particle swarm optimization based on
cooperative hybrid strategy. <em>APIN</em>, <em>50</em>(1), 256–269. (<a
href="https://doi.org/10.1007/s10489-019-01496-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A multi-objective particle swarm optimization based on cooperative hybrid strategy (CHSPSO) is presented in this paper to solve complex multi-objective problems. Most algorithms usually contain only one strategy, which makes them unable to trade off the convergence and diversity when solving the complex multi-objective problems. The proposed cooperative hybrid strategy can effectively guarantee the convergence and the diversity of the algorithm. The multi-population strategy and the dynamic clustering strategy are employed to improve the convergence and the diversity. At the same time, the life strategy and lottery probability selection strategy are used to further ensure the diversity of the population. A series of test functions are used to verify the effectiveness of CHSPSO. The performance of the proposed algorithm is compared with other evolutionary algorithms. The results show that CHSPSO can obtain a better convergence and diversity for the complex multi-objective problems.},
  archive      = {J_APIN},
  author       = {Yu, Hui and Wang, YuJia and Xiao, ShanLi},
  doi          = {10.1007/s10489-019-01496-3},
  journal      = {Applied Intelligence},
  month        = {1},
  number       = {1},
  pages        = {256-269},
  shortjournal = {Appl. Intell.},
  title        = {Multi-objective particle swarm optimization based on cooperative hybrid strategy},
  volume       = {50},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Dynamic uncertain causality graph based on intuitionistic
fuzzy sets and its application to root cause analysis. <em>APIN</em>,
<em>50</em>(1), 241–255. (<a
href="https://doi.org/10.1007/s10489-019-01520-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dynamic uncertain causality graph (DUCG), which is based on probability theory, is used for uncertain knowledge representation and reasoning. However, the traditional DUCG has difficulty expressing the causality of the events with crisp numbers. Therefore, an intuitionistic fuzzy set based dynamic uncertain causality graph (IFDUCG) model is proposed in this paper. The model focuses on describing the uncertain event in the form of intuitionistic fuzzy sets, which can handle with the problem of describing vagueness and uncertainty of an event in the traditional model. Then the technique for order preference by similarity to an ideal solution (TOPSIS) method is combined with IFDUCG for knowledge representation and reasoning so as to integrate more abundant experienced knowledge into the model to make the model more reliable. Then some examples are used to validate the proposed method. The experimental results prove that the proposed method is effective and flexible in dealing with the difficulty of the fuzzy event of knowledge representation and reasoning. Furthermore, we make a practical application to root cause analysis of aluminum electrolysis and the results show that the proposed method is available for workers to make decisions.},
  archive      = {J_APIN},
  author       = {Li, Li and Yue, Weichao},
  doi          = {10.1007/s10489-019-01520-6},
  journal      = {Applied Intelligence},
  month        = {1},
  number       = {1},
  pages        = {241-255},
  shortjournal = {Appl. Intell.},
  title        = {Dynamic uncertain causality graph based on intuitionistic fuzzy sets and its application to root cause analysis},
  volume       = {50},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Multi-stage optimization model for hesitant qualitative
decision making with hesitant fuzzy linguistic preference relations.
<em>APIN</em>, <em>50</em>(1), 222–240. (<a
href="https://doi.org/10.1007/s10489-019-01502-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hesitant fuzzy linguistic preference relation (HFLPR) as a new preference relation is introduced to express the decision makers’ (DMs’) hesitant preference information for each pairwise comparison between different alternatives or criteria. In this paper, the priority vector and consistency of HFLPR are discussed based on a two-stage optimization and multiplicative consistency. Based on the original hesitant preference information, the multiplicative consistency index of an HFLPR is defined to measure the consistency level of the HFLPR. For an unacceptable multiplicative consistent HFLPR, a goal programming model, which is an integer optimization model, is developed to derive an acceptable, multiplicative, consistent HFLPR. According to probability sampling, a linguistic preference relation (LPR) with the best consistency level and an LPR with the worst consistency level with regard to an HFLPR are defined. Combining the two LPRs, a two-stage optimization framework is constructed to obtain the HFLPR’s priority vector, which considers the DM’s risk preference. A multi-stage optimization approach is proposed to solve decision-making problems by integrating the goal programming model and the two-stage optimization framework. Two real life problems are analyzed to show the feasibility of the proposed approach.},
  archive      = {J_APIN},
  author       = {Wu, Peng and Zhou, Ligang and Chen, Huayou and Tao, Zhifu},
  doi          = {10.1007/s10489-019-01502-8},
  journal      = {Applied Intelligence},
  month        = {1},
  number       = {1},
  pages        = {222-240},
  shortjournal = {Appl. Intell.},
  title        = {Multi-stage optimization model for hesitant qualitative decision making with hesitant fuzzy linguistic preference relations},
  volume       = {50},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Bag of contour fragments for improvement of object
segmentation. <em>APIN</em>, <em>50</em>(1), 203–221. (<a
href="https://doi.org/10.1007/s10489-019-01525-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many state-of-the-art shape features have been proposed for the shape recognition task. In this paper, to explore whether a shape feature influences object segmentation, we propose a specific shape feature, Fisher shape (a form of bag of contour fragments), and we combine this with the appearance feature with multiple kernel learning to create a pipeline of object segmentation system. The experimental results on benchmark datasets clearly demonstrate that the pipeline of object segmentation is effective and that the Fisher shape can improve object segmentation with only the appearance feature.},
  archive      = {J_APIN},
  author       = {Yu, Qian and Yang, Chengzhuan and Fan, Honghui and Zhu, Hongjin and Ye, Feiyue and Wei, Hui},
  doi          = {10.1007/s10489-019-01525-1},
  journal      = {Applied Intelligence},
  month        = {1},
  number       = {1},
  pages        = {203-221},
  shortjournal = {Appl. Intell.},
  title        = {Bag of contour fragments for improvement of object segmentation},
  volume       = {50},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Non-convex approximation based l0-norm multiple indefinite
kernel feature selection. <em>APIN</em>, <em>50</em>(1), 192–202. (<a
href="https://doi.org/10.1007/s10489-018-01407-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multiple kernel learning (MKL) for feature selection utilizes kernels to explore complex properties of features, which has been shown to be among the most effective for feature selection. To perform feature selection, a natural way is to use the l0-norm to get sparse solutions. However, the optimization problem involving l0-norm is NP-hard. Therefore, previous MKL methods typically utilize a l1-norm to get sparse kernel combinations. However, the l1-norm, as a convex approximation of l0-norm, sometimes cannot attain the desired solution of the l0-norm regularizer problem and may lead to prediction accuracy loss. In contrast, various non-convex approximations of l0-norm have been proposed and perform better in many linear feature selection methods. In this paper, we propose a novel l0-norm based MKL method (l0-MKL) for feature selection with non-convex approximations constraint on kernel combination coefficients to select features automatically. Considering the better empirical performance of indefinite kernels than positive kernels, our l0-MKL is built on the primal form of multiple indefinite kernel learning for feature selection. The non-convex optimization problem of l0-MKL is further refumated as a difference of convex functions (DC) programming and solved by DC algorithm (DCA). Experiments on real-world datasets demonstrate that l0-MKL is superior to some related state-of-the-art methods in both feature selection and classification performance.},
  archive      = {J_APIN},
  author       = {Xue, Hui and Song, Yu},
  doi          = {10.1007/s10489-018-01407-y},
  journal      = {Applied Intelligence},
  month        = {1},
  number       = {1},
  pages        = {192-202},
  shortjournal = {Appl. Intell.},
  title        = {Non-convex approximation based l0-norm multiple indefinite kernel feature selection},
  volume       = {50},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Effective sanitization approaches to protect sensitive
knowledge in high-utility itemset mining. <em>APIN</em>, <em>50</em>(1),
169–191. (<a href="https://doi.org/10.1007/s10489-019-01524-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For mutual benefit, data is shared among business organizations. However, this may result in privacy and security threats. To address this issue, privacy-preserving data mining is presented to sanitize the original database to hide all sensitive knowledge. Privacy-preserving utility mining is an extension of privacy-preserving data mining, the objective of which is to hide all sensitive high-utility itemsets and minimize the side effects on non-sensitive knowledge caused by the sanitization process. In this paper, three heuristic algorithms for privacy-preserving utility mining are proposed, namely, Selecting Maximum Utility item first (SMAU), Selecting Minimum Utility item first (SMIU) and Selecting Minimum Side Effects item first (SMSE). The quality of the database is well maintained because all of the proposed algorithms consider the side effects on the non-sensitive itemsets. Furthermore, to avoid performing multiple database scans, two table structures, T-table and HUI-table, are adopted to accelerate the hiding process by only scanning the database twice. The experimental results show that the proposed approaches successfully conceal all sensitive itemsets with fewer distortions of non-sensitive knowledge. Moreover, the influence of the database density on the proposed approaches is observed.},
  archive      = {J_APIN},
  author       = {Liu, Xuan and Wen, Shiting and Zuo, Wanli},
  doi          = {10.1007/s10489-019-01524-2},
  journal      = {Applied Intelligence},
  month        = {1},
  number       = {1},
  pages        = {169-191},
  shortjournal = {Appl. Intell.},
  title        = {Effective sanitization approaches to protect sensitive knowledge in high-utility itemset mining},
  volume       = {50},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Fuzzy risk analysis under influence of non-homogeneous
preferences elicitation in fiber industry. <em>APIN</em>,
<em>50</em>(1), 157–168. (<a
href="https://doi.org/10.1007/s10489-019-01508-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fuzzy risk analysis plays an important role in mitigating the levels of harm of a risk. In real world scenarios, it is a big challenge for risk analysts to make a proper and comprehensive decision when coping with risks that are incomplete, vague and fuzzy. Many established fuzzy risk analysis approaches do not have the flexibility to deal with knowledge in the form of preferences elicitation which lead to incorrect risk decision. The inefficiency is reflected when they consider only risk analyst preferences elicitation that is partially known. Nonetheless, the preferences elicited by the risk analyst are often non-homogeneous in nature such that they can be completely known, completely unknown, partially known and partially unknown. In this case, established fuzzy risk analysis methods are considered as inefficient in handling risk, hence an appropriate fuzzy risk analysis method that can deal with the non-homogeneous nature of risk analyst’s preferences elicitation is worth developing. Therefore, this paper proposes a novel fuzzy risk analysis method that is capable to deal with the non-homogeneous risk analyst’s preferences elicitation based on grey numbers. The proposed method aims at resolving the uncertain interactions between homogeneous and non-homogeneous natures of risk analyst’s preferences elicitation by using a novel consensus reaching approach that involves transformation of grey numbers into grey parametric fuzzy numbers. Later on, a novel fuzzy risk assessment score approach is presented to correctly evaluate and distinguish the levels of harm of the risks faced, such that these evaluations are consistent with preferences elicitation of the risk analyst. A real world risk analysis problem in fiber industry is then carried out to demonstrate the novelty, validity and feasibility of the proposed method.},
  archive      = {J_APIN},
  author       = {Bakar, Ahmad Syafadhli Abu and Khalif, Ku Muhammad Naim Ku and Shariff, Asma Ahmad and Gegov, Alexander and Salleh, Fauzani Md},
  doi          = {10.1007/s10489-019-01508-2},
  journal      = {Applied Intelligence},
  month        = {1},
  number       = {1},
  pages        = {157-168},
  shortjournal = {Appl. Intell.},
  title        = {Fuzzy risk analysis under influence of non-homogeneous preferences elicitation in fiber industry},
  volume       = {50},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Aggregated topic models for increasing social media topic
coherence. <em>APIN</em>, <em>50</em>(1), 138–156. (<a
href="https://doi.org/10.1007/s10489-019-01438-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This research presents a novel aggregating method for constructing an aggregated topic model that is composed of the topics with greater coherence than individual models. When generating a topic model, a number of parameters have to be specified. The resulting topics can be very general or very specific, which depend on the chosen parameters. In this study we investigate the process of aggregating multiple topic models generated using different parameters with a focus on whether combining the general and specific topics is able to increase topic coherence. We employ cosine similarity and Jensen-Shannon divergence to compute the similarity among topics and combine them into an aggregated model when their similarity scores exceed a predefined threshold. The model is evaluated against the standard topics models generated by the latent Dirichlet allocation and Non-negative Matrix Factorisation. Specifically we use the coherence of topics to compare the individual models that create aggregated models against those of the aggregated model and models generated by Non-negative Matrix Factorisation, respectively. The results demonstrate that the aggregated model outperforms those topic models at a statistically significant level in terms of topic coherence over an external corpus. We also make use of the aggregated topic model on social media data to validate the method in a realistic scenario and find that again it outperforms individual topic models.},
  archive      = {J_APIN},
  author       = {Blair, Stuart J. and Bi, Yaxin and Mulvenna, Maurice D.},
  doi          = {10.1007/s10489-019-01438-z},
  journal      = {Applied Intelligence},
  month        = {1},
  number       = {1},
  pages        = {138-156},
  shortjournal = {Appl. Intell.},
  title        = {Aggregated topic models for increasing social media topic coherence},
  volume       = {50},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). MOSHEPO: A hybrid multi-objective approach to solve economic
load dispatch and micro grid problems. <em>APIN</em>, <em>50</em>(1),
119–137. (<a href="https://doi.org/10.1007/s10489-019-01522-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes a novel hybrid multi-objective algorithm named Multi-objective Spotted Hyena and Emperor Penguin Optimizer (MOSHEPO) for solving both convex and non-convex economic dispatch and micro grid power dispatch problems. The proposed algorithm combines two newly developed bio-inspired optimization algorithms namely Multi-objective Spotted Hyena Optimizer (MOSHO) and Emperor Penguin Optimizer (EPO). MOSHEPO contemplates many non-linear characteristics of power generators such as transmission losses, multiple fuels, valve-point loading, and prohibited operating zones along with their operational constraints, for practical operation. To evaluate the effectiveness of MOSHEPO, the proposed algorithm has been tested on various benchmark test systems and its performance is compared with other well-known approaches. The experimental results demonstrate that the proposed algorithm outperforms other algorithms with low computational efforts while solving economic and micro grid power dispatch problems.},
  archive      = {J_APIN},
  author       = {Dhiman, Gaurav},
  doi          = {10.1007/s10489-019-01522-4},
  journal      = {Applied Intelligence},
  month        = {1},
  number       = {1},
  pages        = {119-137},
  shortjournal = {Appl. Intell.},
  title        = {MOSHEPO: A hybrid multi-objective approach to solve economic load dispatch and micro grid problems},
  volume       = {50},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Feature selection with symmetrical complementary coefficient
for quantifying feature interactions. <em>APIN</em>, <em>50</em>(1),
101–118. (<a href="https://doi.org/10.1007/s10489-019-01518-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the field of machine learning and data mining, feature interaction is a ubiquitous issue that cannot be ignored and has attracted more attention in recent years. In this paper, we proposed the Symmetrical Complementary Coefficient which can quantify feature interactions very well. Based on it, we improved the Sequential Forward Selection (SFS) algorithm and proposed a new feature subset searching algorithm called SCom-SFS which only needs to consider the feature interactions between adjacent features on a given sequence instead of all of them. Moreover, discovered feature interactions can speed up the process of searching for the optimal feature subset. In addition, we have improved the ReliefF algorithm by screening out representative samples from the original data set, and need not to sample the samples. The improved ReliefF algorithm has been proved to be more efficient and reliable. An effective and complete feature selection algorithm RRSS is obtained through the combination of the two modified algorithms. According to the experimental results, the proposed algorithm RRSS outperformed five classic and two latest feature selection algorithms in terms of size of resulting feature subset, Accuracy, Kappa coefficient, and adjusted Mean-Square Error (MSE).},
  archive      = {J_APIN},
  author       = {Zhang, Rui and Zhang, Zuoquan},
  doi          = {10.1007/s10489-019-01518-0},
  journal      = {Applied Intelligence},
  month        = {1},
  number       = {1},
  pages        = {101-118},
  shortjournal = {Appl. Intell.},
  title        = {Feature selection with symmetrical complementary coefficient for quantifying feature interactions},
  volume       = {50},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A jigsaw puzzle inspired algorithm for solving large-scale
no-wait flow shop scheduling problems. <em>APIN</em>, <em>50</em>(1),
87–100. (<a href="https://doi.org/10.1007/s10489-019-01497-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The no-wait flow shop scheduling problem (NWFSP), as a typical NP-hard problem, has important ramifications in the modern industry. In this paper, a jigsaw puzzle inspired heuristic (JPA) is proposed for solving NWFSP with the objective of minimizing makespan. The core idea behind JPA is to find the best match for each job until all the jobs are scheduled in the set of process. In JPA, a waiting time matrix is constructed to measure the gap between two jobs. Then, a matching matrix based on the waiting time matrix is obtained. Finally, the optimal scheduling sequence is built by using the matching matrix. Experimental results on large-scale benchmark instances show that JPA is superior to the state-of-the-art heuristics.},
  archive      = {J_APIN},
  author       = {Zhao, Fuqing and He, Xuan and Zhang, Yi and Lei, Wenchang and Ma, Weimin and Zhang, Chuck and Song, Houbin},
  doi          = {10.1007/s10489-019-01497-2},
  journal      = {Applied Intelligence},
  month        = {1},
  number       = {1},
  pages        = {87-100},
  shortjournal = {Appl. Intell.},
  title        = {A jigsaw puzzle inspired algorithm for solving large-scale no-wait flow shop scheduling problems},
  volume       = {50},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Enhancing data analysis: Uncertainty-resistance method for
handling incomplete data. <em>APIN</em>, <em>50</em>(1), 74–86. (<a
href="https://doi.org/10.1007/s10489-019-01514-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In data analysis, incomplete data commonly occurs and can have significant effects on the conclusions that can be drawn from the data. Incomplete data cause another problem, so-called uncertainty which leads to producing unreliable results. Hence, developing effective techniques to impute these missing values is crucial. Missing or incomplete data and noise are two common sources of uncertainty. In this paper, an effective method for imputing missing values is introduced which is robust to uncertainties that are arising from incompleteness and noise. A kernel-based method for removing the noise is designed. Using the belief function theory, the class of incomplete data is determined. Finally, every missing dimension is imputed considering the mean value of the same dimension of the members belonging to the determined class. The performance has been evaluated on real-world data sets from UCI repository. The results of the experiments have been compared with state-of-the-art methods, which show the superiority of the proposed method regarding classification accuracy.},
  archive      = {J_APIN},
  author       = {Hamidzadeh, Javad and Moradi, Mona},
  doi          = {10.1007/s10489-019-01514-4},
  journal      = {Applied Intelligence},
  month        = {1},
  number       = {1},
  pages        = {74-86},
  shortjournal = {Appl. Intell.},
  title        = {Enhancing data analysis: Uncertainty-resistance method for handling incomplete data},
  volume       = {50},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Transfer naive bayes algorithm with group probabilities.
<em>APIN</em>, <em>50</em>(1), 61–73. (<a
href="https://doi.org/10.1007/s10489-019-01512-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In order to protect data privacy, a new transfer group probability Naive Bayes algorithm TrGNB is proposed. TrGNB is applied to scenarios in which the source domain contains a large amount of labeled data and only a small amount of unlabeled data group probability information in the target domain. TrGNB integrates the ideology of transfer learning and group probability information into the Naive Bayes model, which not only improves the classification effect of the learning task in the target domain but also protects the data privacy. The TrGNB was verified on the 20-Newsgroups, Reuters-21578 and Email spam datasets. The experimental results show that TrGNB significantly improves the classification accuracy compared with the benchmark algorithms.},
  archive      = {J_APIN},
  author       = {Li, Jingmei and Wu, Weifei and Xue, Di},
  doi          = {10.1007/s10489-019-01512-6},
  journal      = {Applied Intelligence},
  month        = {1},
  number       = {1},
  pages        = {61-73},
  shortjournal = {Appl. Intell.},
  title        = {Transfer naive bayes algorithm with group probabilities},
  volume       = {50},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Recognising innovative companies by using a diversified
stacked generalisation method for website classification. <em>APIN</em>,
<em>50</em>(1), 42–60. (<a
href="https://doi.org/10.1007/s10489-019-01509-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose a classification system which is able to decide whether a company is innovative or not, based only on its public website available on the internet. As innovativeness plays a crucial role in the development of myriad branches of the modern economy, an increasing number of entities are expending effort to be innovative. Thus, a new issue has appeared: how can we recognise them? Not only is grasping the idea of innovativeness challenging for humans, but also impossible for any known machine learning algorithm. Therefore, we propose a new indirect technique: a diversified stacked generalisation method, which is based on a combination of a multi-view approach and a genetic algorithm. The proposed approach achieves better performance than all other classification methods which include: (i) models trained on single datasets; or (ii) a simple voting method on these models. Furthermore, in this study, we check if unaligned feature space improves classification results. The proposed solution has been extensively evaluated on real data collected from companies’ websites. The experimental results verify that the proposed method improves the classification quality of websites which might represent innovative companies.},
  archive      = {J_APIN},
  author       = {Mirończuk, Marcin Michał and Protasiewicz, Jarosław},
  doi          = {10.1007/s10489-019-01509-1},
  journal      = {Applied Intelligence},
  month        = {1},
  number       = {1},
  pages        = {42-60},
  shortjournal = {Appl. Intell.},
  title        = {Recognising innovative companies by using a diversified stacked generalisation method for website classification},
  volume       = {50},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A high-speed d-CART online fault diagnosis algorithm for
rotor systems. <em>APIN</em>, <em>50</em>(1), 29–41. (<a
href="https://doi.org/10.1007/s10489-019-01516-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Intelligent manufacturing poses a challenge for fault diagnosis of rotor systems to meet the three tasks: whether exists faults, faults location and quantitative diagnosis. Traditional methods hardly meet all the three tasks in online fault diagnosis. This paper proposes a modified classification and regression tree (CART) algorithm named D-CART algorithm to provide much faster fault classification by reducing the iteration times in computation while still ensuring accuracy. Experiments are carried on to achieve a comprehensive online fault diagnosis for rotor systems such as faults location, faults types and quantitative analysis of unbalanced mass in this paper. In comparison with the other 4 novel CART-based algorithms, the experimental results indicate that the speed of D-CART algorithm is improved by a factor of 23.92 compared to the fastest improved algorithm (Adaboost-CART) and a model accuracy of up to 96.77%. Thus demonstrating the speed superiority of D-CART algorithm in both diagnosing locations of different faults types and determining the loading masses of unbalanced faults. The proposed method has the potential to realize high-accuracy online fault diagnosis for rotor systems.},
  archive      = {J_APIN},
  author       = {Deng, Huaxia and Diao, Yifan and Wu, Wei and Zhang, Jin and Ma, Mengchao and Zhong, Xiang},
  doi          = {10.1007/s10489-019-01516-2},
  journal      = {Applied Intelligence},
  month        = {1},
  number       = {1},
  pages        = {29-41},
  shortjournal = {Appl. Intell.},
  title        = {A high-speed D-CART online fault diagnosis algorithm for rotor systems},
  volume       = {50},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). How to add new knowledge to already trained deep learning
models applied to semantic localization. <em>APIN</em>, <em>50</em>(1),
14–28. (<a href="https://doi.org/10.1007/s10489-019-01517-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The capacity of a robot to automatically adapt to new environments is crucial, especially in social robotics. Often, when these robots are deployed in home or office environments, they tend to fail because they lack the ability to adapt to new and continuously changing scenarios. In order to accomplish this task, robots must obtain new information from the environment, and then add it to their already learned knowledge. Deep learning techniques are often used to tackle this problem successfully. However, these approaches, complete retraining of the models, which is highly time-consuming. In this work, several strategies are tested to find the best way to include new knowledge in an already learned model in a deep learning pipeline, putting the spotlight on the time spent for this training. We tackle the localization problem in the long term with a deep learning approach and testing several retraining strategies. The results of the experiments are discussed and, finally, the best approach is deployed on a Pepper robot.},
  archive      = {J_APIN},
  author       = {Cruz, Edmanuel and Rangel, Jose Carlos and Gomez-Donoso, Francisco and Cazorla, Miguel},
  doi          = {10.1007/s10489-019-01517-1},
  journal      = {Applied Intelligence},
  month        = {1},
  number       = {1},
  pages        = {14-28},
  shortjournal = {Appl. Intell.},
  title        = {How to add new knowledge to already trained deep learning models applied to semantic localization},
  volume       = {50},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Small traffic sign detection from large image.
<em>APIN</em>, <em>50</em>(1), 1–13. (<a
href="https://doi.org/10.1007/s10489-019-01511-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Automatic traffic sign detection has great potential for intelligent vehicles. The ability to detect small traffic signs in large traffic scenes enhances the safety of intelligent devices. However, small object detection is a challenging problem in computer vision; the main problem involved in accurate traffic sign detection is the small size of the signs. In this paper, we present a deconvolution region-based convolutional neural network (DR-CNN) to cope with this problem. This method first adds a deconvolution layer and a normalization layer to the output of the convolution layer. It concatenates the features of the different layers into a fused feature map to provide sufficient information for small traffic sign detection. To improve training effectiveness and distinguish hard negative samples from easy positive ones, we propose a two-stage adaptive classification loss function for region proposal networks (RPN) and fully connected neural networks within DR-CNN. Finally, we evaluate our proposed method on the new and challenging Tsinghua-Tencent 100K dataset. We further conduct ablation experiments and analyse the effectiveness of the fused feature map and the two-stage classification loss function. The final experimental results demonstrate the superiority of the proposed method for detecting small traffic signs.},
  archive      = {J_APIN},
  author       = {Liu, Zhigang and Li, Dongyu and Ge, Shuzhi Sam and Tian, Feng},
  doi          = {10.1007/s10489-019-01511-7},
  journal      = {Applied Intelligence},
  month        = {1},
  number       = {1},
  pages        = {1-13},
  shortjournal = {Appl. Intell.},
  title        = {Small traffic sign detection from large image},
  volume       = {50},
  year         = {2020},
}
</textarea>
</details></li>
</ul>

</body>
</html>
