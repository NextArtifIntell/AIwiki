<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>PAAA_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="paaa---112">PAAA - 112</h2>
<ul>
<li><details>
<summary>
(2020). Extraction and prioritization of product attributes using an
explainable neural network. <em>PAAA</em>, <em>23</em>(4), 1767–1777.
(<a href="https://doi.org/10.1007/s10044-020-00878-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Identification of product attributes is an important matter in real-world business environments because customers generally make purchase decisions based on their evaluation of the attributes of the product. Numerous studies on product attribute extraction have been performed on the basis of user-generated textual reviews. However, most of them focused only on the attribute extraction process itself and not on the relative importance of the extracted attributes, which are critical information that can be utilized for the promotion or development of specification sheets. Thus, in this study, we focused on the development of an attribute set for a product by considering the relative importance of the extracted attributes. First, we extracted the aspects by utilizing convolutional neural network-based approaches and transfer learning. Second, we propose a novel approach, consisting of variants of the Gradient-weighted class activation mapping (Grad-CAM) algorithm, one of the explainable neural network frameworks, to capture the importance score of each extracted aspect. Using a sentimental prediction model, we calculated the weight of each aspect that affects the sentiment decision. We verified the performance of our proposed method by comparing the similarity of the product attributes that it extracted and their relative importance with the product attributes that customers consider to be the most important and by comparing the attributes used to develop the specification sheet of an existing major commercial site.},
  archive      = {J_PAAA},
  author       = {Lee, Younghoon and Park, Jungmin and Cho, Sungzoon},
  doi          = {10.1007/s10044-020-00878-5},
  journal      = {Pattern Analysis and Applications},
  month        = {11},
  number       = {4},
  pages        = {1767-1777},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Extraction and prioritization of product attributes using an explainable neural network},
  volume       = {23},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). ETIP: A lengthy nested NER problem for chinese insurance
policy analysis. <em>PAAA</em>, <em>23</em>(4), 1755–1765. (<a
href="https://doi.org/10.1007/s10044-020-00885-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Contract analysis can significantly ease the work for humans using AI techniques. This paper shows a lengthy nested NER problem of element tagging on insurance policy (ETIP). Compared to NER, ETIP deals with not only different types of entities which vary from a short phrase to a long sentence, but also phrase or clause entities that could be nested. We present a novel hybrid framework of deep learning and heuristic filtering method to recognize the lengthy nested elements. First, a convolutional neural network is constructed to obtain good initial candidates of sliding windows with high softmax probability. Then, the concatenation operator on adjacent candidate segments is introduced to create phrase, clause, or sentence candidates. We design an effective voting strategy to resolve the classification conflict of the concatenated candidates and present a theoretical proof of F1-score optimization. In experiments, we have collected a large Chinese insurance contract dataset to test the performance of the proposed method. An extensive set of experiments is performed to investigate how sliding window candidates can work effectively in our filtering and voting strategy. The optimal parameters are determined by statistical analysis of the experimental data. The results show the promising performance of our method in the ETIP problem.},
  archive      = {J_PAAA},
  author       = {Sun, Lin and Zhang, Kai and Sun, Yuxuan and Weng, Fangsheng and Zhang, Jianwei},
  doi          = {10.1007/s10044-020-00885-6},
  journal      = {Pattern Analysis and Applications},
  month        = {11},
  number       = {4},
  pages        = {1755-1765},
  shortjournal = {Pattern Anal. Appl.},
  title        = {ETIP: A lengthy nested NER problem for chinese insurance policy analysis},
  volume       = {23},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Face anti-spoofing by identity masking using random walk
patterns and outlier detection. <em>PAAA</em>, <em>23</em>(4),
1735–1754. (<a
href="https://doi.org/10.1007/s10044-020-00875-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing architectures used in face anti-spoofing tend to deploy registered spatial measurements to generate feature vectors for spoof detection. This means that the ordering or sequence in which specific statistics are computed cannot be changed, as one moves from one facial profile to another. While this arrangement works in a person-specific setting, it becomes a major drawback when single-sided training is done based on the natural face class alone. To mitigate subject identity linked content interference within the anti-spoofing frame, we propose a identity-independent architecture based on random correlated scans of natural face images. The same natural face image can be scanned multiple times through independent correlated random walks before deriving simple differential features on the 1D scanned vectors. This proposed frame tends to capture the pixel correlation statistics with minimal content interference and shows great promise, particularly when trained on natural face sets, using a one-class support vector machine and cross-validated on other databases. Performance measured in terms of EER for detection of spoof face is found to be $$3.8291\%$$ with proposed random scan features, and $$2.02\%$$ with auto-population samples for inter database.},
  archive      = {J_PAAA},
  author       = {Katika, Balaji Rao and Karthik, Kannan},
  doi          = {10.1007/s10044-020-00875-8},
  journal      = {Pattern Analysis and Applications},
  month        = {11},
  number       = {4},
  pages        = {1735-1754},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Face anti-spoofing by identity masking using random walk patterns and outlier detection},
  volume       = {23},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). TimeFly algorithm: A novel behavior-inspired movie
recommendation paradigm. <em>PAAA</em>, <em>23</em>(4), 1727–1734. (<a
href="https://doi.org/10.1007/s10044-020-00883-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes a novel behavior-inspired recommendation algorithm named TimeFly algorithm, which works on the idea of altering behavior of the user with respect to time. The proposed model considers solving two recommendation problems (fluctuating user interest over time and high computation time when dataset shifts from scarcity to abundance) and presents a real application of the proposed method in the field of recommendation engine. It describes a system which enrolls the changing behavior of user to furnish personalization suggestions. The results obtained by TimeFly are compared with the results of other well-known algorithms. Simulation results on 100K, 1M, 10M, and 20M MovieLens dataset reveal that using TimeFly leads to high accurate predictions in less computation time.},
  archive      = {J_PAAA},
  author       = {Sinha, Bam Bahadur and Dhanalakshmi, R. and Regmi, Ramchandra},
  doi          = {10.1007/s10044-020-00883-8},
  journal      = {Pattern Analysis and Applications},
  month        = {11},
  number       = {4},
  pages        = {1727-1734},
  shortjournal = {Pattern Anal. Appl.},
  title        = {TimeFly algorithm: A novel behavior-inspired movie recommendation paradigm},
  volume       = {23},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). An instance and variable selection approach in pixel-based
classification for automatic white blood cells segmentation.
<em>PAAA</em>, <em>23</em>(4), 1709–1726. (<a
href="https://doi.org/10.1007/s10044-020-00873-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Instance and variable selection involve identifying a subset of instances and variables such that the learning process will use only this subset with better performances and lower cost. Due to the huge amount of data available in many fields, data reduction is considered as an NP-hard problem. In this paper, we present a simultaneous instance and variable selection approach based on the Random Forest-RI ensemble methods in the aim to discard noisy and useless information from the original data set. We proposed a selection principle based on two concepts: the ensemble margin and the importance variable measure of Random Forest-RI. Experiments were conducted on cytological images for the automatic segmentation and recognition of white blood cells WBC (nucleus and cytoplasm). Moreover, in order to explore the performance of our proposed approach, experiments were carried out on standardized datasets from UCI and ASU repository, and the obtained results of the instances and variable selection by the Random Forest classifier are very encouraging.},
  archive      = {J_PAAA},
  author       = {Settouti, Nesma and Saidi, Meryem and Bechar, Mohammed El Amine and El Habib Daho, Mostafa and Chikh, Mohamed Amine},
  doi          = {10.1007/s10044-020-00873-w},
  journal      = {Pattern Analysis and Applications},
  month        = {11},
  number       = {4},
  pages        = {1709-1726},
  shortjournal = {Pattern Anal. Appl.},
  title        = {An instance and variable selection approach in pixel-based classification for automatic white blood cells segmentation},
  volume       = {23},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Cost-sensitive sample shifting in feature space.
<em>PAAA</em>, <em>23</em>(4), 1689–1707. (<a
href="https://doi.org/10.1007/s10044-020-00890-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The asymmetry of different misclassification costs is a common problem in many realistic applications. As one of the most familiar preprocessing methods, cost-sensitive resampling has drawn great attention due to its easy-implemented and universal properties. However, current methods mainly concentrate on changing the amount of the training set, which will alter the original distribution shapes and lead to the classifiers be over-fitted or unstable. For this case, a new method named cost-sensitive kernel shifting is proposed. The training data are remapped from the input space to the feature space by a particular kernel function, in which a distance metric is defined. Then the outliers are eliminated and the informative samples, including border and edge samples are selected due to the neighbor and geometrical information in the mapped space. Thirdly the positions of all the selected samples in the feature space are shifted. A moving step length is defined in proportion to both the ratio and different of the misclassification costs. In all steps only the kernel matrix is needed to be reshaped due to the kernel trick. Experiments on both synthetic and public datasets verify the effectiveness of the proposed methods.},
  archive      = {J_PAAA},
  author       = {Zhao, Zhenchong and Wang, Xiaodan and Wu, Chongming and Lei, Lei},
  doi          = {10.1007/s10044-020-00890-9},
  journal      = {Pattern Analysis and Applications},
  month        = {11},
  number       = {4},
  pages        = {1689-1707},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Cost-sensitive sample shifting in feature space},
  volume       = {23},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Margin setting algorithm for pattern classification via
spheres. <em>PAAA</em>, <em>23</em>(4), 1677–1688. (<a
href="https://doi.org/10.1007/s10044-020-00888-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Margin setting algorithm (MSA) is a new sphere-based classification algorithm. It employs an artificial immune system approach to construct a number of hyperspheres that cover each class of a given set of data. To gain insights into the classification performance of MSA, it is the first work to analyze two important fundamental problems of MSA as a sphere-based classifier. First, single sphere or multiple spheres are needed to achieve good classification performance in MSA? This problem was presented as sphere analysis, which was experimentally carried out on simulation data sets using Monte Carlo method. The results demonstrated that MSA employs a multiple-sphere strategy instead of one-sphere strategy as its decision boundaries. This strategy allows MSA to achieve lower probabilities of classification error rate. Second, how to adapt the location and size of the hypersphere to achieve good classification performance? This problem was presented as adaption analysis, which was experimentally carried out on real-world data sets compared to the support vector machine and the artificial neural network. The results demonstrated that MSA employs an artificial immune system approach to optimize the locations of the hyperspheres and to shrink the radius of the hypersphere in a certain range using margin as an algorithm parameter. Overall, computational results indicate the advantages of MSA in classification performance.},
  archive      = {J_PAAA},
  author       = {Wang, Yi and Pan, W. David and Fu, Jian and Wei, Bingyang},
  doi          = {10.1007/s10044-020-00888-3},
  journal      = {Pattern Analysis and Applications},
  month        = {11},
  number       = {4},
  pages        = {1677-1688},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Margin setting algorithm for pattern classification via spheres},
  volume       = {23},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Unsupervised visual domain adaptation via discriminative
dictionary evolution. <em>PAAA</em>, <em>23</em>(4), 1665–1675. (<a
href="https://doi.org/10.1007/s10044-020-00881-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This work focuses on unsupervised visual domain adaptation which is still challenging in visual recognition. Most of the attention has been dedicated to seeking the domain-invariant features of cross-domain data, but they ignores the valuable discriminative information in the source domain. In this paper, we propose a Discriminative Dictionary Evolution (DDE) approach to seek discriminative features robust to domain shift. Specifically, DDE gradually adapts a discriminative dictionary learned from the source domain to the target domain through a dictionary evolving procedure, in which self-selected atoms of the dictionary are updated with $$\ell _{2,1}$$ -norm-based regularization. DDE produces domain-invariant representations for cross-domain visual recognition meanwhile promotes the discriminativeness of the dictionary. Empirical results on real-world data sets demonstrate the advantages of the proposed approach over existing competitive methods.},
  archive      = {J_PAAA},
  author       = {Wu, Songsong and Gao, Guangwei and Li, Zuoyong and Wu, Fei and Jing, Xiao-Yuan},
  doi          = {10.1007/s10044-020-00881-w},
  journal      = {Pattern Analysis and Applications},
  month        = {11},
  number       = {4},
  pages        = {1665-1675},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Unsupervised visual domain adaptation via discriminative dictionary evolution},
  volume       = {23},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). ASCRClu: An adaptive subspace combination and reduction
algorithm for clustering of high-dimensional data. <em>PAAA</em>,
<em>23</em>(4), 1651–1663. (<a
href="https://doi.org/10.1007/s10044-020-00884-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The curse of dimensionality in high-dimensional data is one of the major challenges in data clustering. Recently, a considerable amount of literature has been published on subspace clustering to address this challenge. The main objective of the subspace clustering is to discover clusters embedded in any possible combination of the attributes. Previous studies have mostly been generating redundant subspace clusters, leading to clustering accuracy loss and also increasing the running time. In this paper, a bottom-up density-based approach is proposed for clustering of high-dimensional data. We employ the cluster structure as a similarity measure to generate the optimal subspaces which result in raising the accuracy of the subspace clustering. Using this idea, we propose an iterative algorithm to discover similar subspaces using the similarity in the features of subspaces. At each iteration of this algorithm, it first determines similar subspaces, then combines them to generate higher-dimensional subspaces, and finally re-clusters the subspaces. The algorithm repeats these steps and converges to the final clusters. Experiments on various synthetic and real datasets show that the results of the proposed approach are significantly better in both quality and runtime comparing to the state of the art on clustering high-dimensional data. The accuracy of the proposed method is around 34% higher than the CLIQUE algorithm and around 6% higher than DiSH.},
  archive      = {J_PAAA},
  author       = {Fatehi, Kavan and Rezvani, Mohsen and Fateh, Mansoor},
  doi          = {10.1007/s10044-020-00884-7},
  journal      = {Pattern Analysis and Applications},
  month        = {11},
  number       = {4},
  pages        = {1651-1663},
  shortjournal = {Pattern Anal. Appl.},
  title        = {ASCRClu: An adaptive subspace combination and reduction algorithm for clustering of high-dimensional data},
  volume       = {23},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Automatic plagiarism detection in obfuscated text.
<em>PAAA</em>, <em>23</em>(4), 1627–1650. (<a
href="https://doi.org/10.1007/s10044-020-00882-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Plagiarism is a serious problem in education, research, publishing and other fields. Automatic plagiarism detection systems are crucial for ensuring the integrity and genuineness of intellectual work. There are different types of plagiarism, such as copy–paste, obfuscation and translation. In particular, obfuscated text is one of the hardest types of plagiarism to detect. In this paper, we propose an automatic plagiarism detection system for obfuscated text based on a support vector machine classifier that exploits a set of lexical, syntactic and semantic features. We evaluated the performance of the proposed system on benchmark English and Arabic corpora made available by the PAN Workshop series: PAN 2012, PAN 2013, PAN 2014 and PAN@FIRE2015. We also compared the performance of our system to the performances of other systems that participated in the PAN competitions. The obtained results show that our system had the best performance in terms of the F-measure on the PAN 2012 and on the PAN@FIRE2015 obfuscated sub-corpora, was among the top four on the PAN 2013 corpus and was among the top two on the PAN 2014 corpus.},
  archive      = {J_PAAA},
  author       = {Altheneyan, Alaa Saleh and Menai, Mohamed El Bachir},
  doi          = {10.1007/s10044-020-00882-9},
  journal      = {Pattern Analysis and Applications},
  month        = {11},
  number       = {4},
  pages        = {1627-1650},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Automatic plagiarism detection in obfuscated text},
  volume       = {23},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Supervised and semi-supervised twin parametric-margin
regularized extreme learning machine. <em>PAAA</em>, <em>23</em>(4),
1603–1626. (<a
href="https://doi.org/10.1007/s10044-020-00880-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Twin extreme learning machine (TELM) has attracted considerable attention and achieved great success in the machine learning field. However, its performance will be severely affected when outliers exist in the dataset since TELM does not consider heteroscedasticity in practical applications. To improve the performance of TELM, a novel learning framework called twin parametric-margin extreme learning machine (TPMELM) was proposed. Further, to enhance the classification performance of our TPMELM in a semi-supervised learning setting, a Laplacian TPMELM (Lap-TPMELM) was developed by introducing manifold regularization into TPMELM. Using the geometric information of the marginal distribution embedded in unlabeled samples, Lap-TPMELM can effectively construct a more reasonable classifier. The TPMELM and Lap-TPMELM are suitable for many situations, especially when the data has heteroscedastic error structure. Moreover, the TPMELM and Lap-TPMELM are helpful in clarifying theoretical interpretation of parameters which control the bounds on proportions of support vectors and boundary errors. An efficient technique (successive over-relaxation, SOR) is applied in TPMELM and Lap-TPMELM, respectively. Experimental results show the effectiveness and reliability of the proposed methods.},
  archive      = {J_PAAA},
  author       = {Ma, Jun},
  doi          = {10.1007/s10044-020-00880-x},
  journal      = {Pattern Analysis and Applications},
  month        = {11},
  number       = {4},
  pages        = {1603-1626},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Supervised and semi-supervised twin parametric-margin regularized extreme learning machine},
  volume       = {23},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Multi-view region-adaptive multi-temporal DMM and RGB action
recognition. <em>PAAA</em>, <em>23</em>(4), 1587–1602. (<a
href="https://doi.org/10.1007/s10044-020-00886-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Human action recognition remains an important yet challenging task. This work proposes a novel action recognition system. It uses a novel multi-view region-adaptive multi-resolution-in-time depth motion map (MV-RAMDMM) formulation combined with appearance information. Multi-stream 3D convolutional neural networks (CNNs) are trained on the different views and time resolutions of the region-adaptive depth motion maps. Multiple views are synthesised to enhance the view invariance. The region-adaptive weights, based on localised motion, accentuate and differentiate parts of actions possessing faster motion. Dedicated 3D CNN streams for multi-time resolution appearance information are also included. These help to identify and differentiate between small object interactions. A pre-trained 3D-CNN is used here with fine-tuning for each stream along with multi-class support vector machines. Average score fusion is used on the output. The developed approach is capable of recognising both human action and human–object interaction. Three public-domain data-sets, namely MSR 3D Action, Northwestern UCLA multi-view actions and MSR 3D daily activity, are used to evaluate the proposed solution. The experimental results demonstrate the robustness of this approach compared with state-of-the-art algorithms.},
  archive      = {J_PAAA},
  author       = {Al-Faris, Mahmoud and Chiverton, John P. and Yang, Yanyan and Ndzi, David},
  doi          = {10.1007/s10044-020-00886-5},
  journal      = {Pattern Analysis and Applications},
  month        = {11},
  number       = {4},
  pages        = {1587-1602},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Multi-view region-adaptive multi-temporal DMM and RGB action recognition},
  volume       = {23},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Retrieval of colour and texture images using local
directional peak valley binary pattern. <em>PAAA</em>, <em>23</em>(4),
1569–1585. (<a
href="https://doi.org/10.1007/s10044-020-00879-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many content-based image retrieval (CBIR) methods are being developed to store more and more information about images in shorter feature vectors and to improve image retrieval rate. In the proposed method, two-step approach to CBIR has been developed. The first step generates an image mask from local binary pattern (LBP). This LBP mask is then utilized to draw comparison between the centre pixel and the eight surrounding pixels. The second step involves drawing the peak and valley patterns of local directional binary pattern for each image which is then combined with the colour histogram to retrieve similar images. Existing methods suffer from lower average image retrieval accuracy even with larger feature vectors. The proposed method overcomes such problems through shorter feature vectors that can store more information about the image. As illustrated through experimental results, the proposed method produces promising results with shorter feature vector of length 56 and improved image retrieval rate of about 5–10%. Our method outperforms similar techniques when tested with public data sets.},
  archive      = {J_PAAA},
  author       = {Gupta, Srishti and Roy, Partha Pratim and Dogra, Debi Prosad and Kim, Byung-Gyu},
  doi          = {10.1007/s10044-020-00879-4},
  journal      = {Pattern Analysis and Applications},
  month        = {11},
  number       = {4},
  pages        = {1569-1585},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Retrieval of colour and texture images using local directional peak valley binary pattern},
  volume       = {23},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Accurate quaternion radial harmonic fourier moments for
color image reconstruction and object recognition. <em>PAAA</em>,
<em>23</em>(4), 1551–1567. (<a
href="https://doi.org/10.1007/s10044-020-00877-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Orthogonal moments have become a powerful tool for object representation and image analysis. Radial harmonic Fourier moments (RHFMs) are one of such image descriptors based on a set of orthogonal projection bases, which outperform other moments because of their computational efficiency. However, the conventional computational framework of RHFMs produces geometric error and numerical integration error, which will affect the accuracy of RHFMs, thus degrading the image reconstruction performance. To overcome this shortcoming, we propose a new computational framework of RHFMs, namely accurate quaternion radial harmonic Fourier moments (AQRHFMs), for color image processing, and also analyze the properties of AQRHFMs. Firstly, we propose a precise computation method of RHFMs to reduce the geometric and numerical errors. Secondly, by using the algebra of quaternions, we extend the accurate RHFMs to AQRHFMs in order to deal with the color images in a holistic manner. Experimental results show the proposed AQRHFMs achieve promising performance in image reconstruction and object recognition in both noise-free and noisy conditions.},
  archive      = {J_PAAA},
  author       = {Liu, Yunan and Zhang, Shanshan and Li, Guangyu and Wang, Houjun and Yang, Jian},
  doi          = {10.1007/s10044-020-00877-6},
  journal      = {Pattern Analysis and Applications},
  month        = {11},
  number       = {4},
  pages        = {1551-1567},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Accurate quaternion radial harmonic fourier moments for color image reconstruction and object recognition},
  volume       = {23},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Incremental one-class classifier based on convex–concave
hull. <em>PAAA</em>, <em>23</em>(4), 1523–1549. (<a
href="https://doi.org/10.1007/s10044-020-00876-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {One subject that has been considered less is a binary classification on data streams with concept drifting in which only information of one class (target class) is available for learning. Well-known methods such as SVDD and convex hull have tried to find the enclosed boundary around target class, but their high complexity makes them unsuitable for large data sets and also online tasks. This paper presents a novel online one-class classifier adapted to the streaming data. Considering time complexity, an incremental convex–concave hull classification method, called ICCHC, is proposed which can significantly reduce the computational time and expand the target class boundary. Also, it can be adapted to the gradual concept drift. Evaluations have been conducted on seventeen real-world data sets by hold-out validation. Also, noise analysis has been carried out. The results of the experiments have been compared with the state-of-the-art methods, which show the superiority of ICCHC regarding the accuracy, precision, and recall metrics.},
  archive      = {J_PAAA},
  author       = {Hamidzadeh, Javad and Moradi, Mona},
  doi          = {10.1007/s10044-020-00876-7},
  journal      = {Pattern Analysis and Applications},
  month        = {11},
  number       = {4},
  pages        = {1523-1549},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Incremental one-class classifier based on convex–concave hull},
  volume       = {23},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Local binary pattern-based on-road vehicle detection in
urban traffic scene. <em>PAAA</em>, <em>23</em>(4), 1505–1521. (<a
href="https://doi.org/10.1007/s10044-020-00874-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For intelligent traffic monitoring systems and related applications, detecting vehicles on roads is a vital step. However, robust and efficient vehicles detection is still a challenging problem due to variations in the appearance of the vehicles and complicated background of the roads. In this paper, we propose a simple and effective vehicle detection method based on local vehicle&#39;s texture and appearance histograms feed into clustering forests. The interdependency of vehicle&#39;s parts locations is incorporating within a clustering forests framework. Local binary pattern-like descriptors are utilized for texture feature extraction. Through utilizing the LBP descriptors, the local structures of vehicles, such as edge, contour and flat region can be effectively depicted. The align set of histograms generated concurrence with LBPs spatial for random sampled local regions are used to measure the dissimilarity between regions of all training images. Evaluating the fit between histograms is built in clustering forests. That is, clustering discriminative codebooks of latent features are used to search between different LBP features of the random regions utilizing the Chi-square dissimilarity measure. Besides, saliency maps built by the learnt latent features are adopted to determine the vehicles locations in test image. Effectiveness of the proposed method is evaluated on different car datasets stressing various imaging conditions and the obtained results show that the method achieves significant improvements compared to published methods.},
  archive      = {J_PAAA},
  author       = {Hassaballah, M. and Kenk, Mourad A. and El-Henawy, Ibrahim M.},
  doi          = {10.1007/s10044-020-00874-9},
  journal      = {Pattern Analysis and Applications},
  month        = {11},
  number       = {4},
  pages        = {1505-1521},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Local binary pattern-based on-road vehicle detection in urban traffic scene},
  volume       = {23},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). PathQuery pregel: High-performance graph query with bulk
synchronous processing. <em>PAAA</em>, <em>23</em>(3), 1493–1504. (<a
href="https://doi.org/10.1007/s10044-019-00841-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {High-performance graph query systems are a scalable way to mine information in Knowledge Graphs, especially when the queries benefit from a high-level expressive query language. This paper presents techniques to algorithmically compile queries expressed in a high-level language (e.g., Datalog) into a directed acyclic graph query plan and details how these queries can be run on a Pregel graph vertex-centric compute system. Our solution, called PathQuery Pregel, creates plans for any conjunctive or disjunctive queries with aggregation and negation; we describe how the query execution extracts graph results optimally while avoiding many join operations where parallel map execution is permitted. We provide details of how we scaled this system out to execute large set of queries in parallel over the Google Knowledge Graph, a graph of 70B edges, or facts; we describe our production experience with PathQuery Pregel.},
  archive      = {J_PAAA},
  author       = {Arsintescu, Bogdan and Deo, Shardul and Harris, Warren},
  doi          = {10.1007/s10044-019-00841-z},
  journal      = {Pattern Analysis and Applications},
  month        = {8},
  number       = {3},
  pages        = {1493-1504},
  shortjournal = {Pattern Anal. Appl.},
  title        = {PathQuery pregel: High-performance graph query with bulk synchronous processing},
  volume       = {23},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Finite point sets in recognizing location and orientation of
machine parts of complex shapes. <em>PAAA</em>, <em>23</em>(3),
1479–1491. (<a
href="https://doi.org/10.1007/s10044-019-00850-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the article, a method of recognizing the position, location and orientation of irregular machine parts with a complex outline of the external contour is suggested. Recognition is performed on the basis of a raster image that has undergone preliminary processing in a vision system. The developed method is based on matching the finite set of points retrieved from the external contour of the classified object. The originality of the approach consists of the form of the object pattern taken into account in determining the degree of similarity to reference objects. The similarity index was defined in the category of fuzzy sets. A cellular automaton has been proposed to generate the outline of the perimeter quickly while simultaneously smoothing it out. The process chain necessary for the recognition function to be implemented has been presented. The developed method has been illustrated with an example related to recognizing the position, location and orientation of a rear wheel pin. The results were compared to the classic Blair-Bliss, Danielsson and Haralick shape coefficients. The sensitivity of the developed method to a change in the scale and rotating the object as well as errors in outlining its edge has been tested. The obtained results confirmed the advantageous features of the developed method, both in the aspect of the recognition quality and the practically viable time of waiting for the results of processing.},
  archive      = {J_PAAA},
  author       = {Stryczek, Roman},
  doi          = {10.1007/s10044-019-00850-y},
  journal      = {Pattern Analysis and Applications},
  month        = {8},
  number       = {3},
  pages        = {1479-1491},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Finite point sets in recognizing location and orientation of machine parts of complex shapes},
  volume       = {23},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Customs fraud detection. <em>PAAA</em>, <em>23</em>(3),
1457–1477. (<a
href="https://doi.org/10.1007/s10044-019-00852-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this customs fraud detection application, we analyse a unique data set of 9,624,124 records resulting from a collaboration with the Belgian customs administration. They are faced with increasing levels of international trade, which pressurizes regulatory control. Governments therefore rely on data mining to focus their limited resources on the most likely fraud cases. The literature on data mining for customs fraud detection lacks in two main directions that are simultaneously addressed in this paper: (1) behavioural and high-cardinality data types are neglected due to a lack of methodology to include them. We demonstrate that such fine-grained features (e.g. the specific entities such as consignee, consignor and declarant and the commodities involved in a declaration) are very predictive. (2) Studies in the tax domain most often use standard learning algorithms on their fraud detection applications. However, customs data are highly imbalanced and this poses challenges for many inducers. We present a new EasyEnsemble method that integrates a support vector machine base learner in a confidence-rated boosting algorithm. This results in a fast and scalable learner that is able to drastically improve predictive performance over the base application of a support vector machine. The results of our proposed framework reveals high AUC and lift values that translate into an immediate impact on the customs fraud detection domain through an improved retrieval of tax losses and an enhanced deterrence.},
  archive      = {J_PAAA},
  author       = {Vanhoeyveld, Jellis and Martens, David and Peeters, Bruno},
  doi          = {10.1007/s10044-019-00852-w},
  journal      = {Pattern Analysis and Applications},
  month        = {8},
  number       = {3},
  pages        = {1457-1477},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Customs fraud detection},
  volume       = {23},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Robust object tracking with crow search optimized multi-cue
particle filter. <em>PAAA</em>, <em>23</em>(3), 1439–1455. (<a
href="https://doi.org/10.1007/s10044-019-00847-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Particle filter is used extensively for estimation of target nonlinear and non-Gaussian state. However, its performance suffers due to its inherent problem of sample degeneracy and impoverishment. In order to address this, we propose a novel resampling method based upon crow search optimization to overcome low performing particles detected as the outlier. Proposed outlier detection mechanism with transductive reliability achieves faster convergence of the proposed PF tracking framework. In addition, we present an adaptive fusion model to integrate multi-cue extracted for each evaluated particle. Automatic boosting and suppression of particles using the proposed fusion model not only enhance the performance of the resampling method but also achieve optimal state estimation. Performance of the proposed tracker has been evaluated over benchmark video sequences and compared with state-of-the-art solutions. Qualitative and quantitative results reveal that the proposed tracker not only outperforms existing solutions but also efficiently handles various tracking challenges. On average of the outcome, we achieve CLE of 10.99 and F measure of 0.683.},
  archive      = {J_PAAA},
  author       = {Walia, Gurjit Singh and Kumar, Ashish and Saxena, Astitwa and Sharma, Kapil and Singh, Kuldeep},
  doi          = {10.1007/s10044-019-00847-7},
  journal      = {Pattern Analysis and Applications},
  month        = {8},
  number       = {3},
  pages        = {1439-1455},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Robust object tracking with crow search optimized multi-cue particle filter},
  volume       = {23},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Learning discriminative hashing codes for cross-modal
retrieval based on multi-view features. <em>PAAA</em>, <em>23</em>(3),
1421–1438. (<a
href="https://doi.org/10.1007/s10044-020-00870-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hashing techniques have been applied broadly in retrieval tasks due to their low storage requirements and high speed of processing. Many hashing methods based on a single view have been extensively studied for information retrieval. However, the representation capacity of a single view is insufficient and some discriminative information is not captured, which results in limited improvement. In this paper, we employ multiple views to represent images and texts for enriching the feature information. Our framework exploits the complementary information among multiple views to better learn the discriminative compact hash codes. A discrete hashing learning framework that jointly performs classifier learning and subspace learning is proposed to complete multiple search tasks simultaneously. Our framework includes two stages, namely a kernelization process and a quantization process. Kernelization aims to find a common subspace where multi-view features can be fused. The quantization stage is designed to learn discriminative unified hashing codes. Extensive experiments are performed on single-label datasets (WiKi and MMED) and multi-label datasets (MIRFlickr and NUS-WIDE), and the experimental results indicate the superiority of our method compared with the state-of-the-art methods.},
  archive      = {J_PAAA},
  author       = {Yu, Jun and Wu, Xiao-Jun and Kittler, Josef},
  doi          = {10.1007/s10044-020-00870-z},
  journal      = {Pattern Analysis and Applications},
  month        = {8},
  number       = {3},
  pages        = {1421-1438},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Learning discriminative hashing codes for cross-modal retrieval based on multi-view features},
  volume       = {23},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Separable property-based super-resolution of lousy image
data. <em>PAAA</em>, <em>23</em>(3), 1407–1420. (<a
href="https://doi.org/10.1007/s10044-019-00854-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a novel wavelet-based approach for single-image super-resolution. Our technique integrates wavelet transform and the learned locally regularized anchored neighborhood regression model for more robust frequency estimation and image restoration. First, we decomposed the low-resolution input image into four frequency sub-bands by applying discrete wavelet transform and then processed these frequency sub-bands based on separable property of neighborhood filtering to achieve a fast parallel and vectorized operation by reducing computational burden resulting from computing the weighted function of every pixel for each pixel in an image. We then applied inverse discrete wavelet transform to reconstruct the original image. Super-resolution is achieved using the learned model to predict the high-resolution image features. Lastly, we explicitly unified both the locality structure and nonlocal self-similarity properties in natural image and incorporated them into our super-resolution framework to regularize the nonlinear correlation between low-resolution and high-resolution space and improve the reconstructed results. Experiments on standard images validate the effectiveness of our proposed method for effective denoising, deblurring and super-resolution reconstruction tasks compared to other top performing state-of-the-art methods both quantitatively and qualitatively.},
  archive      = {J_PAAA},
  author       = {Ike, Chidiebere Somadina and Muhammad, Nazeer},
  doi          = {10.1007/s10044-019-00854-8},
  journal      = {Pattern Analysis and Applications},
  month        = {8},
  number       = {3},
  pages        = {1407-1420},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Separable property-based super-resolution of lousy image data},
  volume       = {23},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Leveraging deep learning with symbolic sequences for robust
head poses estimation. <em>PAAA</em>, <em>23</em>(3), 1391–1406. (<a
href="https://doi.org/10.1007/s10044-019-00857-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Head pose estimation is a challenging topic in computer vision with a large area of applications. There are a lot of methods which have been presented in the literature to undertake pose estimation so far. Even though the efficiency of these methods is acceptable, the sensitivity to external conditions is still being a big challenge. In this paper, we come up with a new model to overcome the problem of head poses estimation. First, the face images are converted into one-dimensional vectors as a time series using the Peano–Hilbert space-filling curve. Then, we convert these numerical series into symbolic sequences with adequate dimensionality reduction approaches. These sequences are then used as input of an encode–decoder neural network to learn and generate labels of the faces orientations. We have evaluated our model on several databases, and the experimental results have shown that the proposed method is very competitive compared to other well-known approaches.},
  archive      = {J_PAAA},
  author       = {Mekami, Hayet and Bounoua, Abdennacer and Benabderrahmane, Sidahmed},
  doi          = {10.1007/s10044-019-00857-5},
  journal      = {Pattern Analysis and Applications},
  month        = {8},
  number       = {3},
  pages        = {1391-1406},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Leveraging deep learning with symbolic sequences for robust head poses estimation},
  volume       = {23},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Learning a representation with the block-diagonal structure
for pattern classification. <em>PAAA</em>, <em>23</em>(3), 1381–1390.
(<a href="https://doi.org/10.1007/s10044-019-00858-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sparse-representation-based classification (SRC) has been widely studied and developed for various practical signal classification applications. However, the performance of a SRC-based method is degraded when both the training and test data are corrupted. To counteract this problem, we propose an approach that learns representation with block-diagonal structure (RBDS) for robust image recognition. To be more specific, we first introduce a regularization term that captures the block-diagonal structure of the target representation matrix of the training data. The resulting problem is then solved by an optimizer. Last, based on the learned representation, a simple yet effective linear classifier is used for the classification task. The experimental results obtained on several benchmarking datasets demonstrate the efficacy of the proposed RBDS method. The source code of our proposed RBDS is accessible at https://github.com/yinhefeng/RBDS.},
  archive      = {J_PAAA},
  author       = {Yin, He-Feng and Wu, Xiao-Jun and Kittler, Josef and Feng, Zhen-Hua},
  doi          = {10.1007/s10044-019-00858-4},
  journal      = {Pattern Analysis and Applications},
  month        = {8},
  number       = {3},
  pages        = {1381-1390},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Learning a representation with the block-diagonal structure for pattern classification},
  volume       = {23},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Learning multi-scale features for foreground segmentation.
<em>PAAA</em>, <em>23</em>(3), 1369–1380. (<a
href="https://doi.org/10.1007/s10044-019-00845-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Foreground segmentation algorithms aim at segmenting moving objects from the background in a robust way under various challenging scenarios. Encoder–decoder-type deep neural networks that are used in this domain recently perform impressive segmentation results. In this work, we propose a variation of our formerly proposed method (Anonymous 2018) that can be trained end-to-end using only a few training examples. The proposed method extends the feature pooling module of FgSegNet by introducing fusion of features inside this module, which is capable of extracting multi-scale features within images, resulting in a robust feature pooling against camera motion, which can alleviate the need of multi-scale inputs to the network. Sample visualizations highlight the regions in the images on which the model is specially focused. It can be seen that these regions are also the most semantically relevant. Our method outperforms all existing state-of-the-art methods in CDnet2014 datasets by an average overall F-measure of 0.9847. We also evaluate the effectiveness of our method on SBI2015 and UCSD Background Subtraction datasets. The source code of the proposed method is made available at https://github.com/lim-anggun/FgSegNet_v2.},
  archive      = {J_PAAA},
  author       = {Lim, Long Ang and Keles, Hacer Yalim},
  doi          = {10.1007/s10044-019-00845-9},
  journal      = {Pattern Analysis and Applications},
  month        = {8},
  number       = {3},
  pages        = {1369-1380},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Learning multi-scale features for foreground segmentation},
  volume       = {23},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Segmentation of handwritten words using structured support
vector machine. <em>PAAA</em>, <em>23</em>(3), 1355–1367. (<a
href="https://doi.org/10.1007/s10044-019-00843-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Words and characters segmentation is a most indispensable and fundamental task for the handwritten script recognition. However, the complex language structures, deviation in pen breadth and slant in inscription make the feature extraction process very challenging. In this research, a binary quadratic process has been formulated for the word segmentation. It deliberates a co-relationship between the inter-word gap and intra-word gap. The structured support vector machine is used for the experiment. Experimental results of public datasets (i.e., ICDAR2009 and ICDAR2013) show state-of-the-art performance of the designed algorithm.},
  archive      = {J_PAAA},
  author       = {Sharma, Manoj Kumar and Dhaka, Vijaypal Singh},
  doi          = {10.1007/s10044-019-00843-x},
  journal      = {Pattern Analysis and Applications},
  month        = {8},
  number       = {3},
  pages        = {1355-1367},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Segmentation of handwritten words using structured support vector machine},
  volume       = {23},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Robust hand gesture recognition system based on a new set of
quaternion tchebichef moment invariants. <em>PAAA</em>, <em>23</em>(3),
1337–1353. (<a
href="https://doi.org/10.1007/s10044-020-00866-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hand gesture recognition is a challenging task due to the complexity of hand movements and to the variety among the same gesture performed by distinct subjects. Recent technologies, such as Kinect sensor, provide new opportunities, allowing to capture both RGB and depth (RGB-D) images, which offer high discriminant information for efficient hand gesture recognition. In the aspect of feature extraction, the traditional methods process the RGB and depth information independently. In this paper, we propose a robust hand gesture recognition system based on a new feature extraction method, fusing RGB images and depth information simultaneously, by using the quaternion algebra that provide a more robust and holistical representation. In fact, we introduce, for the first time, a novel type of feature extraction method, named quaternion Tchebichef moment invariants. The novelty of the proposed method in this paper lies in the direct derivation of invariants from their orthogonal moments, based on the algebraic properties of the discrete Tchebichef polynomials. The proposed approach based on quaternion algebra is suggested to process the four components holistically, for a robust and efficient hand gesture recognition system. The obtained experimental and theoretical results demonstrate that the present approach is very effective for addressing the problem of hand gesture recognition and have proved its robustness against geometrical distortion, noisy conditions and complex background compared to the state of the art, indicating that it could be highly useful for many computer vision applications.},
  archive      = {J_PAAA},
  author       = {Elouariachi, Ilham and Benouini, Rachid and Zenkouar, Khalid and Zarghili, Arsalane},
  doi          = {10.1007/s10044-020-00866-9},
  journal      = {Pattern Analysis and Applications},
  month        = {8},
  number       = {3},
  pages        = {1337-1353},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Robust hand gesture recognition system based on a new set of quaternion tchebichef moment invariants},
  volume       = {23},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Learning CNN features from DE features for EEG-based emotion
recognition. <em>PAAA</em>, <em>23</em>(3), 1323–1335. (<a
href="https://doi.org/10.1007/s10044-019-00860-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, deep neural networks (DNNs) have shown the remarkable success of feature representations in computer vision, audio analysis, and natural language processing. Furthermore, DNNs have been used for electroencephalography (EEG) signal classification in recent studies on brain–computer interface. However, most works use one-dimensional EEG features to learn DNNs that ignores the local information within multichannel or multiple frequency bands in the EEG signals. In this paper, we propose a novel emotion recognition method using a convolutional neural network (CNN) while preventing the loss of local information. The proposed method consists of two parts. The first part generates topology-preserving differential entropy features while keeping the distance from the center electrode to other electrodes. The second part learns the proposed CNN to estimate three-class emotional states (positive, neutral, negative). We evaluate our work on SEED dataset, including 62-channel EEG signals recorded from 15 subjects. Our experimental results demonstrate that the proposed method achieved superior performance on SEED dataset with an average accuracy of 90.41% with the visualization of extracted features from the proposed CNN using t-SNE to show our representation outperforms the other representations based on standard features for EEG analysis. Besides, with the additional experiment on VIG dataset to estimate the vigilance of EEG dataset, we show the off-the-shelf availability of the proposed method.},
  archive      = {J_PAAA},
  author       = {Hwang, Sunhee and Hong, Kibeom and Son, Guiyoung and Byun, Hyeran},
  doi          = {10.1007/s10044-019-00860-w},
  journal      = {Pattern Analysis and Applications},
  month        = {8},
  number       = {3},
  pages        = {1323-1335},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Learning CNN features from DE features for EEG-based emotion recognition},
  volume       = {23},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A novel approach for scene text extraction from synthesized
hazy natural images. <em>PAAA</em>, <em>23</em>(3), 1305–1322. (<a
href="https://doi.org/10.1007/s10044-019-00855-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The most important intricacy when processing natural scene text images is the existence of fog, smoke or haze. These intrusion elements decrease the contrast and disrupt the color fidelity of the image for various computer vision applications. In this paper, such a challenging issue is addressed. The intended work presents a novel method, that is, single image dehazing, based on transmission map. The contributions are performed in the following ways: (1) text extraction from hazy image is not straightforward due to lack of haze-free images and hazy images. To address this limitation, we introduce synthetic natural scene text image composed of pairs of synthetic hazy and corresponding haze-free images using mainstream datasets. Different from existing dehazing datasets, text in hazy images is considered compulsory content, which needs to be separated from background using the recovered image. For doing this, based on transmission map the scenic depth is calculated using haze density and color attenuation to generate depth map. In the next step, raw transmission map is computed, which is further refined using bilateral filtering to preserve edges and avoid possible noise; (2) text region proposals are estimated on the restored images using novel low-level connected component technique and character bounding is employed to complete the process. Finally, the experimentations are carried out on the images selected from standard datasets including MSRA-TD500, SVT and KAIST. The experimental outcomes demonstrate that the intended method performs better when compared with benchmark standard techniques and publically available dehazing datasets.},
  archive      = {J_PAAA},
  author       = {Ansari, Ghulam Jillani and Shah, Jamal Hussain and Sharif, Muhammad and Rehman, Saeed ur},
  doi          = {10.1007/s10044-019-00855-7},
  journal      = {Pattern Analysis and Applications},
  month        = {8},
  number       = {3},
  pages        = {1305-1322},
  shortjournal = {Pattern Anal. Appl.},
  title        = {A novel approach for scene text extraction from synthesized hazy natural images},
  volume       = {23},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A sparse linear regression model for incomplete datasets.
<em>PAAA</em>, <em>23</em>(3), 1293–1303. (<a
href="https://doi.org/10.1007/s10044-019-00859-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Incomplete data are often neglected when designing machine learning methods. A popular strategy adopted by practitioners to circumvent this consists of taking a preprocessing step to fill the missing components. These preprocessing algorithms are designed independently of the machine learning method that will be applied subsequently, which may lead to sub-optimal results. An alternative solution is to redesign classical machine learning methods to handle missing data directly. In this paper, we propose a variant of the forward stagewise regression (FSR) algorithm for incomplete data. The original FSR is an iterative procedure to estimate parameters of sparse linear models. The proposed method, named forward stagewise regression for incomplete datasets with GMM (FSIG), models the missing components as random variables following a Gaussian mixture distribution. In FSIG, the main steps of FSR are adapted to deaç with the intrinsic uncertainty of incomplete samples. The performance of FSIG was evaluated in an extensive set of experiments, and our model was able to outperform classical methods in most of the tested cases.},
  archive      = {J_PAAA},
  author       = {Veras, Marcelo B. A. and Mesquita, Diego P. P. and Mattos, Cesar L. C. and Gomes, João P. P.},
  doi          = {10.1007/s10044-019-00859-3},
  journal      = {Pattern Analysis and Applications},
  month        = {8},
  number       = {3},
  pages        = {1293-1303},
  shortjournal = {Pattern Anal. Appl.},
  title        = {A sparse linear regression model for incomplete datasets},
  volume       = {23},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Adaptive detection of FOREX repetitive chart patterns.
<em>PAAA</em>, <em>23</em>(3), 1277–1292. (<a
href="https://doi.org/10.1007/s10044-019-00862-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The global financial ecosystem has evolved and matured along with the ever-changing world economy that grew increasingly complicated due to globalisation. As traders are often inundated with information from various sources when formulating trading strategies, numerous analysis methods have been developed to ease the decision-making process. However, factors such as prior experience and knowledge of the trader as well as various psychological factors often influence the final trading decision. Focusing on charting-based analysis, it still suffers from drawbacks due to the time-warping properties of the chart patterns and the reliance on a large number of pre-defined chart patterns. Hence, in order to address the gaps within the FOREX research, the paper endeavours to propose a novel chart detection algorithm. The auto-segmentation implementation within the algorithm utilises piecewise linear regression to detect chart patterns within the FOREX historical data. By successfully extracting the repetitive chart patterns and subsequently establishing its similarities using Agglomerative Hierarchical Clustering, the information provided could potentially be used to assist traders in solidifying their investment decisions. The experimental results obtained show that repetitive chart patterns can indeed be successfully detected and extracted from the FOREX historical data.},
  archive      = {J_PAAA},
  author       = {Yong, Yoke Leng and Lee, Yunli and Ngo, David Chek Ling},
  doi          = {10.1007/s10044-019-00862-8},
  journal      = {Pattern Analysis and Applications},
  month        = {8},
  number       = {3},
  pages        = {1277-1292},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Adaptive detection of FOREX repetitive chart patterns},
  volume       = {23},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Densely connected network for impulse noise removal.
<em>PAAA</em>, <em>23</em>(3), 1263–1275. (<a
href="https://doi.org/10.1007/s10044-020-00871-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, a new convolutional neural network (CNN) architecture, dubbed as densely connected convolutional network (DenseNet), has shown excellent results on image classification tasks. The idea of DenseNet is based on the observation that if each layer is directly connected to every other layer in a feed-forward fashion, then the network will be more accurate and easier to train. In this study, we extend DenseNet to deal with the problem of impulse noise reduction. It aims to explore the densely connected network for impulse noise removal (DNINR), which utilizes CNN to learn pixel-distribution features from noisy images. Compared with the traditional median filter-based and variational regularization methods that utilize the spatial neighbor information around the pixels and optimize in an iterative manner, it is more efficient to capture multi-scale contextual information and directly tackles the original image. Additionally, DNINR turns to capture the pixel-level distribution information by means of wide and transformed network learning. In terms of edge preservation and noise suppression, the proposed DNINR consistently achieved significantly superior performance, which is better than current state-of-the-art methods, particularly at extremely high noise levels.},
  archive      = {J_PAAA},
  author       = {Li, Guanyu and Xu, Xiaoling and Zhang, Minghui and Liu, Qiegen},
  doi          = {10.1007/s10044-020-00871-y},
  journal      = {Pattern Analysis and Applications},
  month        = {8},
  number       = {3},
  pages        = {1263-1275},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Densely connected network for impulse noise removal},
  volume       = {23},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). An over-regression suppression method to discriminate
occluded objects of same category. <em>PAAA</em>, <em>23</em>(3),
1251–1261. (<a
href="https://doi.org/10.1007/s10044-019-00853-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Occlusion is a key challenge in object detection. It is hard to discriminate objects accurately when they gather together and occlude each other, especially when they belong to same category which easily leads to the problem that multiple objects are regressed into the same bounding box. To address this problem, an over-regression suppression (ORS) method is proposed to take full advantage of supervised information. Firstly, annotated information is utilized to compute the overlaps between different ground truth boxes. Then, the regression loss function is redesigned by adding a penalty term which is associated with the aforementioned overlaps to prevent Over-regression. Finally, the validity of the algorithm is proved by making some changes in Faster R-CNN, in which a k-means ++ clustering algorithm is used to automatically generate various size anchors by learning the shape regularities of objects from dataset, and the Soft-NMS, a nearly cost-free method, is introduced to replace the traditional NMS. Extensive evaluations on the challenging PASCAL VOC and MS COCO benchmarks demonstrate the superiority of ORS in handling intra-class occlusion. Its performance increases when dataset contains more large objects and hard samples, as demonstrated by the results on the MS COCO dataset.},
  archive      = {J_PAAA},
  author       = {Zhao, Bin and Wang, Chunping and Fu, Qiang},
  doi          = {10.1007/s10044-019-00853-9},
  journal      = {Pattern Analysis and Applications},
  month        = {8},
  number       = {3},
  pages        = {1251-1261},
  shortjournal = {Pattern Anal. Appl.},
  title        = {An over-regression suppression method to discriminate occluded objects of same category},
  volume       = {23},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Distributed learning automata-based s-learning scheme for
classification. <em>PAAA</em>, <em>23</em>(3), 1235–1250. (<a
href="https://doi.org/10.1007/s10044-019-00848-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes a novel classifier based on the theory of Learning Automata (LA), reckoned to as PolyLA. The essence of our scheme is to search for a separator in the feature space by imposing an LA-based random walk in a grid system. To each node in the grid, we attach an LA whose actions are the choices of the edges forming a separator. The walk is self-enclosing, and a new random walk is started whenever the walker returns to the starting node forming a closed classification path yielding a many-edged polygon. In our approach, the different LA attached to the different nodes search for a polygon that best encircles and separates each class. Based on the obtained polygons, we perform classification by labeling items encircled by a polygon as part of a class using a ray casting function. From a methodological perspective, PolyLA has appealing properties compared to SVM. In fact, unlike PolyLA, the SVM performance is dependent on the right choice of the kernel function (e.g., linear kernel, Gaussian kernel)—which is considered a “black art.” PolyLA, on the other hand, can find arbitrarily complex separator in the feature space. We provide sound theoretical results that prove the optimality of the scheme. Furthermore, experimental results show that our scheme is able to perfectly separate both simple and complex patterns outperforming existing classifiers, such as polynomial and linear SVM, without the need to map the problem to many dimensions or to introduce a “kernel trick.” We believe that the results are impressive, given the simplicity of PolyLA compared to other approaches such as SVM.},
  archive      = {J_PAAA},
  author       = {Goodwin, Morten and Yazidi, Anis and Jonassen, Tore Møller},
  doi          = {10.1007/s10044-019-00848-6},
  journal      = {Pattern Analysis and Applications},
  month        = {8},
  number       = {3},
  pages        = {1235-1250},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Distributed learning automata-based S-learning scheme for classification},
  volume       = {23},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Transductive multi-label learning from missing data using
smoothed rank function. <em>PAAA</em>, <em>23</em>(3), 1225–1233. (<a
href="https://doi.org/10.1007/s10044-020-00869-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose two new algorithms for transductive multi-label learning from missing data. In transductive matrix completion (MC), the challenge is prediction while the data matrix is partially observed. The joint MC and prediction tasks are addressed simultaneously to enhance accuracy in comparison with separate tackling of each. In this setting, the labels to be predicted are modeled as missing entries inside a stacked matrix along the feature-instance data. Assuming the data matrix is of low rank, we propose a new recommendation method for transductive MC by posing the problem as a minimization of the smoothed rank function with non-affine constraints, rather than its convex surrogate. We provide convergence analysis for the proposed algorithms and illustrate their low computational complexity and robustness in comparison with other methods. The simulations are conducted on well-known real datasets in two different scenarios of randomly missing pattern with and without block-loss. The simulations reveal our methods accuracy is superior to state-of-the-art methods up to 10% in low observation rates for the scenario without block-loss. The accuracy of the proposed methods in the scenario with block-loss is comparable to the state-of-the-art while the complexity is reduced up to four times.},
  archive      = {J_PAAA},
  author       = {Esmaeili, Ashkan and Behdin, Kayhan and Fakharian, Mohammad Amin and Marvasti, Farokh},
  doi          = {10.1007/s10044-020-00869-6},
  journal      = {Pattern Analysis and Applications},
  month        = {8},
  number       = {3},
  pages        = {1225-1233},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Transductive multi-label learning from missing data using smoothed rank function},
  volume       = {23},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Ensemble adaptation networks with low-cost unsupervised
hyper-parameter search. <em>PAAA</em>, <em>23</em>(3), 1215–1224. (<a
href="https://doi.org/10.1007/s10044-019-00846-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The development of deep learning makes the learning model have more parameters to be learned, and it means that sufficient samples are needed. On the other hand, it is extremely difficult to find tons of labels to support model training process. The existing methods can extend the model to a new domain by looking for domain-invariant features from different domains. In this paper, we propose a novel deep domain adaptation model. Firstly, we try to make a variety of statistics working on high-level feature layers at the same time to obtain better performance. What is more, inspired by the active learning, we propose ‘uncertainty’ metric to search for hyper-parameters under unsupervised setting. The ‘uncertainty’ uses entropy to describe the learning status of the current discriminator. The smaller the ‘uncertainty’, the more stable the discriminator predicts the data. Finally, the network parameters are obtained by fine-tuning a generic pre-trained deep network. As a conclusion, the performance of our algorithm has been further improved over other compared algorithms on standard benchmarks.},
  archive      = {J_PAAA},
  author       = {Zhang, Haotian and Ding, Shifei and Jia, Weikuan},
  doi          = {10.1007/s10044-019-00846-8},
  journal      = {Pattern Analysis and Applications},
  month        = {8},
  number       = {3},
  pages        = {1215-1224},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Ensemble adaptation networks with low-cost unsupervised hyper-parameter search},
  volume       = {23},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Similarity measures of picture fuzzy sets based on entropy
and their application in MCDM. <em>PAAA</em>, <em>23</em>(3), 1203–1213.
(<a href="https://doi.org/10.1007/s10044-019-00861-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {An extension of fuzzy sets (Zadeh in Inf Control 8:338–353, 1965) and intuitionistic fuzzy sets (Atanassov in Fuzzy Sets Syst 20(1):87–96, 1986) is called picture fuzzy set (PFS). It is a useful tool to deal with uncertain and inconsistent information. The distance, entropy and similarity measures play a critical role in information theory. The similarity measures of picture fuzzy sets caused by entropy have been studied and given interesting results. In this paper, we first introduced the concept of entropy measure of PFS. At the same time, we also investigated some similarity measures induced by entropy measures and applied to propose the multi-criteria decision-making problem for selecting suppliers.},
  archive      = {J_PAAA},
  author       = {Thao, Nguyen Xuan},
  doi          = {10.1007/s10044-019-00861-9},
  journal      = {Pattern Analysis and Applications},
  month        = {8},
  number       = {3},
  pages        = {1203-1213},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Similarity measures of picture fuzzy sets based on entropy and their application in MCDM},
  volume       = {23},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Instance-based entropy fuzzy support vector machine for
imbalanced data. <em>PAAA</em>, <em>23</em>(3), 1183–1202. (<a
href="https://doi.org/10.1007/s10044-019-00851-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Imbalanced classification has been a major challenge for machine learning because many standard classifiers mainly focus on balanced datasets and tend to have biased results toward the majority class. We modify entropy fuzzy support vector machine (EFSVM) and introduce instance-based entropy fuzzy support vector machine (IEFSVM). Both EFSVM and IEFSVM use the entropy information of k-nearest neighbors to determine the fuzzy membership value for each sample which prioritizes the importance of each sample. IEFSVM considers the diversity of entropy patterns for each sample when increasing the size of neighbors, k, while EFSVM uses single entropy information of the fixed size of neighbors for all samples. By varying k, we can reflect the component change of sample’s neighbors from near to far distance in the determination of fuzzy value membership. Numerical experiments on 35 public and 12 real-world imbalanced datasets are performed to validate IEFSVM, and area under the receiver operating characteristic curve (AUC) is used to compare its performance with other SVMs and machine learning methods. IEFSVM shows a much higher AUC value for datasets with high imbalance ratio, implying that IEFSVM is effective in dealing with the class imbalance problem.},
  archive      = {J_PAAA},
  author       = {Cho, Poongjin and Lee, Minhyuk and Chang, Woojin},
  doi          = {10.1007/s10044-019-00851-x},
  journal      = {Pattern Analysis and Applications},
  month        = {8},
  number       = {3},
  pages        = {1183-1202},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Instance-based entropy fuzzy support vector machine for imbalanced data},
  volume       = {23},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Classifying imbalanced data using BalanceCascade-based
kernelized extreme learning machine. <em>PAAA</em>, <em>23</em>(3),
1157–1182. (<a
href="https://doi.org/10.1007/s10044-019-00844-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Imbalanced learning is one of the substantial challenging problems in the field of data mining. The datasets that have skewed class distribution pose hindrance to conventional learning methods. Conventional learning methods give the same importance to all the examples. This leads to the prediction inclined in favor of the majority classes. To solve this intrinsic deficiency, numerous strategies have been proposed such as weighted extreme learning machine (WELM) and boosting WELM (BWELM). This work designs a novel BalanceCascade-based kernelized extreme learning machine (BCKELM) to tackle the class imbalance problem more effectively. BalanceCascade includes the merits of random undersampling and the ensemble methods. The proposed method utilizes random undersampling to design balanced training subsets. The proposed ensemble generates the base learner in a sequential manner. In each iteration, the correctly classified examples belonging to the majority class are replaced by the other majority class examples to create a new balanced training subset, i.e., the base learners differ in the choice of the balanced training subset. The cardinality of the balanced training subsets depends on the imbalance ratio. This work utilizes a kernelized extreme learning machine (KELM) as the base learner to build the ensemble as it is stable and has good generalization performance. The time complexity of BCKELM is considerably lower in contrast to BWELM, BalanceCascade, EasyEnsemble and hybrid artificial bee colony WELM. The exhaustive experimental evaluation on real-world benchmark datasets demonstrates the efficacy of the proposed method.},
  archive      = {J_PAAA},
  author       = {Raghuwanshi, Bhagat Singh and Shukla, Sanyam},
  doi          = {10.1007/s10044-019-00844-w},
  journal      = {Pattern Analysis and Applications},
  month        = {8},
  number       = {3},
  pages        = {1157-1182},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Classifying imbalanced data using BalanceCascade-based kernelized extreme learning machine},
  volume       = {23},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). An empirical comparison of random forest-based and other
learning-to-rank algorithms. <em>PAAA</em>, <em>23</em>(3), 1133–1155.
(<a href="https://doi.org/10.1007/s10044-019-00856-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Random forest (RF)-based pointwise learning-to-rank (LtR) algorithms use surrogate loss functions to minimize the ranking error. In spite of their competitive performance to other state-of-the-art LtR algorithms, these algorithms, unlike other frameworks such as boosting and neural network, have not been thoroughly investigated in the literature so far. In the first part of this study, we aim to better understand and improve the RF-based pointwise LtR algorithms. When working with such an algorithm, currently we need to choose a setting from a number of available options such as (1) classification versus regression setting, (2) using absolute relevance judgements versus mapped labels, (3) the number of features using which a split-point for data is chosen, and (4) using weighted versus un-weighted average of the predictions of multiple base learners (i.e., trees). We conduct a thorough study on these four aspects as well as on a pairwise objective function for RF-based rank-learners. Experimental results on several benchmark LtR datasets demonstrate that performance can be significantly improved by exploring these aspects. In the second part of this paper, we, guided by our investigations performed into RF-based rank-learners, conduct extensive comparison between these and state-of-the-art rank-learning algorithms. This comparison reveals some interesting and insightful findings about LtR algorithms including the finding that RF-based LtR algorithms are among the most robust techniques across datasets with diverse properties.},
  archive      = {J_PAAA},
  author       = {Ibrahim, Muhammad},
  doi          = {10.1007/s10044-019-00856-6},
  journal      = {Pattern Analysis and Applications},
  month        = {8},
  number       = {3},
  pages        = {1133-1155},
  shortjournal = {Pattern Anal. Appl.},
  title        = {An empirical comparison of random forest-based and other learning-to-rank algorithms},
  volume       = {23},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Encoding features from multi-layer gabor filtering for
visual smoke recognition. <em>PAAA</em>, <em>23</em>(3), 1117–1131. (<a
href="https://doi.org/10.1007/s10044-020-00864-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {It is a challenging task to accurately recognize smoke from visual scenes due to large variations in smoke shape, color and texture. To improve recognition accuracy, we propose a framework mainly with a robust local feature extraction module based on Gabor convolutional networks. We first propose a Gabor convolutional network, each layer of which mainly consists of Gabor convolution and feature fusion. To fuse feature maps generated by Gabor convolution, we present three Basic Grouping Methods, which divide the feature maps into several groups along orientation axis, scale axis and both of them. To avoid exponential growth of feature maps and preserve discriminative information simultaneously, we propose three element-wise aggregation functions, which are mean, min and max, to combine feature maps in each group. To further improve efficiency, we use local binary pattern to encode hidden and output maps of Gabor convolutional layers. In addition, we use a weight vector to optimize concatenation of histograms for further improvement. Experiments show that our method achieves very outstanding results on smoke, texture and material image datasets. Although the feature extraction step of our method is training-free, our framework consistently outperforms state-of-the-art methods on small smoke datasets, even including deep learning-based methods.},
  archive      = {J_PAAA},
  author       = {Yuan, Feiniu and Li, Gang and Xia, Xue and Shi, Jinting and Zhang, Lin},
  doi          = {10.1007/s10044-020-00864-x},
  journal      = {Pattern Analysis and Applications},
  month        = {8},
  number       = {3},
  pages        = {1117-1131},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Encoding features from multi-layer gabor filtering for visual smoke recognition},
  volume       = {23},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A new multi-view learning machine with incomplete data.
<em>PAAA</em>, <em>23</em>(3), 1085–1116. (<a
href="https://doi.org/10.1007/s10044-020-00863-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-view learning with incomplete views (MVL-IV) is a reliable algorithm to process incomplete datasets which consist of instances with missing views or features. In MVL-IV, it exploits the connections among multiple views and suggests that different views are generated from a shared subspace such that it can recover the missing views or features well while MVL-IV neglects two facts. One is that different views should always be generated from different subspaces. The other is that the information of view-based classifiers is useful to the design of MVL-IV. Thus, on the base of MVL-IV, we consider these two facts and develop a new multi-view learning with incomplete data (NMVL-IV). Related experiments on clustering, regression, classification, bipartite ranking, and image retrieval have validated that the proposed NMVL-IV can recover the incomplete data much better.},
  archive      = {J_PAAA},
  author       = {Zhu, Changming and Chen, Chao and Zhou, Rigui and Wei, Lai and Zhang, Xiafen},
  doi          = {10.1007/s10044-020-00863-y},
  journal      = {Pattern Analysis and Applications},
  month        = {8},
  number       = {3},
  pages        = {1085-1116},
  shortjournal = {Pattern Anal. Appl.},
  title        = {A new multi-view learning machine with incomplete data},
  volume       = {23},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Scene classification using a new radial basis function
classifier and integrated SIFT–LBP features. <em>PAAA</em>,
<em>23</em>(3), 1071–1084. (<a
href="https://doi.org/10.1007/s10044-020-00868-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Scene classification is one of the most significant and challenging tasks in computer vision. This paper presents a new method for scene classification using bag of visual words and a particle swarm optimization (PSO)-based artificial neural network classifier. Contributions of this paper are introducing a novel feature integration method using scale invariant feature transform (SIFT) and local binary pattern (LBP) and a new framework for training radial basis function neural network, combining optimum steepest decent method with a specially designed PSO-based optimizer for center adjustment of radial basis function neural network. Our study shows that using LBP increases the performance of classification task compared to using SIFT only. In addition, our experiments on Proben1 dataset demonstrate improvements in classification performance (averagely about 6.04%) and convergence speed of the proposed radial basis function neural network. The proposed radial basis function neural network is then employed in scene classification task. Results are reported for classification of the Oliva and Torralba, Fei–Fei and Perona and Lazebnik et al. datasets. We compare the performance of the proposed classifier with a multi-way SVM classifier. Experimental results show the superiority of the proposed classifier over the state-of-the-art on the three datasets.},
  archive      = {J_PAAA},
  author       = {Giveki, Davar and Karami, Maryam},
  doi          = {10.1007/s10044-020-00868-7},
  journal      = {Pattern Analysis and Applications},
  month        = {8},
  number       = {3},
  pages        = {1071-1084},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Scene classification using a new radial basis function classifier and integrated SIFT–LBP features},
  volume       = {23},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Analysing the intermeshed patterns of road transportation
and macroeconomic indicators through neural and clustering techniques.
<em>PAAA</em>, <em>23</em>(3), 1059–1070. (<a
href="https://doi.org/10.1007/s10044-020-00872-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As is widely acknowledged, the transportation of goods by road can, in one way or another, be linked to a range of macroeconomic indicators. A hybrid artificial intelligence system is proposed in this paper to analyse the interaction between transportation patterns and the economy. The temporal patterns of road transportation and macroeconomic trends are studied, by combining the use of both (supervised and unsupervised) neural networks and clustering techniques. The proposed system is validated, by establishing links between road transportation data from Spain and macroeconomic trends over 6 years (2011–2017). The results reveal an interesting inner structure of the data, through data visualizations of intermeshed relations between road transportation patterns and macroeconomic indicators. The same data structure was also visible in the output of the clustering techniques. Furthermore, a number of high-quality predictions were advanced by processing the road transportation data as time series, and forecasting the future values of the main series. These results demonstrated the validity of the proposed linkage between road transportation data and macroeconomic indicators.},
  archive      = {J_PAAA},
  author       = {Alonso de Armiño, Carlos and Manzanedo, Miguel Ángel and Herrero, Álvaro},
  doi          = {10.1007/s10044-020-00872-x},
  journal      = {Pattern Analysis and Applications},
  month        = {8},
  number       = {3},
  pages        = {1059-1070},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Analysing the intermeshed patterns of road transportation and macroeconomic indicators through neural and clustering techniques},
  volume       = {23},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Novel clustering-based pruning algorithms. <em>PAAA</em>,
<em>23</em>(3), 1049–1058. (<a
href="https://doi.org/10.1007/s10044-020-00867-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {One of the crucial problems of designing a classifier ensemble is the proper choice of the base classifier line-up. Basically, such an ensemble is formed on the basis of individual classifiers, which are trained in such a way to ensure their high diversity or they are chosen on the basis of pruning which reduces the number of predictive models in order to improve efficiency and predictive performance of the ensemble. This work is focusing on clustering-based ensemble pruning, which looks for the group of similar classifiers which are replaced by their representatives. We propose a novel pruning criterion based on well-known diversity measures and describe three algorithms using classifier clustering. The first method selects the model with the best predictive performance from each cluster to form the final ensemble, the second one employs the multistage organization, where instead of removing the classifiers from the ensemble each classifier cluster makes the decision independently, while the third proposition combines multistage organization and sampling with replacement. The proposed approaches were evaluated using 30 datasets with different characteristics. Experimentation results validated through statistical tests confirmed the usefulness of the proposed approaches.},
  archive      = {J_PAAA},
  author       = {Zyblewski, Paweł and Woźniak, Michał},
  doi          = {10.1007/s10044-020-00867-8},
  journal      = {Pattern Analysis and Applications},
  month        = {8},
  number       = {3},
  pages        = {1049-1058},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Novel clustering-based pruning algorithms},
  volume       = {23},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Improved quality assessment of colour surfaces for additive
manufacturing based on image entropy. <em>PAAA</em>, <em>23</em>(3),
1035–1047. (<a
href="https://doi.org/10.1007/s10044-020-00865-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A reliable automatic visual quality assessment of 3D-printed surfaces is one of the key issues related to computer and machine vision in the Industry 4.0 era. The colour-independent method based on image entropy proposed in the paper makes it possible to detect and identify some typical problems visible on the surfaces of objects obtained by additive manufacturing. Depending on the quality factor, some of such 3D printing failures may be corrected during the printing process or the operation can be aborted to save time and filament. Since the surface quality of 3D-printed objects may be related to some mechanical or physical properties of obtained objects, its fast and reliable evaluation may also be helpful during the quality monitoring procedures. The method presented in the paper utilizes the assumption of the increase of image entropy for irregularly distorted 3D-printed surfaces. Nevertheless, because of the local nature of distortions, the direct application of the global entropy does not lead to satisfactory results of automatic surface quality assessment. Therefore, the extended method, based on the combination of the local image entropy and its variance with additional colour adjustment, is proposed in the paper, leading to the proper classification of 78 samples used during the experimental verification of the proposed approach.},
  archive      = {J_PAAA},
  author       = {Okarma, Krzysztof and Fastowicz, Jarosław},
  doi          = {10.1007/s10044-020-00865-w},
  journal      = {Pattern Analysis and Applications},
  month        = {8},
  number       = {3},
  pages        = {1035-1047},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Improved quality assessment of colour surfaces for additive manufacturing based on image entropy},
  volume       = {23},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020a). Correction to: Optimal face templates: The next step in
surveillance face recognition. <em>PAAA</em>, <em>23</em>(2), 1033. (<a
href="https://doi.org/10.1007/s10044-019-00849-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the original publication of the article, the below-mentioned acknowledgement was not included and the author would like to add it.},
  archive      = {J_PAAA},
  author       = {Malach, Tobias and Pomenkova, Jitka},
  doi          = {10.1007/s10044-019-00849-5},
  journal      = {Pattern Analysis and Applications},
  month        = {5},
  number       = {2},
  pages        = {1033},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Correction to: optimal face templates: the next step in surveillance face recognition},
  volume       = {23},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020b). Optimal face templates: The next step in surveillance face
recognition. <em>PAAA</em>, <em>23</em>(2), 1021–1032. (<a
href="https://doi.org/10.1007/s10044-019-00842-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The paper deals with surveillance face recognition in security applications such as surveillance camera systems or access control systems. Presented research is focused on enhancing recognition performance, reducing classification time and memory requirements. We aim to make it feasible to implement face recognition in end devices such as cameras, identification terminals or popular IoT devices. Therefore, we utilize algorithms that require low computational power and optimize them in order to reach higher recognition rates. We present a novel higher quantile method that enhances recognition performance via creation of robust and representative face templates for nearest neighbor classifier. Templates computed by the higher quantile method are determined by tolerance intervals which handle feature variability caused by face pose, expression, illumination and possible low image quality. The recognition performance evaluation has been conducted on images captured by surveillance camera system that are contained in unique IFaViD dataset. The IFaViD is the only one dataset captured by real surveillance camera system containing complex scenarios. The results show that the higher quantile method outperforms the contemporary approaches by 4%, respectively, 10% depending on the IFaViD’s test subset.},
  archive      = {J_PAAA},
  author       = {Malach, Tobias and Pomenkova, Jitka},
  doi          = {10.1007/s10044-019-00842-y},
  journal      = {Pattern Analysis and Applications},
  month        = {5},
  number       = {2},
  pages        = {1021-1032},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Optimal face templates: The next step in surveillance face recognition},
  volume       = {23},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A chinese unknown word recognition method for micro-blog
short text based on improved FP-growth. <em>PAAA</em>, <em>23</em>(2),
1011–1020. (<a
href="https://doi.org/10.1007/s10044-019-00833-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unknown word recognition technology is of great significance to improve the precision of text segmentation and syntax analysis. Social network has become an important platform for sharing, disseminating, and acquiring information. Unknown word recognition based on micro-blog short text has become a research hot spot, while the micro-blog text contains a large number of nonstandard terms and network buzzwords, which has increased the difficulty of unknown word recognition. This paper proposes a Chinese unknown word recognition method for micro-blog short text based on improved FP-growth (POS-FP). Firstly, the POS-FP algorithm is used to get frequent itemsets from micro-blog, and the N-grams model is used to filter out unknown words from frequent itemsets. Secondly, the improved mutual information and left–right information entropy are used to verify the internal features of candidate unknown words. Then, context-dependent and open-source methods are used for external verification of candidate unknown words. Compared with traditional methods, this algorithm improves the recognition rate of unknown words in micro-blog short texts.},
  archive      = {J_PAAA},
  author       = {Jia, Yalu and Liu, Lei and Chen, Hao and Sun, Yinghong},
  doi          = {10.1007/s10044-019-00833-z},
  journal      = {Pattern Analysis and Applications},
  month        = {5},
  number       = {2},
  pages        = {1011-1020},
  shortjournal = {Pattern Anal. Appl.},
  title        = {A chinese unknown word recognition method for micro-blog short text based on improved FP-growth},
  volume       = {23},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). BISDBx: Towards batch-incremental clustering for dynamic
datasets using SNN-DBSCAN. <em>PAAA</em>, <em>23</em>(2), 975–1009. (<a
href="https://doi.org/10.1007/s10044-019-00831-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many important applications such as recommender systems, e-commerce sites, web crawlers involve dynamic datasets. Dynamic datasets undergo frequent changes in the form of insertion or deletion of data that affects its size. A naive algorithm may not process these frequent changes efficiently as it involves the entire set of data points each time a change is inflicted. Fast incremental algorithms process these updates to datasets efficiently to avoid redundant computation. In this article, we propose incremental extensions to shared nearest neighbor density-based clustering (SNNDB) algorithm for both addition and deletion of data points. Existing incremental extension to SNNDB viz. InSDB cannot handle deletion and handles insertions one point at a time. Our method overcomes both these bottlenecks by efficiently identifying affected parts of clusters while processing updates to dataset in batch mode. We propose three incremental variants of SNNDB in batch mode for both addition and deletion with the third variant being the most effective. Experimental observations on real world and synthetic datasets showed that our algorithms are up to 4 orders of magnitude faster than the naive SNNDB algorithm and about 2 orders of magnitude faster than the pointwise incremental method.},
  archive      = {J_PAAA},
  author       = {Bhattacharjee, Panthadeep and Mitra, Pinaki},
  doi          = {10.1007/s10044-019-00831-1},
  journal      = {Pattern Analysis and Applications},
  month        = {5},
  number       = {2},
  pages        = {975-1009},
  shortjournal = {Pattern Anal. Appl.},
  title        = {BISDBx: Towards batch-incremental clustering for dynamic datasets using SNN-DBSCAN},
  volume       = {23},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Manifold ranking graph regularization non-negative matrix
factorization with global and local structures. <em>PAAA</em>,
<em>23</em>(2), 967–974. (<a
href="https://doi.org/10.1007/s10044-019-00832-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Non-negative matrix factorization (NMF) is a recently popularized technique for learning parts-based, linear representations of non-negative data. Although the decomposition rate of NMF is very fast, it still suffers from the following deficiency: It only revealed the local geometry structure; global geometric information of data set is ignored. This paper proposes a manifold ranking graph regularization non-negative matrix factorization with local and global geometric structure (MRLGNMF) to overcome the above deficiency. In particular, MRLGNMF induces manifold ranking to the non-negative matrix factorization with Sinkhorn distance. Numerical results show that the new algorithm is superior to the existing algorithm.},
  archive      = {J_PAAA},
  author       = {Li, Xiangli and Yu, Jianglan and Dong, Xiaoliang and Zhao, Pengfei},
  doi          = {10.1007/s10044-019-00832-0},
  journal      = {Pattern Analysis and Applications},
  month        = {5},
  number       = {2},
  pages        = {967-974},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Manifold ranking graph regularization non-negative matrix factorization with global and local structures},
  volume       = {23},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). An automatic multi-camera-based event extraction system for
real soccer videos. <em>PAAA</em>, <em>23</em>(2), 953–965. (<a
href="https://doi.org/10.1007/s10044-019-00830-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we propose a novel and effective system based on multiple cameras to extract the events for soccer matches. A precise ontological definition of the soccer events is still an open point. According to our definition, the events include the free kick, corner kick, penalty kick and the goal, because they are the representative shots for the audience to watch. The events are very important for highlights selection and sport data analysis. At present, the events including the ball and players information are selected and labeled manually from the images, which is a big workload for the staffs. Addressing this problem, our system provides an automatic extraction of the events. For soccer videos, our system first uses the local-based deep neural network for the ball and player detection from the input images. Then, we handle with the ball and player bounding boxes separately. For players, a player can be labeled as one of the three types: two teams or the referee, and a novel unsupervised U-encoder is designed for the player labeling. For soccer ball, the application of multiple cameras allows us to refine the ball detection results. We can get the world coordinate of ball according to the camera parameters and then rebuild the ball trajectory and the court in a top view. Based on the reconstructed map, we get the soccer events by motion analysis of ball trajectory and then apply the ball location and player classification results to display the events for each camera. The test results on real videos of European soccer league show the good detection and labeling performance of our system. We find all the events in the test videos. Our proposed system can deal with many complex cases such as occlusion and pose variation that happen frequently in real applications.},
  archive      = {J_PAAA},
  author       = {Zhang, Kailai and Wu, Ji and Tong, Xiaofeng and Wang, Yumeng},
  doi          = {10.1007/s10044-019-00830-2},
  journal      = {Pattern Analysis and Applications},
  month        = {5},
  number       = {2},
  pages        = {953-965},
  shortjournal = {Pattern Anal. Appl.},
  title        = {An automatic multi-camera-based event extraction system for real soccer videos},
  volume       = {23},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Synchronization correction-based robust digital image
watermarking approach using bessel k-form PDF. <em>PAAA</em>,
<em>23</em>(2), 933–951. (<a
href="https://doi.org/10.1007/s10044-019-00828-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Robustness has played extremely important role in the multiple applications of digital watermarking technology. The reason lies in its influence on the watermarking system’s practicability. As one of the most difficult kinds of digital signal processing for a digital watermark to survive, geometric distortions have become a central problem in digital image watermarking research. Therefore, resigning a greatly robust digital image watermarking approach which can withstand geometric distortions is still a quite challenging work. On account of Bessel K-form (BKF) probability density function (PDF), this paper proposes an optimal synchronization correction-based digital image watermarking method which can resist geometric distortions. This approach consists of watermark embedding, synchronization correction and watermark extraction. Using the quantization index modulation (QIM), the watermark is inserted into the original host images in the nonsubsampled shearlet transform (NSST) domain by adjusting the selected blocks’ low-frequency NSST coefficients. The BKF PDF describes the NSST coefficients’ statistical features. Besides, the BKF statistical model parameters can be used in constructing a compact image characteristic space. Utilizing the compact image feature, the least squares support vector regression (LS-SVR) synchronization correction is performed to estimate the geometric distortions parameters. After LS-SVR synchronization correction, the inverse QIM is performed to recover blindly the inserted watermark. Simulation results demonstrate that the presented digital image watermarking method not only has good imperceptibility performance, but also can well resist challenging common signal processing operations as well as geometric distortions, which can be superior to the most advanced approaches.},
  archive      = {J_PAAA},
  author       = {Wang, Xiang-yang and Zhang, Si-yu and Wen, Tao-tao and Xu, Huan and Yang, Hong-ying},
  doi          = {10.1007/s10044-019-00828-w},
  journal      = {Pattern Analysis and Applications},
  month        = {5},
  number       = {2},
  pages        = {933-951},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Synchronization correction-based robust digital image watermarking approach using bessel K-form PDF},
  volume       = {23},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Finding patterns in the degree distribution of real-world
complex networks: Going beyond power law. <em>PAAA</em>, <em>23</em>(2),
913–932. (<a href="https://doi.org/10.1007/s10044-019-00820-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The most important structural characteristics in the study of large-scale real-world complex networks in pattern analysis are degree distribution. Empirical observations on the pattern of the real-world networks have led to the claim that their degree distributions follow, in general, a single power law. However, a closer observation, while fitting, shows that the single power-law distribution is often inadequate to meet the data characteristics properly. Since the degree distribution in the log–log scale actually displays, under inspection, two different slopes unlike what happens while fitting with the single power law. These two slopes with a transition in between closely resemble the pattern of the sigmoid function. This motivates us to derive a novel double power-law distribution for accurately modeling the real-world networks based on the sigmoid function. The proposed modeling approach further leads to the identification of a transition degree which, it has been demonstrated, may have a significant implication in analyzing the complex networks. The applicability, as well as effectiveness of the proposed methodology, is shown using rigorous experiments and also validated using statistical tests.},
  archive      = {J_PAAA},
  author       = {Chattopadhyay, Swarup and Das, Asit K. and Ghosh, Kuntal},
  doi          = {10.1007/s10044-019-00820-4},
  journal      = {Pattern Analysis and Applications},
  month        = {5},
  number       = {2},
  pages        = {913-932},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Finding patterns in the degree distribution of real-world complex networks: Going beyond power law},
  volume       = {23},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Parallel cycle-based branch-and-bound method for bayesian
network learning. <em>PAAA</em>, <em>23</em>(2), 897–911. (<a
href="https://doi.org/10.1007/s10044-019-00815-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Bayesian networks (BNs) are one of the most commonly used models for representing uncertainty in medical diagnosis. Learning the exact structure of a BN is a challenging problem. This paper proposes a multi-threaded branch-and-bound (B&amp;B) method, called parallel cycle-based branch-and-bound (parallel CB-B&amp;B). On the one hand, CB-B&amp;B improves the standard B&amp;B method by leveraging two heuristics, namely the branching strategy and the bounding operators; on the other hand, the learning procedure is alleviated by executing CB-B&amp;B over a set of parallel processors. In comparison with conventional exact structure learning approaches for BN, the obtained results demonstrate that the proposed CB-B&amp;B is efficient. On average, it produces the exact structure for BN three times faster than the standard B&amp;B version. We also present simulations on parallel CB-B&amp;B which show a significant gain in terms of execution time.},
  archive      = {J_PAAA},
  author       = {Benmouna, Youcef and Mezmaz, Mohand Said and Mahmoudi, Said and Chikh, Med Amine},
  doi          = {10.1007/s10044-019-00815-1},
  journal      = {Pattern Analysis and Applications},
  month        = {5},
  number       = {2},
  pages        = {897-911},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Parallel cycle-based branch-and-bound method for bayesian network learning},
  volume       = {23},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A novel contextual memory algorithm for edge detection.
<em>PAAA</em>, <em>23</em>(2), 883–895. (<a
href="https://doi.org/10.1007/s10044-019-00808-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Edge detection plays an important role in many computer vision systems. In this paper, we propose a novel application agnostic algorithm for prediction of probabilities based on the contextual information available and then apply the algorithm for estimating the probability of pixels belonging to an edge using surrounding pixel values as local contexts. We then proceed to test different image transformations as input layers, such as the Canny edge detector. We propose two different architectures, one single layered and one multilayered, which approach the scaling problem by creating scaled side outputs and combining them via a logistic regression layer. We tested our approach on the BSDS500 edge detection dataset with optimistic results.},
  archive      = {J_PAAA},
  author       = {Dorobanţiu, Alexandru and Brad, Remus},
  doi          = {10.1007/s10044-019-00808-0},
  journal      = {Pattern Analysis and Applications},
  month        = {5},
  number       = {2},
  pages        = {883-895},
  shortjournal = {Pattern Anal. Appl.},
  title        = {A novel contextual memory algorithm for edge detection},
  volume       = {23},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Text area segmentation from document images by novel
adaptive thresholding and template matching using texture cues.
<em>PAAA</em>, <em>23</em>(2), 869–881. (<a
href="https://doi.org/10.1007/s10044-019-00811-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a new perspective of text area segmentation from document images using a novel adaptive thresholding for image enhancement. Using sliding windows, the texture of the enhanced image is matched with that of a fixed training template image containing the typed letters ‘dB.’ The affine-invariant, low-dimensional difference theoretic texture feature set is used for the texture measurement. The distance matrix is binarized using Otsu threshold, and the ‘0’ pixels indicate the text area. One primary contribution of this paper is the novel adaptive thresholding for document image enhancement prior to the extraction of texture cues. The proposed adaptive thresholding mimics the ability of the human eye to iteratively adjust to varying light intensities through iterative gamma correction followed by contrast stretching so that the text becomes well defined against the background clutter. The text blobs so segmented are binarized using Yanowitz and Bruckstein method of text binarization, and the results are applied for evaluation with respect to the ground-truth annotations. We tested our algorithm on the benchmark DIBCO 2009, 2010, 2011, 2012, 2013 document image datasets in comparison with the state of the art. The high precision–recall and F-score values establish the efficiency of our approach.},
  archive      = {J_PAAA},
  author       = {Susan, Seba and Rachna Devi, K. M.},
  doi          = {10.1007/s10044-019-00811-5},
  journal      = {Pattern Analysis and Applications},
  month        = {5},
  number       = {2},
  pages        = {869-881},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Text area segmentation from document images by novel adaptive thresholding and template matching using texture cues},
  volume       = {23},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Local tangent space alignment based on hilbert–schmidt
independence criterion regularization. <em>PAAA</em>, <em>23</em>(2),
855–868. (<a href="https://doi.org/10.1007/s10044-019-00810-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Local tangent space alignment (LTSA) is a famous manifold learning algorithm, and many other manifold learning algorithms are developed based on LTSA. However, from the viewpoint of dimensionality reduction, LTSA is only a local feature preserving algorithm. What the community of dimensionality reduction is now pursuing are those algorithms capable of preserving both local and global features at the same time. In this paper, a new algorithm for dimensionality reduction, called HSIC-regularized LTSA (HSIC–LTSA), is proposed, in which a HSIC regularization term is added to the objective function of LTSA. HSIC is an acronym for Hilbert–Schmidt independence criterion and has been used in many applications of machine learning. However, HSIC has not been directly applied to dimensionality reduction so far, neither used as a regularization term to combine with other machine learning algorithms. Therefore, the proposed HSIC–LTSA is a new try for both HSIC and LTSA. In HSIC–LTSA, HSIC makes the high- and low-dimensional data statistically correlative as much as possible, while LTSA reduces the data dimension under the local homeomorphism-preserving criterion. The experimental results presented in this paper show that, on several commonly used datasets, HSIC–LTSA performs better than LTSA as well as some state-of-the-art local and global preserving algorithms.},
  archive      = {J_PAAA},
  author       = {Zheng, Xinghua and Ma, Zhengming and Li, Lei},
  doi          = {10.1007/s10044-019-00810-6},
  journal      = {Pattern Analysis and Applications},
  month        = {5},
  number       = {2},
  pages        = {855-868},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Local tangent space alignment based on Hilbert–Schmidt independence criterion regularization},
  volume       = {23},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A novel CT image segmentation algorithm using PCNN and
sobolev gradient methods in GPU frameworks. <em>PAAA</em>,
<em>23</em>(2), 837–854. (<a
href="https://doi.org/10.1007/s10044-019-00837-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate brain tumor segmentation plays a significant role in the area of radiotherapy diagnosis and in the proper treatment for brain tumor detection. Typically, the brain tumor has poor boundary and low contrast between normal and lesion soft tissues that makes segmentation of brain tumor in the CT images a challenging task. This paper presents a novel approach to brain image segmentation using pulse-coupled neural network (PCNN) and zero level set (ZL) with Sobolev gradient (SG) method. In this article, PCNN is designed to use as an edge mapper to provide a regional description for the ZL to segregate the CT images based on contour maps. The PCNN is used to estimate the exact threshold to obtain the prominent edges of the images. Resulting edges are utilized in the ZL to extract image contour from the source image. Due to the over-sensitivity of the ZL method on the initial contour, a level set with the SG has been equipped to overcome the limitation of the ZL method. The experimental results show satisfactory segmentation outcomes with excellent accuracy and acceleration in comparison with the state-of-the-art methods.},
  archive      = {J_PAAA},
  author       = {Biswas, Biswajit and Ghosh, Swarup Kr. and Ghosh, Anupam},
  doi          = {10.1007/s10044-019-00837-9},
  journal      = {Pattern Analysis and Applications},
  month        = {5},
  number       = {2},
  pages        = {837-854},
  shortjournal = {Pattern Anal. Appl.},
  title        = {A novel CT image segmentation algorithm using PCNN and sobolev gradient methods in GPU frameworks},
  volume       = {23},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Fractal dimension of synthesized and natural color images in
lab space. <em>PAAA</em>, <em>23</em>(2), 819–836. (<a
href="https://doi.org/10.1007/s10044-019-00839-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fractal dimension (FD) is a useful metric for the analysis of natural images that exhibit a high degree of complexity, randomness and irregularity in color and texture. Several approaches exist in the literature to measure FD of gray-scale images. The aim of this study is to introduce a FD estimation method for color images with color proximity in Lab space. The proposed method uses a xy-plane partitioning–shifting mechanism, where the divisors of image size are used as grid sizes. The proposed method simulates on synthesized color fractal Brownian motion (FBM) images, publicly available Brodatz database, Google color fractal images and noisy Brodatz database. The random midpoint displacement algorithm for the formation of gray-scale images is extended in this work to synthesize color FBM images. Noisy Brodatz database is obtained by adding salt-and-pepper noise with different noise densities to understand the behavior of FD. The experimental results illustrate that the proposed method is effective and efficient and outperforms the three state-of-the-art methods by observing the values of two proposed metrics, namely average error and average computed FD. A new mathematical expression for estimating FD of a color image is demonstrated, which relies on the number of edge pixels of individual color channel using multiple linear regression.},
  archive      = {J_PAAA},
  author       = {Panigrahy, Chinmaya and Seal, Ayan and Mahato, Nihar Kumar},
  doi          = {10.1007/s10044-019-00839-7},
  journal      = {Pattern Analysis and Applications},
  month        = {5},
  number       = {2},
  pages        = {819-836},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Fractal dimension of synthesized and natural color images in lab space},
  volume       = {23},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Local gradient of gradient pattern: A robust image
descriptor for the classification of brain strokes from computed
tomography images. <em>PAAA</em>, <em>23</em>(2), 797–817. (<a
href="https://doi.org/10.1007/s10044-019-00838-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a new feature extraction method for the classification of brain computed tomography (CT) scan images into hemorrhagic strokes, ischemic strokes and normal CT images. The most popular feature extraction method is local binary pattern (LBP), which works by thresholding the neighboring pixel values with the center pixel value of the image. Unlike LBP, our proposed method is based on comparing neighbors of center pixel and the mean of whole image intensities in the first step, and computing double gradients of local neighborhoods of a center pixel of the original image in x and y directions in the second step. Further, values obtained from the first step are compared with double gradients of neighbors in order to generate codes for the center pixel. We have also calculated the codes for the first step. Thereafter, histograms of all the codes are generated and finally concatenated to form a single feature vector. We termed this descriptor as the local gradient of gradient pattern. We have performed nine different experiments where images have been classified using various classifiers. The efficacy of our feature descriptor for image classification is identified by comparing it with seven different feature extraction methods. Performances of these methods are tested using metrics such as precision, true positive rate, false positive rate, F-measure and accuracies of the classifier. Results obtained show that our method is superior to other previous descriptors.},
  archive      = {J_PAAA},
  author       = {Gautam, Anjali and Raman, Balasubramanian},
  doi          = {10.1007/s10044-019-00838-8},
  journal      = {Pattern Analysis and Applications},
  month        = {5},
  number       = {2},
  pages        = {797-817},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Local gradient of gradient pattern: A robust image descriptor for the classification of brain strokes from computed tomography images},
  volume       = {23},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). The modified generic polar harmonic transforms for image
representation. <em>PAAA</em>, <em>23</em>(2), 785–795. (<a
href="https://doi.org/10.1007/s10044-019-00840-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper introduces four classes of orthogonal transforms by modifying the generic polar harmonic transforms. Then, the rotation invariant feature of the proposed transforms is investigated. Compared with the traditional generic polar harmonic transforms, the proposed transforms have the ability to describe the central region of the image with a parameter controlling the area of the region. Experimental results verified the image representation capability of the proposed transforms and showed better performance of the proposed transform in terms of rotation invariant pattern recognition.},
  archive      = {J_PAAA},
  author       = {Liu, Xilin and Wu, Yongfei and Shao, Zhuhong and Wu, Jiasong},
  doi          = {10.1007/s10044-019-00840-0},
  journal      = {Pattern Analysis and Applications},
  month        = {5},
  number       = {2},
  pages        = {785-795},
  shortjournal = {Pattern Anal. Appl.},
  title        = {The modified generic polar harmonic transforms for image representation},
  volume       = {23},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Evaluating dynamic texture descriptors to recognize human
iris in video image sequence. <em>PAAA</em>, <em>23</em>(2), 771–784.
(<a href="https://doi.org/10.1007/s10044-019-00836-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the last decades, iris features have been widely used in biometric systems. Because iris features are virtually unique for each person, their usage is highly reliable. However, biometric systems based on iris features are not completely fraud-resistant, as most systems use static images and do not distinguish between a live iris and a photograph. The iris structure and texture change with light variations, and traditional techniques for iris recognition always identify the iris texture in a controlled environment. However, in uncontrolled environments, live irises are recognized by their dynamic response to light: If the light changes, the pupils dilate or contract, and their texture dynamically changes. If a biometric system can identify people during the constriction or dilation time interval, that system will be more fraud-resistant. This paper proposes a new methodology to evaluate the “dynamic texture” from iris image sequences (motion analysis) and measure the discriminant power of these features for biometric system applications. We propose two new dynamic descriptors—dynamic local mapped pattern and dynamic sampled local mapped pattern—which are extensions of the local mapped pattern previously published for texture classification. We applied our proposed dynamic texture descriptors in a sequence of iris images segmented from video under light variation. Then, we compared our results with the well-known dynamic texture descriptor local binary pattern from three orthogonal planes (LBP-TOP). We used statistical measures to evaluate the performance of both descriptors and concluded that our methodology performed better than the LBP-TOP. Moreover, our descriptors can extract dynamic textures faster than the LBP-TOP.},
  archive      = {J_PAAA},
  author       = {de Melo Langoni, Virgílio and Gonzaga, Adilson},
  doi          = {10.1007/s10044-019-00836-w},
  journal      = {Pattern Analysis and Applications},
  month        = {5},
  number       = {2},
  pages        = {771-784},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Evaluating dynamic texture descriptors to recognize human iris in video image sequence},
  volume       = {23},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Pedestrian detection using multiple feature channels and
contour cues with census transform histogram and random forest
classifier. <em>PAAA</em>, <em>23</em>(2), 751–769. (<a
href="https://doi.org/10.1007/s10044-019-00835-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a reliable and real-time method to detect pedestrians in image scenes that can vary greatly in appearance. To achieve greater reliability in what can be detected, a combination of visual cues is used in conjunction with edge-based features and colour information as a basis for training a random forest (RF) classifier to detect the local contour cues for pedestrian images. To achieve a real-time detection rate, the contour cues, edge-based features and colour information are incorporated and then trained using a cascade RF classifier with a census transform histogram visual descriptor that implicitly captures the global contours of the pedestrians. The contour detector favourably exceeded previous leading contour detectors and achieved a 95% detection rate. The reliability and specificity of the pedestrian detector are demonstrated on more than 5000 positive images containing street furniture, lamp posts and trees, structures that are frequently confused with persons by computer vision systems. Evaluation with over 220 video sequences with $$640 \times 480$$ pixel resolution presented a true positive rate of 96%. The proposed pedestrian detector outperforms previous competitive pedestrian detectors on many varied person data sets. The speed of execution in a robot is about 62 ms per frame for images of $$640 \times 480$$ pixels on an Intel Core i3-2310M™ processor running at 2.10 GHz with a RAM of 4 GB.},
  archive      = {J_PAAA},
  author       = {Braik, Malik and Al-Zoubi, Hussein and Al-Hiary, Heba},
  doi          = {10.1007/s10044-019-00835-x},
  journal      = {Pattern Analysis and Applications},
  month        = {5},
  number       = {2},
  pages        = {751-769},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Pedestrian detection using multiple feature channels and contour cues with census transform histogram and random forest classifier},
  volume       = {23},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Locality preserving difference component analysis based on
the lq norm. <em>PAAA</em>, <em>23</em>(2), 735–749. (<a
href="https://doi.org/10.1007/s10044-019-00834-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper develops locality preserving difference component analysis in which the intrinsic and global structure of data is exploited, and the model we propose also provides the flexibility to adapt some characteristics of data by applying the Lq norm. In order to solve the proposed model that is non-convex or non-smooth, we resort to the proximal alternating linearized optimization approach where each subproblem has good optimization properties. It is observed that the objective function in the proposed model is a semi-algebraic function. This allows us to give the convergence analysis of algorithms in terms of the Kurdyka–Lojasiewicz property. To be specific, the sequence of iterations generated by the proposed approach converges to a critical point of the objective function. The experiments on several data sets have been conducted to demonstrate the effectiveness of the proposed approach.},
  archive      = {J_PAAA},
  author       = {Liang, Zhizheng and Chen, Xuewen and Zhang, Lei and Liu, Jin and Zhou, Yong},
  doi          = {10.1007/s10044-019-00834-y},
  journal      = {Pattern Analysis and Applications},
  month        = {5},
  number       = {2},
  pages        = {735-749},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Locality preserving difference component analysis based on the lq norm},
  volume       = {23},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). CHSPAM: A multi-domain model for sequential pattern
discovery and monitoring in contexts histories. <em>PAAA</em>,
<em>23</em>(2), 725–734. (<a
href="https://doi.org/10.1007/s10044-019-00829-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Context-aware applications adapt their functionalities based on users contexts. Complementarily, a context history has information about previous contexts visited by a user. Context history enables applications to explore users past behavior. Researchers have studied different ways to analyze these data. This article addresses a specific type of data analysis in contexts histories, which is the discovery and monitoring of sequential patterns. The article proposes a model, called CHSPAM, that allows the discovery of sequential patterns in contexts histories databases and keeps track of these patterns to monitor their evolution over time. There are two main contributions of this work. The first one is the use of a generic representation for stored context information on pattern recognition field, which enables the model to be used for different research domains. The second contribution is the fact that CHSPAM monitors discovered pattern evolution over time. We have build a functional prototype that allowed us to conduct experiments in two different applications. The first experiment used the model to perform pattern analysis and evaluate the prediction based on monitored sequential patterns. Prediction accuracy increased by up to 17% when compared to the use of common sequential patterns. On the second experiment, CHSPAM was used as a component of a learning object recommendation application. The application was able to recommend learning objects related to students interests based on monitored sequential patterns extracted from users session history. Usefulness for recommendations reached 84%.},
  archive      = {J_PAAA},
  author       = {Dupont, Daniel and Barbosa, Jorge Luis Victória and Alves, Bruno Mota},
  doi          = {10.1007/s10044-019-00829-9},
  journal      = {Pattern Analysis and Applications},
  month        = {5},
  number       = {2},
  pages        = {725-734},
  shortjournal = {Pattern Anal. Appl.},
  title        = {CHSPAM: A multi-domain model for sequential pattern discovery and monitoring in contexts histories},
  volume       = {23},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A novel feature descriptor for image retrieval by combining
modified color histogram and diagonally symmetric co-occurrence texture
pattern. <em>PAAA</em>, <em>23</em>(2), 703–723. (<a
href="https://doi.org/10.1007/s10044-019-00827-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we have proposed a novel feature descriptors combining color and texture information collectively. In our proposed color descriptor component, the inter-channel relationship between Hue (H) and Saturation (S) channels in the HSV color space has been explored which was not done earlier. We have quantized the H channel into a number of bins and performed the voting with saturation values and vice versa by following a principle similar to that of the HOG descriptor, where orientation of the gradient is quantized into a certain number of bins and voting is done with gradient magnitude. This helps us to study the nature of variation of saturation with variation in Hue and nature of variation of Hue with the variation in saturation. The texture component of our descriptor considers the co-occurrence relationship between the pixels symmetric about both the diagonals of a 3 × 3 window. Our work is inspired from the work done by Dubey et al. (IEEE Signal Process Lett 22(9):1215–1219, [2015]). These two components, viz. color and texture information individually perform better than existing texture and color descriptors. Moreover, when concatenated the proposed descriptors provide a significant improvement over existing descriptors for content base color image retrieval. The proposed descriptor has been tested for image retrieval on five databases, including texture image databases—MIT-VisTex database and Salzburg texture database and natural scene databases Corel 1K, Corel 5K and Corel 10K. The precision and recall values experimented on these databases are compared with some state-of-art local patterns. The proposed method provided satisfactory results from the experiments.},
  archive      = {J_PAAA},
  author       = {Bhunia, Ayan Kumar and Bhattacharyya, Avirup and Banerjee, Prithaj and Roy, Partha Pratim and Murala, Subrahmanyam},
  doi          = {10.1007/s10044-019-00827-x},
  journal      = {Pattern Analysis and Applications},
  month        = {5},
  number       = {2},
  pages        = {703-723},
  shortjournal = {Pattern Anal. Appl.},
  title        = {A novel feature descriptor for image retrieval by combining modified color histogram and diagonally symmetric co-occurrence texture pattern},
  volume       = {23},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Color image segmentation using proximal classifier and
quaternion radial harmonic fourier moments. <em>PAAA</em>,
<em>23</em>(2), 683–702. (<a
href="https://doi.org/10.1007/s10044-019-00826-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Segmentation involves separating an object from the background in a given image. Image segmentation has a variety of applications and has received considerable attention in multimedia application and computer vision. Although numerous approaches have been introduced, image segmentation is still far from being solved due to most of image segmentation algorithms are often so complicated and some unsatisfactory results appear frequently. Therefore, developing a suitable technique of image segmentation is still a challenging problem. In this article, a novel color image segmentation will be introduced based on quaternion radial harmonic Fourier moments (QRHFMs) and proximal classifier. Firstly, the image feature of pixel-level is represented by the accurate and invariant QRHFMs holistically as a vector field, which can describe sufficiently the image pixel information due to take into account the relationship among different color channels. Secondly, the image feature from pixel-level is utilized as the input of the proximal classifier with consistency (PCC), which not only has lower computation time but also has better generalization compared to traditional support vector machines classifiers. Then, we choose the training samples by Tsallis entropy thresholding to train PCC classification model. Finally, the color image is classified by the trained PCC classification model. Our algorithm can make full use of the accurate and robust local image feature, as well the quickness and generalization ability of PCC classifier. A series of experimental results shows that this algorithm has better segmentation performance than the state-of-the-art method from the literature.},
  archive      = {J_PAAA},
  author       = {Wang, Xiang-Yang and Wang, Qian and Wang, Xue-Bin and Yang, Hong-Ying and Wu, Zhi-Fang and Niu, Pan-Pan},
  doi          = {10.1007/s10044-019-00826-y},
  journal      = {Pattern Analysis and Applications},
  month        = {5},
  number       = {2},
  pages        = {683-702},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Color image segmentation using proximal classifier and quaternion radial harmonic fourier moments},
  volume       = {23},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). PCAPooL: Unsupervised feature learning for face recognition
using PCA, LBP, and pyramid pooling. <em>PAAA</em>, <em>23</em>(2),
673–682. (<a href="https://doi.org/10.1007/s10044-019-00818-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Human face is a widely used biometric modality for verification and revealing the identity of a person. In spite of a great deal of research on face recognition, it still is a challenging issue. Recently, the outstanding performance of deep learning has attracted a good deal of research interest for face recognition. In comparison with hand-engineered features, learning-based face features have proven their superiority in encoding discriminative information. Inspired by deep learning, we introduce a simple and efficient unsupervised feature learning scheme for face recognition. This scheme employs principle component analysis (PCA), local binary pattern (LBP), and pyramid pooling. Following the architecture of a convolutional neural network, the proposed scheme contains three types of layers: convolutional, nonlinear, and pooling layers. PCA is used to learn a filter bank for the convolutional layer. This is followed by LBP operator that encodes the local texture and adds nonlinearity to the feature maps of convolutional layer, which are then pooled using spatial pyramid pooling. To corroborate the effectiveness of the scheme (which we call as PCAPool), extensive experiments were performed on challenging benchmark databases: FERET, Yale, Extended Yale B, AR, and multi-PIE. The comparison reveals that PCAPool performs better than the state-of-the-art methods.},
  archive      = {J_PAAA},
  author       = {Alahmadi, Amani and Hussain, Muhammad and Aboalsamh, Hatim A. and Zuair, Mansour},
  doi          = {10.1007/s10044-019-00818-y},
  journal      = {Pattern Analysis and Applications},
  month        = {5},
  number       = {2},
  pages        = {673-682},
  shortjournal = {Pattern Anal. Appl.},
  title        = {PCAPooL: Unsupervised feature learning for face recognition using PCA, LBP, and pyramid pooling},
  volume       = {23},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A robust tangent PCA via shape restoration for shape
variability analysis. <em>PAAA</em>, <em>23</em>(2), 653–671. (<a
href="https://doi.org/10.1007/s10044-019-00822-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a novel method for handling the effects of shape outliers in statistical shape analysis. Usually performed by a variant of classical principal component analysis (PCA), variability analysis may be highly affected by erroneous shapes. Principal components may thus imply aberrant modes, while eigenshapes may not accurately describe variability in a given set of shapes. Our robust analysis is performed using an elastic metric associated with the square-root velocity representation of shapes. This elastic shape analysis allows shape variability to be described with natural and intuitive deformations. The proposed method based on shape outlier detection applies the shape restoration procedure to rectify aberrant shapes. The resultant components are thus obtained from a tangent PCA on the restored database. By performing experiments based on MPEG-7 and HAND databases, we demonstrate that the proposed scheme is effective for shape variability analysis in the presence of outlying shapes. Our method is then compared with two existing schemes for robust data variability analysis: minimum covariance determinant-based PCA and projection pursuit-based PCA.},
  archive      = {J_PAAA},
  author       = {Abboud, Michel and Benzinou, Abdesslam and Nasreddine, Kamal},
  doi          = {10.1007/s10044-019-00822-2},
  journal      = {Pattern Analysis and Applications},
  month        = {5},
  number       = {2},
  pages        = {653-671},
  shortjournal = {Pattern Anal. Appl.},
  title        = {A robust tangent PCA via shape restoration for shape variability analysis},
  volume       = {23},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Segmentation of scanning tunneling microscopy images using
variational methods and empirical wavelets. <em>PAAA</em>,
<em>23</em>(2), 625–651. (<a
href="https://doi.org/10.1007/s10044-019-00824-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the fields of nanoscience and nanotechnology, it is important to be able to functionalize surfaces chemically for a wide variety of applications. Scanning tunneling microscopes (STMs) are important instruments in this area used to measure the surface structure and chemistry with better than molecular resolution. Self-assembly is frequently used to create monolayers that redefine the surface chemistry in just a single-molecule-thick layer (Love et al. in Chem Rev 105(4):1103–1170, 2005; Nuzzo and Allara in J Am Chem Soc 105(13):4481–4483, 1983; Smith et al. in Prog Surf Sci 75(1):1–68, 2004). Indeed, STM images reveal rich information about the structure of self-assembled monolayers since they convey chemical and physical properties of the studied material. In order to assist in and to enhance the analysis of STM and other images (Thomas et al. in ACS Nano 10(5):5446–5451, 2016; Thomas et al. in ACS Nano 9(5):4734–4742, 2015), we propose and demonstrate an image processing framework that produces two image segmentations: One is based on intensities (apparent heights in STM images) and the other is based on textural patterns. The proposed framework begins with a cartoon + texture decomposition, which separates an image into its cartoon and texture components. Afterward, the cartoon image is segmented by a modified multiphase version of the local Chan–Vese model (Wang et al. in Pattern Recognit 43(3):603–618, 2010), while the texture image is segmented by a combination of 2D empirical wavelet transform and a clustering algorithm. Overall, our proposed framework contains several new features, specifically in presenting a new application of cartoon + texture decomposition and of the empirical wavelet transforms and in developing a specialized framework to segment STM images and other data. To demonstrate the potential of our approach, we apply it to raw STM images of various monolayers and present their corresponding segmentation results.},
  archive      = {J_PAAA},
  author       = {Bui, Kevin and Fauman, Jacob and Kes, David and Torres Mandiola, Leticia and Ciomaga, Adina and Salazar, Ricardo and Bertozzi, Andrea L. and Gilles, Jérôme and Goronzy, Dominic P. and Guttentag, Andrew I. and Weiss, Paul S.},
  doi          = {10.1007/s10044-019-00824-0},
  journal      = {Pattern Analysis and Applications},
  month        = {5},
  number       = {2},
  pages        = {625-651},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Segmentation of scanning tunneling microscopy images using variational methods and empirical wavelets},
  volume       = {23},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Violence detection in videos for an intelligent surveillance
system using MoBSIFT and movement filtering algorithm. <em>PAAA</em>,
<em>23</em>(2), 611–623. (<a
href="https://doi.org/10.1007/s10044-019-00821-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Action recognition is an active research area in computer vision as it has enormous applications in today’s world, out of which, recognizing violent action is of great importance since it is closely related to our safety and security. An intelligent surveillance system is the idea of automatically recognizing suspicious activities in surveillance videos and thereby supporting security personals to take up right action on the right time. Under this area, most of the researchers were focused on people detection and tracking, loitering, etc., whereas detecting violent actions or fights is comparatively a less studied area. Previous works considered the local spatiotemporal feature extractors; however, it accompanies the overhead of complex optical flow estimation. Even though the temporal derivative is a fast alternative to optical flow, it alone gives very low accuracy and scales-dependent result. Hence, here we propose a cascaded method of violence detection based on motion boundary SIFT (MoBSIFT) and movement filtering. In this method, the surveillance videos are checked through a movement filtering algorithm based on temporal derivative and avoid most of the nonviolent actions from going through feature extraction. Only the filtered frames may allow going through feature extraction. In addition to scale-invariant feature transform (SIFT) and histogram of optical flow feature, motion boundary histogram is also extracted and combined to form MoBSIFT descriptor. The experimental results show that the proposed MoBSIFT outperforms the existing methods in accuracy by its high tolerance to camera movements. Time complexity has also proved to be reduced by the use of movement filtering along with MoBSIFT.},
  archive      = {J_PAAA},
  author       = {Febin, I. P. and Jayasree, K. and Joy, Preetha Theresa},
  doi          = {10.1007/s10044-019-00821-3},
  journal      = {Pattern Analysis and Applications},
  month        = {5},
  number       = {2},
  pages        = {611-623},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Violence detection in videos for an intelligent surveillance system using MoBSIFT and movement filtering algorithm},
  volume       = {23},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Segmentation-free word spotting in historical bangla
handwritten document using wave kernel signature. <em>PAAA</em>,
<em>23</em>(2), 593–610. (<a
href="https://doi.org/10.1007/s10044-019-00823-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we present a segmentation-free word spotting method based on Wave Kernel Signature (WKS) under the foundation of quantum mechanics. The query word and the document page are smoothened first, then SIFT detector is used to obtain the keypoints in both the query image and the document page. A window is placed centered at each keypoint to obtain the WKS descriptors. The WKS descriptors represent the average probability of measuring a quantum mechanical particle at a specific location based on quantum energy. We use an efficient search technique which calculates minimum energy difference between query word and document image to spot where the query word appears in the document image. The proposed method is tested on three historical Bangla handwritten datasets, one Bangla handwritten dataset, one old Bangla-printed dataset and one historical English handwritten dataset. To substantiate the goodness of the proposed method, its performance is measured using standard metrics.},
  archive      = {J_PAAA},
  author       = {Das, Sugata and Mandal, Sekhar},
  doi          = {10.1007/s10044-019-00823-1},
  journal      = {Pattern Analysis and Applications},
  month        = {5},
  number       = {2},
  pages        = {593-610},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Segmentation-free word spotting in historical bangla handwritten document using wave kernel signature},
  volume       = {23},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A novel 3D dual active contours approach. <em>PAAA</em>,
<em>23</em>(2), 581–591. (<a
href="https://doi.org/10.1007/s10044-019-00796-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper investigates a 3D novel dual active contours approach to segment multiple regions in medical images. The locally based segmentation approaches can handle the heterogeneity of the image as well as the noise artefacts. In this light, a locally based dual active contours approach is proposed to separate among three regions constituting the image. The dual contours approach combines the local information along each point in the two curves conjointly with the information between them. Different parameters in this approach determine its accuracy, including the initial distance between the two curves and how much local the information is used in each curve. The approach’s efficiency is evaluated on synthetic images as well as HRpQCT and MRI data compared to state-of-the-art techniques. The computational cost of this approach is reduced using the convolution operator and the FFT transform. The experimental evaluation of the approach demonstrates its segmentation performance on synthetic images and real medical images.},
  archive      = {J_PAAA},
  author       = {Hafri, Mohamed and Toumi, Hechmi and Lespessailles, Eric and Jennane, Rachid},
  doi          = {10.1007/s10044-019-00796-1},
  journal      = {Pattern Analysis and Applications},
  month        = {5},
  number       = {2},
  pages        = {581-591},
  shortjournal = {Pattern Anal. Appl.},
  title        = {A novel 3D dual active contours approach},
  volume       = {23},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Confidence-based early classification of multivariate time
series with multiple interpretable rules. <em>PAAA</em>, <em>23</em>(2),
567–580. (<a href="https://doi.org/10.1007/s10044-019-00782-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the process of early classification, earliness and accuracy are two key indicators to evaluate the performance of classification, and early classification usually weaken its accuracy to some degree. Therefore, how to find a tradeoff between two conflict objectives is a challenging work. So far, there are just a few work touched the quality of early classification on univariate time series, and the confidence estimation for early classification on multivariate time series (MTS) is still an open issue. In this paper, we focus on interpretably classifying MTS examples as early as possible while guaranteeing the quality of the classification results. First, a fast method is proposed to mine interpretable and local rules from the MTS training data. Second, a valid measure is advanced to estimate the confidence of early classification on MTS examples. Finally, a strategy is designed to execute confident early classification to assume the classification confidence meets customers’ requirement. Experiment results on seven datasets show that the effectiveness and efficiency of our proposed algorithm for confident early classification on multivariate time series.},
  archive      = {J_PAAA},
  author       = {He, Guoliang and Zhao, Wen and Xia, Xuewen},
  doi          = {10.1007/s10044-019-00782-7},
  journal      = {Pattern Analysis and Applications},
  month        = {5},
  number       = {2},
  pages        = {567-580},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Confidence-based early classification of multivariate time series with multiple interpretable rules},
  volume       = {23},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Smooth estimates of multiple quantiles in dynamically
varying data streams. <em>PAAA</em>, <em>23</em>(2), 555–566. (<a
href="https://doi.org/10.1007/s10044-019-00794-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we investigate the problem of estimating multiple quantiles when samples are received online (data stream). We assume a dynamical system, i.e., the distribution of the samples from the data stream changes with time. A major challenge of using incremental quantile estimators to track multiple quantiles is that we are not guaranteed that the monotone property of quantiles will be satisfied, i.e, an estimate of a lower quantile might erroneously overpass that of a higher quantile estimate. Surprisingly, we have only found two papers in the literature that attempt to counter these challenges, namely the works of Cao et al. (Proceedings of the first ACM workshop on mobile internet through cellular networks, ACM, 2009) and Hammer and Yazidi (Proceedings of the 30th international conference on industrial engineering and other applications of applied intelligent systems (IEA/AIE), France, Springer, 2017) where the latter is a preliminary version of the work in this paper. Furthermore, the state-of-the-art incremental quantile estimator called deterministic update-based multiplicative incremental quantile estimator (DUMIQE), due to Yazidi and Hammer (IEEE Trans Cybernet, 2017), fails to guarantee the monotone property when estimating multiple quantiles. A challenge with the solutions, in Cao et al. (2009) and Hammer and Yazidi (2017), is that even though the estimates satisfy the monotone property of quantiles, the estimates can be highly irregular relative to each other which usually is unrealistic from a practical point of view. In this paper, we suggest to generate the quantile estimates by inserting the quantile probabilities (e.g., $$0.1, 0.2, \ldots , 0.9$$) into a monotonically increasing and infinitely smooth function (can be differentiated infinitely many times). The function is incrementally updated from the data stream. The monotonicity and smoothness of the function ensure that both the monotone property and regularity requirement of the quantile estimates are satisfied. The experimental results show that the method performs very well and estimates multiple quantiles more precisely than the original DUMIQE (Yazidi and Hammer 2017), and the approaches reported in Hammer and Yazidi (2017) and Cao et al. (2009).},
  archive      = {J_PAAA},
  author       = {Hammer, Hugo Lewi and Yazidi, Anis},
  doi          = {10.1007/s10044-019-00794-3},
  journal      = {Pattern Analysis and Applications},
  month        = {5},
  number       = {2},
  pages        = {555-566},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Smooth estimates of multiple quantiles in dynamically varying data streams},
  volume       = {23},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). DBSCAN-like clustering method for various data densities.
<em>PAAA</em>, <em>23</em>(2), 541–554. (<a
href="https://doi.org/10.1007/s10044-019-00809-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose a modification of the well-known DBSCAN algorithm, which recognizes clusters with various data densities in a given set of data points $${\mathcal {A}}=\{a^i\in {\mathbb {R}}^n:i=1,\dots ,m\}$$. First, we define the parameter $$MinPts=\lfloor \ln |{\mathcal {A}}|\rfloor$$ and after that, by using a standard procedure from DBSCAN algorithm, for each $$a\in {\mathcal {A}}$$ we determine radius $$\epsilon _a$$ of the circle containing MinPts elements from the set $${\mathcal {A}}$$. We group the set of all these radii into the most appropriate number (t) of clusters by using Least Squares distance-like function applying SymDIRECT or SepDIRECT algorithm. In that way, we obtain parameters $$\epsilon _1&gt;\dots &gt;\epsilon _t$$. Furthermore, for parameters $$\{MinPts,\epsilon _1\}$$ we construct a partition starting with one cluster and then add new clusters for as long as the isolated groups of at least MinPts data points in some circle with radius $$\epsilon _1$$ exist. We follow a similar procedure for other parameters $$\epsilon _2,\dots ,\epsilon _t$$. After the implementation of the algorithm, a larger number of clusters appear than can be expected in the optimal partition. Along with defined criteria, some of them are merged by applying a merging process for which a detailed algorithm has been written. Compared to the standard DBSCAN algorithm, we show an obvious advantage for the case of data with various densities.},
  archive      = {J_PAAA},
  author       = {Scitovski, Rudolf and Sabo, Kristian},
  doi          = {10.1007/s10044-019-00809-z},
  journal      = {Pattern Analysis and Applications},
  month        = {5},
  number       = {2},
  pages        = {541-554},
  shortjournal = {Pattern Anal. Appl.},
  title        = {DBSCAN-like clustering method for various data densities},
  volume       = {23},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Performance evaluation of psycho-acoustically motivated
front-end compensator for TIMIT phone recognition. <em>PAAA</em>,
<em>23</em>(2), 527–539. (<a
href="https://doi.org/10.1007/s10044-019-00816-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Wavelet-based front-end processing technique has gained popularity for its noise removing capability. In this paper, a robust automatic speech recognition system is proposed by utilizing the advantages of psycho-acoustically motivated wavelet-based front-end compensator. In the front-end compensator block, voiced speech probability-based voice activity detector system is designed to separate voiced and unvoiced frames and to update noise statistics. The wavelet packet decomposition tree is designed according to equal rectangular bandwidth (ERB) scale. Wavelet decomposition based on ERB scale is utilized here as the central frequency of the ERB distribution resembles frequency response of human cochlea. Voiced and unvoiced frames are separately decomposed into 24 sub-bands to estimate average sub-band energy (ASE) of each frame. ASE is then used to calculate threshold value. Lastly, Wiener filtering is employed for reducing the residual noise before final reconstruction stage. The proposed system is evaluated on TIMIT database under various noise conditions. The phoneme recognition accuracy of the proposed system is compared with different baseline and robust features as well as with existing front-end compensation techniques. Additionally, the proposed front-end compensator is evaluated in terms of phoneme classification accuracy. Performance improvement is observed in all above experiments.},
  archive      = {J_PAAA},
  author       = {Bhowmick, Anirban and Biswas, Astik and Chandra, Mahesh},
  doi          = {10.1007/s10044-019-00816-0},
  journal      = {Pattern Analysis and Applications},
  month        = {5},
  number       = {2},
  pages        = {527-539},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Performance evaluation of psycho-acoustically motivated front-end compensator for TIMIT phone recognition},
  volume       = {23},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). On enhancing the deadlock-preventing object migration
automaton using the pursuit paradigm. <em>PAAA</em>, <em>23</em>(2),
509–526. (<a href="https://doi.org/10.1007/s10044-019-00817-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Probably, the most reputed solution for partitioning, which has applications in databases, attribute partitioning, processor-based assignment and many other similar scenarios, is the object migration automata (OMA). However, one of the known deficiencies of the OMA is that when the problem size is large, i.e., the number of objects and partitions are large, the probability of receiving a reward, which “strengthens” the current partitioning, from the Environment is not significant. This is because of an internal deadlock scenario which is discussed in this paper. As a result of this, it can take the OMA a considerable number of iterations to recover from an inferior configuration. This property, which characterizes learning automaton (LA) in general, is especially true for the OMA-based methods. In spite of the fact that various solutions have been proposed to remedy this issue for general families of LA, overcoming this hurdle is a completely unexplored area of research for conceptualizing how the OMA should interact with the Environment. Indeed, the best reported version of the OMA, the enhanced OMA (EOMA), has been proposed to mitigate the consequent deadlock scenario. In this paper, we demonstrate that the incorporation of the intrinsic properties of the Environment into the OMA’s design leads to a higher learning capacity and to a more consistent partitioning. To achieve this, we incorporate the state-of-the-art pursuit principle utilized in the field of LA by estimating the Environment’s reward/penalty probabilities and using them to further augment the EOMA. We also verify the performance of our proposed method, referred to as the pursuit EOMA (PEOMA), through simulation, and demonstrate a significant increase in the convergence rate, i.e., by a factor of about forty. It also yields a noticeable reduction in sensitivity to the noise in the Environment. The paper also includes some results obtained for a real-world application domain involving faulty sensors.},
  archive      = {J_PAAA},
  author       = {Shirvani, Abdolreza and Oommen, B. John},
  doi          = {10.1007/s10044-019-00817-z},
  journal      = {Pattern Analysis and Applications},
  month        = {5},
  number       = {2},
  pages        = {509-526},
  shortjournal = {Pattern Anal. Appl.},
  title        = {On enhancing the deadlock-preventing object migration automaton using the pursuit paradigm},
  volume       = {23},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Multi-task non-negative matrix factorization for visual
object tracking. <em>PAAA</em>, <em>23</em>(1), 493–507. (<a
href="https://doi.org/10.1007/s10044-019-00812-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes an online object tracking algorithm in which the object tracking is achieved by using multi-task sparse learning and non-negative matrix factorization under the particle filtering framework. The object appearance is first modeled by subspace learning to reflect the target variations across frames. Combination of non-negative components is learned from examples observed in previous frames. In order to robust tracking an object, group sparsity constraints are included to the non-negativity one. Furthermore, the alternating direction method of multipliers algorithm is employed to compute the model efficiently. Qualitative and quantitative experiments on a variety of challenging sequences show favorable performance of the proposed algorithm against state-of-the-art methods.},
  archive      = {J_PAAA},
  author       = {Wang, Yong and Luo, Xinbin and Ding, Lu and Fu, Shan and Hu, Shiqiang},
  doi          = {10.1007/s10044-019-00812-4},
  journal      = {Pattern Analysis and Applications},
  month        = {2},
  number       = {1},
  pages        = {493-507},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Multi-task non-negative matrix factorization for visual object tracking},
  volume       = {23},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Filter-based feature selection in the context of
evolutionary neural networks in supervised machine learning.
<em>PAAA</em>, <em>23</em>(1), 467–491. (<a
href="https://doi.org/10.1007/s10044-019-00798-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a workbench to get simple neural classification models based on product evolutionary networks via a prior data preparation at attribute level by means of filter-based feature selection. Therefore, the computation to build the classifier is shorter, compared to a full model without data pre-processing, which is of utmost importance since the evolutionary neural models are stochastic and different classifiers with different seeds are required to get reliable results. Feature selection is one of the most common techniques for pre-processing the data within any kind of learning task. Six filters have been tested to assess the proposal. Fourteen (binary and multi-class) difficult classification data sets from the University of California repository at Irvine have been established as the test bed. An empirical study between the evolutionary neural network models obtained with and without feature selection has been included. The results have been contrasted with nonparametric statistical tests and show that the current proposal improves the test accuracy of the previous models significantly. Moreover, the current proposal is much more efficient than the previous methodology; the time reduction percentage is above 40%, on average. Our approach has also been compared with several classifiers both with and without feature selection in order to illustrate the performance of the different filters considered. Lastly, a statistical analysis for each feature selector has been performed providing a pairwise comparison between machine learning algorithms.},
  archive      = {J_PAAA},
  author       = {Tallón-Ballesteros, Antonio J. and Riquelme, José C. and Ruiz, Roberto},
  doi          = {10.1007/s10044-019-00798-z},
  journal      = {Pattern Analysis and Applications},
  month        = {2},
  number       = {1},
  pages        = {467-491},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Filter-based feature selection in the context of evolutionary neural networks in supervised machine learning},
  volume       = {23},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Effect of cluster size distribution on clustering: A
comparative study of k-means and fuzzy c-means clustering.
<em>PAAA</em>, <em>23</em>(1), 455–466. (<a
href="https://doi.org/10.1007/s10044-019-00783-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data distribution has a significant impact on clustering results. This study focuses on the effect of cluster size distribution on clustering, namely the uniform effect of k-means and fuzzy c-means (FCM) clustering. We first provide some related works of k-means and FCM clustering. Then, the structure decomposition analysis of the objective functions of k-means and FCM is presented. Afterward, extensive experiments on both synthetic two-dimensional and three-dimensional data sets and real-world data sets from the UCI machine learning repository are conducted. The results demonstrate that FCM has stronger uniform effect than k-means clustering. Also, it reveals that the fuzzifier value m = 2 in FCM, which has been widely adopted in many applications, is not a good choice, particularly for data sets with great variation in cluster sizes. Therefore, for data sets with significant uneven distributions in cluster sizes, a smaller fuzzifier value is preferred for FCM clustering, and k-means clustering is a better choice compared with FCM clustering.},
  archive      = {J_PAAA},
  author       = {Zhou, Kaile and Yang, Shanlin},
  doi          = {10.1007/s10044-019-00783-6},
  journal      = {Pattern Analysis and Applications},
  month        = {2},
  number       = {1},
  pages        = {455-466},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Effect of cluster size distribution on clustering: A comparative study of k-means and fuzzy c-means clustering},
  volume       = {23},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Adaptive graph-regularized fixed rank representation for
subspace segmentation. <em>PAAA</em>, <em>23</em>(1), 443–453. (<a
href="https://doi.org/10.1007/s10044-019-00786-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Low-rank representation (LRR) has shown its great power in subspace segmentation tasks. However, by using matrix factorization skill, fixed-rank representation dominates LRR in many subspace segmentation applications. In this paper, based on the depth analyses on fixed-rank representation (FRR), we propose a new graph-regularized FRR method which is termed adaptive graph-regularized fixed-rank representation (AGFRR). Different from the existing methods which use the original data set to build the graph regularizer for a reconstruction coefficient matrix, AGFRR uses one of the matrix factor of a reconstruction matrix to construct the graph regularizer for the reconstruction matrix itself. We claim that the constructed graph regularizer can discover the manifold structure of a given data set more faithfully. Hence, AGFRR is more suitable for revealing the nonlinear subspace structures of data sets than FRR. Moreover, an optimization algorithm for solving AGFRR problem is also provided. Finally, the subspace segmentation experiments on both synthetic and real-world data sets show that AGFRR is superior to the existing LRR and FRR-related algorithms.},
  archive      = {J_PAAA},
  author       = {Wei, Lai and Zhou, Rigui and Zhu, Changming and Zhang, Xiafen and Yin, Jun},
  doi          = {10.1007/s10044-019-00786-3},
  journal      = {Pattern Analysis and Applications},
  month        = {2},
  number       = {1},
  pages        = {443-453},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Adaptive graph-regularized fixed rank representation for subspace segmentation},
  volume       = {23},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Pore-based indexing for fingerprints acquired using
high-resolution sensors. <em>PAAA</em>, <em>23</em>(1), 429–441. (<a
href="https://doi.org/10.1007/s10044-019-00805-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the advent of high-resolution fingerprint sensors, there have been efforts to develop high-resolution fingerprint identification systems. In this paper, we present an indexing method for high-resolution fingerprints using pore-based features. In the proposed approach, the dynamic pore filtering method is employed to extract pores from high-resolution fingerprint images. The extracted pores are treated as keypoints, and a pore descriptor is computed for each of the keypoints. The pore descriptor thus obtained is used as a feature vector for indexing. Finally, a cluster-based retrieval scheme is employed for fast and effective retrieval of the candidate list. Performance of the proposed approach has been evaluated on the Hong Kong PolyU high-resolution fingerprint databases, DBI and DBII and in-house databases, IITI-HRFP and IITI-HRF. The proposed indexing approach achieves an improvement of 67%, 49%, 42% and 28% points in pre-selection error rate (for penetration rate of 10%) over the existing method that employs pore features for indexing on DBI, DBII, IITI-HRFP and IITI-HRF, respectively. Most importantly, our approach provides better performance than the state-of-the-art minutiae-based fingerprint indexing algorithm on DBI and IITI-HRFP, which contain partial fingerprints.},
  archive      = {J_PAAA},
  author       = {Anand, Vijay and Kanhangad, Vivek},
  doi          = {10.1007/s10044-019-00805-3},
  journal      = {Pattern Analysis and Applications},
  month        = {2},
  number       = {1},
  pages        = {429-441},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Pore-based indexing for fingerprints acquired using high-resolution sensors},
  volume       = {23},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Single-shot 3D hand pose estimation using radial basis
function networks trained on synthetic data. <em>PAAA</em>,
<em>23</em>(1), 415–428. (<a
href="https://doi.org/10.1007/s10044-019-00801-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this work, we present a novel framework to perform single-shot hand pose estimation using depth data as input. The method follows a coarse to fine strategy and employs several radial basis function networks (RBFNs) that are trained on a dataset containing only synthetically generated depth maps. Thus, compared to most contemporary deep learning approaches, it does not require the laborious annotation of large, real-world datasets. At run time, an initialization RBFN is used to provide a rough estimation of the hand’s 3D pose. Subsequently, several specialized RBFNs are employed to improve that initial estimation in an iterative refinement scheme. To train the RBFNs, we select a set of hand poses from a real-world sequence that are as diverse as possible. We use this representative set, along with a dense sampling of all possible rotations, as a seed to generate a large synthetic training set. The method is parallelizable, taking advantage of the inherent data parallelism of RBFNs. Furthermore, the method requires few real-world data and virtually no manual annotation. We perform a quantitative evaluation of our method on a testing sequence of our own. We also present quantitative and qualitative results on a public dataset that is commonly used to evaluate hand pose estimation and tracking methods. We show that in all cases, our approach achieves promising results. Moreover, it can achieve comparable or even faster computational performance than current deep learning approaches but on a single CPU core, i.e., without requiring GPU processing.},
  archive      = {J_PAAA},
  author       = {Nicodemou, Vassilis C. and Oikonomidis, Iason and Argyros, Antonis},
  doi          = {10.1007/s10044-019-00801-7},
  journal      = {Pattern Analysis and Applications},
  month        = {2},
  number       = {1},
  pages        = {415-428},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Single-shot 3D hand pose estimation using radial basis function networks trained on synthetic data},
  volume       = {23},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Real-time and robust multiple-view gender classification
using gait features in video surveillance. <em>PAAA</em>,
<em>23</em>(1), 399–413. (<a
href="https://doi.org/10.1007/s10044-019-00802-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {It is common to view people in real applications walking in arbitrary directions, holding items, or wearing heavy coats. These factors are challenges in gait-based application methods, because they significantly change a person‘s appearance. This paper proposes a novel method for classifying human gender in real time using gait information. The use of an average gait image, rather than a gait energy image, allows this method to be computationally efficient and robust against view changes. A viewpoint model is created for automatically determining the viewing angle during the testing phase. A distance signal model is constructed to remove any areas with an attachment (carried items, worn coats) from a silhouette to reduce the interference in the resulting classification. Finally, the human gender is classified using multiple-view-dependent classifiers trained using a support vector machine. Experiment results confirm that the proposed method achieves a high accuracy of 98.8% on the CASIA Dataset B and outperforms the recent state-of-the-art methods.},
  archive      = {J_PAAA},
  author       = {Do, Trung Dung and Nguyen, Van Huan and Kim, Hakil},
  doi          = {10.1007/s10044-019-00802-6},
  journal      = {Pattern Analysis and Applications},
  month        = {2},
  number       = {1},
  pages        = {399-413},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Real-time and robust multiple-view gender classification using gait features in video surveillance},
  volume       = {23},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). RTS-ELM: An approach for saliency-directed image
segmentation with ripplet transform. <em>PAAA</em>, <em>23</em>(1),
385–397. (<a href="https://doi.org/10.1007/s10044-019-00800-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In spite of great advancements in the field of computer vision in recent times, efficient identification of salient regions in an image/scene and applying the results to image segmentation are a fertile area to be explored by researchers. This paper deals with a novel approach for image segmentation called RTS-ELM which uses cues from salient region identification. Initially, salient regions of an image are identified using ripplet transform. Based on the saliency map, a trimap is generated for an image which highlights the dominant regions of an image. Using histogram analysis, the dominant pixels of foreground and background are grouped together to produce the positive and negative groups of training data. The salient regions are then segmented using the trained ELM classifier. After a rigmarole process of comparing with eleven extant approaches using three benchmark datasets, RTS-ELM is found to be an efficient method for reifying effective segmentation in different types of images with only a few errors.},
  archive      = {J_PAAA},
  author       = {Andrushia, A. Diana and Thangarajan, R.},
  doi          = {10.1007/s10044-019-00800-8},
  journal      = {Pattern Analysis and Applications},
  month        = {2},
  number       = {1},
  pages        = {385-397},
  shortjournal = {Pattern Anal. Appl.},
  title        = {RTS-ELM: An approach for saliency-directed image segmentation with ripplet transform},
  volume       = {23},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A novel high-performance holistic descriptor for face
retrieval. <em>PAAA</em>, <em>23</em>(1), 371–383. (<a
href="https://doi.org/10.1007/s10044-019-00803-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Texture extraction-based classification has become the facto methodology applied in face recognition. Haralick feature extraction from gray-level co-occurrence matrix (GLCM) is one of the basic holistic studies that has inspired many face recognition algorithms. This paper presents a theoretically simple, yet efficient, holistic approach that utilizes the spatial relationships of the same pixel patterns occurring at different positions in an image rather than their occurrence statistics as applied in GLCM-based counterparts. The matrix holding the statistical values for the total displacement of the pixel patterns is called the gray-level total displacement matrix (GLTDM). Three approaches are proposed for feature extraction. In the first approach, classical Haralick features extraction is conducted. The second approach (D_GLTDM) utilizes the GLTDM directly as the feature vector rather than extra feature extraction process. In the last approach, principle component analysis (PCA) is used as the feature extraction method. Comprehensive simulations are conducted on images retrieved from the popular face databases, namely face94, ORL, JAFFE and Yale. The performance of the proposed method is compared with that of GLCM, local binary pattern and PCA used in the leading studies. The simulation results and their comparative analysis show that D_GLTDM exhibits promising results and outperforms the other leading methods in terms of classification accuracy.},
  archive      = {J_PAAA},
  author       = {Çevik, Nazife and Çevik, Taner},
  doi          = {10.1007/s10044-019-00803-5},
  journal      = {Pattern Analysis and Applications},
  month        = {2},
  number       = {1},
  pages        = {371-383},
  shortjournal = {Pattern Anal. Appl.},
  title        = {A novel high-performance holistic descriptor for face retrieval},
  volume       = {23},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Entropy-based multi-view matrix completion for clustering
with side information. <em>PAAA</em>, <em>23</em>(1), 359–370. (<a
href="https://doi.org/10.1007/s10044-019-00797-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-view clustering aims to group multi-view samples into different clusters based on the similarity. Since side information can describe the relation between samples, for example, must-links and cannot-links, thus multi-view clustering with the consideration about side information along with samples can get more feasible clustering results. As a recent developed multi-view clustering approach, multi-view matrix completion (MVMC) constructs similarity matrix for each view and casts clustering into a matrix completion problem. Different from traditional multi-view clustering approaches, MVMC enforces the consistency of clustering results on different views as constraints for alternative optimization and the global optimal solution can be obtained. Although related experiments show that MVMC exhibits impressive performance, it still neglects the possibility of a sample belonging to a cluster. In this paper, we consider the possibility on the base of entropy and develop an entropy-based multi-view matrix completion for clustering with side information (EMVMC). Experiments on multi-view datasets Course, Citeseer, Cora, WebKB, NewsGroup, and Reuters validate the effectiveness of EMVMC.},
  archive      = {J_PAAA},
  author       = {Zhu, Changming and Miao, Duoqian},
  doi          = {10.1007/s10044-019-00797-0},
  journal      = {Pattern Analysis and Applications},
  month        = {2},
  number       = {1},
  pages        = {359-370},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Entropy-based multi-view matrix completion for clustering with side information},
  volume       = {23},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Correlation maximization machine for multi-modalities
multiclass classification. <em>PAAA</em>, <em>23</em>(1), 349–358. (<a
href="https://doi.org/10.1007/s10044-019-00795-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Support vector machine (SVM) learns the maximum margin to separate training examples that belong to two classes and has been widely used in many pattern recognition tasks due to its high effectiveness. However, conventional SVM suffers from the following deficiencies: (1) SVM cannot take full advantage of multiple modalities in a dataset if they are available, and (2) SVM trains the marginal hyper-plane by solving the quadratic programming problem, and thus costs too much computational overheads. In this paper, we propose a correlation maximization machine (CMM) model to overcome the aforementioned deficiencies by integrating two modalities in a dataset to boost the classification performance and utilizing randomized nonlinear features to output labels from multiple classes. In particular, CMM reveals the nonlinear relationships among both modalities by generating randomized nonlinear features for each modality. CMM learns to project these features into a common subspace with a constraint that their coefficients are highly correlated, and narrows the gap between the coefficients of both modalities and the class indicator of each training example to deal with multiclass classification problem. At the classification stage, CMM indicates the classes of a test example by using the summation of its coefficients of both modalities. Since the objective function of CMM is non-convex, it is quite difficult to obtain the global minimum. In this paper, we developed a block coordinate descent-based algorithm to optimize CMM and theoretically proved its convergence to a local minimum. Experimental results of face recognition on three popular face image datasets and experimental results of image retrieval on CIFAR-10, NUS-Wide, and Wikipedia datasets demonstrate that CMM outperforms the representative methods.},
  archive      = {J_PAAA},
  author       = {Yang, Canqun and Guan, Naiyang},
  doi          = {10.1007/s10044-019-00795-2},
  journal      = {Pattern Analysis and Applications},
  month        = {2},
  number       = {1},
  pages        = {349-358},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Correlation maximization machine for multi-modalities multiclass classification},
  volume       = {23},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Automatic grayscale image segmentation based on affinity
propagation clustering. <em>PAAA</em>, <em>23</em>(1), 331–348. (<a
href="https://doi.org/10.1007/s10044-019-00785-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image segmentation is an important research subject in the field of image analysis and pattern recognition. Based on the affinity propagation (AP) clustering algorithm, an automatic segmentation method is proposed for grayscale images. The AP algorithm is used for image segmentation to avoid the choice of initial clustering centers and enhance the stability of the segmentation results, and a new index is proposed to analyze the validity of segmentation results and determine the optimal number of segments. Moreover, gray values instead of pixels in gray space are clustered as the dataset to decrease the time complexity of the similarity matrix and validity analysis. Theoretical analysis and experimental results demonstrate the effectiveness of the proposed index and method.},
  archive      = {J_PAAA},
  author       = {Zhou, Shibing and Xu, Zhenyuan},
  doi          = {10.1007/s10044-019-00785-4},
  journal      = {Pattern Analysis and Applications},
  month        = {2},
  number       = {1},
  pages        = {331-348},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Automatic grayscale image segmentation based on affinity propagation clustering},
  volume       = {23},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Face detection based on evolutionary haar filter.
<em>PAAA</em>, <em>23</em>(1), 309–330. (<a
href="https://doi.org/10.1007/s10044-019-00784-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Face detection is considered to be one of the principal techniques of biometrics. Several methods for face detection have been proposed and described in the literature, but the Viola and Jones method is one of the most prominent. This method is based on the principle of Haar filters. In this study, we propose a new type of Haar filter called a dispersed Haar filter. This new structure provides more flexibility for very complex geometry, such as the human face. To create the structure of the filter, we used three optimizations methods: differential evolution (DE), genetic algorithm (GA), and particle swarm optimization (PSO). To test our approaches rigorously, we performed two types of tests. The first test is facial detection on fixed images from three different databases (Caltech 10K, FDDB, and CMU-MIT), which presents a significant challenge. The second test is more efficient and involves the recognition of human faces from a video database. For our experiment, we used a YouTube celebrity dataset. This system consists of two stages:The proposed Haar-DE algorithm demonstrates good detection performance on several databases compared with the state-of-the-art methods.},
  archive      = {J_PAAA},
  author       = {Besnassi, Miloud and Neggaz, Nabil and Benyettou, Abdelkader},
  doi          = {10.1007/s10044-019-00784-5},
  journal      = {Pattern Analysis and Applications},
  month        = {2},
  number       = {1},
  pages        = {309-330},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Face detection based on evolutionary haar filter},
  volume       = {23},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). ML-SLSTSVM: A new structural least square twin support
vector machine for multi-label learning. <em>PAAA</em>, <em>23</em>(1),
295–308. (<a href="https://doi.org/10.1007/s10044-019-00779-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-label learning (MLL) is a special supervised learning task, where any single instance possibly belongs to several classes simultaneously. Nowadays, MLL methods are increasingly required by modern applications, such as protein function classification, speech recognition and textual data classification. In this paper, a structural least square twin support vector machine (SLSTSVM) classifier for multi-label learning is presented. This proposed ML-SLSTSVM focuses on the cluster-based structural information of the corresponding class in each optimization problem, which is vital for designing a good classifier in different real-world problems. This method is extended to a nonlinear version by the kernel trick. Experimental results demonstrate that proposed method is superior in generalization performance to other classifiers.},
  archive      = {J_PAAA},
  author       = {Azad-Manjiri, Meisam and Amiri, Ali and Saleh Sedghpour, Alireza},
  doi          = {10.1007/s10044-019-00779-2},
  journal      = {Pattern Analysis and Applications},
  month        = {2},
  number       = {1},
  pages        = {295-308},
  shortjournal = {Pattern Anal. Appl.},
  title        = {ML-SLSTSVM: A new structural least square twin support vector machine for multi-label learning},
  volume       = {23},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Human action recognition: A framework of statistical
weighted segmentation and rank correlation-based selection.
<em>PAAA</em>, <em>23</em>(1), 281–294. (<a
href="https://doi.org/10.1007/s10044-019-00789-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Human action recognition from a video sequence has received much attention lately in the field of computer vision due to its range of applications in surveillance, healthcare, smart homes, tele-immersion, to name but a few. However, it is still facing several challenges such as human variations, occlusion, change in illumination, complex background. In this article, we consider the problems related to multiple human detection and classification using novel statistical weighted segmentation and rank correlation-based feature selection approach. Initially, preprocessing is performed on a set of frames to remove existing noise and to make the foreground maximal differentiable compared to the background. A novel weighted segmentation method is also introduced for human extraction prior to feature extraction. Ternary features are exploited including color, shape, and texture, which are later combined using serial-based features fusion method. To avoid redundancy, rank correlation-based feature selection technique is employed, which acts as a feature optimizer and leads to improved classification accuracy. The proposed method is validated on six datasets including Weizmann, KTH, Muhavi, WVU, UCF sports, and MSR action and validated based on seven performance measures. A fair comparison with existing work is also provided which proves the significance of proposed compared to other techniques.},
  archive      = {J_PAAA},
  author       = {Sharif, Muhammad and Khan, Muhammad Attique and Zahid, Farooq and Shah, Jamal Hussain and Akram, Tallha},
  doi          = {10.1007/s10044-019-00789-0},
  journal      = {Pattern Analysis and Applications},
  month        = {2},
  number       = {1},
  pages        = {281-294},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Human action recognition: A framework of statistical weighted segmentation and rank correlation-based selection},
  volume       = {23},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Video spatiotemporal mapping for human action recognition by
convolutional neural network. <em>PAAA</em>, <em>23</em>(1), 265–279.
(<a href="https://doi.org/10.1007/s10044-019-00788-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, a 2D representation of a video clip called video spatiotemporal map (VSTM) is presented. VSTM is a compact representation of a video clip which incorporates its spatial and temporal properties. It is created by vertical concatenation of feature vectors generated from subsequent frames. The feature vector corresponding to each frame is generated by applying wavelet transform to that frame (or its subtraction from the subsequent frame) and computing vertical and horizontal projection of quantized coefficients of some specific wavelet subbands. VSTM enables convolutional neural networks (CNNs) to process a video clip for human action recognition (HAR). The proposed approach benefits from power of CNNs to analyze visual patterns and attempts to overcome some CNN challenges such as variable video length problem and lack of training data that leads to over-fitting. VSTM presents a sequence of frames to CNN without imposing any additional computational cost to the CNN learning algorithm. The experimental results of the proposed method on the KTH, Weizmann, and UCF Sports HAR benchmark datasets have shown the supremacy of the proposed method compared with the state-of-the-art methods that used CNN to solve HAR problem.},
  archive      = {J_PAAA},
  author       = {Zare, Amin and Abrishami Moghaddam, Hamid and Sharifi, Arash},
  doi          = {10.1007/s10044-019-00788-1},
  journal      = {Pattern Analysis and Applications},
  month        = {2},
  number       = {1},
  pages        = {265-279},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Video spatiotemporal mapping for human action recognition by convolutional neural network},
  volume       = {23},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Weighted feature-task-aware regularization learner for
multitask learning. <em>PAAA</em>, <em>23</em>(1), 253–263. (<a
href="https://doi.org/10.1007/s10044-019-00781-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multitask learning has recently received extensive attention due to the fact that it can share knowledge between tasks and improve the collective performance leverage shared structures among the tasks to jointly build a better model for each task. However, most existing multitask learning methods only focus on selecting features across the tasks, how to enhance the sparsity of the learned variables is not taken into consideration there is little concern on how to enhance the sparsity of the learned variables. In this paper, we first present a weighted feature-task-aware regularization learning model for multitask learning in order to enhance the sparsity of the weight matrices, and then propose an online learning algorithm to train the proposed model together with theoretical guarantee. Finally, we conduct experiments to compare the resulting approach with some related methods used for multitask learning, which illustrates the efficiency of the proposed method. To verify the effectiveness of the proposed approach, extensive experiments are conducted on two widely used data for multitask learning. The encouraging performance of the proposed approach over the related methods demonstrates its superiority.},
  archive      = {J_PAAA},
  author       = {Xue, Wei},
  doi          = {10.1007/s10044-019-00781-8},
  journal      = {Pattern Analysis and Applications},
  month        = {2},
  number       = {1},
  pages        = {253-263},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Weighted feature-task-aware regularization learner for multitask learning},
  volume       = {23},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Deep learned inter-channel colored texture pattern: A new
chromatic-texture descriptor. <em>PAAA</em>, <em>23</em>(1), 239–251.
(<a href="https://doi.org/10.1007/s10044-019-00780-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Texture is a dominant tool for feature extraction. Incorporation of inter-channel chromatic information along with the texture feature will improve the accuracy of feature extraction. This paper provides deep learned Inter-Channel Colored Texture Pattern which gives the inter-channel chromatic texture information of an image. The information is extracted individually from the co-occurrent pixel values of various channels. It affords the unique channel-wise information and its relation with neighboring pixel information of opponent space. Deep learning with convolutional neural network is applied for learning the feature based on color and texture. The experiments for content-based image retrieval are carried out on three different databases which vary in nature: CIFAR-10 dataset (DB1) (Krizhevsky in Learning multiple layers of features from tiny images, University of Toronto, 2009), Corel database (DB2) (Corel 1000 and Corel 10000 image database, http://wang.ist.psu.edu/docs/related.shtml) and Facescrub dataset (DB3) (Ng and Winkler in 2014 IEEE international conference on image processing (ICIP), pp 343–347, 2014). Facescrub dataset is used for face recognition. The experimental analysis by applying this descriptor provides considerable improvement from the previous works for content-based image retrieval and face recognition.},
  archive      = {J_PAAA},
  author       = {Jeena Jacob, I. and Srinivasagan, K. G. and Ebby Darney, P. and Jayapriya, K.},
  doi          = {10.1007/s10044-019-00780-9},
  journal      = {Pattern Analysis and Applications},
  month        = {2},
  number       = {1},
  pages        = {239-251},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Deep learned inter-channel colored texture pattern: A new chromatic-texture descriptor},
  volume       = {23},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Tracking of multiple quantiles in dynamically varying data
streams. <em>PAAA</em>, <em>23</em>(1), 225–237. (<a
href="https://doi.org/10.1007/s10044-019-00778-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we consider the problem of tracking multiple quantiles of dynamically varying data stream distributions. The method is based on making incremental updates of the quantile estimates every time a new sample is received. The method is memory and computationally efficient since it only stores one value for each quantile estimate and only performs one operation per quantile estimate when a new sample is received from the data stream. The estimates are realistic in the sense that the monotone property of quantiles is satisfied in every iteration. Experiments show that the method efficiently tracks multiple quantiles and outperforms state-of-the-art methods.},
  archive      = {J_PAAA},
  author       = {Hammer, Hugo Lewi and Yazidi, Anis and Rue, Håvard},
  doi          = {10.1007/s10044-019-00778-3},
  journal      = {Pattern Analysis and Applications},
  month        = {2},
  number       = {1},
  pages        = {225-237},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Tracking of multiple quantiles in dynamically varying data streams},
  volume       = {23},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Hybrid minutiae and edge corners feature points for
increased fingerprint recognition performance. <em>PAAA</em>,
<em>23</em>(1), 213–224. (<a
href="https://doi.org/10.1007/s10044-018-00766-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In general, most fingerprint recognition systems are based on the minutiae feature points. When matching two fingerprint images, the goal in most recognition systems is to find the optimal transformation model that aligns their feature points in order to find among them the number of matched or aligned points and then generate a matching score. A major problem in feature extraction stage is that when the fingerprint image is of a poor quality due to skin conditions and sensor noise, that leads to many broken ridges in the image caused by cutline. In this case, the extraction of minutiae leads to a lot of spurious points and the performance of the system will degrade. Usually, image enhancement techniques are applied as preprocessing step to overcome this problem. In this work, we propose to use corner points on fingerprint ridges as new features in addition to the ridges minutiae in order to improve the recognition performance. Every ridge is decomposed into several straight edges (SEs). A straight edge is defined as a straight link of ridge points. On a ridge, the head of the first straight edge and the tail of the last one are two minutia and the intersections of the SEs are the ridge corners. Thus, we propose to use a ridge as primitive rather than individual points for matching. This primitive is a structure consisting of groups of both feature points which are minutiae and corners belonging to the same ridge. Based on this primitive, an intelligent matching technique is introduced using sets of feature points on the same primitive. As a result, the recognition performance is increased since it is based on ridge primitive matching rather than individual minutiae matching. Finally, our experimental results compared with those obtained by other well-known techniques in the literature demonstrate the effectiveness and efficiency of our proposed algorithm.},
  archive      = {J_PAAA},
  author       = {Nachar, Rabih and Inaty, Elie and Bonnin, Patrick J. and Alayli, Yasser},
  doi          = {10.1007/s10044-018-00766-z},
  journal      = {Pattern Analysis and Applications},
  month        = {2},
  number       = {1},
  pages        = {213-224},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Hybrid minutiae and edge corners feature points for increased fingerprint recognition performance},
  volume       = {23},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A novel nearest interest point classifier for offline tamil
handwritten character recognition. <em>PAAA</em>, <em>23</em>(1),
199–212. (<a href="https://doi.org/10.1007/s10044-018-00776-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Handwritten character recognition is the most widely used branch of study in image pattern recognition. Tamil, the official language of Tamil Nadu in South India, Sri Lanka, Singapore and Malaysia, has a script which contains many loops and compound characters, with small differences between character classes. Most of the research on offline Tamil handwritten character recognition system was done only on few character classes as it is very difficult to distinguish between minute dissimilarities of large character classes. It is important to design a complete recognition system that can process all character classes of Tamil and distinguish natural variability between inter-class images. Unlike conventional machine learning approaches for pattern recognition problems, we have proposed a nearest interest point classifier, which can choose sufficient and necessary subset of features from a variable length high dimensional feature vector. Since this is a practical problem, in this work, a study on image to image matching is included through feature analysis without using machine learning approaches. The proposed algorithm gave a good recognition accuracy for all the character classes on the standard database available for Tamil, HP Labs offline Tamil handwritten character database. Our proposed classifier produced a recognition accuracy of 90.2% while including the whole dataset. The method has been compared with the standard classifiers and has been proved to be a state-of-the-art performance in recognition of accuracy over the previous results given in the literature.},
  archive      = {J_PAAA},
  author       = {Ashlin Deepa, R. N. and Rajeswara Rao, R.},
  doi          = {10.1007/s10044-018-00776-x},
  journal      = {Pattern Analysis and Applications},
  month        = {2},
  number       = {1},
  pages        = {199-212},
  shortjournal = {Pattern Anal. Appl.},
  title        = {A novel nearest interest point classifier for offline tamil handwritten character recognition},
  volume       = {23},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Enhancement of speech signal using diminished empirical mean
curve decomposition-based adaptive wiener filtering. <em>PAAA</em>,
<em>23</em>(1), 179–198. (<a
href="https://doi.org/10.1007/s10044-018-00768-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {During the last few decades, speech signal enhancement has been one of the wide-spreading research topics. Numerous algorithms are being proposed to enhance the perceptibility and the quality of speech signal. These algorithms are often formulated to recover the clear signal from the signals that are ruined by noise. Usually, short-time Fourier transform and wavelet transform are widely used to process the speech signal. This paper attempts to overcome the regular drawbacks of the speech enhancement algorithms. As the frequency domain has good noise-removing ability, the short-time Fourier domain is also aimed to enhance the speech. Additionally, this paper introduces a decomposition model, named diminished empirical mean curve decomposition, to adaptively tune the Wiener filtering process and to accomplish effective speech enhancement. The performances of the proposed method and the conventional methods are compared, and it is observed that the proposed method is superior to the conventional methods.},
  archive      = {J_PAAA},
  author       = {Garg, Anil and Sahu, O. P.},
  doi          = {10.1007/s10044-018-00768-x},
  journal      = {Pattern Analysis and Applications},
  month        = {2},
  number       = {1},
  pages        = {179-198},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Enhancement of speech signal using diminished empirical mean curve decomposition-based adaptive wiener filtering},
  volume       = {23},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A recursive partitioning approach for subgroup
identification in brain–behaviour correlation analysis. <em>PAAA</em>,
<em>23</em>(1), 161–177. (<a
href="https://doi.org/10.1007/s10044-018-00775-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In neural correlates studies, the goal is to understand the brain–behaviour relationship characterized by correlation between brain activation responses and human behaviour measures. Such correlation depends on subject-related covariates such as age and gender, so it is necessary to identify subgroups within the population that have different brain–behaviour correlations. The subgrouping is made by manual specification in current practice, which is inefficient and may ignore potential covariates whose effects are unknown in the literature. This study proposes a recursive partitioning approach, called correlation tree, for automatic subgroup identification in brain–behaviour correlation analysis. In constructing a correlation tree, the split variable at each node is selected through an unbiased variable selection method based on partial correlation test, and then, the optimal cutpoint of the selected split variable is determined through exhaustive search under an objective function. Three types of meaningful objective functions are considered to meet various practical needs. Results of simulation and application to real data from optical brain imaging demonstrate effectiveness of the proposed approach.},
  archive      = {J_PAAA},
  author       = {Choi, Doowon and Li, Lin and Liu, Hanli and Zeng, Li},
  doi          = {10.1007/s10044-018-00775-y},
  journal      = {Pattern Analysis and Applications},
  month        = {2},
  number       = {1},
  pages        = {161-177},
  shortjournal = {Pattern Anal. Appl.},
  title        = {A recursive partitioning approach for subgroup identification in brain–behaviour correlation analysis},
  volume       = {23},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Understanding temporal structure for video captioning.
<em>PAAA</em>, <em>23</em>(1), 147–159. (<a
href="https://doi.org/10.1007/s10044-018-00770-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent research in convolutional and recurrent neural networks has fueled incredible advances in video understanding. We propose a video captioning framework that achieves the performance and quality necessary to be deployed in distributed surveillance systems. Our method combines an efficient hierarchical architecture with novel attention mechanisms at both the local and global levels. By shifting focus to different spatiotemporal locations, attention mechanisms correlate sequential outputs with activation maps, offering a clever way to adaptively combine multiple frames and locations of video. As soft attention mixing weights are solved via back-propagation, the number of weights or input frames needs to be known in advance. To remove this restriction, our video understanding framework combines continuous attention mechanisms over a family of Gaussian distributions. Our efficient multistream hierarchical model combines a recurrent architecture with a soft hierarchy layer using both equally spaced and dynamically localized boundary cuts. As opposed to costly volumetric attention approaches, we use video attributes to steer temporal attention. Our fully learnable end-to-end approach helps predict salient temporal regions of action/objects in the video. We demonstrate state-of-the-art captioning results on the popular MSVD, MSR-VTT and M-VAD video datasets and compare several variants of the algorithm suitable for real-time applications. By adjusting the frame rate, we show a single computer can generate effective video captions for 100 simultaneous cameras. We additionally perform studies to show how bit rate compression modifies captioning results.},
  archive      = {J_PAAA},
  author       = {Sah, Shagan and Nguyen, Thang and Ptucha, Ray},
  doi          = {10.1007/s10044-018-00770-3},
  journal      = {Pattern Analysis and Applications},
  month        = {2},
  number       = {1},
  pages        = {147-159},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Understanding temporal structure for video captioning},
  volume       = {23},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Iterative scheme-inspired network for impulse noise removal.
<em>PAAA</em>, <em>23</em>(1), 135–145. (<a
href="https://doi.org/10.1007/s10044-018-0762-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a supervised data-driven algorithm for impulse noise removal via iterative scheme-inspired network (IIN). IIN is defined over a data flow graph, which is derived from the iterative procedures in Alternating Direction Method of Multipliers (ADMM) algorithm for optimizing the L1-guided variational model. In the training phase, the L1-minimization is reformulated into an augmented Lagrangian scheme through adding a new auxiliary variable. In the testing phase, it has computational overhead similar to ADMM but uses optimized parameters learned from the training data for restoration task. Experimental results demonstrate that the newly proposed method can obtain very significantly superior performance than current state-of-the-art variational and dictionary learning-based approaches for salt-and-pepper noise removal.},
  archive      = {J_PAAA},
  author       = {Zhang, Minghui and Liu, Yiling and Li, Guanyu and Qin, Binjie and Liu, Qiegen},
  doi          = {10.1007/s10044-018-0762-8},
  journal      = {Pattern Analysis and Applications},
  month        = {2},
  number       = {1},
  pages        = {135-145},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Iterative scheme-inspired network for impulse noise removal},
  volume       = {23},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Rare association rule mining from incremental databases.
<em>PAAA</em>, <em>23</em>(1), 113–134. (<a
href="https://doi.org/10.1007/s10044-018-0759-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Rare association rule mining is an imperative field of data mining that attempts to identify rare correlations among the items in a database. Although numerous attempts pertaining to rare association rule mining can be found in the literature, there are still certain issues that need utmost attention. The most prominent one among them is the rare association rule mining from incremental databases. The existing rare association rule mining techniques are capable of operating only on static databases, assuming that the entire database to be operated on is available during the outset of the mining process. Inclusion of new records, however, may lead to the generation of some new interesting rules from the current set of data, invalidating the previously extracted significant rare association rules. Executing the entire mining process from scratch for the newly arrived set of data could be a tedious affair. With a view to resolve the issue of incremental rare association rule mining, this study presents a single-pass tree-based approach for extracting rare association rules when new data are inserted into the original database. The proposed approach is capable of generating the complete set of frequent and rare patterns without rescanning the updated database and reconstructing the entire tree structure when new transactions are added to the existent database. Experimental evaluation has been carried out on several benchmark real and synthetic datasets to analyze the efficiency of the proposed approach. Furthermore, to assess its applicability in real-world applications, experimental analysis has been performed on a real geological dataset where earthquake records are incrementally being added on an annual basis. Comparative performance analysis demonstrates the preeminence of proposed approach over existing frequent and rare association rule mining techniques.},
  archive      = {J_PAAA},
  author       = {Borah, Anindita and Nath, Bhabesh},
  doi          = {10.1007/s10044-018-0759-3},
  journal      = {Pattern Analysis and Applications},
  month        = {2},
  number       = {1},
  pages        = {113-134},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Rare association rule mining from incremental databases},
  volume       = {23},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Towards instance-dependent label noise-tolerant
classification: A probabilistic approach. <em>PAAA</em>, <em>23</em>(1),
95–111. (<a href="https://doi.org/10.1007/s10044-018-0750-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Learning from labelled data is becoming more and more challenging due to inherent imperfection of training labels. Existing label noise-tolerant learning machines were primarily designed to tackle class-conditional noise which occurs at random, independently from input instances. However, relatively less attention was given to a more general type of label noise which is influenced by input features. In this paper, we try to address the problem of learning a classifier in the presence of instance-dependent label noise by developing a novel label noise model which is expected to capture the variation of label noise rate within a class. This is accomplished by adopting a probability density function of a mixture of Gaussians to approximate the label flipping probabilities. Experimental results demonstrate the effectiveness of the proposed method over existing approaches.},
  archive      = {J_PAAA},
  author       = {Bootkrajang, Jakramate and Chaijaruwanich, Jeerayut},
  doi          = {10.1007/s10044-018-0750-z},
  journal      = {Pattern Analysis and Applications},
  month        = {2},
  number       = {1},
  pages        = {95-111},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Towards instance-dependent label noise-tolerant classification: A probabilistic approach},
  volume       = {23},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). An image segmentation method based on mumford–shah model
with mask factor and neighborhood factor. <em>PAAA</em>, <em>23</em>(1),
85–94. (<a href="https://doi.org/10.1007/s10044-018-0730-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A novel image segmentation model is proposed to improve the stability of existing segmentation methods. In the proposed model, we introduce two factors into the Mumford–Shah model, including mask factor and neighborhood factor. Firstly, the mask factor can express the image more accurately. Therefore, the new segmentation model can more realistically reflect the structure of the image. Moreover, neighborhood factor is used to constrain the evolution of the initial contour. Then the segmentation model is converted into an equivalent form by a level set function. At last, the model can be solved in a simple way based on partial differential equations and extreme values. The experimental results show the proposed method could generate accurate segmentation results, and the segmentation results are not sensitive to initial contour and external disturbances, such as noise and blurring.},
  archive      = {J_PAAA},
  author       = {Zhou, Luoyu and Zhang, Zhengbing},
  doi          = {10.1007/s10044-018-0730-3},
  journal      = {Pattern Analysis and Applications},
  month        = {2},
  number       = {1},
  pages        = {85-94},
  shortjournal = {Pattern Anal. Appl.},
  title        = {An image segmentation method based on Mumford–Shah model with mask factor and neighborhood factor},
  volume       = {23},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Automatic computing of number of clusters for color image
segmentation employing fuzzy c-means by extracting chromaticity features
of colors. <em>PAAA</em>, <em>23</em>(1), 59–84. (<a
href="https://doi.org/10.1007/s10044-018-0729-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper we introduce a method for color image segmentation by computing automatically the number of clusters the data, pixels, are divided into using fuzzy c-means. In several works the number of clusters is defined by the user. In other ones the number of clusters is computed by obtaining the number of dominant colors, which is determined with unsupervised neural networks (NN) trained with the image’s colors; the number of dominant colors is defined by the number of the most activated neurons. The drawbacks with this approach are as follows: (1) The NN must be trained every time a new image is given and (2) despite employing different color spaces, the intensity data of colors are used, so the undesired effects of non-uniform illumination may affect computing the number of dominant colors. Our proposal consists in processing the images with an unsupervised NN trained previously with chromaticity samples of different colors; the number of the neurons with the highest activation occurrences defines the number of clusters the image is segmented. By training the NN with chromatic data of colors it can be employed to process any image without training it again, and our approach is, to some extent, robust to non-uniform illumination. We perform experiments with the images of the Berkeley segmentation database, using competitive NN and self-organizing maps; we compute and compare the quantitative evaluation of the segmented images obtained with related works using the probabilistic random index and variation of information metrics.},
  archive      = {J_PAAA},
  author       = {García-Lamont, Farid and Cervantes, Jair and López-Chau, Asdrúbal and Yee-Rendón, Arturo},
  doi          = {10.1007/s10044-018-0729-9},
  journal      = {Pattern Analysis and Applications},
  month        = {2},
  number       = {1},
  pages        = {59-84},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Automatic computing of number of clusters for color image segmentation employing fuzzy c-means by extracting chromaticity features of colors},
  volume       = {23},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Design of a frequency spectrum-based versatile
two-dimensional arbitrary shape filter bank: Application to contact lens
detection. <em>PAAA</em>, <em>23</em>(1), 45–58. (<a
href="https://doi.org/10.1007/s10044-018-0764-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A filter bank (FB) is an integral part of any image processing system. The designing of a FB generally involves modifying an existing FB or focusing on a particular property of the filter bank. Such FBs limit their use to a particular image. Through our work, we have devised a unique and novel approach for designing a two-dimensional arbitrary shape filter bank (2-D ASFB). This FB is inherently 2-D and eliminates the need for transforming a one-dimensional FB into 2-D. Its arbitrary nature expands its application to any image as compared to regular-shaped FBs currently in use. The novelty of the design lies in the fact that the designed FB can match the frequency spectrum of any image by reducing the error function between the frequency spectrum and the desired filter response of the FB. The error function has been minimized using the eigenfilter approach. After designing the low-pass analysis filter, perfect reconstruction constraint has been used to get a low-pass synthesis filter. In this paper, we have demonstrated the use of the 2-D ASFB specifically for contact lens detection (CLD). The proposed CLD system focuses on feature extraction using the 2-D ASFB. The support vector machine classifier is the same as in the existing systems. The results show improved correct classification rate as compared to the existing systems for IIITD and ND2013 contact lens database. This 2-D ASFB overcomes limitations posed by the existing filter banks with respect to separability, directionality, orthogonality, and shape. This FB can be effectively applied to any feature extraction application such as pattern recognition, biometrics, medical image processing.},
  archive      = {J_PAAA},
  author       = {Madhe, Swati P. and Patil, Bhushan D. and Holambe, Raghunath S.},
  doi          = {10.1007/s10044-018-0764-6},
  journal      = {Pattern Analysis and Applications},
  month        = {2},
  number       = {1},
  pages        = {45-58},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Design of a frequency spectrum-based versatile two-dimensional arbitrary shape filter bank: Application to contact lens detection},
  volume       = {23},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Using semantic context for multiple concepts detection in
still images. <em>PAAA</em>, <em>23</em>(1), 27–44. (<a
href="https://doi.org/10.1007/s10044-018-0761-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multimedia documents indexing systems performances have been improved significantly in recent years, especially after the involvement of deep learning approaches. However, this progress remains insufficient with the evolution of users&#39; needs that become complex in terms of semantics and the number of words that compose their queries. So, it is important to think about indexing images by a group of concepts simultaneously (multi-concepts) and not just single ones. This would allow systems to better respond to queries composed of several terms. This task is much more difficult than indexing images by single concepts. Multi-concepts detection in images has been little dealt in the state of the art compared to the detection of visual single concepts. On the other hand, the use of context has proved its effectiveness in the field of multimedia semantic indexing. In this work, we propose two approaches that consider the semantic context for multi-concepts detection in still images. We tested and evaluated our proposal on the international standard corpus Pascal VOC for the detection of concepts pairs and triplets of concepts. Our contributions have shown that context is useful and improves multi-concepts detection in images. The combination of the use of semantic context and deep learning-based features yielded much better results than those of the state of the art. This difference in performance is estimated by a relative gain on mean average precision reaching + 70% for concepts pairs and + 34% for the case of triplets of concepts.},
  archive      = {J_PAAA},
  author       = {Hamadi, Abdelkader and Lattar, Hafsa and Khoussa, Mohamed El Bachir and Safadi, Bahjat},
  doi          = {10.1007/s10044-018-0761-9},
  journal      = {Pattern Analysis and Applications},
  month        = {2},
  number       = {1},
  pages        = {27-44},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Using semantic context for multiple concepts detection in still images},
  volume       = {23},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Recognition of mixture control chart patterns based on
fusion feature reduction and fireworks algorithm-optimized MSVM.
<em>PAAA</em>, <em>23</em>(1), 15–26. (<a
href="https://doi.org/10.1007/s10044-018-0748-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unnatural control chart patterns (CCPs) can be associated with the quality problems of the production process. It is quite critical to detect and identify these patterns effectively based on process data. Various machine learning techniques to CCPs recognition have been studied on the process only suffer from basic CCPs of unnatural patterns. Practical production process data may be the combination of two or more basic patterns simultaneously in reality. This paper proposes a mixture CCPs recognition method based on fusion feature reduction (FFR) and fireworks algorithm-optimized multiclass support vector machine (MSVM). FFR algorithm consists of three main sub-networks: statistical and shape features, features fusion and kernel principal component analysis feature dimensionality reduction, which make the features more effective. In MSVM classifier algorithm, the kernel function parameters play a very significant role in mixture CCPs recognition accuracy. Therefore, fireworks algorithm is proposed to select the two-dimensional parameters of the classifier. The results of the proposed algorithm are benchmarked with popular genetic algorithm and particle swarm optimization methods. Simulation results demonstrate that the proposed method can gain the higher recognition accuracy and significantly reduce the running time.},
  archive      = {J_PAAA},
  author       = {Zhang, Min and Yuan, Yi and Wang, Ruiqi and Cheng, Wenming},
  doi          = {10.1007/s10044-018-0748-6},
  journal      = {Pattern Analysis and Applications},
  month        = {2},
  number       = {1},
  pages        = {15-26},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Recognition of mixture control chart patterns based on fusion feature reduction and fireworks algorithm-optimized MSVM},
  volume       = {23},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Locality preserving projection least squares twin support
vector machine for pattern classification. <em>PAAA</em>,
<em>23</em>(1), 1–13. (<a
href="https://doi.org/10.1007/s10044-018-0728-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {During the last few years, multiple surface classification algorithms, such as twin support vector machine (TWSVM), least squares twin support vector machine (LSTSVM) and least squares projection twin support vector machine (LSPTSVM), have attracted much attention. However, these algorithms did not consider the local geometrical structure information of training samples. To alleviate this problem, in this paper, a locality preserving projection least squares twin support vector machine (LPPLSTSVM) is presented by introducing the basic idea of the locality preserving projection into LSPTSVM. This method not only inherits the ability of TWSVM, LSTSVM and LSPTSVM for pattern classification, but also fully considers the local geometrical structure between samples and shows the local underlying discriminatory information. Experimental results conducted on both synthetic and real-world datasets illustrate the effectiveness of the proposed LPPLSTSVM method.},
  archive      = {J_PAAA},
  author       = {Chen, Su-Gen and Wu, Xiao-Jun and Xu, Juan},
  doi          = {10.1007/s10044-018-0728-x},
  journal      = {Pattern Analysis and Applications},
  month        = {2},
  number       = {1},
  pages        = {1-13},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Locality preserving projection least squares twin support vector machine for pattern classification},
  volume       = {23},
  year         = {2020},
}
</textarea>
</details></li>
</ul>

</body>
</html>
