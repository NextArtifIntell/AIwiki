<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>Alg_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="alg---140">Alg - 140</h2>
<ul>
<li><details>
<summary>
(2020). Dynamic and internal longest common substring. <em>Alg</em>,
<em>82</em>(12), 3707–3743. (<a
href="https://doi.org/10.1007/s00453-020-00744-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Given two strings S and T, each of length at most n, the longest common substring (LCS) problem is to find a longest substring common to S and T. This is a classical problem in computer science with an $$\mathcal {O}(n)$$ -time solution. In the fully dynamic setting, edit operations are allowed in either of the two strings, and the problem is to find an LCS after each edit. We present the first solution to the fully dynamic LCS problem requiring sublinear time in n per edit operation. In particular, we show how to find an LCS after each edit operation in $$\tilde{\mathcal {O}}(n^{2/3})$$ time, after $$\tilde{\mathcal {O}}(n)$$ -time and space preprocessing. This line of research has been recently initiated in a somewhat restricted dynamic variant by Amir et al. [SPIRE 2017]. More specifically, the authors presented an $$\tilde{\mathcal {O}}(n)$$ -sized data structure that returns an LCS of the two strings after a single edit operation (that is reverted afterwards) in $$\tilde{\mathcal {O}}(1)$$ time. At CPM 2018, three papers (Abedin et al., Funakoshi et al., and Urabe et al.) studied analogously restricted dynamic variants of problems on strings; specifically, computing the longest palindrome and the Lyndon factorization of a string after a single edit operation. We develop dynamic sublinear-time algorithms for both of these problems as well. We also consider internal LCS queries, that is, queries in which we are to return an LCS of a pair of substrings of S and T. We show that answering such queries is hard in general and propose efficient data structures for several restricted cases.},
  archive      = {J_Alg},
  author       = {Amir, Amihood and Charalampopoulos, Panagiotis and Pissis, Solon P. and Radoszewski, Jakub},
  doi          = {10.1007/s00453-020-00744-0},
  journal      = {Algorithmica},
  number       = {12},
  pages        = {3707-3743},
  shortjournal = {Algorithmica},
  title        = {Dynamic and internal longest common substring},
  volume       = {82},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). On the benefits of populations for the exploitation speed of
standard steady-state genetic algorithms. <em>Alg</em>, <em>82</em>(12),
3676–3706. (<a
href="https://doi.org/10.1007/s00453-020-00743-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {It is generally accepted that populations are useful for the global exploration of multi-modal optimisation problems. Indeed, several theoretical results are available showing such advantages over single-trajectory search heuristics. In this paper we provide evidence that evolving populations via crossover and mutation may also benefit the optimisation time for hillclimbing unimodal functions. In particular, we prove bounds on the expected runtime of the standard ( $$\mu +1$$ ) GA for OneMax that are lower than its unary black box complexity and decrease in the leading constant with the population size up to $$\mu =o\left( \sqrt{\log n}\right) $$ . Our analysis suggests that the optimal mutation strategy is to flip two bits most of the time. To achieve the results we provide two interesting contributions to the theory of randomised search heuristics: (1) A novel application of drift analysis which compares absorption times of different Markov chains without defining an explicit potential function. (2) The inversion of fundamental matrices to calculate the absorption times of the Markov chains. The latter strategy was previously proposed in the literature but to the best of our knowledge this is the first time is has been used to show non-trivial bounds on expected runtimes.},
  archive      = {J_Alg},
  author       = {Corus, Dogan and Oliveto, Pietro S.},
  doi          = {10.1007/s00453-020-00743-1},
  journal      = {Algorithmica},
  number       = {12},
  pages        = {3676-3706},
  shortjournal = {Algorithmica},
  title        = {On the benefits of populations for the exploitation speed of standard steady-state genetic algorithms},
  volume       = {82},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). An adversarial model for scheduling with testing.
<em>Alg</em>, <em>82</em>(12), 3630–3675. (<a
href="https://doi.org/10.1007/s00453-020-00742-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce a novel adversarial model for scheduling with explorable uncertainty. In this model, the processing time of a job can potentially be reduced (by an a priori unknown amount) by testing the job. Testing a job j takes one unit of time and may reduce its processing time from the given upper limit $$\bar{p}_j$$ (which is the time taken to execute the job if it is not tested) to any value between 0 and $$\bar{p}_j$$ . This setting is motivated e.g., by applications where a code optimizer can be run on a job before executing it. We consider the objective of minimizing the sum of completion times on a single machine. All jobs are available from the start, but the reduction in their processing times as a result of testing is unknown, making this an online problem that is amenable to competitive analysis. The need to balance the time spent on tests and the time spent on job executions adds a novel flavor to the problem. We give the first and nearly tight lower and upper bounds on the competitive ratio for deterministic and randomized algorithms. We also show that minimizing the makespan is a considerably easier problem for which we give optimal deterministic and randomized online algorithms.},
  archive      = {J_Alg},
  author       = {Dürr, Christoph and Erlebach, Thomas and Megow, Nicole and Meißner, Julie},
  doi          = {10.1007/s00453-020-00742-2},
  journal      = {Algorithmica},
  number       = {12},
  pages        = {3630-3675},
  shortjournal = {Algorithmica},
  title        = {An adversarial model for scheduling with testing},
  volume       = {82},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). On scheduling coflows. <em>Alg</em>, <em>82</em>(12),
3604–3629. (<a
href="https://doi.org/10.1007/s00453-020-00741-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Applications designed for data-parallel computation frameworks such as MapReduce usually alternate between computation and communication stages. Coflow scheduling is a recent popular networking abstraction introduced to capture such application-level communication patterns in datacenters. In this framework, a datacenter is modeled as a single non-blocking switch with m input ports and m output ports. A coflow j is a collection of flow demands $${d^j_{io}}_{i \in {1,\ldots ,m}, o \in {1,\ldots ,m}}$$ that is said to be complete once all of its requisite flows have been scheduled. We consider the offline coflow scheduling problem with and without release times to minimize the total weighted completion time. Coflow scheduling generalizes the well studied concurrent open shop scheduling problem and is thus NP-hard. Qiu et al. (in: ACM Symposium on parallelism in algorithms and architectures. ACM, New York, pp 294–303, 2015) obtain the first constant approximation algorithms for this problem via LP rounding and give a deterministic $$\frac{67}{3}$$ -approximation and a randomized $$(9 + \frac{16\sqrt{2}}{3}) \approx 16.54$$ -approximation algorithm. In this paper, we give a combinatorial algorithm that yields a deterministic 5-approximation algorithm for coflow scheduling with release times, and a deterministic 4-approximation for the case without release times. As for concurrent open shop problem with release times, we give a combinatorial 3-approximation algorithm.},
  archive      = {J_Alg},
  author       = {Ahmadi, Saba and Khuller, Samir and Purohit, Manish and Yang, Sheng},
  doi          = {10.1007/s00453-020-00741-3},
  journal      = {Algorithmica},
  number       = {12},
  pages        = {3604-3629},
  shortjournal = {Algorithmica},
  title        = {On scheduling coflows},
  volume       = {82},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Enumerating k-arc-connected orientations. <em>Alg</em>,
<em>82</em>(12), 3588–3603. (<a
href="https://doi.org/10.1007/s00453-020-00738-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study the problem of enumerating the k-arc-connected orientations of a graph G, i.e., generating each exactly once. A first algorithm using submodular flow optimization is easy to state, but intricate to implement. In a second approach we present a simple algorithm with $$O(knm^2)$$ time delay and amortized time $$O(m^2)$$ , which improves over the analysis of the submodular flow algorithm. As ingredients, we obtain enumeration algorithms for the $$\alpha $$ -orientations of a graph G in $$O(m^2)$$ time delay and for the outdegree sequences attained by k-arc-connected orientations of G in $$O(knm^2)$$ time delay.},
  archive      = {J_Alg},
  author       = {Blind, Sarah and Knauer, Kolja and Valicov, Petru},
  doi          = {10.1007/s00453-020-00738-y},
  journal      = {Algorithmica},
  number       = {12},
  pages        = {3588-3603},
  shortjournal = {Algorithmica},
  title        = {Enumerating k-arc-connected orientations},
  volume       = {82},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Subgraph isomorphism on graph classes that exclude a
substructure. <em>Alg</em>, <em>82</em>(12), 3566–3587. (<a
href="https://doi.org/10.1007/s00453-020-00737-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study Subgraph Isomorphism on graph classes defined by a fixed forbidden graph. Although there are several ways for forbidding a graph, we observe that it is reasonable to focus on the minor relation since other well-known relations lead to either trivial or equivalent problems. When the forbidden minor is connected, we present a near dichotomy of the complexity of Subgraph Isomorphism with respect to the forbidden minor, where the only unsettled case is $$P_{5}$$ , the path of five vertices. We then also consider the general case of possibly disconnected forbidden minors. We show fixed-parameter tractable cases and randomized XP-time solvable cases parameterized by the size of the forbidden minor H. We also show that by slightly generalizing the tractable cases, the problem becomes NP-complete. All unsettle cases are equivalent to $$P_{5}$$ or the disjoint union of two $$P_{5}$$ ’s. As a byproduct, we show that Subgraph Isomorphism is fixed-parameter tractable parameterized by vertex integrity. Using similar techniques, we also observe that Subgraph Isomorphism is fixed-parameter tractable parameterized by neighborhood diversity.},
  archive      = {J_Alg},
  author       = {Bodlaender, Hans L. and Hanaka, Tesshu and Kobayashi, Yasuaki and Kobayashi, Yusuke and Okamoto, Yoshio and Otachi, Yota and van der Zanden, Tom C.},
  doi          = {10.1007/s00453-020-00737-z},
  journal      = {Algorithmica},
  number       = {12},
  pages        = {3566-3587},
  shortjournal = {Algorithmica},
  title        = {Subgraph isomorphism on graph classes that exclude a substructure},
  volume       = {82},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). The power of linear-time data reduction for maximum
matching. <em>Alg</em>, <em>82</em>(12), 3521–3565. (<a
href="https://doi.org/10.1007/s00453-020-00736-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Finding maximum-cardinality matchings in undirected graphs is arguably one of the most central graph primitives. For m-edge and n-vertex graphs, it is well-known to be solvable in $$O(m\sqrt{n})$$ time; however, for several applications this running time is still too slow. We investigate how linear-time (and almost linear-time) data reduction (used as preprocessing) can alleviate the situation. More specifically, we focus on linear-time kernelization. We start a deeper and systematic study both for general graphs and for bipartite graphs. Our data reduction algorithms easily comply (in form of preprocessing) with every solution strategy (exact, approximate, heuristic), thus making them attractive in various settings.},
  archive      = {J_Alg},
  author       = {Mertzios, George B. and Nichterlein, André and Niedermeier, Rolf},
  doi          = {10.1007/s00453-020-00736-0},
  journal      = {Algorithmica},
  number       = {12},
  pages        = {3521-3565},
  shortjournal = {Algorithmica},
  title        = {The power of linear-time data reduction for maximum matching},
  volume       = {82},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Using a min-cut generalisation to go beyond boolean
surjective VCSPs. <em>Alg</em>, <em>82</em>(12), 3492–3520. (<a
href="https://doi.org/10.1007/s00453-020-00735-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this work, we first study a natural generalisation of the Min-Cut problem, where a graph is augmented by a superadditive set function defined on its vertex subsets. The goal is to select a vertex subset such that the weight of the induced cut plus the set function value are minimised. In addition, a lower and upper bound is imposed on the solution size. We present a polynomial-time algorithm for enumerating all near-optimal solutions of this Bounded Generalised Min-Cut problem. Second, we apply this novel algorithm to surjective general-valued constraint satisfaction problems (VCSPs), i.e., VCSPs in which each label has to be used at least once. On the Boolean domain, Fulla, Uppman, and Živný (ACM ToCT’18) have recently established a complete classification of surjective VCSPs based on an unbounded version of the Generalised Min-Cut problem. Their result features the discovery of a new non-trivial tractable case called EDS that does not appear in the non-surjective setting. As our main result, we extend the class EDS to arbitrary finite domains and provide a conditional complexity classification for surjective VCSPs of this type based on a reduction to smaller domains. On three-element domains, this leads to a complete classification of such VCSPs.},
  archive      = {J_Alg},
  author       = {Matl, Gregor and Živný, Stanislav},
  doi          = {10.1007/s00453-020-00735-1},
  journal      = {Algorithmica},
  number       = {12},
  pages        = {3492-3520},
  shortjournal = {Algorithmica},
  title        = {Using a min-cut generalisation to go beyond boolean surjective VCSPs},
  volume       = {82},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Fault tolerant approximate BFS structures with additive
stretch. <em>Alg</em>, <em>82</em>(12), 3458–3491. (<a
href="https://doi.org/10.1007/s00453-020-00734-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper addresses the problem of designing a $$\beta $$ -additive fault-tolerant approximate BFS (or FT-ABFS for short) structure, namely, a subgraph H of the network G such that subsequent to the failure of a single edge e, the surviving part of H still contains an approximate BFS spanning tree for (the surviving part of) G, whose distances satisfy $$\mathrm{dist}(s,v,H{\setminus } {e}) \le \mathrm{dist}(s,v,G{\setminus } {e})+\beta $$ for every $$v \in V$$ . It was shown in Parter and Peleg (SODA, 2014), that for every $$\beta \in [1, O(\log n)]$$ there exists an n-vertex graph G with a source s for which any $$\beta $$ -additive FT-ABFS structure rooted at s has $$\Omega (n^{1+\epsilon (\beta )})$$ edges, for some function $$\epsilon (\beta ) \in (0,1)$$ . In particular, 3-additive FT-ABFS structures admit a lower bound of $$\Omega (n^{5/4})$$ edges. In this paper we present the first upper bound, showing that there exists a poly-time algorithm that for every n-vertex unweighted undirected graph G and source s constructs a 4-additive FT-ABFS structure rooted at s with at most $$O(n^{4/3})$$ edges. The main technical contribution of our algorithm is in adapting the path-buying strategy used in Baswana et al. (ACM Trans Algorithms 7:A5, 2010) and Cygan et al. (Proceedings of the 30th symposium on theoretical aspects of computer science, pp 209–220, 2013) to failure-prone settings.},
  archive      = {J_Alg},
  author       = {Parter, Merav and Peleg, David},
  doi          = {10.1007/s00453-020-00734-2},
  journal      = {Algorithmica},
  number       = {12},
  pages        = {3458-3491},
  shortjournal = {Algorithmica},
  title        = {Fault tolerant approximate BFS structures with additive stretch},
  volume       = {82},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Non-preemptive scheduling in a smart grid model and its
implications on machine minimization. <em>Alg</em>, <em>82</em>(12),
3415–3457. (<a
href="https://doi.org/10.1007/s00453-020-00733-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study a scheduling problem arising in demand response management in smart grid. Consumers send in power requests with a flexible feasible time interval during which their requests can be served. The grid controller, upon receiving power requests, schedules each request within the specified interval. The electricity cost is measured by a convex function of the load in each timeslot. The objective is to schedule all requests with the minimum total electricity cost. Previous work has studied cases where jobs have unit power requirement and unit duration. We extend the study to arbitrary power requirement and duration, which has been shown to be NP-hard. We give the first online algorithm for the general problem and prove that the problem is fixed parameter tractable. We also show that the online algorithm is the best-possible in an asymptotically sense when the objective is to minimize the peak load. In addition, we observe that the classical non-preemptive machine minimization problem is a special case of the smart grid problem with min-peak objective and show that we can achieve the best-possible competitive ratio in an asymptotically sense when solving the non-preemptive machine minimization problem.},
  archive      = {J_Alg},
  author       = {Liu, Fu-Hong and Liu, Hsiang-Hsuan and Wong, Prudence W. H.},
  doi          = {10.1007/s00453-020-00733-3},
  journal      = {Algorithmica},
  number       = {12},
  pages        = {3415-3457},
  shortjournal = {Algorithmica},
  title        = {Non-preemptive scheduling in a smart grid model and its implications on machine minimization},
  volume       = {82},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Correction to: Randomized algorithms for tracking
distributed count, frequencies, and ranks. <em>Alg</em>,
<em>82</em>(11), 3413. (<a
href="https://doi.org/10.1007/s00453-020-00755-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {After publication of the article [1] the authors have noticed that the funding information are not published in online and print version of the article. The omitted funding acknowledgement is given below.},
  archive      = {J_Alg},
  author       = {Huang, Zengfeng and Yi, Ke and Zhang, Qin},
  doi          = {10.1007/s00453-020-00755-x},
  journal      = {Algorithmica},
  number       = {11},
  pages        = {3413},
  shortjournal = {Algorithmica},
  title        = {Correction to: Randomized algorithms for tracking distributed count, frequencies, and ranks},
  volume       = {82},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Efficient online string matching based on characters
distance text sampling. <em>Alg</em>, <em>82</em>(11), 3390–3412. (<a
href="https://doi.org/10.1007/s00453-020-00732-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Searching for all occurrences of a pattern in a text is a fundamental problem in computer science with applications in many other fields, like natural language processing, information retrieval and computational biology. Sampled string matching is an efficient approach recently introduced in order to overcome the prohibitive space requirements of an index construction, on the one hand, and drastically reduce searching time for the online solutions, on the other hand. In this paper we present a new algorithm for the sampled string matching problem, based on a characters distance sampling approach. The main idea is to sample the distances between consecutive occurrences of a given pivot character and then to search online the sampled data for any occurrence of the sampled pattern, before verifying the original text. From a theoretical point of view we prove that, under suitable conditions, our solution can achieve both linear worst-case time complexity and optimal average-time complexity. From a practical point of view it turns out that our solution shows a sub-linear behaviour in practice and speeds up online searching by a factor of up to 9, using limited additional space whose amount goes from 11 to 2.8\% of the text size, with a gain up to 50\% if compared with previous solutions.},
  archive      = {J_Alg},
  author       = {Faro, Simone and Marino, Francesco Pio and Pavone, Arianna},
  doi          = {10.1007/s00453-020-00732-4},
  journal      = {Algorithmica},
  number       = {11},
  pages        = {3390-3412},
  shortjournal = {Algorithmica},
  title        = {Efficient online string matching based on characters distance text sampling},
  volume       = {82},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Greed is good for deterministic scale-free networks.
<em>Alg</em>, <em>82</em>(11), 3338–3389. (<a
href="https://doi.org/10.1007/s00453-020-00729-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Large real-world networks typically follow a power-law degree distribution. To study such networks, numerous random graph models have been proposed. However, real-world networks are not drawn at random. Therefore, Brach et al. (27th symposium on discrete algorithms (SODA), pp 1306–1325, 2016) introduced two natural deterministic conditions: (1) a power-law upper bound on the degree distribution (PLB-U) and (2) power-law neighborhoods, that is, the degree distribution of neighbors of each vertex is also upper bounded by a power law (PLB-N). They showed that many real-world networks satisfy both properties and exploit them to design faster algorithms for a number of classical graph problems. We complement their work by showing that some well-studied random graph models exhibit both of the mentioned PLB properties. PLB-U and PLB-N hold with high probability for Chung–Lu Random Graphs and Geometric Inhomogeneous Random Graphs and almost surely for Hyperbolic Random Graphs. As a consequence, all results of Brach et al. also hold with high probability or almost surely for those random graph classes. In the second part we study three classical $$\textsf {NP}$$ -hard optimization problems on PLB networks. It is known that on general graphs with maximum degree $$\Delta$$ , a greedy algorithm, which chooses nodes in the order of their degree, only achieves a $$\Omega (\ln \Delta )$$ -approximation for Minimum Vertex Cover and Minimum Dominating Set, and a $$\Omega (\Delta )$$ -approximation for Maximum Independent Set. We prove that the PLB-U property with $$\beta &gt;2$$ suffices for the greedy approach to achieve a constant-factor approximation for all three problems. We also show that these problems are APX-hard even if PLB-U, PLB-N, and an additional power-law lower bound on the degree distribution hold. Hence, a PTAS cannot be expected unless P = NP. Furthermore, we prove that all three problems are in MAX SNP if the PLB-U property holds.},
  archive      = {J_Alg},
  author       = {Chauhan, Ankit and Friedrich, Tobias and Rothenberger, Ralf},
  doi          = {10.1007/s00453-020-00729-z},
  journal      = {Algorithmica},
  number       = {11},
  pages        = {3338-3389},
  shortjournal = {Algorithmica},
  title        = {Greed is good for deterministic scale-free networks},
  volume       = {82},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Explicit correlation amplifiers for finding outlier
correlations in deterministic subquadratic time. <em>Alg</em>,
<em>82</em>(11), 3306–3337. (<a
href="https://doi.org/10.1007/s00453-020-00727-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We derandomize Valiant’s (J ACM 62, Article 13, 2015) subquadratic-time algorithm for finding outlier correlations in binary data. This demonstrates that it is possible to perform a deterministic subquadratic-time similarity join of high dimensionality. Our derandomized algorithm gives deterministic subquadratic scaling essentially for the same parameter range as Valiant’s randomized algorithm, but the precise constants we save over quadratic scaling are more modest. Our main technical tool for derandomization is an explicit family of correlation amplifiers built via a family of zigzag-product expanders by Reingold et al. (Ann Math 155(1):157–187, 2002). We say that a function $$f:{-1,1}^d\rightarrow {-1,1}^D$$ is a correlation amplifier with threshold $$0\le \tau \le 1$$ , error $$\gamma \ge 1$$ , and strength p an even positive integer if for all pairs of vectors $$x,y\in {-1,1}^d$$ it holds that (i) $$|\langle x,y\rangle |&lt;\tau d$$ implies $$|\langle f(x),f(y)\rangle |\le (\tau \gamma )^pD$$ ; and (ii) $$|\langle x,y\rangle |\ge \tau d$$ implies $$\left (\frac{\langle x,y\rangle }{\gamma d}\right )^pD \le \langle f(x),f(y)\rangle \le \left (\frac{\gamma \langle x,y\rangle }{d}\right )^pD$$ .},
  archive      = {J_Alg},
  author       = {Karppa, Matti and Kaski, Petteri and Kohonen, Jukka and Ó Catháin, Padraig},
  doi          = {10.1007/s00453-020-00727-1},
  journal      = {Algorithmica},
  number       = {11},
  pages        = {3306-3337},
  shortjournal = {Algorithmica},
  title        = {Explicit correlation amplifiers for finding outlier correlations in deterministic subquadratic time},
  volume       = {82},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Embedding graphs into embedded graphs. <em>Alg</em>,
<em>82</em>(11), 3282–3305. (<a
href="https://doi.org/10.1007/s00453-020-00725-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A (possibly degenerate) drawing of a graph G in the plane is approximable by an embedding if it can be turned into an embedding by an arbitrarily small perturbation. We show that testing whether a piece-wise linear drawing of a planar graph G in the plane is approximable by an embedding can be carried out in polynomial time, if a desired embedding of G belongs to a fixed isotopy class. In other words, we show that c-planarity with embedded pipes is tractable for graphs with prescribed combinatorial embedding. To the best of our knowledge, an analogous result was previously known essentially only when G is a cycle.},
  archive      = {J_Alg},
  author       = {Fulek, Radoslav},
  doi          = {10.1007/s00453-020-00725-3},
  journal      = {Algorithmica},
  number       = {11},
  pages        = {3282-3305},
  shortjournal = {Algorithmica},
  title        = {Embedding graphs into embedded graphs},
  volume       = {82},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Deterministic treasure hunt in the plane with angular hints.
<em>Alg</em>, <em>82</em>(11), 3250–3281. (<a
href="https://doi.org/10.1007/s00453-020-00724-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A mobile agent equipped with a compass and a measure of length has to find an inert treasure in the Euclidean plane. Both the agent and the treasure are modeled as points. In the beginning, the agent is at a distance at most $$D&gt;0$$ from the treasure, but knows neither the distance nor any bound on it. Finding the treasure means getting at distance at most 1 from it. The agent makes a series of moves. Each of them consists in moving straight in a chosen direction at a chosen distance. In the beginning and after each move the agent gets a hint consisting of a positive angle smaller than $$2\pi$$ whose vertex is at the current position of the agent and within which the treasure is contained. We investigate the problem of how these hints permit the agent to lower the cost of finding the treasure, using a deterministic algorithm, where the cost is the worst-case total length of the agent’s trajectory. It is well known that without any hint the optimal (worst case) cost is $$\varTheta (D^2)$$ . We show that if all angles given as hints are at most $$\pi$$ , then the cost can be lowered to O(D), which is the optimal complexity. If all angles are at most $$\beta$$ , where $$\beta 0$$ . For both these positive results we present deterministic algorithms achieving the above costs. Finally, if angles given as hints can be arbitrary, smaller than $$2\pi$$ , then we show that cost complexity $$\varTheta (D^2)$$ cannot be beaten.},
  archive      = {J_Alg},
  author       = {Bouchard, Sébastien and Dieudonné, Yoann and Pelc, Andrzej and Petit, Franck},
  doi          = {10.1007/s00453-020-00724-4},
  journal      = {Algorithmica},
  number       = {11},
  pages        = {3250-3281},
  shortjournal = {Algorithmica},
  title        = {Deterministic treasure hunt in the plane with angular hints},
  volume       = {82},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Efficient 1-space bounded hypercube packing algorithm.
<em>Alg</em>, <em>82</em>(11), 3216–3249. (<a
href="https://doi.org/10.1007/s00453-020-00723-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A space bounded $$ O(d/\log d)$$ -competitive hypercube packing algorithm with one active bin only is presented. As a starting point we give a simple 1-space bounded hypercube packing algorithm with competitive ratio $$ (3/2)^{d}+O((21/16)^d)$$ , for $$d\ge 3.$$},
  archive      = {J_Alg},
  author       = {Grzegorek, Paulina and Januszewski, Janusz and Zielonka, Łukasz},
  doi          = {10.1007/s00453-020-00723-5},
  journal      = {Algorithmica},
  number       = {11},
  pages        = {3216-3249},
  shortjournal = {Algorithmica},
  title        = {Efficient 1-space bounded hypercube packing algorithm},
  volume       = {82},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Lempel–ziv-like parsing in small space. <em>Alg</em>,
<em>82</em>(11), 3195–3215. (<a
href="https://doi.org/10.1007/s00453-020-00722-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Lempel–Ziv (LZ77 or, briefly, LZ) is one of the most effective and widely-used compressors for repetitive texts. However, the existing efficient methods computing the exact LZ parsing have to use linear or close to linear space to index the input text during the construction of the parsing, which is prohibitive for long inputs. An alternative is Relative Lempel–Ziv (RLZ), which indexes only a fixed reference sequence, whose size can be controlled. Deriving the reference sequence by sampling the text yields reasonable compression ratios for RLZ, but performance is not always competitive with that of LZ and depends heavily on the similarity of the reference to the text. In this paper we introduce ReLZ, a technique that uses RLZ as a preprocessor to approximate the LZ parsing using little memory. RLZ is first used to produce a sequence of phrases, and these are regarded as metasymbols that are input to LZ for a second-level parsing on a (most often) drastically shorter sequence. This parsing is finally translated into one on the original sequence. We analyze the new scheme and prove that, like LZ, it achieves the kth order empirical entropy compression $$n H_k + o(n\log \sigma )$$ with $$k = o(\log _\sigma n)$$ , where n is the input length and $$\sigma$$ is the alphabet size. In fact, we prove this entropy bound not only for ReLZ but for a wide class of LZ-like encodings. Then, we establish a lower bound on ReLZ approximation ratio showing that the number of phrases in it can be $$\Omega (\log n)$$ times larger than the number of phrases in LZ. Our experiments show that ReLZ is faster than existing alternatives to compute the (exact or approximate) LZ parsing, at the reasonable price of an approximation factor below 2.0 in all tested scenarios, and sometimes below 1.05, to the size of LZ.},
  archive      = {J_Alg},
  author       = {Kosolobov, Dmitry and Valenzuela, Daniel and Navarro, Gonzalo and Puglisi, Simon J.},
  doi          = {10.1007/s00453-020-00722-6},
  journal      = {Algorithmica},
  number       = {11},
  pages        = {3195-3215},
  shortjournal = {Algorithmica},
  title        = {Lempel–Ziv-like parsing in small space},
  volume       = {82},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Dynamic clustering to minimize the sum of radii.
<em>Alg</em>, <em>82</em>(11), 3183–3194. (<a
href="https://doi.org/10.1007/s00453-020-00721-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we study the problem of opening centers to cluster a set of clients in a metric space so as to minimize the sum of the costs of the centers and of the cluster radii, in a dynamic environment where clients arrive and depart, and the solution must be updated efficiently while remaining competitive with respect to the current optimal solution. We call this dynamic sum-of-radii clustering problem. We present a data structure that maintains a solution whose cost is within a constant factor of the cost of an optimal solution in metric spaces with bounded doubling dimension and whose worst-case update time is logarithmic in the parameters of the problem.},
  archive      = {J_Alg},
  author       = {Henzinger, Monika and Leniowski, Dariusz and Mathieu, Claire},
  doi          = {10.1007/s00453-020-00721-7},
  journal      = {Algorithmica},
  number       = {11},
  pages        = {3183-3194},
  shortjournal = {Algorithmica},
  title        = {Dynamic clustering to minimize the sum of radii},
  volume       = {82},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Local search algorithms for the maximum carpool matching
problem. <em>Alg</em>, <em>82</em>(11), 3165–3182. (<a
href="https://doi.org/10.1007/s00453-020-00719-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Maximum Carpool Matching problem is a star packing problem in directed graphs. Formally, given a directed graph $$G = (V, A)$$ , a capacity function $$c: V \rightarrow {\mathbb {N}}$$ , and a weight function $$w: A \rightarrow {\mathbb {R}}^+$$ , a carpool matching is a subset of arcs, $$M \subseteq A$$ , such that every $$v \in V$$ satisfies: (1) $$d^{\text {in}}_{M}(v) \cdot d^{\text {out}}_{M}(v) = 0$$ , (2) $$d^{\text {in}}_{M}(v) \le c(v)$$ , and (3) $$d^{\text {out}}_{M}(v) \le 1$$ . A vertex v for which $$d^{\text {out}}_{M}(v) = 1$$ is a passenger, and a vertex for which $$d^{\text {out}}_{M}(v) = 0$$ is a driver who has $$d^{\text {in}}_{M}(v)$$ passengers. In the Maximum Carpool Matching problem the goal is to find a carpool matching M of maximum total weight. The problem arises when designing an online carpool service, such as Zimride (Zimride by enterprise. https://zimride.com/ ), which tries to connect between users based on a similarity function. The problem is known to be NP-hard, even in the unweighted and uncapacitated case. The Maximum Group Carpool Matching problem, is an extension of the Maximum Carpool Matching where each vertex represents an unsplittable group of passengers. Formally, each vertex $$u \in V$$ has a size $$s(u) \in {\mathbb {N}}$$ , and the constraint $$d^{\text {in}}_{M}(v) \le c(v)$$ is replaced with $$\sum _{u:(u,v) \in M} s(u) \le c(v)$$ . We show that Maximum Carpool Matching can be formulated as an unconstrained submodular maximization problem, thus it admits a $$\frac{1}{2}$$ -approximation algorithm. We show that the same formulation does not work for Maximum Group Carpool Matching, nevertheless, we present a local search $$(\frac{1}{2} - \varepsilon )$$ -approximation algorithm for Maximum Group Carpool Matching. For the unweighted variant of both problems when the maximum possible capacity, $$c_{\max }$$ , is bounded by a constant, we provide a local search $$(\frac{1}{2}+ \frac{1}{2c_{\max }} - \varepsilon )$$ -approximation algorithm. We also provide an APX-hardness result, even if the maximum degree and $$c_{\max }$$ are at most 3.},
  archive      = {J_Alg},
  author       = {Kutiel, Gilad and Rawitz, Dror},
  doi          = {10.1007/s00453-020-00719-1},
  journal      = {Algorithmica},
  number       = {11},
  pages        = {3165-3182},
  shortjournal = {Algorithmica},
  title        = {Local search algorithms for the maximum carpool matching problem},
  volume       = {82},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Improved solution to data gathering with mobile mule.
<em>Alg</em>, <em>82</em>(11), 3125–3164. (<a
href="https://doi.org/10.1007/s00453-020-00718-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this work we study the problem of collecting protected data in ad-hoc sensor network using a mobile entity called MULE. The objective is to increase information survivability in the network. Sensors from all over the network, route their sensing data through a data gathering tree, towards a particular node, called the sink. In case of a failed sensor, all the aggregated data from the sensor and from its children is lost. In order to retrieve the lost data, the MULE is required to travel among all the children of the failed sensor and to re-collect the data. There is a cost to travel between two points in the plane. We aim to minimize the MULE traveling cost, given that any sensor can fail. In order to reduce the traveling cost, it is necessary to find the optimal data gathering tree and the MULE location. We are considering the problem for the unit disk graphs and Euclidean distance cost function. We propose a primal–dual algorithm that produces a $$\left( 20+\varepsilon \right) $$ -approximate solution for the problem, where $$\varepsilon \rightarrow 0$$ as the sensor network spreads over a larger area. The algorithm requires $$O\left( n^{3}\cdot \varDelta \left( G\right) \right) $$ time to construct a gathering tree and to place the MULE, where $$\varDelta \left( G\right) $$ is the maximum degree in the graph and n is the number of nodes.},
  archive      = {J_Alg},
  author       = {Zur, Yoad and Segal, Michael},
  doi          = {10.1007/s00453-020-00718-2},
  journal      = {Algorithmica},
  number       = {11},
  pages        = {3125-3164},
  shortjournal = {Algorithmica},
  title        = {Improved solution to data gathering with mobile mule},
  volume       = {82},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Correction to: Reoptimization time analysis of evolutionary
algorithms on linear functions under dynamic uniform constraints.
<em>Alg</em>, <em>82</em>(10), 3117–3123. (<a
href="https://doi.org/10.1007/s00453-020-00739-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the article Reoptimization Time Analysis of Evolutionary Algorithms on Linear Functions Under Dynamic Uniform Constraints, we claimed a worst-case runtime of and for the Multi-Objective Evolutionary Algorithm and the Multi-Objective Genetic Algorithm, respectively, on linear profit functions under dynamic uniform constraint, where denotes the difference between the original constraint bound B and the new one. The technique used to prove these results contained an error. We correct this mistake and show a weaker bound of for both algorithms instead.},
  archive      = {J_Alg},
  author       = {Shi, Feng and Schirneck, Martin and Friedrich, Tobias and Kötzing, Timo and Neumann, Frank},
  doi          = {10.1007/s00453-020-00739-x},
  journal      = {Algorithmica},
  number       = {10},
  pages        = {3117-3123},
  shortjournal = {Algorithmica},
  title        = {Correction to: Reoptimization time analysis of evolutionary algorithms on linear functions under dynamic uniform constraints},
  volume       = {82},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Approximating global optimum for probabilistic truth
discovery. <em>Alg</em>, <em>82</em>(10), 3091–3116. (<a
href="https://doi.org/10.1007/s00453-020-00715-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The problem of truth discovery arises in many areas such as database, data mining, data crowdsourcing and machine learning. It seeks trustworthy information from possibly conflicting data provided by multiple sources. Due to its practical importance, the problem has been studied extensively in recent years. Two competing models were proposed for truth discovery, weight-based model and probabilistic model. While $$(1+\epsilon )$$ -approximations have already been obtained for the weight-based model, no quality guaranteed solution has been discovered yet for the probabilistic model. In this paper, we focus on the probabilistic model and formulate it as a geometric optimization problem. Based on a sampling technique and a few other ideas, we achieve the first $$(1 + \epsilon )$$ -approximation solution. Our techniques can also be used to solve the more general multi-truth discovery problem. We validate our method by conducting experiments on both synthetic and real-world datasets (teaching evaluation data) and comparing its performance to some existing approaches. Our solutions are closer to the truth as well as global optimum based on the experimental result. The general technique we developed has the potential to be used to solve other geometric optimization problems.},
  archive      = {J_Alg},
  author       = {Li, Shi and Xu, Jinhui and Ye, Minwei},
  doi          = {10.1007/s00453-020-00715-5},
  journal      = {Algorithmica},
  number       = {10},
  pages        = {3091-3116},
  shortjournal = {Algorithmica},
  title        = {Approximating global optimum for probabilistic truth discovery},
  volume       = {82},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Characterizing star-PCGs. <em>Alg</em>, <em>82</em>(10),
3066–3090. (<a
href="https://doi.org/10.1007/s00453-020-00712-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A graph G is called a pairwise compatibility graph (PCG, for short) if it admits a tuple $$(T,w, d_{\min },d_{\max })$$ of a tree T whose leaf set is equal to the vertex set of G, a non-negative edge weight w, and two non-negative reals $$d_{\min }\le d_{\max }$$ such that G has an edge between two vertices $$u,v\in V$$ if and only if the distance between the two leaves u and v in the weighted tree (T, w) is in the interval $$[d_{\min }, d_{\max }]$$ . The tree T is also called a witness tree of the PCG G. How to recognize PCGs is a wide-open problem in the literature. This paper gives a complete characterization for a graph to be a star-PCG (a PCG that admits a star as its witness tree), which provides us the first polynomial-time algorithm for recognizing star-PCGs.},
  archive      = {J_Alg},
  author       = {Xiao, Mingyu and Nagamochi, Hiroshi},
  doi          = {10.1007/s00453-020-00712-8},
  journal      = {Algorithmica},
  number       = {10},
  pages        = {3066-3090},
  shortjournal = {Algorithmica},
  title        = {Characterizing star-PCGs},
  volume       = {82},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Special issue on computing and combinatorics. <em>Alg</em>,
<em>82</em>(10), 3065. (<a
href="https://doi.org/10.1007/s00453-020-00740-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_Alg},
  doi          = {10.1007/s00453-020-00740-4},
  journal      = {Algorithmica},
  number       = {10},
  pages        = {3065},
  shortjournal = {Algorithmica},
  title        = {Special issue on computing and combinatorics},
  volume       = {82},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Improved approximation algorithms for path vertex covers in
regular graphs. <em>Alg</em>, <em>82</em>(10), 3041–3064. (<a
href="https://doi.org/10.1007/s00453-020-00717-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Given a simple graph $$G = (V, E)$$ and a constant integer $$k \ge 2$$ , the k-path vertex cover problem (PkVC) asks for a minimum subset $$F \subseteq V$$ of vertices such that the induced subgraph $$G[V - F]$$ does not contain any path of order k. When $$k = 2$$ , this turns out to be the classic vertex cover (VC) problem, which admits a $$\left( 2 - {\Theta }\left( \frac{1}{\log |V|}\right) \right)$$ -approximation. The general PkVC admits a trivial k-approximation; when $$k = 3$$ and $$k = 4$$ , the best known approximation results are a 2-approximation and a 3-approximation, respectively. On d-regular graphs, the approximation ratios can be reduced to $$\min \left{ 2 - \frac{5}{d+3} + \epsilon , 2 - \frac{(2 - o(1))\log \log d}{\log d}\right}$$ for VC (i.e., P2VC), $$2 - \frac{1}{d} + \frac{4d - 2}{3d |V|}$$ for P3VC, $$\frac{\lfloor d/2\rfloor (2d - 2)}{(\lfloor d/2\rfloor + 1) (d - 2)}$$ for P4VC, and $$\frac{2d - k + 2}{d - k + 2}$$ for PkVC when $$1 \le k-2 &lt; d \le 2(k-2)$$ . By utilizing an existing algorithm for graph defective coloring, we first present a $$\frac{\lfloor d/2\rfloor (2d - k + 2)}{(\lfloor d/2\rfloor + 1) (d - k + 2)}$$ -approximation for PkVC on d-regular graphs when $$1 \le k - 2 &lt; d$$ . This beats all the best known approximation results for PkVC on d-regular graphs for $$k \ge 3$$ , except for P4VC it ties with the best prior work and in particular they tie at 2 on cubic graphs and 4-regular graphs. We then propose a $$(1.875 + \epsilon )$$ -approximation and a 1.852-approximation for P4VC on cubic graphs and 4-regular graphs, respectively. We also present a better approximation algorithm for P4VC on d-regular bipartite graphs.},
  archive      = {J_Alg},
  author       = {Zhang, An and Chen, Yong and Chen, Zhi-Zhong and Lin, Guohui},
  doi          = {10.1007/s00453-020-00717-3},
  journal      = {Algorithmica},
  number       = {10},
  pages        = {3041-3064},
  shortjournal = {Algorithmica},
  title        = {Improved approximation algorithms for path vertex covers in regular graphs},
  volume       = {82},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). The inverse voronoi problem in graphs i: hardness.
<em>Alg</em>, <em>82</em>(10), 3018–3040. (<a
href="https://doi.org/10.1007/s00453-020-00716-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce the inverse Voronoi diagram problem in graphs: given a graph G with positive edge-lengths and a collection $${\mathbb {U}}$$ of subsets of vertices of V(G), decide whether $${\mathbb {U}}$$ is a Voronoi diagram in G with respect to the shortest-path metric. We show that the problem is NP-hard, even for planar graphs where all the edges have unit length. We also study the parameterized complexity of the problem and show that the problem is W[1]-hard when parameterized by the number of Voronoi cells or by the pathwidth of the graph.},
  archive      = {J_Alg},
  author       = {Bonnet, Édouard and Cabello, Sergio and Mohar, Bojan and Pérez-Rosés, Hebert},
  doi          = {10.1007/s00453-020-00716-4},
  journal      = {Algorithmica},
  number       = {10},
  pages        = {3018-3040},
  shortjournal = {Algorithmica},
  title        = {The inverse voronoi problem in graphs i: Hardness},
  volume       = {82},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Upward planar morphs. <em>Alg</em>, <em>82</em>(10),
2985–3017. (<a
href="https://doi.org/10.1007/s00453-020-00714-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We prove that, given two topologically-equivalent upward planar straight-line drawings of an n-vertex directed graph G, there always exists a morph between them such that all the intermediate drawings of the morph are upward planar and straight-line. Such a morph consists of O(1) morphing steps if G is a reduced planar st-graph, O(n) morphing steps if G is a planar st-graph, O(n) morphing steps if G is a reduced upward planar graph, and $$O(n^2)$$ morphing steps if G is a general upward planar graph. Further, we show that $$\varOmega (n)$$ morphing steps might be necessary for an upward planar morph between two topologically-equivalent upward planar straight-line drawings of an n-vertex path.},
  archive      = {J_Alg},
  author       = {Da Lozzo, Giordano and Di Battista, Giuseppe and Frati, Fabrizio and Patrignani, Maurizio and Roselli, Vincenzo},
  doi          = {10.1007/s00453-020-00714-6},
  journal      = {Algorithmica},
  number       = {10},
  pages        = {2985-3017},
  shortjournal = {Algorithmica},
  title        = {Upward planar morphs},
  volume       = {82},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Computational aspects of ordered integer partitions with
bounds. <em>Alg</em>, <em>82</em>(10), 2955–2984. (<a
href="https://doi.org/10.1007/s00453-020-00713-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper is dedicated to the counting problem of writing an integer number z as a sum of an ordered sequence of n integers from n given intervals, i.e., counting the number of configurations $$(z_1,\ldots ,z_n)$$ with $$z = z_1 + \cdots + z_n$$ for $$z_i \in [x_i, y_i]$$ with integers $$x_i$$ and $$y_i$$ and $$1 \le i \le n$$ . We show an algorithm computing this number in $$ \mathop {}\mathopen {}{\mathcal {O}}\mathopen {}\left( n z^{\lg n}\right) $$ average time, and a data structure computing this number in $$\mathop {}\mathopen {}{\mathcal {O}}\mathopen {}\left( n\right) $$ time, independently of z. The data structure is constructed in $$ \mathop {}\mathopen {}{\mathcal {O}}\mathopen {}\left( 2^n n^3\right) $$ time. Its construction algorithm only depends on the intervals $$[x_i,y_i]$$ ( $$1 \le i \le n$$ ). This construction algorithm can be parallelized with $$\pi = \mathop {}\mathopen {}{\mathcal {O}}\mathopen {}\left( n^3\right) $$ processors, yielding $$\mathop {}\mathopen {}{\mathcal {O}}\mathopen {}\left( 2^n \frac{n^3}{\pi }\right) $$ construction time with high probability.},
  archive      = {J_Alg},
  author       = {Glück, Roland and Köppl, Dominik},
  doi          = {10.1007/s00453-020-00713-7},
  journal      = {Algorithmica},
  number       = {10},
  pages        = {2955-2984},
  shortjournal = {Algorithmica},
  title        = {Computational aspects of ordered integer partitions with bounds},
  volume       = {82},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Lipschitz continuity and approximate equilibria.
<em>Alg</em>, <em>82</em>(10), 2927–2954. (<a
href="https://doi.org/10.1007/s00453-020-00709-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we study games with continuous action spaces and non-linear payoff functions. Our key insight is that Lipschitz continuity of the payoff function allows us to provide algorithms for finding approximate equilibria in these games. We begin by studying Lipschitz games, which encompass, for example, all concave games with Lipschitz continuous payoff functions. We provide an efficient algorithm for computing approximate equilibria in these games. Then we turn our attention to penalty games, which encompass biased games and games in which players take risk into account. Here we show that if the penalty function is Lipschitz continuous, then we can provide a quasi-polynomial time approximation scheme. Finally, we study distance biased games, where we present simple strongly polynomial time algorithms for finding best responses in $$L_1$$ and $$L_2^2$$ biased games, and then use these algorithms to provide strongly polynomial algorithms that find 2/3 and 5/7 approximate equilibria for these norms, respectively.},
  archive      = {J_Alg},
  author       = {Deligkas, Argyrios and Fearnley, John and Spirakis, Paul},
  doi          = {10.1007/s00453-020-00709-3},
  journal      = {Algorithmica},
  number       = {10},
  pages        = {2927-2954},
  shortjournal = {Algorithmica},
  title        = {Lipschitz continuity and approximate equilibria},
  volume       = {82},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). On the approximate compressibility of connected vertex
cover. <em>Alg</em>, <em>82</em>(10), 2902–2926. (<a
href="https://doi.org/10.1007/s00453-020-00708-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Connected Vertex Cover problem, where the goal is to compute a minimum set of vertices in a given graph which forms a vertex cover and induces a connected subgraph, is a fundamental combinatorial problem and has received extensive attention in various subdomains of algorithmics. In the area of kernelization, it is known that this problem is unlikely to have a polynomial kernelization algorithm. However, it has been shown in a recent work of Lokshtanov et al. (Proceedings of the 49th annual ACM SIGACT symposium on theory of computing, STOC 2017, Montreal, QC, Canada, 2017) that if one considered an appropriate notion of approximate kernelization, then this problem parameterized by the solution size does admit an approximate polynomial kernelization. In fact, Lokshtanov et al. were able to obtain a polynomial size approximate kernelization scheme (PSAKS) for Connected Vertex Cover parameterized by the solution size. A PSAKS is essentially a preprocessing algorithm whose error can be made arbitrarily close to 0. In this paper we revisit this problem, and consider parameters that are strictly smaller than the size of the solution and obtain the first polynomial size approximate kernelization schemes for the Connected Vertex Cover problem when parameterized by the deletion distance of the input graph to the class of cographs, the class of bounded treewidth graphs, and the class of all chordal graphs.},
  archive      = {J_Alg},
  author       = {Majumdar, Diptapriyo and Ramanujan, M. S. and Saurabh, Saket},
  doi          = {10.1007/s00453-020-00708-4},
  journal      = {Algorithmica},
  number       = {10},
  pages        = {2902-2926},
  shortjournal = {Algorithmica},
  title        = {On the approximate compressibility of connected vertex cover},
  volume       = {82},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Sequential metric dimension. <em>Alg</em>, <em>82</em>(10),
2867–2901. (<a
href="https://doi.org/10.1007/s00453-020-00707-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the localization game, introduced by Seager in 2013, an invisible and immobile target is hidden at some vertex of a graph G. At every step, one vertex v of G can be probed which results in the knowledge of the distance between v and the secret location of the target. The objective of the game is to minimize the number of steps needed to locate the target whatever be its location. We address the generalization of this game where $$k\ge 1$$ vertices can be probed at every step. Our game also generalizes the notion of the metric dimension of a graph. Precisely, given a graph G and two integers $$k,\ell \ge 1$$ , the Localization problem asks whether there exists a strategy to locate a target hidden in G in at most $$\ell$$ steps and probing at most k vertices per step. We first show that, in general, this problem is NP-complete for every fixed $$k \ge 1$$ (resp., $$\ell \ge 1$$ ). We then focus on the class of trees. On the negative side, we prove that the Localization problem is NP-complete in trees when k and $$\ell$$ are part of the input. On the positive side, we design a $$(+\,1)$$ -approximation algorithm for the problem in n-node trees, i.e., an algorithm that computes in time $$O(n \log n)$$ (independent of k) a strategy to locate the target in at most one more step than an optimal strategy. This algorithm can be used to solve the Localization problem in trees in polynomial time if k is fixed. We also consider some of these questions in the context where, upon probing the vertices, the relative distances to the target are retrieved. This variant of the problem generalizes the notion of the centroidal dimension of a graph.},
  archive      = {J_Alg},
  author       = {Bensmail, Julien and Mazauric, Dorian and Mc Inerney, Fionn and Nisse, Nicolas and Pérennes, Stéphane},
  doi          = {10.1007/s00453-020-00707-5},
  journal      = {Algorithmica},
  number       = {10},
  pages        = {2867-2901},
  shortjournal = {Algorithmica},
  title        = {Sequential metric dimension},
  volume       = {82},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). On cycle transversals and their connected variants in the
absence of a small linear forest. <em>Alg</em>, <em>82</em>(10),
2841–2866. (<a
href="https://doi.org/10.1007/s00453-020-00706-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A graph is H-free if it contains no induced subgraph isomorphic to H. We prove new complexity results for the two classical cycle transversal problems Feedback Vertex Set and Odd Cycle Transversal by showing that they can be solved in polynomial time on $$(sP_1+ P_3)$$ -free graphs for every integer $$s\ge 1$$ . We show the same result for the variants Connected Feedback Vertex Set and Connected Odd Cycle Transversal. We also prove that the latter two problems are polynomial-time solvable on cographs; this was already known for Feedback Vertex Set and Odd Cycle Transversal. We complement these results by proving that Odd Cycle Transversal and Connected Odd Cycle Transversal are NP-complete on $$(P_2+ P_5,P_6)$$ -free graphs.},
  archive      = {J_Alg},
  author       = {Dabrowski, Konrad K. and Feghali, Carl and Johnson, Matthew and Paesani, Giacomo and Paulusma, Daniël and Rzążewski, Paweł},
  doi          = {10.1007/s00453-020-00706-6},
  journal      = {Algorithmica},
  number       = {10},
  pages        = {2841-2866},
  shortjournal = {Algorithmica},
  title        = {On cycle transversals and their connected variants in the absence of a small linear forest},
  volume       = {82},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Paired-domination problem on distance-hereditary graphs.
<em>Alg</em>, <em>82</em>(10), 2809–2840. (<a
href="https://doi.org/10.1007/s00453-020-00705-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A paired-dominating set of a graph G is a dominating set S of G such that the subgraph of G induced by S has a perfect matching. Haynes and Slater (Networks 32(3):199–206, 1998) introduced the concept of paired-domination and showed that the problem of determining minimum paired-dominating sets is NP-complete on general graphs. Ever since then many algorithmic results are studied on some important classes of graphs. In this paper, we extend the results by providing an $$O(n^2)$$ -time algorithm on distance-hereditary graphs.},
  archive      = {J_Alg},
  author       = {Lin, Ching-Chi and Ku, Keng-Chu and Hsu, Chan-Hung},
  doi          = {10.1007/s00453-020-00705-7},
  journal      = {Algorithmica},
  number       = {10},
  pages        = {2809-2840},
  shortjournal = {Algorithmica},
  title        = {Paired-domination problem on distance-hereditary graphs},
  volume       = {82},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Sensor network topology design and analysis for efficient
data gathering by a mobile mule. <em>Alg</em>, <em>82</em>(10),
2784–2808. (<a
href="https://doi.org/10.1007/s00453-020-00704-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we study the problem of data gathering in ad-hoc sensor networks using a mobile entity called mule. The mule traverses the children of failed sensors, to prevent loss of data. Our objective is to define the optimal communication tree, and the mule’s placement such that the mule’s overall traveling distance is minimized. We explore this problem in several network topologies including: unit disc graph on a line (UDL), general unit disc graph (UDG), and a complete graph with failing probabilities on the nodes (CGFP). We provide an optimal solution for the UDL problem and three approximation algorithms for the UDG problem. For the CGFP problem we outline the two possible structures of an optimal solution and provide near optimal approximation algorithms.},
  archive      = {J_Alg},
  author       = {Yedidsion, Harel and Ashur, Stav and Banik, Aritra and Carmi, Paz and Katz, Matthew J. and Segal, Michael},
  doi          = {10.1007/s00453-020-00704-8},
  journal      = {Algorithmica},
  number       = {10},
  pages        = {2784-2808},
  shortjournal = {Algorithmica},
  title        = {Sensor network topology design and analysis for efficient data gathering by a mobile mule},
  volume       = {82},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020b). Online stochastic matching: New algorithms and bounds.
<em>Alg</em>, <em>82</em>(10), 2737–2783. (<a
href="https://doi.org/10.1007/s00453-020-00698-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Online matching has received significant attention in recent years due to its close connection to Internet advertising. As the seminal work of Karp, Vazirani, and Vazirani has an optimal $$(1 - 1/{\mathbf {\mathsf{{e}}}})$$ competitive ratio in the standard adversarial online model, much effort has gone into developing useful online models that incorporate some stochasticity in the arrival process. One such popular model is the “known I.I.D. model” where different customer-types arrive online from a known distribution. We develop algorithms with improved competitive ratios for some basic variants of this model with integral arrival rates, including: (a) the case of general weighted edges, where we improve the best-known ratio of 0.667 due to Haeupler, Mirrokni and Zadimoghaddam (WINE, 2011) to 0.705; and (b) the vertex-weighted case, where we improve the 0.7250 ratio of Jaillet and Lu (Math Oper Res 39(3):624–646, 2013) to 0.7299. We also consider an extension of stochastic rewards, a variant where each edge has an independent probability of being present. For the setting of stochastic rewards with non-integral arrival rates, we present a simple optimal non-adaptive algorithm with a ratio of $$1-1/{\mathbf {\mathsf{{e}}}}$$ . For the special case where each edge is unweighted and has a uniform constant probability of being present, we improve upon $$1-1/{\mathbf {\mathsf{{e}}}}$$ by proposing a strengthened LP benchmark. One of the key ingredients of our improvement is the following (offline) approach to bipartite-matching polytopes with additional constraints. We first add several valid constraints in order to get a good fractional solution $$\mathbf {f}$$ ; however, these give us less control over the structure of $$\mathbf {f}$$ . We next remove all these additional constraints and randomly move from $$\mathbf{f }$$ to a feasible point on the matching polytope with all coordinates being from the set $${0, 1/k, 2/k, \ldots , 1}$$ for a chosen integer k. The structure of this solution is inspired by Jaillet and Lu (2013) and is a tractable structure for algorithm design and analysis. The appropriate random move preserves many of the removed constraints (approximately with high probability and exactly in expectation). This underlies some of our improvements and could be of independent interest.},
  archive      = {J_Alg},
  author       = {Brubach, Brian and Sankararaman, Karthik Abinav and Srinivasan, Aravind and Xu, Pan},
  doi          = {10.1007/s00453-020-00698-3},
  journal      = {Algorithmica},
  number       = {10},
  pages        = {2737-2783},
  shortjournal = {Algorithmica},
  title        = {Online stochastic matching: New algorithms and bounds},
  volume       = {82},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020a). A unified model and algorithms for temporal map labeling.
<em>Alg</em>, <em>82</em>(10), 2709–2736. (<a
href="https://doi.org/10.1007/s00453-020-00694-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider map labeling for the case that a map undergoes a sequence of operations such as rotation, zoom and translation over a specified time span. We unify and generalize several previous models for dynamic map labeling into one versatile and flexible model. In contrast to previous research, we completely abstract from the particular operations and express the labeling problem as a set of time intervals representing the labels’ presences, activities and conflicts. One of the model’s strength is manifested in its simplicity and broad range of applications. In particular, it supports label selection both for map features with fixed position as well as for moving entities (e.g., for tracking vehicles in logistics or air traffic control). We study the active range maximization problem in this model. We prove that the problem is NP-complete and W[1]-hard, and present constant-factor approximation algorithms. In the restricted, yet practically relevant case that no more than k labels can be active at any time, we give polynomial-time algorithms as well as constant-factor approximation algorithms.},
  archive      = {J_Alg},
  author       = {Gemsa, Andreas and Niedermann, Benjamin and Nöllenburg, Martin},
  doi          = {10.1007/s00453-020-00694-7},
  journal      = {Algorithmica},
  number       = {10},
  pages        = {2709-2736},
  shortjournal = {Algorithmica},
  title        = {A unified model and algorithms for temporal map labeling},
  volume       = {82},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Compression of dynamic graphs generated by a duplication
model. <em>Alg</em>, <em>82</em>(9), 2687–2707. (<a
href="https://doi.org/10.1007/s00453-020-00699-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We continue building up the information theory of non-sequential data structures such as trees, sets, and graphs. In this paper, we consider dynamic graphs generated by a full duplication model in which a new vertex selects an existing vertex and copies all of its neighbors. We ask how many bits are needed to describe the labeled and unlabeled versions of such graphs. We first estimate entropies of both versions and then present asymptotically optimal compression algorithms up to two bits. Interestingly, for the full duplication model the labeled version needs $$\Theta (n)$$ bits while its unlabeled version (structure) can be described by $$\Theta (\log n)$$ bits due to significant amount of symmetry (i.e. large average size of the automorphism group of sample graphs).},
  archive      = {J_Alg},
  author       = {Turowski, Krzysztof and Magner, Abram and Szpankowski, Wojciech},
  doi          = {10.1007/s00453-020-00699-2},
  journal      = {Algorithmica},
  number       = {9},
  pages        = {2687-2707},
  shortjournal = {Algorithmica},
  title        = {Compression of dynamic graphs generated by a duplication model},
  volume       = {82},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). On the approximability of the stable matching problem with
ties of size two. <em>Alg</em>, <em>82</em>(9), 2668–2686. (<a
href="https://doi.org/10.1007/s00453-020-00703-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The stable matching problem is one of the central problems of algorithmic game theory. If participants are allowed to have ties, the problem of finding a stable matching of maximum cardinality is an $$\mathcal{NP}$$ -hard problem, even when the ties are of size two. Moreover, in this setting it is UGC-hard to provide an approximation with a constant factor smaller than 4/3. In this paper, we give a tight analysis of an approximation algorithm given by Huang and Kavitha for the maximum cardinality stable matching problem with ties of size two, demonstrating an improved 4/3-approximation factor.},
  archive      = {J_Alg},
  author       = {Chiang, Robert and Pashkovich, Kanstantsin},
  doi          = {10.1007/s00453-020-00703-9},
  journal      = {Algorithmica},
  number       = {9},
  pages        = {2668-2686},
  shortjournal = {Algorithmica},
  title        = {On the approximability of the stable matching problem with ties of size two},
  volume       = {82},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Parameterized multi-scenario single-machine scheduling
problems. <em>Alg</em>, <em>82</em>(9), 2644–2667. (<a
href="https://doi.org/10.1007/s00453-020-00702-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study a class of multi-scenario single-machine scheduling problems. In this class of problems, we are given a set of scenarios with each one having a different realization of job characteristics. We consider these multi-scenario problems where the scheduling criterion can be any one of the following three: The total weighted completion time, the weighted number of tardy jobs, and the weighted number of jobs completed exactly at their due-date. As all the resulting problems are NP-hard, our analysis focuses on whether any one of the problems becomes tractable when some specific natural parameters are of limited size. The analysis includes the following parameters: The number of jobs with scenario-dependent processing times, the number of jobs with scenario-dependent weights, and the number of different due-dates.},
  archive      = {J_Alg},
  author       = {Hermelin, Danny and Manoussakis, George and Pinedo, Michael and Shabtay, Dvir and Yedidsion, Liron},
  doi          = {10.1007/s00453-020-00702-w},
  journal      = {Algorithmica},
  number       = {9},
  pages        = {2644-2667},
  shortjournal = {Algorithmica},
  title        = {Parameterized multi-scenario single-machine scheduling problems},
  volume       = {82},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). The complexity of tree partitioning. <em>Alg</em>,
<em>82</em>(9), 2606–2643. (<a
href="https://doi.org/10.1007/s00453-020-00701-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Given a tree T on n vertices, and $$k, b, s_1, \ldots , s_b \in \mathbb {N}$$ , the Tree Partitioning problem asks if at most k edges can be removed from T so that the resulting components can be grouped into b groups such that the number of vertices in group i is $$s_i$$ , for $$i =1, \ldots , b$$ . The case where $$s_1=\cdots =s_b =n/b$$ , referred to as the Balanced Tree Partitioning problem, was shown to be $${\mathcal {NP}}$$ -complete for trees of maximum degree at most 5, and the complexity of the problem for trees of maximum degree 4 and 3 was posed as an open question. The parameterized complexity of Balanced Tree Partitioning was also posed as an open question in another work. In this paper, we answer both open questions negatively. We show that Balanced Tree Partitioning (and hence, Tree Partitioning) is $${\mathcal {NP}}$$ -complete for trees of maximum degree 3, thus closing the door on the complexity of Balanced Tree Partitioning, as the simple case when T is a path is in $${\mathcal {P}}$$ . In terms of the parameterized complexity of the problems, we show that both Balanced Tree Partitioning and Tree Partitioning are W[1]-complete parameterized by k. Using a compact representation of the solution space for an instance of the problem, we present a dynamic programming algorithm for the weighted version of Tree Partitioning (and hence for that of Balanced Tree Partitioning) that runs in subexponential-time $$2^{O(\sqrt{n})}$$ , adding a natural problem to the list of problems that can be solved in subexponential time. Finally, we extend this subexponential-time algorithm to the Weighted Graph Partitioning problem on graphs of treewidth $$o(n/\lg {n})$$ , and we also show an application of this subexponential-time algorithm for approximating the Weighted Graph Partitioning problem.},
  archive      = {J_Alg},
  author       = {An, Zhao and Feng, Qilong and Kanj, Iyad and Xia, Ge},
  doi          = {10.1007/s00453-020-00701-x},
  journal      = {Algorithmica},
  number       = {9},
  pages        = {2606-2643},
  shortjournal = {Algorithmica},
  title        = {The complexity of tree partitioning},
  volume       = {82},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Independent set reconfiguration parameterized by
modular-width. <em>Alg</em>, <em>82</em>(9), 2586–2605. (<a
href="https://doi.org/10.1007/s00453-020-00700-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Independent Set Reconfiguration is one of the most well-studied problems in the setting of combinatorial reconfiguration. It is known that the problem is PSPACE-complete even for graphs of bounded bandwidth. This fact rules out the tractability of parameterizations by most well-studied structural parameters as most of them generalize bandwidth. In this paper, we study the parameterization by modular-width, which is not comparable with bandwidth. We show that the problem parameterized by modular-width is fixed-parameter tractable under all previously studied rules $${\mathsf {TAR}}$$ , $${\mathsf {TJ}}$$ , and $${\mathsf {TS}}$$ . The result under $${\mathsf {TAR}}$$ resolves an open problem posed by Bonsma (J Graph Theory 83(2):164–195, 2016).},
  archive      = {J_Alg},
  author       = {Belmonte, Rémy and Hanaka, Tesshu and Lampis, Michael and Ono, Hirotaka and Otachi, Yota},
  doi          = {10.1007/s00453-020-00700-y},
  journal      = {Algorithmica},
  number       = {9},
  pages        = {2586-2605},
  shortjournal = {Algorithmica},
  title        = {Independent set reconfiguration parameterized by modular-width},
  volume       = {82},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Queue layouts of planar 3-trees. <em>Alg</em>,
<em>82</em>(9), 2564–2585. (<a
href="https://doi.org/10.1007/s00453-020-00697-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A queue layout of a graph G consists of a linear order of the vertices of G and a partition of the edges of G into queues, so that no two independent edges of the same queue are nested. The queue number of graph G is defined as the minimum number of queues required by any queue layout of G. In this paper, we continue the study of the queue number of planar 3-trees, which form a well-studied subclass of planar graphs. Prior to this work, it was known that the queue number of planar 3-trees is at most seven. In this work, we improve this upper bound to five. We also show that there exist planar 3-trees whose queue number is at least four. Notably, this is the first example of a planar graph with queue number greater than three.},
  archive      = {J_Alg},
  author       = {Alam, Jawaherul Md. and Bekos, Michael A. and Gronemann, Martin and Kaufmann, Michael and Pupyrev, Sergey},
  doi          = {10.1007/s00453-020-00697-4},
  journal      = {Algorithmica},
  number       = {9},
  pages        = {2564-2585},
  shortjournal = {Algorithmica},
  title        = {Queue layouts of planar 3-trees},
  volume       = {82},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A <span class="math display">3 + <em>𝛺</em>(1)</span> lower
bound for page migration. <em>Alg</em>, <em>82</em>(9), 2535–2563. (<a
href="https://doi.org/10.1007/s00453-020-00696-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We address the page migration problem, one of the most classical online problems. In this problem, we are given online requests from nodes in a network for accessing a single page, i.e., a data set stored in a node, and asked to determine a node for the page to be stored in after each request. Serving a request costs the distance between the request and the page at the point of the request, and migrating the page costs the migration distance multiplied by the page size. The objective is to minimize the total sum of the service and migration costs. This problem is motivated by efficient cache management in multiprocessor systems. In this paper, we prove that no deterministic online page migration algorithm is $$(3+o(1))$$ -competitive, where the o-notation is with respect to the page size. Our lower bound first breaks the barrier of 3 by an additive constant for an arbitrarily large page size and disproves Black and Sleator’s conjecture even in the asymptotic sense.},
  archive      = {J_Alg},
  author       = {Matsubayashi, Akira},
  doi          = {10.1007/s00453-020-00696-5},
  journal      = {Algorithmica},
  number       = {9},
  pages        = {2535-2563},
  shortjournal = {Algorithmica},
  title        = {A $$3+\varOmega (1)$$ lower bound for page migration},
  volume       = {82},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). An efficient exhaustive search algorithm for the
escherization problem. <em>Alg</em>, <em>82</em>(9), 2502–2534. (<a
href="https://doi.org/10.1007/s00453-020-00695-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the Escherization problem, given a closed figure in the plane, the objective is to find a closed figure that is as close as possible to the input figure that tiles the plane. Koizumi and Sugihara’s formulation reduces this problem to an eigenvalue problem. In their formulation, only one of the potentially numerous templates is used to parameterize the possible tile shapes for each isohedral type. In this research, we try to search for the best tile shape using all possible templates for the nine most general isohedral types. This extension provides a considerable flexibility in possible tile shapes and improves the quality of the obtained tile shapes. However, the exhaustive search of all possible templates is computationally unrealistic using conventional calculation methods, and we develop an efficient algorithm to perform this in a reasonable computation time.},
  archive      = {J_Alg},
  author       = {Nagata, Yuichi and Imahori, Shinji},
  doi          = {10.1007/s00453-020-00695-6},
  journal      = {Algorithmica},
  number       = {9},
  pages        = {2502-2534},
  shortjournal = {Algorithmica},
  title        = {An efficient exhaustive search algorithm for the escherization problem},
  volume       = {82},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). The approximability of multiple facility location on
directed networks with random arc failures. <em>Alg</em>,
<em>82</em>(9), 2474–2501. (<a
href="https://doi.org/10.1007/s00453-020-00693-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce and study the maximum reliability coverage problem, where multiple facilities are to be located on a network whose arcs are subject to random failures. Our model assumes that arcs fail independently with non-uniform probabilities, and the objective is to locate a given number of facilities, aiming to maximize the expected demand serviced. In this context, each demand point is said to be serviced (or covered) when it is reachable from at least one facility by an operational path. The main contribution of this paper is to establish tight bounds on the approximability of maximum reliability coverage on bidirected trees as well as on general networks. Quite surprisingly, we show that this problem is NP-hard on bidirected trees via a carefully-constructed reduction from the partition problem. On the positive side, we make use of approximate dynamic programming ideas to devise an FPTAS on bidirected trees. For general networks, while maximum reliability coverage is $$(1 - 1/e + \epsilon )$$ -inapproximable as an extension of the max k-cover problem, even estimating its objective value is #P-complete, due to generalizing certain network reliability problems. Nevertheless, we prove that by plugging-in a sampling-based additive estimator into the standard greedy algorithm, a matching approximation ratio of $$1 - 1/e - \epsilon $$ can be attained.},
  archive      = {J_Alg},
  author       = {Hassin, Refael and Ravi, R. and Salman, F. Sibel and Segev, Danny},
  doi          = {10.1007/s00453-020-00693-8},
  journal      = {Algorithmica},
  number       = {9},
  pages        = {2474-2501},
  shortjournal = {Algorithmica},
  title        = {The approximability of multiple facility location on directed networks with random arc failures},
  volume       = {82},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). On the tractability of optimization problems on h-graphs.
<em>Alg</em>, <em>82</em>(9), 2432–2473. (<a
href="https://doi.org/10.1007/s00453-020-00692-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For a graph H, a graph G is an H-graph if it is an intersection graph of connected subgraphs of some subdivision of H. H-graphs naturally generalize several important graph classes like interval graphs or circular-arc graph. This class was introduced in the early 1990s by Bíró, Hujter, and Tuza. Recently, Chaplick et al. initiated the algorithmic study of H-graphs by showing that a number of fundamental optimization problems like Maximum Clique, Maximum Independent Set, or Minimum Dominating Set are solvable in polynomial time on H-graphs. We extend and complement these algorithmic findings in several directions. First we show that for every fixed H, the class of H-graphs is of logarithmically-bounded boolean-width (via mim-width). Pipelined with the plethora of known algorithms on graphs of bounded boolean-width, this describes a large class of problems solvable in polynomial time on H-graphs. We also observe that H-graphs are graphs with polynomially many minimal separators. Combined with the work of Fomin, Todinca and Villanger on algorithmic properties of such classes of graphs, this identify another wide class of problems solvable in polynomial time on H-graphs. The most fundamental optimization problems among the problems solvable in polynomial time on H-graphs are Maximum Clique, Maximum Independent Set, and Minimum Dominating Set. We provide a more refined complexity analysis of these problems from the perspective of parameterized complexity. We show that Maximum Independent Set and Minimum Dominating Set are W[1]-hard being parameterized by the size of H plus the size of the solution. On the other hand, we prove that when H is a tree, then Minimum Dominating Set is fixed-parameter tractable parameterized by the size of H. For Maximum Clique we show that it admits a polynomial kernel parameterized by H and the solution size.},
  archive      = {J_Alg},
  author       = {Fomin, Fedor V. and Golovach, Petr A. and Raymond, Jean-Florent},
  doi          = {10.1007/s00453-020-00692-9},
  journal      = {Algorithmica},
  number       = {9},
  pages        = {2432-2473},
  shortjournal = {Algorithmica},
  title        = {On the tractability of optimization problems on H-graphs},
  volume       = {82},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). An efficient sum query algorithm for distance-based locally
dominating functions. <em>Alg</em>, <em>82</em>(9), 2415–2431. (<a
href="https://doi.org/10.1007/s00453-020-00691-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we consider the following sum query problem: Given a point set P in $${\mathbb {R}}^d$$ , and a distance-based function f(p, q) (i.e., a function of the distance between p and q) satisfying some general properties, the goal is to develop a data structure and a query algorithm for efficiently computing a $$(1+\epsilon )$$ -approximate solution to the sum $$\sum _{p \in P} f(p,q)$$ for any query point $$q \in {\mathbb {R}}^d$$ and any small constant $$\epsilon &gt;0$$ . Existing techniques for this problem are mainly based on some core-set techniques which often have difficulties to deal with functions with local domination property. Based on several new insights to this problem, we develop in this paper a novel technique to overcome these encountered difficulties. Our algorithm is capable of answering queries with high success probability in time no more than $${\tilde{O}}_{\epsilon ,d}(n^{0.5 + c})$$ , and the underlying data structure can be constructed in $${\tilde{O}}_{\epsilon ,d}(n^{1+c})$$ time for any $$c&gt;0$$ , where the hidden constant has only polynomial dependence on $$1/\epsilon$$ and d. Our technique is simple and can be easily implemented for practical purpose.},
  archive      = {J_Alg},
  author       = {Huang, Ziyun and Xu, Jinhui},
  doi          = {10.1007/s00453-020-00691-w},
  journal      = {Algorithmica},
  number       = {9},
  pages        = {2415-2431},
  shortjournal = {Algorithmica},
  title        = {An efficient sum query algorithm for distance-based locally dominating functions},
  volume       = {82},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A new vertex coloring heuristic and corresponding chromatic
number. <em>Alg</em>, <em>82</em>(9), 2395–2414. (<a
href="https://doi.org/10.1007/s00453-020-00689-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {One method to obtain a proper vertex coloring of graphs using a reasonable number of colors is to start from any arbitrary proper coloring and then repeat some local re-coloring techniques to reduce the number of color classes. The Grundy (First-Fit) coloring and color-dominating colorings of graphs are two well-known such techniques. The color-dominating colorings are also known and commonly referred as b-colorings. But these two topics have been studied separately in graph theory. We introduce a new coloring procedure which combines the strategies of these two techniques and satisfies an additional property. We first prove that the vertices of every graph G can be effectively colored using color classes say $$C_1, \ldots , C_k$$ such that (i) for any two colors i and j with $$1\le i&lt; j \le k$$ , any vertex of color j is adjacent to a vertex of color i, (ii) there exists a set $${u_1, \ldots , u_k}$$ of vertices of G such that $$u_j\in C_j$$ for any $$j\in {1, \ldots , k}$$ and $$u_k$$ is adjacent to $$u_j$$ for each $$1\le j \le k$$ with $$j\not = k$$ , and (iii) for each i and j with $$i\not = j$$ , the vertex $$u_j$$ has a neighbor in $$C_i$$ . This provides a new vertex coloring heuristic which improves both Grundy and color-dominating colorings. Denote by z(G) the maximum number of colors used in any proper vertex coloring satisfying the above properties. The z(G) quantifies the worst-case behavior of the heuristic. We prove the existence of $${G_n}_{n\ge 1}$$ such that $$\min {\Gamma (G_n), b(G_n)} \rightarrow \infty $$ but $$z(G_n)\le 3$$ for each n. For each positive integer t we construct a family of finitely many colored graphs $${{\mathcal {D}}}_t$$ satisfying the property that if $$z(G)\ge t$$ for a graph G then G contains an element from $${{\mathcal {D}}}_t$$ as a colored subgraph. This provides an algorithmic method for proving numeric upper bounds for z(G).},
  archive      = {J_Alg},
  author       = {Zaker, Manouchehr},
  doi          = {10.1007/s00453-020-00689-4},
  journal      = {Algorithmica},
  number       = {9},
  pages        = {2395-2414},
  shortjournal = {Algorithmica},
  title        = {A new vertex coloring heuristic and corresponding chromatic number},
  volume       = {82},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Parameterized complexity of independent set in h-free
graphs. <em>Alg</em>, <em>82</em>(8), 2360–2394. (<a
href="https://doi.org/10.1007/s00453-020-00730-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we investigate the complexity of Maximum Independent Set (MIS) in the class of H-free graphs, that is, graphs excluding a fixed graph as an induced subgraph. Given that the problem remains NP-hard for most graphs H, we study its fixed-parameter tractability and make progress towards a dichotomy between FPT and W[1]-hard cases. We first show that MIS remains W[1]-hard in graphs forbidding simultaneously $$K_{1, 4}$$ , any finite set of cycles of length at least 4, and any finite set of trees with at least two branching vertices. In particular, this answers an open question of Dabrowski et al. concerning $$C_4$$ -free graphs. Then we extend the polynomial algorithm of Alekseev when H is a disjoint union of edges to an FPT algorithm when H is a disjoint union of cliques. We also provide a framework for solving several other cases, which is a generalization of the concept of iterative expansion accompanied by the extraction of a particular structure using Ramsey’s theorem. Iterative expansion is a maximization version of the so-called iterative compression. We believe that our framework can be of independent interest for solving other similar graph problems. Finally, we present positive and negative results on the existence of polynomial (Turing) kernels for several graphs H.},
  archive      = {J_Alg},
  author       = {Bonnet, Édouard and Bousquet, Nicolas and Charbit, Pierre and Thomassé, Stéphan and Watrigant, Rémi},
  doi          = {10.1007/s00453-020-00730-6},
  journal      = {Algorithmica},
  number       = {8},
  pages        = {2360-2394},
  shortjournal = {Algorithmica},
  title        = {Parameterized complexity of independent set in H-free graphs},
  volume       = {82},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Parameterized leaf power recognition via embedding into
graph products. <em>Alg</em>, <em>82</em>(8), 2337–2359. (<a
href="https://doi.org/10.1007/s00453-020-00720-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The k-leaf power graph G of a tree T is a graph whose vertices are the leaves of T and whose edges connect pairs of leaves at unweighted distance at most k in T. Recognition of the k-leaf power graphs for $$k \ge 7$$ is still an open problem. In this paper, we provide two algorithms for this problem for sparse leaf power graphs. Our results shows that the problem of recognizing these graphs is fixed-parameter tractable when parameterized both by k and by the degeneracy of the given graph. To prove this, we first describe how to embed a leaf root of a leaf power graph into a product of the graph with a cycle graph. We bound the treewidth of the resulting product in terms of k and the degeneracy of G. The first presented algorithm uses methods based on monadic second-order logic ( $${\text{MSO}}_2$$ ) to recognize the existence of a leaf power as a subgraph of the graph product. Using the same embedding in the graph product, the second algorithm presents a dynamic programming approach to solve the problem and provide a better dependence on the parameters.},
  archive      = {J_Alg},
  author       = {Eppstein, David and Havvaei, Elham},
  doi          = {10.1007/s00453-020-00720-8},
  journal      = {Algorithmica},
  number       = {8},
  pages        = {2337-2359},
  shortjournal = {Algorithmica},
  title        = {Parameterized leaf power recognition via embedding into graph products},
  volume       = {82},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Dual parameterization of weighted coloring. <em>Alg</em>,
<em>82</em>(8), 2316–2336. (<a
href="https://doi.org/10.1007/s00453-020-00686-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Given a graph G, a properk-coloring of G is a partition $$c = (S_i)_{i\in [1,k]}$$ of V(G) into k stable sets $$S_1,\ldots , S_{k}$$ . Given a weight function $$w: V(G) \rightarrow {\mathbb {R}}^+$$ , the weight of a color $$S_i$$ is defined as $$w(i) = \max _{v \in S_i} w(v)$$ and the weight of a coloringc as $$w(c) = \sum _{i=1}^{k}w(i)$$ . Guan and Zhu (Inf Process Lett 61(2):77–81, 1997) defined the weighted chromatic number of a pair (G, w), denoted by $$\sigma (G,w)$$ , as the minimum weight of a proper coloring of G. The problem of determining $$\sigma (G,w)$$ has received considerable attention during the last years, and has been proved to be notoriously hard: for instance, it is NP-hard on split graphs, unsolvable on n-vertex trees in time $$n^{o(\log n)}$$ unless the ETH fails, and W[1]-hard on forests parameterized by the size of a largest tree. We focus on the so-called dual parameterization of the problem: given a vertex-weighted graph (G, w) and an integer k, is $$\sigma (G,w) \le \sum _{v \in V(G)} w(v) - k$$ ? This parameterization has been recently considered by Escoffier (in: Proceedings of the 42nd international workshop on graph-theoretic concepts in computer science (WG). LNCS, vol 9941, pp 50–61, 2016), who provided an FPT algorithm running in time $$2^{{\mathcal {O}}(k \log k)} \cdot n^{{\mathcal {O}}(1)}$$ , and asked which kernel size can be achieved for the problem. We provide an FPT algorithm in time $$9^k \cdot n^{{\mathcal {O}}(1)}$$ , and prove that no algorithm in time $$2^{o(k)} \cdot n^{{\mathcal {O}}(1)}$$ exists under the ETH. On the other hand, we present a kernel with at most $$(2^{k-1}+1) (k-1)$$ vertices, and rule out the existence of polynomial kernels unless $$\mathsf{NP} \subseteq \mathsf{coNP} / \mathsf{poly}$$ , even on split graphs with only two different weights. Finally, we identify classes of graphs allowing for polynomial kernels, namely interval graphs, comparability graphs, and subclasses of circular-arc and split graphs, and in the latter case we present lower bounds on the degrees of the polynomials.},
  archive      = {J_Alg},
  author       = {Araújo, Júlio and Campos, Victor A. and Lima, Carlos Vinícius G. C. and dos Santos, Vinícius Fernandes and Sau, Ignasi and Silva, Ana},
  doi          = {10.1007/s00453-020-00686-7},
  journal      = {Algorithmica},
  number       = {8},
  pages        = {2316-2336},
  shortjournal = {Algorithmica},
  title        = {Dual parameterization of weighted coloring},
  volume       = {82},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Multivariate analysis of orthogonal range searching and
graph distances. <em>Alg</em>, <em>82</em>(8), 2292–2315. (<a
href="https://doi.org/10.1007/s00453-020-00680-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We show that the eccentricities, diameter, radius, and Wiener index of an undirected n-vertex graph with nonnegative edge lengths can be computed in time $$O(n\cdot \left( {\begin{array}{c}k+\lceil \log n\rceil \\ k\end{array}}\right) \cdot 2^k \log n)$$ , where k is linear in the treewidth of the graph. For every $$\epsilon &gt;0$$ , this bound is $$n^{1+\epsilon }\exp O(k)$$ , which matches a hardness result of Abboud et al. (in: Proceedings of the Twenty-Seventh Annual ACM-SIAM Symposium on Discrete Algorithms, 2016. https://doi.org/10.1137/1.9781611974331.ch28 ) and closes an open problem in the multivariate analysis of polynomial-time computation. To this end, we show that the analysis of an algorithm of Cabello and Knauer (Comput Geom 42:815–824, 2009. https://doi.org/10.1016/j.comgeo.2009.02.001 ) in the regime of non-constant treewidth can be improved by revisiting the analysis of orthogonal range searching, improving bounds of the form $$\log ^d n$$ to $$\left( {\begin{array}{c}d+\lceil \log n\rceil \\ d\end{array}}\right)$$ , as originally observed by Monier (J Algorithms 1:60–74, 1980. https://doi.org/10.1016/0196-6774(80)90005-X ). We also investigate the parameterization by vertex cover number.},
  archive      = {J_Alg},
  author       = {Bringmann, Karl and Husfeldt, Thore and Magnusson, Måns},
  doi          = {10.1007/s00453-020-00680-z},
  journal      = {Algorithmica},
  number       = {8},
  pages        = {2292-2315},
  shortjournal = {Algorithmica},
  title        = {Multivariate analysis of orthogonal range searching and graph distances},
  volume       = {82},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Counting induced subgraphs: A topological approach to
#w[1]-hardness. <em>Alg</em>, <em>82</em>(8), 2267–2291. (<a
href="https://doi.org/10.1007/s00453-020-00676-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We investigate the problem $$\#{{\mathsf {IndSub}}}(\varPhi )$$ of counting all induced subgraphs of size k in a graph G that satisfy a given property $$\varPhi $$ . This continues the work of Jerrum and Meeks who proved the problem to be $$\#{{\mathrm {W[1]}}}$$ -hard for some families of properties which include (dis)connectedness [JCSS 15] and even- or oddness of the number of edges [Combinatorica 17]. Using the recent framework of graph motif parameters due to Curticapean, Dell and Marx [STOC 17], we discover that for monotone properties $$\varPhi $$ , the problem $$\#{{\mathsf {IndSub}}}(\varPhi )$$ is hard for $$\#{{\mathrm {W[1]}}}$$ if the reduced Euler characteristic of the associated simplicial (graph) complex of $$\varPhi $$ is non-zero. This observation links $$\#{{\mathsf {IndSub}}}(\varPhi )$$ to Karp’s famous Evasiveness Conjecture, as every graph complex with non-vanishing reduced Euler characteristic is known to be evasive. Applying tools from the “topological approach to evasiveness” which was introduced in the seminal paper of Khan, Saks and Sturtevant [FOCS 83], we prove that $$\#{{\mathsf {IndSub}}}(\varPhi )$$ is $$\#{{\mathrm {W[1]}}}$$ -hard for every monotone property $$\varPhi $$ that does not hold on the Hamilton cycle as well as for some monotone properties that hold on the Hamilton cycle such as being triangle-free or not k-edge-connected for $$k &gt; 2$$ . Moreover, we show that for those properties $$\#{{\mathsf {IndSub}}}(\varPhi )$$ can not be solved in time $$f(k)\cdot n^{o(k)}$$ for any computable function f unless the Exponential Time Hypothesis (ETH) fails. In the final part of the paper, we investigate non-monotone properties and prove that $$\#{{\mathsf {IndSub}}}(\varPhi )$$ is $$\#{{\mathrm {W[1]}}}$$ -hard if $$\varPhi $$ is any non-trivial modularity constraint on the number of edges with respect to some prime q or if $$\varPhi $$ enforces the presence of a fixed isolated subgraph.},
  archive      = {J_Alg},
  author       = {Roth, Marc and Schmitt, Johannes},
  doi          = {10.1007/s00453-020-00676-9},
  journal      = {Algorithmica},
  number       = {8},
  pages        = {2267-2291},
  shortjournal = {Algorithmica},
  title        = {Counting induced subgraphs: A topological approach to #W[1]-hardness},
  volume       = {82},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). On the distance identifying set meta-problem and
applications to the complexity of identifying problems on graphs.
<em>Alg</em>, <em>82</em>(8), 2243–2266. (<a
href="https://doi.org/10.1007/s00453-020-00674-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Numerous problems consisting in identifying vertices in graphs using distances are useful in domains such as network verification and graph isomorphism. Unifying them into a meta-problem may be of main interest. We introduce here a promising solution named Distance Identifying Set. The model contains Identifying Code (IC), Locating Dominating Set (LD) and their generalizations r-IC and r-LD where the closed neighborhood is considered up to distance r. It also contains Metric Dimension (MD) and its refinement r-MD in which the distance between two vertices is considered as infinite if the real distance exceeds r. Note that while IC = 1-IC and LD = 1-LD, we have MD = $$\infty$$ -MD; we say that MD is not local. In this article, we prove computational lower bounds for several problems included in Distance Identifying Set by providing generic reductions from (Planar) Hitting Set to the meta-problem. We focus on two families of problems from the meta-problem: the first one, called local, contains r-IC, r-LD and r-MD for each positive integer r while the second one, called 1-layered, contains LD, MD and r-MD for each positive integer r. We have: (1) the 1-layered problems are NP-hard even in bipartite apex graphs, (2) the local problems are NP-hard even in bipartite planar graphs, (3) assuming ETH, all these problems cannot be solved in $$2^{o(\sqrt{n})}$$ when restricted to bipartite planar or apex graph, respectively, and they cannot be solved in $$2^{o(n)}$$ on bipartite graphs, and (4) except if $${\mathsf{W}[0]} = {\mathsf{W}[2]}$$ , they do not admit parameterized algorithms in $$2^{{\mathcal {O}}(k)} \cdot n^{{\mathcal {O}}(1)}$$ even when restricted to bipartite graphs. Here k is the solution size of a relevant identifying set. In particular, Metric Dimension cannot be solved in $$2^{o(n)}$$ under ETH, answering a question of Hartung and Nichterlein (Proceedings of the 28th conference on computational complexity, CCC, 2013).},
  archive      = {J_Alg},
  author       = {Barbero, Florian and Isenmann, Lucas and Thiebaut, Jocelyn},
  doi          = {10.1007/s00453-020-00674-x},
  journal      = {Algorithmica},
  number       = {8},
  pages        = {2243-2266},
  shortjournal = {Algorithmica},
  title        = {On the distance identifying set meta-problem and applications to the complexity of identifying problems on graphs},
  volume       = {82},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Best-case and worst-case sparsifiability of boolean CSPs.
<em>Alg</em>, <em>82</em>(8), 2200–2242. (<a
href="https://doi.org/10.1007/s00453-019-00660-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We continue the investigation of polynomial-time sparsification for NP-complete Boolean Constraint Satisfaction Problems (CSPs). The goal in sparsification is to reduce the number of constraints in a problem instance without changing the answer, such that a bound on the number of resulting constraints can be given in terms of the number of variables n. We investigate how the worst-case sparsification size depends on the types of constraints allowed in the problem formulation—the constraint language—and identify constraint languages giving the best-possible and worst-possible behavior for worst-case sparsifiability. Two algorithmic results are presented. The first result essentially shows that for any arity k, the only constraint type for which no nontrivial sparsification is possible has exactly one falsifying assignment, and corresponds to logical OR (up to negations). Our second result concerns linear sparsification, that is, a reduction to an equivalent instance with $$O(n)$$ constraints. Using linear algebra over rings of integers modulo prime powers, we give an elegant necessary and sufficient condition for a constraint type to be captured by a degree-1 polynomial over such a ring, which yields linear sparsifications. The combination of these algorithmic results allows us to prove two characterizations that capture the optimal sparsification sizes for a range of Boolean CSPs. For NP-complete Boolean CSPs whose constraints are symmetric (the satisfaction depends only on the number of 1 values in the assignment, not on their positions), we give a complete characterization of which constraint languages allow for a linear sparsification. For Boolean CSPs in which every constraint has arity at most three, we characterize the optimal size of sparsifications in terms of the largest OR that can be expressed by the constraint language.},
  archive      = {J_Alg},
  author       = {Chen, Hubie and Jansen, Bart M. P. and Pieterse, Astrid},
  doi          = {10.1007/s00453-019-00660-y},
  journal      = {Algorithmica},
  number       = {8},
  pages        = {2200-2242},
  shortjournal = {Algorithmica},
  title        = {Best-case and worst-case sparsifiability of boolean CSPs},
  volume       = {82},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). The parameterised complexity of computing the maximum
modularity of a graph. <em>Alg</em>, <em>82</em>(8), 2174–2199. (<a
href="https://doi.org/10.1007/s00453-019-00649-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The maximum modularity of a graph is a parameter widely used to describe the level of clustering or community structure in a network. Determining the maximum modularity of a graph is known to be $$\textsf {NP}$$ -complete in general, and in practice a range of heuristics are used to construct partitions of the vertex-set which give lower bounds on the maximum modularity but without any guarantee on how close these bounds are to the true maximum. In this paper we investigate the parameterised complexity of determining the maximum modularity with respect to various standard structural parameterisations of the input graph G. We show that the problem belongs to $$\textsf {FPT}$$ when parameterised by the size of a minimum vertex cover for G, and is solvable in polynomial time whenever the treewidth or max leaf number of G is bounded by some fixed constant; we also obtain an FPT algorithm, parameterised by treewidth, to compute any constant-factor approximation to the maximum modularity. On the other hand we show that the problem is W[1]-hard (and hence unlikely to admit an FPT algorithm) when parameterised simultaneously by pathwidth and the size of a minimum feedback vertex set.},
  archive      = {J_Alg},
  author       = {Meeks, Kitty and Skerman, Fiona},
  doi          = {10.1007/s00453-019-00649-7},
  journal      = {Algorithmica},
  number       = {8},
  pages        = {2174-2199},
  shortjournal = {Algorithmica},
  title        = {The parameterised complexity of computing the maximum modularity of a graph},
  volume       = {82},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A faster tree-decomposition based algorithm for counting
linear extensions. <em>Alg</em>, <em>82</em>(8), 2156–2173. (<a
href="https://doi.org/10.1007/s00453-019-00633-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We investigate the problem of computing the number of linear extensions of a given n-element poset whose cover graph has treewidth t. We present an algorithm that runs in time $${\tilde{O}}(n^{t+3})$$ for any constant t; the notation $${\tilde{O}}$$ hides polylogarithmic factors. Our algorithm applies dynamic programming along a tree decomposition of the cover graph; the join nodes of the tree decomposition are handled by fast multiplication of multivariate polynomials. We also investigate the algorithm from a practical point of view. We observe that the running time is not well characterized by the parameters n and t alone: fixing these parameters leaves large variance in running times due to uncontrolled features of the selected optimal-width tree decomposition. We compare two approaches to select an efficient tree decomposition: one is to include additional features of the tree decomposition to build a more accurate, heuristic cost function; the other approach is to fit a statistical regression model to collected running time data. Both approaches are shown to yield a tree decomposition that typically is significantly more efficient than a random optimal-width tree decomposition.},
  archive      = {J_Alg},
  author       = {Kangas, Kustaa and Koivisto, Mikko and Salonen, Sami},
  doi          = {10.1007/s00453-019-00633-1},
  journal      = {Algorithmica},
  number       = {8},
  pages        = {2156-2173},
  shortjournal = {Algorithmica},
  title        = {A faster tree-decomposition based algorithm for counting linear extensions},
  volume       = {82},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Multi-budgeted directed cuts. <em>Alg</em>, <em>82</em>(8),
2135–2155. (<a
href="https://doi.org/10.1007/s00453-019-00609-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we study multi-budgeted variants of the classic minimum cut problem and graph separation problems that turned out to be important in parameterized complexity: Skew Multicut and Directed Feedback Arc Set. In our generalization, we assign colors $$1,2,\ldots ,\ell $$ to some edges and give separate budgets $$k_{1},k_{2},\ldots ,k_{\ell }$$ for colors $$1,2,\ldots ,\ell $$ . For every color $$i\in {1,\ldots ,\ell }$$ , let $$E_{i}$$ be the set of edges of color i. The solution C for the multi-budgeted variant of a graph separation problem not only needs to satisfy the usual separation requirements (i.e., be a cut, a skew multicut, or a directed feedback arc set, respectively), but also needs to satisfy that $$|C\cap E_{i}|\le k_{i}$$ for every $$i\in {1,\ldots ,\ell }$$ . Contrary to the classic minimum cut problem, the multi-budgeted variant turns out to be NP-hard even for $$\ell = 2$$ . We propose FPT algorithms parameterized by $$k=k_{1}+\cdots +k_{\ell }$$ for all three problems. To this end, we develop a branching procedure for the multi-budgeted minimum cut problem that measures the progress of the algorithm not by reducing k as usual, by but elevating the capacity of some edges and thus increasing the size of maximum source-to-sink flow. Using the fact that a similar strategy is used to enumerate all important separators of a given size, we merge this process with the flow-guided branching and show an FPT bound on the number of (appropriately defined) important multi-budgeted separators. This allows us to extend our algorithm to the Skew Multicut and Directed Feedback Arc Set problems. Furthermore, we show connections of the multi-budgeted variants with weighted variants of the directed cut problems and the Chain $$\ell $$ -SAT problem, whose parameterized complexity remains an open problem. We show that these problems admit a bounded-in-parameter number of “maximally pushed” solutions (in a similar spirit as important separators are maximally pushed), giving somewhat weak evidence towards their tractability.},
  archive      = {J_Alg},
  author       = {Kratsch, Stefan and Li, Shaohua and Marx, Dániel and Pilipczuk, Marcin and Wahlström, Magnus},
  doi          = {10.1007/s00453-019-00609-1},
  journal      = {Algorithmica},
  number       = {8},
  pages        = {2135-2155},
  shortjournal = {Algorithmica},
  title        = {Multi-budgeted directed cuts},
  volume       = {82},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Special issue dedicated to the 13th international symposium
on parameterized and exact computation. <em>Alg</em>, <em>82</em>(8),
2133–2134. (<a
href="https://doi.org/10.1007/s00453-020-00748-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_Alg},
  author       = {Paul, Christophe and Pilipczuk, Michał},
  doi          = {10.1007/s00453-020-00748-w},
  journal      = {Algorithmica},
  number       = {8},
  pages        = {2133-2134},
  shortjournal = {Algorithmica},
  title        = {Special issue dedicated to the 13th international symposium on parameterized and exact computation},
  volume       = {82},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). On the optimality of tape merge of two lists with similar
size. <em>Alg</em>, <em>82</em>(7), 2107–2132. (<a
href="https://doi.org/10.1007/s00453-020-00690-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The problem of merging sorted lists in the least number of pairwise comparisons has been solved completely only for a few special cases. Graham and Karp (Sorting Search 3:197–207, 1999) independently discovered that the tape merge algorithm is optimal in the worst case when the two lists have the same size. In their seminal papers, Stockmeyer and Yao (SIAM J Comput 9(1):85–90, 1980), Murphy and Paull (Inf Control 42(1):87–96, 1979), and Christen (On the optimality of the straight merging algorithm, 1978) independently showed when the lists to be merged are of size m and n satisfying $$m\le n\le \lfloor \frac{3}{2}m\rfloor +1$$, the tape merge algorithm is optimal in the worst case. This paper extends this result by showing that the tape merge algorithm is optimal in the worst case whenever the size of one list is no larger than 1.52 times the size of the other. The main tool we use to prove the lower bound is Knuth’s (1999) adversary methods. In addition, we show that the lower bound cannot be improved to 1.8 via Knuth’s adversary methods. Moreover, we design a simple procedure, and by invoking this procedure recursively until the remaining subproblem can be solved efficiently by another known algorithm, we achieve constant improvement of the upper bound for $$2m-2\le n\le 3m $$.},
  archive      = {J_Alg},
  author       = {Li, Qian and Sun, Xiaoming and Zhang, Jialin},
  doi          = {10.1007/s00453-020-00690-x},
  journal      = {Algorithmica},
  number       = {7},
  pages        = {2107-2132},
  shortjournal = {Algorithmica},
  title        = {On the optimality of tape merge of two lists with similar size},
  volume       = {82},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). The distance-constrained matroid median problem.
<em>Alg</em>, <em>82</em>(7), 2087–2106. (<a
href="https://doi.org/10.1007/s00453-020-00688-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Alamdari and Shmoys introduced the following variant of the k-median problem. In this variant, we are given an instance of the k-median problem and a threshold value. Then this variant is the same as the k-median problem except that if the distance between a client i and a facility j is more than the threshold value, then i is not allowed to be connected to j. In this paper, we consider a matroid generalization of this variant of the k-median problem. First, we introduce a generalization of this variant in which the constraint on the number of opened facilities is replaced by a matroid constraint. Then we propose a polynomial-time bicriteria approximation algorithm for this problem by combining the algorithm of Alamdari and Shmoys and the algorithm of Krishnaswamy, Kumar, Nagarajan, Sabharwal, and Saha for a matroid generalization of the k-median problem.},
  archive      = {J_Alg},
  author       = {Kamiyama, Naoyuki},
  doi          = {10.1007/s00453-020-00688-5},
  journal      = {Algorithmica},
  number       = {7},
  pages        = {2087-2106},
  shortjournal = {Algorithmica},
  title        = {The distance-constrained matroid median problem},
  volume       = {82},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Compressed dynamic range majority and minority data
structures. <em>Alg</em>, <em>82</em>(7), 2063–2086. (<a
href="https://doi.org/10.1007/s00453-020-00687-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the range $$\alpha$$-majority query problem, we are given a sequence $$S[1\ldots n]$$ and a fixed threshold $$\alpha \in (0, 1)$$, and are asked to preprocess S such that, given a query range $$[i\ldots j]$$, we can efficiently report the symbols that occur more than $$\alpha (j-i+1)$$ times in $$S[i\ldots j]$$, which are called the range $$\alpha$$-majorities. In this article we describe the first compressed dynamic data structure for range $$\alpha$$-majority queries. It represents S in compressed space—$$nH_k+ o(n\lg \sigma )$$ bits for any $$k = o(\lg _{\sigma } n)$$, where $$\sigma$$ is the alphabet size and $$H_k \le H_0 \le \lg \sigma$$ is the kth order empirical entropy of S—and answers queries in $$O \left( \frac{\lg n}{\alpha \lg \lg n} \right)$$ time while supporting insertions and deletions in S in $$O \left( \frac{\lg n}{\alpha } \right)$$ amortized time. We then show how to modify our data structure to receive some $$\beta \ge \alpha$$ at query time and report the range $$\beta$$-majorities in $$O \left( \frac{\lg n}{\beta \lg \lg n} \right)$$ time, without increasing the asymptotic space or update-time bounds. The best previous dynamic solution has the same query and update times as ours, but it occupies O(n) words and cannot take advantage of being given a larger threshold $$\beta$$ at query time. We also design the first dynamic data structure for range $$\alpha$$-minority—i.e., find a non-$$\alpha$$-majority that occurs in a range—and obtain space and time bounds similar to those for $$\alpha$$-majorities. We extend the structure to find $$\varTheta (1/\alpha )$$$$\alpha$$-minorities at the same space and time cost. By giving up updates, we obtain static data structures with query time $$O((1 / \alpha ) \lg \lg _w \sigma )$$ for both problems, on a RAM with word size $$w = \varOmega (\lg n)$$ bits, without increasing our space bound. Static alternatives reach time $$O(1/\alpha )$$, but they compress S only to zeroth order entropy ($$H_0$$) or they only handle small values of $$\alpha$$, that is, $$\lg (1/\alpha ) = o(\lg \sigma )$$.},
  archive      = {J_Alg},
  author       = {Gagie, Travis and He, Meng and Navarro, Gonzalo},
  doi          = {10.1007/s00453-020-00687-6},
  journal      = {Algorithmica},
  number       = {7},
  pages        = {2063-2086},
  shortjournal = {Algorithmica},
  title        = {Compressed dynamic range majority and minority data structures},
  volume       = {82},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). The generalized definitions of the two-dimensional largest
common substructure problems. <em>Alg</em>, <em>82</em>(7), 2039–2062.
(<a href="https://doi.org/10.1007/s00453-020-00685-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The similarity of two one-dimensional sequences is usually measured by the longest common subsequence (LCS) algorithms. However, these algorithms cannot be directly extended to solve the two or higher dimensional data. Thus, for the two-dimensional data, computing the similarity with an LCS-like approach remains worthy of investigation. In this paper, we utilize a systematic way to give the generalized definition of the two-dimensional largest common substructure (TLCS) problem by referring to the traditional LCS concept. With various matching rules, eight possible versions of TLCS problems may be defined. However, only four of them are shown to be valid. We prove that all of these four TLCS problems are $${\mathcal {NP}}$$-hard and $${\mathcal {APX}}$$-hard. To accomplish the proofs, two of the TLCS problems are reduced from the 3-satisfiability problem, and the other two are reduced from the 3-dimensional matching problem.},
  archive      = {J_Alg},
  author       = {Chan, Huang-Ting and Chiu, Hsuan-Tsung and Yang, Chang-Biau and Peng, Yung-Hsing},
  doi          = {10.1007/s00453-020-00685-8},
  journal      = {Algorithmica},
  number       = {7},
  pages        = {2039-2062},
  shortjournal = {Algorithmica},
  title        = {The generalized definitions of the two-dimensional largest common substructure problems},
  volume       = {82},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Parameterized aspects of strong subgraph closure.
<em>Alg</em>, <em>82</em>(7), 2006–2038. (<a
href="https://doi.org/10.1007/s00453-020-00684-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Motivated by the role of triadic closures in social networks, and the importance of finding a maximum subgraph avoiding a fixed pattern, we introduce and initiate the parameterized study of the StrongF-closure problem, where F is a fixed graph. This is a generalization of Strong Triadic Closure, whereas it is a relaxation of F-free Edge Deletion. In StrongF-closure, we want to select a maximum number of edges of the input graph G, and mark them as strong edges, in the following way: whenever a subset of the strong edges forms a subgraph isomorphic to F, then the corresponding induced subgraph of G is not isomorphic to F. Hence, the subgraph of G defined by the strong edges is not necessarily F-free, but whenever it contains a copy of F, there are additional edges in G to forbid that strong copy of F in G. We study StrongF-closure from a parameterized perspective with various natural parameterizations. Our main focus is on the number k of strong edges as the parameter. We show that the problem is FPT with this parameterization for every fixed graph F, whereas it does not admit a polynomial kernel even when $$F =P_3$$. In fact, this latter case is equivalent to the Strong Triadic Closure problem, which motivates us to study this problem on input graphs belonging to well known graph classes. We show that Strong Triadic Closure does not admit a polynomial kernel even when the input graph is a split graph, whereas it admits a polynomial kernel when the input graph is planar, and even d-degenerate. Furthermore, on graphs of maximum degree at most 4, we show that Strong Triadic Closure is FPT with the above guarantee parameterization $$k - \mu (G)$$, where $$\mu (G)$$ is the maximum matching size of G. We conclude with some results on the parameterization of StrongF-closure by the number of edges of G that are not selected as strong.},
  archive      = {J_Alg},
  author       = {Golovach, Petr A. and Heggernes, Pinar and Konstantinidis, Athanasios L. and Lima, Paloma T. and Papadopoulos, Charis},
  doi          = {10.1007/s00453-020-00684-9},
  journal      = {Algorithmica},
  number       = {7},
  pages        = {2006-2038},
  shortjournal = {Algorithmica},
  title        = {Parameterized aspects of strong subgraph closure},
  volume       = {82},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). The parameterized hardness of the k-center problem in
transportation networks. <em>Alg</em>, <em>82</em>(7), 1989–2005. (<a
href="https://doi.org/10.1007/s00453-020-00683-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper we study the hardness of the $$k$$ -Center problem on inputs that model transportation networks. For the problem, a graph $$G=(V,E)$$ with edge lengths and an integer k are given and a center set $$C\subseteq V$$ needs to be chosen such that $$|C|\le k$$ . The aim is to minimize the maximum distance of any vertex in the graph to the closest center. This problem arises in many applications of logistics, and thus it is natural to consider inputs that model transportation networks. Such inputs are often assumed to be planar graphs, low doubling metrics, or bounded highway dimension graphs. For each of these models, parameterized approximation algorithms have been shown to exist. We complement these results by proving that the $$k$$ -Center problem is W[1]-hard on planar graphs of constant doubling dimension, where the parameter is the combination of the number of centers k, the highway dimension h, and the pathwidth p. Moreover, under the exponential time hypothesis there is no $$f(k,p,h)\cdot n^{o(p+\sqrt{k+h})}$$ time algorithm for any computable function f. Thus it is unlikely that the optimum solution to $$k$$ -Center can be found efficiently, even when assuming that the input graph abides to all of the above models for transportation networks at once! Additionally we give a simple parameterized $$(1+{\varepsilon })$$ -approximation algorithm for inputs of doubling dimension d with runtime $$(k^k/{\varepsilon }^{O(kd)})\cdot n^{O(1)}$$ . This generalizes a previous result, which considered inputs in D-dimensional $$L_q$$ metrics.},
  archive      = {J_Alg},
  author       = {Feldmann, Andreas Emil and Marx, Dániel},
  doi          = {10.1007/s00453-020-00683-w},
  journal      = {Algorithmica},
  number       = {7},
  pages        = {1989-2005},
  shortjournal = {Algorithmica},
  title        = {The parameterized hardness of the k-center problem in transportation networks},
  volume       = {82},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Robust reoptimization of steiner trees. <em>Alg</em>,
<em>82</em>(7), 1966–1988. (<a
href="https://doi.org/10.1007/s00453-020-00682-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In reoptimization, one is given an optimal solution to a problem instance and a (locally) modified instance. The goal is to obtain a solution for the modified instance. We aim to use information obtained from the given solution in order to obtain a better solution for the new instance than we are able to compute from scratch. In this paper, we consider Steiner tree reoptimization and address the optimality requirement of the provided solution. Instead of assuming that we are provided an optimal solution, we relax the assumption to the more realistic scenario where we are given an approximate solution with an upper bound on its performance guarantee. We show that for Steiner tree reoptimization there is a clear separation between local modifications where optimality is crucial for obtaining improved approximations and those instances where approximate solutions are acceptable starting points. For some of the local modifications that have been considered in previous research, we show that for every fixed $$\varepsilon &gt; 0$$ , approximating the reoptimization problem with respect to a given $$(1+\varepsilon )$$ -approximation is as hard as approximating the Steiner tree problem itself. In contrast, with a given optimal solution to the original problem it is known that one can obtain considerably improved results. Furthermore, we provide a new algorithmic technique that, with some further insights, allows us to obtain improved performance guarantees for Steiner tree reoptimization with respect to all remaining local modifications that have been considered in the literature: a required node of degree more than one becomes a Steiner node; a Steiner node becomes a required node; the cost of one edge is increased.},
  archive      = {J_Alg},
  author       = {Goyal, Keshav and Mömke, Tobias},
  doi          = {10.1007/s00453-020-00682-x},
  journal      = {Algorithmica},
  number       = {7},
  pages        = {1966-1988},
  shortjournal = {Algorithmica},
  title        = {Robust reoptimization of steiner trees},
  volume       = {82},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Parameterized complexity of conflict-free matchings and
paths. <em>Alg</em>, <em>82</em>(7), 1939–1965. (<a
href="https://doi.org/10.1007/s00453-020-00681-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {An input to a conflict-free variant of a classical problem $$\Gamma $$, called Conflict-Free$$\Gamma $$, consists of an instance I of $$\Gamma $$ coupled with a graph H, called the conflict graph. A solution to Conflict-Free$$\Gamma $$ in (I, H) is a solution to I in $$\Gamma $$, which is also an independent set in H. In this paper, we study conflict-free variants of Maximum Matching and Shortest Path, which we call Conflict-Free Maximum Matching (CF-MM) and Conflict-Free Shortest Path (CF-SP), respectively. We show that both CF-MM and CF-SP are W[1]-hard, when parameterized by the solution size. Moreover, W[1]-hardness for CF-MM holds even when the input graph where we want to find a matching is itself a matching, and W[1]-hardness for CF-SP holds for conflict graph being a unit-interval graph. Next, we study these problems with restriction on the conflict graphs. We give FPT algorithms for CF-MM when the conflict graph is chordal. Also, we give FPT algorithms for both CF-MM and CF-SP, when the conflict graph is d-degenerate. Finally, we design FPT algorithms for variants of CF-MM and CF-SP, where the conflicting conditions are given by a (representable) matroid.},
  archive      = {J_Alg},
  author       = {Agrawal, Akanksha and Jain, Pallavi and Kanesh, Lawqueen and Saurabh, Saket},
  doi          = {10.1007/s00453-020-00681-y},
  journal      = {Algorithmica},
  number       = {7},
  pages        = {1939-1965},
  shortjournal = {Algorithmica},
  title        = {Parameterized complexity of conflict-free matchings and paths},
  volume       = {82},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Parameterized orientable deletion. <em>Alg</em>,
<em>82</em>(7), 1909–1938. (<a
href="https://doi.org/10.1007/s00453-020-00679-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A graph is d-orientable if its edges can be oriented so that the maximum in-degree of the resulting digraph is at most d. d-orientability is a well-studied concept with close connections to fundamental graph-theoretic notions and applications as a load balancing problem. In this paper we consider the $$d$$ -Orientable Deletion problem: given a graph $$G=(V,E)$$ , delete the minimum number of vertices to make Gd-orientable. We contribute a number of results that improve the state of the art on this problem. Specifically:},
  archive      = {J_Alg},
  author       = {Hanaka, Tesshu and Katsikarelis, Ioannis and Lampis, Michael and Otachi, Yota and Sikora, Florian},
  doi          = {10.1007/s00453-020-00679-6},
  journal      = {Algorithmica},
  number       = {7},
  pages        = {1909-1938},
  shortjournal = {Algorithmica},
  title        = {Parameterized orientable deletion},
  volume       = {82},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020b). Placing labels in road maps: Algorithms and complexity.
<em>Alg</em>, <em>82</em>(7), 1881–1908. (<a
href="https://doi.org/10.1007/s00453-020-00678-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A road map can be interpreted as a graph embedded in the plane, in which each vertex corresponds to a road junction and each edge to a particular road section. In this paper, we consider the computational cartographic problem to place non-overlapping road labels along the edges so that as many road sections as possible are identified by their name, i.e., covered by a label. We show that this is NP-hard in general, but the problem can be solved in $$O(n^3)$$ time if the road map is an embedded tree with n vertices and constant maximum degree. This special case is not only of theoretical interest, but our algorithm in fact provides a very useful subroutine in exact or heuristic algorithms for labeling general road maps.},
  archive      = {J_Alg},
  author       = {Gemsa, Andreas and Niedermann, Benjamin and Nöllenburg, Martin},
  doi          = {10.1007/s00453-020-00678-7},
  journal      = {Algorithmica},
  number       = {7},
  pages        = {1881-1908},
  shortjournal = {Algorithmica},
  title        = {Placing labels in road maps: Algorithms and complexity},
  volume       = {82},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Subgraph complementation. <em>Alg</em>, <em>82</em>(7),
1859–1880. (<a
href="https://doi.org/10.1007/s00453-020-00677-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A subgraph complement of the graph G is a graph obtained from G by complementing all the edges in one of its induced subgraphs. We study the following algorithmic question: for a given graph G and graph class $${\mathscr {G}}$$, is there a subgraph complement of G which is in $${\mathscr {G}}$$? We show that this problem can be solved in polynomial time for various choices of the graphs class $${\mathscr {G}}$$, such as bipartite, d-degenerate, or cographs. We complement these results by proving that the problem is $${{\mathrm{NP}}}$$-complete when $${\mathscr {G}}$$ is the class of regular graphs.},
  archive      = {J_Alg},
  author       = {Fomin, Fedor V. and Golovach, Petr A. and Strømme, Torstein J. F. and Thilikos, Dimitrios M.},
  doi          = {10.1007/s00453-020-00677-8},
  journal      = {Algorithmica},
  number       = {7},
  pages        = {1859-1880},
  shortjournal = {Algorithmica},
  title        = {Subgraph complementation},
  volume       = {82},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Colouring (pr + ps)-free graphs. <em>Alg</em>,
<em>82</em>(7), 1833–1858. (<a
href="https://doi.org/10.1007/s00453-020-00675-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The k-Colouring problem is to decide if the vertices of a graph can be coloured with at most k colours for a fixed integer k such that no two adjacent vertices are coloured alike. If each vertex u must be assigned a colour from a prescribed list $$L(u)\subseteq {1,\ldots ,k},$$ then we obtain the List k-Colouring problem. A graph G is H-free if G does not contain H as an induced subgraph. We continue an extensive study into the complexity of these two problems for H-free graphs. The graph $$P_r+P_s$$ is the disjoint union of the r-vertex path $$P_r$$ and the s-vertex path $$P_s.$$ We prove that List 3-Colouring is polynomial-time solvable for $$(P_2+P_5)$$ -free graphs and for $$(P_3+P_4)$$ -free graphs. Combining our results with known results yields complete complexity classifications of 3-Colouring and List 3-Colouring on H-free graphs for all graphs H up to seven vertices.},
  archive      = {J_Alg},
  author       = {Klimošová, Tereza and Malík, Josef and Masařík, Tomáš and Novotná, Jana and Paulusma, Daniël and Slívová, Veronika},
  doi          = {10.1007/s00453-020-00675-w},
  journal      = {Algorithmica},
  number       = {7},
  pages        = {1833-1858},
  shortjournal = {Algorithmica},
  title        = {Colouring (Pr + ps)-free graphs},
  volume       = {82},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Shortest paths in the plane with obstacle violations.
<em>Alg</em>, <em>82</em>(7), 1813–1832. (<a
href="https://doi.org/10.1007/s00453-020-00673-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study the problem of finding shortest paths in the plane among h convex obstacles, where the path is allowed to pass through (violate) up to k obstacles, for $$k \le h$$. Equivalently, the problem is to find shortest paths that become obstacle-free if k obstacles are removed from the input. Given a fixed source point s, we show how to construct a map, called a shortest k-path map, so that all destinations in the same region of the map have the same combinatorial shortest path passing through at most k obstacles. We prove a tight bound of $$\varTheta (kn)$$ on the size of this map, and show that it can be computed in $$O(k^2n \log n)$$ time, where n is the total number of obstacle vertices.},
  archive      = {J_Alg},
  author       = {Hershberger, John and Kumar, Neeraj and Suri, Subhash},
  doi          = {10.1007/s00453-020-00673-y},
  journal      = {Algorithmica},
  number       = {7},
  pages        = {1813-1832},
  shortjournal = {Algorithmica},
  title        = {Shortest paths in the plane with obstacle violations},
  volume       = {82},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Walking through waypoints. <em>Alg</em>, <em>82</em>(7),
1784–1812. (<a
href="https://doi.org/10.1007/s00453-020-00672-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We initiate the study of a fundamental combinatorial problem: Given a capacitated graph $$G=(V,E)$$, find a shortest walk (“route”) from a source $${s\in V}$$ to a destination $$t\in V$$ that includes all vertices specified by a set $$WP \subseteq V$$: the waypoints. This Waypoint Routing Problem finds immediate applications in the context of modern networked systems. Our main contribution is an exact polynomial-time algorithm for graphs of bounded treewidth. We also show that if the number of waypoints is logarithmically bounded, exact polynomial-time algorithms exist even for general graphs. Our two algorithms provide an almost complete characterization of what can be solved exactly in polynomial time: we show that more general problems (e.g., on grid graphs of maximum degree 3, with slightly more waypoints) are computationally intractable.},
  archive      = {J_Alg},
  author       = {Akhoondian Amiri, Saeed and Foerster, Klaus-Tycho and Schmid, Stefan},
  doi          = {10.1007/s00453-020-00672-z},
  journal      = {Algorithmica},
  number       = {7},
  pages        = {1784-1812},
  shortjournal = {Algorithmica},
  title        = {Walking through waypoints},
  volume       = {82},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Why did the shape of your network change? (On detecting
network anomalies via non-local curvatures). <em>Alg</em>,
<em>82</em>(7), 1741–1783. (<a
href="https://doi.org/10.1007/s00453-019-00665-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Anomaly detection problems (also called change-point detection problems) have been studied in data mining, statistics and computer science over the last several decades (mostly in non-network context) in applications such as medical condition monitoring, weather change detection and speech recognition. In recent days, however, anomaly detection problems have become increasing more relevant in the context of network science since useful insights for many complex systems in biology, finance and social science are often obtained by representing them via networks. Notions of local and non-local curvatures of higher-dimensional geometric shapes and topological spaces play a fundamental role in physics and mathematics in characterizing anomalous behaviours of these higher dimensional entities. However, using curvature measures to detect anomalies in networks is not yet very common. To this end, a main goal in this paper to formulate and analyze curvature analysis methods to provide the foundations of systematic approaches to find critical components and detect anomalies in networks. For this purpose, we use two measures of network curvatures which depend on non-trivial global properties, such as distributions of geodesics and higher-order correlations among nodes, of the given network. Based on these measures, we precisely formulate several computational problems related to anomaly detection in static or dynamic networks, and provide non-trivial computational complexity results for these problems. This paper must not be viewed as delivering the final word on appropriateness and suitability of specific curvature measures. Instead, it is our hope that this paper will stimulate and motivate further theoretical or empirical research concerning the exciting interplay between notions of curvatures from network and non-network domains, a much desired goal in our opinion.},
  archive      = {J_Alg},
  author       = {DasGupta, Bhaskar and Janardhanan, Mano Vikash and Yahyanejad, Farzane},
  doi          = {10.1007/s00453-019-00665-7},
  journal      = {Algorithmica},
  number       = {7},
  pages        = {1741-1783},
  shortjournal = {Algorithmica},
  title        = {Why did the shape of your network change? (On detecting network anomalies via non-local curvatures)},
  volume       = {82},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Quasi-polynomial time approximation schemes for packing and
covering problems in planar graphs. <em>Alg</em>, <em>82</em>(6),
1703–1739. (<a
href="https://doi.org/10.1007/s00453-019-00670-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider two optimization problems in planar graphs. In Maximum Weight Independent Set of Objects we are given a graph G and a family $${\mathcal {D}}$$ of objects, each being a connected subgraph of G with a prescribed weight, and the task is to find a maximum-weight subfamily of $${\mathcal {D}}$$ consisting of pairwise disjoint objects. In Minimum Weight Distance Set Cover we are given a graph G in which the edges might have different lengths, two sets $${\mathcal {D}},{\mathcal {C}}$$ of vertices of G, where vertices of $${\mathcal {D}}$$ have prescribed weights, and a nonnegative radius r. The task is to find a minimum-weight subset of $${\mathcal {D}}$$ such that every vertex of $${\mathcal {C}}$$ is at distance at most r from some selected vertex. Via simple reductions, these two problems generalize a number of geometric optimization tasks, notably Maximum Weight Independent Set for polygons in the plane and Weighted Geometric Set Cover for unit disks and unit squares. We present quasi-polynomial time approximation schemes (QPTASs) for both of the above problems in planar graphs: given an accuracy parameter $$\epsilon &gt;0$$ we can compute a solution whose weight is within multiplicative factor of $$(1+\epsilon )$$ from the optimum in time $$2^{{\mathrm {poly}}(1/\epsilon ,\log |{\mathcal {D}}|)}\cdot n^{{\mathcal {O}}(1)}$$, where n is the number of vertices of the input graph. We note that a QPTAS for Maximum Weight Independent Set of Objects would follow from existing work. However, our main contribution is to provide a unified framework that works for both problems in both a planar and geometric setting and to transfer the techniques used for recursive approximation schemes for geometric problems due to Adamaszek and Wiese (in Proceedings of the FOCS 2013, IEEE, 2013; in Proceedings of the SODA 2014, SIAM, 2014) and Har-Peled and Sariel (in Proceedings of the SOCG 2014, SIAM, 2014) to the setting of planar graphs. In particular, this yields a purely combinatorial viewpoint on these methods as a phenomenon in planar graphs.},
  archive      = {J_Alg},
  author       = {Pilipczuk, Michał and van Leeuwen, Erik Jan and Wiese, Andreas},
  doi          = {10.1007/s00453-019-00670-w},
  journal      = {Algorithmica},
  number       = {6},
  pages        = {1703-1739},
  shortjournal = {Algorithmica},
  title        = {Quasi-polynomial time approximation schemes for packing and covering problems in planar graphs},
  volume       = {82},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). High-dimensional approximate r-nets. <em>Alg</em>,
<em>82</em>(6), 1675–1702. (<a
href="https://doi.org/10.1007/s00453-019-00664-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The construction of r-nets offers a powerful tool in computational and metric geometry. We focus on high-dimensional spaces and present a new randomized algorithm which efficiently computes approximate r-nets with respect to Euclidean distance. For any fixed $$\epsilon &gt;0$$, the approximation factor is $$1+\epsilon$$ and the complexity is polynomial in the dimension and subquadratic in the number of points; the algorithm succeeds with high probability. Specifically, we improve upon the best previously known (LSH-based) construction of Eppstein et al. (Approximate greedy clustering and distance selection for graph metrics, 2015. CoRR arxiv: abs/1507.01555) in terms of complexity, by reducing the dependence on $$\epsilon$$, provided that $$\epsilon$$ is sufficiently small. Moreover, our method does not require LSH but follows Valiant’s (J ACM 62(2):13, 2015. https://doi.org/10.1145/2728167) approach in designing a sequence of reductions of our problem to other problems in different spaces, under Euclidean distance or inner product, for which r-nets are computed efficiently and the error can be controlled. Our result immediately implies efficient solutions to a number of geometric problems in high dimension, such as finding the $$(1+\epsilon )$$-approximate k-th nearest neighbor distance in time subquadratic in the size of the input.},
  archive      = {J_Alg},
  author       = {Avarikioti, Z. and Emiris, I. Z. and Kavouras, L. and Psarros, I.},
  doi          = {10.1007/s00453-019-00664-8},
  journal      = {Algorithmica},
  number       = {6},
  pages        = {1675-1702},
  shortjournal = {Algorithmica},
  title        = {High-dimensional approximate r-nets},
  volume       = {82},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). An optimal XP algorithm for hamiltonian cycle on graphs of
bounded clique-width. <em>Alg</em>, <em>82</em>(6), 1654–1674. (<a
href="https://doi.org/10.1007/s00453-019-00663-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we prove that, given a clique-width k-expression of an n-vertex graph, Hamiltonian Cycle can be solved in time $$n^{\mathcal {O}(k)}$$. This improves the naive algorithm that runs in time $$n^{\mathcal {O}(k^2)}$$ by Espelage et al. (Graph-theoretic concepts in computer science, vol 2204. Springer, Berlin, 2001), and it also matches with the lower bound result by Fomin et al. that, unless the Exponential Time Hypothesis fails, there is no algorithm running in time $$n^{o(k)}$$ (Fomin et al. in SIAM J Comput 43:1541–1563, 2014). We present a technique of representative sets using two-edge colored multigraphs on k vertices. The essential idea is that, for a two-edge colored multigraph, the existence of an Eulerian trail that uses edges with different colors alternately can be determined by two information: the number of colored edges incident with each vertex, and the connectedness of the multigraph. With this idea, we avoid the bottleneck of the naive algorithm, which stores all the possible multigraphs on k vertices with at most n edges.},
  archive      = {J_Alg},
  author       = {Bergougnoux, Benjamin and Kanté, Mamadou Moustapha and Kwon, O-joung},
  doi          = {10.1007/s00453-019-00663-9},
  journal      = {Algorithmica},
  number       = {6},
  pages        = {1654-1674},
  shortjournal = {Algorithmica},
  title        = {An optimal XP algorithm for hamiltonian cycle on graphs of bounded clique-width},
  volume       = {82},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Nested convex bodies are chaseable. <em>Alg</em>,
<em>82</em>(6), 1640–1653. (<a
href="https://doi.org/10.1007/s00453-019-00661-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the Convex Body Chasing problem, we are given an initial point $$v_0 \in \mathbb {R}^d$$ and an online sequence of n convex bodies $$F_1, \ldots , F_n$$. When we receive $$F_t$$, we are required to move inside $$F_t$$. Our goal is to minimize the total distance traveled. This fundamental online problem was first studied by Friedman and Linial (DCG 1993). They proved an $$\varOmega (\sqrt{d})$$ lower bound on the competitive ratio, and conjectured that a competitive ratio depending only on d is possible. However, despite much interest in the problem, the conjecture remains wide open. We consider the setting in which the convex bodies are nested: $$F_1 \supset \cdots \supset F_n$$. The nested setting is closely related to extending the online LP framework of Buchbinder and Naor (ESA 2005) to arbitrary linear constraints. Moreover, this setting retains much of the difficulty of the general setting and captures an essential obstacle in resolving Friedman and Linial’s conjecture. In this work, we give a f(d)-competitive algorithm for chasing nested convex bodies in $$\mathbb {R}^d$$.},
  archive      = {J_Alg},
  author       = {Bansal, Nikhil and Böhm, Martin and Eliáš, Marek and Koumoutsos, Grigorios and Umboh, Seeun William},
  doi          = {10.1007/s00453-019-00661-x},
  journal      = {Algorithmica},
  number       = {6},
  pages        = {1640-1653},
  shortjournal = {Algorithmica},
  title        = {Nested convex bodies are chaseable},
  volume       = {82},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). On the complexity of finding internally vertex-disjoint long
directed paths. <em>Alg</em>, <em>82</em>(6), 1616–1639. (<a
href="https://doi.org/10.1007/s00453-019-00659-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For two positive integers k and $$\ell $$, a $$(k \times \ell )$$-spindle is the union of k pairwise internally vertex-disjoint directed paths with $$\ell $$ arcs each between two vertices u and v. We are interested in the (parameterized) complexity of several problems consisting in deciding whether a given digraph contains a subdivision of a spindle, which generalize both the Maximum Flow and Longest Path problems. We obtain the following complexity dichotomy: for a fixed $$\ell \ge 1$$, finding the largest k such that an input digraph G contains a subdivision of a $$(k \times \ell )$$-spindle is polynomial-time solvable if $$\ell \le 3$$, and NP-hard otherwise. We place special emphasis on finding spindles with exactly two paths and present FPT algorithms that are asymptotically optimal under the ETH. These algorithms are based on the technique of representative families in matroids, and use also color-coding as a subroutine. Finally, we study the case where the input graph is acyclic, and present several algorithmic and hardness results.},
  archive      = {J_Alg},
  author       = {Araújo, Júlio and Campos, Victor A. and Maia, Ana Karolinna and Sau, Ignasi and Silva, Ana},
  doi          = {10.1007/s00453-019-00659-5},
  journal      = {Algorithmica},
  number       = {6},
  pages        = {1616-1639},
  shortjournal = {Algorithmica},
  title        = {On the complexity of finding internally vertex-disjoint long directed paths},
  volume       = {82},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Speeding up the gomory-hu parallel cut tree algorithm with
efficient graph contractions. <em>Alg</em>, <em>82</em>(6), 1601–1615.
(<a href="https://doi.org/10.1007/s00453-019-00658-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A cut tree is a combinatorial structure that represents the edge-connectivity between all pairs of nodes of an undirected graph. Cut trees have multiple applications in dependability, as they represent how much it takes to disconnect every pair of network nodes. They have been used for solving connectivity problems, routing, and in the analysis of complex networks, among several other applications. This work presents a parallel version of the classical Gomory-Hu cut tree algorithm. The algorithm is heavily based on tasks that compute the minimum cut on contracted graphs. The main contribution is an efficient strategy to compute the contracted graphs, that allows processes to take advantage of previously contracted graph instances, instead of always computing all contractions from the original input graph. The proposed algorithm was implemented using MPI and experimental results are presented for several families of graphs and show significant performance gains.},
  archive      = {J_Alg},
  author       = {Maske, Charles and Cohen, Jaime and Duarte, Elias P.},
  doi          = {10.1007/s00453-019-00658-6},
  journal      = {Algorithmica},
  number       = {6},
  pages        = {1601-1615},
  shortjournal = {Algorithmica},
  title        = {Speeding up the gomory-hu parallel cut tree algorithm with efficient graph contractions},
  volume       = {82},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). On the complexity of computing treebreadth. <em>Alg</em>,
<em>82</em>(6), 1574–1600. (<a
href="https://doi.org/10.1007/s00453-019-00657-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {During the last decade, metric properties of the bags of tree decompositions of graphs have been studied. Roughly, the length and the breadth of a tree decomposition are the maximum diameter and radius of its bags respectively. The treelength and the treebreadth of a graph are the minimum length and breadth of its tree decompositions respectively. Pathlength and pathbreadth are defined similarly for path decompositions. In this paper, we answer open questions of Dragan and Köhler (Algorithmica 69(4):884–905, 2014) and Dragan et al. (Algorithm theory—SWAT 2014, Springer, pp 158–169, 2014) about the computational complexity of treebreadth, pathbreadth and pathlength. Namely, we prove that computing these graph invariants is NP-hard. We further investigate graphs with treebreadth one, i.e., graphs that admit a tree decomposition where each bag has a dominating vertex. We show that it is NP-complete to decide whether a graph belongs to this class. We then prove some structural properties of such graphs which allows us to design polynomial-time algorithms to decide whether a bipartite graph, resp., a planar graph (or more generally, a triangle-free graph, resp., a $$K_{3,3}$$-minor-free graph), has treebreadth one.},
  archive      = {J_Alg},
  author       = {Ducoffe, Guillaume and Legay, Sylvain and Nisse, Nicolas},
  doi          = {10.1007/s00453-019-00657-7},
  journal      = {Algorithmica},
  number       = {6},
  pages        = {1574-1600},
  shortjournal = {Algorithmica},
  title        = {On the complexity of computing treebreadth},
  volume       = {82},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Sublinear-space and bounded-delay algorithms for maximal
clique enumeration in graphs. <em>Alg</em>, <em>82</em>(6), 1547–1573.
(<a href="https://doi.org/10.1007/s00453-019-00656-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to the sheer size of real-world networks, delay and space have become quite relevant measures of the cost of enumerating patterns for network analytics. This paper presents efficient algorithms for listing maximal cliques in undirected graphs, providing the first sublinear-space bounds with guaranteed delay per solution.},
  archive      = {J_Alg},
  author       = {Conte, Alessio and Grossi, Roberto and Marino, Andrea and Versari, Luca},
  doi          = {10.1007/s00453-019-00656-8},
  journal      = {Algorithmica},
  number       = {6},
  pages        = {1547-1573},
  shortjournal = {Algorithmica},
  title        = {Sublinear-space and bounded-delay algorithms for maximal clique enumeration in graphs},
  volume       = {82},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Energy-optimal routes for battery electric vehicles.
<em>Alg</em>, <em>82</em>(5), 1490–1546. (<a
href="https://doi.org/10.1007/s00453-019-00655-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study the problem of computing paths that minimize energy consumption of a battery electric vehicle. For that, we must cope with specific properties, such as regenerative braking and constraints imposed by the battery capacity. These restrictions can be captured by profiles, which are a functional representation of optimal energy consumption between two locations, subject to initial state of charge. Efficient computation of profiles is a relevant problem on its own, but also a fundamental ingredient to many route planning approaches for battery electric vehicles. In this work, we prove that profiles have linear complexity. We examine different variants of Dijkstra’s algorithm to compute energy-optimal paths or profiles. Further, we derive a polynomial-time algorithm for the problem of finding an energy-optimal path between two locations that allows stops at charging stations. We also discuss a heuristic variant that is easy to implement, and carefully integrate it with the well-known Contraction Hierarchies algorithm and A* search. Finally, we propose a practical approach that enables computation of energy-optimal routes within milliseconds after fast (metric-dependent) preprocessing of the whole network. This enables flexible updates due to, e. g., weather forecasts or refinements of the consumption model. Practicality of our approaches is demonstrated in a comprehensive experimental study on realistic, large-scale road networks.},
  archive      = {J_Alg},
  author       = {Baum, Moritz and Dibbelt, Julian and Pajor, Thomas and Sauer, Jonas and Wagner, Dorothea and Zündorf, Tobias},
  doi          = {10.1007/s00453-019-00655-9},
  journal      = {Algorithmica},
  number       = {5},
  pages        = {1490-1546},
  shortjournal = {Algorithmica},
  title        = {Energy-optimal routes for battery electric vehicles},
  volume       = {82},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Solving and sampling with many solutions. <em>Alg</em>,
<em>82</em>(5), 1474–1489. (<a
href="https://doi.org/10.1007/s00453-019-00654-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We investigate the complexity of satisfiability problems parameterized by the size of the solution set compared to all solution candidates. Our main result is a uniform sampling algorithm for satisfying assignments of 2-CNF formulas that runs in expected time $$O^*(\varepsilon ^{-0.617})$$ where $$\varepsilon $$ is the fraction of assignments that are satisfying. This improves significantly over the trivial sampling bound of expected $$\Theta ^*(\varepsilon ^{-1})$$, and on all previous algorithms whenever $$\varepsilon = \Omega (0.708^n)$$, where n is the number of variables. We also consider algorithms for 3-SAT with an $$\varepsilon $$ fraction of satisfying assignments, and prove that we can output a satisfying assignment in $$O^*(\varepsilon ^{-0.936})$$ randomized time, and sample uniformly a satisfying assignment in time $$O^*(\varepsilon ^{-0.908}1.021^n)$$. In the end we also present sampling results in the cases of 1-IN-3-SAT, monotone bounded degree 3-SAT and planar k-SAT.},
  archive      = {J_Alg},
  author       = {Cardinal, Jean and Nummenpalo, Jerri and Welzl, Emo},
  doi          = {10.1007/s00453-019-00654-w},
  journal      = {Algorithmica},
  number       = {5},
  pages        = {1474-1489},
  shortjournal = {Algorithmica},
  title        = {Solving and sampling with many solutions},
  volume       = {82},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). The geodesic farthest-point voronoi diagram in a simple
polygon. <em>Alg</em>, <em>82</em>(5), 1434–1473. (<a
href="https://doi.org/10.1007/s00453-019-00651-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Given a set of point sites in a simple polygon, the geodesic farthest-point Voronoi diagram partitions the polygon into cells, at most one cell per site, such that every point in a cell has the same farthest site with respect to the geodesic metric. We present an $$O(n\log \log n+ m\log m)$$-time algorithm to compute the geodesic farthest-point Voronoi diagram of m point sites in a simple n-gon. This improves the previously best known algorithm by Aronov et al. (Discrete Comput Geom 9(3):217–255, 1993). In the case that all point sites are on the boundary of the simple polygon, we can compute the geodesic farthest-point Voronoi diagram in $$O((n+m) \log \log n)$$ time.},
  archive      = {J_Alg},
  author       = {Oh, Eunjin and Barba, Luis and Ahn, Hee-Kap},
  doi          = {10.1007/s00453-019-00651-z},
  journal      = {Algorithmica},
  number       = {5},
  pages        = {1434-1473},
  shortjournal = {Algorithmica},
  title        = {The geodesic farthest-point voronoi diagram in a simple polygon},
  volume       = {82},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Stable matching with uncertain linear preferences.
<em>Alg</em>, <em>82</em>(5), 1410–1433. (<a
href="https://doi.org/10.1007/s00453-019-00650-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider the two-sided stable matching setting in which there may be uncertainty about the agents’ preferences due to limited information or communication. We consider three models of uncertainty: (1) lottery model—for each agent, there is a probability distribution over linear preferences, (2) compact indifference model—for each agent, a weak preference order is specified and each linear order compatible with the weak order is equally likely and (3) joint probability model—there is a lottery over preference profiles. For each of the models, we study the computational complexity of computing the stability probability of a given matching as well as finding a matching with the highest probability of being stable. We also examine more restricted problems such as deciding whether a certainly stable matching exists. We find a rich complexity landscape for these problems, indicating that the form uncertainty takes is significant.},
  archive      = {J_Alg},
  author       = {Aziz, Haris and Biró, Péter and Gaspers, Serge and de Haan, Ronald and Mattei, Nicholas and Rastegari, Baharak},
  doi          = {10.1007/s00453-019-00650-0},
  journal      = {Algorithmica},
  number       = {5},
  pages        = {1410-1433},
  shortjournal = {Algorithmica},
  title        = {Stable matching with uncertain linear preferences},
  volume       = {82},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Consensus strings with small maximum distance and small
distance sum. <em>Alg</em>, <em>82</em>(5), 1378–1409. (<a
href="https://doi.org/10.1007/s00453-019-00647-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The parameterised complexity of various consensus string problems (Closest String, Closest Substring, Closest String with Outliers) is investigated in a more general setting, i. e., with a bound on the maximum Hamming distance and a bound on the sum of Hamming distances between solution and input strings. We completely settle the parameterised complexity of these generalised variants of Closest String and Closest Substring, and partly for Closest String with Outliers; in addition, we answer some open questions from the literature regarding the classical problem variants with only one distance bound. Finally, we investigate the question of polynomial kernels and respective lower bounds.},
  archive      = {J_Alg},
  author       = {Bulteau, Laurent and Schmid, Markus L.},
  doi          = {10.1007/s00453-019-00647-9},
  journal      = {Algorithmica},
  number       = {5},
  pages        = {1378-1409},
  shortjournal = {Algorithmica},
  title        = {Consensus strings with small maximum distance and small distance sum},
  volume       = {82},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Fully-online suffix tree and directed acyclic word graph
construction for multiple texts. <em>Alg</em>, <em>82</em>(5),
1346–1377. (<a
href="https://doi.org/10.1007/s00453-019-00646-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider the construction of the suffix tree and the directed acyclic word graph (DAWG) indexing data structures for a collection $$\mathcal {T}$$ of texts, where a new symbol may be appended to any text in $$\mathcal {T} = {T_1, \ldots , T_K}$$, at any time. This fully-online scenario, which arises in dynamically indexing multi-sensor data, is a natural generalization of the long solved semi-online text indexing problem, where texts $$T_1, \ldots , T_{k}$$ are permanently fixed before the next text $$T_{k+1}$$ is processed for each k ($$1 \le k &lt; K$$). We first show that a direct application of Weiner’s right-to-left online construction for the suffix tree of a single text to fully-online multiple texts requires superlinear time. This also means that Blumer et al.’s left-to-right online construction for the DAWG of a single text requires superlinear time in the fully-online setting. We then present our fully-online versions of these algorithms that run in $$O(N \log \sigma )$$ time and O(N) space, where N is the total length of the texts in $$\mathcal {T}$$ and $$\sigma $$ is their alphabet size. We then show how to extend Ukkonen’s left-to-right online suffix tree construction to fully-online multiple strings, with the aid of Weiner’s suffix tree for the reversed texts.},
  archive      = {J_Alg},
  author       = {Takagi, Takuya and Inenaga, Shunsuke and Arimura, Hiroki and Breslauer, Dany and Hendrian, Diptarama},
  doi          = {10.1007/s00453-019-00646-w},
  journal      = {Algorithmica},
  number       = {5},
  pages        = {1346-1377},
  shortjournal = {Algorithmica},
  title        = {Fully-online suffix tree and directed acyclic word graph construction for multiple texts},
  volume       = {82},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). An analysis of budgeted parallel search on conditional
galton–watson trees. <em>Alg</em>, <em>82</em>(5), 1329–1345. (<a
href="https://doi.org/10.1007/s00453-019-00645-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently Avis and Jordan have demonstrated the efficiency of a simple technique called budgeting for the parallelization of a number of tree search algorithms. The idea is to limit the amount of work that a processor performs before it terminates its search and returns any unexplored nodes to a master process. This limit is set by a critical budget parameter which determines the overhead of the process. In this paper we study the behaviour of the budget parameter on conditional Galton–Watson trees obtaining asymptotically tight bounds on this overhead. We present empirical results to show that this bound is surprisingly accurate in practice.},
  archive      = {J_Alg},
  author       = {Avis, David and Devroye, Luc},
  doi          = {10.1007/s00453-019-00645-x},
  journal      = {Algorithmica},
  number       = {5},
  pages        = {1329-1345},
  shortjournal = {Algorithmica},
  title        = {An analysis of budgeted parallel search on conditional Galton–Watson trees},
  volume       = {82},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Dividing splittable goods evenly and with limited
fragmentation. <em>Alg</em>, <em>82</em>(5), 1298–1328. (<a
href="https://doi.org/10.1007/s00453-019-00643-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A splittable good provided in n pieces shall be divided as evenly as possible among m agents, where every agent can take shares from at most F pieces. We call F the fragmentation and mainly restrict attention to the cases $$F=1$$ and $$F=2$$. For $$F=1$$, the max–min and min–max problems are solvable in linear time. The case $$F=2$$ has neat formulations and structural characterizations in terms of weighted graphs. First we focus on perfectly balanced solutions. While the problem is strongly NP-hard in general, it can be solved in linear time if $$m\ge n-1$$, and a solution always exists in this case, in contrast to $$F=1$$. Moreover, the problem is fixed-parameter tractable in the parameter $$2m-n$$. (Note that this parameter measures the number of agents above the trivial threshold $$m=n/2$$.) The structural results suggest another related problem where unsplittable items shall be assigned to subsets so as to balance the average sizes (rather than the total sizes) in these subsets. We give an approximation-preserving reduction from our original splitting problem with fragmentation $$F=2$$ to this averaging problem, and some approximation results in cases when m is close to either n or n / 2.},
  archive      = {J_Alg},
  author       = {Damaschke, Peter},
  doi          = {10.1007/s00453-019-00643-z},
  journal      = {Algorithmica},
  number       = {5},
  pages        = {1298-1328},
  shortjournal = {Algorithmica},
  title        = {Dividing splittable goods evenly and with limited fragmentation},
  volume       = {82},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Testing proximity to subspaces: Approximate <span
class="math display"><em>ℓ</em><sub>∞</sub></span> minimization in
constant time. <em>Alg</em>, <em>82</em>(5), 1277–1297. (<a
href="https://doi.org/10.1007/s00453-019-00642-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider the subspace proximity problem: Given a vector $${\varvec{x}} \in {\mathbb {R}}^n$$ and a basis matrix $$V \in {\mathbb {R}}^{n \times m}$$, the objective is to determine whether $${\varvec{x}}$$ is close to the subspace spanned by V. Although the problem is solvable by linear programming, it is time consuming especially when n is large. In this paper, we propose a quick tester that solves the problem correctly with high probability. Our tester runs in time independent of n and can be used as a sieve before computing the exact distance between $${\varvec{x}}$$ and the subspace. The number of coordinates of $${\varvec{x}}$$ queried by our tester is $$O(\frac{m}{\epsilon }\log \frac{m}{\epsilon })$$, where $$\epsilon $$ is an error parameter, and we show almost matching lower bounds. By experiments, we demonstrate the scalability and applicability of our tester using synthetic and real data sets.},
  archive      = {J_Alg},
  author       = {Hayashi, Kohei and Yoshida, Yuichi},
  doi          = {10.1007/s00453-019-00642-0},
  journal      = {Algorithmica},
  number       = {5},
  pages        = {1277-1297},
  shortjournal = {Algorithmica},
  title        = {Testing proximity to subspaces: Approximate $$\ell _\infty $$ minimization in constant time},
  volume       = {82},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Reachability oracles for directed transmission graphs.
<em>Alg</em>, <em>82</em>(5), 1259–1276. (<a
href="https://doi.org/10.1007/s00453-019-00641-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Let $$P \subset \mathbb {R}^d$$ be a set of n points in d dimensions such that each point $$p \in P$$ has an associated radius$$r_p &gt; 0$$. The transmission graphG for P is the directed graph with vertex set P such that there is an edge from p to q if and only if $$|pq| \le r_p$$, for any $$p, q \in P$$. A reachability oracle is a data structure that decides for any two vertices $$p, q \in G$$ whether G has a path from p to q. The quality of the oracle is measured by the space requirement S(n), the query time Q(n), and the preprocessing time. For transmission graphs of one-dimensional point sets, we can construct in $$O(n \log n)$$ time an oracle with $$Q(n) = O(1)$$ and $$S(n) = O(n)$$. For planar point sets, the ratio $$\Psi $$ between the largest and the smallest associated radius turns out to be an important parameter. We present three data structures whose quality depends on $$\Psi $$: the first works only for $$\Psi &lt; \sqrt{3}$$ and achieves $$Q(n) = O(1)$$ with $$S(n) = O(n)$$ and preprocessing time $$O(n\log n)$$; the second data structure gives $$Q(n) = O(\Psi ^3 \sqrt{n})$$ and $$S(n) = O(\Psi ^3 n^{3/2})$$; the third data structure is randomized with $$Q(n) = O(n^{2/3}\log ^{1/3} \Psi \log ^{2/3} n)$$ and $$S(n) = O(n^{5/3}\log ^{1/3} \Psi \log ^{2/3} n)$$ and answers queries correctly with high probability.},
  archive      = {J_Alg},
  author       = {Kaplan, Haim and Mulzer, Wolfgang and Roditty, Liam and Seiferth, Paul},
  doi          = {10.1007/s00453-019-00641-1},
  journal      = {Algorithmica},
  number       = {5},
  pages        = {1259-1276},
  shortjournal = {Algorithmica},
  title        = {Reachability oracles for directed transmission graphs},
  volume       = {82},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A constant-time algorithm for middle levels gray codes.
<em>Alg</em>, <em>82</em>(5), 1239–1258. (<a
href="https://doi.org/10.1007/s00453-019-00640-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For any integer $$n\ge 1$$, a middle levels Gray code is a cyclic listing of all n-element and $$(n+1)$$-element subsets of $${1,2,\ldots ,2n+1}$$ such that any two consecutive sets differ in adding or removing a single element. The question whether such a Gray code exists for any $$n\ge 1$$ has been the subject of intensive research during the last 30 years, and has been answered affirmatively only recently [T. Mütze. Proof of the middle levels conjecture. Proc. London Math. Soc., 112(4):677–713, 2016]. In a follow-up paper [T. Mütze and J. Nummenpalo. An efficient algorithm for computing a middle levels Gray code. ACM Trans. Algorithms, 14(2):29 pp., 2018] this existence proof was turned into an algorithm that computes each new set in the Gray code in time $${{\mathcal {O}}}(n)$$ on average. In this work we present an algorithm for computing a middle levels Gray code in optimal time and space: each new set is generated in time $${{\mathcal {O}}}(1)$$ on average, and the required space is $${{\mathcal {O}}}(n)$$.},
  archive      = {J_Alg},
  author       = {Mütze, Torsten and Nummenpalo, Jerri},
  doi          = {10.1007/s00453-019-00640-2},
  journal      = {Algorithmica},
  number       = {5},
  pages        = {1239-1258},
  shortjournal = {Algorithmica},
  title        = {A constant-time algorithm for middle levels gray codes},
  volume       = {82},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Computing a pessimistic stackelberg equilibrium with
multiple followers: The mixed-pure case. <em>Alg</em>, <em>82</em>(5),
1189–1238. (<a
href="https://doi.org/10.1007/s00453-019-00648-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The search problem of computing a Stackelberg (or leader-follower)equilibrium (also referred to as an optimal strategy to commit to) has been widely investigated in the scientific literature in, almost exclusively, the single-follower setting. Although the optimistic and pessimistic versions of the problem, i.e., those where the single follower breaks any ties among multiple equilibria either in favour or against the leader, are solved with different methodologies, both cases allow for efficient, polynomial-time algorithms based on linear programming. The situation is different with multiple followers, where results are only sporadic and depend strictly on the nature of the followers’ game. In this paper, we investigate the setting of a normal-form game with a single leader and multiple followers who, after observing the leader’s commitment, play a Nash equilibrium. When both leader and followers are allowed to play mixed strategies, the corresponding search problem, both in the optimistic and pessimistic versions, is known to be inapproximable in polynomial time to within any multiplicative polynomial factor unless $$\textsf {P}=\textsf {NP}$$. Exact algorithms are known only for the optimistic case. We focus on the case where the followers play pure strategies—a restriction that applies to a number of real-world scenarios and which, in principle, makes the problem easier—under the assumption of pessimism (the optimistic version of the problem can be straightforwardly solved in polynomial time). After casting this search problem (with followers playing pure strategies) as a pessimistic bilevel programming problem, we show that, with two followers, the problem is NP-hard and, with three or more followers, it cannot be approximated in polynomial time to within any multiplicative factor which is polynomial in the size of the normal-form game, nor, assuming utilities in [0, 1], to within any constant additive loss stricly smaller than 1 unless $$\textsf {P}=\textsf {NP}$$. This shows that, differently from what happens in the optimistic version, hardness and inapproximability in the pessimistic problem are not due to the adoption of mixed strategies. We then show that the problem admits, in the general case, a supremum but not a maximum, and we propose a single-level mathematical programming reformulation which asks for the maximization of a nonconcave quadratic function over an unbounded nonconvex feasible region defined by linear and quadratic constraints. Since, due to admitting a supremum but not a maximum, only a restricted version of this formulation can be solved to optimality with state-of-the-art methods, we propose an exact ad hoc algorithm (which we also embed within a branch-and-bound scheme) capable of computing the supremum of the problem and, for cases where there is no leader’s strategy where such value is attained, also an $$\alpha $$-approximate strategy where $$\alpha &gt; 0$$ is an arbitrary additive loss (at most as large as the supremum). We conclude the paper by evaluating the scalability of our algorithms via computational experiments on a well-established testbed of game instances.},
  archive      = {J_Alg},
  author       = {Coniglio, Stefano and Gatti, Nicola and Marchesi, Alberto},
  doi          = {10.1007/s00453-019-00648-8},
  journal      = {Algorithmica},
  number       = {5},
  pages        = {1189-1238},
  shortjournal = {Algorithmica},
  title        = {Computing a pessimistic stackelberg equilibrium with multiple followers: The mixed-pure case},
  volume       = {82},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Stable matchings with covering constraints: A complete
computational trichotomy. <em>Alg</em>, <em>82</em>(5), 1136–1188. (<a
href="https://doi.org/10.1007/s00453-019-00636-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Stable matching problems with lower quotas are fundamental in academic hiring and ensuring operability of rural hospitals. Only few tractable (polynomial-time solvable) cases of stable matching with lower quotas have been identified; most such problems are $$\mathsf {NP}$$-hard and also hard to approximate (Hamada et al. in Algorithmica 74(1):440–465, 2016). We therefore consider stable matching problems with lower quotas under a relaxed notion of tractability, namely fixed-parameter tractability. By cloning hospitals we focus on the case when all hospitals have upper quota equal to 1, which generalizes the setting of “arranged marriages” first considered by Knuth (Mariages stables et leurs relations avec d’autres problèmes combinatoires, Les Presses de l’Université de Montréal, Montreal, 1976). We investigate how a set of natural parameters, namely the maximum length of preference lists for men and women, the number of distinguished men and women, and the number of blocking pairs allowed determine the computational tractability of this problem. Our main result is a complete complexity trichotomy: for each choice of parameters we either provide a polynomial-time algorithm, or an $$\mathsf {NP}$$-hardness proof and fixed-parameter algorithm, or $$\mathsf {NP}$$-hardness proof and $$\mathsf {W}[1]$$-hardness proof. As corollary, we negatively answer a question by Hamada et al. (Algorithmica 74(1):440–465, 2016) by showing fixed-parameter intractability parameterized by optimal solution size. We also classify all cases of one-sided constraints where only women may be distinguished.},
  archive      = {J_Alg},
  author       = {Mnich, Matthias and Schlotter, Ildikó},
  doi          = {10.1007/s00453-019-00636-y},
  journal      = {Algorithmica},
  number       = {5},
  pages        = {1136-1188},
  shortjournal = {Algorithmica},
  title        = {Stable matchings with covering constraints: A complete computational trichotomy},
  volume       = {82},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Stochastic dominance and the bijective ratio of online
algorithms. <em>Alg</em>, <em>82</em>(5), 1101–1135. (<a
href="https://doi.org/10.1007/s00453-019-00638-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Stochastic dominance is a technique for evaluating the performance of online algorithms that provides an intuitive, yet powerful stochastic order between the compared algorithms. When there is a uniform distribution over the request sequences, this technique reduces to bijective analysis. These methods have been applied in problems such as paging, list update, bin colouring, routing in array mesh networks, and in connection with Bloom filters, and have often provided a clear separation between algorithms whose performance varies significantly in practice. Despite their appealing properties, the above techniques are quite stringent, in that a relation between online algorithms may be either too difficult to establish analytically, or worse, may not even exist. In this paper, we propose remedies to these shortcomings. Our objective is to make all online algorithms amenable to the techniques of stochastic dominance and bijective analysis. First, we establish sufficient conditions that allow us to prove the bijective optimality of a certain class of algorithms for a wide range of problems; we demonstrate this approach in the context of well-studied online problems such as weighted paging, reordering buffer management, and 2-server on the circle. Second, to account for situations in which two algorithms are incomparable or there is no clear optimum, we introduce the bijective ratio as a natural extension of (exact) bijective analysis. Our definition readily generalizes to stochastic dominance. This makes it possible to compare two arbitrary online algorithms for an arbitrary online problem. In addition, the bijective ratio is a generalization of the Max/Max ratio (due to Ben-David and Borodin), and allows for the incorporation of other useful techniques such as amortized analysis. We demonstrate the applicability of the bijective ratio to one of the fundamental online problems, namely the continuous k-server problem on metrics such as the line, the circle, and the star. Among other results, we show that the greedy algorithm attains bijective ratios of O(k) across these metrics.},
  archive      = {J_Alg},
  author       = {Angelopoulos, Spyros and Renault, Marc P. and Schweitzer, Pascal},
  doi          = {10.1007/s00453-019-00638-w},
  journal      = {Algorithmica},
  number       = {5},
  pages        = {1101-1135},
  shortjournal = {Algorithmica},
  title        = {Stochastic dominance and the bijective ratio of online algorithms},
  volume       = {82},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Non-monochromatic and conflict-free colorings on tree spaces
and planar network spaces. <em>Alg</em>, <em>82</em>(5), 1081–1100. (<a
href="https://doi.org/10.1007/s00453-019-00639-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {It is well known that any set of n intervals in $$\mathbb {R} ^1$$ admits a non-monochromatic coloring with two colors and a conflict-free coloring with three colors. We investigate generalizations of this result to colorings of objects in more complex 1-dimensional spaces, namely so-called tree spaces and planar network spaces.},
  archive      = {J_Alg},
  author       = {Aronov, Boris and de Berg, Mark and Markovic, Aleksandar and Woeginger, Gerhard},
  doi          = {10.1007/s00453-019-00639-9},
  journal      = {Algorithmica},
  number       = {5},
  pages        = {1081-1100},
  shortjournal = {Algorithmica},
  title        = {Non-monochromatic and conflict-free colorings on tree spaces and planar network spaces},
  volume       = {82},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Deterministic dynamic matching in o(1) update time.
<em>Alg</em>, <em>82</em>(4), 1057–1080. (<a
href="https://doi.org/10.1007/s00453-019-00630-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider the problems of maintaining an approximate maximum matching and an approximate minimum vertex cover in a dynamic graph undergoing a sequence of edge insertions/deletions. Starting with the seminal work of Onak and Rubinfeld (in: Proceedings of the ACM symposium on theory of computing (STOC), 2010), this problem has received significant attention in recent years. Very recently, extending the framework of Baswana et al. (in: Proceedings of the IEEE symposium on foundations of computer science (FOCS), 2011) , Solomon (in: Proceedings of the IEEE symposium on foundations of computer science (FOCS), 2016) gave a randomized dynamic algorithm for this problem that has an approximation ratio of 2 and an amortized update time of O(1) with high probability. This algorithm requires the assumption of an oblivious adversary, meaning that the future sequence of edge insertions/deletions in the graph cannot depend in any way on the algorithm’s past output. A natural way to remove the assumption on oblivious adversary is to give a deterministic dynamic algorithm for the same problem in O(1) update time. In this paper, we resolve this question. We present a new deterministic fully dynamic algorithm that maintains a O(1)-approximate minimum vertex cover and maximum fractional matching, with an amortized update time of O(1). Previously, the best deterministic algorithm for this problem was due to Bhattacharya et al. (in: Proceedings of the ACM-SIAM symposium on discrete algorithms (SODA), 2015); it had an approximation ratio of $$(2+\varepsilon )$$ and an amortized update time of $$O(\log n/\varepsilon ^2)$$. Our result can be generalized to give a fully dynamic $$O(f^3)$$-approximate algorithm with $$O(f^2)$$ amortized update time for the hypergraph vertex cover and fractional hypergraph matching problem, where every hyperedge has at most f vertices.},
  archive      = {J_Alg},
  author       = {Bhattacharya, Sayan and Chakrabarty, Deeparnab and Henzinger, Monika},
  doi          = {10.1007/s00453-019-00630-4},
  journal      = {Algorithmica},
  number       = {4},
  pages        = {1057-1080},
  shortjournal = {Algorithmica},
  title        = {Deterministic dynamic matching in o(1) update time},
  volume       = {82},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Space-efficient DFS and applications to connectivity
problems: Simpler, leaner, faster. <em>Alg</em>, <em>82</em>(4),
1033–1056. (<a
href="https://doi.org/10.1007/s00453-019-00629-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The problem of space-efficient depth-first search (DFS) is reconsidered. A particularly simple and fast algorithm is presented that, on a directed or undirected input graph $$G=(V,E)$$ with n vertices and m edges, carries out a DFS in $$O(n+m)$$ time with $$n+\sum _{v\in V_{\ge 3}}\lceil \log _2(d_v-1)\rceil +O(\log n)\le n+m+O(\log n)$$ bits of working memory, where $$d_v$$ is the (total) degree of v, for each $$v\in V$$, and $$V_{\ge 3}={v\in V\mid d_v\ge 3}$$. A slightly more complicated variant of the algorithm works in the same time with at most $$n+({4/5})m+O(\log n)$$ bits. It is also shown that a DFS can be carried out in a graph with n vertices and m edges in $$O(n+m+\min {n,m}\log ^*\!n)$$ time with O(n) bits or in $$O(n+m)$$ time with either $$O(n\log \log (4+{m/n}))$$ bits or, for arbitrary integer $$k\ge 1$$, $$O(n\log ^{(k)}\! n)$$ bits. These results among them subsume or improve most earlier results on space-efficient DFS. Some of the new time and space bounds are shown to extend to applications of DFS such as the computation of cut vertices, bridges, biconnected components and 2-edge-connected components in undirected graphs.},
  archive      = {J_Alg},
  author       = {Hagerup, Torben},
  doi          = {10.1007/s00453-019-00629-x},
  journal      = {Algorithmica},
  number       = {4},
  pages        = {1033-1056},
  shortjournal = {Algorithmica},
  title        = {Space-efficient DFS and applications to connectivity problems: Simpler, leaner, faster},
  volume       = {82},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Streaming algorithms for maximizing monotone submodular
functions under a knapsack constraint. <em>Alg</em>, <em>82</em>(4),
1006–1032. (<a
href="https://doi.org/10.1007/s00453-019-00628-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we consider the problem of maximizing a monotone submodular function subject to a knapsack constraint in the streaming setting. In particular, the elements arrive sequentially and at any point of time, the algorithm has access only to a small fraction of the data stored in primary memory. For this problem, we propose a $$(0.363-\varepsilon )$$-approximation algorithm, requiring only a single pass through the data; moreover, we propose a $$(0.4-\varepsilon )$$-approximation algorithm requiring a constant number of passes through the data. The required memory space of both algorithms depends only on the size of the knapsack capacity and $$\varepsilon $$.},
  archive      = {J_Alg},
  author       = {Huang, Chien-Chung and Kakimura, Naonori and Yoshida, Yuichi},
  doi          = {10.1007/s00453-019-00628-y},
  journal      = {Algorithmica},
  number       = {4},
  pages        = {1006-1032},
  shortjournal = {Algorithmica},
  title        = {Streaming algorithms for maximizing monotone submodular functions under a knapsack constraint},
  volume       = {82},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Extended learning graphs for triangle finding. <em>Alg</em>,
<em>82</em>(4), 980–1005. (<a
href="https://doi.org/10.1007/s00453-019-00627-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present new quantum algorithms for Triangle Finding improving its best previously known quantum query complexities for both dense and sparse instances. For dense graphs on n vertices, we get a query complexity of $$O(n^{5/4})$$ without any of the extra logarithmic factors present in the previous algorithm of Le Gall [FOCS’14]. For sparse graphs with $$m\ge n^{5/4}$$ edges, we get a query complexity of $$O(n^{11/12}m^{1/6}\sqrt{\log n})$$, which is better than the one obtained by Le Gall and Nakajima [ISAAC’15] when $$m \ge n^{3/2}$$. We also obtain an algorithm with query complexity $${O}(n^{5/6}(m\log n)^{1/6}+d_2\sqrt{n})$$ where $$d_2$$ is the quadratic mean of the degree distribution. Our algorithms are designed and analyzed in a new model of learning graphs that we call extended learning graphs. In addition, we present a framework in order to easily combine and analyze them. As a consequence we get much simpler algorithms and analyses than previous algorithms of Le Gall et al. based on the MNRS quantum walk framework [SICOMP’11].},
  archive      = {J_Alg},
  author       = {Carette, Titouan and Laurière, Mathieu and Magniez, Frédéric},
  doi          = {10.1007/s00453-019-00627-z},
  journal      = {Algorithmica},
  number       = {4},
  pages        = {980-1005},
  shortjournal = {Algorithmica},
  title        = {Extended learning graphs for triangle finding},
  volume       = {82},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020a). Correction to: Sorting real numbers in <span
class="math display">$$O(n\sqrt{\log n})$$</span> time and linear space.
<em>Alg</em>, <em>82</em>(4), 979. (<a
href="https://doi.org/10.1007/s00453-019-00652-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The original version of this article unfortunately contained an error in article title and abstract.},
  archive      = {J_Alg},
  author       = {Han, Yijie},
  doi          = {10.1007/s00453-019-00652-y},
  journal      = {Algorithmica},
  number       = {4},
  pages        = {979},
  shortjournal = {Algorithmica},
  title        = {Correction to: Sorting real numbers in $$O(n\sqrt{\log n})$$ time and linear space},
  volume       = {82},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020b). Sorting real numbers in <span class="math display">$$O\big
(n\sqrt{\log n}\big )$$</span> time and linear space. <em>Alg</em>,
<em>82</em>(4), 966–978. (<a
href="https://doi.org/10.1007/s00453-019-00626-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present an $$O(n\sqrt{\log n})$$ time and linear space algorithm for sorting real numbers. This breaks the $$O(n\log n)$$ time bound for sorting real numbers which was thought by some researchers to be the lower bound.},
  archive      = {J_Alg},
  author       = {Han, Yijie},
  doi          = {10.1007/s00453-019-00626-0},
  journal      = {Algorithmica},
  number       = {4},
  pages        = {966-978},
  shortjournal = {Algorithmica},
  title        = {Sorting real numbers in $$O\big (n\sqrt{\log n}\big )$$ time and linear space},
  volume       = {82},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Online clique clustering. <em>Alg</em>, <em>82</em>(4),
938–965. (<a href="https://doi.org/10.1007/s00453-019-00625-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Clique clustering is the problem of partitioning the vertices of a graph into disjoint clusters, where each cluster forms a clique in the graph, while optimizing some objective function. In online clustering, the input graph is given one vertex at a time, and any vertices that have previously been clustered together are not allowed to be separated. The goal is to maintain a clustering with an objective value close to the optimal solution. For the variant where we want to maximize the number of edges in the clusters, we propose an online algorithm based on the doubling technique. It has an asymptotic competitive ratio at most 15.646 and a strict competitive ratio at most 22.641. We also show that no deterministic algorithm can have an asymptotic competitive ratio better than 6. For the variant where we want to minimize the number of edges between clusters, we show that the deterministic competitive ratio of the problem is $$n-\omega (1)$$, where n is the number of vertices in the graph.},
  archive      = {J_Alg},
  author       = {Chrobak, Marek and Dürr, Christoph and Fabijan, Aleksander and Nilsson, Bengt J.},
  doi          = {10.1007/s00453-019-00625-1},
  journal      = {Algorithmica},
  number       = {4},
  pages        = {938-965},
  shortjournal = {Algorithmica},
  title        = {Online clique clustering},
  volume       = {82},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A nearly optimal algorithm for the geodesic voronoi diagram
of points in a simple polygon. <em>Alg</em>, <em>82</em>(4), 915–937.
(<a href="https://doi.org/10.1007/s00453-019-00624-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The geodesic Voronoi diagram of m point sites inside a simple polygon of n vertices is a subdivision of the polygon into m cells, one to each site, such that all points in a cell share the same nearest site under the geodesic distance. The best known lower bound for the construction time is $$\varOmega (n+m\log m)$$, and a matching upper bound is a long-standing open question. The state-of-the-art construction algorithms achieve $$O( (n+m) \log (n+m) )$$ and $$O(n+m\log m\log ^2n)$$ time, which are optimal for $$m=\varOmega (n)$$ and $$m=O(\frac{n}{\log ^3n})$$, respectively. In this paper, we give a construction algorithm with $$O( n + m ( \log m+ \log ^2 n ) )$$ time, and it is nearly optimal in the sense that if a single Voronoi vertex can be computed in $$O(\log n)$$ time, then the construction time will become the optimal $$O(n+m\log m)$$. In other words, we reduce the problem of constructing the diagram in the optimal time to the problem of computing a single Voronoi vertex in $$O(\log n)$$ time.},
  archive      = {J_Alg},
  author       = {Liu, Chih-Hung},
  doi          = {10.1007/s00453-019-00624-2},
  journal      = {Algorithmica},
  number       = {4},
  pages        = {915-937},
  shortjournal = {Algorithmica},
  title        = {A nearly optimal algorithm for the geodesic voronoi diagram of points in a simple polygon},
  volume       = {82},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). On bubble generators in directed graphs. <em>Alg</em>,
<em>82</em>(4), 898–914. (<a
href="https://doi.org/10.1007/s00453-019-00619-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Bubbles are pairs of internally vertex-disjoint (s, t)-paths in a directed graph, which have many applications in the processing of DNA and RNA data. Listing and analysing all bubbles in a given graph is usually unfeasible in practice, due to the exponential number of bubbles present in real data graphs. In this paper, we propose a notion of bubble generator set, i.e., a polynomial-sized subset of bubbles from which all the other bubbles can be obtained through a suitable application of a specific symmetric difference operator. This set provides a compact representation of the bubble space of a graph. A bubble generator can be useful in practice, since some pertinent information about all the bubbles can be more conveniently extracted from this compact set. We provide a polynomial-time algorithm to decompose any bubble of a graph into the bubbles of such a generator in a tree-like fashion. Finally, we present two applications of the bubble generator on a real RNA-seq dataset.},
  archive      = {J_Alg},
  author       = {Acuña, V. and Grossi, R. and Italiano, G. F. and Lima, L. and Rizzi, R. and Sacomoto, G. and Sagot, M.-F. and Sinaimeri, B.},
  doi          = {10.1007/s00453-019-00619-z},
  journal      = {Algorithmica},
  number       = {4},
  pages        = {898-914},
  shortjournal = {Algorithmica},
  title        = {On bubble generators in directed graphs},
  volume       = {82},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Quadratic vertex kernel for rainbow matching. <em>Alg</em>,
<em>82</em>(4), 881–897. (<a
href="https://doi.org/10.1007/s00453-019-00618-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we study the NP-complete colorful variant of the classic matching problem, namely, the Rainbow Matching problem. Given an edge-colored graph G and a positive integer k, the goal is to decide whether there exists a matching of size at least k such that the edges in the matching have distinct colors. Previously, in [MFCS’17], we studied this problem from the view point of Parameterized Complexity and gave efficient FPT algorithms as well as a quadratic kernel on paths. In this paper we design a quadratic vertex kernel for Rainbow Matching on general graphs; generalizing the earlier quadratic kernel on paths to general graphs. For our kernelization algorithm we combine a graph decomposition method with an application of expansion lemma.},
  archive      = {J_Alg},
  author       = {Gupta, Sushmita and Roy, Sanjukta and Saurabh, Saket and Zehavi, Meirav},
  doi          = {10.1007/s00453-019-00618-0},
  journal      = {Algorithmica},
  number       = {4},
  pages        = {881-897},
  shortjournal = {Algorithmica},
  title        = {Quadratic vertex kernel for rainbow matching},
  volume       = {82},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). On the relation of strong triadic closure and cluster
deletion. <em>Alg</em>, <em>82</em>(4), 853–880. (<a
href="https://doi.org/10.1007/s00453-019-00617-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study the parameterized and classical complexity of two problems that are concerned with induced paths on three vertices, called $$P_3$$s, in undirected graphs $$G=(V,E)$$. In Strong Triadic Closure we aim to label the edges in E as strong and weak such that at most k edges are weak and G contains no induced $$P_3$$ with two strong edges. In Cluster Deletion we aim to destroy all induced $$P_3$$s by a minimum number of edge deletions. We first show that Strong Triadic Closure admits a 4k-vertex kernel. Then, we study parameterization by $$\ell :=|E|-k$$ and show that both problems are fixed-parameter tractable and unlikely to admit a polynomial kernel with respect to $$\ell $$. Finally, we give a dichotomy of the classical complexity of both problems on H-free graphs for all H of order at most four.},
  archive      = {J_Alg},
  author       = {Grüttemeier, Niels and Komusiewicz, Christian},
  doi          = {10.1007/s00453-019-00617-1},
  journal      = {Algorithmica},
  number       = {4},
  pages        = {853-880},
  shortjournal = {Algorithmica},
  title        = {On the relation of strong triadic closure and cluster deletion},
  volume       = {82},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A unified framework for clustering constrained data without
locality property. <em>Alg</em>, <em>82</em>(4), 808–852. (<a
href="https://doi.org/10.1007/s00453-019-00616-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we consider a class of constrained clustering problems of points in $$\mathbb {R}^{d}$$, where d could be rather high. A common feature of these problems is that their optimal clusterings no longer have the locality property (due to the additional constraints), which is a key property required by many algorithms for their unconstrained counterparts. To overcome the difficulty caused by the loss of locality, we present in this paper a unified framework, called Peeling-and-Enclosing, to iteratively solve two variants of the constrained clustering problems, constrained k-means clustering (k-CMeans) and constrained k-median clustering (k-CMedian). Our framework generalizes Kumar et al.’s (J ACM 57(2):5, 2010) elegant k-means clustering approach from unconstrained data to constrained data, and is based on two standalone geometric techniques, called Simplex Lemma and Weaker Simplex Lemma, for k-CMeans and k-CMedian, respectively. The simplex lemma (or weaker simplex lemma) enables us to efficiently approximate the mean (or median) point of an unknown set of points by searching a small-size grid, independent of the dimensionality of the space, in a simplex (or the surrounding region of a simplex), and thus can be used to handle high dimensional data. If k and $$\frac{1}{\epsilon }$$ are fixed numbers, our framework generates, in nearly linear time (i.e., $$O(n(\log n)^{k+1}d)$$), $$O((\log n)^{k})$$k-tuple candidates for the k mean or median points, and one of them induces a $$(1+\epsilon )$$-approximation for k-CMeans or k-CMedian, where n is the number of points. Combining this unified framework with a problem-specific selection algorithm (which determines the best k-tuple candidate), we obtain a $$(1+\epsilon )$$-approximation for each of the constrained clustering problems. Our framework improves considerably the best known results for these problems. We expect that our technique will be applicable to other variants of k-means and k-median clustering problems without locality.},
  archive      = {J_Alg},
  author       = {Ding, Hu and Xu, Jinhui},
  doi          = {10.1007/s00453-019-00616-2},
  journal      = {Algorithmica},
  number       = {4},
  pages        = {808-852},
  shortjournal = {Algorithmica},
  title        = {A unified framework for clustering constrained data without locality property},
  volume       = {82},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Making bidirected graphs strongly connected. <em>Alg</em>,
<em>82</em>(4), 787–807. (<a
href="https://doi.org/10.1007/s00453-019-00613-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The problem of making a given directed graph strongly connected was well-investigated by Eswaran and Tarjan (SIAM J Comput 5:653–665, 1976). In contrast, the problem of making a given bidirected graph strongly connected has not yet been formulated. In this paper, we consider two related problems: making a given bidirected graph strongly connected with either the minimum cardinality of additional signs or the minimum cardinality of additional arcs. For the former problem, we show a closed formula of the minimum number of additional signs and give a linear-time algorithm for finding an optimal solution. For the latter problem, we give a linear-time algorithm for finding a feasible solution whose size is either equal to or more than the obvious lower bound by one.},
  archive      = {J_Alg},
  author       = {Matsuoka, Tatsuya and Sato, Shun},
  doi          = {10.1007/s00453-019-00613-5},
  journal      = {Algorithmica},
  number       = {4},
  pages        = {787-807},
  shortjournal = {Algorithmica},
  title        = {Making bidirected graphs strongly connected},
  volume       = {82},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Local algorithms for sparse spanning graphs. <em>Alg</em>,
<em>82</em>(4), 747–786. (<a
href="https://doi.org/10.1007/s00453-019-00612-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Constructing a spanning tree of a graph is one of the most basic tasks in graph theory. We consider a relaxed version of this problem in the setting of local algorithms. The relaxation is that the constructed subgraph is a sparse spanning subgraph containing at most $$(1+\epsilon )n$$ edges (where n is the number of vertices and $$\epsilon $$ is a given approximation/sparsity parameter). In the local setting, the goal is to quickly determine whether a given edge e belongs to such a subgraph, without constructing the whole subgraph, but rather by inspecting (querying) the local neighborhood of e. The challenge is to maintain consistency. That is, to provide answers concerning different edges according to the same spanning subgraph. We first show that for general bounded-degree graphs, the query complexity of any such algorithm must be $$\Omega (\sqrt{n})$$. This lower bound holds for constant-degree graphs that have high expansion. Next we design an algorithm for (bounded-degree) graphs with high expansion, obtaining a result that roughly matches the lower bound. We then turn to study graphs that exclude a fixed minor (and are hence non-expanding). We design an algorithm for such graphs, which may have an unbounded maximum degree. The query complexity of this algorithm is $$\mathrm{poly}(1/\epsilon , h)$$ (independent of n and the maximum degree), where h is the number of vertices in the excluded minor. Though our two algorithms are designed for very different types of graphs (and have very different complexities), on a high-level there are several similarities, and we highlight both the similarities and the differences.},
  archive      = {J_Alg},
  author       = {Levi, Reut and Ron, Dana and Rubinfeld, Ronitt},
  doi          = {10.1007/s00453-019-00612-6},
  journal      = {Algorithmica},
  number       = {4},
  pages        = {747-786},
  shortjournal = {Algorithmica},
  title        = {Local algorithms for sparse spanning graphs},
  volume       = {82},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Weak coverage of a rectangular barrier. <em>Alg</em>,
<em>82</em>(4), 721–746. (<a
href="https://doi.org/10.1007/s00453-019-00611-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Assume n wireless mobile sensors are initially dispersed in an ad hoc manner in a rectangular region. Each sensor can monitor a circular area of specific diameter around its position, called the sensor diameter. Sensors are required to move to final locations so that they can there detect any intruder crossing the region in a direction parallel to the sides of the rectangle, and thus provide weak barrier coverage of the region. We study three optimization problems related to the movement of sensors to achieve weak barrier coverage: minimizing the number of sensors moved (MinNum), minimizing the average distance moved by the sensors (MinSum), and minimizing the maximum distance moved by any sensor (MinMax). We give an $$O(n^{3/2})$$ time algorithm for the MinNum problem for sensors of diameter 1 that are initially placed at integer positions; in contrast the problem is shown to be NP-complete even for sensors of diameter 2 that are initially placed at integer positions. We show that the MinSum problem is solvable in $$O(n \log n)$$ time for the Manhattan metric and sensors of identical diameter (homogeneous sensors) in arbitrary initial positions, while it is NP-complete for heterogeneous sensors. Finally, we prove that even very restricted homogeneous versions of the MinMax problem are NP-complete.},
  archive      = {J_Alg},
  author       = {Dobrev, Stefan and Kranakis, Evangelos and Krizanc, Danny and Lafond, Manuel and Maňuch, Ján and Narayanan, Lata and Opatrny, Jaroslav and Stacho, Ladislav},
  doi          = {10.1007/s00453-019-00611-7},
  journal      = {Algorithmica},
  number       = {4},
  pages        = {721-746},
  shortjournal = {Algorithmica},
  title        = {Weak coverage of a rectangular barrier},
  volume       = {82},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Assortment optimisation under a general discrete choice
model: A tight analysis of revenue-ordered assortments. <em>Alg</em>,
<em>82</em>(4), 681–720. (<a
href="https://doi.org/10.1007/s00453-019-00610-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The assortment problem in revenue management is the problem of deciding which subset of products to offer to consumers in order to maximise revenue. A simple and natural strategy is to select the best assortment out of all those that are constructed by fixing a threshold revenue $$\pi $$ and then choosing all products with revenue at least $$\pi $$ . This is known as the revenue-ordered assortments strategy. In this paper we study the approximation guarantees provided by revenue-ordered assortments when customers are rational in the following sense: the probability of selecting a specific product from the set being offered cannot increase if the set is enlarged. This rationality assumption, known as regularity, is satisfied by almost all discrete choice models considered in the revenue management and choice theory literature, and in particular by random utility models. The bounds we obtain are tight and improve on recent results in that direction, such as for the Mixed Multinomial Logit model by Rusmevichientong et al. (Prod Oper Manag 23(11):2023–2039, 2014). An appealing feature of our analysis is its simplicity, as it relies only on the regularity condition. We also draw a connection between assortment optimisation and two pricing problems called unit demand envy-free pricing and Stackelberg minimum spanning tree: these problems can be restated as assortment problems under discrete choice models satisfying the regularity condition, and moreover revenue-ordered assortments correspond then to the well-studied uniform pricing heuristic. When specialised to that setting, the general bounds we establish for revenue-ordered assortments match and unify the best known results on uniform pricing.},
  archive      = {J_Alg},
  author       = {Berbeglia, Gerardo and Joret, Gwenaël},
  doi          = {10.1007/s00453-019-00610-8},
  journal      = {Algorithmica},
  number       = {4},
  pages        = {681-720},
  shortjournal = {Algorithmica},
  title        = {Assortment optimisation under a general discrete choice model: A tight analysis of revenue-ordered assortments},
  volume       = {82},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A central limit theorem for almost local additive tree
functionals. <em>Alg</em>, <em>82</em>(3), 642–679. (<a
href="https://doi.org/10.1007/s00453-019-00622-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {An additive functional of a rooted tree is a functional that can be calculated recursively as the sum of the values of the functional over the branches, plus a certain toll function. Svante Janson recently proved a central limit theorem for additive functionals of conditioned Galton–Watson trees under the assumption that the toll function is local, i.e. only depends on a fixed neighbourhood of the root. We extend his result to functionals that are “almost local” in a certain sense, thus covering a wider range of functionals. The notion of almost local functional intuitively means that the toll function can be approximated well by considering only a neighbourhood of the root. Our main result is illustrated by several explicit examples including natural graph-theoretic parameters such as the number of independent sets, the number of matchings, and the number of dominating sets. We also cover a functional stemming from a tree reduction procedure that was studied by Hackl, Heuberger, Kropf, and Prodinger.},
  archive      = {J_Alg},
  author       = {Ralaivaosaona, Dimbinaina and Šileikis, Matas and Wagner, Stephan},
  doi          = {10.1007/s00453-019-00622-4},
  journal      = {Algorithmica},
  number       = {3},
  pages        = {642-679},
  shortjournal = {Algorithmica},
  title        = {A central limit theorem for almost local additive tree functionals},
  volume       = {82},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Patterns in random permutations avoiding some sets of
multiple patterns. <em>Alg</em>, <em>82</em>(3), 616–641. (<a
href="https://doi.org/10.1007/s00453-019-00586-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider a random permutation drawn from the set of permutations of length n that avoid some given set of patterns of length 3. We show that the number of occurrences of another pattern $$\sigma $$ has a limit distribution, after suitable scaling. In several cases, the number is asymptotically normal; this contrasts to the cases of permutations avoiding a single pattern of length 3 studied in earlier papers.},
  archive      = {J_Alg},
  author       = {Janson, Svante},
  doi          = {10.1007/s00453-019-00586-5},
  journal      = {Algorithmica},
  number       = {3},
  pages        = {616-641},
  shortjournal = {Algorithmica},
  title        = {Patterns in random permutations avoiding some sets of multiple patterns},
  volume       = {82},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Embedding small digraphs and permutations in binary trees
and split trees. <em>Alg</em>, <em>82</em>(3), 589–615. (<a
href="https://doi.org/10.1007/s00453-019-00667-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We investigate the number of permutations that occur in random labellings of trees. This is a generalisation of the number of subpermutations occurring in a random permutation. It also generalises some recent results on the number of inversions in randomly labelled trees (Cai et al. in Combin Probab Comput 28(3):335–364, 2019). We consider complete binary trees as well as random split trees a large class of random trees of logarithmic height introduced by Devroye (SIAM J Comput 28(2):409–432, 1998. https://doi.org/10.1137/s0097539795283954). Split trees consist of nodes (bags) which can contain balls and are generated by a random trickle down process of balls through the nodes. For complete binary trees we show that asymptotically the cumulants of the number of occurrences of a fixed permutation in the random node labelling have explicit formulas. Our other main theorem is to show that for a random split tree, with probability tending to one as the number of balls increases, the cumulants of the number of occurrences are asymptotically an explicit parameter of the split tree. For the proof of the second theorem we show some results on the number of embeddings of digraphs into split trees which may be of independent interest.},
  archive      = {J_Alg},
  author       = {Albert, Michael and Holmgren, Cecilia and Johansson, Tony and Skerman, Fiona},
  doi          = {10.1007/s00453-019-00667-5},
  journal      = {Algorithmica},
  number       = {3},
  pages        = {589-615},
  shortjournal = {Algorithmica},
  title        = {Embedding small digraphs and permutations in binary trees and split trees},
  volume       = {82},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). QuickXsort: A fast sorting scheme in theory and practice.
<em>Alg</em>, <em>82</em>(3), 509–588. (<a
href="https://doi.org/10.1007/s00453-019-00634-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {QuickXsort is a highly efficient in-place sequential sorting scheme that mixes Hoare’s Quicksort algorithm with X, where X can be chosen from a wider range of other known sorting algorithms, like Heapsort, Insertionsort and Mergesort. Its major advantage is that QuickXsort can be in-place even if X is not. In this work we provide general transfer theorems expressing the number of comparisons of QuickXsort in terms of the number of comparisons of X. More specifically, if pivots are chosen as medians of (not too fast) growing size samples, the average number of comparisons of QuickXsort and X differ only by o(n)-terms. For median-of-k pivot selection for some constant k, the difference is a linear term whose coefficient we compute precisely. For instance, median-of-three QuickMergesort uses at most $$n \lg n - 0.8358n + {\mathcal {O}}(\log n)$$ comparisons. Furthermore, we examine the possibility of sorting base cases with some other algorithm using even less comparisons. By doing so the average-case number of comparisons can be reduced down to $$n \lg n - 1.4112n + o(n)$$ for a remaining gap of only 0.0315n comparisons to the known lower bound (while using only $${\mathcal {O}}(\log n)$$ additional space and $${\mathcal {O}}(n\log n)$$ time overall). Implementations of these sorting strategies show that the algorithms challenge well-established library implementations like Musser’s Introsort.},
  archive      = {J_Alg},
  author       = {Edelkamp, Stefan and Weiß, Armin and Wild, Sebastian},
  doi          = {10.1007/s00453-019-00634-0},
  journal      = {Algorithmica},
  number       = {3},
  pages        = {509-588},
  shortjournal = {Algorithmica},
  title        = {QuickXsort: A fast sorting scheme in theory and practice},
  volume       = {82},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Asymptotic analysis of regular sequences. <em>Alg</em>,
<em>82</em>(3), 429–508. (<a
href="https://doi.org/10.1007/s00453-019-00631-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, q-regular sequences in the sense of Allouche and Shallit are analysed asymptotically. It is shown that the summatory function of a regular sequence can asymptotically be decomposed as a finite sum of periodic fluctuations multiplied by a scaling factor. Each of these terms corresponds to an eigenvalue of the sum of matrices of a linear representation of the sequence; only the eigenvalues of absolute value larger than the joint spectral radius of the matrices contribute terms which grow faster than the error term. The paper has a particular focus on the Fourier coefficients of the periodic fluctuations: they are expressed as residues of the corresponding Dirichlet generating function. This makes it possible to compute them in an efficient way. The asymptotic analysis deals with Mellin–Perron summations and uses two arguments to overcome convergence issues, namely Hölder regularity of the fluctuations together with a pseudo-Tauberian argument. Apart from the very general result, three examples are discussed in more detail: For these examples, very precise asymptotic formulæ are presented. In the latter two examples, prior to this analysis only rough estimates were known.},
  archive      = {J_Alg},
  author       = {Heuberger, Clemens and Krenn, Daniel},
  doi          = {10.1007/s00453-019-00631-3},
  journal      = {Algorithmica},
  number       = {3},
  pages        = {429-508},
  shortjournal = {Algorithmica},
  title        = {Asymptotic analysis of regular sequences},
  volume       = {82},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Analytic combinatorics of lattice paths with forbidden
patterns, the vectorial kernel method, and generating functions for
pushdown automata. <em>Alg</em>, <em>82</em>(3), 386–428. (<a
href="https://doi.org/10.1007/s00453-019-00623-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article we develop a vectorial kernel method—a powerful method which solves in a unified framework all the problems related to the enumeration of words generated by a pushdown automaton. We apply it for the enumeration of lattice paths that avoid a fixed word (a pattern), or for counting the occurrences of a given pattern. We unify results from numerous articles concerning patterns like peaks, valleys, humps, etc., in Dyck and Motzkin paths. This refines the study by Banderier and Flajolet from 2002 on enumeration and asymptotics of lattice paths: we extend here their results to pattern-avoiding walks/bridges/meanders/excursions. We show that the autocorrelation polynomial of this forbidden pattern, as introduced by Guibas and Odlyzko in 1981 in the context of rational languages, still plays a crucial role for our algebraic languages. En passant, our results give the enumeration of some classes of self-avoiding walks, and prove several conjectures from the On-Line Encyclopedia of Integer Sequences. Finally, we also give the trivariate generating function (length, final altitude, number of occurrences of the pattern p), and we prove that the number of occurrences is normally distributed and linear with respect to the length of the walk: this is what Flajolet and Sedgewick call an instance of Borges’s theorem.},
  archive      = {J_Alg},
  author       = {Asinowski, Andrei and Bacher, Axel and Banderier, Cyril and Gittenberger, Bernhard},
  doi          = {10.1007/s00453-019-00623-3},
  journal      = {Algorithmica},
  number       = {3},
  pages        = {386-428},
  shortjournal = {Algorithmica},
  title        = {Analytic combinatorics of lattice paths with forbidden patterns, the vectorial kernel method, and generating functions for pushdown automata},
  volume       = {82},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Special issue on analysis of algorithms. <em>Alg</em>,
<em>82</em>(3), 385. (<a
href="https://doi.org/10.1007/s00453-019-00668-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_Alg},
  author       = {Fill, James Allen and Ward, Mark Daniel},
  doi          = {10.1007/s00453-019-00668-4},
  journal      = {Algorithmica},
  number       = {3},
  pages        = {385},
  shortjournal = {Algorithmica},
  title        = {Special issue on analysis of algorithms},
  volume       = {82},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Crossing number for graphs with bounded pathwidth.
<em>Alg</em>, <em>82</em>(2), 355–384. (<a
href="https://doi.org/10.1007/s00453-019-00653-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The crossing number is the smallest number of pairwise edge crossings when drawing a graph into the plane. There are only very few graph classes for which the exact crossing number is known or for which there at least exist constant approximation ratios. Furthermore, up to now, general crossing number computations have never been successfully tackled using bounded width of graph decompositions, like treewidth or pathwidth. In this paper, we show that the crossing number is tractable (even in linear time) for maximal graphs of bounded pathwidth 3. The technique also shows that the crossing number and the rectilinear (a.k.a. straight-line) crossing number are identical for this graph class, and that we require only an $$O(n)\times O(n)$$-grid to achieve such a drawing. Our techniques can further be extended to devise a 2-approximation for general graphs with pathwidth 3. One crucial ingredient here is that the crossing number of a graph with a separation pair can be lower-bounded using the crossing numbers of its cut-components, a result that may be interesting in its own right. Finally, we give a $$4{\mathbf{w}}^3$$-approximation of the crossing number for maximal graphs of pathwidth $${\mathbf{w}}$$. This is a constant approximation for bounded pathwidth. We complement this with an NP-hardness proof of the weighted crossing number already for pathwidth 3 graphs and bicliques $$K_{3,n}$$.},
  archive      = {J_Alg},
  author       = {Biedl, Therese and Chimani, Markus and Derka, Martin and Mutzel, Petra},
  doi          = {10.1007/s00453-019-00653-x},
  journal      = {Algorithmica},
  number       = {2},
  pages        = {355-384},
  shortjournal = {Algorithmica},
  title        = {Crossing number for graphs with bounded pathwidth},
  volume       = {82},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). On using toeplitz and circulant matrices for
johnson–lindenstrauss transforms. <em>Alg</em>, <em>82</em>(2), 338–354.
(<a href="https://doi.org/10.1007/s00453-019-00644-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Johnson–Lindenstrauss lemma is one of the cornerstone results in dimensionality reduction. A common formulation of it, is that there exists a random linear mapping $$f : {\mathbb {R}}^n \rightarrow {\mathbb {R}}^m$$ such that for any vector $$x \in {\mathbb {R}}^n$$, f preserves its norm to within $$(1 {\pm } \varepsilon )$$ with probability $$1 - \delta $$ if $$m = \varTheta (\varepsilon ^{-2} \lg (1/\delta ))$$. Much effort has gone into developing fast embedding algorithms, with the Fast Johnson–Lindenstrauss transform of Ailon and Chazelle being one of the most well-known techniques. The current fastest algorithm that yields the optimal $$m = {\mathcal {O}}(\varepsilon ^{-2}\lg (1/\delta ))$$ dimensions has an embedding time of $${\mathcal {O}}(n \lg n + \varepsilon ^{-2} \lg ^3 (1/\delta ))$$. An exciting approach towards improving this, due to Hinrichs and Vybíral, is to use a random $$m \times n$$ Toeplitz matrix for the embedding. Using Fast Fourier Transform, the embedding of a vector can then be computed in $${\mathcal {O}}(n \lg m)$$ time. The big question is of course whether $$m = {\mathcal {O}}(\varepsilon ^{-2} \lg (1/\delta ))$$ dimensions suffice for this technique. If so, this would end a decades long quest to obtain faster and faster Johnson–Lindenstrauss transforms. The current best analysis of the embedding of Hinrichs and Vybíral shows that $$m = {\mathcal {O}}(\varepsilon ^{-2}\lg ^2 (1/\delta ))$$ dimensions suffice. The main result of this paper, is a proof that this analysis unfortunately cannot be tightened any further, i.e., there exist vectors requiring $$m = \varOmega (\varepsilon ^{-2} \lg ^2 (1/\delta ))$$ for the Toeplitz approach to work.},
  archive      = {J_Alg},
  author       = {Freksen, Casper Benjamin and Larsen, Kasper Green},
  doi          = {10.1007/s00453-019-00644-y},
  journal      = {Algorithmica},
  number       = {2},
  pages        = {338-354},
  shortjournal = {Algorithmica},
  title        = {On using toeplitz and circulant matrices for Johnson–Lindenstrauss transforms},
  volume       = {82},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Fast compressed self-indexes with deterministic linear-time
construction. <em>Alg</em>, <em>82</em>(2), 316–337. (<a
href="https://doi.org/10.1007/s00453-019-00637-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce a compressed suffix array representation that, on a text T of length n over an alphabet of size $$\sigma $$, can be built in O(n) deterministic time, within $$O(n\log \sigma )$$ bits of working space, and counts the number of occurrences of any pattern P in T in time $$O(|P| + \log \log _w \sigma )$$ on a RAM machine of $$w=\Omega (\log n)$$-bit words. This time is almost optimal for large alphabets ($$\log \sigma =\Theta (\log n)$$), and it outperforms all the other compressed indexes that can be built in linear deterministic time, as well as some others. The only faster indexes can be built in linear time only in expectation, or require $$\Theta (n\log n)$$ bits. For smaller alphabets, where $$\log \sigma = o(\log n)$$, we show how, by using space proportional to a compressed representation of the text, we can build in linear time an index that counts in time $$O(|P|/\log _\sigma n + \log _\sigma ^\epsilon n)$$ for any constant $$\epsilon &gt;0$$. This is almost RAM-optimal in the typical case where $$w=\Theta (\log n)$$.},
  archive      = {J_Alg},
  author       = {Munro, J. Ian and Navarro, Gonzalo and Nekrich, Yakov},
  doi          = {10.1007/s00453-019-00637-x},
  journal      = {Algorithmica},
  number       = {2},
  pages        = {316-337},
  shortjournal = {Algorithmica},
  title        = {Fast compressed self-indexes with deterministic linear-time construction},
  volume       = {82},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). On the convergence time of a natural dynamics for linear
programming. <em>Alg</em>, <em>82</em>(2), 300–315. (<a
href="https://doi.org/10.1007/s00453-019-00615-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider a system of nonlinear ordinary differential equations for the solution of linear programming (LP) problems that was first proposed in the mathematical biology literature as a model for the foraging behavior of acellular slime mold Physarum polycephalum, and more recently considered as a method to solve LP instances. We study the convergence time of the continuous Physarum dynamics in the context of the linear programming problem, and derive a new time bound to approximate optimality that depends on the relative entropy between projected versions of the optimal point and of the initial point. The bound scales logarithmically with the LP cost coefficients and linearly with the inverse of the relative accuracy, establishing the efficiency of the dynamics for arbitrary LP instances with positive costs.},
  archive      = {J_Alg},
  author       = {Bonifaci, Vincenzo},
  doi          = {10.1007/s00453-019-00615-3},
  journal      = {Algorithmica},
  number       = {2},
  pages        = {300-315},
  shortjournal = {Algorithmica},
  title        = {On the convergence time of a natural dynamics for linear programming},
  volume       = {82},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). An improved algorithm for computing all the best swap edges
of a tree spanner. <em>Alg</em>, <em>82</em>(2), 279–299. (<a
href="https://doi.org/10.1007/s00453-019-00549-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A tree $$\sigma $$ -spanner of a positively real-weighted n-vertex and m-edge undirected graph G is a spanning tree T of G which approximately preserves (i.e., up to a multiplicative stretch factor $$\sigma $$ ) distances in G. Tree spanners with provably good stretch factors find applications in communication networks, distributed systems, and network design. However, finding an optimal or even a good tree spanner is a very hard computational task. Thus, if one has to face a transient edge failure in T, the overall effort that has to be afforded to rebuild a new tree spanner (i.e., computational costs, set-up of new links, updating of the routing tables, etc.) can be rather prohibitive. To circumvent this drawback, an effective alternative is that of associating with each tree edge a best possible (in terms of resulting stretch) swap edge—a well-established approach in the literature for several other tree topologies. Correspondingly, the problem of computing all the best swap edges of a tree spanner is a challenging algorithmic problem, since solving it efficiently means to exploit the structure of shortest paths not only in G, but also in all the scenarios in which an edge of T has failed. For this problem we provide a very efficient solution, running in $$O(n^2 \log ^4 n)$$ time, which drastically improves (almost by a quadratic factor in n in dense graphs) on the previous known best result.},
  archive      = {J_Alg},
  author       = {Bilò, Davide and Colella, Feliciano and Gualà, Luciano and Leucci, Stefano and Proietti, Guido},
  doi          = {10.1007/s00453-019-00549-w},
  journal      = {Algorithmica},
  number       = {2},
  pages        = {279-299},
  shortjournal = {Algorithmica},
  title        = {An improved algorithm for computing all the best swap edges of a tree spanner},
  volume       = {82},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Maximum induced matching algorithms via vertex ordering
characterizations. <em>Alg</em>, <em>82</em>(2), 260–278. (<a
href="https://doi.org/10.1007/s00453-018-00538-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study the maximum induced matching problem on a graph G. Induced matchings correspond to independent sets in $$L^2(G)$$, the square of the line graph of G. The problem is NP-complete on bipartite graphs. In this work, we show that for a number of graph families characterized by vertex orderings, almost all forbidden patterns on three vertices are preserved when taking the square of the line graph. These orderings can be computed in linear time in the size of the input graph. In particular, given $$\mathcal {G}$$ a graph class, and a graph $$G=(V,E) \in \mathcal {G}$$ with a corresponding vertex ordering $$\sigma $$ of V, one can produce (in linear time in the size of G) an ordering on the vertices of $$L^2(G)$$, that shows that $$L^2(G) \in \mathcal {G}$$ without computing the line graph or the square of the line graph of G. These results generalize and unify previous ones on showing closure under $$L^2(\cdot )$$ for various graph families. Furthermore, these orderings on $$L^2(G)$$ can be exploited algorithmically to compute a maximum induced matching on G faster. We illustrate this latter fact in the second half of the paper where we focus on cocomparability graphs, a large graph class that includes interval, permutation, trapezoid graphs, and co-graphs, and we present the first $$\mathcal {O}(mn)$$ time algorithm to compute a maximum weighted induced matching on cocomparability graphs; an improvement from the best known $$\mathcal {O}(n^4)$$ time algorithm for the unweighted case.},
  archive      = {J_Alg},
  author       = {Habib, Michel and Mouatadid, Lalla},
  doi          = {10.1007/s00453-018-00538-5},
  journal      = {Algorithmica},
  number       = {2},
  pages        = {260-278},
  shortjournal = {Algorithmica},
  title        = {Maximum induced matching algorithms via vertex ordering characterizations},
  volume       = {82},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A simple greedy algorithm for dynamic graph orientation.
<em>Alg</em>, <em>82</em>(2), 245–259. (<a
href="https://doi.org/10.1007/s00453-018-0528-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph orientations with low out-degree are one of several ways to efficiently store sparse graphs. If the graphs allow for insertion and deletion of edges, one may have to flip the orientation of some edges to prevent blowing up the maximum out-degree. We use arboricity as our sparsity measure. With an immensely simple greedy algorithm, we get parametrized trade-off bounds between out-degree and worst case number of flips, which previously only existed for amortized number of flips. We match the previous best worst-case algorithm (in $$\mathcal {O}\left( \log n\right) $$ flips) for almost all values of arboricity and beat it for either constant or super-logarithmic arboricity. We also match a previous best amortized result for at least logarithmic arboricity, and give the first results with worst-case $$\mathcal {O}\left( 1\right) $$ and $$\mathcal {O}\left( \sqrt{\log n}\right) $$ flips nearly matching out-degree bounds to their respective amortized solutions.},
  archive      = {J_Alg},
  author       = {Berglin, Edvin and Brodal, Gerth Stølting},
  doi          = {10.1007/s00453-018-0528-0},
  journal      = {Algorithmica},
  number       = {2},
  pages        = {245-259},
  shortjournal = {Algorithmica},
  title        = {A simple greedy algorithm for dynamic graph orientation},
  volume       = {82},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Study of a combinatorial game in graphs through linear
programming. <em>Alg</em>, <em>82</em>(2), 212–244. (<a
href="https://doi.org/10.1007/s00453-018-0503-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the Spy game played on a graph G, a single spy travels the vertices of G at speed s, while multiple slow guards strive to have, at all times, one of them within distance d of that spy. In order to determine the smallest number of guards necessary for this task, we analyze the game through a Linear Programming formulation and the fractional strategies it yields for the guards. We then show the equivalence of fractional and integral strategies in trees. This allows us to design a polynomial-time algorithm for computing an optimal strategy in this class of graphs. Using duality in Linear Programming, we also provide non-trivial bounds on the fractional guard-number of grids and tori, which gives a lower bound for the integral guard-number in these graphs. We believe that the approach using fractional relaxation and Linear Programming is promising to obtain new results in the field of combinatorial games.},
  archive      = {J_Alg},
  author       = {Cohen, Nathann and Mc Inerney, Fionn and Nisse, Nicolas and Pérennes, Stéphane},
  doi          = {10.1007/s00453-018-0503-9},
  journal      = {Algorithmica},
  number       = {2},
  pages        = {212-244},
  shortjournal = {Algorithmica},
  title        = {Study of a combinatorial game in graphs through linear programming},
  volume       = {82},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Envy-free matchings with lower quotas. <em>Alg</em>,
<em>82</em>(2), 188–211. (<a
href="https://doi.org/10.1007/s00453-018-0493-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {While every instance of the Hospitals/Residents problem admits a stable matching, the problem with lower quotas (HR-LQ) has instances with no stable matching. For such an instance, we expect the existence of an envy-free matching, which is a relaxation of a stable matching preserving a kind of fairness property. In this paper, we investigate the existence of an envy-free matching in several settings, in which hospitals have lower quotas and not all doctor–hospital pairs are acceptable. We first provide an algorithm that decides whether a given HR-LQ instance has an envy-free matching or not. Then, we consider envy-freeness in the Classified Stable Matching model due to Huang (in: Procedings of 21st annual ACM-SIAM symposium on discrete algorithms (SODA2010), SIAM, Philadelphia, pp 1235–1253, 2010), i.e., each hospital has lower and upper quotas on subsets of doctors. We show that, for this model, deciding the existence of an envy-free matching is NP-hard in general, but solvable in polynomial time if quotas are paramodular.},
  archive      = {J_Alg},
  author       = {Yokoi, Yu},
  doi          = {10.1007/s00453-018-0493-7},
  journal      = {Algorithmica},
  number       = {2},
  pages        = {188-211},
  shortjournal = {Algorithmica},
  title        = {Envy-free matchings with lower quotas},
  volume       = {82},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Tilt assembly: Algorithms for micro-factories that build
objects with uniform external forces. <em>Alg</em>, <em>82</em>(2),
165–187. (<a href="https://doi.org/10.1007/s00453-018-0483-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present algorithmic results for the parallel assembly of many micro-scale objects in two and three dimensions from tiny particles, which has been proposed in the context of programmable matter and self-assembly for building high-yield micro-factories. The underlying model has particles moving under the influence of uniform external forces until they hit an obstacle. Particles bond when forced together with another appropriate particle. Due to the physical and geometric constraints, not all shapes can be built in this manner; this gives rise to the Tilt Assembly Problem (TAP) of deciding constructibility. For simply-connected polyominoes P in 2D consisting of N unit-squares (“tiles”), we prove that TAP can be decided in $$O(N\log N)$$ time. For the optimization variant MaxTAP (in which the objective is to construct a subshape of maximum possible size), we show polyAPX-hardness: unless P = NP, MaxTAP cannot be approximated within a factor of $$\Omega (N^{\frac{1}{3}})$$; for tree-shaped structures, we give an $$\Omega (N^{\frac{1}{2}})$$-approximation algorithm. For the efficiency of the assembly process itself, we show that any constructible shape allows pipelined assembly, which produces copies of P in O(1) amortized time, i.e., N copies of P in O(N) time steps. These considerations can be extended to three-dimensional objects: For the class of polycubes P we prove that it is NP-hard to decide whether it is possible to construct a path between two points of P; it is also NP-hard to decide constructibility of a polycube P. Moreover, it is expAPX-hard to maximize a sequentially constructible path from a given start point.},
  archive      = {J_Alg},
  author       = {Becker, Aaron T. and Fekete, Sándor P. and Keldenich, Phillip and Krupke, Dominik and Rieck, Christian and Scheffer, Christian and Schmidt, Arne},
  doi          = {10.1007/s00453-018-0483-9},
  journal      = {Algorithmica},
  number       = {2},
  pages        = {165-187},
  shortjournal = {Algorithmica},
  title        = {Tilt assembly: Algorithms for micro-factories that build objects with uniform external forces},
  volume       = {82},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Guest editorial: Selected papers from ISAAC 2017.
<em>Alg</em>, <em>82</em>(2), 163–164. (<a
href="https://doi.org/10.1007/s00453-019-00669-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_Alg},
  author       = {Okamoto, Yoshio},
  doi          = {10.1007/s00453-019-00669-3},
  journal      = {Algorithmica},
  number       = {2},
  pages        = {163-164},
  shortjournal = {Algorithmica},
  title        = {Guest editorial: Selected papers from ISAAC 2017},
  volume       = {82},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Computing vertex-disjoint paths in large graphs using MAOs.
<em>Alg</em>, <em>82</em>(1), 146–162. (<a
href="https://doi.org/10.1007/s00453-019-00608-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider the problem of computing $$k \in {\mathbb {N}}$$ internally vertex-disjoint paths between special vertex pairs of simple connected graphs. For general vertex pairs, the best deterministic time bound is, since 42 years, $$O(\min {k,\sqrt{n}}m)$$ for each pair by using traditional flow-based methods. The restriction of our vertex pairs comes from the machinery of maximal adjacency orderings (MAOs). Henzinger showed for every MAO and every $$1 \le k \le \delta $$ (where $$\delta $$ is the minimum degree of the graph) the existence of k internally vertex-disjoint paths between every pair of the last $$\delta -k+2$$ vertices of this MAO. Later, Nagamochi generalized this result by using the machinery of mixed connectivity. Both results are however inherently non-constructive. We present the first algorithm that computes these k internally vertex-disjoint paths in linear time $$O(n+m)$$, which improves the previously best time $$O(\min {k,\sqrt{n}}m)$$. Due to the linear running time, this algorithm is suitable for large graphs. The algorithm is simple, works directly on the MAO structure, and completes a long history of purely existential proofs with a constructive method. We extend our algorithm to compute several other path systems and discuss its impact for certifying algorithms.},
  archive      = {J_Alg},
  author       = {Preißer, Johanna E. and Schmidt, Jens M.},
  doi          = {10.1007/s00453-019-00608-2},
  journal      = {Algorithmica},
  number       = {1},
  pages        = {146-162},
  shortjournal = {Algorithmica},
  title        = {Computing vertex-disjoint paths in large graphs using MAOs},
  volume       = {82},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Mim-width II. The feedback vertex set problem. <em>Alg</em>,
<em>82</em>(1), 118–145. (<a
href="https://doi.org/10.1007/s00453-019-00607-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We give a first polynomial-time algorithm for (Weighted) Feedback Vertex Set on graphs of bounded maximum induced matching width (mim-width). Explicitly, given a branch decomposition of mim-width w, we give an $$n^{\mathcal {O}(w)}$$-time algorithm that solves Feedback Vertex Set. This provides a unified polynomial-time algorithm for many well-known classes, such as Interval graphs, Permutation graphs, and Leaf power graphs (given a leaf root), and furthermore, it gives the first polynomial-time algorithms for other classes of bounded mim-width, such as Circular Permutation and Circular k-Trapezoid graphs (given a circular k-trapezoid model) for fixed k. We complement our result by showing that Feedback Vertex Set is $$\textsf {W}[1]$$-hard when parameterized by w and the hardness holds even when a linear branch decomposition of mim-width w is given.},
  archive      = {J_Alg},
  author       = {Jaffke, Lars and Kwon, O-joung and Telle, Jan Arne},
  doi          = {10.1007/s00453-019-00607-3},
  journal      = {Algorithmica},
  number       = {1},
  pages        = {118-145},
  shortjournal = {Algorithmica},
  title        = {Mim-width II. the feedback vertex set problem},
  volume       = {82},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Succinct non-overlapping indexing. <em>Alg</em>,
<em>82</em>(1), 107–117. (<a
href="https://doi.org/10.1007/s00453-019-00605-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Text indexing is a fundamental problem in computer science. The objective is to preprocess a text T, so that, given a pattern P, we can find all starting positions (or simply, occurrences) of P in $$T$$ efficiently. In some cases, additional restrictions are imposed. We consider two variants, namely the non-overlapping indexing problem, and the range non-overlapping indexing problem. Given a text $$T$$ having n characters, the non-overlapping indexing problem is defined as follows: pre-process $$T$$ into a data structure, such that for any pattern P, containing |P| characters, we can report a set containing the maximum number of non-overlapping occurrences of P in $$T$$. Cohen and Porat (in: Algorithms and computation, 20th international symposium, ISAAC 2009, Honolulu, Hawaii. Proceedings, 2009) showed that by maintaining a linear space index in which the suffix tree of $$T$$ is augmented with an O(n) word data structure, a query P can be answered in optimal time $$O(|P|+nocc)$$, where $$nocc$$ is the number of occurrences reported. We present the following new result. Let $$\mathsf {CSA}$$ (not necessarily a compressed suffix array) be an index of $$T$$ that can compute (i) the suffix range of P in $$\mathsf {search}(P)$$ time, and (ii) a suffix array or an inverse suffix array value in $$\mathsf {t}_\mathsf {SA}$$ time. By using $$\mathsf {CSA}$$ alone, we can answer a query P in $$\mathsf {search}(P)+\mathsf {sort}(nocc)+O(nocc\cdot \mathsf {t}_\mathsf {SA})$$ time. The function $$\mathsf {sort}(k)$$ denotes the time for sorting k numbers in $${1,2,\dots ,n}$$. In the range non-overlapping indexing problem, along with the pattern P, two integers a and b, $$b \ge a$$, are provided as input. The task is to report a set containing the maximum number of non-overlapping occurrences of P that lie within the range [a, b]. For any arbitrarily small positive constant $$\epsilon $$, we present an $$O(n \log ^\epsilon n)$$ word index with $$O(|P| + nocc_{a,b})$$ query time, where $$nocc_{a,b}$$ is the number of occurrences reported. Our index improves upon the result of Cohen and Porat [6].},
  archive      = {J_Alg},
  author       = {Ganguly, Arnab and Shah, Rahul and Thankachan, Sharma V.},
  doi          = {10.1007/s00453-019-00605-5},
  journal      = {Algorithmica},
  number       = {1},
  pages        = {107-117},
  shortjournal = {Algorithmica},
  title        = {Succinct non-overlapping indexing},
  volume       = {82},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Extensions of self-improving sorters. <em>Alg</em>,
<em>82</em>(1), 88–106. (<a
href="https://doi.org/10.1007/s00453-019-00604-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ailon et al. (SIAM J Comput 40(2):350–375, 2011) proposed a self-improving sorter that tunes its performance to an unknown input distribution in a training phase. The input numbers $$x_1,x_2,\ldots ,x_n$$ come from a product distribution, that is, each $$x_i$$ is drawn independently from an arbitrary distribution $${{{\mathcal {D}}}}_i$$. We study two relaxations of this requirement. The first extension models hidden classes in the input. We consider the case that numbers in the same class are governed by linear functions of the same hidden random parameter. The second extension considers a hidden mixture of product distributions.},
  archive      = {J_Alg},
  author       = {Cheng, Siu-Wing and Jin, Kai and Yan, Lie},
  doi          = {10.1007/s00453-019-00604-6},
  journal      = {Algorithmica},
  number       = {1},
  pages        = {88-106},
  shortjournal = {Algorithmica},
  title        = {Extensions of self-improving sorters},
  volume       = {82},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020a). Attenuate locally, win globally: Attenuation-based
frameworks for online stochastic matching with timeouts. <em>Alg</em>,
<em>82</em>(1), 64–87. (<a
href="https://doi.org/10.1007/s00453-019-00603-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Online matching problems have garnered significant attention in recent years due to numerous applications in e-commerce, online advertisements, ride-sharing, etc. Many of them capture the uncertainty in the real world by including stochasticity in both the arrival and matching processes. The online stochastic matching with timeouts problem introduced by Bansal et al. (Algorithmica, 2012) models matching markets (e.g., E-Bay, Amazon). Buyers arrive from an independent and identically distributed (i.i.d.) known distribution on buyer profiles and can be shown a list of items one at a time. Each buyer has some probability of purchasing each item and a limit (timeout) on the number of items they can be shown. Bansal et al. (Algorithmica, 2012) gave a 0.12-competitive algorithm which was improved by Adamczyk et al. (ESA, 2015) to 0.24. We present several online attenuation frameworks that use an algorithm for offline stochastic matching as a black box. On the upper bound side, we show that one framework, combined with a black-box adapted from Bansal et al. (Algorithmica, 2012), yields an online algorithm which nearly doubles the ratio to 0.46. Additionally, our attenuation frameworks extend to the more general setting of fractional arrival rates for online vertices. On the lower bound side, we show that no algorithm can achieve a ratio better than 0.632 using the standard LP for this problem. This framework has a high potential for further improvements since new algorithms for offline stochastic matching can directly improve the ratio for the online problem. Our online frameworks also have the potential for a variety of extensions. For example, we introduce a natural generalization: online stochastic matching with two-sided timeouts in which both online and offline vertices have timeouts. Our frameworks provide the first algorithm for this problem achieving a ratio of 0.30. We once again use the algorithm of Bansal et al. (Algorithmica, 2012) as a black-box and plug it into one of our frameworks.},
  archive      = {J_Alg},
  author       = {Brubach, Brian and Sankararaman, Karthik A. and Srinivasan, Aravind and Xu, Pan},
  doi          = {10.1007/s00453-019-00603-7},
  journal      = {Algorithmica},
  number       = {1},
  pages        = {64-87},
  shortjournal = {Algorithmica},
  title        = {Attenuate locally, win globally: Attenuation-based frameworks for online stochastic matching with timeouts},
  volume       = {82},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A polynomial sized kernel for tracking paths problem.
<em>Alg</em>, <em>82</em>(1), 41–63. (<a
href="https://doi.org/10.1007/s00453-019-00602-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Consider a secure environment (say an airport) that has a unique entry and a unique exit point with multiple inter-crossing paths between them. We want to place (minimum number of) trackers (or check points) at some specific intersections so that based on the sequence of trackers a person has encountered, we can identify the exact path traversed by the person. Motivated by such applications, we study the Tracking Paths problem in this paper. Given an undirected graph with a source s, a destination t and a non-negative integer k, the goal is to find a set of at most k vertices, a tracking set, that intersects each s–t path in a unique sequence. Such a set enables a central controller to track all the paths from s to t. We first show that the problem is NP-complete. Then we show that finding a tracking set of size at most k is fixed-parameter tractable when parameterized by the solution size. More specifically, given an undirected graph on n vertices and an integer k, we give a polynomial time algorithm that either determines that the graph cannot be tracked by k trackers or produces an equivalent instance with $$\mathcal {O}(k^7)$$ edges.},
  archive      = {J_Alg},
  author       = {Banik, Aritra and Choudhary, Pratibha and Lokshtanov, Daniel and Raman, Venkatesh and Saurabh, Saket},
  doi          = {10.1007/s00453-019-00602-8},
  journal      = {Algorithmica},
  number       = {1},
  pages        = {41-63},
  shortjournal = {Algorithmica},
  title        = {A polynomial sized kernel for tracking paths problem},
  volume       = {82},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Connected vertex cover for <span
class="math display">(<em>s</em><em>P</em><sub>1</sub> + <em>P</em><sub>5</sub>)</span>-free
graphs. <em>Alg</em>, <em>82</em>(1), 20–40. (<a
href="https://doi.org/10.1007/s00453-019-00601-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Connected Vertex Cover problem is to decide if a graph G has a vertex cover of size at most k that induces a connected subgraph of G. This is a well-studied problem, known to be NP-complete for restricted graph classes, and, in particular, for H-free graphs if H is not a linear forest. On the other hand, the problem is known to be polynomial-time solvable for $$sP_2$$-free graphs for any integer $$s\ge 1$$. We give a polynomial-time algorithm to solve the problem for $$(sP_1+P_5)$$-free graphs for every integer $$s\ge ~0$$. Our algorithm can also be used for the Weighted Connected Vertex Cover problem.},
  archive      = {J_Alg},
  author       = {Johnson, Matthew and Paesani, Giacomo and Paulusma, Daniël},
  doi          = {10.1007/s00453-019-00601-9},
  journal      = {Algorithmica},
  number       = {1},
  pages        = {20-40},
  shortjournal = {Algorithmica},
  title        = {Connected vertex cover for $$(sP_1+P_5)$$-free graphs},
  volume       = {82},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Parameterized complexity of geometric covering problems
having conflicts. <em>Alg</em>, <em>82</em>(1), 1–19. (<a
href="https://doi.org/10.1007/s00453-019-00600-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The input for the Geometric Coverage problem consists of a pair $$\varSigma =(P,\mathcal {R})$$, where P is a set of points in $${\mathbb {R}}^d$$ and $$\mathcal {R}$$ is a set of subsets of P defined by the intersection of P with some geometric objects in $${\mathbb {R}}^d$$. Motivated by what are called choice problems in geometry, we consider a variation of the Geometric Coverage problem where there are conflicts on the covering objects that precludes some objects from being part of the solution if some others are in the solution. As our first contribution, we propose two natural models in which the conflict relations are given: (a) by a graph on the covering objects, and (b) by a representable matroid on the covering objects. Our main result is that as long as the conflict graph has bounded arboricity there is a parameterized reduction to the conflict-free version. As a consequence, we have the following results when the conflict graph has bounded arboricity. (1) If the Geometric Coverage problem is fixed parameter tractable (FPT), then so is the conflict free version. (2) If the Geometric Coverage problem admits a factor $$\alpha $$-approximation, then the conflict free version admits a factor $$\alpha $$-approximation algorithm running in FPT time. As a corollary to our main result we get a plethora of approximation algorithms that run in FPT time. Our other results include an FPT algorithm and a hardness result for conflict-free version of Covering Points by Intervals. The FPT algorithm is for the case when the conflicts are given by a representable matroid. We prove that conflict-free version of Covering Points by Intervals does not admit an FPT algorithm, unless FPT =W[1], for the family of conflict graphs for which the Independent Set problem is W[1]-hard.},
  archive      = {J_Alg},
  author       = {Banik, Aritra and Panolan, Fahad and Raman, Venkatesh and Sahlot, Vibha and Saurabh, Saket},
  doi          = {10.1007/s00453-019-00600-w},
  journal      = {Algorithmica},
  number       = {1},
  pages        = {1-19},
  shortjournal = {Algorithmica},
  title        = {Parameterized complexity of geometric covering problems having conflicts},
  volume       = {82},
  year         = {2020},
}
</textarea>
</details></li>
</ul>

</body>
</html>
