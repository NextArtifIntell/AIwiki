<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>JRTIP_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="jrtip---148">JRTIP - 148</h2>
<ul>
<li><details>
<summary>
(2020). Real-time adversarial GAN-based abnormal crowd behavior
detection. <em>JRTIP</em>, <em>17</em>(6), 2153–2162. (<a
href="https://doi.org/10.1007/s11554-020-01029-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Detecting abnormal events in the crowd is a challenging problem. Insufficient samples make those traditional model-based methods cannot cope with sophisticated anomaly monitoring. Therefore, we design a real-time generative adversarial network plus an add-on encoder to deal with the continually changing environment. After the generator reconstructs the compressed pattern to generate the design to the latent vector, a discriminator is used to construct better videos by minimizing the adversarial loss function. We calculated the abnormal score by the distance between the two underlying patterns encoded by the first and the second encoders. The unusual event is detected when the anomaly score is above the threshold. To accelerate the processing efficiency, we introduced the grouped pointwise convolution method to decrease the computing complexity. The frame-level and video-level experiments on the benchmark dataset show the accuracy and reliance of our approach. The acceleration approach can increase the efficiency of the network with only limited accuracy loss.},
  archive      = {J_JRTIP},
  author       = {Han, Qiulei and Wang, Haofeng and Yang, Lin and Wu, Min and Kou, Jinqiao and Du, Qinsheng and Li, Nianfeng},
  doi          = {10.1007/s11554-020-01029-z},
  journal      = {Journal of Real-Time Image Processing},
  month        = {12},
  number       = {6},
  pages        = {2153-2162},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Real-time adversarial GAN-based abnormal crowd behavior detection},
  volume       = {17},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Efficient image encryption scheme based on generalized
logistic map for real time image processing. <em>JRTIP</em>,
<em>17</em>(6), 2139–2151. (<a
href="https://doi.org/10.1007/s11554-020-01008-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this era of the information age with digitalization, the transmission of sensitive real-time image information over insecure channels is highly-likely to be accessed or even attacked by an adversary. To prevent such unauthorized access, cryptography is being used to convert sensitive information in real-time images into unintelligible data. Most of the time, schemes are proposed with a high level of security. However, the challenge always remains the slower speeds due to their high complexity which makes them unusable in the applications of real-time images. In this paper, an efficient image encryption algorithm has been developed and tested for real-time images. The proposed scheme makes use of encryption with an efficient permutation technique based on a modular logistic map to bring down the size of the chaotic value vector, required to permute real-time image. We show that an efficient permutation is obtained using only $$\sqrt{N}$$ chaotic numbers for a square image with 3N pixels (N Pixels in each color bit plane). The algorithm makes use of a 192-bit key; divided into smaller blocks and each block selected chaotically to diffuse the pixel using multiple XOR operations. The experimental analysis reveals that the proposed algorithm is immune to various statistical and differential attacks such as entropy, histogram analysis, spectral characteristic analysis, etc. A comparison of the proposed scheme with some state-of-the-art techniques show that it performs better, and as such, can be utilized for efficient real-time image encryption.},
  archive      = {J_JRTIP},
  author       = {Shah, Asif A. and Parah, Shabir A. and Rashid, Mamoon and Elhoseny, Mohamed},
  doi          = {10.1007/s11554-020-01008-4},
  journal      = {Journal of Real-Time Image Processing},
  month        = {12},
  number       = {6},
  pages        = {2139-2151},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Efficient image encryption scheme based on generalized logistic map for real time image processing},
  volume       = {17},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). ROPDet: Real-time anchor-free detector based on point set
representation for rotating object. <em>JRTIP</em>, <em>17</em>(6),
2127–2138. (<a
href="https://doi.org/10.1007/s11554-020-01013-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Remote-sensing object detection is a challenging task due to the difficulties of separating the objects with arbitrary direction from complex backgrounds. Though substantial progress has been made, there still exist challenges for object detection under the scenario of small scale, large aspect ratio, and dense distribution. Besides, the current mainstream approach falls under anchor-based multi-stage method, which has a serious shortcoming of slower inference speed. To conquer the aforementioned issues, this paper used RoPoints (points in rotation objects), a new better representation of objects as a set of sample points to perform object localization and classification. Then, we propose an anchor-free refined rotation detector:ROPDet based on RoPoints for more accurate and faster object detection. In our method, there is no need to predefine a large number anchors with different shapes. We only need to learn RoPoints for each object followed by converting to the corresponding bounding box, which greatly accelerates the inference process. Extensive experiments on two public remote-sensing datasets DOTA and HRSC-2016 demonstrate the competitive ability in terms of accuracy and inference speed.},
  archive      = {J_JRTIP},
  author       = {Yang, Zhixiang and He, Kunkun and Zou, Fuhao and Cao, Wanhua and Jia, Xiaoyun and Li, Kai and Jiang, Chuntao},
  doi          = {10.1007/s11554-020-01013-7},
  journal      = {Journal of Real-Time Image Processing},
  month        = {12},
  number       = {6},
  pages        = {2127-2138},
  shortjournal = {J. Real-Time Image Process.},
  title        = {ROPDet: Real-time anchor-free detector based on point set representation for rotating object},
  volume       = {17},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). An efficient and adaptable multimedia system for converting
PAL to VGA in real-time video processing. <em>JRTIP</em>,
<em>17</em>(6), 2113–2125. (<a
href="https://doi.org/10.1007/s11554-019-00889-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Real-time video processing has found its range of applications from defense to consumer electronics for surveillance, video conferencing, etc. With the advent of Field Programmable Gate Arrays (FPGAs), flexible real-time video processing systems which can meet hard real-time constraints are easily realized with short development time. Most of the existing solutions have high utilization of system resources and are not quite flexible with many applications. Here we propose a hardware–software co-design for an FPGA-based real-time video processing system to convert video in standard Phase Alternating Line (PAL) 576i format to standard video of Video Graphics Array (VGA)/Super Video Graphics Array (SVGA) format with little utilization of resources. Switching between multiple video streams, character/text overlaying, and skin color detection are also incorporated with the system. The system is also adaptable for rugged applications. VHSIC Hardware Description Language (VHDL) codes for the architecture were synthesized using Altera Quartus II and targeted for Altera Stratix I FPGA. Results achieved confirm that the proposed system performs efficient conversion with very less resource utilization compared to the existing solutions. Since the proposed system is also flexible, many other applications can be incorporated in the future.},
  archive      = {J_JRTIP},
  author       = {Jain, Deepak Kumar and Jacob, Sunil and Alzubi, Jafar and Menon, Varun},
  doi          = {10.1007/s11554-019-00889-4},
  journal      = {Journal of Real-Time Image Processing},
  month        = {12},
  number       = {6},
  pages        = {2113-2125},
  shortjournal = {J. Real-Time Image Process.},
  title        = {An efficient and adaptable multimedia system for converting PAL to VGA in real-time video processing},
  volume       = {17},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Deep learning model for real-time image compression in
internet of underwater things (IoUT). <em>JRTIP</em>, <em>17</em>(6),
2097–2111. (<a
href="https://doi.org/10.1007/s11554-019-00879-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, the advancements of Internet-of-Things (IoT) have expanded its application in underwater environment which leads to the development of a new field of Internet of Underwater Things (IoUT). It offers a broader view of applications such as atmosphere observation, habitat monitoring of sea animals, defense and disaster prediction. Data transmission of images captured by the smart underwater objects is very challenging due to the nature of underwater environment and necessitates an efficient image transmission strategy for IoUT. In this paper, we model and implement a discrete wavelet transform (DWT) based deep learning model for image compression in IoUT. For achieving effective compression with better reconstruction image quality, convolution neural network (CNN) is used at the encoding as well as decoding side. We validate DWT–CNN model using extensive set of experimentations and depict that the presented deep learning model is superior to existing methods such as super-resolution convolutional neural networks (SRCNN), JPEG and JPEG2000 in terms of compression performance as well as reconstructed image quality. The DWT–CNN model attains an average peak signal-to-noise ratio (PSNR) of 53.961 with average space saving (SS) of 79.7038%.},
  archive      = {J_JRTIP},
  author       = {Krishnaraj, N. and Elhoseny, Mohamed and Thenmozhi, M. and Selim, Mahmoud M. and Shankar, K.},
  doi          = {10.1007/s11554-019-00879-6},
  journal      = {Journal of Real-Time Image Processing},
  month        = {12},
  number       = {6},
  pages        = {2097-2111},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Deep learning model for real-time image compression in internet of underwater things (IoUT)},
  volume       = {17},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Real-time watermark reconstruction for the identification of
source information based on deep neural network. <em>JRTIP</em>,
<em>17</em>(6), 2077–2095. (<a
href="https://doi.org/10.1007/s11554-019-00937-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A novel deep neural network-based image watermarking method is presented to identify the source of digital data that is shared/forwarded on the internet using various messenger apps. The app that is used to share/communicate the image at the very first time is also identified in the proposed method. The ten-digit mobile number of the source (user) and identification data of particular messenger app (i.e. WhatsApp, Snapchat, Kik, Facebook messenger, etc.) is combined to get the text watermark signal. The part of the watermark signal representing specific mobile-based messenger application is obtained by randomizing the Walsh orthogonal codes using secret keys. To embed the watermark, the host image (shared/forwarded) is divided into blocks of equal size and then, slantlet transform is applied on each block. To get high reliability, three copies of the source information (user and app) are embedded during watermark embedding. Watermark extraction is performed using trained multilayer deep neural network. Furthermore, an optimal block selection logic is used to get improved results for real-time applications. The method is examined against various signal-processing attacks and high robustness with significant imperceptibility is attained. The method is also found to be fast enough for real-time applications. The prime objective of identifying the first user (source) and the shared/forwarded status (app detection) is successfully accomplished.},
  archive      = {J_JRTIP},
  author       = {Sinhal, Rishi and Ansari, Irshad Ahmad and Jain, Deepak Kumar},
  doi          = {10.1007/s11554-019-00937-z},
  journal      = {Journal of Real-Time Image Processing},
  month        = {12},
  number       = {6},
  pages        = {2077-2095},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Real-time watermark reconstruction for the identification of source information based on deep neural network},
  volume       = {17},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Low-complexity CNN with 1D and 2D filters for
super-resolution. <em>JRTIP</em>, <em>17</em>(6), 2065–2076. (<a
href="https://doi.org/10.1007/s11554-020-01019-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes a low-complexity convolutional neural network (CNN) for super-resolution (SR). The proposed deep-learning model for SR has two layers to deal with horizontal, vertical, and diagonal visual information. The front-end layer extracts the horizontal and vertical high-frequency signals using a CNN with one-dimensional (1D) filters. In the high-resolution image-restoration layer, the high-frequency signals in the diagonal directions are processed by additional two-dimensional (2D) filters. The proposed model consists of 1D and 2D filters, and as a result, we can reduce the computational complexity of the existing SR algorithms, with negligible visual loss. The computational complexity of the proposed algorithm is 71.37%, 61.82%, and 50.78% lower in CPU, TPU, and GPU than the very-deep SR (VDSR) algorithm, with a peak signal-to-noise ratio loss of 0.49 dB.},
  archive      = {J_JRTIP},
  author       = {Park, Jangsoo and Lee, Jongseok and Sim, Donggyu},
  doi          = {10.1007/s11554-020-01019-1},
  journal      = {Journal of Real-Time Image Processing},
  month        = {12},
  number       = {6},
  pages        = {2065-2076},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Low-complexity CNN with 1D and 2D filters for super-resolution},
  volume       = {17},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Low delay error resilience algorithm for h.265|HEVC video
transmission. <em>JRTIP</em>, <em>17</em>(6), 2047–2063. (<a
href="https://doi.org/10.1007/s11554-019-00923-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Transmission of high-resolution compressed video on unreliable transmission channels with time-varying characteristics such as wireless channels can adversely affect the decoded visual quality at the decoder side. This task becomes more challenging when the video codec computational complexity is an essential factor for low delay video transmission. High-efficiency video coding (H.265|HEVC) standard is the most recent video coding standard produced by ITU-T and ISO/IEC organisations. In this paper, a robust error resilience algorithm is proposed to reduce the impact of erroneous H.265|HEVC bitstream on the perceptual video quality at the decoder side. The proposed work takes into consideration the compatibility of the algorithm implementations with and without feedback channel update. The proposed work identifies and locates the frame’s most sensitive areas to errors and encodes them in intra mode. The intra-refresh map is generated at the encoder by utilising a grey projection method. The conducted experimental work includes testing the codec performance with the proposed work in error-free and error-prone conditions. The simulation results demonstrate that the proposed algorithm works effectively at high packet loss rates. These results come at the cost of a slight increase in the encoding bit rate overhead and computational processing time compared with the default HEVC HM16 reference software.},
  archive      = {J_JRTIP},
  author       = {Alfaqheri, Taha T. and Sadka, Abdul Hamid},
  doi          = {10.1007/s11554-019-00923-5},
  journal      = {Journal of Real-Time Image Processing},
  month        = {12},
  number       = {6},
  pages        = {2047-2063},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Low delay error resilience algorithm for H.265|HEVC video transmission},
  volume       = {17},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Real-time UHD video super-resolution and transcoding on
heterogeneous hardware. <em>JRTIP</em>, <em>17</em>(6), 2029–2045. (<a
href="https://doi.org/10.1007/s11554-019-00913-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Videos have become the major type of data produced and consumed every day. With screens grow larger, ultra high definition (UHD) videos are becoming more popular since they provide better visual experience. However, video contents with UHD resolution are still scarce. High-performance video super-resolution (SR) techniques that can obtain high resolution (HR) videos from low resolution (LR) sources are recently used in UHD video production. Deep learning (DL)-based SR methods can provide HR videos with appreciable objective and subjective qualities, while their massive computational complexity makes the processing speed far slower than real-time even on GPU servers when producing UHD videos. Moreover, transcoding and other video processing algorithms executed during the enhancement are also time and resource consuming, which performs relatively slow on ordinary CPU and GPU servers. Nowadays, hardware including GPU, field-programmable gate array (FPGA) and application specific integrated circuit (ASIC) are proved to have outstanding capability on image and video processing tasks in different aspects, and there are also dedicated hardware accelerators meant for specific video processing tasks. In this paper, we focus on accelerating a UHD video enhancement workflow on heterogeneous system with multiple hardware accelerators. First, we optimize the most time consuming task, video SR, with CUDNN and CUDA libraries to achieve real-time processing speed for a single UHD output frame on an ordinary GPU. Second, we design a GPU-friendly multi-thread scheduling algorithm for data and computation to better utilize GPU resources and achieve real-time performance on outputting UHD video clips. Third, targeting on production environment, we build a UHD video enhancement application on selected heterogeneous hardware, with an integrated command line tool of our proposed algorithm, and achieve 60 fps real-time end to end processing speed. Experiments show high efficiency, robustness and compatibility of our approach.},
  archive      = {J_JRTIP},
  author       = {Dong, Yu and Song, Li and Xie, Rong and Zhang, Wenjun},
  doi          = {10.1007/s11554-019-00913-7},
  journal      = {Journal of Real-Time Image Processing},
  month        = {12},
  number       = {6},
  pages        = {2029-2045},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Real-time UHD video super-resolution and transcoding on heterogeneous hardware},
  volume       = {17},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Low-delay single holoscopic 3D computer-generated image to
multiview images. <em>JRTIP</em>, <em>17</em>(6), 2015–2027. (<a
href="https://doi.org/10.1007/s11554-020-00991-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to the nature of holoscopic 3D (H3D) imaging technology, H3D cameras can capture more angular information than their conventional 2D counterparts. This is mainly attributed to the macrolens array which captures the 3D scene with slightly different viewing angles and generates holoscopic elemental images based on fly’s eyes imaging concept. However, this advantage comes at the cost of decreasing the spatial resolution in the reconstructed images. On the other hand, the consumer market is looking to find an efficient multiview capturing solution for the commercially available autostereoscopic displays. The autostereoscopic display provides multiple viewers with the ability to simultaneously enjoy a 3D viewing experience without the need for wearing 3D display glasses. This paper proposes a low-delay content adaptation framework for converting a single holoscopic 3D computer-generated image into multiple viewpoint images. Furthermore, it investigates the effects of varying interpolation step sizes on the converted multiview images using the nearest neighbour and bicubic sampling interpolation techniques. In addition, it evaluates the effects of changing the macrolens array size, using the proposed framework, on the perceived visual quality both objectively and subjectively. The experimental work is conducted on computer-generated H3D images with different macrolens sizes. The experimental results show that the proposed content adaptation framework can be used to capture multiple viewpoint images to be visualised on autostereoscopic displays.},
  archive      = {J_JRTIP},
  author       = {Alfaqheri, Taha and Aondoakaa, Akuha Solomon and Swash, Mohammad Rafiq and Sadka, Abdul Hamid},
  doi          = {10.1007/s11554-020-00991-y},
  journal      = {Journal of Real-Time Image Processing},
  month        = {12},
  number       = {6},
  pages        = {2015-2027},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Low-delay single holoscopic 3D computer-generated image to multiview images},
  volume       = {17},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Toward a robust and fast real-time point cloud registration
with factor analysis and student’s-t mixture model. <em>JRTIP</em>,
<em>17</em>(6), 2005–2014. (<a
href="https://doi.org/10.1007/s11554-020-00964-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Three-dimensional (3D) point cloud registration generally involves in unsatisfied situations like Gaussian white noise, data missing and disorder in affine. This paper proposes a robust and real-time point cloud registration, which combines the Student’s-t mixture model (SMM) with factor analysis. The proposed method extending the point cloud mathematical model to the orthogonal factor model and employs the SMM to fit the point cloud data, because the degree of freedom of Student’s t-distribution makes it more flexible in fitting the probability distribution of data. Since the Expectation Maximization (EM) algorithm has a stable estimation ability for the mixture model, the EM algorithm is used to estimate the factor load matrix. The filed data and experimental results show that the proposed algorithm can achieve accurate registration and fast convergence even in the case of point cloud disorder, data occlusion, incomplete loss and noise.},
  archive      = {J_JRTIP},
  author       = {Tang, Zhirong and Liu, Mingzhe and Zhao, Feixiang and Li, Shaoda and Zong, Ming},
  doi          = {10.1007/s11554-020-00964-1},
  journal      = {Journal of Real-Time Image Processing},
  month        = {12},
  number       = {6},
  pages        = {2005-2014},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Toward a robust and fast real-time point cloud registration with factor analysis and student’s-t mixture model},
  volume       = {17},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Dual-mode power reduction technique for real-time image and
video processing board. <em>JRTIP</em>, <em>17</em>(6), 1991–2004. (<a
href="https://doi.org/10.1007/s11554-020-00992-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In real-time image and video processing boards, power, speed, and area are the most often used measures for determining the performance of motion imagery applications. Due to technological advancement, power consumption has gained major attention in real-time image processing ability compared to speed. The increase in on-chip temperature due to larger power consumption has resulted in reduced operating life of chip and battery-driven devices. In this work, a new logic family has been introduced i.e., dual-mode logic (DML), which provides flexibility between the optimization of energy and delay (E-D optimization). This gate can be switched between two modes of operation that is a static mode (CMOS-like mode), which provides low power consumption and dynamic mode, which provides high speed. Recently, power leakage has become a dominant problem due to continuous data transfer among a large number of connected devices. Thus, to reduce power leakage, a self-controllable voltage level (SVL) power reduction technique is used along with DML logic. In the SVL technique, a maximum dc voltage is provided to the active load circuit on-demand or decrease the dc supplied to the load circuit in the standby mode. Integrating DML with the SVL technique reduces power consumption as well as leakage power. A 4-bit RCA, 8-bit RCA, and 16-bit RCA are used for verifying the proposed method and comparison of performance parameters is done with a conventional circuit. Complete circuit implementation and simulation are carried out in TANNER EDA version 13 tools with operating voltage of 1 V. The proposed system is further applied to real-time image, and we obtain the finest resolution level with minimum power consumption.},
  archive      = {J_JRTIP},
  author       = {Jacob, Sunil and Menon, Varun G. and Joseph, Saira and Sehdev, Paramjit},
  doi          = {10.1007/s11554-020-00992-x},
  journal      = {Journal of Real-Time Image Processing},
  month        = {12},
  number       = {6},
  pages        = {1991-2004},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Dual-mode power reduction technique for real-time image and video processing board},
  volume       = {17},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Joint multi-task cascade for instance segmentation.
<em>JRTIP</em>, <em>17</em>(6), 1983–1989. (<a
href="https://doi.org/10.1007/s11554-020-01007-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Instance segmentation requires both pixel-level classification accuracy and high-level semantic features at the target instance level, which is very challenging, and the cascade structure can effectively improve both of these problems. To make full use of the relationship between detection and segmentation, this paper proposes a joint multi-tasking cascade structure, which is not simply to cascade the two tasks of detection and segmentation, but to unitedly put them into multi-stage processing, and especially to integrate the information at different stages of the mask branch. The entire structure can effectively utilize the superior characteristics of each stage in the matter of detection and segmentation, thus improving the quality of mask prediction. The feature fusion process is introduced in the full convolution networks (FCN) branch, and the high-level and low-level features are effectively fused to enhance the contextual information of the picture semantic features. The experiments demonstrate the better results on the COCO dataset.},
  archive      = {J_JRTIP},
  author       = {Wen, Yaole and Hu, Fuyuan and Ren, Jinchang and Shang, Xinru and Li, Linyan and Xi, Xuefeng},
  doi          = {10.1007/s11554-020-01007-5},
  journal      = {Journal of Real-Time Image Processing},
  month        = {12},
  number       = {6},
  pages        = {1983-1989},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Joint multi-task cascade for instance segmentation},
  volume       = {17},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Fast intra-coding unit partition decision in h.266/FVC based
on deep learning. <em>JRTIP</em>, <em>17</em>(6), 1971–1981. (<a
href="https://doi.org/10.1007/s11554-020-00998-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the recent Future Video Coding (FVC) standard developed by the Joint Video Exploration Team (JVET), the quad-tree binary-tree (QTBT) block partition module makes use of rectangular block forms and additional square block sizes compared to quad-tree (QT) block partitioning module proposed in the predecessor High-Efficiency Video Coding (HEVC) standard. This block flexibility, induced with the QTBT module, significantly improves compression performance while it dramatically increases coding complexity due to the brute force search for Rate Distortion Optimization (RDO). To cope with this issue, it is necessary to consider the unique characteristics of QTBT in FVC. In this paper, we propose a fast QT partitioning algorithm based on a deep convolutional neural network (CNN) model to predict coding unit (CU) partition instead of RDO which enhances considerably QTBT performance for intra-mode coding. Based on a suitable diversified CU partition patterns database, the optimization process is set up with three levels CNN structure developed to learn the split or non-split decision from the established database. Experimental results reveal that the proposed algorithm can accelerate the QTBT block partition structure by reducing the intra-mode encoding time by an average of 35% with a bit rate increase of 1.7%, allowing its application in practical scenarios.},
  archive      = {J_JRTIP},
  author       = {Amna, Maraoui and Imen, Werda and Ezahra, Sayadi Fatma and Mohamed, Atri},
  doi          = {10.1007/s11554-020-00998-5},
  journal      = {Journal of Real-Time Image Processing},
  month        = {12},
  number       = {6},
  pages        = {1971-1981},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Fast intra-coding unit partition decision in H.266/FVC based on deep learning},
  volume       = {17},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Optimized highway deep learning network for fast single
image super-resolution reconstruction. <em>JRTIP</em>, <em>17</em>(6),
1961–1970. (<a
href="https://doi.org/10.1007/s11554-020-00973-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the success of the deep residual network for image recognition tasks, the residual connection or skip connection has been widely used in deep learning models for various vision tasks, including single image super-resolution (SISR). Most existing SISR approaches pay particular attention to residual learning, while few studies investigate highway connection for SISR. Although skip connection can help to alleviate the vanishing gradient problem and enable fast training of the deep network, it still provides the coarse level of approximation in both forward and backward propagation paths and thus challenging to recover high-frequency details. To address this issue, we propose a novel model for SISR by using highway connection (HNSR), which composes of a nonlinear gating mechanism to further regulate the information. By using the global residual learning and replacing all local residual learning with designed gate unit in highway connection, HNSR has the capability of efficiently learning different hierarchical features and recovering much more details in image reconstruction. The experimental results have validated that HNSR can provide not only improved quality but also less prone to a few common problems during training. Besides, the more robust and efficient model is suitable for implementation in real-time and mobile systems.},
  archive      = {J_JRTIP},
  author       = {Ha, Viet Khanh and Ren, Jinchang and Xu, Xinying and Liao, Wenzhi and Zhao, Sophia and Ren, Jie and Yan, Gaowei},
  doi          = {10.1007/s11554-020-00973-0},
  journal      = {Journal of Real-Time Image Processing},
  month        = {12},
  number       = {6},
  pages        = {1961-1970},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Optimized highway deep learning network for fast single image super-resolution reconstruction},
  volume       = {17},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Effective and efficient multitask learning for brain tumor
segmentation. <em>JRTIP</em>, <em>17</em>(6), 1951–1960. (<a
href="https://doi.org/10.1007/s11554-020-00961-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, brain tumor segmentation has achieved great success, partially because of deep learning-based relation exploration and multiscale analysis. However, the computational complexity hinders the real-time application. In this paper, we propose a revised multitask learning approach in which a lightweight network with only two scales is adopted to segment different kinds of tumor regions. Moreover, we design a hybrid hard sampling method that considers both sample sparsity and effectiveness. Extensive experiments on the BraTS19 segmentation challenge dataset have shown that our proposed method improves the Dice coefficient by a margin of 0.4–1.0 for different kinds of brain tumor regions and obtains results that are competitive with state-of-the-art brain tumor segmentation approaches.},
  archive      = {J_JRTIP},
  author       = {Cheng, Guohua and Cheng, Jingliang and Luo, Mengyan and He, Linyang and Tian, Yan and Wang, Ruili},
  doi          = {10.1007/s11554-020-00961-4},
  journal      = {Journal of Real-Time Image Processing},
  month        = {12},
  number       = {6},
  pages        = {1951-1960},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Effective and efficient multitask learning for brain tumor segmentation},
  volume       = {17},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A novel real-time fall detection method based on head
segmentation and convolutional neural network. <em>JRTIP</em>,
<em>17</em>(6), 1939–1949. (<a
href="https://doi.org/10.1007/s11554-020-00982-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As the computer vision develops, real-time fall detection based on computer vision has become increasingly popular in recent years. In this paper, a novel real-time indoor fall detection method based on computer vision by using geometric features and convolutional neural network (CNN) is proposed. Gaussian mixture model (GMM) is applied to detect the human target and find out the minimum external elliptical contour. Differently from the traditional fall detection method based on geometric features, we consider the importance of the head in fall detection and propose to use two different ellipses to represent the head and the torso, respectively. Three features including the long and short axis ratio, the orientation angle and the vertical velocity are extracted from the two different ellipses in each frame, respectively, and fused into a motion feature based on time series. In addition, a shallow CNN is applied to find out the correlation between the two elliptic contour features for detecting indoor falls and distinguishing some similar activities. Our novel method can effectively distinguish some similar activities in real time, which cannot be distinguished by some traditional methods based on geometric features, and has a better detection rate.},
  archive      = {J_JRTIP},
  author       = {Yao, Chenguang and Hu, Jun and Min, Weidong and Deng, Zhifeng and Zou, Song and Min, Weiqiong},
  doi          = {10.1007/s11554-020-00982-z},
  journal      = {Journal of Real-Time Image Processing},
  month        = {12},
  number       = {6},
  pages        = {1939-1949},
  shortjournal = {J. Real-Time Image Process.},
  title        = {A novel real-time fall detection method based on head segmentation and convolutional neural network},
  volume       = {17},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A deep attention-based ensemble network for real-time face
hallucination. <em>JRTIP</em>, <em>17</em>(6), 1927–1937. (<a
href="https://doi.org/10.1007/s11554-020-01009-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Face hallucination (FH) aims to reconstruct high-resolution faces from low-resolution face inputs, making it significant to other face-related tasks. Different from general super resolution issue, it often requires facial priors other than general extracted features thus leading to fusion of more than one kind of feature. The existing CNN-based FH methods often fuse different features indiscriminately which may introduce noises. Also the latent relations among different features which may be useful are taken into less consideration. To address the above issues, we propose an end-to-end deep ensemble network which aggregates three extraction sub-nets in attention-based manner. In our ensemble strategy, both relations among different features and inter-dependencies among different channels are dug out through the exploitation of spatial attention and channel attention. And for the diversity of extracted features, we aggregate three different sub-nets, which are the basic sub-net for basic features, the auto-encoder sub-net for facial shape priors and the dense residual attention sub-net for fine-grained texture features. Conducted ablation studies and experimental results show that our method achieves effectiveness not only in PSNR (Peak Signal to Noise Ratio) and SSIM (Structural Similarity Index) metrics but more importantly in clearer details within both key facial areas and whole range. Also results show that our method achieves real-time hallucinating faces by generating one image in 0.0237s.},
  archive      = {J_JRTIP},
  author       = {Liu, Dongdong and Chen, Jincai and Huang, Zhenxing and Zeng, Ni and Lu, Ping and Yang, Lin and Wang, Haofeng and Kou, Jinqiao and Wu, Min},
  doi          = {10.1007/s11554-020-01009-3},
  journal      = {Journal of Real-Time Image Processing},
  month        = {12},
  number       = {6},
  pages        = {1927-1937},
  shortjournal = {J. Real-Time Image Process.},
  title        = {A deep attention-based ensemble network for real-time face hallucination},
  volume       = {17},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Investigating low-delay deep learning-based cultural image
reconstruction. <em>JRTIP</em>, <em>17</em>(6), 1911–1926. (<a
href="https://doi.org/10.1007/s11554-020-00975-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Numerous cultural assets host a great historical and moral value, but due to their degradation, this value is heavily affected as their attractiveness is lost. One of the solutions that most heritage organizations and museums currently choose is to leverage the knowledge of art and history experts in addition to curators to recover and restore the damaged assets. This process is labor-intensive, expensive and more often results in just an assumption over the damaged or missing region. In this work, we tackle the issue of completing missing regions in artwork through advanced deep learning and image reconstruction (inpainting) techniques. Following our analysis of different image completion and reconstruction approaches, we noticed that these methods suffer from various limitations such as lengthy processing times and hard generalization when trained with multiple visual contexts. Most of the existing learning-based image completion and reconstruction techniques are trained on large datasets with the objective of retrieving the original data distribution of the training samples. However, this distribution becomes more complex when the training data is diverse making the training process difficult and the reconstruction inefficient. Through this paper, we present a clustering-based low-delay image completion and reconstruction approach which combines supervised and unsupervised learning to address the highlighted issues. We compare our technique to the current state of the art using a real-world dataset of artwork collected from various cultural institutions. Our approach is evaluated using statistical methods and a surveyed audience to better interpret our results objectively and subjectively.},
  archive      = {J_JRTIP},
  author       = {Belhi, Abdelhak and Al-Ali, Abdulaziz Khalid and Bouras, Abdelaziz and Foufou, Sebti and Yu, Xi and Zhang, Haiqing},
  doi          = {10.1007/s11554-020-00975-y},
  journal      = {Journal of Real-Time Image Processing},
  month        = {12},
  number       = {6},
  pages        = {1911-1926},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Investigating low-delay deep learning-based cultural image reconstruction},
  volume       = {17},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Deep learning methods in real-time image super-resolution: A
survey. <em>JRTIP</em>, <em>17</em>(6), 1885–1909. (<a
href="https://doi.org/10.1007/s11554-019-00925-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Super-resolution is generally defined as a process to obtain high-resolution images form inputs of low-resolution observations, which has attracted quantity of attention from researchers of image-processing community. In this paper, we aim to analyze, compare, and contrast technical problems, methods, and the performance of super-resolution research, especially real-time super-resolution methods based on deep learning structures. Specifically, we first summarize fundamental problems, perform algorithm categorization, and analyze possible application scenarios that should be considered. Since increasing attention has been drawn in utilizing convolutional neural networks (CNN) or generative adversarial networks (GAN) to predict high-frequency details lost in low- resolution images, we provide a general overview on background technologies and pay special attention to super-resolution methods built on deep learning architectures for real-time super-resolution, which not only produce desirable reconstruction results, but also enlarge possible application scenarios of super resolution to systems like cell phones, drones, and embedding systems. Afterwards, benchmark datasets with descriptions are enumerated, and performance of most representative super-resolution approaches is provided to offer a fair and comparative view on performance of current approaches. Finally, we conclude the paper and suggest ways to improve usage of deep learning methods on real-time image super-resolution.},
  archive      = {J_JRTIP},
  author       = {Li, Xiaofang and Wu, Yirui and Zhang, Wen and Wang, Ruichao and Hou, Feng},
  doi          = {10.1007/s11554-019-00925-3},
  journal      = {Journal of Real-Time Image Processing},
  month        = {12},
  number       = {6},
  pages        = {1885-1909},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Deep learning methods in real-time image super-resolution: A survey},
  volume       = {17},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Advances in deep learning for real-time image and video
reconstruction and processing. <em>JRTIP</em>, <em>17</em>(6),
1883–1884. (<a
href="https://doi.org/10.1007/s11554-020-01026-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_JRTIP},
  author       = {Shamsolmoali, Pourya and Celebi, M. Emre and Wang, Ruili},
  doi          = {10.1007/s11554-020-01026-2},
  journal      = {Journal of Real-Time Image Processing},
  month        = {12},
  number       = {6},
  pages        = {1883-1884},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Advances in deep learning for real-time image and video reconstruction and processing},
  volume       = {17},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Communication and computation inter-effects in people
counting using intelligence partitioning. <em>JRTIP</em>,
<em>17</em>(6), 1869–1882. (<a
href="https://doi.org/10.1007/s11554-020-00943-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The rapid development of the Internet of Things is affecting the requirements towards wireless vision sensor networks (WVSN). Future smart camera architectures require battery-operated devices to facilitate deployment for scenarios such as industrial monitoring, environmental monitoring and smart city, consequently imposing constraints on the node energy consumption. This paper provides an analysis of the inter-effects between computation and communication energy for a smart camera node. Based on a people counting scenario, we evaluate the trade-off for the node energy consumption with different processing configurations of the image processing tasks, and several communication technologies. The results indicate that the optimal partition between the smart camera node and remote processing is with background modelling, segmentation, morphology and binary compression implemented in the smart camera, supported by Bluetooth Low Energy (BLE) version 5 technologies. The comparative assessment of these results with other implementation scenarios underlines the energy efficiency of this approach. This work changes pre-conceptions regarding design space exploration in WVSN, motivating further investigation regarding the inclusion of intermediate processing layers between the node and the cloud to interlace low-power configurations of communication and processing architectures.},
  archive      = {J_JRTIP},
  author       = {Shallari, Irida and Krug, Silvia and O’Nils, Mattias},
  doi          = {10.1007/s11554-020-00943-6},
  journal      = {Journal of Real-Time Image Processing},
  month        = {12},
  number       = {6},
  pages        = {1869-1882},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Communication and computation inter-effects in people counting using intelligence partitioning},
  volume       = {17},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Supervised deep semantics-preserving hashing for real-time
pulmonary nodule image retrieval. <em>JRTIP</em>, <em>17</em>(6),
1857–1868. (<a
href="https://doi.org/10.1007/s11554-020-00963-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hashing-based medical image retrieval has drawn extensive attention recently, which aims at providing effective aided diagnosis for medical personnel. In the paper, a novel deep hashing framework is proposed in the medical image retrieval, where the processes of deep feature extraction, binary code learning, and deep hash function learning are jointly carried out in supervised fashion. Particularly, the discrete constrained objective function in the hash code learning is optimized iteratively, where the binary code can be directly solved with no need for relaxation. In the meantime, the semantic similarity is maintained by fully exploring supervision information during the discrete optimization, where the neighborhood structure of training data is preserved by applying a graph regularization term. Additionally, to gain the fine-grained ranking of the returned medical images sharing the same Hamming distance, a novel image re-ranking scheme is proposed to refine the similarity measurement by jointly considering Euclidean distance between the real-valued feature descriptors and their category information between those images. Extensive experiments on the pulmonary nodule image dataset demonstrate that the proposed method can achieve better retrieval performance over the state of the arts.},
  archive      = {J_JRTIP},
  author       = {Qi, Yongjun and Gu, Junhua and Zhang, Yajuan and Wu, Gengshen and Wang, Feng},
  doi          = {10.1007/s11554-020-00963-2},
  journal      = {Journal of Real-Time Image Processing},
  month        = {12},
  number       = {6},
  pages        = {1857-1868},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Supervised deep semantics-preserving hashing for real-time pulmonary nodule image retrieval},
  volume       = {17},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Real-time moving human detection using HOG and fourier
descriptor based on CUDA implementation. <em>JRTIP</em>, <em>17</em>(6),
1841–1856. (<a
href="https://doi.org/10.1007/s11554-019-00935-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Real-time applications of image and video processing algorithms have seen explosive growth in number and complexity over the past decade driven by consumer, scientific and defense applications exploiting inexpensive digital video cameras and networked computing device. This growth has opened up different alternatives to greatly enhance the surveillance capabilities using new architectures and parallelization strategies developed due to the increased accessibility of multicore, multi-threaded processors along with general purpose graphics processing units (GPUs). In this paper, we present a new implementation of a moving human detection algorithm on GPU based on the programming language CUDA. In our approach, the moving object is extracted by background subtraction based on the GMM (Gaussian Mixture Model) on GPU. Then, two complementary features are extracted for moving object classification. They are contour-based description: FD or Fourier Descriptor and region-based description: HOG or Histogram of Oriented Gradient. Both descriptors will then be effectively integrated to SVM (Support Vector Machine), which is able to provide the posterior probability, to achieve better performance. The implementation of such algorithm on a GPU allows a great performance in terms of execution time since it is 19 times faster than that on a CPU. Experimental results show also that the proposed approach outperforms some existing techniques and can detect pedestrians in real-time effectively.},
  archive      = {J_JRTIP},
  author       = {Bahri, Haythem and Chouchene, Marwa and Sayadi, Fatma Ezahra and Atri, Mohamed},
  doi          = {10.1007/s11554-019-00935-1},
  journal      = {Journal of Real-Time Image Processing},
  month        = {12},
  number       = {6},
  pages        = {1841-1856},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Real-time moving human detection using HOG and fourier descriptor based on CUDA implementation},
  volume       = {17},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Real-time face attributes recognition via HPGC: Horizontal
pyramid global convolution. <em>JRTIP</em>, <em>17</em>(6), 1829–1840.
(<a href="https://doi.org/10.1007/s11554-019-00932-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recognizing face attributes in the wild is a challenging problem. With the development of embedded devices, smart phone and deep learning, face attributes recognition based on deep learning has raised increasing attention. Accuracy and efficiency are the two key-points in any application which uses these face attributes as an aid system. In most of the previous papers, multi-independency classifiers are proposed; and most of them just focus on accurate rate while neglecting efficiency. This paper proposes a horizontal pyramid global convolution (HPGC) module as feature mapping operator to extract more local information; designs a light-weight attribute convolution neural network (LACNN) combining with HPGC; and utilizes sigmoid cross entropy loss function for improving the accuracy and efficiency of the face attributes recognition model. Replacing full connection or global average pooling with the proposed HPGC module, we balance the accuracy performance and computation cost. As a result, we not only get high accuracy but also reduce the computational cost. Extensive experiments results on two widely used face attribute datasets, LFW and CelebA, demonstrate that our LACNN-HPGC framework achieves significantly improved efficiency compared with state-of-the-art lightweight models for face attributes recognition.},
  archive      = {J_JRTIP},
  author       = {Yang, Shimeng and Nian, Fudong and Wang, Yan and Li, Teng},
  doi          = {10.1007/s11554-019-00932-4},
  journal      = {Journal of Real-Time Image Processing},
  month        = {12},
  number       = {6},
  pages        = {1829-1840},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Real-time face attributes recognition via HPGC: Horizontal pyramid global convolution},
  volume       = {17},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Multiple player tracking in basketball court videos.
<em>JRTIP</em>, <em>17</em>(6), 1811–1828. (<a
href="https://doi.org/10.1007/s11554-020-00968-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To build a smart basketball court, one basic task is to track players with the aid of the basketball court monitoring. This task can be regarded as a special case of the multiple object tracking (MOT) problem. But different from it, the task puts request to the existing MOT methods with both good accuracy and operational efficiency (toward real time) in the new basketball scenario. To deal with this task, we make the following attempts: (1) Considering the differences between pedestrians and basketball players and the lack of corresponding dataset for basketball players tracking, we construct a new MOT dataset under the basketball court monitoring scene, to better evaluate MOT methods in our task and to help promote future research in the related task, (2) evaluating the performance of the several candidate MOT methods on the new dataset, and (3) proposing the issues to be further addressed in this specific scenario.},
  archive      = {J_JRTIP},
  author       = {Fu, Xubo and Zhang, Kun and Wang, Changgang and Fan, Chao},
  doi          = {10.1007/s11554-020-00968-x},
  journal      = {Journal of Real-Time Image Processing},
  month        = {12},
  number       = {6},
  pages        = {1811-1828},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Multiple player tracking in basketball court videos},
  volume       = {17},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Research and implementation of multi-object tracking based
on vision DSP. <em>JRTIP</em>, <em>17</em>(6), 1801–1809. (<a
href="https://doi.org/10.1007/s11554-020-00958-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper applies a pragmatic approach to study the real-time performance effect of software design methods for multiple object tracking (MOT) based on vision digital signal processing (vision DSP). The MOT system in the paper combines target detection, the Hungarian algorithm and the Kernel correlation filter (KCF) tracker. In addition, the MOT system needs to support multiway video streams, so higher speed and storage requirements are necessary for target tracking. Therefore, we carried out some studies on how to improve tracker speed performance and reduce system resource consumption under limited system resources. In the paper, we achieved the goal in two respects. Regarding the data processing, we studied how to efficiently process tracking data by utilizing the parallel characteristics of iDMA (integrated direct memory access) and a DSP core; and regarding the data storage, we proposed a time-sharing strategy to solve the DSP local memory (data RAM) usage issue for multiple tracking objects. In addition, regarding the software design, we propose a new strategy, which includes two levels of parallel computations: the frame-level parallel computations and the tracking object-level parallel computations. The experimental results show that the KCF tracking algorithm based on vision DSP achieves not only the desired real-time tracking speed but also the expected goal of system resource utilization. Our research methods also provide a reference for algorithm embedded applications in the field of computer vision.},
  archive      = {J_JRTIP},
  author       = {Gong, Xuan and Le, Zichun},
  doi          = {10.1007/s11554-020-00958-z},
  journal      = {Journal of Real-Time Image Processing},
  month        = {12},
  number       = {6},
  pages        = {1801-1809},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Research and implementation of multi-object tracking based on vision DSP},
  volume       = {17},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Fast simultaneous image super-resolution and motion
deblurring with decoupled cooperative learning. <em>JRTIP</em>,
<em>17</em>(6), 1787–1800. (<a
href="https://doi.org/10.1007/s11554-020-00976-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, deep convolutional neural networks (CNNs) have been widely applied to handle low-level vision problems. However, most existing CNN-based approaches can either handle single degeneration each time or treat them jointly through feature entangling, thus likely leading to poor performance when the actual degradation is inconsistent with hypothetical degradation condition. Furthermore, feature coupling will bring a large amount of computation, which may make the methods impractical to real-time mobile scenarios. In order to address these problems, we propose a deep decoupled cooperative learning model which can not only develop the corresponding recover network to deal with each degradation, but also flexibly handle multiple degradations at the same time. Thus, our approach can achieve disentangling and synthesizing single image super-resolution and motion deblurring, which has high practicability. We evaluate the proposed approach on various benchmark datasets, covering both natural images and synthetic images. The results demonstrate its superiority, compared to the state-of-the-art, where image SR and motion deblurring can be accomplished effectively concurrently. The source code of the work is available at https://github.com/hengliusky/Cooperative-Learning-Deblur-SR .},
  archive      = {J_JRTIP},
  author       = {Liu, Heng and Qin, Jiajun and Fu, Zilin and Li, Xue and Han, Jungong},
  doi          = {10.1007/s11554-020-00976-x},
  journal      = {Journal of Real-Time Image Processing},
  month        = {12},
  number       = {6},
  pages        = {1787-1800},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Fast simultaneous image super-resolution and motion deblurring with decoupled cooperative learning},
  volume       = {17},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). An FPGA-based design for real-time super-resolution
reconstruction. <em>JRTIP</em>, <em>17</em>(6), 1769–1785. (<a
href="https://doi.org/10.1007/s11554-020-00944-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For several decades, the camera spatial resolution is gradually increasing with the CMOS technology evolution. The image sensors provide more and more pixels, generating new constraints for suitable optics. As an alternative, promising solutions propose super-resolution (SR) techniques to reconstruct high-resolution images or video without modifying the sensor architecture. However, most of the SR implementations are far from reaching real-time performance on a low-budget hardware platform. Moreover, convincing state-of-the-art studies reveal that artifacts can be observed in highly textured areas of the image. In this paper, we propose a local adaptive spatial super resolution (LASSR) method to fix this limitation. LASSR is a two-step SR method including a machine learning-based texture analysis and a fast interpolation method that performs a pixel-by-pixel SR. Multiple evaluations of our method are also provided using standard image metrics for quantitative evaluation and also a psycho-visual assessment for a perceptual evaluation. A first FPGA-based implementation of the proposed method is then presented. It enables high-quality 2–4 k super-resolution videos to be performed at 16 fps, using only 13% of the FPGA capacity, opening the way to reach more than 60 fps by executing several parallel instances of the LASSR code on the FPGA.},
  archive      = {J_JRTIP},
  author       = {Marin, Yoan and Miteran, Johel and Dubois, Julien and Heyrman, Barthélémy and Ginhac, Dominique},
  doi          = {10.1007/s11554-020-00944-5},
  journal      = {Journal of Real-Time Image Processing},
  month        = {12},
  number       = {6},
  pages        = {1769-1785},
  shortjournal = {J. Real-Time Image Process.},
  title        = {An FPGA-based design for real-time super-resolution reconstruction},
  volume       = {17},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Fast automatic camera network calibration through human mesh
recovery. <em>JRTIP</em>, <em>17</em>(6), 1757–1768. (<a
href="https://doi.org/10.1007/s11554-020-01002-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Camera calibration is a necessary preliminary step in computer vision for the estimation of the position of objects in the 3D world. Despite the intrinsic camera parameters can be easily computed offline, extrinsic parameters need to be computed each time a camera changes its position, thus not allowing for fast and dynamic network re-configuration. In this paper we present an unsupervised and automatic framework for the estimation of the extrinsic parameters of a camera network, which leverages on optimised 3D human mesh recovery from a single image, and which does not require the use of additional markers. We show how it is possible to retrieve the real-world position of the cameras in the network together with the floor plane, exploiting regular RGB images and with a weak prior knowledge of the internal parameters. Our framework can also work with a single camera and in real-time, allowing the user to add, re-position, or remove cameras from the network in a dynamic fashion.},
  archive      = {J_JRTIP},
  author       = {Garau, Nicola and De Natale, Francesco G. B. and Conci, Nicola},
  doi          = {10.1007/s11554-020-01002-w},
  journal      = {Journal of Real-Time Image Processing},
  month        = {12},
  number       = {6},
  pages        = {1757-1768},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Fast automatic camera network calibration through human mesh recovery},
  volume       = {17},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Special issue on smart cameras for real-time image and video
processing. <em>JRTIP</em>, <em>17</em>(6), 1755–1756. (<a
href="https://doi.org/10.1007/s11554-020-01006-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_JRTIP},
  author       = {Shan, Caifeng and Brea, Victor Manuel and Velipasalar, Senem},
  doi          = {10.1007/s11554-020-01006-6},
  journal      = {Journal of Real-Time Image Processing},
  month        = {12},
  number       = {6},
  pages        = {1755-1756},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Special issue on smart cameras for real-time image and video processing},
  volume       = {17},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Exploring high-order adder compressors for power reduction
in sum of absolute differences architectures for real-time UHD video
encoding. <em>JRTIP</em>, <em>17</em>(5), 1735–1754. (<a
href="https://doi.org/10.1007/s11554-019-00939-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The sum of absolute difference (SAD) calculation is one of the most computing-intensive operations in video encoders compatible with recent standards, such as high-efficiency video coding (HEVC). SAD hardware architectures employ an adder tree to accumulate the coefficients from the absolute difference between two video blocks. This paper employs high-order adder compressors (HOAC) structures into SAD hardware architectures to achieve ultra-high definition (UHD) encoding in real time, using block sizes compatible with HEVC. The proposed HOAC architectures are power-efficient and enable low-power SAD hardware accelerators. Our throughput analysis shows that the HOAC-based SAD hardware architecture is capable of encoding UHD 4K ( $$3840\times 2160$$ ) videos in real-time at 60 frames per second. The architectures were entirely designed as dedicated ASIC blocks and were synthesized to ST 65 nm CMOS standard cells. Synthesis results show that SAD architectures using 64-2, 32-2, 16-2 and 8-2 compressors built from 4-2 compressors are significantly more efficient in terms of circuit area and total power dissipation when compared with SAD architectures using conventional adders selected by a commercial logic synthesis tool.},
  archive      = {J_JRTIP},
  author       = {Paim, Guilherme and Santana, Gustavo M. and Abreu, Brunno A. and Rocha, Leandro M. G. and Grellert, Mateus and da Costa, Eduardo A. C. and Bampi, Sergio},
  doi          = {10.1007/s11554-019-00939-x},
  journal      = {Journal of Real-Time Image Processing},
  month        = {10},
  number       = {5},
  pages        = {1735-1754},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Exploring high-order adder compressors for power reduction in sum of absolute differences architectures for real-time UHD video encoding},
  volume       = {17},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A streaming architecture for convolutional neural networks
based on layer operations chaining. <em>JRTIP</em>, <em>17</em>(5),
1715–1733. (<a
href="https://doi.org/10.1007/s11554-019-00938-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Convolutional Neural Networks (CNN) have become one of the best algorithms in machine learning for content classification of digital images. The CNN computational complexity is much larger than traditional algorithms, that is why the use of Graphical Processor Units (GPU) and online servers to achieve operations acceleration is a common solution. However, there is a growing demand for real-time processing solutions in the object recognition field mainly implemented on embedded systems, which are limited both in resources and energy consumption. Recently, reported works are focused on minimizing the required resources through two design strategies. The first one is by implementing one accelerator that can be adapted to the operations of the whole CNN. The CNN architecture proposals with one accelerator for each convolution layer belong to the second design strategy, where higher performance is achieved in multiple image processing. A new design strategy is proposed in this paper, which is based on multiple accelerators using a layer operation chaining scheme for computing in parallel the operations corresponding to multiple CNN layers. Three types of parallel data processing are adopted in the proposed architecture, where the parallelism level for convolution layers is determined by defined cost-function-based algorithms. The proposed design strategy is shown by implementing three naive CNNs on a De2i-150 board, in which a peak acceleration of 18.04x was achieved in contrast with state-of-the-art design methods without layer operation chaining. Furthermore, the design results of one modified Alexnet CNN were obtained. According to the obtained results, the proposed design strategy allows to achieve a smaller processing time than that obtained by reported works using the other two design strategies. In addition, a competitive result in resources utilization is obtained for naive CNNs.},
  archive      = {J_JRTIP},
  author       = {Arredondo-Velázquez, Moisés and Diaz-Carmona, Javier and Torres-Huitzil, Cesar and Padilla-Medina, Alfredo and Prado-Olivarez, Juan},
  doi          = {10.1007/s11554-019-00938-y},
  journal      = {Journal of Real-Time Image Processing},
  month        = {10},
  number       = {5},
  pages        = {1715-1733},
  shortjournal = {J. Real-Time Image Process.},
  title        = {A streaming architecture for convolutional neural networks based on layer operations chaining},
  volume       = {17},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A novel hardware-oriented ultra-high-speed object detection
algorithm based on convolutional neural network. <em>JRTIP</em>,
<em>17</em>(5), 1703–1714. (<a
href="https://doi.org/10.1007/s11554-019-00931-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper describes a hardware-oriented two-stage algorithm that can be deployed in a resource-limited field-programmable gate array (FPGA) for fast-object detection and recognition with out external memory. The first stage is the bounding boxes proposal with a conventional object detection method, and the second is convolutional neural network (CNN)-based classification for accuracy improvement. Frequently accessing external memories significantly affects the execution efficiency of object classification. Unfortunately, the existing CNN models with a large number of parameters are difficult to deploy in FPGAs with limited on-chip memory resources. In this study, we designed a compact CNN model and performed the hardware-oriented quantization for parameters and intermediate results. As a result, CNN-based ultra-fast-object classification was realized with all parameters and intermediate results stored on chip. Several evaluations were performed to demonstrate the performance of the proposed algorithm. The object classification module consumes only 163.67 Kbits of on-chip memories for ten regions of interest (ROIs), this is suitable for low-end FPGA devices. In the aspect of accuracy, our method provides a correctness rate of 98.01% in open-source data set MNIST and over 96.5% in other three self-built data sets, which is distinctly better than conventional ultra-high-speed object detection algorithms.},
  archive      = {J_JRTIP},
  author       = {Li, Jianquan and Long, Xianlei and Hu, Shenhua and Hu, Yiming and Gu, Qingyi and De Xu},
  doi          = {10.1007/s11554-019-00931-5},
  journal      = {Journal of Real-Time Image Processing},
  month        = {10},
  number       = {5},
  pages        = {1703-1714},
  shortjournal = {J. Real-Time Image Process.},
  title        = {A novel hardware-oriented ultra-high-speed object detection algorithm based on convolutional neural network},
  volume       = {17},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). UHD 8K energy-quality scalable HEVC intra-prediction SAD
unit hardware using optimized and configurable imprecise adders.
<em>JRTIP</em>, <em>17</em>(5), 1685–1701. (<a
href="https://doi.org/10.1007/s11554-019-00934-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Real-time digital video coding became a mandatory feature in current consumer electronic devices due to the popularization of video applications. However, efficiently encoding videos is an extremely processing/energy-demanding task, especially at high resolutions and frame rates. Thus, the limited energy resources and the dynamically varying system status (such as workload, battery level, user settings, etc.) require energy-efficient solutions capable to support run-time energy-quality scalability. In this work, we present an energy-quality scalable SAD Unit hardware architecture for the HEVC intra-frame prediction targeting real-time processing of UHD 8K (7680 × 4320) videos at 60 frames per second. Approximate computing is used to provide energy-quality scalability by employing configurable imprecise operators. The proposed Energy-Quality scalable architecture supports four operation points: precise computing, and 3-bit, 5-bit or 7-bit imprecision. When implemented in a 45-nm technology using Nangate standard cells library and running at 269 MHz, the proposed architecture consumes from 8.42 to 7.38 mJ to process each UHD 8K frame, according to the selected imprecision level. As a drawback, the coding efficiency (measured in BD rate) is reduced from 0.28 to 1.72%. Compared to the related works, this is the only intra-frame prediction SAD unit able to provide energy-quality scalability.},
  archive      = {J_JRTIP},
  author       = {Porto, Roger and Correa, Marcel and Goebel, Jones and Zatt, Bruno and Roma, Nuno and Agostini, Luciano and Porto, Marcelo},
  doi          = {10.1007/s11554-019-00934-2},
  journal      = {Journal of Real-Time Image Processing},
  month        = {10},
  number       = {5},
  pages        = {1685-1701},
  shortjournal = {J. Real-Time Image Process.},
  title        = {UHD 8K energy-quality scalable HEVC intra-prediction SAD unit hardware using optimized and configurable imprecise adders},
  volume       = {17},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). DSP-based image real-time dehazing optimization for improved
dark-channel prior algorithm. <em>JRTIP</em>, <em>17</em>(5), 1675–1684.
(<a href="https://doi.org/10.1007/s11554-019-00933-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To solve the problem of non-real-time processing of image dehazing using traditional dark-channel prior algorithm, this work studies image real-time penetrating fog optimization technologies based on digital signal processor (DSP) devices. Using jointed optimization mechanism between algorithm and device, we can achieve real-time processing. During algorithm optimization, mean filter characterized low computation substitutes the guided filter which is the most complex in dark-channel algorithm for dehazing. In optimization of image processing task under the embedded device, we empirically construct two-step optimization strategy for raising speed of processing. Thereupon, the awful division calculation for DSP device is achieved approximately by multiplication after the reciprocal operation. We utilize the specified template which is considerably designed to realize mean filter. Thus, the division factor in the template can be calculated innovatively via shift instructions featured on DSP. The experimental results show that the optimization solution provided has realized real-time image dehazing processing for standard-definition and high-definition at frame rate of 25 fps over C6748 pure DSP device featured 456 MHz clock, at the same time the effect of penetrating fog is not remarkably degraded. The optimization methods or ideas can easily be transplanted to similar platform.},
  archive      = {J_JRTIP},
  author       = {Lu, Jinzheng and Dong, Chuan},
  doi          = {10.1007/s11554-019-00933-3},
  journal      = {Journal of Real-Time Image Processing},
  month        = {10},
  number       = {5},
  pages        = {1675-1684},
  shortjournal = {J. Real-Time Image Process.},
  title        = {DSP-based image real-time dehazing optimization for improved dark-channel prior algorithm},
  volume       = {17},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). FPGA-accelerated textured surface defect segmentation based
on complete period fourier reconstruction. <em>JRTIP</em>,
<em>17</em>(5), 1659–1673. (<a
href="https://doi.org/10.1007/s11554-019-00927-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Real-time detection of surface defects in high-generation, large-size Liquid Crystal Display (LCD) panels is a serious challenge for both image algorithms and processing speed. For the defect detection of Thin Film Transistor-Liquid Crystal Display (TFT-LCD) images, effectively processing the periodic texture composed of gate lines and data lines is a prerequisite for the success of the algorithm. The traditional one-dimensional Fourier reconstruction algorithm uses a filter-based method to remove most of the texture, but due to the spectral leakage problem, the image boundary cannot be effectively processed. The compensation based on the period extension introduces a more complex ringing effect at the image connection. Starting from the implicit periodic principle of Fourier transform, we propose a strategy of complete period truncation based on subpixel period, which completely eliminates the boundary texture. Furthermore, we fully exploit the potential of parallel execution of the algorithm, resampling the liquid crystal segments truncated in the complete period to an integer power length of 2. The FPGA structure of one-dimensional Fourier reconstruction defect segmentation algorithm with dual-task parallelism and two-pixel parallelism is designed and the calculation bandwidth of 500 MB/s can be realized at 125-MHz clock frequency. We demonstrate the superiority of the proposed method qualitatively and quantitatively. The one-dimensional Fourier reconstruction algorithm based on the complete period truncation can effectively detect various defects such as spots, scratches, fibers and dirt, and the false-positive rate of defects has been reduced by half. The resampling-based Fourier transform speeds up the computational process and the FPGA parallel acceleration architecture is three times faster than comparable server CPUs, reducing the scan detection time of the entire 8.5-generation LCD panel to 8.5 s.},
  archive      = {J_JRTIP},
  author       = {Pan, Yinfei and Lu, Rongsheng and Zhang, Tengda},
  doi          = {10.1007/s11554-019-00927-1},
  journal      = {Journal of Real-Time Image Processing},
  month        = {10},
  number       = {5},
  pages        = {1659-1673},
  shortjournal = {J. Real-Time Image Process.},
  title        = {FPGA-accelerated textured surface defect segmentation based on complete period fourier reconstruction},
  volume       = {17},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Siamese network for real-time tracking with
action-selection. <em>JRTIP</em>, <em>17</em>(5), 1647–1657. (<a
href="https://doi.org/10.1007/s11554-019-00922-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Considering that most deep learning based trackers capture accurate locations for targets at the expense of consuming much time in training phrase, in this paper we present a new powerful tracker using the Siamese network which can be implemented with low computation resource. Our proposed tracker can track targets accurately by a fine-tuned model which is convenient to train. During the tracking, we apply a new sampling method that is independent of training called action-selection to conduct selective and flexible sampling step by step with a variable stride, by which we can get bounding boxes with varied aspect radio. By verifying its performance on online tracking benchmarks, it turns out that our tracker achieves higher accuracy than most traditional trackers. In addition, our tracker operates at frame-rates beyond real-time.},
  archive      = {J_JRTIP},
  author       = {Zhang, Zhuoyi and Zhang, Yifeng and Cheng, Xu and Li, Ke},
  doi          = {10.1007/s11554-019-00922-6},
  journal      = {Journal of Real-Time Image Processing},
  month        = {10},
  number       = {5},
  pages        = {1647-1657},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Siamese network for real-time tracking with action-selection},
  volume       = {17},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Fast intra-mode decision for depth map coding in 3D-HEVC.
<em>JRTIP</em>, <em>17</em>(5), 1637–1646. (<a
href="https://doi.org/10.1007/s11554-019-00920-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {3D-high efficiency video coding (3D-HEVC) contains more encoding viewpoints than traditional HEVC, resulting in a significant increase of coding complexity. In this paper, we propose a low complexity intra mode decision algorithm to reduce the number of intra modes by detecting the flat area and texture direction of the depth map. The corresponding intra prediction modes are skipped when the flat region condition is satisfied. Otherwise, the direction of the edge is detected to decrease the number of angle modes in rough mode decision, which can reduce the intra-coding complexity and coding time cost. Experimental results demonstrate that the proposed algorithm achieves on average 36.48% time saving with negligible degradation of coding performance.},
  archive      = {J_JRTIP},
  author       = {Zhang, Ruyi and Jia, Kebin and Liu, Pengyu and Sun, Zhonghua},
  doi          = {10.1007/s11554-019-00920-8},
  journal      = {Journal of Real-Time Image Processing},
  month        = {10},
  number       = {5},
  pages        = {1637-1646},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Fast intra-mode decision for depth map coding in 3D-HEVC},
  volume       = {17},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Real-time high-resolution omnidirectional imaging platform
for drone detection and tracking. <em>JRTIP</em>, <em>17</em>(5),
1625–1635. (<a
href="https://doi.org/10.1007/s11554-019-00921-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Drones have become steadily affordable, which raises privacy and security concerns as well as interest in drone detection systems. On the other hand, drone detection is a challenging task due to small dimensions of drones, difficulty of long-distance detection, strict real-time constraints and necessity of wide angle coverage for drones. Although different radar and audio-assisted drone detection systems have been presented, they suffer from the cost, range, or interference problems. On the contrary, a long-range detection can be obtained by a vision-based system. Aiming that, we propose a real-time moving object detection and tracking system optimized for drone detection using 16 cameras with 20 MP resolution. The proposed system detects drones from short range and long range with 360 $$^{\circ }$$ surveillance coverage owing high-performance ultra-high-resolution (320 MP) video-processing capability. It is able to detect drones with 100 cm diameter from 700 m distance despite deceptive background. It is interference free, so multiple systems can properly operate in the vicinity without effecting each other. It integrates processing power of embedded systems with flexibility of software to generate a full platform for drone detection and tracking.},
  archive      = {J_JRTIP},
  author       = {Demir, Bilal and Ergunay, Selman and Nurlu, Gokcen and Popovic, Vladan and Ott, Beat and Wellig, Peter and Thiran, Jean-Philippe and Leblebici, Yusuf},
  doi          = {10.1007/s11554-019-00921-7},
  journal      = {Journal of Real-Time Image Processing},
  month        = {10},
  number       = {5},
  pages        = {1625-1635},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Real-time high-resolution omnidirectional imaging platform for drone detection and tracking},
  volume       = {17},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Fast color quantization using MacQueen’s k-means algorithm.
<em>JRTIP</em>, <em>17</em>(5), 1609–1624. (<a
href="https://doi.org/10.1007/s11554-019-00914-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Color quantization (CQ) is an important operation with many applications in computer graphics and image processing and analysis. Clustering algorithms have been extensively applied to this problem. However, despite its popularity as a general purpose clustering algorithm, k-means has not received much attention in the CQ literature because of its high computational requirements and sensitivity to initialization. In this paper, we propose a novel CQ method based on an online k-means formulation due to MacQueen. The proposed method utilizes adaptive and efficient cluster center initialization and quasirandom sampling to attain deterministic, high speed, and high-quality quantization. Experiments on a diverse set of publicly available images demonstrate that the proposed method is significantly faster than the more common batch k-means formulation due to Lloyd while delivering nearly identical results.},
  archive      = {J_JRTIP},
  author       = {Thompson, Skyler and Celebi, M. Emre and Buck, Krizia H.},
  doi          = {10.1007/s11554-019-00914-6},
  journal      = {Journal of Real-Time Image Processing},
  month        = {10},
  number       = {5},
  pages        = {1609-1624},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Fast color quantization using MacQueen’s k-means algorithm},
  volume       = {17},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Pruned improved eight-point approximate DCT for image
encoding in visual sensor networks requiring only ten additions.
<em>JRTIP</em>, <em>17</em>(5), 1597–1608. (<a
href="https://doi.org/10.1007/s11554-019-00918-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A low-complexity pruned eight-point discrete cosine transform (DCT) approximation for image compression in visual sensor networks is introduced. The proposed transform consists of using an approximate DCT in combination with pruning approach. The aim of the former is to reduce the computational complexity by not computing the DCT exactly, while the latter aims at computing only the more important low-frequency coefficients. An algorithm for the fast computation of the proposed transform is developed. Only ten additions are required for both forward and backward transformations. The proposed pruned DCT transform exhibits extremely low computational complexity while maintaining competitive image compression performance in comparison with the state-of-the-art methods. An efficient parallel-pipelined hardware architecture for the proposed pruned DCT is also designed. The resulting design is implemented on Xilinx Virtex-6 XC6VSX475T-2ff1156 FPGA technology and evaluated for hardware resource utilization, power consumption, and real-time performance. All the metrics we investigated showed clear advantages of the proposed pruned approximate transform over the state-of-the-art competitors.},
  archive      = {J_JRTIP},
  author       = {Araar, Chaouki and Ghanemi, Salim and Benmohammed, Mohamed and Atoui, Hamza},
  doi          = {10.1007/s11554-019-00918-2},
  journal      = {Journal of Real-Time Image Processing},
  month        = {10},
  number       = {5},
  pages        = {1597-1608},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Pruned improved eight-point approximate DCT for image encoding in visual sensor networks requiring only ten additions},
  volume       = {17},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Voxel transformation: Scalable scene geometry discretization
for global illumination. <em>JRTIP</em>, <em>17</em>(5), 1585–1596. (<a
href="https://doi.org/10.1007/s11554-019-00919-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In real-time computer graphics, efficient discretization of scenes is required in order to accelerate graphics related algorithms such as realistic rendering with indirect illumination and visibility checking. Sparse voxel octree (SVO) is a popular data structure for such a discretization task. Populating an SVO with data is challenging when dynamic object count is high, especially when data per spatial location is large. Problem of populating such trees is adressed with our Voxel Transformation method, where pre-generated voxel data is transformed from model space to world space on demand, in contrast to the common way of voxelizing each dynamic object over each frame. Additionally, an accompanying filtering technique for voxel transformation is also proposed. This technique serves proposed system in two ways: (1) resolves issues introduced by the proposed fast and scalable voxel transformation method, and (2) enables smooth transitions between frames and handles the aliasing problem naturally as shown in the supplementary video. As an application use case, the proposed Voxel Transformation method is demonstrated in order to achieve indirect illumination using the well-known voxel cone tracing method. Results, which is compared with the standard voxelization method and ground-truth, are visually appealing and also scalable over large number of dynamic objects as shown in the supplementary video.},
  archive      = {J_JRTIP},
  author       = {Yalçıner, Bora and Sahillioğlu, Yusuf},
  doi          = {10.1007/s11554-019-00919-1},
  journal      = {Journal of Real-Time Image Processing},
  month        = {10},
  number       = {5},
  pages        = {1585-1596},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Voxel transformation: Scalable scene geometry discretization for global illumination},
  volume       = {17},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Efficient adaptive load balancing approach for compressive
background subtraction algorithm on heterogeneous CPU–GPU platforms.
<em>JRTIP</em>, <em>17</em>(5), 1567–1583. (<a
href="https://doi.org/10.1007/s11554-019-00916-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mixture of Gaussians (MoG) and compressive sensing (CS) are two common approaches in many image and audio processing systems. The combination of these algorithms is recently used for the compressive background subtraction task. Nevertheless, the result of this combination has not been exploited to take advantage of the evolution of parallel computing architectures. This paper proposes an efficient strategy to implement CS-MoG on heterogeneous CPU–GPU computing platforms. This is achieved through two elements. The first one is ensuring the better acceleration and accuracy that can be achieved for this algorithm on both CPU and GPU processors: The obtained results of the improved CS-MoG are more accurate and performant than other published MoG implementations. The second contribution is the proposition of the Optimal Data Distribution Cursor ODDC, a novel adaptive data partitioning approach to exploit simultaneously the heterogeneous processors on any given platform. It aims to ensure an automatic workload balancing by estimating the optimal data chunk size that must be assigned to each processor, with taking into consideration its computing capacity. Furthermore, our method ensures an update of the partitioning at runtime to take into account any influence of data content irregularity. The experimental results, on different platforms and data sets, show that the combination of these two contributions allows reaching 98% of the maximal possible performance of targeted platforms.},
  archive      = {J_JRTIP},
  author       = {Mabrouk, Lhoussein and Huet, Sylvain and Houzet, Dominique and Belkouch, Said and Hamzaoui, Abdelkrim and Zennayi, Yahya},
  doi          = {10.1007/s11554-019-00916-4},
  journal      = {Journal of Real-Time Image Processing},
  month        = {10},
  number       = {5},
  pages        = {1567-1583},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Efficient adaptive load balancing approach for compressive background subtraction algorithm on heterogeneous CPU–GPU platforms},
  volume       = {17},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Low-complexity open-loop coding of IDR infrared images
having JPEG compatibility. <em>JRTIP</em>, <em>17</em>(5), 1547–1565.
(<a href="https://doi.org/10.1007/s11554-019-00898-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we present a low-complexity open-loop Intermediate Dynamic Range (IDR) infrared image coding algorithm which provides two scalability layers. The first layer corresponds to a low dynamic range (LDR) version of the image which can be obtained by a JPEG decoder, while the second layer corresponds to the original IDR image. To achieve bit-depth scalability, we separate an input IDR image into LDR and residual images by simple operations, so that the residual image has a bit depth not higher than 8 bits per pixel, i.e., it can be compressed by JPEG baseline or other coding algorithm, developed for 8-bit depth formats. For real-time applications, we introduce a predefined look-up table containing quality factors for both the LDR and the residual images, so that increase of the table index corresponds to reduction of bit rate and increase of distortion close to an operational rate-distortion curve. Experimental results show that the proposed algorithm is significantly less complex than JPEG-XT Profile C, part 6, and provides better coding performance than its reference software with default tone mapping operator for the vast majority of the infrared test images.},
  archive      = {J_JRTIP},
  author       = {Belyaev, Evgeny and Forchhammer, Søren},
  doi          = {10.1007/s11554-019-00898-3},
  journal      = {Journal of Real-Time Image Processing},
  month        = {10},
  number       = {5},
  pages        = {1547-1565},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Low-complexity open-loop coding of IDR infrared images having JPEG compatibility},
  volume       = {17},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A labeling algorithm based on a forest of decision trees.
<em>JRTIP</em>, <em>17</em>(5), 1527–1545. (<a
href="https://doi.org/10.1007/s11554-019-00912-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Connected component labeling (CCL) is one of the most fundamental operations in image processing. CCL  is a procedure for assigning a unique label to each connected component. It is a mandatory step between low-level and high-level image processing. In this work, a general method is given to improve the neighbourhood exploration in a two-scan labeling. The neighbourhood values are considered as commands of a decision table. This decision table can be represented as a decision tree. A block-based approach is proposed so that values of several pixels are given by one decision tree. This block-based approach can be extended to multiple connectivities, 2D and 3D. In a raster scan, already seen pixels can be exploited to generate smaller decision trees. New decision trees are automatically generated from every possible command. This process creates a decision forest that minimises the number of memory accesses. Experimental results show that this method is faster than the state-of-the-art labelling algorithms and require fewer memory accesses. The whole process can be generalised to any given connectivity.},
  archive      = {J_JRTIP},
  author       = {Chabardès, T. and Dokládal, P. and Bilodeau, M.},
  doi          = {10.1007/s11554-019-00912-8},
  journal      = {Journal of Real-Time Image Processing},
  month        = {10},
  number       = {5},
  pages        = {1527-1545},
  shortjournal = {J. Real-Time Image Process.},
  title        = {A labeling algorithm based on a forest of decision trees},
  volume       = {17},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Real-time localization of multi-oriented text in natural
scene images using a linear spatial filter. <em>JRTIP</em>,
<em>17</em>(5), 1505–1525. (<a
href="https://doi.org/10.1007/s11554-019-00911-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes a multi-oriented text localization method in natural images suitable for real-time processing of high-definition video on portable and mobile devices. Our method is based on the connected components (CC) approach: first, CC are isolated by convolving a multi-scale pyramid with a specifically designed linear spatial filter followed by hysteresis thresholding. Next, non-textual CC are pruned employing a local classifier consisting of a cascade of multilayer perceptron (MLP) fed with increasingly extended feature vectors. The stroke width feature is estimated in linear time complexity by computing the maximal inscribed squares in the CC. Candidate CC and their neighbors are then checked using a more context aware neural network classifier that takes into account the target CC and their vicinity. Finally, text sequences are extracted in all pyramid levels and fused using dynamic programming. The main contribution of the work presented here is execution speed: the CPU-only parallel implementation of the proposed method is capable of processing 1080p HD video at nearly 30 frames per second on a standard laptop. Furthermore, when benchmarked on the ICDAR 2013 Robust Reading and on the ICDAR 2015 Incidental Scene Text data sets, our system performs more than twice faster than the state-of-the-art, while still delivering competitive results in terms of precision and recall.},
  archive      = {J_JRTIP},
  author       = {Gironés, Xavier and Julià, Carme},
  doi          = {10.1007/s11554-019-00911-9},
  journal      = {Journal of Real-Time Image Processing},
  month        = {10},
  number       = {5},
  pages        = {1505-1525},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Real-time localization of multi-oriented text in natural scene images using a linear spatial filter},
  volume       = {17},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Speeding up inference on deep neural networks for object
detection by performing partial convolution. <em>JRTIP</em>,
<em>17</em>(5), 1487–1503. (<a
href="https://doi.org/10.1007/s11554-019-00906-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Real-time object detection is an expected application of deep neural networks (DNNs). It can be achieved by employing graphic processing units (GPUs) or dedicated hardware accelerators. Alternatively, in this work, we present a software scheme to accelerate the inference stage of DNNs designed for object detection. The scheme relies on partial processing within the consecutive convolution layers of a DNN. It makes use of different relationships between the locations of the components of an input feature, an intermediate feature representation, and an output feature to effectively identify the modified components. This downsizes the matrix multiplicand to cover only those modified components. Therefore, matrix multiplication is accelerated within a convolution layer. In addition, the aforementioned relationships can also be employed to signal the next consecutive convolution layer regarding the modified components. This further helps reduce the overhead of the comparison on a member-by-member basis to identify the modified components. The proposed scheme has been experimentally benchmarked against a similar concept approach, namely, CBinfer, and against the original Darknet on the Tiny-You Only Look Once network. The experiments were conducted on a personal computer with dual CPU running at 3.5 GHz without GPU acceleration upon video data sets from YouTube. The results show that improvement ratios of 1.56 and 13.10 in terms of detection frame rate over CBinfer and Darknet, respectively, are attainable on average. Our scheme was also extended to exploit GPU-assisted acceleration. The experimental results of NVIDIA Jetson TX2 reached a detection frame rate of 28.12 frames per second (1.25 $$\times$$ with respect to CBinfer). The accuracy of detection of all experiments was preserved at 90% of the original Darknet.},
  archive      = {J_JRTIP},
  author       = {Kurdthongmee, Wattanapong},
  doi          = {10.1007/s11554-019-00906-6},
  journal      = {Journal of Real-Time Image Processing},
  month        = {10},
  number       = {5},
  pages        = {1487-1503},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Speeding up inference on deep neural networks for object detection by performing partial convolution},
  volume       = {17},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Speedup evaluation of HEVC parallel video coding using
tiles. <em>JRTIP</em>, <em>17</em>(5), 1469–1486. (<a
href="https://doi.org/10.1007/s11554-019-00900-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents an extensive evaluation of the HEVC parallel video coding when using Tiles. The evaluation consists on finding the tiling pattern that yields the maximum possible speedup for a set of video sequences considering several encoding parameters, measuring the coding efficiency variation of using such tiling pattern instead of the uniform tiling pattern, and calculating how far from the uniform tiling the maximum speedup tiling pattern is. To perform these evaluations, a different number of Tiles with different tiling patterns are applied; apart from that, different encoding profiles are employed. The results show that the speedup yielded by the uniform tiling is highly dependent on the video sequence and employed encoding profile. When encoding a set of video sequences with the same encoding parameters, the greater speedup may be up to 25% higher than the minor speedup, whereas when encoding the same video sequence with different encoding profiles, the greater speedup may be up to 21% higher than the minor speedup. When applying the maximum speedup tiling pattern to an encoding, distinct speedup gains may be achieved. While for some video sequences the maximum possible speedup equals the speedup yielded by the uniform tiling pattern, for others, changing from the uniform tiling to a better one may result in more than 40% of speedup gain. The results also show that when changing from the uniform tiling pattern to one that results in the maximum possible speedup, the coding efficiency variation is negligible; therefore, it is rewarding to seek better tiling patterns.},
  archive      = {J_JRTIP},
  author       = {Storch, Iago and Palomino, Daniel and Zatt, Bruno and Agostini, Luciano},
  doi          = {10.1007/s11554-019-00900-y},
  journal      = {Journal of Real-Time Image Processing},
  month        = {10},
  number       = {5},
  pages        = {1469-1486},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Speedup evaluation of HEVC parallel video coding using tiles},
  volume       = {17},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). An FPGA-based real-time occlusion robust stereo vision
system using semi-global matching. <em>JRTIP</em>, <em>17</em>(5),
1447–1468. (<a
href="https://doi.org/10.1007/s11554-019-00902-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Stereo matching approaches are an appealing choice for acquiring depth information in a number of video processing applications. It is desirable that these solutions generate dense, robust disparity maps in real time. However, occlusion regions may disturb the applications that need these maps. Among the best of these approaches is the semi-global matching (SGM) technique. This paper presents an FPGA-based stereo vision system based on SGM. This system calculates disparity maps by streaming, which are scalable to several resolutions and disparity ranges. To increase the robustness of the SGM technique even further, the present work has implemented a combination of the gradient filter and the sampling-insensitive absolute difference in the pre-processing phase. Furthermore, as a post-processing step, this paper proposes a novel streaming architecture to detect noisy and occluded regions. The FPGA-based implementations of the proposed stereo matching system in two distinct heterogeneous architecture (GPP—general purpose processor, and FPGA) were evaluated using the Middlebury stereo vision benchmark. The achieved results reported a frame rate of 25 FPS for the disparity maps processing in HD resolution (1024 $$\times$$ 768 pixels), with 256 disparity levels. The results have demonstrated that the memory utilization, processing performance, and accuracy are among the best of FPGA-based stereo vision systems.},
  archive      = {J_JRTIP},
  author       = {Cambuim, Lucas F. S. and Oliveira, Luiz A. and Barros, Edna N. S. and Ferreira, Antonyus P. A.},
  doi          = {10.1007/s11554-019-00902-w},
  journal      = {Journal of Real-Time Image Processing},
  month        = {10},
  number       = {5},
  pages        = {1447-1468},
  shortjournal = {J. Real-Time Image Process.},
  title        = {An FPGA-based real-time occlusion robust stereo vision system using semi-global matching},
  volume       = {17},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A memory and area-efficient distributed arithmetic based
modular VLSI architecture of 1D/2D reconfigurable 9/7 and 5/3 DWT
filters for real-time image decomposition. <em>JRTIP</em>,
<em>17</em>(5), 1421–1446. (<a
href="https://doi.org/10.1007/s11554-019-00901-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we have proposed the internal architecture of a dedicated hardware for 1D/2D convolution-based 9/7 and 5/3 DWT filters, exploiting bit-parallel ‘distributed arithmetic’ (DA) to reduce the computation time of our proposed DWT design while retaining the area at a comparable level to other recent existing designs. Despite using memory extensive bit-parallel DA, we have successfully achieved 90% reduction in the memory size than that of the other notable architectures. Through our proposed architecture, both the 9/7 and 5/3 DWT filters can be realized with a selection input, mode. With the introduction of DA, we have incorporated pipelining and parallelism into our proposed convolution-based 1D/2D DWT architectures. We have reduced the area by 38.3% and memory requirement by 90% than that of the latest remarkable designs. The critical-path delay of our design is almost 50% than that of the other latest designs. We have successfully applied our prototype 2D design for real-time image decomposition. The quality of the architecture in case of real-time image decomposition is measured by ‘peak signal-to-noise ratio’ and ‘computation time’, where our proposed design outperforms other similar kind of software- and hardware-based implementations.},
  archive      = {J_JRTIP},
  author       = {Chakraborty, Anirban and Banerjee, Ayan},
  doi          = {10.1007/s11554-019-00901-x},
  journal      = {Journal of Real-Time Image Processing},
  month        = {10},
  number       = {5},
  pages        = {1421-1446},
  shortjournal = {J. Real-Time Image Process.},
  title        = {A memory and area-efficient distributed arithmetic based modular VLSI architecture of 1D/2D reconfigurable 9/7 and 5/3 DWT filters for real-time image decomposition},
  volume       = {17},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Dynamic multifoveated structure for real-time vision tasks
in robotic systems. <em>JRTIP</em>, <em>17</em>(5), 1403–1419. (<a
href="https://doi.org/10.1007/s11554-019-00895-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Foveation is a technique that allows real-time image processing by drastically reducing the amount of visual data without loosing essential information around some focused area. When a robot needs to pay attention at two or more regions of the image at the same time, e.g., for tracking two or more objects, multifoveation is necessary. In this case, computing features twice in the intersections between the different foveated structures, which could linearly increase the processing time, must be avoided. To solve this redundancy removal problem, we propose two algorithms. The first one is based on the previous calculation of redundant blocks and the second one is based on a pixel-by-pixel processing at execution time. Experimental results show a gain in processing time for the block-based model in comparison with the pixel-by-pixel and also of both in comparison with other approaches that sequentially calculate various single foveated images. Robotics vision and other tasks related to dynamic visual attention, as recognition, real-time surveillance, video transmission, and image rendering, are examples of applications that can rely on and strongly benefit from such model.},
  archive      = {J_JRTIP},
  author       = {Medeiros, Petrucio R. T. and Gomes, Rafael B. and Clua, Esteban W. G. and Gonçalves, Luiz},
  doi          = {10.1007/s11554-019-00895-6},
  journal      = {Journal of Real-Time Image Processing},
  month        = {10},
  number       = {5},
  pages        = {1403-1419},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Dynamic multifoveated structure for real-time vision tasks in robotic systems},
  volume       = {17},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A new pipeline for the recognition of universal expressions
of multiple faces in a video sequence. <em>JRTIP</em>, <em>17</em>(5),
1389–1402. (<a
href="https://doi.org/10.1007/s11554-019-00896-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Facial expression recognition (FER) is a crucial issue in human–machine interaction. It allows machines to act according to facial expression changes. However, acting in real time requires recognizing the expressions at video speed. Usually, the video speed differs from one device to another. However, one of the standard settings for shooting videos is 24 fps. This speed is considered as the low end of what our brain can perceive as fluid video. From this perspective, to achieve a real-time FER, the image analysis must be completed, strictly, in less than 0.042 s no matter how the background complexity is or how many faces exists in the scene. In this paper, a new pipeline has been proposed to recognize the fundamental facial expressions for more than one person in real-world sequence videos. First, the pipeline takes as input a video and performs a face detection and tracking. Regions of Interest (ROI) are extracted from the detected face to extract the shape information when applying the histogram of oriented gradient (HOG) descriptor. The number of features yield by HOG descriptor is reduced by means of a linear discriminant analysis (LDA). Then, a deep data analysis was carried out, exploiting the pipeline, for the objective of setting up the LDA classifier. The analysis aimed at proving the suitability of the decision rule selected to separate the facial expression clusters in the LDA training phase. To conduct our analysis, we used ChonKanade (CK+) database and F-measure as an evaluation metric to calculate the average recognition rates. An automatic evaluation over time is proposed, where labelled videos is utilized to investigate the suitability of the pipeline in real-world condition. The pipeline results showed that the use of HOG descriptor and the LDA gives a high recognition rate of 94.66%. It should be noted that the proposed pipeline achieves an average processing time of 0.018 s, without requiring any device that speeds up the processing.},
  archive      = {J_JRTIP},
  author       = {Greche, Latifa and Akil, Mohamed and Kachouri, Rostom and Es-sbai, Najia},
  doi          = {10.1007/s11554-019-00896-5},
  journal      = {Journal of Real-Time Image Processing},
  month        = {10},
  number       = {5},
  pages        = {1389-1402},
  shortjournal = {J. Real-Time Image Process.},
  title        = {A new pipeline for the recognition of universal expressions of multiple faces in a video sequence},
  volume       = {17},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). CUDA implementation of fractal image compression.
<em>JRTIP</em>, <em>17</em>(5), 1375–1387. (<a
href="https://doi.org/10.1007/s11554-019-00894-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fractal coding is a lossy image compression technique, which encodes the image in a way that would require less storage space using the self-similar nature of the image. The main drawback of fractal compression is the high encoding time. This is due to the hard task of finding all fractals during the partition step and the search for the best match of fractals. Lately, GPUs (Graphical Processing Unit) have been exploited to implement fractal image compression algorithms due to their high computational power. The prime aim of this paper is to design and implement a parallel version of the Fisher classification scheme using CUDA to exploit the computational power available in the GPUs. Fisher classification scheme is used to reduce the encoding time of fractal images by limiting the search for the best match of fractals. Encoding time, compression ratio and peak signal-to-noise ratio was used as metrics to assess the correctness and the performance of the developed algorithm. Eight images with different sizes (512 × 512, 1024 × 1024 and 2048 × 2048) have been used for the experiments. The conducted experiments showed that a speedup of 6.4 × was achieved in some images using NVIDIA GeForce GT 660 M GPU.},
  archive      = {J_JRTIP},
  author       = {Al Sideiri, Abir and Alzeidi, Nasser and Al Hammoshi, Mayyada and Chauhan, Munesh Singh and AlFarsi, Ghaliya},
  doi          = {10.1007/s11554-019-00894-7},
  journal      = {Journal of Real-Time Image Processing},
  month        = {10},
  number       = {5},
  pages        = {1375-1387},
  shortjournal = {J. Real-Time Image Process.},
  title        = {CUDA implementation of fractal image compression},
  volume       = {17},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Reviewing GPU architectures to build efficient back
projection for parallel geometries. <em>JRTIP</em>, <em>17</em>(5),
1331–1373. (<a
href="https://doi.org/10.1007/s11554-019-00883-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Back-Projection is the major algorithm in Computed Tomography to reconstruct images from a set of recorded projections. It is used for both fast analytical methods and high-quality iterative techniques. X-ray imaging facilities rely on Back-Projection to reconstruct internal structures in material samples and living organisms with high spatial and temporal resolution. Fast image reconstruction is also essential to track and control processes under study in real-time. In this article, we present efficient implementations of the Back-Projection algorithm for parallel hardware. We survey a range of parallel architectures presented by the major hardware vendors during the last 10 years. Similarities and differences between these architectures are analyzed and we highlight how specific features can be used to enhance the reconstruction performance. In particular, we build a performance model to find hardware hotspots and propose several optimizations to balance the load between texture engine, computational and special function units, as well as different types of memory maximizing the utilization of all GPU subsystems in parallel. We further show that targeting architecture-specific features allows one to boost the performance 2–7 times compared to the current state-of-the-art algorithms used in standard reconstructions codes. The suggested load-balancing approach is not limited to the back-projection but can be used as a general optimization strategy for implementing parallel algorithms.},
  archive      = {J_JRTIP},
  author       = {Chilingaryan, Suren and Ametova, Evelina and Kopmann, Anreas and Mirone, Alessandro},
  doi          = {10.1007/s11554-019-00883-w},
  journal      = {Journal of Real-Time Image Processing},
  month        = {10},
  number       = {5},
  pages        = {1331-1373},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Reviewing GPU architectures to build efficient back projection for parallel geometries},
  volume       = {17},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A high throughput two-dimensional discrete cosine transform
and MPEG4 motion estimation using vector coprocessor. <em>JRTIP</em>,
<em>17</em>(5), 1319–1330. (<a
href="https://doi.org/10.1007/s11554-019-00892-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this work a configurable and scalable vector coprocessor for real time processing of MPEG4 motion estimation (ME) and two-dimensional DCT (2D DCT) is presented. A sequential DSP processor based on a reduced instruction set computer (RISC) processor architecture would require a frequency of 15 GHz for the real time processing of these two processes for a common intermediate format (CIF) sized sequence at 25 frames per second (fps). This frequency requirement will increase further if the image dimensions are increased. On the other hand our architecture on FPGA can achieve the real time processing rate at low frequency for CIF sized sequence and at higher frequency for full high definition (FHD) sequence for combined ME and 2D DCT. Due to configurable nature of the architecture and FPGA, this can be extended to higher dimensional image sequences. An important aspect of the architecture is that same datapath that is used for ME is also used for 2D DCT, with minor modification, leading to saving in area and time consumption. In addition the processor–coprocessor architecture has lower energy consumption and cost than the sequential processor.},
  archive      = {J_JRTIP},
  author       = {Agha, Shahrukh and Gulzari, Usman Ali and Shaheen, Farzana and Jan, Farmanullah},
  doi          = {10.1007/s11554-019-00892-9},
  journal      = {Journal of Real-Time Image Processing},
  month        = {10},
  number       = {5},
  pages        = {1319-1330},
  shortjournal = {J. Real-Time Image Process.},
  title        = {A high throughput two-dimensional discrete cosine transform and MPEG4 motion estimation using vector coprocessor},
  volume       = {17},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Real-time implementation of moving object detection in UAV
videos using GPUs. <em>JRTIP</em>, <em>17</em>(5), 1301–1317. (<a
href="https://doi.org/10.1007/s11554-019-00888-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unmanned aerial vehicles (UAVs) are being increasingly used for video surveillance and remote sensing. Moving object detection is an important algorithm for many such applications. Real-time processing of moving object detection is required for various decision-making tasks in many of these applications. However, being compute-intensive in nature, it is difficult to process high-resolution UAV-sourced videos in real-time. GPU vendors regularly release newer architectures with new features to speed up various kinds of applications. Hence, it becomes imperative to explore parallel implementations of such algorithms using the new GPU architectures. This paper describes parallel implementation strategies for algorithms like feature detection, feature matching, image transformation, frame differencing, morphological processing and connected component labeling which are used to detect moving objects in UAV-sourced videos. The implementation is tested on different NVIDIA GPU microarchitectures (Fermi, Maxwell, and Pascal). Experimental results show the achieved frame processing rates of 43.1 fps, 35.5 fps and 9.1 fps for 1080p videos on Pascal, Maxwell, and Fermi microarchitectures respectively.},
  archive      = {J_JRTIP},
  author       = {Jaiswal, Deepak and Kumar, Praveen},
  doi          = {10.1007/s11554-019-00888-5},
  journal      = {Journal of Real-Time Image Processing},
  month        = {10},
  number       = {5},
  pages        = {1301-1317},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Real-time implementation of moving object detection in UAV videos using GPUs},
  volume       = {17},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Fast 3D-HEVC PU size decision algorithm for depth map
intra-video coding. <em>JRTIP</em>, <em>17</em>(5), 1285–1299. (<a
href="https://doi.org/10.1007/s11554-019-00890-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {High-Efficiency Video Coding (HEVC)-based 3D video coding (3D-HEVC) is the most recent standard and last exertion of ISO/IEC MPEG and ITU-T Video Coding Experts Group (VCEG) for 3D video coding using a new data video format called Multi-View Video plus Depth map (MVD). This new standard achieves a high coding improvement. In any case, one of the most critical difficulties in 3D-HEVC is time computational complexity. The depth map intra-prediction is a critical factor in 3D-HEVC intra-coding, in which, the 3D-HEVC uses a highly adaptable Coding Unit (CU) structure with a specific end goal to expand the coding efficiency of all depth map characteristics. However, it results in an enormous Rate Distortion Optimization Cost (RDO-Cost) because of the broad recursive search for the best CU size from $$64\times 64$$ down to $$4\times 4$$ . This computational complexity excludes the 3D-HEVC from true and real-time application. Hence, it is imperative to build up an algorithm to diminish the complexity of the size decision in depth map intra-coding. To determine the previously mentioned issue, this paper proposes an effective 3D-HEVC PU size decision algorithm for depth map intra-video coding based on tensor features and statistical data analyses. The experimental results demonstrate that the proposed model diminishes the complexity of depth map size decision significantly with low rate distortion increase.},
  archive      = {J_JRTIP},
  author       = {Hamout, Hamza and Elyousfi, Abderrahmane},
  doi          = {10.1007/s11554-019-00890-x},
  journal      = {Journal of Real-Time Image Processing},
  month        = {10},
  number       = {5},
  pages        = {1285-1299},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Fast 3D-HEVC PU size decision algorithm for depth map intra-video coding},
  volume       = {17},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Parallel implementation of multiple kernel self-organizing
maps for spectral unmixing. <em>JRTIP</em>, <em>17</em>(5), 1267–1284.
(<a href="https://doi.org/10.1007/s11554-019-00880-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Spectral unmixing algorithms are commonly used in processing of hyperspectral images to identify the elemental components, called end-members, and their corresponding information in each pixel of the image. However, these algorithms are computationally intensive and can become a bottleneck for remote sensing hyperspectral image processing, especially in large aerial imagery processing centers. This paper, explores the use of massive parallel processing graphical processing unit to speed up the multi kernel self-organizing map (MKSOM) unmixing algorithm. MKSOM is based on artificial neural networks, which makes it suitable to be efficiently parallelized. Two real benchmark hyperspectral images; AVIRIS Cuprite and Brullus are used to evaluate the performance of the parallel algorithm. The experimental results show that the proposed implementation is appropriated for real-time hyperspectral remote sensing applications due to a very small worst case parallel execution time (0.83 s when the number of classes is less than 9) which makes it feasible to be integrated as on-board processing on any Hyperspectral remote sensors. Our parallel technique achieved a significant speedup compared with a multi-threaded CPU implementation applied on the same hyperspectral image. The results showed a speedup of 93.46 × for SOM size of 256 and trained for 100 epochs on medium-sized HSI such as AVIRIS Cuprite.},
  archive      = {J_JRTIP},
  author       = {Fathy, Ghada M. and Hassan, Hanan A. and Rahwan, Shaheera and Sheta, Walaa M.},
  doi          = {10.1007/s11554-019-00880-z},
  journal      = {Journal of Real-Time Image Processing},
  month        = {10},
  number       = {5},
  pages        = {1267-1284},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Parallel implementation of multiple kernel self-organizing maps for spectral unmixing},
  volume       = {17},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Real-time thinning algorithms for 2D and 3D images using GPU
processors. <em>JRTIP</em>, <em>17</em>(5), 1255–1266. (<a
href="https://doi.org/10.1007/s11554-019-00886-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The skeletonization of binary images is a common task in many image processing and machine learning applications. Some of these applications require very fast image processing. We propose novel techniques for efficient 2D and 3D thinning of binary images using GPU processors. The algorithms use bit-encoded binary images to process multiple points simultaneously in each thread. The simpleness of a point is determined based on Boolean algebra using only bitwise logical operators. This avoids computationally expensive decoding and encoding steps and allows for additional parallelization. The 2D algorithm is evaluated using a data set of handwritten characters images. It required an average computation time of 3.53 ns for 32 $$\times$$ 32 pixels and 0.25 ms for 1024 $$\times$$ 1024 pixels. This is 52–18,380 times faster than a multi-threaded border-parallel algorithm. The 3D algorithm was evaluated based on clinical images of the human vasculature and required computation times of 0.27 ms for 128 $$\times$$ 128 $$\times$$ 128 voxels and 20.32 ms for 512 $$\times$$ 512 $$\times$$ 512 voxels, which is 32–46 times faster than the compared border-sequential algorithm using the same GPU processor. The proposed techniques enable efficient real-time 2D and 3D skeletonization of binary images, which could improve the performance of many existing machine learning applications.},
  archive      = {J_JRTIP},
  author       = {Wagner, Martin G.},
  doi          = {10.1007/s11554-019-00886-7},
  journal      = {Journal of Real-Time Image Processing},
  month        = {10},
  number       = {5},
  pages        = {1255-1266},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Real-time thinning algorithms for 2D and 3D images using GPU processors},
  volume       = {17},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A novel unified method for the fast computation of discrete
image moments on grayscale images. <em>JRTIP</em>, <em>17</em>(5),
1239–1253. (<a
href="https://doi.org/10.1007/s11554-019-00878-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We proposed a new method to compute the discrete image moments in this paper. By simple mathematical deduction, the discrete image moments can be transformed into first-order moments. Therefore, the fast algorithm for first-order moments’ calculation can be used to compute discrete image moments. We also design an efficient computation structure based on systolic array to implement this approach. Since our method does not use the moment kernel polynomials’ properties in the calculation process, the proposed method can be used to compute any discrete image moments in the same way. The presented algorithm has several advantages such as regular and simple computation structure, without multiplication, independent of the image’s intensity distribution, applicable to any discrete moment family. Various experiments demonstrate the effectiveness of the proposed algorithm in comparison with some state-of-the-art methods.},
  archive      = {J_JRTIP},
  author       = {Hua, Xia and Hong, Hanyu and Liu, Jianguo and Shi, Yu},
  doi          = {10.1007/s11554-019-00878-7},
  journal      = {Journal of Real-Time Image Processing},
  month        = {10},
  number       = {5},
  pages        = {1239-1253},
  shortjournal = {J. Real-Time Image Process.},
  title        = {A novel unified method for the fast computation of discrete image moments on grayscale images},
  volume       = {17},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Hybrid stopping model-based fast PU and CU decision for
3D-HEVC texture coding. <em>JRTIP</em>, <em>17</em>(5), 1227–1238. (<a
href="https://doi.org/10.1007/s11554-019-00876-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As an extension of High-Efficiency Video Coding (HEVC) standard, 3D-HEVC needs to encode multiple texture views and depth maps, which further increases the computational complexity. To reduce the complexity of dependent texture view coding, a fast prediction unit (PU) and coding unit (CU) decision method is proposed for 3D-HEVC based on hybrid stopping model. The inter-view correlation is used as a priori information to roughly predict the possible optimal PU and CU sizes. Then, by exploiting the encoded posterior information, the rate distortion cost correlation and the code block flag, the optimal PU and CU are further examined as being optimal or not. Experimental results show that the proposed fast PU and CU decision method achieves 52.7% encoding time saving on average with negligible loss of coding efficiency for 3D-HEVC-dependent texture view coding.},
  archive      = {J_JRTIP},
  author       = {Li, Yue and Yang, Gaobo and Zhu, Yapei and Ding, Xiangling and Song, Yun and Zhang, Dengyong},
  doi          = {10.1007/s11554-019-00876-9},
  journal      = {Journal of Real-Time Image Processing},
  month        = {10},
  number       = {5},
  pages        = {1227-1238},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Hybrid stopping model-based fast PU and CU decision for 3D-HEVC texture coding},
  volume       = {17},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Real-time video freezing detection for 4K UHD videos.
<em>JRTIP</em>, <em>17</em>(5), 1211–1225. (<a
href="https://doi.org/10.1007/s11554-019-00873-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Video frame freezing is a common artifact which can occur during video content delivery due to errors in the video coding process, video transmission, storage or reproduction. This artifact can significantly decrease the end-user Quality of Experience. Therefore, accurate video frame freezing detection is of great importance for different parties involved in video content delivery. In this paper, a new Real-Time no-reference Freezing Detection Algorithm, called the RTFDA, is proposed. As newly video frames are acquired, the RTFDA performs comparison of the current video frame with the corresponding previous one to detect whether video freezing is occurring. The comparison is made by calculating the corresponding subsampled video frames and their pixel-by-pixel absolute difference comparison. The benefit of such approach is twofold: the influence of noise in freezing frames on frame comparison is significantly reduced as well as computational complexity of frame comparison. The RTFDA has a high detection rate with a very low rate of both false-positive and false-negative detections, outperforming four freezing detection algorithms on four different video databases. The proposed implementation on an x86-64 platform achieves real-time performance on 4K Ultra High Definition (UHD) videos by processing 216 frames per second (fps). Apart from that, FPGA implementation is proposed, which has efficient FPGA resource utilization.},
  archive      = {J_JRTIP},
  author       = {Grbić, Ratko and Stefanović, Dejan and Vranješ, Mario and Herceg, Marijan},
  doi          = {10.1007/s11554-019-00873-y},
  journal      = {Journal of Real-Time Image Processing},
  month        = {10},
  number       = {5},
  pages        = {1211-1225},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Real-time video freezing detection for 4K UHD videos},
  volume       = {17},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). GPU-based chromatic co-occurrence matrices for tracking
moving objects. <em>JRTIP</em>, <em>17</em>(5), 1197–1210. (<a
href="https://doi.org/10.1007/s11554-019-00874-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Generally, a good tracking system requires a huge computation time to localize, with accuracy, the target object. For real-time tracking applications, the running time is a critical factor. In this paper, a GPU implementation of the chromatic co-occurrence matrices (CCM) tracking system is proposed. Indeed, the descriptors based on CCM help to improve the accuracy of the tracking. However, they require a long computation time. To overcome this limitation, a parallel implementation of these matrices based on GPU is incorporated to the tracker. The developed algorithm is then integrated into an embedded system to build a real-time autonomous embedded tracking system. The experimental results show a speed up of 150% in the GPU version of the tracker compared to the CPU version.},
  archive      = {J_JRTIP},
  author       = {Elafi, Issam and Jedra, Mohamed and Zahid, Noureddine},
  doi          = {10.1007/s11554-019-00874-x},
  journal      = {Journal of Real-Time Image Processing},
  month        = {10},
  number       = {5},
  pages        = {1197-1210},
  shortjournal = {J. Real-Time Image Process.},
  title        = {GPU-based chromatic co-occurrence matrices for tracking moving objects},
  volume       = {17},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Efficient cascading of multi-domain image gaussian noise
filters. <em>JRTIP</em>, <em>17</em>(5), 1183–1195. (<a
href="https://doi.org/10.1007/s11554-019-00868-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image denoising is a well explored but still an active research topic. The focus is usually on achieving higher numerical quality which is theoretically interesting, however, often the factor of computation cost is not considered. Our idea is to employ different image Gaussian noise filters to construct an effective image denoiser, where the deficiency of each filter is compensated with others, while a wide variation of quality versus speed can be achieved. We integrate filters using different cascaded forms and show that if two filters use uncorrelated features, their cascaded form provides a higher quality than each separately. We start with easy-to-implement filters employing pixel- and frequency-domain with different kernel size to construct a fast yet high-quality multi-domain denoiser. Then, we propose more complex denoisers by integrating our cascaded multi-domain denoiser to other state-of-the-art denoising methods. Simulations show that the quality of proposed multi-domain denoiser is significantly higher than its building-blocks. We also show that the proposed multi-domain denoiser can be integrated to state-of-the-art denoisers to from a more effective denoiser, while adding negligible complexity.},
  archive      = {J_JRTIP},
  author       = {Rakhshanfar, Meisam and Amer, Maria A.},
  doi          = {10.1007/s11554-019-00868-9},
  journal      = {Journal of Real-Time Image Processing},
  month        = {10},
  number       = {5},
  pages        = {1183-1195},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Efficient cascading of multi-domain image gaussian noise filters},
  volume       = {17},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). GPU acceleration of the KAZE image feature extraction
algorithm. <em>JRTIP</em>, <em>17</em>(5), 1169–1182. (<a
href="https://doi.org/10.1007/s11554-019-00861-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The recently proposed, KAZE image feature detection and description algorithm (Alcantarilla et al. in Proceedings of the British machine vision conference. LNCS, vol 7577, no 6, pp 13.1–13.11, 2013) offers significantly improved robustness in comparison to conventional algorithms like SIFT (scale-invariant feature transform) and SURF (speeded-up robust features). The improved robustness comes at a significant computational cost, however, limiting its use for many applications. We report a GPU acceleration of the KAZE algorithm that is significantly faster than its CPU counterpart. Unlike previous reports, our acceleration does not resort to binary descriptors and can serve as a drop-in replacement for CPU-KAZE, SIFT, SURF etc. By achieving nearly tenfold speedup (for a 1920 by 1200 sized image, our Compute Unified Device Architecture (CUDA)-C implementation took around 245 ms on a single GPU in comparison to nearly 2400 ms for a 16-threaded CPU version) without degradation in feature extraction performance, our work expands the applicability of the KAZE algorithm. Additionally, the strategies described here could also prove useful for the GPU implementation of other nonlinear scale-space-based image processing algorithms.},
  archive      = {J_JRTIP},
  author       = {Ramkumar, B. and Laber, Rob and Bojinov, Hristo and Hegde, Ravi Sadananda},
  doi          = {10.1007/s11554-019-00861-2},
  journal      = {Journal of Real-Time Image Processing},
  month        = {10},
  number       = {5},
  pages        = {1169-1182},
  shortjournal = {J. Real-Time Image Process.},
  title        = {GPU acceleration of the KAZE image feature extraction algorithm},
  volume       = {17},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Mass center direction-based decision method for
intraprediction in HEVC standard. <em>JRTIP</em>, <em>17</em>(5),
1153–1168. (<a
href="https://doi.org/10.1007/s11554-019-00864-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Increasing the prediction modes and recursive quad-tree structure, can be the reasons for high compression efficiency in high-efficiency video coding (HEVC) standard. For the same reasons, the computational complexity of HEVC is increased compared with the previous standards. One of the parts that impose a high computational load is the intraprediction unit. In this paper, an improved intraprediction mode decision method is presented to accelerate intracoding of HEVC. A mass center direction-based approach is used to find correlation direction of pixels which can effectively eliminate the prediction modes that have low chance to be selected as the best intramode from the rate-distortion optimization computations. According to the founded correlation direction for 4 × 4 blocks, the depth range of coding units can also be reduced to accelerate the intracoding further. The performance of the proposed method evaluated with different test sequences proposed by the joint collaborative team on video coding. The coding results indicate that the proposed method reduces the encoding time significantly as compared with the HM-16.19 reference software with negligible loss of peak signal-to-noise ratio quality.},
  archive      = {J_JRTIP},
  author       = {Najafabadi, Narjes and Ramezanpour, Mohammadreza},
  doi          = {10.1007/s11554-019-00864-z},
  journal      = {Journal of Real-Time Image Processing},
  month        = {10},
  number       = {5},
  pages        = {1153-1168},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Mass center direction-based decision method for intraprediction in HEVC standard},
  volume       = {17},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Video stabilization performance enhancement for low-texture
videos. <em>JRTIP</em>, <em>17</em>(5), 1135–1152. (<a
href="https://doi.org/10.1007/s11554-019-00862-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Digital video stabilization (DVS) aims to remove irregular global motion effects from an image sequence. This work aims at developing a real-time video stabilization algorithm for rectifying high-frequency jitter in marine surveillance applications. A DVS system consists of a global motion estimation system and motion correction system. The development of global motion estimation system resistant to failures in low texture videos is the primary goal. Due to the computational advantage and inherent properties, the phase correlation method is adopted as the basic global motion estimation algorithm. The basic algorithm is then modified to adapt to the varying texture content of the video sequences under consideration. An adaptive phase correlation-based global motion estimation is suggested and verified on the videos of varying textures.},
  archive      = {J_JRTIP},
  author       = {Unnikrishnan, Supriya and Sreelekha, G.},
  doi          = {10.1007/s11554-019-00862-1},
  journal      = {Journal of Real-Time Image Processing},
  month        = {10},
  number       = {5},
  pages        = {1135-1152},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Video stabilization performance enhancement for low-texture videos},
  volume       = {17},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Scalable implementation of particle filter-based visual
object tracking on network-on-chip (NoC). <em>JRTIP</em>,
<em>17</em>(5), 1117–1134. (<a
href="https://doi.org/10.1007/s11554-018-0841-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Particle filter algorithms have been successfully used in various visual object tracking applications. They handle non-linear model and non-Gaussian noise, but are computationally demanding. In this paper, we propose a scalable implementation of particle filter algorithm for visual object tracking, using scalable interconnect such as network-on-chip on an FPGA platform. Here, several processing elements execute parallelly to handle large number of particles. We propose two designs and implementations, with one optimized for speed and other optimized for area. These implementations can easily support different image sizes, object sizes, and number of particles, without modifying the complete architecture. Multi-target tracking is also demonstrated for four objects. We validated the particle filter-based visual tracking with video feed from a Petalinux-based system. With image size of $$320\times 240$$ , frame rates of 348 fps and 310 fps were achieved for single-object tracking of size $$17\times 17$$ and $$33\times 33$$ pixels, respectively, with a reasonable low-power consumption of 1.7 mW/fps on Zynq XC7Z020 (Zedboard) with an operating frequency of 69 MHz. This makes our implementation a good candidate for low-power, visual object tracking using FPGA, especially in low-power, smart camera applications.},
  archive      = {J_JRTIP},
  author       = {Engineer, Pinalkumar and Velmurugan, Rajbabu and Patkar, Sachin},
  doi          = {10.1007/s11554-018-0841-5},
  journal      = {Journal of Real-Time Image Processing},
  month        = {10},
  number       = {5},
  pages        = {1117-1134},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Scalable implementation of particle filter-based visual object tracking on network-on-chip (NoC)},
  volume       = {17},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Real-time adaptive visible and infrared image registration
based on morphological gradient and c_SIFT. <em>JRTIP</em>,
<em>17</em>(5), 1103–1115. (<a
href="https://doi.org/10.1007/s11554-019-00858-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Since the visible and infrared images have different imaging mechanisms, the difficulty of image registration has greatly increased. The grayscale difference between visible and infrared images is very disadvantageous for extracting feature points in homogenous region, but they both retain the obvious contour edge in the scene. After using the morphological gradient method, the grayscale edge of visible and infrared images can be obtained and their similarity is greatly improved, and their difference may be seen as the difference in brightness or grayscale. Therefore, we proposed a novel algorithm to realise real-time adaptive registration of visible and infrared images using morphological gradient and C_SIFT. Firstly, the morphological gradient method is used to extract the rough edges of visible and infrared images for aligning their visual features as a single similar type. Secondly, the C_SIFT feature detection operator is used to detect and extract feature points from the extracted edges. The C_SIFT uses the centroid method to describe the main direction of feature points, makes rotation invariance feasible. Finally, to verify the effectiveness of the proposed algorithm, we carried out a series of experiments in eight various scenarios. The experimental results show that the proposed algorithm has achieved good experimental results. The registration of visible and infrared images can be completed quickly by the proposed algorithm, and the registration accuracy is satisfactory.},
  archive      = {J_JRTIP},
  author       = {Zeng, Qiang and Adu, Jianhua and Liu, Jiexin and Yang, Jianxing and Xu, Yuanping and Gong, Mei},
  doi          = {10.1007/s11554-019-00858-x},
  journal      = {Journal of Real-Time Image Processing},
  month        = {10},
  number       = {5},
  pages        = {1103-1115},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Real-time adaptive visible and infrared image registration based on morphological gradient and C_SIFT},
  volume       = {17},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Real-time iris segmentation and its implementation on FPGA.
<em>JRTIP</em>, <em>17</em>(5), 1089–1102. (<a
href="https://doi.org/10.1007/s11554-019-00859-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a real-time iris segmentation technique that is well suited to a fast implementation on an FPGA. One major hurdle associated with iris segmentation techniques is the use of iterative processes that lead to expensive hardware implementations. To circumvent this, the proposed algorithm uses the sign image obtained from subtracting the background, along with morphological operators to localise the pupil. The outer boundary is located by first normalising a selected image region that contains the iris, and then using a first-order gradient operator. The proposed non-iterative algorithm is implemented on an FPGA. Four near infrared (NIR) iris public databases, namely: CASIA-IrisV3-Lamp, MMU v1.0, ND-IRIS-0405 and NIST ICE 2005, are used to test the proposed algorithm. The proposed method for iris segmentation and normalization gives much better accuracy than the existing state-of-the-art methods implemented on hardware. The proposed realisation requires about 45% fewer logic registers and 52% fewer logic elements than the existing state-of-the-art implementations.},
  archive      = {J_JRTIP},
  author       = {Khan, Tariq M. and Bailey, Donald G. and Khan, Mohammad A. U. and Kong, Yinan},
  doi          = {10.1007/s11554-019-00859-w},
  journal      = {Journal of Real-Time Image Processing},
  month        = {10},
  number       = {5},
  pages        = {1089-1102},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Real-time iris segmentation and its implementation on FPGA},
  volume       = {17},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Self-tuning fast adaptive algorithm for impulsive noise
suppression in color images. <em>JRTIP</em>, <em>17</em>(4), 1067–1087.
(<a href="https://doi.org/10.1007/s11554-019-00853-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, a self-tuning version of the newly introduced Fast Adaptive Switching Trimmed Arithmetic Mean Filter, which is a very efficient technique for impulsive noise suppression, is elaborated. Most of the methods presented in the rich literature have numerous parameters, whose proper settings are crucial for efficient noise suppression. Although researchers often provide recommended values for their algorithms’ parameters, the actual choice remains in the hands of the user. Our goal is to free the operator from parameter selection dilemma and to propose an algorithm which includes required expert knowledge within itself. The only obligatory inputs of the proposed algorithm (from the user perspective) are the image itself and the size of the operating window.},
  archive      = {J_JRTIP},
  author       = {Malinski, Lukasz and Smolka, Bogdan},
  doi          = {10.1007/s11554-019-00853-2},
  journal      = {Journal of Real-Time Image Processing},
  month        = {8},
  number       = {4},
  pages        = {1067-1087},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Self-tuning fast adaptive algorithm for impulsive noise suppression in color images},
  volume       = {17},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Convolution neural network joint with mixture of extreme
learning machines for feature extraction and classification of accident
images. <em>JRTIP</em>, <em>17</em>(4), 1051–1066. (<a
href="https://doi.org/10.1007/s11554-019-00852-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper considers the accident images and develops a deep learning method for feature extraction together with a mixture of experts for classification. For the first task, the outputs of the last max-pooling layer of a Convolution Neural Network (CNN) are used to extract the hidden features automatically. For the second task, a mixture of advanced variations of Extreme Learning Machine (ELM) including basic ELM, constraint ELM (CELM), On-Line Sequential ELM (OSELM) and Kernel ELM (KELM), is developed. This ensemble classifier combines the advantages of different ELMs using a gating network and its accuracy is very high while the processing time is close to real-time. To show the efficiency, the different combinations of the traditional feature extraction and feature selection methods and the various classifiers are examined on two kinds of benchmarks including accident images’ data set and some general data sets. It is shown that the proposed system detects the accidents with 99.31% precision, recall and F-measure. Besides, the precisions of accident-severity classification and involved-vehicle classification are 90.27% and 92.73%, respectively. This system is suitable for on-line processing on the accident images that will be captured by Unmanned Aerial Vehicles (UAV) or other surveillance systems.},
  archive      = {J_JRTIP},
  author       = {Pashaei, Ali and Ghatee, Mehdi and Sajedi, Hedieh},
  doi          = {10.1007/s11554-019-00852-3},
  journal      = {Journal of Real-Time Image Processing},
  month        = {8},
  number       = {4},
  pages        = {1051-1066},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Convolution neural network joint with mixture of extreme learning machines for feature extraction and classification of accident images},
  volume       = {17},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Fast video encoding based on random forests. <em>JRTIP</em>,
<em>17</em>(4), 1029–1049. (<a
href="https://doi.org/10.1007/s11554-019-00854-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Machine learning approaches have been increasingly used to reduce the high computational complexity of high-efficiency video coding (HEVC), as this is a major limiting factor for real-time implementations, due to the decision process required to find optimal coding modes and partition sizes for the quad-tree data structures defined by the standard. This paper proposes a systematic approach to reduce the computational complexity of HEVC based on an ensemble of online and offline Random Forests classifiers. A reduced set of features for training the Random Forests classifier is proposed, based on the rankings obtained from information gain and a wrapper-based approach. The best model parameters are also obtained through a consistent and generalizable method. The proposed Random Forests classifier is used to model the coding unit and transform unit-splitting decision and the SKIP-mode prediction, as binary classification problems, taking advantage from the combination of online and offline approaches, which adapts better to the dynamic characteristics of video content. Experimental results show that, on average, the proposed approach reduces the computational complexity of HEVC by 62.64% for the random access (RA) profile and 54.57% for the low-delay (LD) main profile, with an increase in BD-Rate of 2.58% for RA and 2.97% for LD, respectively. These results outperform the previous works also using ensemble classifiers for the same purpose.},
  archive      = {J_JRTIP},
  author       = {Tahir, Muhammad and Taj, Imtiaz A. and Assuncao, Pedro A. and Asif, Muhammad},
  doi          = {10.1007/s11554-019-00854-1},
  journal      = {Journal of Real-Time Image Processing},
  month        = {8},
  number       = {4},
  pages        = {1029-1049},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Fast video encoding based on random forests},
  volume       = {17},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Knowledge-based automatic extraction of multi-structured
light stripes. <em>JRTIP</em>, <em>17</em>(4), 1015–1027. (<a
href="https://doi.org/10.1007/s11554-019-00851-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To achieve automatic processing of the multi-structured light stripe images, we need to extract many stripes automatically in a short time. However, due to the complexity and diversity of its work, the related research progress is very slow. Therefore, we first use the Radon transformation and grayscale transform enhancement to eliminate noise in images. Then, a new and adaptive feature model is created based on the knowledge of stripes distribution. The distribution of the stripes is matched by the model, and then the target stripe regions are picked up to realize the extraction of the stripes. In the actual verification, the qualified rate of the detection is basically over 80%, and the detection time is controlled at about 2 s. The automatic extraction of target stripes effectively avoids the tedious work that workers need to do it manually, and it is of great significance for the application of multi-structured light detection technology.},
  archive      = {J_JRTIP},
  author       = {Ding, Chao and Tang, Liwei and Cao, Lijun and Shao, Xinjie and Wang, Wei and Deng, Shijie},
  doi          = {10.1007/s11554-019-00851-4},
  journal      = {Journal of Real-Time Image Processing},
  month        = {8},
  number       = {4},
  pages        = {1015-1027},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Knowledge-based automatic extraction of multi-structured light stripes},
  volume       = {17},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Multimodal image feature detection with ROI-based
optimization for image registration. <em>JRTIP</em>, <em>17</em>(4),
1007–1013. (<a href="https://doi.org/10.1007/s11554-018-0847-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image registration plays an imperative part of multimodal video analysis system. In video surveillance applications, change in the environmental conditions makes the registration process hard. Use of multiple sensors makes the system more robust to environmental changes as compared to single sensor imaging system. Using multiple modalities such as infrared(IR)/thermal sensors and CMOS image sensors augment the sturdiness of the surveillance system. Here we propose hardware implementation of feature detection on Genesys 2 Kintex-7 FPGA for a multimodal surveillance system, which is robust in poor lighting conditions and affine changes. To reduce the processing time, a region of interest (ROI) is identified and feature extraction is performed in this region. Design optimization in hardware architecture resulted in achieving the real-time performance of image registration on HD 720p video.},
  archive      = {J_JRTIP},
  author       = {Nandalike, Rajesh and Sarojadevi, H.},
  doi          = {10.1007/s11554-018-0847-z},
  journal      = {Journal of Real-Time Image Processing},
  month        = {8},
  number       = {4},
  pages        = {1007-1013},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Multimodal image feature detection with ROI-based optimization for image registration},
  volume       = {17},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). GPU-accelerated 2D OTSU and 2D entropy-based thresholding.
<em>JRTIP</em>, <em>17</em>(4), 993–1005. (<a
href="https://doi.org/10.1007/s11554-018-00848-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image thresholding methods are commonly used to distinguish foreground objects from a background. 2D thresholding methods consider both the value of a pixel and the mean of the pixel’s neighbors, so they are less sensitive to noises than 1D thresholding methods. However, the time complexity increases from $$O(\ell ^2)$$ to $$O(\ell ^4)$$ , where $$\ell$$ is the number of gray levels. This paper proposes a parallel algorithm ( $$O(\ell + \ell \log \ell )$$ ) to accelerate both 2D OTSU and 2D entropy-based thresholding on GPU. By dividing the thresholding methods into seven cascaded parallelizable computational steps, our algorithm performs all the computations on GPU and requires no data transfer between GPU memory and main memory. The time complexity analysis explains the theoretical superiority over the state-of-the-art CPU sequential algorithm (O( $$\ell ^2)$$ ). Experimental results show that our parallel thresholding runs 50 times faster than the sequential one without loss of accuracy.},
  archive      = {J_JRTIP},
  author       = {Zhu, Xianyi and Xiao, Yi and Tan, Guanghua and Zhou, Shizhe and Leung, Chi-Sing and Zheng, Yan},
  doi          = {10.1007/s11554-018-00848-5},
  journal      = {Journal of Real-Time Image Processing},
  month        = {8},
  number       = {4},
  pages        = {993-1005},
  shortjournal = {J. Real-Time Image Process.},
  title        = {GPU-accelerated 2D OTSU and 2D entropy-based thresholding},
  volume       = {17},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Fast background removal of JPEG images based on HSV
polygonal cuts for a foot scanner device. <em>JRTIP</em>,
<em>17</em>(4), 981–992. (<a
href="https://doi.org/10.1007/s11554-019-00850-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Foot scanning devices aim to provide information on a patient’s foot and to help in diagnosing issues to be corrected with orthoses. The Galaxy foot scanner developed by the Aetrex company aims to provide a computer-aided framework to help physicians in their diagnoses. As numerous embedded devices, used for image processing and 3D-reconstruction, it includes cameras which provide JPEG pictures of the object to reconstruct. In this framework, an important step is the segmentation of the image, to isolate the object of interest, but the JPEG compression introduces artifacts which can lower the performance of any segmentation procedure. In this paper, we suggest a model which takes the artifacts stemming from the JPEG compression into account. The pixels are first sorted into layers of pixels with similar value V in the HSV color space, and the background is modeled by a polygon from an additional picture. Segmentation based on the knowledge of the background and the layer to be processed is then performed. Results obtained with the Galaxy foot scanner illustrate that this method provides good results for segmentation, while being sufficiently fast to be implemented for near real-time applications.},
  archive      = {J_JRTIP},
  author       = {Trigano, T. and Bechor, Y.},
  doi          = {10.1007/s11554-019-00850-5},
  journal      = {Journal of Real-Time Image Processing},
  month        = {8},
  number       = {4},
  pages        = {981-992},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Fast background removal of JPEG images based on HSV polygonal cuts for a foot scanner device},
  volume       = {17},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Generation of differential topographic images for surface
inspection of long products. <em>JRTIP</em>, <em>17</em>(4), 967–980.
(<a href="https://doi.org/10.1007/s11554-018-0844-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The current manufacturing industries need efficient quality control systems to ensure their products are free of defects. In most cases, surface inspection is carried out by automatic systems that process 2D images which lack measurable information such as the height or depth of the surface defects. An alternative technology for surface inspection is laser scanning. Using this technique, a 3D representation of a product can be generated and therefore, defects can be easily measured. This paper proposes a real-time algorithm to generate differential topographic images of the surface of a product using laser scanning. The images generated by the proposed method are a flattened representation of the surface of the product which compare it to a perfect-shaped product. In these images, the volumetric defects can be easily segmented and measured using computer vision techniques to fulfill the requirements of the international standards of quality. The proposed algorithm is tested on 500,000 profiles meeting the constraints of real time.},
  archive      = {J_JRTIP},
  author       = {delaCalle, F. J. and García, Daniel and Usamentiaga, Rubén},
  doi          = {10.1007/s11554-018-0844-2},
  journal      = {Journal of Real-Time Image Processing},
  month        = {8},
  number       = {4},
  pages        = {967-980},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Generation of differential topographic images for surface inspection of long products},
  volume       = {17},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Fast 3D image reconstruction by cuboids and 3D charlier’s
moments. <em>JRTIP</em>, <em>17</em>(4), 949–965. (<a
href="https://doi.org/10.1007/s11554-018-0846-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we propose a novel approach to accelerate the processing of 3D images by the discrete orthogonal moments of Charlier. The proposed approach is based on two fundamental notions: The first is the acceleration of the computing time of Charlier discrete orthogonal polynomials and moments in the case of the 3D image using digital filters. The second is the description of the 3D image by a set of cuboids of fixed size instead of individual voxels by decomposing the image by cuboids of small sizes to ensure numerical stability. By applying this method, the 3D Charlier moments are calculated from the cuboids instead of the whole image, as the image processing will be locally in each cuboid. This method allows us to speed up the computation time of the moments and to avoid the problem of propagation of digital errors encountered as well when using of digital filters for 3D images of large sizes. The simulation results show the effectiveness of the proposed method in terms of the computation time of the 3D moments of Charlier and in terms of quality of 3D image.},
  archive      = {J_JRTIP},
  author       = {Karmouni, Hicham and Jahid, Tarik and Sayyouri, Mhamed and El Alami, Rachid and Qjidaa, Hassan},
  doi          = {10.1007/s11554-018-0846-0},
  journal      = {Journal of Real-Time Image Processing},
  month        = {8},
  number       = {4},
  pages        = {949-965},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Fast 3D image reconstruction by cuboids and 3D charlier’s moments},
  volume       = {17},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Highly parallel steered mixture-of-experts rendering at
pixel-level for image and light field data. <em>JRTIP</em>,
<em>17</em>(4), 931–947. (<a
href="https://doi.org/10.1007/s11554-018-0843-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A novel image approximation framework called steered mixture-of-experts (SMoE) was recently presented. SMoE has multiple applications in coding, scale-conversion, and general processing of image modalities. In particular, it has strong potential for coding and streaming higher dimensional image modalities that are necessary to leverage full translational and rotational freedom (6 degrees-of-freedom) in virtual reality for camera captured images. In this paper, we analyze the rendering performance of SMoE for 2D images and 4D light fields. Two different GPU implementations that parallelize the SMoE regression step at pixel-level are presented, including experimental evaluations based on rendering performance and quality. In this paper it is shown that on appropriate hardware, an OpenCL implementation can achieve 85 fps and 22 fps for, respectively, 1080p and 4K renderings of large models with more than 100,000 of Gaussian kernels.},
  archive      = {J_JRTIP},
  author       = {Avramelos, Vasileios and Verhack, Ruben and Saenen, Ignace and Van Wallendael, Glenn and Goossens, Bart and Lambert, Peter},
  doi          = {10.1007/s11554-018-0843-3},
  journal      = {Journal of Real-Time Image Processing},
  month        = {8},
  number       = {4},
  pages        = {931-947},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Highly parallel steered mixture-of-experts rendering at pixel-level for image and light field data},
  volume       = {17},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). An intelligent video analytics model for abnormal event
detection in online surveillance video. <em>JRTIP</em>, <em>17</em>(4),
915–930. (<a href="https://doi.org/10.1007/s11554-018-0840-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {One of the primary tools for monitoring human movement and to prevent unwanted or unintended activities is surveillance camera. Nowadays, security management professional rely heavily on video surveillance to challenge crime and avert negative incidents that impact human society. Large numbers of surveillance camera installations have been increasing dramatically in the private and public sectors to monitor public activities. Video surveillance is one of the most effective methods to guarantee security. Fitting a surveillance camera simply transfers the captured video to security personnel. But the abnormal activities can be identified only by integrating an intelligent system to analyze the videos. Hence, this paper is motivated to design and implement an Intelligent Video Analytics Model (IVAM) also known as Human Object Detection (HOD) method for analyzing and detecting video-based abundant objects and abnormal human activities. IVAM can be deployed along with surveillance cameras in any public places like airport, hospital, shopping malls and railway station to automatically identify any abnormal event. IVAM is experimented with MATLAB software and the results are verified. The performance of IVAM is evaluated by comparing the obtained results with the existing approaches and it is proved that IVAM performs better compared to other contemporary methods in terms of accurately detecting the anomalies with less error rate and high classification accuracy.},
  archive      = {J_JRTIP},
  author       = {Balasundaram, A. and Chellappan, C.},
  doi          = {10.1007/s11554-018-0840-6},
  journal      = {Journal of Real-Time Image Processing},
  month        = {8},
  number       = {4},
  pages        = {915-930},
  shortjournal = {J. Real-Time Image Process.},
  title        = {An intelligent video analytics model for abnormal event detection in online surveillance video},
  volume       = {17},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Fast parallel blur detection on GPU. <em>JRTIP</em>,
<em>17</em>(4), 903–913. (<a
href="https://doi.org/10.1007/s11554-018-0837-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Blur detection, a task to determine whether an image is blurred or not, is very helpful in various applications of image processing and computer vision. In this paper, we propose a novel method to accelerate blur detection algorithms based on Haar wavelet transform. The method decouples data dependency to gain fast 3-level Haar wavelet transform. With the obtained independence, the blur detection steps can be performed in parallel using native GPU thread blocks. We evaluated our proposed method on embedded devices, desktop and server. Our experiments show that on desktop and server, the proposed method obtains a huge performance speedup. On embedded devices, our GPU-based 3-level Haar wavelet transform is up to 4.9 times better performance and 4.3 times better power efficiency than CPU-based blur detection algorithms.},
  archive      = {J_JRTIP},
  author       = {Tran, Giang Son and Nghiem, Thi Phuong and Burie, Jean-Christophe},
  doi          = {10.1007/s11554-018-0837-1},
  journal      = {Journal of Real-Time Image Processing},
  month        = {8},
  number       = {4},
  pages        = {903-913},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Fast parallel blur detection on GPU},
  volume       = {17},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Enhanced multicore–manycore interaction in high-performance
video encoding. <em>JRTIP</em>, <em>17</em>(4), 887–902. (<a
href="https://doi.org/10.1007/s11554-018-0834-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents an efficient cooperative interaction between multicore (CPU) and manycore (GPU) resources in the design of a high-performance video encoder. The proposed technique, applied to the well-established and highly optimized VP8 encoding format, can achieve a significant speed-up with respect to the mostly optimized software encoder (up to $$\times$$ 6), with minimum degradation of the visual quality and low processing latency. This result has been obtained through a highly optimized CPU–GPU interaction, the exploitation of specific GPU features, and a modified search algorithm specifically adapted to the GPU execution model. Several experimental results are reported and discussed, confirming the effectiveness of the proposed technique. The presented approach, though implemented for the VP8 standard, is of general interest, as it could be applied to any other video encoding scheme.},
  archive      = {J_JRTIP},
  author       = {Grossi, Giuliano and Paglierani, Pietro and Pedersini, Federico and Petrini, Alessandro},
  doi          = {10.1007/s11554-018-0834-4},
  journal      = {Journal of Real-Time Image Processing},
  month        = {8},
  number       = {4},
  pages        = {887-902},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Enhanced multicore–manycore interaction in high-performance video encoding},
  volume       = {17},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Fast restoration of geometric details of automobile castings
scanned by RGB-d sensor. <em>JRTIP</em>, <em>17</em>(4), 871–886. (<a
href="https://doi.org/10.1007/s11554-018-0835-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The depth data of automobile castings obtained by RGB-D sensor are usually combined with noise, the classical regularization method can eliminate the noise efficiently. Yet the regularization step is too time-consuming to reconstruct the geometric details of automobile castings efficiently. Given this, we present a fast method called fast restoration of automobile castings (FRAC) to restore the geometric details of automobile castings in fast manner. First, the implicit surface data is extracted from globally aligned RGB-D images, the voxel data structure is extended to index and process the implicit surface in real time. Then, an inverse shading formula is constructed to compute TSDF (truncated signed distance field) values of casting surfaces quickly, and an objective function is designed to optimize the geometric details of casting surfaces in real time. Finally, a GPU-based Gauss–Newton solver is used to accelerate restoration of castings further. The defective casting models scanned by RGB-D sensor are quickly refined to a complete model with better accuracy. Experimental results show that with respect to the sampled automobile castings which include 359,470 points in average, the average optimization time reaches 0.66 s per frame, the average restoration time is about 6.48 s. Computing TSDF requires only about 34.8 MB GPU caches in average. FRAC is able to restore the geometric details of automobile castings in real time.},
  archive      = {J_JRTIP},
  author       = {Lin, Jinhua and Ma, Lin},
  doi          = {10.1007/s11554-018-0835-3},
  journal      = {Journal of Real-Time Image Processing},
  month        = {8},
  number       = {4},
  pages        = {871-886},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Fast restoration of geometric details of automobile castings scanned by RGB-D sensor},
  volume       = {17},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Exploiting architectural features of a computer vision
platform towards reducing memory stalls. <em>JRTIP</em>, <em>17</em>(4),
853–870. (<a href="https://doi.org/10.1007/s11554-018-0830-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Computer vision applications are becoming more and more popular in embedded systems such as drones, robots, tablets, and mobile devices. These applications are both compute and memory intensive, with memory bound stalls (MBS) making a significant part of their execution time. For maximum reduction in memory stalls, compilers need to consider architectural details of a platform and utilize its hardware components efficiently. In this paper, we propose a compiler optimization for a vision-processing system through classification of memory references to reduce MBS. As the proposed optimization is based on the architectural features of a specific platform, i.e., Myriad 2, it can only be applied to other platforms having similar architectural features. The optimization consists of two steps: affinity analysis and affinity-aware instruction scheduling. We suggest two different approaches for affinity analysis, i.e., source code annotation and automated analysis. We use LLVM compiler infrastructure for implementation of the proposed optimization. Application of annotation-based approach on a memory-intensive program shows a reduction in stall cycles by 67.44%, leading to 25.61% improvement in execution time. We use 11 different image-processing benchmarks for evaluation of automated analysis approach. Experimental results show that classification of memory references reduces stall cycles, on average, by 69.83%. As all benchmarks are both compute and memory intensive, we achieve improvement in execution time by up to 30%, with a modest average of 5.79%.},
  archive      = {J_JRTIP},
  author       = {Ul Mustafa, Naveed and O’Riordan, Martin J. and Rogers, Stephen and Ozturk, Ozcan},
  doi          = {10.1007/s11554-018-0830-8},
  journal      = {Journal of Real-Time Image Processing},
  month        = {8},
  number       = {4},
  pages        = {853-870},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Exploiting architectural features of a computer vision platform towards reducing memory stalls},
  volume       = {17},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Fast and accurate line detection with GPU-based least median
of squares. <em>JRTIP</em>, <em>17</em>(4), 839–851. (<a
href="https://doi.org/10.1007/s11554-018-0827-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose an accurate and efficient 2D line detection technique based on the standard Hough transform (SHT) and least median of squares (LMS). We prove our method to be very accurate and robust to noise and occlusions by comparing it with state-of-the-art line detection methods using both qualitative and quantitative experiments. LMS is known as being very robust but also as having high computation complexity. To make our method practical for real-time applications, we propose a parallel algorithm for LMS computation which is based on point-line duality. We also offer a very efficient implementation of this algorithm for GPU on CUDA architecture. Despite many years since LMS methods have first been described and the widespread use of GPU technology in computer vision and image-processing systems, we are unaware of previous work reporting the use of GPUs for LMS and line detection. We measure the computation time of our GPU-accelerated algorithm and prove it is suitable for real-time applications. Our accelerated LMS algorithm is up to 40 times faster than the fastest single-threaded CPU-based implementation of the state-of-the-art sequential algorithm.},
  archive      = {J_JRTIP},
  author       = {Shapira, Gil and Hassner, Tal},
  doi          = {10.1007/s11554-018-0827-3},
  journal      = {Journal of Real-Time Image Processing},
  month        = {8},
  number       = {4},
  pages        = {839-851},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Fast and accurate line detection with GPU-based least median of squares},
  volume       = {17},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Reconstruction of 3D human motion in real-time using
particle swarm optimization with GPU-accelerated fitness function.
<em>JRTIP</em>, <em>17</em>(4), 821–838. (<a
href="https://doi.org/10.1007/s11554-018-0825-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, a novel framework for acceleration of 3D model-based, markerless visual tracking in multi-camera videos is proposed. The objective function being the most computationally demanding part of model-based 3D motion reconstruction is calculated on a GPU. The proposed framework effectively utilizes the rendering power of OpenGL to render the 3D models in the predicted poses, whereas the CUDA threads are used to match such rendered models with the image observations and to perform particle swarm optimization-based tracking. We demonstrate effective parallelization of the particle swarm optimization on GPU. Execution of time-consuming parts of the algorithm on GPU using CUDA-OpenGL significantly accelerates the 3D motion reconstruction, making our method capable of tracking full-body movements with a maximum speed of 15 fps. Qualitative and quantitative experimental results on various four-camera benchmark datasets demonstrate the efficiency and accuracy of our method for real-time motion tracking.},
  archive      = {J_JRTIP},
  author       = {Kwolek, Bogdan and Rymut, Boguslaw},
  doi          = {10.1007/s11554-018-0825-5},
  journal      = {Journal of Real-Time Image Processing},
  month        = {8},
  number       = {4},
  pages        = {821-838},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Reconstruction of 3D human motion in real-time using particle swarm optimization with GPU-accelerated fitness function},
  volume       = {17},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). No-reference real-time video transmission artifact detection
for video signals. <em>JRTIP</em>, <em>17</em>(4), 799–820. (<a
href="https://doi.org/10.1007/s11554-018-0824-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Video signals are a very important part of multimedia applications. Due to limited network bandwidth, video signals are subjected to the compression process, which introduces different compression artifacts. During network transmission, additional artifacts are introduced in video signals due to random bit errors and packet loss (PL). Both mentioned artifact types degrade visual quality of the video signal and thus, it has to be continuously monitored to ensure the required quality of service (QoS) provided to end users. An important component of the video quality monitoring system deals with video transmission artifact detection. In this paper, a no-reference (NR) pixel-based video transmission artifact detection algorithm is proposed, called the packet loss area measure (PLAM) algorithm. When detecting video transmission artifacts, the PLAM algorithm takes into account spatial and temporal information of a video signal. The performance of the proposed PLAM algorithm has been compared to those of the three existing different PL detection algorithms on a broad set of significantly different video signals from two publicly available video databases. One of these databases, called the Referent Packet Loss (RPL) database, has been created within this research and is presented in this paper. The algorithm performance testing results show that PLAM achieves high performance and overcomes other tested algorithms. Furthermore, the results show that the PLAM algorithm is very robust when detecting video transmission artifacts in video signals of different contents, with distinct degradation levels and PL error-concealment methods used in decoder post-processing. Due to its low computational complexity, the PLAM algorithm is capable of processing Full HD and Ultra HD video signals with the frame rate up to 100 and 25 frames per second (fps), respectively, in real time, in the case when high-end CPU is used.},
  archive      = {J_JRTIP},
  author       = {Glavota, Ivan and Kaprocki, Zvonimir and Vranješ, Mario and Herceg, Marijan},
  doi          = {10.1007/s11554-018-0824-6},
  journal      = {Journal of Real-Time Image Processing},
  month        = {8},
  number       = {4},
  pages        = {799-820},
  shortjournal = {J. Real-Time Image Process.},
  title        = {No-reference real-time video transmission artifact detection for video signals},
  volume       = {17},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Parallelism exploration for 3D high-efficiency video coding
depth modeling mode one. <em>JRTIP</em>, <em>17</em>(4), 787–797. (<a
href="https://doi.org/10.1007/s11554-018-0819-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article presents a parallelism exploration over the depth modeling mode 1 (DMM-1) encoding algorithm of the 3D high-efficiency video coding (3D-HEVC) standard and applied the proposed solutions in a multicore central processing unit (CPU) and two graphics processor unities (GPU). The article evaluates efficient parallel algorithms for DMM-1, which also take advantage of simplifications proposed in our previous works. We demonstrate that DMM-1 can obtain a scalable speedup when running in systems with several available cores even when simplifications are being applied. Experimental results for 1920 × 1088 resolution videos show that the proposed parallel algorithms achieved up to 2 frames per second (fps) in a four-cores (with eight threads) CPU and more than 30 fps in two different GPUs. Therefore, the speedup attained with GPU enables real-time 3D-video encoding applying the proposed parallelism strategies together with the DMM-1 proposed simplifications.},
  archive      = {J_JRTIP},
  author       = {Sanchez, Gustavo and Agostini, Luciano and Sousa, Leonel and Marcon, César},
  doi          = {10.1007/s11554-018-0819-3},
  journal      = {Journal of Real-Time Image Processing},
  month        = {8},
  number       = {4},
  pages        = {787-797},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Parallelism exploration for 3D high-efficiency video coding depth modeling mode one},
  volume       = {17},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Security of MVD-based 3D video in 3D-HEVC using data hiding
and encryption. <em>JRTIP</em>, <em>17</em>(4), 773–785. (<a
href="https://doi.org/10.1007/s11554-018-0817-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To safely transmit secret data and protect three-dimensional (3D) videos, a novel jointly data hiding and encryption method for multi-view video plus depth based 3D video is proposed. Both data hiding and encryption are all format complaint for the 3D high-efficiency video coding (3D-HEVC), which obtains a real-time requirement. Since the depth map is not used for viewing but for rendering the virtual view, it is used to embed data by modulating the quantization parameter value of the largest encoding unit (LCU) block. According to the edge information of the depth map and the texture of the colour video, LCU blocks of the depth map are classified into four types. Different types of LCU blocks allow different embedding strength considering the quality of the rendered virtual view and the stable bitrate. Moreover, the colour video is encrypted using codeword substitution for 3D-HEVC format compliance without changing the bitrate. Experimental results demonstrate that the proposed method keeps the good quality of the virtual view after embedding data, protects the video contents efficiently, and has a limited influence on the bitrate.},
  archive      = {J_JRTIP},
  author       = {Luo, Ting and Zuo, Liwen and Jiang, Gangyi and Gao, Wei and Xu, Haiyong and Jiang, Qiuping},
  doi          = {10.1007/s11554-018-0817-5},
  journal      = {Journal of Real-Time Image Processing},
  month        = {8},
  number       = {4},
  pages        = {773-785},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Security of MVD-based 3D video in 3D-HEVC using data hiding and encryption},
  volume       = {17},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Fast deep neural networks for image processing using posits
and ARM scalable vector extension. <em>JRTIP</em>, <em>17</em>(3),
759–771. (<a href="https://doi.org/10.1007/s11554-020-00984-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the advent of image processing and computer vision for automotive under real-time constraints, the need for fast and architecture-optimized arithmetic operations is crucial. Alternative and efficient representations for real numbers are starting to be explored, and among them, the recently introduced posit$$^{\mathrm{TM}}$$ number system is highly promising. Furthermore, with the implementation of the architecture-specific mathematical library thoroughly targeting single-instruction multiple-data (SIMD) engines, the acceleration provided to deep neural networks framework is increasing. In this paper, we present the implementation of some core image processing operations exploiting the posit arithmetic and the ARM scalable vector extension SIMD engine. Moreover, we present applications of real-time image processing to the autonomous driving scenario, presenting benchmarks on the tinyDNN deep neural network (DNN) framework.},
  archive      = {J_JRTIP},
  author       = {Cococcioni, Marco and Rossi, Federico and Ruffaldi, Emanuele and Saponara, Sergio},
  doi          = {10.1007/s11554-020-00984-x},
  journal      = {Journal of Real-Time Image Processing},
  month        = {6},
  number       = {3},
  pages        = {759-771},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Fast deep neural networks for image processing using posits and ARM scalable vector extension},
  volume       = {17},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Smoke vehicle detection based on multi-feature fusion and
hidden markov model. <em>JRTIP</em>, <em>17</em>(3), 745–758. (<a
href="https://doi.org/10.1007/s11554-019-00856-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing smoke vehicle detection methods and vision-based smoke detection methods are vulnerable to false alarms. This paper presents an automatic smoke vehicle detection method based on multi-feature fusion and hidden Markov model (HMM). In this method, we first detect moving objects using an improved visual background extractor (ViBe) algorithm and obtain smoke-colored blocks using color histogram features in the HSI (hue, saturation, and intensity) color space. The adaptive scale local binary pattern (AS-LBP) and the discriminative edge orientation histogram (disEOH) are proposed and combined to characterize the smoke-colored blocks. More specifically, the proposed AS-LBP, a texture feature descriptor, is based on the quadratic fitting of our labelled data to obtain the best scale. The proposed disEOH, a gradient-based feature descriptor, is robust to noise by extracting discriminative edge information using Gaussian filters and principal component analysis (PCA). The discrete cosine transform (DCT) is employed to extract frequency domain information from the region fused by smoke blocks. To utilize the dynamic features, the HMMs are employed to analyze and classify the smoke-colored block sequences and region sequences in continuous frames. The experimental results show that the proposed method achieves better performances than existing smoke detection methods, especially achieves lower false alarms.},
  archive      = {J_JRTIP},
  author       = {Tao, Huanjie and Lu, Xiaobo},
  doi          = {10.1007/s11554-019-00856-z},
  journal      = {Journal of Real-Time Image Processing},
  month        = {6},
  number       = {3},
  pages        = {745-758},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Smoke vehicle detection based on multi-feature fusion and hidden markov model},
  volume       = {17},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Performance and energy-efficient implementation of a smart
city application on FPGAs. <em>JRTIP</em>, <em>17</em>(3), 729–743. (<a
href="https://doi.org/10.1007/s11554-018-0792-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The continuous growth of modern cities and the request for better quality of life, coupled with the increased availability of computing resources, lead to an increased attention to smart city services. Smart cities promise to deliver a better life to their inhabitants while simultaneously reducing resource requirements and pollution. They are thus perceived as a key enabler to sustainable growth. Out of many other issues, one of the major concerns for most cities in the world is traffic, which leads to a huge waste of time and energy, and to increased pollution. To optimize traffic in cities, one of the first steps is to get accurate information in real time about the traffic flows in the city. This can be achieved through the application of automated video analytics to the video streams provided by a set of cameras distributed throughout the city. Image sequence processing can be performed both peripherally and centrally. In this paper, we argue that, since centralized processing has several advantages in terms of availability, maintainability and cost, it is a very promising strategy to enable effective traffic management even in large cities. However, the computational costs are enormous, and thus require an energy-efficient High-Performance Computing approach. Field Programmable Gate Arrays (FPGAs) provide comparable computational resources to CPUs and GPUs, yet require much lower amounts of energy per operation (around 6$$\times$$ and 10$$\times$$ for the application considered in this case study). They are thus preferred resources to reduce both energy supply and cooling costs in the huge datacenters that will be needed by Smart Cities. In this paper, we describe efficient implementations of high-performance algorithms that can process traffic camera image sequences to provide traffic flow information in real-time at a low energy and power cost.},
  archive      = {J_JRTIP},
  author       = {Arif, Arslan and Barrigon, Felipe A. and Gregoretti, Francesco and Iqbal, Javed and Lavagno, Luciano and Lazarescu, Mihai Teodor and Ma, Liang and Palomino, Manuel and Segura, Javier L. L.},
  doi          = {10.1007/s11554-018-0792-x},
  journal      = {Journal of Real-Time Image Processing},
  month        = {6},
  number       = {3},
  pages        = {729-743},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Performance and energy-efficient implementation of a smart city application on FPGAs},
  volume       = {17},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Realization of CUDA-based real-time multi-camera visual SLAM
in embedded systems. <em>JRTIP</em>, <em>17</em>(3), 713–727. (<a
href="https://doi.org/10.1007/s11554-019-00924-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The real-time capability of multi-camera visual simultaneous localization and mapping (SLAM) in embedded systems is vital for robotic autonomous navigation. However, owing to the incredibly time-consuming feature extraction, multi-camera visual SLAM has high computational complexity and is difficult to run in real-time in embedded systems. This study proposes a central processing unit and graphics processing unit (CPU–GPU) combination acceleration strategy for multi-camera visual SLAM to solve the computational complexity problem, improve computational efficiency, and realize real-time running in embedded systems. First, the GPU-based feature extraction acceleration algorithm is introduced for multi-camera visual SLAM to accelerate the time-consuming feature extraction by using compute unified device architecture to parallelize feature extraction algorithm. Then, a CPU-based multi-threading pipelining method that conducts image reading, feature extraction, and tracking concurrently is proposed to improve the computational efficiency of multi-camera visual SLAM by solving the load imbalance problem caused by GPU use and improving the use of computing resources. Extensive experiment results demonstrate that the improved multi-camera visual SLAM has a speed of 15 frames per second in embedded systems and meets the real-time requirement. Moreover, the improved multi-camera visual SLAM is three times faster than the original CPU-based method. Our open-source code can be found online: https://github.com/CASHIPS-ComputerVision.},
  archive      = {J_JRTIP},
  author       = {Li, Jincheng and Deng, Guoqing and Zhang, Wen and Zhang, Chaofan and Wang, Fan and Liu, Yong},
  doi          = {10.1007/s11554-019-00924-4},
  journal      = {Journal of Real-Time Image Processing},
  month        = {6},
  number       = {3},
  pages        = {713-727},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Realization of CUDA-based real-time multi-camera visual SLAM in embedded systems},
  volume       = {17},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A high-dynamic range CMOS camera based on dual-gain
channels. <em>JRTIP</em>, <em>17</em>(3), 703–712. (<a
href="https://doi.org/10.1007/s11554-019-00877-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To overcome the ghosting phenomenon of multi-exposure technology, a new high-dynamic range (HDR) image processing method is proposed in this paper, which combines the features from dual-gain channel images. Further, a complete CMOS camera based on the HDR method is implemented, which produces a real-time HDR live video streams. This camera can capture the details of bright and dark areas in the scene completely with an extended dynamic range up to 95 dB. An ALTERA FPGA is the core processing unit of the entire camera, and it completes all the functional modules of the camera efficiently, including dual-channel video capture, image caching, HDR synthesis and tone mapping. Finally, the real-time HDR video flow has a display resolution of 1920 × 1080 and a frame rate of 60 fps.},
  archive      = {J_JRTIP},
  author       = {Tang, Xiaodong and Qian, Yunsheng and Kong, Xiangyu and Wang, Honggang},
  doi          = {10.1007/s11554-019-00877-8},
  journal      = {Journal of Real-Time Image Processing},
  month        = {6},
  number       = {3},
  pages        = {703-712},
  shortjournal = {J. Real-Time Image Process.},
  title        = {A high-dynamic range CMOS camera based on dual-gain channels},
  volume       = {17},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A low-latency DMM-1 encoder for 3D-HEVC. <em>JRTIP</em>,
<em>17</em>(3), 691–702. (<a
href="https://doi.org/10.1007/s11554-019-00875-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Depth modeling mode 1 (DMM-1) is vital for 3D high-efficiency video coding, since it results in low distortion and is suitable for sharp boundaries. When designing DMM-1 encoders, traditional work used serial methods to calculate all the wedgelets, leading to large latency; they may meet the requirements of 30 fps 1080P 3D video at most, but may be powerless for higher frame rate or 3D video resolution. In this paper, we propose a flexible parallel architecture for DMM-1 encoder. It can simultaneously evaluate all the wedgelets, saving encoding time without increasing distortion; it can also be configured into a partial-parallel architecture to save area. Experiments show that our method can save 33.6–94.3% encoding time for different test schemes. Synthesis results in SMIC 55 nm show that the VLSI design for proposed parallel architecture can meet the requirements of 1080P and higher-resolution video processing in real time.},
  archive      = {J_JRTIP},
  author       = {Du, Gaoming and Cao, Yifan and Li, Zhenmin and Zhang, Duoli and Wang, Li and Song, Yukun and Ouyang, Yiming},
  doi          = {10.1007/s11554-019-00875-w},
  journal      = {Journal of Real-Time Image Processing},
  month        = {6},
  number       = {3},
  pages        = {691-702},
  shortjournal = {J. Real-Time Image Process.},
  title        = {A low-latency DMM-1 encoder for 3D-HEVC},
  volume       = {17},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). HW/SW co-design of a visual SLAM application.
<em>JRTIP</em>, <em>17</em>(3), 667–689. (<a
href="https://doi.org/10.1007/s11554-018-0836-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Vision-based advanced driver assistance systems (ADAS), appeared in the 2000s, are increasingly integrated on-board mass-produced vehicles, as off-the-shelf low-cost cameras are now available. But ADAS implement most of the time-specific and basic functionalities such as lane departure or control of the distance to other vehicles. Integrating accurate localization and mapping functionalities meeting the constraints of ADAS (high-throughput, low-consumption, and small-design footprint) would pave the way towards obstacle detection, identification and tracking on-board vehicles at potential high speed. While the SLAM problem has been widely addressed by the robotics community, very few embedded operational implementations can be found, and they do not meet the ADAS-related constraints. In this paper, we implement the first 3D monocular EKF-SLAM chain on a heterogeneous architecture, on a single System on Chip (SoC), meeting these constraints. In order to do so, we picked up a standard co-design method (Shaout et al. Specification and modeling of hw/sw co-design for heterogeneous embedded systems, 2009) and adapted it to the implementation of potentially any of such complex processing chains. The refined method encompasses a hardware-in-the-loop approach allowing to progressively integrate hardware accelerators on the basis of a systematic rule. We also have designed original hardware accelerators for all the image processing functions involved, and for some algebraic operations involved in the filtering process.},
  archive      = {J_JRTIP},
  author       = {Piat, Jonathan and Fillatreau, Philippe and Tortei, Daniel and Brenot, Francois and Devy, Michel},
  doi          = {10.1007/s11554-018-0836-2},
  journal      = {Journal of Real-Time Image Processing},
  month        = {6},
  number       = {3},
  pages        = {667-689},
  shortjournal = {J. Real-Time Image Process.},
  title        = {HW/SW co-design of a visual SLAM application},
  volume       = {17},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Real-time image carrier generation based on generative
adversarial network and fast object detection. <em>JRTIP</em>,
<em>17</em>(3), 655–665. (<a
href="https://doi.org/10.1007/s11554-020-00969-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image steganography aims to conceal the secret information inside another carrier image. And by embedding the information into the carrier image, the carrier image may suffer certain image distortion. Thus, not only the hiding algorithm should be carefully designed, but also the carrier image should be meticulously selected during the hiding process. This paper follows the idea of creating suitable cover images instead of selecting the ones by presenting a unified architecture which combines real-time object detection based on convolutional neural network, local style transfer using generative adversarial network and steganography together to realize real-time carrier image generation. The object in the carrier image is first detected using a fast object detector and then the detected area is reconstructed through a local generative network. The secret message is embedded into the intermediate generated images during the training process in order to generate an image which is suitable as an image carrier. The experimental results show that the reconstructed stego images are nearly indistinguishable to both human eyes and steganalysis tools. Furthermore, the whole carrier image generation process with GPU implementation can achieve around 5 times faster than the regular CPU implementation which meets the requirement of real-time image processing.},
  archive      = {J_JRTIP},
  author       = {Li, Chuanlong and Sun, Xingming and Zhou, Zhili and Yang, Yimin},
  doi          = {10.1007/s11554-020-00969-w},
  journal      = {Journal of Real-Time Image Processing},
  month        = {6},
  number       = {3},
  pages        = {655-665},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Real-time image carrier generation based on generative adversarial network and fast object detection},
  volume       = {17},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Application of deep learning model based on image definition
in real-time digital image fusion. <em>JRTIP</em>, <em>17</em>(3),
643–654. (<a href="https://doi.org/10.1007/s11554-020-00956-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper focuses on pulse coupled neural network (PCNN) and digital image fusion. Aiming at the existing problems, this paper proposes a real-time deep learning model with dual-channel PCNN fusion algorithm based on image definition. It will also be helpful to digital image forensics. With the integration of the orthogonal color space that conforms to HVS, this algorithm simplifies the traditional PCNN model to a parallel dual-channel adaptive PCNN structure. Also, it can realize the adaptive processing by defining the image definition to be β, the coupled linking coefficient. As the dynamic threshold can be increased exponentially with this method, it can effectively solve the problems. The experimental result proves that our algorithm outperforms the traditional fusion algorithms according to the subjective visual effect or the objective assessment standard.},
  archive      = {J_JRTIP},
  author       = {Zhou, Hui and Peng, Jianhua and Liao, Changwu and Li, Jue},
  doi          = {10.1007/s11554-020-00956-1},
  journal      = {Journal of Real-Time Image Processing},
  month        = {6},
  number       = {3},
  pages        = {643-654},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Application of deep learning model based on image definition in real-time digital image fusion},
  volume       = {17},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Real-time attacks on robust watermarking tools in the wild
by CNN. <em>JRTIP</em>, <em>17</em>(3), 631–641. (<a
href="https://doi.org/10.1007/s11554-020-00941-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Robust watermarking is a widely used technology to protect image copyright. Robustness, the ability to resist various distortions, is the most important property of robust watermarking algorithm. So to improve the robustness of the watermarking schemes, watermark attacking algorithms also attract much attention. Far from now, the existing watermarking attack methods cannot well balance the removal ability and visual quality. To address this issue, this paper proposed a removal attack by a convolutional neural network (CNN). Considering the speed requirements of real-time attack applications, for short computing time, we use a simple but powerful CNN. According to the amount of knowledge of watermarking, a corresponding dataset of watermark images is constructed. After that, the CNN model is trained to remove watermark with these datasets. The experiments show that the trained model can not only effectively remove the watermark, but also recover the original image without much image quality degradation.},
  archive      = {J_JRTIP},
  author       = {Geng, Linfeng and Zhang, Weiming and Chen, Haozhe and Fang, Han and Yu, Nenghai},
  doi          = {10.1007/s11554-020-00941-8},
  journal      = {Journal of Real-Time Image Processing},
  month        = {6},
  number       = {3},
  pages        = {631-641},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Real-time attacks on robust watermarking tools in the wild by CNN},
  volume       = {17},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Algorithm optimization and hardware implementation for merge
mode in HEVC. <em>JRTIP</em>, <em>17</em>(3), 623–630. (<a
href="https://doi.org/10.1007/s11554-018-0818-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Merge mode is a new tool for improving inter-frame coding efficiency in high-efficiency video coding. This tool can save the bitrate for the motion vector by sharing this vector with neighboring blocks. Merge is a process that selects a candidate motion vector by calculating the cost of rate-distortion. However, this process requires a large number of complex computations and memory access, thereby resulting in the low efficiency of hardware implementation. This paper proposes a new Merge candidate decision scheme that determines the most favorable Merge candidate from a full list of candidates by comparing the sum of absolute transformed difference with the weighted header bit instead of performing a complex calculation for sum of squared difference and entropy coding process in HM16.7. The simulation results show that the performance of the proposed algorithm is close to that of HM16.7 and increases the BD-rate only by 0.22–1.21%. The multilevel pipelines architecture is also exploited in the hardware design. The weighted header bit operation is performed by using the look-up table, which reduces both the complexity and encoding clock cycle. The designed system is implemented with a register transfer level code. The synthesis results from the Design Compiler show that compared with other architecture, the proposed architecture offers great advantages in resource utilization and can process 1920 × 1080 at 353 frame/s for P-slices with a clock frequency of 1057 MHz and logic gate count of 285.2 K.},
  archive      = {J_JRTIP},
  author       = {Shi, Long-zhao and Gao, Xiaohong and Yang, Xiuzhi and Chen, Zhifeng and Zheng, Mingkui},
  doi          = {10.1007/s11554-018-0818-4},
  journal      = {Journal of Real-Time Image Processing},
  month        = {6},
  number       = {3},
  pages        = {623-630},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Algorithm optimization and hardware implementation for merge mode in HEVC},
  volume       = {17},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A fast image dehazing method that does not introduce color
artifacts. <em>JRTIP</em>, <em>17</em>(3), 607–622. (<a
href="https://doi.org/10.1007/s11554-018-0816-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a method for color dehazing with four main characteristics: it does not introduce color artifacts, it does not depend on inverting any physical equation, it is based on models of visual perception, and it is fast, potentially real time. Our method converts the original input image to the HSV color space and works in the saturation and value domains by: (1) reducing the value component via a global constrained histogram flattening; (2) modifying the saturation component in consistency with the previous reduced value; and (3) performing a local contrast enhancement in the value component. Results show that our method competes with the state-of-the-art when dealing with standard hazy images, and outperforms it when dealing with challenging haze cases. Furthermore, our method is able to dehaze a FullHD image on a GPU in 90 ms.},
  archive      = {J_JRTIP},
  author       = {Vazquez-Corral, Javier and Galdran, Adrian and Cyriac, Praveen and Bertalmío, Marcelo},
  doi          = {10.1007/s11554-018-0816-6},
  journal      = {Journal of Real-Time Image Processing},
  month        = {6},
  number       = {3},
  pages        = {607-622},
  shortjournal = {J. Real-Time Image Process.},
  title        = {A fast image dehazing method that does not introduce color artifacts},
  volume       = {17},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A two-stage method to improve the quality of quantized
images. <em>JRTIP</em>, <em>17</em>(3), 581–605. (<a
href="https://doi.org/10.1007/s11554-018-0814-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article proposes a color quantization strategy that combines two color quantization methods: Binary Splitting and Ant tree for Color Quantization. This solution combines a splitting method, which is faster, and a clustering-based method, which generates better quantized images. Given that time is a fundamental factor when considering a method for real-time applications, the proposed strategy attempts to exploit both of these methods for obtaining good quantized images with a low computational cost. The result of this approach not only generates better images than when Binary Splitting and Ant tree for Color Quantization are applied separately, but also helps to improve other methods frequently used for color quantization such as Wu’s method, Octree, Variance-based method and Neuquant.},
  archive      = {J_JRTIP},
  author       = {Pérez-Delgado, María-Luisa and Román Gallego, Jesús-Ángel},
  doi          = {10.1007/s11554-018-0814-8},
  journal      = {Journal of Real-Time Image Processing},
  month        = {6},
  number       = {3},
  pages        = {581-605},
  shortjournal = {J. Real-Time Image Process.},
  title        = {A two-stage method to improve the quality of quantized images},
  volume       = {17},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Reduction of intra-coding time for HEVC based on temporary
direction map. <em>JRTIP</em>, <em>17</em>(3), 567–579. (<a
href="https://doi.org/10.1007/s11554-018-0815-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The high-efficiency video coding (HEVC) standard uses 35 intra-prediction modes for 2N × 2N (N is an integer number ranging from six to two) luma blocks and five modes for chroma blocks. To find the luma block with the minimum rate–distortion, it must perform 11935 different rate–distortion cost calculations. Although this approach improves coding efficiency compared to the previous standards such as H.264/AVC, computational complexity is increased significantly. In this paper, an intra-prediction technique has been described to improve the performance of the HEVC standard by minimizing its computational complexity. The proposed algorithm consists of two stages. The first stage, called prediction unit size decision (PUSD) was introduced to decrease evaluation of prediction unit sizes by ~ 38%. The second stage called prediction mode fast decision (PMFD) was developed to minimize the number of modes in the rough mode decision (RMD) stage. The simulation results show that the time complexity is decreased by ~ 47%, while the BD rate is increased by 1.08%, and PSNR is decreased by 0.04 db. Accordingly, the proposed algorithms have a negligible effect on the video quality with great saving in the time complexity.},
  archive      = {J_JRTIP},
  author       = {Heidari, Behnam and Ramezanpour, Mohammadreza},
  doi          = {10.1007/s11554-018-0815-7},
  journal      = {Journal of Real-Time Image Processing},
  month        = {6},
  number       = {3},
  pages        = {567-579},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Reduction of intra-coding time for HEVC based on temporary direction map},
  volume       = {17},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Color HDR video processing architecture for smart camera.
<em>JRTIP</em>, <em>17</em>(3), 555–566. (<a
href="https://doi.org/10.1007/s11554-018-0810-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a novel FPGA architecture of high dynamic range (HDR) video processing pipeline, based on the capturing of a sequence of differently exposed images. An acquisition process enabling multi-exposure HDR as well as fast implementation of local tone mapping operator involving bilateral filtering is proposed. The HDR acquisition process is enhanced by the application of novel deghosting method, which is dedicated for hardware implementation and proposed in this paper. The hardware processing pipeline is designed with regards to efficiency and performance and the calculations are performed in fixed point arithmetic. The pipeline is suitable for programmable hardware (FPGA—Field Programmable Gate Arrays) implementation and it achieves real-time performance on full HD HDR video which overcomes state-of-the-art solutions that use local tone mapping and deghosting algorithm.},
  archive      = {J_JRTIP},
  author       = {Nosko, Svetozar and Musil, Martin and Zemcik, Pavel and Juranek, Roman},
  doi          = {10.1007/s11554-018-0810-z},
  journal      = {Journal of Real-Time Image Processing},
  month        = {6},
  number       = {3},
  pages        = {555-566},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Color HDR video processing architecture for smart camera},
  volume       = {17},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Real-time ultrasound image reconstruction as an inverse
problem on a GPU. <em>JRTIP</em>, <em>17</em>(3), 543–554. (<a
href="https://doi.org/10.1007/s11554-018-0806-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ultrasonic image reconstruction methods based on inverse problems have been shown to produce sharp, high-quality images using more information about the acquisition process in its processing. This improved reconstruction has high computational cost, usually requiring to solve large systems and making real-time imaging very difficult. Parallelizing the reconstruction using graphics processing units (GPU) can significantly accelerate this processing, but the amount of memory needed by current system models is high for current GPU capacity. This paper presents a new system model to halve this memory requirement; it exploits the symmetry of the point spread functions (PSF) of the system matrix that occurs when symmetric transducers are used for acquisition. In this case, only one of the two symmetric PSFs needs to be stored; the other function is produced by reordering the stored one. Thus, we can reconstruct ultrasound images that are twice as large, making real-time reconstruction on a GPU possible for this application.},
  archive      = {J_JRTIP},
  author       = {Bueno, Paulo R. and Zibetti, Marcelo V. W. and Maia, Joaquim M.},
  doi          = {10.1007/s11554-018-0806-8},
  journal      = {Journal of Real-Time Image Processing},
  month        = {6},
  number       = {3},
  pages        = {543-554},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Real-time ultrasound image reconstruction as an inverse problem on a GPU},
  volume       = {17},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Parallel computation of watershed transform in weighted
graphs on shared memory machines. <em>JRTIP</em>, <em>17</em>(3),
527–542. (<a href="https://doi.org/10.1007/s11554-018-0804-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Watershed Transform is a widely used image segmentation technique that is known to be very data intensive and time consuming. The M-border Kernel Algorithm computes watersheds in the framework of Edge-Weighted Graphs and allows to preserve the topology of the initial map. Parallelization represents an effective solution to accelerate it. However, this task remains challenging due to the nature of this technique. In this paper, we address this problem. We start by analyzing the data dependency issues that this algorithm raises when dealing with parallel execution. With respect to that, we propose a parallelization strategy that opts for vertex scanning instead of edges scanning of the graph while preserving the thinning paradigm on which the M-border Kernel Algorithm is based. We show that this strategy overcomes the problem of the simultaneous lowering of two adjacent M-border edges that may occur when edge scan is used. The implementation of the proposed algorithm on a shared memory multicore architecture proves its effectiveness in terms of speedup. In fact, the experimental results show that a speedup factor of 5.55 is achieved using eight processors for $$2048 \times 2048$$ images over the performance of the sequential algorithm using a single processor on the same architecture. Furthermore, the gain in terms of execution time and thus speedup is guaranteed whatever is the size of images on which the algorithm is applied. In fact, a speedup factor of 5.55 is obtained for $$2048 \times 2048$$ images, 5.11 for $$1024 \times 1024$$ images and 4.45 for $$512 \times 512$$ images using eight cores.},
  archive      = {J_JRTIP},
  author       = {Braham, Yosra and Elloumi, Yaroub and Akil, Mohamed and Bedoui, Mohamed Hedi},
  doi          = {10.1007/s11554-018-0804-x},
  journal      = {Journal of Real-Time Image Processing},
  month        = {6},
  number       = {3},
  pages        = {527-542},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Parallel computation of watershed transform in weighted graphs on shared memory machines},
  volume       = {17},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Fast guided filter for power-efficient real-time 1080p
streaming video processing. <em>JRTIP</em>, <em>17</em>(3), 511–525. (<a
href="https://doi.org/10.1007/s11554-018-0802-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the advent of embedded vision systems, smart sensors with integrated image signal processing (ISP) become a hot topic. This poses a need for efficient hardware implementation, regarding resource utilization and power consumption, of core image processing algorithms. Power consumption is especially important, since many of the target devices are usually battery operated. Edge-aware filtering, although it is used in many core image processing algorithms, is still challenging operation, especially in cases where large kernels are needed. In this paper, efficient hardware realization of fast guided filter (FGF) is proposed. It is based on idea that large filter of size $$R=K \cdot S$$ can be calculated by downsampling input image by factor S and using filter of size K. Besides reduced memory and logic requirements, this optimization enables that, for the scaling factor S, core processing is done at $$1/S^{2}$$ pixel clock, providing significantly lower power consumption. Experimental results on Cyclone V FPGA chip demonstrate that, for FGF of size $$35 \times 35$$ with downsampling factor $$S=7$$, the proposed design achieves 60 fps for 1080p video. Memory utilization is 147.3 kB without need for any off-chip memory. Core dynamic power consumption is 79.89 mW. Proposed design consumes less total power than state-of-the-art guided filter realizations including ASIC-based solutions. This module can be seamlessly integrated into smart sensors ISP units, because it is designed for power-efficient streaming processing.},
  archive      = {J_JRTIP},
  author       = {El Mezeni, Dragomir and Saranovac, Lazar},
  doi          = {10.1007/s11554-018-0802-z},
  journal      = {Journal of Real-Time Image Processing},
  month        = {6},
  number       = {3},
  pages        = {511-525},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Fast guided filter for power-efficient real-time 1080p streaming video processing},
  volume       = {17},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Fast intra coding unit partition decision in h.266/FVC based
on spatial features. <em>JRTIP</em>, <em>17</em>(3), 493–510. (<a
href="https://doi.org/10.1007/s11554-018-0794-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the development of technology, the requirements of hardware equipment and user expectations of visual enjoyment are increasingly gradually. The Joint Video Exploration Team (JVET) has established the latest video compression standard, Future Video Coding (FVC). FVC adopts QuadTree plus Binary Tree (QTBT) based Coding Unit (CU) structure, which not only removes the complex hierarchical structure of the CU, Prediction Unit (PU), and Transform Unit (TU) but also supports square and rectangular coding blocks based on the texture of the video content. Although the QTBT structure can provide superior coding performance, it significantly increases the encoding time, particularly in intra coding. Therefore, developing a fast intra CU partition decision algorithm is essential. In this paper, a fast CU partition decision algorithm in FVC intra coding based on spatial features is proposed. Different spatial features in the pixel domain are proposed in the binary tree and quadtree decision processes. Spatial features for the binary tree are employed for early skipping of the encoding process of CUs with binary tree depth and for early determination of binary tree split mode. Spatial features for the quadtree are employed for early splitting or termination of CUs with quadtree depth. Compared with JEM 5.0, the proposed method can save 23% encoding time on average with a slight increase of 0.62% in the Bjontegaard delta bitrate (BDBR).},
  archive      = {J_JRTIP},
  author       = {Lin, Ting-Lan and Jiang, Hui-Yu and Huang, Jing-Ya and Chang, Pao-Chi},
  doi          = {10.1007/s11554-018-0794-8},
  journal      = {Journal of Real-Time Image Processing},
  month        = {6},
  number       = {3},
  pages        = {493-510},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Fast intra coding unit partition decision in H.266/FVC based on spatial features},
  volume       = {17},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Real-time shadow detection using multi-channel binarization
and noise removal. <em>JRTIP</em>, <em>17</em>(3), 479–492. (<a
href="https://doi.org/10.1007/s11554-018-0799-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {High-quality automatic shadow detection remains a challenging problem in image processing and computer vision. Existing techniques for shadow detection typically make use of deep learning strategies to obtain accurate shadow detection results, at the cost of demanding high processing time, making their use unsuitable for augmented reality and robotic applications. In this paper, we propose a novel approach to perform high-quality shadow detection in real time. To do so, we convert an input image into different color spaces to perform multi-channel binarization and detect different shadow regions in the image. Then, a filtering algorithm is proposed to remove the noisy false-positive shadow regions on the basis of their sizes. Experimental results evaluated in two different datasets show that the proposed approach may run entirely on the GPU, requiring only $$\approx$$ 13 ms to detect shadows in an image with $$3840 \times 2160$$ (4k) resolution. That makes our approach about 1.8 (66$$\times$$) to 4.6 (37,284$$\times$$) orders of magnitude faster than related work for 4k resolution images, at the cost of only $$\approx$$ 5% of accuracy loss compared to the best results achieved for each dataset.},
  archive      = {J_JRTIP},
  author       = {Macedo, Márcio C. F. and Nascimento, Verônica P. and Souza, Antonio C. S.},
  doi          = {10.1007/s11554-018-0799-3},
  journal      = {Journal of Real-Time Image Processing},
  month        = {6},
  number       = {3},
  pages        = {479-492},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Real-time shadow detection using multi-channel binarization and noise removal},
  volume       = {17},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Performance optimization of rotation-tolerant
viola–jones-based blackbird detection. <em>JRTIP</em>, <em>17</em>(3),
471–478. (<a href="https://doi.org/10.1007/s11554-018-0795-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The research described in this paper investigates the rotational robustness of the Viola–Jones algorithm (VJA) object detection method when used for red-winged blackbird (Agelaius phoeniceus) detection. VJA has been successfully used for face detection, but can be adapted to detect a variety of objects. This work uses the histogram of oriented gradients (HOG) descriptor to train the blackbird classifier. Since VJA object detection is inherently not invariant to in-plane object rotation, additional effort is required during training and detection. The proposed method extends the object detection framework developed by Viola and Jones to efficiently handle rotated blackbirds and provide a balance between detection accuracy and computation cost.},
  archive      = {J_JRTIP},
  author       = {Jalil, Nauman and Smith, Scott C. and Green, Roger},
  doi          = {10.1007/s11554-018-0795-7},
  journal      = {Journal of Real-Time Image Processing},
  month        = {6},
  number       = {3},
  pages        = {471-478},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Performance optimization of rotation-tolerant Viola–Jones-based blackbird detection},
  volume       = {17},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Real-time impulse noise removal. <em>JRTIP</em>,
<em>17</em>(3), 459–469. (<a
href="https://doi.org/10.1007/s11554-018-0791-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {An adaptive interpolation-based impulse noise removal (AIBINR) algorithm is proposed to remove impulse noise from color and gray-scale images in real time. AIBINR works fast and has no need for parameter tuning to remove fixed-valued impulse noise. A GPU application has been developed to demonstrate the speed and inherent parallelization capabilities of the proposed method. Using the high-speed implementation, we have shown that AIBINR can denoise color images fast enough to be used in real-time video denoising, while having comparable denoising performance when compared to the state-of-the-art methods without any modification to its parameters.},
  archive      = {J_JRTIP},
  author       = {Gökcen, Alpaslan and Kalyoncu, Cem},
  doi          = {10.1007/s11554-018-0791-y},
  journal      = {Journal of Real-Time Image Processing},
  month        = {6},
  number       = {3},
  pages        = {459-469},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Real-time impulse noise removal},
  volume       = {17},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Parallel binocular stereo-vision-based GPU accelerated
pedestrian detection and distance computation. <em>JRTIP</em>,
<em>17</em>(3), 447–457. (<a
href="https://doi.org/10.1007/s11554-018-0783-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Pedestrian detection has become a very hot research field in computer vision, because it is widely used in many practical applications. However, the real-time requirement of these applications is a great challenge for pedestrian detection. To address this problem, this paper accelerates the pedestrian detection in parallel using NVIDIA’s Graphics Processing Units (GPUs). In addition, we developed a distance estimation system based on the results of the pedestrian detection, which aims to obtain the distance between the pedestrians and the camera. The whole system including pedestrian detection and distance estimation is for embedded applications. The method of pedestrian detection is to combine the Histogram of Oriented Gradients (HOG) feature with the cascade classifier, and the distance estimation system is built by utilizing a parallel binocular vision system. The performance of the parallel implementation of the whole system is tested on two kinds of different GPUs, an embedded board Jetson TK1 and a Tesla K80 GPU specialized for science computation. The speed of the whole system on Jetson TK1 over 640 × 480 images is about 16 fps, which basically reaches the real-time requirement, and the speed on Tesla K80 over 640 × 480 images is much higher, about 86 fps.},
  archive      = {J_JRTIP},
  author       = {Li, Jiaojiao and Wu, Jiaji and You, Yang and Jeon, Gwanggil},
  doi          = {10.1007/s11554-018-0783-y},
  journal      = {Journal of Real-Time Image Processing},
  month        = {6},
  number       = {3},
  pages        = {447-457},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Parallel binocular stereo-vision-based GPU accelerated pedestrian detection and distance computation},
  volume       = {17},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Real-time drogue detection and template tracking strategy
for autonomous aerial refueling. <em>JRTIP</em>, <em>17</em>(3),
437–446. (<a href="https://doi.org/10.1007/s11554-018-0787-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Autonomous aerial refueling technology is an effective solution to extend flight duration of unmanned aerial vehicles, and also a great challenge due to its high risk. A novel real-time drogue detection and tracking strategy of monocular vision system for autonomous aerial refueling is proposed. It uses a direct image registration-based tracking method to ensure reliable and real-time tracking, and an ROI detection based on edge features to solve the tracking drift problem. A multiply patches fusion structure is adopted in the tracking method to improve the tracking accuracy and slow the divergence speed. Finally, various experiments are conducted to validate the proposed image processing strategy. These results show that the proposed strategy obtains a high accuracy as well as the real-time performance, and achieves a better performance than state-of-the-art methods.},
  archive      = {J_JRTIP},
  author       = {Huang, Bin and Sun, Yongrong and Zeng, Qinghua},
  doi          = {10.1007/s11554-018-0787-7},
  journal      = {Journal of Real-Time Image Processing},
  month        = {6},
  number       = {3},
  pages        = {437-446},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Real-time drogue detection and template tracking strategy for autonomous aerial refueling},
  volume       = {17},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). An improvement of multi-scale covariance descriptor for
embedded system. <em>JRTIP</em>, <em>17</em>(3), 419–435. (<a
href="https://doi.org/10.1007/s11554-018-0759-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Video surveillance has been a major area of focus for researchers and engineers. Actually, video surveillance includes several useful and complex tasks such as tracking, human detection, re-identification and recognition. Multi-scale covariance (MSCOV) descriptor has recently grown in interest due to its good performances for person detection, re-identification and matching. Unfortunately, its original version requires heavy computations, and it is difficult to be executed in real time on embedded systems. This paper presents two aspects of improvement to adapt the MSCOV descriptor for embedded systems. First, the local binary pattern (LBP) features are introduced and a trade-off between accuracy and processing cost is used to define the best features combination. Second, parallel implementation and embedded co-processor are exploited to accelerate processing time on multi-core CPU architectures. Both optimizations are implemented and evaluated for executing a complete application of person re-identification systems. The software implementation is performed using the VIPeR dataset. Using LBP, 21.57% processing speed-up and 50% less memory requirements for the descriptor computation are achieved without any accuracy performance degradation. We also prototype the proposed design using Zynq platform based on ARM Cortex-A9. The results demonstrate the effectiveness of the parallelization and conduct more than 11 times processing speed-up against the original algorithm.},
  archive      = {J_JRTIP},
  author       = {Abid, Nesrine and Loukil, Kais and Ouni, Tarek and Ayedi, Walid and Ammari, Ahmed Chiheb and Abid, Mohamed},
  doi          = {10.1007/s11554-018-0759-y},
  journal      = {Journal of Real-Time Image Processing},
  month        = {6},
  number       = {3},
  pages        = {419-435},
  shortjournal = {J. Real-Time Image Process.},
  title        = {An improvement of multi-scale covariance descriptor for embedded system},
  volume       = {17},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Journal of real-time image processing: Third issue of volume
17. <em>JRTIP</em>, <em>17</em>(3), 417. (<a
href="https://doi.org/10.1007/s11554-020-00981-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_JRTIP},
  author       = {Kehtarnavaz, Nasser and Carlsohn, Matthias F.},
  doi          = {10.1007/s11554-020-00981-0},
  journal      = {Journal of Real-Time Image Processing},
  month        = {6},
  number       = {3},
  pages        = {417},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Journal of real-time image processing: Third issue of volume 17},
  volume       = {17},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Stereo vision architecture for heterogeneous
systems-on-chip. <em>JRTIP</em>, <em>17</em>(2), 393–415. (<a
href="https://doi.org/10.1007/s11554-018-0782-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Stereo vision is a crucial operation in state-of-the-art computer vision applications. Several efficient algorithms have been recently proposed either to increase the quality of the produced disparity maps or to reach higher computational speed, or both. Among them, hardware-oriented algorithms are desirable when the main objective is including stereo vision in portable consumer electronic and multimedia systems. Modern FPGA-based platforms allow all-programmable heterogeneous embedded systems to be realized as SoCs and they are certainly appropriate also to implement stereo vision. This paper proposes a novel stereo vision algorithm and a specific implementation suitable for heterogeneous SoC FPGA-based embedded systems. A complete embedded system design is also demonstrated using the Xilinx Zynq-7000 SoCs device family. The novel algorithm has been characterized in terms of accuracy by referring to the benchmark sets Middlebury and Kitti. When 640 × 480 8-bit greyscale stereo pairs are processed, the fastest prototype, realized within the XC7Z020 and the XC7Z045 device, respectively, exhibits an 81 and 101 fps frame rate. Conversely, the cheapest implementation occupies only 52691 LUTs, 59715 FFs, 93 DSPs, 40 BRAM18k and 6 BRAM36k memory blocks. In comparison to several counterparts existing in literature, the novel algorithm can achieve higher accuracies, and makes the proposed complete system design able to reach the most favourable accuracy/performance trade-off.},
  archive      = {J_JRTIP},
  author       = {Perri, Stefania and Frustaci, Fabio and Spagnolo, Fanny and Corsonello, Pasquale},
  doi          = {10.1007/s11554-018-0782-z},
  journal      = {Journal of Real-Time Image Processing},
  month        = {4},
  number       = {2},
  pages        = {393-415},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Stereo vision architecture for heterogeneous systems-on-chip},
  volume       = {17},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Image analyzer for stereoscopic camera rig alignment.
<em>JRTIP</em>, <em>17</em>(2), 383–391. (<a
href="https://doi.org/10.1007/s11554-018-0779-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The paper presents a versatile solution facilitating calibration of stereoscopic camera rigs for 3D cinematography and machine vision. Manual calibration of the rig and the camera can easily take several hours. The proposed device eases this process by providing the camera operator with several predefined analyses of the images from the cameras. The Image Analyzer is a compact stand-alone device designed for portable 19″ racks. Almost all video processing is performed on a modern Xilinx FPGA. It is supported by an ARM computer which provides control and video streaming over the Ethernet. The article presents its hardware, firmware and software architectures. The main focus is put on the image processing system implemented in the FPGA.},
  archive      = {J_JRTIP},
  author       = {Mielczarek, Aleksander and Makowski, Dariusz and Perek, Piotr and Plewiński, Paweł and Szubert, Aleksander and Napieralski, Andrzej},
  doi          = {10.1007/s11554-018-0779-7},
  journal      = {Journal of Real-Time Image Processing},
  month        = {4},
  number       = {2},
  pages        = {383-391},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Image analyzer for stereoscopic camera rig alignment},
  volume       = {17},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). All-hardware SIFT implementation for real-time VGA images
feature extraction. <em>JRTIP</em>, <em>17</em>(2), 371–382. (<a
href="https://doi.org/10.1007/s11554-018-0781-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a real time hardware implementation of the scale invariant feature transform (SIFT) algorithm. To achieve real time requirements, pipeline structures have been widely exploited both in the keypoint extraction and in the descriptor generation stages. Simplifications to the original algorithm have been also applied to allow a simpler hardware implementation. The proposed architecture has been synthesized on a Xilinx Virtex 5 FPGA. It generates 3072 descriptor vectors for VGA images at 99 frames per second at a clock rate of 100 MHz.},
  archive      = {J_JRTIP},
  author       = {Doménech-Asensi, Ginés and Zapata-Pérez, Juan and Ruiz-Merino, Ramón and López-Alcantud, José Alejandro and Díaz-Madrid, José Ángel and Brea, Víctor Manuel and López, Paula},
  doi          = {10.1007/s11554-018-0781-0},
  journal      = {Journal of Real-Time Image Processing},
  month        = {4},
  number       = {2},
  pages        = {371-382},
  shortjournal = {J. Real-Time Image Process.},
  title        = {All-hardware SIFT implementation for real-time VGA images feature extraction},
  volume       = {17},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). FPGA implementation of optimized karhunen–loeve transform
for image processing applications. <em>JRTIP</em>, <em>17</em>(2),
357–370. (<a href="https://doi.org/10.1007/s11554-018-0776-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The various transformation techniques play vital role in the field of Digital Image Processing. In this paper, we propose FPGA implementation of optimized Karhunen–Loeve transform for image processing applications. The Data Format Conversion block is introduced to represent the input data to suitable format and are fed to the Covariance computation block to calculate corresponding covariance values with accuracy. The Optimized Square Root block has been designed in the Eigenvalue computation block to obtain eigenvalues which are in turn fed to the Eigenvector computation block to produce eigenvectors using Modified divider. Further the Karhunen–Loeve Transformed matrix of the input data is obtained by performing multiplication of eigenvectors with covariance values in the matrix multiplication block. The errors are introduced due to fixed point binary calculations and are minimized by novel Error correction block. The proposed architecture is tested on Sparan-6 (XC6SLX45-3CSG324) FPGA board. The performance of the architecture is compared with respect to hardware utilization and accuracy of various existing techniques to prove the efficiency.},
  archive      = {J_JRTIP},
  author       = {Bhairannawar, Satish S. and Sarkar, Sayantam and Raja, K. B.},
  doi          = {10.1007/s11554-018-0776-x},
  journal      = {Journal of Real-Time Image Processing},
  month        = {4},
  number       = {2},
  pages        = {357-370},
  shortjournal = {J. Real-Time Image Process.},
  title        = {FPGA implementation of optimized Karhunen–Loeve transform for image processing applications},
  volume       = {17},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Online action recognition from RGB-d cameras based on
reduced basis decomposition. <em>JRTIP</em>, <em>17</em>(2), 341–356.
(<a href="https://doi.org/10.1007/s11554-018-0778-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Human action recognition from RGB-D cameras has recently become one of the major fields of research. While accuracy improvement was given more importance in previous action/gesture recognition methods, there are opportunities to work on improving the computational efficiency too. This paper introduces an efficient dimensionality reduction technique and classification mechanism to recognize actions from depth motion map features. For our proposed work, a recently introduced technique called reduced basis decomposition (RBD) is employed, which manages faster dimensional reduction with its unique mechanism of generating compressed basis vectors. The RBD has an offline error-determination and an online approximation mechanism, and it is faster than PCA/SVD. For classification, this paper employs a Probabilistic Collaborative Representation Classifier (Pro-CRC). The recommended classifier works based on probability in connection with $${l_2}$$-regularization. The combined effect of the methods above helps in achieving the state-of-the-art efficiency. In the standard protocol tests carried out in the MSR-Action3D dataset, our proposed method achieved a considerable accuracy of 91.7% which is better than the currently efficient method. Further, our proposed method also proved its effectiveness in the challenging, subject-generic test with a reported accuracy of 89.64% and an average accuracy of 85.70% in the cross fixed tests which included 252 combinations of all the subjects without repetition.},
  archive      = {J_JRTIP},
  author       = {Arunraj, Muniandi and Srinivasan, Andy and Vimala Juliet, A.},
  doi          = {10.1007/s11554-018-0778-8},
  journal      = {Journal of Real-Time Image Processing},
  month        = {4},
  number       = {2},
  pages        = {341-356},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Online action recognition from RGB-D cameras based on reduced basis decomposition},
  volume       = {17},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Efficient approximate core transform and its reconfigurable
architectures for HEVC. <em>JRTIP</em>, <em>17</em>(2), 329–339. (<a
href="https://doi.org/10.1007/s11554-018-0768-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper describes a new approximate transform for the high efficiency video coding (HEVC). A 8 × 8 discrete cosine transform (DCT) approximation is proposed and then down-sampled or expanded to generate the 4 × 4, 16 × 16, and 32 × 32 approximate matrices. The proposed 8 × 8 approximation is carried out in part by neighbourhood in order to take the advantage of adjacent pixels correlation of natural images. Hence, rather than approximating the odd basis vectors of DCT kernel by referring to their intrinsic values, we choose to quantize that by taking into account their signs and positions. The proposed approximation matrices respect the properties of transform matrices prescribed by HEVC like orthogonality and bit-length of the basis vector elements. Furthermore, they have nearly the same arithmetic complexity and hardware requirement as those of recently proposed related methods, but involve significantly less error energy. Moreover, a reconfigurable design based on the 8 × 8 approximation transform is proposed in order to allow the simultaneous computation of eight 4-, four 8-, two 16-, or one 32-point approximate DCTs. It is found that the reconfigurable design can involve nearly 26% less area-delay product (ADP) when compared with the separate non-reconfigurable designs. Experimental results obtained from FPGA prototype and HM simulations have demonstrated the advantages of the proposed transforms.},
  archive      = {J_JRTIP},
  author       = {Jridi, Maher and Alfalou, Ayman and Meher, Pramod K.},
  doi          = {10.1007/s11554-018-0768-x},
  journal      = {Journal of Real-Time Image Processing},
  month        = {4},
  number       = {2},
  pages        = {329-339},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Efficient approximate core transform and its reconfigurable architectures for HEVC},
  volume       = {17},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Accelerating kernel classifiers through borders mapping.
<em>JRTIP</em>, <em>17</em>(2), 313–327. (<a
href="https://doi.org/10.1007/s11554-018-0769-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Support vector machine (SVM) and other kernel techniques represent a family of powerful statistical classification methods with high accuracy and broad applicability. Because they use all or a significant portion of the training data, however, they can be slow, especially for large problems. Piecewise linear classifiers are similarly versatile, yet have the additional advantages of simplicity, ease of interpretation and, if the number of component linear classifiers is not too large, speed. Here we show how a simple, piecewise linear classifier can be trained from a kernel-based classifier in order to improve the classification speed. The method works by finding the root of the difference in conditional probabilities between pairs of opposite classes to build up a representation of the decision boundary. When tested on 17 different datasets, it succeeded in improving the classification speed of a SVM for 12 of them by up to two orders of magnitude. Of these, two were less accurate than a simple, linear classifier. The method is best suited to problems with continuum features data and smooth probability functions. Because the component linear classifiers are built up individually from an existing classifier, rather than through a simultaneous optimization procedure, the classifier is also fast to train.},
  archive      = {J_JRTIP},
  author       = {Mills, Peter},
  doi          = {10.1007/s11554-018-0769-9},
  journal      = {Journal of Real-Time Image Processing},
  month        = {4},
  number       = {2},
  pages        = {313-327},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Accelerating kernel classifiers through borders mapping},
  volume       = {17},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). In-loop perceptual model-based rate-distortion optimization
for HEVC real-time encoder. <em>JRTIP</em>, <em>17</em>(2), 293–311. (<a
href="https://doi.org/10.1007/s11554-018-0772-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, a novel High Efficiency Video Coding (HEVC)-compliant perceptual rate-distortion optimization (RDO) scheme is proposed based on motion attention and visual distortion sensitivity models, which both fully utilize in-loop coding information of HEVC. In detail, the motion attention model is designed by using the motion vectors (MVs) estimated during the inter-prediction process. The MV field is refined based on maximum a posteriori (MAP) estimation to remove MV outliers and improve the model’s efficiency. In addition, the visual distortion sensitivity is modeled by using the spatiotemporal energy of AC coefficients, which are obtained from HEVC transform process. Then, these two models are incorporated together into the RDO process. As a result, the Lagrange multiplier and quantization parameter are adjusted adaptively in an analytical way. Since the two models are calculated within the HEVC coding loop, the complexity increase is limited. The experimental results indicate that the proposed perceptual RDO scheme can achieve significantly better rate-VQM performance than the conventional RDO scheme. Specifically, the BD-rate can reach a maximum 24.45% and an average 13.68% reduction in terms of the Bjontegaard Delta metric compared to HEVC practical encoder x265.},
  archive      = {J_JRTIP},
  author       = {Hu, Qiang and Zhou, Jun and Zhang, Xiaoyun and Gao, Zhiyong and Sun, Ming-Ting},
  doi          = {10.1007/s11554-018-0772-1},
  journal      = {Journal of Real-Time Image Processing},
  month        = {4},
  number       = {2},
  pages        = {293-311},
  shortjournal = {J. Real-Time Image Process.},
  title        = {In-loop perceptual model-based rate-distortion optimization for HEVC real-time encoder},
  volume       = {17},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Fast mode decision algorithm for HEVC intra coding based on
texture partition and direction. <em>JRTIP</em>, <em>17</em>(2),
275–292. (<a href="https://doi.org/10.1007/s11554-018-0766-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {High efficiency video coding (HEVC) is the newest video coding standard, which employs some advanced coding techniques as compared to the previous standard H.264. The flexible quad-tree partitioning of coding tree unit (CTU) and various candidate modes of prediction unit (PU) significantly promote the video compression efficiency; however, these techniques lead to a great amount of computational loads. In this paper, a fast mode decision algorithm for HEVC intra coding is proposed based on texture partition and direction. It consists of two sub-algorithms: the CTU depth range prediction (CDRP) and the intra-prediction mode selection (IPMS). The CDRP reduces the recursive partition number of coding unit (CU) based on the correlation between the CTU texture partition and the optimum CU partition, and it first calculates the texture partition flags of different-size CUs from bottom to top. Then, it employs these partition flags to predict the depth range of the current CTU and decide whether to terminate the CU partition in advance. In order to reduce the number of candidate PU modes for the Hadamard optimization, the IPMS first uses the three-step selection of the candidate modes. The first step selects the candidate modes based on the correlation between the texture directions and the optimum PU modes. The second step selects the candidate modes by using the best modes among the selected modes in the first step. The third step selects the candidate modes by using the spatial correlation of the optimum modes between the current PU and its adjacent PUs. Then, in order to reduce the number of candidate modes for the rate-distortion optimization, the IPMS utilizes the numerical relationship of the sorted Hadamard costs of above selected modes, the optimum modes of adjacent PUs and the statistical characteristics of the small-size PUs. Compared to the original algorithm in HEVC test model, the proposed overall algorithm can reduce 60% encoding time on average with only a 1.45% increase in Bjontegaard delta bit rate under the all-intra configuration. Compared to the most of state-of-the-art algorithms, the proposed overall algorithm has better computational performances and similar rate-distortion performances.},
  archive      = {J_JRTIP},
  author       = {Zhu, Wei and Yi, Yao and Zhang, Hanyu and Chen, Peng and Zhang, Hua},
  doi          = {10.1007/s11554-018-0766-z},
  journal      = {Journal of Real-Time Image Processing},
  month        = {4},
  number       = {2},
  pages        = {275-292},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Fast mode decision algorithm for HEVC intra coding based on texture partition and direction},
  volume       = {17},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Hierarchical prediction-based motion vector refinement for
video frame-rate up-conversion. <em>JRTIP</em>, <em>17</em>(2), 259–273.
(<a href="https://doi.org/10.1007/s11554-018-0767-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Motion-compensated frame-rate up-conversion (MC-FRUC) often exploits either bilateral motion estimation (ME) or unidirectional ME with a fixed block size, which constrains the perceptual quality of up-converted video. In this paper, an advanced MC-FRUC approach is proposed by exploiting hierarchical prediction-based motion vector refinement. To reduce block mismatching in texture regions and color areas, an adaptive multi-layered block matching criterion is designed to extract color and edge information, which is integrated with motion information as constraint term. A hierarchical prediction-based motion vector refinement approach is proposed to obtain more accurate and dense motion vector fields (MVFs). To eliminate the outliers of MVFs, a robust dual-weighted motion vector smoothing scheme is adopted by using both spatial correlation and reliability of neighboring blocks. Experimental results show that the proposed approach has low computational complexity and outperforms state-of-the-art works in both objective and subjective qualities of interpolated frames.},
  archive      = {J_JRTIP},
  author       = {He, Jiale and Yang, Gaobo and Song, Jingyu and Ding, Xiangling and Li, Ran},
  doi          = {10.1007/s11554-018-0767-y},
  journal      = {Journal of Real-Time Image Processing},
  month        = {4},
  number       = {2},
  pages        = {259-273},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Hierarchical prediction-based motion vector refinement for video frame-rate up-conversion},
  volume       = {17},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). GMMSP on GPU. <em>JRTIP</em>, <em>17</em>(2), 245–257. (<a
href="https://doi.org/10.1007/s11554-018-0762-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Superpixel segmentation is a fundamental task in computer vision. Existing works contribute to superpixel segmentation either by improving segmentation accuracy or by reducing execution time. The former modifies existing models or develops new models to improve accuracy. The latter accelerates existing implementations or reduces algorithm complexity to improve execution rate. This work falls into the second category. Recently, a superpixel algorithm using Gaussian mixture model (GMMSP) achieves state-of-the-art performance in accuracy. After exploring this algorithm, we reached new conclusions on GMMSP that unlock potential concerning fine-grain parallelism implementation. We implement GMMSP with CUDA and make it run on GPUs. Experiments are conducted to validate the consistency between CPU and GPU implementations and to evaluate the performance of our implementation with respect to a serial and an OpenMP implementation. When we consider a full implementation with a postprocessing step executed on CPU to guarantee connectivity constraint, the proposed implementation achieves a speedup of 21× compared to the OpenMP implementation for images of size 240 × 320, using NVIDIA GTX 1080. It is also mentionable that we achieve a performance of over 1000 FPS on GTX 1080 (speedup of 77× compared to the OpenMP implementation) if the connectivity constraint is not included.},
  archive      = {J_JRTIP},
  author       = {Ban, Zhihua and Liu, Jianguo and Fouriaux, Jeremy},
  doi          = {10.1007/s11554-018-0762-3},
  journal      = {Journal of Real-Time Image Processing},
  month        = {4},
  number       = {2},
  pages        = {245-257},
  shortjournal = {J. Real-Time Image Process.},
  title        = {GMMSP on GPU},
  volume       = {17},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Toward reliable experiments on the performance of connected
components labeling algorithms. <em>JRTIP</em>, <em>17</em>(2), 229–244.
(<a href="https://doi.org/10.1007/s11554-018-0756-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The problem of labeling the connected components of a binary image is well defined, and several proposals have been presented in the past. Since an exact solution to the problem exists, algorithms mainly differ on their execution speed. In this paper, we propose and describe YACCLAB, Yet Another Connected Components Labeling Benchmark. Together with a rich and varied dataset, YACCLAB contains an open source platform to test new proposals and to compare them with publicly available competitors. Textual and graphical outputs are automatically generated for many kinds of tests, which analyze the methods from different perspectives. An extensive set of experiments among state-of-the-art techniques is reported and discussed.},
  archive      = {J_JRTIP},
  author       = {Bolelli, Federico and Cancilla, Michele and Baraldi, Lorenzo and Grana, Costantino},
  doi          = {10.1007/s11554-018-0756-1},
  journal      = {Journal of Real-Time Image Processing},
  month        = {4},
  number       = {2},
  pages        = {229-244},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Toward reliable experiments on the performance of connected components labeling algorithms},
  volume       = {17},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). An FPGA accelerator for PatchMatch multi-view stereo using
OpenCL. <em>JRTIP</em>, <em>17</em>(2), 215–227. (<a
href="https://doi.org/10.1007/s11554-017-0745-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {PatchMatch multi-view stereo (MVS) is one method generating depth maps from multi-view images and is expected to be used for various applications such as robot vision, 3D measurement, and 3D reconstruction. The major drawback of PatchMatch MVS is its large computational amount, and its acceleration is strongly desired. However, this acceleration is prevented by two problems. First, though PatchMatch MVS estimates depth maps by propagating estimation results among neighbor pixels, it is not suitable for GPU-based acceleration. Second, since the shape of a matching window used for stereo matching is changed dynamically, reading its pixels is inefficient in memory access. This paper proposes an FPGA accelerator exploiting on-chip FIFOs efficiently to solve the propagation problem. Moreover, reading pixels of a matching window is improved by a cover window which has the fixed shape and covers the matching window. The FPGA accelerator is designed using a design tool based on Open Computing Language (OpenCL). Although parameters of PatchMatch MVS depend on object images, these parameters can be changed easily by the OpenCL-based design. The experimental results demonstrate that the FPGA implementation achieves 3.4 and 2.2 times faster processing speeds than the CPU and GPU ones, respectively, and the power-delay product of the FPGA implementation is 3.2 and 5.7% of the CPU and GPU ones, respectively.},
  archive      = {J_JRTIP},
  author       = {Tatsumi, Shunsuke and Hariyama, Masanori and Ito, Koichi and Aoki, Takafumi},
  doi          = {10.1007/s11554-017-0745-9},
  journal      = {Journal of Real-Time Image Processing},
  month        = {4},
  number       = {2},
  pages        = {215-227},
  shortjournal = {J. Real-Time Image Process.},
  title        = {An FPGA accelerator for PatchMatch multi-view stereo using OpenCL},
  volume       = {17},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Multi-level complexity reduction for HEVC multiview coding.
<em>JRTIP</em>, <em>17</em>(2), 197–213. (<a
href="https://doi.org/10.1007/s11554-018-0757-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Standardized in 2014, multiview extension of high efficiency video coding (MV-HEVC) offers significantly better compression performance of up to 50% for multiview and 3D videos compared to multiple independent single view HEVC coding. However, the extreme high computational complexity of MV-HEVC demands significant optimization of the encoder. In this work, we propose a series of optimization techniques at various levels of abstraction: non-aggregation massively parallel motion estimation (ME) and disparity estimation (DE) for prediction units, fractional and bidirectional ME/DE, quantization parameter-based early termination of coding tree unit (CTU), and optimized resource-scheduled wave front parallel processing for CTU. When evaluated over three views for all available official multiview video coding test sequences, proposed optimization outperforms the anchor encoder by average factor of 5.4 at the cost of 4.4% bitrate (DBR) increase at no loss in PSNR, or alternatively a PSNR degradation of 0.12 dB at no change to the DBR.},
  archive      = {J_JRTIP},
  author       = {Jiang, Caoyang and Nooshabadi, Saeid},
  doi          = {10.1007/s11554-018-0757-0},
  journal      = {Journal of Real-Time Image Processing},
  month        = {4},
  number       = {2},
  pages        = {197-213},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Multi-level complexity reduction for HEVC multiview coding},
  volume       = {17},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Fast CU partition-based machine learning approach for
reducing HEVC complexity. <em>JRTIP</em>, <em>17</em>(1), 185–196. (<a
href="https://doi.org/10.1007/s11554-019-00936-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the development of video coding technology, the high efficiency video coding (HEVC) provides better coding efficiency compared to its predecessors H.264/AVC. HEVC improves rate distortion (RD) performance significantly with increased encoding complexity. Due to the adoption of a large variety of coding unit (CU) sizes, at RD optimization level, the quadtree partition of the CU consumes a large proportion of the encoding complexity. Hence, the computational complexity cost remains a critical issue that must be properly considered in the optimization task. In this paper, two machine learning-based fast CU partition method for inter-mode HEVC are proposed, to optimize the complexity allocation at CU level. First, we propose an online support vector machine (SVM)-based fast CU algorithm for reducing HEVC complexity. The later was trained in an online way. Second, a deep convolutional neural network (CNN) is designed to predict the CU partition, in which large-scale training database including substantial CU partition data is considered. Experimental results demonstrate that the proposed online SVM can achieve a time saving of 52.28% with a degradation of 1.928% in the bitrate (BR). However, the proposed deep CNN can reduce the encoding time by 53.99% with 0.195% BR degradation. Compared to the state-of-the art, the two proposed approaches outperform the related works in terms of both RD performance and complexity reduction at inter-mode.},
  archive      = {J_JRTIP},
  author       = {Bouaafia, Soulef and Khemiri, Randa and Sayadi, Fatma Ezahra and Atri, Mohamed},
  doi          = {10.1007/s11554-019-00936-0},
  journal      = {Journal of Real-Time Image Processing},
  month        = {2},
  number       = {1},
  pages        = {185-196},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Fast CU partition-based machine learning approach for reducing HEVC complexity},
  volume       = {17},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Secure real-time image protection scheme with near-duplicate
detection in cloud computing. <em>JRTIP</em>, <em>17</em>(1), 175–184.
(<a href="https://doi.org/10.1007/s11554-019-00887-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With advancements in technologies of the Internet and multi-media, various images need to be generated and transmitted anytime. Restricted by local constrained storage space, users can store their images with the assist of the cloud. However, the cloud is a remote semi-trusted party that may extract stored images for adversaries due to monetary reasons. In this paper, a secure real-time image protection scheme is proposed, which can be used to enhance the security of the stored images in cloud computing. Moreover, the convergent encryption is used to construct our scheme, which can provide functionalities of image deduplication checking and near-duplicate detection for the image owner. To improve the efficiency of the near-duplicate detection, deep learning is exploited in our scheme to extract images. Security analysis indicates that the proposed scheme can meet the security requirements of correctness and security. Performance analysis shows that the proposed scheme can be performed with low computational cost.},
  archive      = {J_JRTIP},
  author       = {Liu, Dengzhi and Shen, Jian and Wang, Anxi and Wang, Chen},
  doi          = {10.1007/s11554-019-00887-6},
  journal      = {Journal of Real-Time Image Processing},
  month        = {2},
  number       = {1},
  pages        = {175-184},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Secure real-time image protection scheme with near-duplicate detection in cloud computing},
  volume       = {17},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A privacy-preserving image retrieval method based on deep
learning and adaptive weighted fusion. <em>JRTIP</em>, <em>17</em>(1),
161–173. (<a href="https://doi.org/10.1007/s11554-019-00909-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the development of big data and cloud computing, more and more data owners store the data in cloud server. Considering privacy preserving, image data need to be encrypted before uploaded to the cloud, which will lead to inefficient image retrieval of ciphertext domain. Therefore, the challenge of encrypted image retrieval is how to improve the performance. Toward this goal, this paper proposes a privacy-preserving image retrieval method based on deep learning and adaptive weighted fusion. Firstly, extracting low-level feature EHD (edge histogram descriptor), BOW (bag of words) and high-level semantic feature of images. Secondly, reducing the dimension of 1024-dim high-level semantic feature by PCA (principal component analysis), and the three features were binarized. Then these types of features are adaptively fused. Finally, constructing a prefilter table for fusion features to improve search efficiency by locality sensitive hashing (LSH) algorithm. K-nearest neighbor (KNN) algorithm and logistic encryption method were used to protect the privacy of fused features and images, respectively. The experiments show that the proposed method can not only ensure image security but also improve the retrieval accuracy of encrypted image.},
  archive      = {J_JRTIP},
  author       = {Qin, Jiaohua and Chen, Jianhua and Xiang, Xuyu and Tan, Yun and Ma, Wentao and Wang, Jing},
  doi          = {10.1007/s11554-019-00909-3},
  journal      = {Journal of Real-Time Image Processing},
  month        = {2},
  number       = {1},
  pages        = {161-173},
  shortjournal = {J. Real-Time Image Process.},
  title        = {A privacy-preserving image retrieval method based on deep learning and adaptive weighted fusion},
  volume       = {17},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Deep learning for real-time image steganalysis: A survey.
<em>JRTIP</em>, <em>17</em>(1), 149–160. (<a
href="https://doi.org/10.1007/s11554-019-00915-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Steganography is a technique that transmits secret data or message in an appropriate multimedia carrier, e.g., image, audio, and video files. It comes under the assumption that if the feature is visible, the point of attack is evident. However, such technology is always used by criminals who do not want to be easily discovered to hide harmful information in various media, especially in images. Massive spreading of those harmful information will increase the difficulty of social security management. In this case, excellent image steganalysis should be developed and applied. Specially, real-time image steganalysis is necessary when information timelines need to be protected. If detection scene has large amounts of users, deep learning can be applied to improve performance of image steganalysis benefiting from its powerful processing capability. Using deep learning, real-time image steganalysis system gets higher accuracy and efficiency. In this paper, we give an account of preliminary knowledge first. A brief overview of the deep neural networks (DNN) is also presented. The combination of DNN and real-time image steganalysis is introduced. Then, we import the concept of CNN in DNN, and expound theory as well as advantages of combining CNN and image steganalysis. For multi-user scenarios, we analyze a practical real-time image steganalysis application based on outlier detection methods. At last, we prospect the future issues of real-time image steganalysis.},
  archive      = {J_JRTIP},
  author       = {Ruan, Feng and Zhang, Xing and Zhu, Dawei and Xu, Zhanyang and Wan, Shaohua and Qi, Lianyong},
  doi          = {10.1007/s11554-019-00915-5},
  journal      = {Journal of Real-Time Image Processing},
  month        = {2},
  number       = {1},
  pages        = {149-160},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Deep learning for real-time image steganalysis: A survey},
  volume       = {17},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Efficient binary image steganalysis based on ensemble neural
network of multi-module. <em>JRTIP</em>, <em>17</em>(1), 137–147. (<a
href="https://doi.org/10.1007/s11554-019-00885-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {There are few studies on binary image steganalysis based on convolutional neural network (CNN). In this paper, an efficient binary image steganalysis scheme based on CNN which integrates high-pass filters, truncated linear unit and subnetworks is proposed. In the process of binary image steganography, flipped pixels usually scatter on the boundaries of the content in the image. Therefore, the first convolutional layer is constructed with high-pass filters to capture the structure of embedded signals better. Truncated linear unit (TLU) is also adopted after the first convolutional layer for the same purpose. 4 truncated linear units with different truncated values are adopted to capture embedding signals of different intensities. We also adopt 4 subnets after the 4 truncated linear units to further boost the performance of the CNN network. The experimental results show that our proposed scheme is efficient and effective on binary steganalysis.},
  archive      = {J_JRTIP},
  author       = {Liu, Jiarui and Lu, Wei and Zhan, Yilin and Chen, Junjia and Xu, Zhaopeng and Li, Ruipeng},
  doi          = {10.1007/s11554-019-00885-8},
  journal      = {Journal of Real-Time Image Processing},
  month        = {2},
  number       = {1},
  pages        = {137-147},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Efficient binary image steganalysis based on ensemble neural network of multi-module},
  volume       = {17},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Coverless real-time image information hiding based on image
block matching and dense convolutional network. <em>JRTIP</em>,
<em>17</em>(1), 125–135. (<a
href="https://doi.org/10.1007/s11554-019-00917-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Information security has become a key issue of public concern recently. In order to radically resist the decryption and analysis in the field of image information hiding and significantly improve the security of the secret information, a novel coverless information hiding approach based on deep learning is proposed in this paper. Deep learning can select the appropriate carrier according to requirements to achieve real-time image data hiding and the high-level semantic features extracted by CNN are more accurate than the low-level features. This method does not need to employ the designated image for embedding the secret data but transfer a set of real-time stego-images which share one or several visually similar blocks with the given secret image. In this approach, a group of real-time images searched online are segmented according to specific requirements. Then, the DenseNet is used to extract the high-level semantic features of each similar block. At the same time, a robust hash sequence with feature sequence, DC and location is generated by DCT. The inverted index structure based on the hash sequence is constructed to attain real-time image matching efficiently. At the sending end, the stego-images are matched and sent through feature retrieval. At the receiving end, the secret image can be recovered by extracting similar blocks through the received stego-images and stitching the image blocks according to the location information. Experimental results demonstrate that the proposed method without any modification traces provides better robustness and has higher retrieval accuracy and capacity when compared with some existing coverless image information hiding.},
  archive      = {J_JRTIP},
  author       = {Luo, Yuanjing and Qin, Jiaohua and Xiang, Xuyu and Tan, Yun and Liu, Qiang and Xiang, Lingyun},
  doi          = {10.1007/s11554-019-00917-3},
  journal      = {Journal of Real-Time Image Processing},
  month        = {2},
  number       = {1},
  pages        = {125-135},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Coverless real-time image information hiding based on image block matching and dense convolutional network},
  volume       = {17},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Enhancing reliability and efficiency for real-time robust
adaptive steganography using cyclic redundancy check codes.
<em>JRTIP</em>, <em>17</em>(1), 115–123. (<a
href="https://doi.org/10.1007/s11554-019-00905-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The development of multimedia and deep learning technology bring new challenges to steganography and steganalysis techniques. Meanwhile, robust steganography, as a class of new techniques aiming to solve the problem of covert communication under lossy channels, has become a new research hotspot in the field of information hiding. To improve the communication reliability and efficiency for current real-time robust steganography methods, a concatenated code, composed of Syndrome–Trellis codes (STC) and cyclic redundancy check (CRC) codes, is proposed in this paper. The enhanced robust adaptive steganography framework proposed is this paper is characterized by a strong error detection capability, high coding efficiency, and low embedding costs. On this basis, three adaptive steganographic methods resisting JPEG compression and detection are proposed. Then, the fault tolerance of the proposed steganography methods is analyzed using the residual model of JPEG compression, thus obtaining the appropriate coding parameters. Experimental results show that the proposed methods have a significantly stronger robustness against compression, and are more difficult to be detected by statistical based steganalytic methods.},
  archive      = {J_JRTIP},
  author       = {Zhang, Yi and Luo, Xiangyang and Zhu, Xiaodong and Li, Zhenyu and Bors, Adrian G.},
  doi          = {10.1007/s11554-019-00905-7},
  journal      = {Journal of Real-Time Image Processing},
  month        = {2},
  number       = {1},
  pages        = {115-123},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Enhancing reliability and efficiency for real-time robust adaptive steganography using cyclic redundancy check codes},
  volume       = {17},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Deep learning-based real-time VPN encrypted traffic
identification methods. <em>JRTIP</em>, <em>17</em>(1), 103–114. (<a
href="https://doi.org/10.1007/s11554-019-00930-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the widespread application of virtual private network (VPN) technology, real-time VPN traffic identification has become an increasingly important task in network management and security maintenance. Since traditional encrypted traffic identification technology is not effective in feature extraction and selection, this paper proposes two deep learning-based models to classify the traffic into VPN and non-VPN traffic, identify VPN traffic generated by six different applications much further. Our models utilize convolutional auto-encoding (CAE) and convolutional neural network (CNN), respectively, preprocessing the traffic samples into session pictures, to accomplish the experiment objectives. The CAE-based method, utilizing the unsupervised nature of CAE to extract the hidden layer features, can automatically learn the nonlinear relationship between original input and expected output. The CNN-based method performs well in extracting two-dimensional local features of images. Experimental results show that our models perform better than traditional identification methods. In the two-category identification, the best result comes from the CAE-based model; the overall identification accuracy rate is 98.77%. Among the six-category identification, the best result comes from CNN-based model; the overall identification accuracy rate is 92.92%.},
  archive      = {J_JRTIP},
  author       = {Guo, Lulu and Wu, Qianqiong and Liu, Shengli and Duan, Ming and Li, Huijie and Sun, Jianwen},
  doi          = {10.1007/s11554-019-00930-6},
  journal      = {Journal of Real-Time Image Processing},
  month        = {2},
  number       = {1},
  pages        = {103-114},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Deep learning-based real-time VPN encrypted traffic identification methods},
  volume       = {17},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A real-time typhoon eye detection method based on deep
learning for meteorological information forensics. <em>JRTIP</em>,
<em>17</em>(1), 95–102. (<a
href="https://doi.org/10.1007/s11554-019-00899-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The development of meteorological satellite technology has made it feasible to observe cloud cover over the Earth’s surface, and the number of high-precision meteorological satellite images available has increased dramatically over the years. However, there exists a gap between meteorological satellite cloud images and the true information of the pictured clouds. Therefore, extracting the true atmospheric information from “forged” satellite images in real time is a challenging task. In this paper, we proposed a real-time typhoon eye detection method from meteorological satellite cloud images based on deep learning. This new approach is the first step in detecting hidden information in satellite cloud images and provides important data support to detect true typhoon information. We performed simulation experiments and the results showed that the proposed method performs well in identifying typhoons, where the positive sample accuracy rate, negative sample accuracy rate, and total average accuracy rate are 94.22%, 99.43%, and 96.83%, respectively. In the testing process, the average time needed to detect each sample is 6 ms, which fulfills the requirement for real-time typhoon eye detection. Our method outperforms the k-nearest neighbors (KNN) and support vector machine (SVM) algorithms.},
  archive      = {J_JRTIP},
  author       = {Zhao, Liling and Chen, Yifei and Sheng, Victor S.},
  doi          = {10.1007/s11554-019-00899-2},
  journal      = {Journal of Real-Time Image Processing},
  month        = {2},
  number       = {1},
  pages        = {95-102},
  shortjournal = {J. Real-Time Image Process.},
  title        = {A real-time typhoon eye detection method based on deep learning for meteorological information forensics},
  volume       = {17},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Real-time human cross-race aging-related face appearance
detection with deep convolution architecture. <em>JRTIP</em>,
<em>17</em>(1), 83–93. (<a
href="https://doi.org/10.1007/s11554-019-00903-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Human age estimation (AE) is an emerging research topic in computer vision and machine learning and has attracted increasing amount of research due its wide potential applications. In the process of human aging, facial appearances change from glabrous to crinkly similarly across all races, from European, Hispanic and African to Asian. To specially explore the relationships between aging and facial appearances across races, this paper is devoted to determining the correspondence between facial aging and facial appearances. Specifically, we first extract appearance vector features from facial images with their spatial structure preserved. Then, we propose to select the aging-related features shared by different races to explore their aging-related common facial regions, while removing redundant features. Thirdly, we improve the proposed model by incorporating potential cross-race relationships in an automated learning manner. Additionally, we extend our model with deep convolution architecture. Finally, we evaluate the proposed methodologies on a large face aging database with real-time efficiency.},
  archive      = {J_JRTIP},
  author       = {Tian, Qing and Zhang, Wenqiang and Mao, Junxiang and Yin, Hujun},
  doi          = {10.1007/s11554-019-00903-9},
  journal      = {Journal of Real-Time Image Processing},
  month        = {2},
  number       = {1},
  pages        = {83-93},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Real-time human cross-race aging-related face appearance detection with deep convolution architecture},
  volume       = {17},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Multi-level feature fusion model-based real-time person
re-identification for forensics. <em>JRTIP</em>, <em>17</em>(1), 73–81.
(<a href="https://doi.org/10.1007/s11554-019-00908-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Person forensics aims to retrieve the specified person across non-overlapping cameras. It is difficult owing to the appearance variations caused by occlusion, human pose change, background clutter, illumination variation, etc. In this scenario, current models face great challenges in extracting effective features. Recent deep learning models mainly focus on extracting representative deep features to cope with appearance variations, while handcrafted features are not fully explored. In this paper, a multi-level feature fusion model (MFFM) is designed to combine both deep features and handcrafted features in real time. MFFM is first utilized to describe person appearance. Then, local binary pattern (LBP) and histogram of oriented gradient (HOG) are extracted to cope with geometric change and illumination variance. Using LBP and HOG, 11.89% on the CUHK03, 15.30% on the Market-1501 and 8.25% on the VIPeR top-1 recognition accuracy improvement for the proposed method are achieved with only 9.66%, 4.90%, and 7.59% extra processing time. Experimental results indicate MFFM can achieve the best performance compared to the state-of-the-art models on the Market1501, CUHK03, and VIPeR datasets.},
  archive      = {J_JRTIP},
  author       = {Wang, Shiqin and Xu, Xin and Liu, Lei and Tian, Jing},
  doi          = {10.1007/s11554-019-00908-4},
  journal      = {Journal of Real-Time Image Processing},
  month        = {2},
  number       = {1},
  pages        = {73-81},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Multi-level feature fusion model-based real-time person re-identification for forensics},
  volume       = {17},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Semi-supervised stacked autoencoder-based deep hierarchical
semantic feature for real-time fingerprint liveness detection.
<em>JRTIP</em>, <em>17</em>(1), 55–71. (<a
href="https://doi.org/10.1007/s11554-019-00928-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The popularity of biometric authentication technology benefits from the rapid development of smart mobile devices in recent years, and fingerprints, which are inherent human traits and neither easily revealed nor deciphered, can be used for real-time individual authentication systems. However, the main security issue of real-time fingerprint authentication systems is that most fingerprint scanners are vulnerable to presentation attacks by artificial replicas, made from plastic clay, gelatin, silicon, wood glue, etc. One anti-spoofing attack scheme, called real-time fingerprint liveness detection (RFLD), has been proposed to discriminate live or fake fingerprints. Currently, to resolve the presentation attacks, most RFLD solutions all relied on handcrafted feature extraction and selection. The features extracted by manual method are shallow features of the samples; however, autoencoder can automatically learn deep hierarchical semantic features representation of the samples, thus replacing the operations extracted with hand-designed features. In this paper, we apply stacked autoencoder to RFLD to significantly lower the work-force burden of the feature extraction engineering, and our model consists of two parts: parameter pre-training based on unsupervised learning and FLD based on supervised learning. The performance has been verified on two public fingerprint datasets: LivDet 2011 and 2013, and the experimental results indicate that our proposed approach works well for RFLD as well as the detection performance is satisfactory.},
  archive      = {J_JRTIP},
  author       = {Yuan, Chengsheng and Chen, Xianyi and Yu, Peipeng and Meng, Ruohan and Cheng, Weijin and Wu, Q. M. Jonathan and Sun, Xingming},
  doi          = {10.1007/s11554-019-00928-0},
  journal      = {Journal of Real-Time Image Processing},
  month        = {2},
  number       = {1},
  pages        = {55-71},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Semi-supervised stacked autoencoder-based deep hierarchical semantic feature for real-time fingerprint liveness detection},
  volume       = {17},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A real-time reversible image authentication method using
uniform embedding strategy. <em>JRTIP</em>, <em>17</em>(1), 41–54. (<a
href="https://doi.org/10.1007/s11554-019-00904-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Reversible image authentication (RIA) is an emerging research field for image tampering operation detection. Tampered regions can be localized precisely by embedding an authentication code (AC) into each divided image block in advance. Once the image is identified as an authentic image, the original image can be recovered without any loss. Under these two preconditions, an efficient RIA scheme is proposed to further improve the detection precision of the final authentication results. Compared with existing methods, a uniform embedding strategy is adopted in this paper, in which one AC bit is embedded into each divided image block to ensure they have the same authentication capability. To improve the forgery localization precision, the block size is adaptively sought according to the embedding capacity of the image. In addition, during the image authentication process, the embedding parameters and location map information are verified to increase the process’s rigorousness. The experimental results demonstrate the superiority of the detection precision of the proposed method.},
  archive      = {J_JRTIP},
  author       = {Yao, Heng and Wei, Hongbin and Qin, Chuan and Tang, Zhenjun},
  doi          = {10.1007/s11554-019-00904-8},
  journal      = {Journal of Real-Time Image Processing},
  month        = {2},
  number       = {1},
  pages        = {41-54},
  shortjournal = {J. Real-Time Image Process.},
  title        = {A real-time reversible image authentication method using uniform embedding strategy},
  volume       = {17},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A real-time image forensics scheme based on multi-domain
learning. <em>JRTIP</em>, <em>17</em>(1), 29–40. (<a
href="https://doi.org/10.1007/s11554-019-00893-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, researchers have attempted to explore methods for real-time image forgery detection. Many approaches were developed to detect a certain number of image modification methods. There are many limitations in practical application. In this paper, a multi-domain learning convolutional neural network (MDL-CNN) is proposed to overcome this limitation. We extract the periodicity property from the original and modified image. Features of modified image extracted from different datasets are then fed into the neural network in training process. Since the proposed MDL-CNN is trained by different types of tempering datasets, our method can distinguish many types of image modifications. To decrease the computation of proposed scheme, 1 × 1 kernel convolution layer is used in the second convolutional layer of each network. Furthermore, a multi-domain loss function is developed to enhance the recognition ability of in-depth learning features. Experimental evaluation results show that MDL-CNN method can significantly improve the forensic performance.},
  archive      = {J_JRTIP},
  author       = {Yang, Bin and Li, Zhenyu and Zhang, Tao},
  doi          = {10.1007/s11554-019-00893-8},
  journal      = {Journal of Real-Time Image Processing},
  month        = {2},
  number       = {1},
  pages        = {29-40},
  shortjournal = {J. Real-Time Image Process.},
  title        = {A real-time image forensics scheme based on multi-domain learning},
  volume       = {17},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Real-time estimation for the parameters of gaussian
filtering via deep learning. <em>JRTIP</em>, <em>17</em>(1), 17–27. (<a
href="https://doi.org/10.1007/s11554-019-00907-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Driven by the development of digital technology, manipulation towards digital images becomes simpler than ever before in recent years. Many smartphone applications bring the convenience for ordinary people to edit images in real-time without any professional skills. The digital forensics is an important research field in information security against the situation. In image forensics, it is necessary to validate all possible manipulation during the forming history of given images. Thus, many image forensics researchers focus on detecting certain manipulations to protect the integrity of images such as verifying Gaussian filtering. However, these works tend to make binary classification that if the image is processed by certain manipulation or not. The classification of same manipulation based on parameters are ignored. Here, we propose a method to estimate the parameters of Gaussian filtering to process images based on convolutional neural networks (CNN). Besides, in the modern world, it is also extremely important to enable the simulation in real-time to process with the given data immediately. The proposed method can also validate the given image in a quite short time. Our experiments show that the proposed method can provide excellent real-time performance in estimating the window size and standard deviation of Gaussian filterings. The well-trained model can satisfy us with not only the estimation accuracy, but also the validation time simultaneously.},
  archive      = {J_JRTIP},
  author       = {Ding, Feng and Shi, Yuxi and Zhu, Guopu and Shi, Yun-qing},
  doi          = {10.1007/s11554-019-00907-5},
  journal      = {Journal of Real-Time Image Processing},
  month        = {2},
  number       = {1},
  pages        = {17-27},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Real-time estimation for the parameters of gaussian filtering via deep learning},
  volume       = {17},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Non-aligned double JPEG compression detection based on
refined markov features in QDCT domain. <em>JRTIP</em>, <em>17</em>(1),
7–16. (<a href="https://doi.org/10.1007/s11554-019-00929-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to the widespread use of the JPEG format, non-aligned double JPEG (NA-DJPEG) compression is very common in image tampering. Therefore, non-aligned double JPEG compression detection has attracted significant attention in digital forensics in recent years. In most of the previous detection algorithms, grayscale images are used directly, or color images are first converted into grayscale images and then processed. However, it is worth noting that most tampered images are color images. To make full use of the color information in images, a detection algorithm, which uses color images directly, is put forward in this paper. The algorithm based on refined Markov in quaternion discrete cosine transform (QDCT) domain is proposed for NA-DJPEG compression detection. Firstly, color information of a given JPEG image is extracted from blocked images to construct quaternion, and then block image QDCT coefficient matrices, including amplitude and three angles ($$\psi $$, $$\phi $$, and $$\theta $$) can be obtained. Secondly, the refined Markov features are generated from the transition probability matrix in the corresponding refinement process. Our proposed refinement method not only reduces redundant features but also makes the acquired features more efficient in detection. Therefore, the refined Markov features can not only capture the intra-block correlation between block QDCT coefficients but also improve computing efficiency in real-time. Finally, support vector machine (SVM) method is employed for NA-DJPEG compression detection. The experiment results demonstrate that the proposed algorithm not only make use of color information of images, but also can achieve better detection performance with small size images (i.e., $$64 \times 64$$) outperforming state-of-the-art detection methods tested on the same dataset.},
  archive      = {J_JRTIP},
  author       = {Wang, Jinwei and Huang, Wei and Luo, Xiangyang and Shi, Yun-Qing and Jha, Sunil Kr.},
  doi          = {10.1007/s11554-019-00929-z},
  journal      = {Journal of Real-Time Image Processing},
  month        = {2},
  number       = {1},
  pages        = {7-16},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Non-aligned double JPEG compression detection based on refined markov features in QDCT domain},
  volume       = {17},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Introduction to the special issue on deep learning for
real-time information hiding and forensics. <em>JRTIP</em>,
<em>17</em>(1), 1–5. (<a
href="https://doi.org/10.1007/s11554-020-00947-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_JRTIP},
  author       = {Zhou, Zhili and Yang, Ching-Nung and Kim, Cheonshik and Cimato, Stelvio},
  doi          = {10.1007/s11554-020-00947-2},
  journal      = {Journal of Real-Time Image Processing},
  month        = {2},
  number       = {1},
  pages        = {1-5},
  shortjournal = {J. Real-Time Image Process.},
  title        = {Introduction to the special issue on deep learning for real-time information hiding and forensics},
  volume       = {17},
  year         = {2020},
}
</textarea>
</details></li>
</ul>

</body>
</html>
