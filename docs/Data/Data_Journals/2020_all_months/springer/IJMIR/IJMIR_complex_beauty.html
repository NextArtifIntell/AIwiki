<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>IJMIR_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="ijmir---21">IJMIR - 21</h2>
<ul>
<li><details>
<summary>
(2020). MRECN: Mixed representation enhanced (de)compositional
network for caption generation from visual features, modeling as pseudo
tensor product representation. <em>IJMIR</em>, <em>9</em>(4), 291–316.
(<a href="https://doi.org/10.1007/s13735-020-00198-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Semantic feature composition from image features has a drawback because it is unable to capture the content of the captions and failed to evolve as longer and meaningful captions. In this paper, we have proposed improvements on semantic features that can generate and evolve captions through the new approach called mixed fusion of representations and decomposition. Semantic works on the principle of using CNN visual features to generate context-word distribution and use that to generate captions using language decoder. Generated semantics are used for captioning, but have limitations. We have introduced a far better and newer approach with an enhanced representation-based network known as mixed representation enhanced (de)compositional network (MRECN), which can help produce better and different content for captions. As denoted from the results (0.351 BLUE_4), it has outperformed most of the state of the art. We defined a better feature decoding scheme using learned networks, which establishes an incoherence of related words into captions. From our research, we have come to some important conclusions regarding mixed representation strategies as it emerges as the most viable and promising way of representing the relationships of the sophisticated features for decision making and complex applications like the image to natural languages.},
  archive      = {J_IJMIR},
  author       = {Sur, Chiranjib},
  doi          = {10.1007/s13735-020-00198-8},
  journal      = {International Journal of Multimedia Information Retrieval},
  month        = {12},
  number       = {4},
  pages        = {291-316},
  shortjournal = {Int. J. Multimed. Inf. Retr.},
  title        = {MRECN: Mixed representation enhanced (de)compositional network for caption generation from visual features, modeling as pseudo tensor product representation},
  volume       = {9},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A study of classification and feature extraction techniques
for brain tumor detection. <em>IJMIR</em>, <em>9</em>(4), 271–290. (<a
href="https://doi.org/10.1007/s13735-020-00199-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Medical imaging aids in the analysis of interior parts of the human body such as the functioning of the organs or tissues for early treatment of diseases. Many different types of medical imaging technologies exist, for example, X-ray radiography, magnetic resonance imaging, endoscopy, positron emission tomography, CT scan (computed tomography), and many more. A tumor is an abnormal tissue in the brain which causes damage to the functioning of the cell. Therefore, brain tumor detection is an incredibly tricky task. Manual detection of a tumor is quite risky as it involves the insertion of a needle in the brain. Thus, there is a need for automated brain tumor detection systems. The well-timed detection of the tumor can add to accurate treatment and can increase the survival rate of patients. From machine learning techniques, namely K-nearest neighbor, support vector machine, and more to soft computing techniques, namely artificial neural network, self-organizing map, and others hold a significant stand in detection and categorization of brain tumor. Various methods including deep learning-based classifiers such as convolutional neural network, recurrent neural network, deep belief network (DBN), and others are used to make it easier to detect the tumor. Hybrid classifiers were also used for classification systems such as combining the machine learning approach with soft computing. This study is to summarize and compare the work of various authors on automatic brain tumor detection using medical imaging. Based on the accuracy, specificity, and sensitivity parameters, the results of different techniques are analyzed and compared graphically.},
  archive      = {J_IJMIR},
  author       = {Jalali, Vatika and Kaur, Dapinder},
  doi          = {10.1007/s13735-020-00199-7},
  journal      = {International Journal of Multimedia Information Retrieval},
  month        = {12},
  number       = {4},
  pages        = {271-290},
  shortjournal = {Int. J. Multimed. Inf. Retr.},
  title        = {A study of classification and feature extraction techniques for brain tumor detection},
  volume       = {9},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Recent trends in image watermarking techniques for copyright
protection: A survey. <em>IJMIR</em>, <em>9</em>(4), 249–270. (<a
href="https://doi.org/10.1007/s13735-020-00197-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Digital copyright protection signifies the technology of rights protection, utilization control, and management of digital substances in the activity of production, propagation, retailing, and consumption. In the era of digital boom in information technology and multimedia, malicious exploitations and piracy have become a global phenomenon; thus, the need for digital content protection is inexorable. Image watermarking has been advocated universally to resolve innumerable dilemmas correlated with issues in the fields of digital rights management and multimedia security. According to anticipated applications, various suitable image watermarking procedures have been designed to safeguard the copyright of digital subjects. In this study, contemporary developments of digital image watermarking techniques are appraised so as to identify state-of-the-art practices and their limitations. This research contribution will be expedient for the academics in instigating efficient watermarking techniques for copyright authentication.},
  archive      = {J_IJMIR},
  author       = {Ray, Arkadip and Roy, Somaditya},
  doi          = {10.1007/s13735-020-00197-9},
  journal      = {International Journal of Multimedia Information Retrieval},
  month        = {12},
  number       = {4},
  pages        = {249-270},
  shortjournal = {Int. J. Multimed. Inf. Retr.},
  title        = {Recent trends in image watermarking techniques for copyright protection: A survey},
  volume       = {9},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Recent advances in local feature detector and descriptor: A
literature survey. <em>IJMIR</em>, <em>9</em>(4), 231–247. (<a
href="https://doi.org/10.1007/s13735-020-00200-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The computer vision system is the technology that deals with identifying and detecting the objects of a particular class in digital images and videos. Local feature detection and description play an essential role in many computer vision applications like object detection, object classification, etc. The accuracy of these applications depends on the performance of local feature detectors and descriptors used in the methods. Over the past decades, new algorithms and techniques have been introduced with the development of machine learning and deep learning techniques. The machine learning techniques can lead the work to the next level when sufficient data is provided. Deep learning algorithms can handle a large amount of data efficiently. However, this may raise questions in a researcher’s mind about selecting the best algorithm and best method for a particular application to increase the performance. The selection of the algorithms highly depends on the type of application and amount of data to be handled. This encouraged us to write a comprehensive survey of local image feature detectors and descriptors from state-of-the-art to the recent ones. This paper presents feature detection and description methods in the visible band with their advantages and disadvantages. We also gave an overview of current performance evaluations and benchmark datasets. Besides, the methods and algorithms are described to find the features beyond the visible band. Finally, we concluded the survey with future directions. This survey may help researchers and serve as a reference in the field of the computer vision system.},
  archive      = {J_IJMIR},
  author       = {Joshi, Khushbu and Patel, Manish I.},
  doi          = {10.1007/s13735-020-00200-3},
  journal      = {International Journal of Multimedia Information Retrieval},
  month        = {12},
  number       = {4},
  pages        = {231-247},
  shortjournal = {Int. J. Multimed. Inf. Retr.},
  title        = {Recent advances in local feature detector and descriptor: A literature survey},
  volume       = {9},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). State of the journal. <em>IJMIR</em>, <em>9</em>(4), 229.
(<a href="https://doi.org/10.1007/s13735-020-00201-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_IJMIR},
  author       = {Lew, Michael S.},
  doi          = {10.1007/s13735-020-00201-2},
  journal      = {International Journal of Multimedia Information Retrieval},
  month        = {12},
  number       = {4},
  pages        = {229},
  shortjournal = {Int. J. Multimed. Inf. Retr.},
  title        = {State of the journal},
  volume       = {9},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Effective video hyperlinking by means of enriched feature
sets and monomodal query combinations. <em>IJMIR</em>, <em>9</em>(3),
215–227. (<a href="https://doi.org/10.1007/s13735-019-00173-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Video content has been increasing at an unprecedented rate in recent years, bringing the need for improved tools providing efficient access to specific contents of interest. Within the management of video content, hyperlinking aims at determining related video segments from a collection with respect to an input video anchor. This paper describes the system we designed to address feature selection for the video hyperlinking challenge, as defined by TRECVID, one of the top worldwide venues for multimedia benchmarking. The proposed solution is based on different combinations of textual and visual features, enriched to capture the various facets of the videos: automatically generated transcripts, visual concepts, video metadata, named-entity recognition, and concept-mapping techniques. The different combinations of monomodal queries are experimentally evaluated, and the impact of both parameters and single features are discussed to identify their contributions. The best performing approach at the TRECVID 2017 video hyperlinking challenge was the ensemble feature selection, which includes three different monomodal queries based on enriched feature sets.},
  archive      = {J_IJMIR},
  author       = {Kavoosifar, Mohammad Reza and Apiletti, Daniele and Baralis, Elena and Garza, Paolo and Huet, Benoit},
  doi          = {10.1007/s13735-019-00173-y},
  journal      = {International Journal of Multimedia Information Retrieval},
  month        = {9},
  number       = {3},
  pages        = {215-227},
  shortjournal = {Int. J. Multimed. Inf. Retr.},
  title        = {Effective video hyperlinking by means of enriched feature sets and monomodal query combinations},
  volume       = {9},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Hypergraph learning with collaborative representation for
image search reranking. <em>IJMIR</em>, <em>9</em>(3), 205–214. (<a
href="https://doi.org/10.1007/s13735-019-00191-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image search reranking has received considerable attention in recent years. It aims at refining the text-based image search results by boosting the rank of relevant images. Hypergraph has been widely used for relevance estimation, where textual results are taken as vertices and the hypergraph ranking is performed to learn their relevance scores. Rather than using the K-nearest neighbor method, recent works have adopted the sparse representation to effectively construct an informative hypergraph. The sparse representation is insensitive to noise and can capture the real neighborhood structure. However, it suffers from a heavy computational cost. Motivated by this observation, in this paper, we leveraged the ridge regression for hypergraph construction. By imposing an $$\ell _2$$ -regularizer on the size of their regression coefficients, the ridge regression enforces the training samples to collaborate to represent one query. The so-called collaborative representation exhibits more discriminative power and robustness while being computationally efficient. Thereafter, based on the obtained collaborative representation vectors, we measured the pairwise similarities among samples and generated hyperedges. Extensive experiments on the public MediaEval benchmarks demonstrated the effectiveness and superiority of our method over the state-of-the-art reranking methods.},
  archive      = {J_IJMIR},
  author       = {Bouhlel, Noura and Feki, Ghada and Ben Ammar, Anis and Ben Amar, Chokri},
  doi          = {10.1007/s13735-019-00191-w},
  journal      = {International Journal of Multimedia Information Retrieval},
  month        = {9},
  number       = {3},
  pages        = {205-214},
  shortjournal = {Int. J. Multimed. Inf. Retr.},
  title        = {Hypergraph learning with collaborative representation for image search reranking},
  volume       = {9},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Image annotation: The effects of content, lexicon and
annotation method. <em>IJMIR</em>, <em>9</em>(3), 191–203. (<a
href="https://doi.org/10.1007/s13735-020-00193-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image annotation is the process of assigning metadata to images, allowing effective retrieval by text-based search techniques. Despite the lots of efforts in automatic multimedia analysis, automatic semantic annotation of multimedia is still inefficient due to the problems in modeling high-level semantic terms. In this paper, we examine the factors affecting the quality of annotations collected through crowdsourcing platforms. An image dataset was manually annotated utilizing: (1) a vocabulary consists of preselected set of keywords, (2) an hierarchical vocabulary and (3) free keywords. The results show that the annotation quality is affected by the image content itself and the used lexicon. As we expected while annotation using the hierarchical vocabulary is more representative, the use of free keywords leads to increased invalid annotation. Finally, it is shown that images requiring annotations that are not directly related to their content (i.e., annotation using abstract concepts) lead to accrue annotator inconsistency revealing in that way the difficulty in annotating such kind of images is not limited to automatic annotation, but it is a generic problem of annotation.},
  archive      = {J_IJMIR},
  author       = {Theodosiou, Zenonas and Tsapatsoulis, Nicolas},
  doi          = {10.1007/s13735-020-00193-z},
  journal      = {International Journal of Multimedia Information Retrieval},
  month        = {9},
  number       = {3},
  pages        = {191-203},
  shortjournal = {Int. J. Multimed. Inf. Retr.},
  title        = {Image annotation: The effects of content, lexicon and annotation method},
  volume       = {9},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A survey on instance segmentation: State of the art.
<em>IJMIR</em>, <em>9</em>(3), 171–189. (<a
href="https://doi.org/10.1007/s13735-020-00195-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Object detection or localization is an incremental step in progression from coarse to fine digital image inference. It not only provides the classes of the image objects, but also provides the location of the image objects which have been classified. The location is given in the form of bounding boxes or centroids. Semantic segmentation gives fine inference by predicting labels for every pixel in the input image. Each pixel is labelled according to the object class within which it is enclosed. Furthering this evolution, instance segmentation gives different labels for separate instances of objects belonging to the same class. Hence, instance segmentation may be defined as the technique of simultaneously solving the problem of object detection as well as that of semantic segmentation. In this survey paper on instance segmentation, its background, issues, techniques, evolution, popular datasets, related work up to the state of the art and future scope have been discussed. The paper provides valuable information for those who want to do research in the field of instance segmentation.},
  archive      = {J_IJMIR},
  author       = {Hafiz, Abdul Mueed and Bhat, Ghulam Mohiuddin},
  doi          = {10.1007/s13735-020-00195-x},
  journal      = {International Journal of Multimedia Information Retrieval},
  month        = {9},
  number       = {3},
  pages        = {171-189},
  shortjournal = {Int. J. Multimed. Inf. Retr.},
  title        = {A survey on instance segmentation: State of the art},
  volume       = {9},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A survey of traditional and deep learning-based feature
descriptors for high dimensional data in computer vision.
<em>IJMIR</em>, <em>9</em>(3), 135–170. (<a
href="https://doi.org/10.1007/s13735-019-00183-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Higher dimensional data such as video and 3D are the leading edge of multimedia retrieval and computer vision research. In this survey, we give a comprehensive overview and key insights into the state of the art of higher dimensional features from deep learning and also traditional approaches. Current approaches are frequently using 3D information from the sensor or are using 3D in modeling and understanding the 3D world. With the growth of prevalent application areas such as 3D games, self-driving automobiles, health monitoring and sports activity training, a wide variety of new sensors have allowed researchers to develop feature description models beyond 2D. Although higher dimensional data enhance the performance of methods on numerous tasks, they can also introduce new challenges and problems. The higher dimensionality of the data often leads to more complicated structures which present additional problems in both extracting meaningful content and in adapting it for current machine learning algorithms. Due to the major importance of the evaluation process, we also present an overview of the current datasets and benchmarks. Moreover, based on more than 330 papers from this study, we present the major challenges and future directions.},
  archive      = {J_IJMIR},
  author       = {Georgiou, Theodoros and Liu, Yu and Chen, Wei and Lew, Michael},
  doi          = {10.1007/s13735-019-00183-w},
  journal      = {International Journal of Multimedia Information Retrieval},
  month        = {9},
  number       = {3},
  pages        = {135-170},
  shortjournal = {Int. J. Multimed. Inf. Retr.},
  title        = {A survey of traditional and deep learning-based feature descriptors for high dimensional data in computer vision},
  volume       = {9},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A retrieval-based approach for diverse and image-specific
adversary selection. <em>IJMIR</em>, <em>9</em>(2), 125–133. (<a
href="https://doi.org/10.1007/s13735-019-00177-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {While deep neural network-based models have demonstrated compelling performance on various tasks in computer vision and other fields, they have been found to be vulnerable to adversarial attacks. Particularly, deep convolutional neural network (CNN)-based models can be easily fooled by adding a small quasi-imperceptible perturbation to the input, thus resulting in significant drop in prediction accuracies. While most of the previous works have focused on generating one adversary/perturbation per model, it was recently shown that it is possible to learn a continuous distribution over adversarial perturbations for a model. Building upon this work, in this paper, we propose a new technique for image-specific adversary selection and treat it as a retrieval task. The proposed technique utilizes a learned model that ranks the perturbations in a given set of perturbations based on their ability to fool with respect to a given sample. This model is a conditional determinantal point process model that also explicitly induces diversity among the retrieved perturbations. We conduct experiments on the ImageNet dataset using four popular deep CNN image classification models, and demonstrate that the proposed method consistently achieves state-of-the-art fooling rates.},
  archive      = {J_IJMIR},
  author       = {Ravat, Rajvardhan Singh and Verma, Yashaswi},
  doi          = {10.1007/s13735-019-00177-8},
  journal      = {International Journal of Multimedia Information Retrieval},
  month        = {6},
  number       = {2},
  pages        = {125-133},
  shortjournal = {Int. J. Multimed. Inf. Retr.},
  title        = {A retrieval-based approach for diverse and image-specific adversary selection},
  volume       = {9},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Learning visual features for relational CBIR.
<em>IJMIR</em>, <em>9</em>(2), 113–124. (<a
href="https://doi.org/10.1007/s13735-019-00178-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent works in deep-learning research highlighted remarkable relational reasoning capabilities of some carefully designed architectures. In this work, we employ a relationship-aware deep learning model to extract compact visual features used relational image descriptors. In particular, we are interested in relational content-based image retrieval (R-CBIR), a task consisting in finding images containing similar inter-object relationships. Inspired by the relation networks (RN) employed in relational visual question answering (R-VQA), we present novel architectures to explicitly capture relational information from images in the form of network activations that can be subsequently extracted and used as visual features. We describe a two-stage relation network module (2S-RN), trained on the R-VQA task, able to collect non-aggregated visual features. Then, we propose the aggregated visual features relation network (AVF-RN) module that is able to produce better relationship-aware features by learning the aggregation directly inside the network. We employ an R-CBIR ground-truth built by exploiting scene-graphs similarities available in the CLEVR dataset in order to rank images in a relational fashion. Experiments show that features extracted from our 2S-RN model provide an improved retrieval performance with respect to standard non-relational methods. Moreover, we demonstrate that the features extracted from the novel AVF-RN can further improve the performance measured on the R-CBIR task, reaching the state-of-the-art on the proposed dataset.},
  archive      = {J_IJMIR},
  author       = {Messina, Nicola and Amato, Giuseppe and Carrara, Fabio and Falchi, Fabrizio and Gennaro, Claudio},
  doi          = {10.1007/s13735-019-00178-7},
  journal      = {International Journal of Multimedia Information Retrieval},
  month        = {6},
  number       = {2},
  pages        = {113-124},
  shortjournal = {Int. J. Multimed. Inf. Retr.},
  title        = {Learning visual features for relational CBIR},
  volume       = {9},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Multi-level context extraction and attention-based
contextual inter-modal fusion for multimodal sentiment analysis and
emotion classification. <em>IJMIR</em>, <em>9</em>(2), 103–112. (<a
href="https://doi.org/10.1007/s13735-019-00185-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The recent advancements in the Internet technology and its associated services, led the users to post a large amount of multimodal data into social media Web sites, online shopping portals, video repositories, etc. The availability of the huge amount of multimodal content, multimodal sentiment classification, and affective computing has become the most researched topic. The extraction of context among the neighboring utterances and considering the importance of inter-modal utterances before multimodal fusion are the most important research issues in this field. This article presents a novel approach to extract the context at multiple levels and to understand the importance of inter-modal utterances in sentiment and emotion classification. Experiments are conducted on two publically accepted datasets such as CMU-MOSI for sentiment analysis and IEMOCAP for emotion classification. By incorporating the utterance-level contextual information and importance of inter-modal utterances, the proposed model outperforms the standard baselines by over 3% in classification accuracy.},
  archive      = {J_IJMIR},
  author       = {Huddar, Mahesh G. and Sannakki, Sanjeev S. and Rajpurohit, Vijay S.},
  doi          = {10.1007/s13735-019-00185-8},
  journal      = {International Journal of Multimedia Information Retrieval},
  month        = {6},
  number       = {2},
  pages        = {103-112},
  shortjournal = {Int. J. Multimed. Inf. Retr.},
  title        = {Multi-level context extraction and attention-based contextual inter-modal fusion for multimodal sentiment analysis and emotion classification},
  volume       = {9},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A study on deep learning spatiotemporal models and feature
extraction techniques for video understanding. <em>IJMIR</em>,
<em>9</em>(2), 81–101. (<a
href="https://doi.org/10.1007/s13735-019-00190-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Video understanding requires abundant semantic information. Substantial progress has been made on deep learning models in the image, text, and audio domains, and notable efforts have been recently dedicated to the design of deep networks in the video domain. We discuss the state-of-the-art convolutional neural network (CNN) and its pipelines for the exploration of video features, various fusion strategies, and their performances; we also discuss the limitations of CNN for long-term motion cues and the use of sequential learning models such as long short-term memory to overcome these limitations. In addition, we address various multi-model approaches for extracting important cues and score fusion techniques from hybrid deep learning frameworks. Then, we highlight future plans in this domain, recent trends, and substantial challenges for video understanding. This survey’s objectives are to study the plethora of approaches that have been developed for solving video understanding problems, to comprehensively study spatiotemporal cues, to explore the various models that are available for solving these problems and to identify the most promising approaches.},
  archive      = {J_IJMIR},
  author       = {Suresha, M. and Kuppa, S. and Raghukumar, D. S.},
  doi          = {10.1007/s13735-019-00190-x},
  journal      = {International Journal of Multimedia Information Retrieval},
  month        = {6},
  number       = {2},
  pages        = {81-101},
  shortjournal = {Int. J. Multimed. Inf. Retr.},
  title        = {A study on deep learning spatiotemporal models and feature extraction techniques for video understanding},
  volume       = {9},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Single-image crowd counting: A comparative survey on deep
learning-based approaches. <em>IJMIR</em>, <em>9</em>(2), 63–80. (<a
href="https://doi.org/10.1007/s13735-019-00181-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Crowd counting is an attracting computer vision problem. Solutions to crowd counting hold high adaptability to other counting problems such as traffic counting and cell counting. Numerous methods have been proposed for the problem. Deep learning-based methods play a significant role in recent advancement. However, no existing literature reviews capture their sophisticated development by challenges. In this paper, we discuss and categorize recent deep learning works in crowd counting by considering how they address the challenges.},
  archive      = {J_IJMIR},
  author       = {Nguyen, Vy and Ngo, Thanh Duc},
  doi          = {10.1007/s13735-019-00181-y},
  journal      = {International Journal of Multimedia Information Retrieval},
  month        = {6},
  number       = {2},
  pages        = {63-80},
  shortjournal = {Int. J. Multimed. Inf. Retr.},
  title        = {Single-image crowd counting: A comparative survey on deep learning-based approaches},
  volume       = {9},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Special issue on deep learning in image and video retrieval.
<em>IJMIR</em>, <em>9</em>(2), 61–62. (<a
href="https://doi.org/10.1007/s13735-020-00194-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_IJMIR},
  author       = {Oerlemans, Ard and Guo, Yanming and Lew, Michael S. and Chua, Tat-Seng},
  doi          = {10.1007/s13735-020-00194-y},
  journal      = {International Journal of Multimedia Information Retrieval},
  month        = {6},
  number       = {2},
  pages        = {61-62},
  shortjournal = {Int. J. Multimed. Inf. Retr.},
  title        = {Special issue on deep learning in image and video retrieval},
  volume       = {9},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). The focus–aspect–value model for predicting subjective
visual attributes. <em>IJMIR</em>, <em>9</em>(1), 47–60. (<a
href="https://doi.org/10.1007/s13735-019-00188-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Predicting subjective visual interpretation is important for several prominent tasks in computer vision, including multimedia retrieval. Many approaches reduce this problem to the prediction of adjective or attribute labels from images while neglecting attribute semantics and only processing the image in a holistic manner. Furthermore, there is a lack of relevant datasets with fine-grained subjective labels and sufficient scale for machine learning. In this paper, we explain the Focus–Aspect–Value (FAV) model to break down the process of subjective image interpretation into three steps and describe a dataset following this way of modeling. We train and evaluate several deep learning methods on this dataset, while we extend the experiments of the paper originally introducing FAV by adding a new evaluation metric, improving the concatenation approach and adding Multiplicative Fusion as another method. In our experiments, Tensor Fusion is among the best performing methods across all measures and outperforms the default way of information fusion (concatenation). In addition, we find that the way of combining information in neural networks not only affects prediction performance but can drastically change other properties of the model as well.},
  archive      = {J_IJMIR},
  author       = {Blandfort, Philipp and Karayil, Tushar and Hees, Jörn and Dengel, Andreas},
  doi          = {10.1007/s13735-019-00188-5},
  journal      = {International Journal of Multimedia Information Retrieval},
  month        = {3},
  number       = {1},
  pages        = {47-60},
  shortjournal = {Int. J. Multimed. Inf. Retr.},
  title        = {The Focus–Aspect–Value model for predicting subjective visual attributes},
  volume       = {9},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Characterization and classification of semantic image-text
relations. <em>IJMIR</em>, <em>9</em>(1), 31–45. (<a
href="https://doi.org/10.1007/s13735-019-00187-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The beneficial, complementary nature of visual and textual information to convey information is widely known, for example, in entertainment, news, advertisements, science, or education. While the complex interplay of image and text to form semantic meaning has been thoroughly studied in linguistics and communication sciences for several decades, computer vision and multimedia research remained on the surface of the problem more or less. An exception is previous work that introduced the two metrics Cross-Modal Mutual Information and Semantic Correlation in order to model complex image-text relations. In this paper, we motivate the necessity of an additional metric called Status in order to cover complex image-text relations more completely. This set of metrics enables us to derive a novel categorization of eight semantic image-text classes based on three dimensions. In addition, we demonstrate how to automatically gather and augment a dataset for these classes from the Web. Further, we present a deep learning system to automatically predict either of the three metrics, as well as a system to directly predict the eight image-text classes. Experimental results show the feasibility of the approach, whereby the predict-all approach outperforms the cascaded approach of the metric classifiers.},
  archive      = {J_IJMIR},
  author       = {Otto, Christian and Springstein, Matthias and Anand, Avishek and Ewerth, Ralph},
  doi          = {10.1007/s13735-019-00187-6},
  journal      = {International Journal of Multimedia Information Retrieval},
  month        = {3},
  number       = {1},
  pages        = {31-45},
  shortjournal = {Int. J. Multimed. Inf. Retr.},
  title        = {Characterization and classification of semantic image-text relations},
  volume       = {9},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). ContextNet: Representation and exploration for painting
classification and retrieval in context. <em>IJMIR</em>, <em>9</em>(1),
17–30. (<a href="https://doi.org/10.1007/s13735-019-00189-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In automatic art analysis, models that besides the visual elements of an artwork represent the relationships between the different artistic attributes could be very informative. Those kinds of relationships, however, usually appear in a very subtle way, being extremely difficult to detect with standard convolutional neural networks. In this work, we propose to capture contextual artistic information from fine-art paintings with a specific ContextNet network. As context can be obtained from multiple sources, we explore two modalities of ContextNets: one based on multitask learning and another one based on knowledge graphs. Once the contextual information is obtained, we use it to enhance visual representations computed with a neural network. In this way, we are able to (1) capture information about the content and the style with the visual representations and (2) encode relationships between different artistic attributes with the ContextNet. We evaluate our models on both painting classification and retrieval, and by visualising the resulting embeddings on a knowledge graph, we can confirm that our models represent specific stylistic aspects present in the data.},
  archive      = {J_IJMIR},
  author       = {Garcia, Noa and Renoust, Benjamin and Nakashima, Yuta},
  doi          = {10.1007/s13735-019-00189-4},
  journal      = {International Journal of Multimedia Information Retrieval},
  month        = {3},
  number       = {1},
  pages        = {17-30},
  shortjournal = {Int. J. Multimed. Inf. Retr.},
  title        = {ContextNet: Representation and exploration for painting classification and retrieval in context},
  volume       = {9},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Hierarchical attentive deep neural networks for semantic
music annotation through multiple music representations. <em>IJMIR</em>,
<em>9</em>(1), 3–16. (<a
href="https://doi.org/10.1007/s13735-019-00186-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Automatically assigning a group of appropriate semantic tags to one music piece provides an effective way for people to efficiently utilize the massive and ever increasing online and off-line music data. In this paper, we propose a novel end-to-end deep neural network model for automatic music annotation, which effectively integrates available multiple complementary music representations and jointly accomplishes music representation learning, structure modeling, and tag prediction. The model first hierarchically leverages attentive convolutional networks and recurrent networks to learn informative descriptions from Mel-spectrogram and raw waveform of the music and depict time-varying structures embedded in the description sequence. A dual-state LSTM network is then employed to capture the correlations between two representation channels as supplementary music descriptions. Finally, the model aggregates music description sequence into a holistic embedding with a self-attentive multi-weighting mechanism, which adaptively captures multi-aspect summarized information of the music for tag prediction. Experiments on the public MagnaTagATune benchmark music dataset show that the proposed model outperforms state-of-the-art methods for automatic music annotation.},
  archive      = {J_IJMIR},
  author       = {Wang, Qianqian and Su, Feng and Wang, Yuyang},
  doi          = {10.1007/s13735-019-00186-7},
  journal      = {International Journal of Multimedia Information Retrieval},
  month        = {3},
  number       = {1},
  pages        = {3-16},
  shortjournal = {Int. J. Multimed. Inf. Retr.},
  title        = {Hierarchical attentive deep neural networks for semantic music annotation through multiple music representations},
  volume       = {9},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Editorial for the ICMR 2019 special issue. <em>IJMIR</em>,
<em>9</em>(1), 1–2. (<a
href="https://doi.org/10.1007/s13735-020-00192-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_IJMIR},
  author       = {Candan, K. Selçuk and Bertini, Marco and Wei, Xiao-Yong and Xie, Lexing},
  doi          = {10.1007/s13735-020-00192-0},
  journal      = {International Journal of Multimedia Information Retrieval},
  month        = {3},
  number       = {1},
  pages        = {1-2},
  shortjournal = {Int. J. Multimed. Inf. Retr.},
  title        = {Editorial for the ICMR 2019 special issue},
  volume       = {9},
  year         = {2020},
}
</textarea>
</details></li>
</ul>

</body>
</html>
