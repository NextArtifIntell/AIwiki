<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>IJMLC_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="ijmlc---177">IJMLC - 177</h2>
<ul>
<li><details>
<summary>
(2020). A hybrid method of recurrent neural network and graph neural
network for next-period prescription prediction. <em>IJMLC</em>,
<em>11</em>(12), 2849–2856. (<a
href="https://doi.org/10.1007/s13042-020-01155-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Electronic health records (EHRs) have been widely used to help physicians to make decisions by predicting medical events such as diseases, prescriptions, outcomes, and so on. How to represent patient longitudinal medical data is the key to making these predictions. Recurrent neural network (RNN) is a popular model for patient longitudinal medical data representation from the view of patient status sequences, but it cannot represent complex interactions among different types of medical information, i.e., temporal medical event graphs, which can be represented by graph neural network (GNN). In this paper, we propose a hybrid method of RNN and GNN, called RGNN, for next-period prescription prediction from two views, where RNN is used to represent patient status sequences, and GNN is used to represent temporal medical event graphs. Experiments conducted on the public MIMIC-III ICU data show that the proposed method is effective for next-period prescription prediction, and RNN and GNN are mutually complementary.},
  archive      = {J_IJMLC},
  author       = {Liu, Sicen and Li, Tao and Ding, Haoyang and Tang, Buzhou and Wang, Xiaolong and Chen, Qingcai and Yan, Jun and Zhou, Yi},
  doi          = {10.1007/s13042-020-01155-x},
  journal      = {International Journal of Machine Learning and Cybernetics},
  number       = {12},
  pages        = {2849-2856},
  shortjournal = {Int. J. Mach. Learn. Cybern.},
  title        = {A hybrid method of recurrent neural network and graph neural network for next-period prescription prediction},
  volume       = {11},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Benchmarking algorithms for food localization and semantic
segmentation. <em>IJMLC</em>, <em>11</em>(12), 2827–2847. (<a
href="https://doi.org/10.1007/s13042-020-01153-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The problem of food segmentation is quite challenging since food is characterized by intrinsic high intra-class variability. Also, segmentation of food images taken in-the-wild may be characterized by acquisition artifacts, and that could be problematic for the segmentation algorithms. A proper evaluating of segmentation algorithms is of paramount importance for the design and improvement of food analysis systems that can work in less-than-ideal real scenarios. In this paper, we evaluate the performance of different deep learning-based segmentation algorithms in the context of food. Due to the lack of large-scale food segmentation datasets, we initially create a new dataset composed of 5000 images of 50 diverse food categories. The images are accurately annotated with pixel-wise annotations. In order to test the algorithms under different conditions, the dataset is augmented with the same images but rendered under different acquisition distortions that comprise illuminant change, JPEG compression, Gaussian noise, and Gaussian blur. The final dataset is composed of 120,000 images. Using standard benchmark measures, we conducted extensive experiments to evaluate ten state-of-the-art segmentation algorithms on two tasks: food localization and semantic food segmentation.},
  archive      = {J_IJMLC},
  author       = {Aslan, Sinem and Ciocca, Gianluigi and Mazzini, Davide and Schettini, Raimondo},
  doi          = {10.1007/s13042-020-01153-z},
  journal      = {International Journal of Machine Learning and Cybernetics},
  number       = {12},
  pages        = {2827-2847},
  shortjournal = {Int. J. Mach. Learn. Cybern.},
  title        = {Benchmarking algorithms for food localization and semantic segmentation},
  volume       = {11},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A bibliometric analysis on deep learning during 2007–2019.
<em>IJMLC</em>, <em>11</em>(12), 2807–2826. (<a
href="https://doi.org/10.1007/s13042-020-01152-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As an emerging and applicable method, deep learning (DL) has attracted much attention in recent years. With the development of DL and the massive of publications and researches in this direction, a comprehensive analysis of DL is necessary. In this paper, from the perspective of bibliometrics, a comprehensive analysis of publications of DL is deployed from 2007 to 2019 (the first publication with keywords “deep learning” and “machine learning” was published in 2007). By preprocessing, 5722 publications are exported from Web of Science and they are imported into the professional science mapping tools: VOS viewer and Cite Space. Firstly, the publication structures are analyzed based on annual publications, and the publication of the most productive countries/regions, institutions and authors. Secondly, by the use of VOS viewer, the co-citation networks of countries/regions, institutions, authors and papers are depicted. The citation structure of them and the most influential of them are further analyzed. Thirdly, the cooperation networks of countries/regions, institutions and authors are illustrated by VOS viewer. Time-line review and citation burst detection of keywords are exported from Cite Space to detect the hotspots and research trend. Finally, some conclusions of this paper are given. This paper provides a preliminary knowledge of DL for researchers who are interested in this area, and also makes a conclusive and comprehensive analysis of DL for these who want to do further research on this area.},
  archive      = {J_IJMLC},
  author       = {Li, Yang and Xu, Zeshui and Wang, Xinxin and Wang, Xizhao},
  doi          = {10.1007/s13042-020-01152-0},
  journal      = {International Journal of Machine Learning and Cybernetics},
  number       = {12},
  pages        = {2807-2826},
  shortjournal = {Int. J. Mach. Learn. Cybern.},
  title        = {A bibliometric analysis on deep learning during 2007–2019},
  volume       = {11},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Multivariate morphological reconstruction based fuzzy
clustering with a weighting multi-channel guided image filter for color
image segmentation. <em>IJMLC</em>, <em>11</em>(12), 2793–2806. (<a
href="https://doi.org/10.1007/s13042-020-01151-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The fuzzy c-means clustering with guided image filter (GF) is a useful method for image segmentation. The single-channel GF can be efficiently applied to the gray-scale guidance image, but for the color guidance image, due to the high run-time overhead on the calculation of the inverse of the covariance matrix, it is a hard work to perform the multi-channel GF. To address this issue, we propose a novel weighting multi-channel guided image filter (WMGF) method. In this method, each channel of the color guidance image is utilized to guide the filtering for the input image independently and a novel weight is defined for each channel according to the variance of the image pixels in a local window, which greatly eliminates the mutual influence between different channels and brings about a low run-time overhead. In addition, based on the WMGF method, we present a new fuzzy c-means clustering algorithm ( $$\hbox {FCM}_{\scriptscriptstyle {WMGF }}$$ ) for the color image segmentation, in which the WMGF is performed on the membership matrix in each iteration of the fuzzy c-means clustering. To further enhance the different noise-immunity and edge preservation, the multivariate morphological reconstruction (MMR) method is introduced into the proposed fuzzy clustering method (MMR $$\_\hbox {FCM}_{\scriptscriptstyle {WMGF }}$$ ) to obtain higher segmentation precision. Experiments on color images with Salt &amp; Pepper and Gaussian noises demonstrate the superiority of the proposed methods.},
  archive      = {J_IJMLC},
  author       = {Xu, Guangmei and Zhou, Jin and Dong, Jiwen and Chen, C. L. Philip and Zhang, Tong and Chen, Long and Han, Shiyuan and Wang, Lin and Chen, Yuehui},
  doi          = {10.1007/s13042-020-01151-1},
  journal      = {International Journal of Machine Learning and Cybernetics},
  number       = {12},
  pages        = {2793-2806},
  shortjournal = {Int. J. Mach. Learn. Cybern.},
  title        = {Multivariate morphological reconstruction based fuzzy clustering with a weighting multi-channel guided image filter for color image segmentation},
  volume       = {11},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A faster tensor robust PCA via tensor factorization.
<em>IJMLC</em>, <em>11</em>(12), 2771–2791. (<a
href="https://doi.org/10.1007/s13042-020-01150-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many kinds of real-world multi-way signal, like color images, videos, etc., are represented in tensor form and may often be corrupted by outliers. To recover an unknown signal tensor corrupted by outliers, tensor robust principal component analysis (TRPCA) serves as a robust tensorial modification of the fundamental PCA. Recently, a successful TRPCA model based on the tubal nuclear norm (TNN) (Lu et al. in IEEE Trans Pattern Anal Mach Intell 42:925–938, 2019) has attracted much attention thanks to its superiority in many applications. However, TNN is computationally expensive due to the requirement of full singular value decompositions, seriously limiting its scalability to large tensors. To address this issue, we propose a new TRPCA model which adopts a factorization strategy. Algorithmically, an algorithm based on the non-convex augmented Lagrangian method is developed with convergence guarantee. Theoretically, we rigorously establish the sub-optimality of the proposed algorithm. We also extend the proposed model to the robust tensor completion problem. Both the effectiveness and efficiency of the proposed algorithm is demonstrated through extensive experiments on both synthetic and real data sets.},
  archive      = {J_IJMLC},
  author       = {Wang, An-Dong and Jin, Zhong and Yang, Jing-Yu},
  doi          = {10.1007/s13042-020-01150-2},
  journal      = {International Journal of Machine Learning and Cybernetics},
  number       = {12},
  pages        = {2771-2791},
  shortjournal = {Int. J. Mach. Learn. Cybern.},
  title        = {A faster tensor robust PCA via tensor factorization},
  volume       = {11},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Using search-based techniques for testing executable
software models specified through graph transformations. <em>IJMLC</em>,
<em>11</em>(12), 2743–2770. (<a
href="https://doi.org/10.1007/s13042-020-01149-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Design by contract is a software development methodology that uses contracts for defining interfaces among interacting components of a software system. Graph transformation system is used to specify the behavioral aspects of software components by defining the pre- and post-conditions of methods as contracts. In this paper, we focus on testing executable software models specified by a graph transformation system. A set of model-specific coverage criteria and a cost-aware search-based test generation approach are introduced. To evaluate the effectiveness of the proposed coverage criteria and the test generation approach, a type of mutation analysis is presented at the model level. Furthermore, a couple of fault-detection methods are used to assess the quality of the generated tests in the model-level mutation analysis. The proposed approach is implemented in GROOVE, a toolset for model checking graph transformation systems. The empirical results based on some well-known case studies demonstrate the efficiency and scalability of each proposed coverage criterion and testing approach. The comparison of the proposed test generation approach with state-of-the-art techniques indicates a significant improvement in terms of fault-detection capability and testing costs.},
  archive      = {J_IJMLC},
  author       = {Bahrampour, Anvar and Rafe, Vahid},
  doi          = {10.1007/s13042-020-01149-9},
  journal      = {International Journal of Machine Learning and Cybernetics},
  number       = {12},
  pages        = {2743-2770},
  shortjournal = {Int. J. Mach. Learn. Cybern.},
  title        = {Using search-based techniques for testing executable software models specified through graph transformations},
  volume       = {11},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Multi-expert multi-criteria decision making based on the
likelihoods of interval type-2 trapezoidal fuzzy preference relations.
<em>IJMLC</em>, <em>11</em>(12), 2719–2741. (<a
href="https://doi.org/10.1007/s13042-020-01148-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Interval type-2 trapezoidal fuzzy sets, as a particular form of interval type-2 fuzzy sets, can precisely characterize the subjective assessments and qualitative evaluations of a group of experts. In this paper, a novel likelihood-based interval type-2 trapezoidal fuzzy multi-expert multi-criteria decision-making approach is proposed. To do so, the concepts of likelihood-based performance index, likelihood-based comprehensive evaluation value, and signed distance-based evaluation value are adopted. The interval type-2 trapezoidal fuzzy Bonferroni aggregation operator is utilized to construct the likelihood-based interval type-2 trapezoidal fuzzy preference relations. Then, the consistent lower and upper likelihoods are adopted to enhance the efficiency of the group decision making framework. The proposed multi-expert decision making approach works well when there is high degree of fluctuations in the number of criteria and experts. The practicability and feasibility of the proposed approach are validated by applications to four cases. Several comparative analyses are conducted to authenticate the dominancy of the proposed method over conventional interval type-2 trapezoidal fuzzy multi-criteria decision-making approaches.},
  archive      = {J_IJMLC},
  author       = {Hendiani, Sepehr and Jiang, Lisheng and Sharifi, Ebrahim and Liao, Huchang},
  doi          = {10.1007/s13042-020-01148-w},
  journal      = {International Journal of Machine Learning and Cybernetics},
  number       = {12},
  pages        = {2719-2741},
  shortjournal = {Int. J. Mach. Learn. Cybern.},
  title        = {Multi-expert multi-criteria decision making based on the likelihoods of interval type-2 trapezoidal fuzzy preference relations},
  volume       = {11},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Multiresolution analysis relying on beta wavelet transform
and multi-mother wavelet network for a novel 3D mesh alignment and
deformation technique. <em>IJMLC</em>, <em>11</em>(12), 2703–2717. (<a
href="https://doi.org/10.1007/s13042-020-01146-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose a new 3D high mesh deformation technique to extract intuitive and interpretable deformation and alignment components. Our framework is based on a fast Beta wavelet transform for a multi-resolution analysis relying on multi-library wavelet neural network architecture. The main drawback of 3D high mesh deformation is the large number of triangles necessary to characterize a smooth surface; the majority of these techniques impose a very high computational cost. Our approach is based on the idea of combining the decomposition technique of multi-resolution analysis by Beta wavelet transform for each level of deformation process and a multi-mother wavelet network structure to construct an effective 3D alignment algorithm. We use, in our experiment, only the approximation coefficients at a chosen decomposition level to reduce the complexity of the mesh and to facilitate the alignment until reaching the target mesh, for the purpose of improving various executions and obtaining an optimal solution while reducing the error between the original and the reconstructed object to create a well-formed object. Then, to enhance the performance of wavelet networks, a novel learning algorithm based on multi-mother wavelet neural network architecture using trust region spherical is employed as an approximation tool for feature alignment between the source and the target models. This network architecture ensures the use of several mother wavelets to solve the problem of high mesh deformation utilizing the best wavelet mother that well models the object. Extensive experimental results demonstrate that the progressive deformation processes aim at avoiding the weaknesses of traditional approaches such as the slowness and the difficulty of finding an exact reconstruction.},
  archive      = {J_IJMLC},
  author       = {Dhibi, Naziha and Ben amar, Chokri},
  doi          = {10.1007/s13042-020-01146-y},
  journal      = {International Journal of Machine Learning and Cybernetics},
  number       = {12},
  pages        = {2703-2717},
  shortjournal = {Int. J. Mach. Learn. Cybern.},
  title        = {Multiresolution analysis relying on beta wavelet transform and multi-mother wavelet network for a novel 3D mesh alignment and deformation technique},
  volume       = {11},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Incremental hashing with sample selection using dominant
sets. <em>IJMLC</em>, <em>11</em>(12), 2689–2702. (<a
href="https://doi.org/10.1007/s13042-020-01145-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the world of big data, large amounts of images are available in social media, corporate and even personal collections. A collection may grow quickly as new images are generated at high rates. The new images may cause changes in the distribution of existing classes or the emergence of new classes, resulting in the collection being dynamic and having concept drift. For efficient image retrieval from an image collection using a query, a hash table consisting of a set of hash functions is needed to transform images into binary hash codes which are used as the basis to find similar images to the query. If the image collection is dynamic, the hash table built at one time step may not work well at the next due to changes in the collection as a result of new images being added. Therefore, the hash table needs to be rebuilt or updated at successive time steps. Incremental hashing (ICH) is the first effective method to deal with the concept drift problem in image retrieval from dynamic collections. In ICH, a new hash table is learned based on newly emerging images only which represent data distribution of the current data environment. The new hash table is used to generate hash codes for all images including old and new ones. Due to the dynamic nature, new images of one class may not be similar to old images of the same class. In order to learn new hash table that preserves within-class similarity in both old and new images, incremental hashing with sample selection using dominant sets (ICHDS) is proposed in this paper, which selects representative samples from each class for training the new hash table. Experimental results show that ICHDS yields better retrieval performance than existing dynamic and static hashing methods.},
  archive      = {J_IJMLC},
  author       = {Ng, Wing W. Y. and Jiang, Xiaoxia and Tian, Xing and Pelillo, Marcello and Wang, Hui and Kwong, Sam},
  doi          = {10.1007/s13042-020-01145-z},
  journal      = {International Journal of Machine Learning and Cybernetics},
  number       = {12},
  pages        = {2689-2702},
  shortjournal = {Int. J. Mach. Learn. Cybern.},
  title        = {Incremental hashing with sample selection using dominant sets},
  volume       = {11},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Online planning for relative optimal and safe paths for USVs
using a dual sampling domain reduction-based RRT* method.
<em>IJMLC</em>, <em>11</em>(12), 2665–2687. (<a
href="https://doi.org/10.1007/s13042-020-01144-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A heuristic Dual sampling domain Reduction-based Optimal Rapidly-exploring Random Tree scheme is proposed by guiding the planning procedure of the optimal rapidly-exploring random tree (RRT*) method through learning environmental knowledge. The scheme aims to plan low fuel expenditure, easy-execution, and low collision probability paths online for an unmanned surface vehicle (USV) under constraints. First, an elliptic sampling domain, which is subject to an elliptic equation and the shortest obstacle avoidance path estimation, is created to plan short paths. Second, by the consideration of the USV motion states, obstacles and external interferences of the current, the near sampling domains of tree nodes are reduced to exclude high-cost sampling domains. Path feasibility is ensured by explicitly handling motion constraints. Third, a safe distance-based collision detection (CD) scheme and a velocity-based bounding box of USV are proposed to decrease the path collision probability. Additionally, a layered USV online path planning framework is built in accordance with the model predictive control method, and the path smoothing scheme is applied via the Dubins curve under the curvature constraint. Results demonstrate that the proposed dual sampling domain reduction method outperforms traditional reduction schemes in terms of improving the execution efficiency of RRT*. Meanwhile, the proposed CD method is more reliable than the conventional one.},
  archive      = {J_IJMLC},
  author       = {Wen, Naifeng and Zhang, Rubo and Wu, Junwei and Liu, Guanqun},
  doi          = {10.1007/s13042-020-01144-0},
  journal      = {International Journal of Machine Learning and Cybernetics},
  number       = {12},
  pages        = {2665-2687},
  shortjournal = {Int. J. Mach. Learn. Cybern.},
  title        = {Online planning for relative optimal and safe paths for USVs using a dual sampling domain reduction-based RRT* method},
  volume       = {11},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Consensus of nonlinear multi-agent systems with fuzzy
modelling uncertainties via state-constraint hybrid impulsive protocols.
<em>IJMLC</em>, <em>11</em>(12), 2653–2664. (<a
href="https://doi.org/10.1007/s13042-020-01140-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, the nonlinear multi-agent system which contains uncertainty and is controlled by state-constraint impulsive protocol is taken into consideration. For the uncertainty of the multi-agent system, it is replaced by fuzzy logic system approximately and a judgement strategy which only contains the relative information with neighbors is proposed in this paper. In order to do research in state-constraint impulsive protocol, three kinds of impulsive control protocols which conclude partial input saturation, double actuator saturation and single actuator saturation are discussed. Then, some sufficient conditions of the system are obtained to reach consensus. Finally, some numerical simulation examples are provided to prove the effectiveness of the theoretical analysis.},
  archive      = {J_IJMLC},
  author       = {You, Le and Li, Chuandong and Han, Yiyan},
  doi          = {10.1007/s13042-020-01140-4},
  journal      = {International Journal of Machine Learning and Cybernetics},
  number       = {12},
  pages        = {2653-2664},
  shortjournal = {Int. J. Mach. Learn. Cybern.},
  title        = {Consensus of nonlinear multi-agent systems with fuzzy modelling uncertainties via state-constraint hybrid impulsive protocols},
  volume       = {11},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Attentive convolutional gated recurrent network: A
contextual model to sentiment analysis. <em>IJMLC</em>, <em>11</em>(12),
2637–2651. (<a
href="https://doi.org/10.1007/s13042-020-01135-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Considering contextual features is a key issue in sentiment analysis. Existing approaches including convolutional neural networks (CNNs) and recurrent neural networks (RNNs) lack the ability to account and prioritize informative contextual features that are necessary for better sentiment interpretation. CNNs present limited capability since they are required to be very deep, which can lead to the gradient vanishing whereas, RNNs fail because they sequentially process input sequences. Furthermore, the two approaches treat all words equally. In this paper, we suggest a novel approach named attentive convolutional gated recurrent network (ACGRN) that alleviates the above issues for sentiment analysis. The motivation behind ACGRN is to avoid the vanishing gradient caused by deep CNN via applying a shallow-and-wide CNN that learns local contextual features. Afterwards, to solve the problem caused by the sequential structure of RNN and prioritizing informative contextual information, we use a novel prior knowledge attention based bidirectional gated recurrent unit (ATBiGRU). Prior knowledge ATBiGRU captures global contextual features with a strong focus on the previous hidden states that carry more valuable information to the current time step. The experimental results show that ACGRN significantly outperforms the baseline models over six small and large real-world datasets for the sentiment classification task.},
  archive      = {J_IJMLC},
  author       = {Habimana, Olivier and Li, Yuhua and Li, Ruixuan and Gu, Xiwu and Yan, Wenjin},
  doi          = {10.1007/s13042-020-01135-1},
  journal      = {International Journal of Machine Learning and Cybernetics},
  number       = {12},
  pages        = {2637-2651},
  shortjournal = {Int. J. Mach. Learn. Cybern.},
  title        = {Attentive convolutional gated recurrent network: A contextual model to sentiment analysis},
  volume       = {11},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). View-independent representation with frame interpolation
method for skeleton-based human action recognition. <em>IJMLC</em>,
<em>11</em>(12), 2625–2636. (<a
href="https://doi.org/10.1007/s13042-020-01132-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Human action recognition is an important branch of computer vision science. It is a challenging task based on skeletal data because of joints’ complex spatiotemporal information. In this work, we propose a method for action recognition, which consists of three parts: view-independent representation, frame interpolation, and combined model. First, the action sequence becomes view-independent representations independent of the view. Second, when judgment conditions are met, differentiated frame interpolations are used to expand the temporal dimensional information. Then, a combined model is adopted to extract these representation features and classify actions. Experimental results on two multi-view benchmark datasets Northwestern-UCLA and NTU RGB+D demonstrate the effectiveness of our complete method. Although using only one type of action feature and a simple architecture combined model, our complete method still outperforms most of the referential state-of-the-art methods and has strong robustness.},
  archive      = {J_IJMLC},
  author       = {Jiang, Yingguo and Xu, Jun and Zhang, Tong},
  doi          = {10.1007/s13042-020-01132-4},
  journal      = {International Journal of Machine Learning and Cybernetics},
  number       = {12},
  pages        = {2625-2636},
  shortjournal = {Int. J. Mach. Learn. Cybern.},
  title        = {View-independent representation with frame interpolation method for skeleton-based human action recognition},
  volume       = {11},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Fast feature selection for interval-valued data through
kernel density estimation entropy. <em>IJMLC</em>, <em>11</em>(12),
2607–2624. (<a
href="https://doi.org/10.1007/s13042-020-01131-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Kernel density estimation, which is a non-parametric method about estimating probability density distribution of random variables, has been used in feature selection. However, existing feature selection methods based on kernel density estimation seldom consider interval-valued data. Actually, interval-valued data exist widely. In this paper, a feature selection method based on kernel density estimation for interval-valued data is proposed. Firstly, the kernel function in kernel density estimation is defined for interval-valued data. Secondly, the interval-valued kernel density estimation probability structure is constructed by the defined kernel function, including kernel density estimation conditional probability, kernel density estimation joint probability and kernel density estimation posterior probability. Thirdly, kernel density estimation entropies for interval-valued data are proposed by the constructed probability structure, including information entropy, conditional entropy and joint entropy of kernel density estimation. Fourthly, we propose a feature selection approach based on kernel density estimation entropy. Moreover, we improve the proposed feature selection algorithm and propose a fast feature selection algorithm based on kernel density estimation entropy. Finally, comparative experiments are conducted from three perspectives of computing time, intuitive identifiability and classification performance to show the feasibility and the effectiveness of the proposed method.},
  archive      = {J_IJMLC},
  author       = {Dai, Jianhua and Liu, Ye and Chen, Jiaolong and Liu, Xiaofeng},
  doi          = {10.1007/s13042-020-01131-5},
  journal      = {International Journal of Machine Learning and Cybernetics},
  number       = {12},
  pages        = {2607-2624},
  shortjournal = {Int. J. Mach. Learn. Cybern.},
  title        = {Fast feature selection for interval-valued data through kernel density estimation entropy},
  volume       = {11},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A novel handover detection model via frequent trajectory
patterns mining. <em>IJMLC</em>, <em>11</em>(12), 2587–2606. (<a
href="https://doi.org/10.1007/s13042-020-01126-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As the cellular wireless communication techniques grow rapidly, the cells become smaller than the traditional communication system, then the handover events are very frequent and need to support a large number of users, and handover detection has become a very active research direction in a mobile computing environment. In order to copy with the problem of frequent handover operations between base stations in current cellular communication networks as cybernetic systems, we propose a novel handover detection approach by integrating frequent trajectory patterns mining and location prediction techniques. The proposed model contains the following essential steps: (1) mining frequent trajectory patterns from large-scale historical trajectory databases by applying an improved Apriori-like rule-based machine learning algorithm, which finds the intersection of candidate items by applying the trajectory connection operation instead of calculating the support count of each trajectory patterns and the candidate items are considerably reduced; (2) discovering movement rules based on the frequent trajectory pattern set by finding the postfix items of given prefix items satisfying the minimum support threshold; (3) inferring the future locations of moving objects by applying the movement rules matching strategy; (4) determining whether or not to perform handover detection across base stations in a cellular network beyond the discovered prediction results, according to the coverage area of cellular networks. Extensive experiments were conducted on the mobile datasets and the experimental results demonstrate the advantages of the proposed algorithm from the following four aspects: (1) the accuracy of handover detection is above 95\% at average which is a very satisfactory result in a mobile computing environment; (2) the time cost is less than 20 s when the number of movement rules and handover detection is 1000, which further shows the merit of the runtime performance of the proposed method; (3) the frequent-trajectory-patterns based handover detection algorithm can successfully avoid the ping-pong effect due to unnecessary handover operations; (4) and lastly significantly reduce the error rate of frequent handover decisions and the average unnecessary handover rate is lower than 0.05 when compared with the state-of-the-art methods.},
  archive      = {J_IJMLC},
  author       = {Han, Nan and Qiao, Shaojie and Yuan, Guan and Mao, Rui and Yue, Kun and Yuan, Chang-an},
  doi          = {10.1007/s13042-020-01126-2},
  journal      = {International Journal of Machine Learning and Cybernetics},
  number       = {12},
  pages        = {2587-2606},
  shortjournal = {Int. J. Mach. Learn. Cybern.},
  title        = {A novel handover detection model via frequent trajectory patterns mining},
  volume       = {11},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Category-preserving binary feature learning and binary
codebook learning for finger vein recognition. <em>IJMLC</em>,
<em>11</em>(11), 2573–2586. (<a
href="https://doi.org/10.1007/s13042-020-01143-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Local binary feature learning has attracted a lot of researches in image recognition due to its vital effectiveness. Generally, in the traditional local feature learning methods, a projection is learned to map the patches of image into binary features and then a codebook is generated by clustering the binary features with K-means clustering. However, these local feature learning methods, such as compact binary face descriptor and discriminative binary descriptor, ignore the category specific distributions of the original features during the feature learning process and use the real-valued clustering approach to generate the codebook, the discriminant of feature is degraded and the merits of binary feature are lost. To tack these problems, in this paper, we propose a novel category-preserving binary feature learning and binary codebook leaning (CPBFL-BCL) method for finger vein recognition. In CPBFL-BCL, the discrimination of learned binary features is generated by criteria of fisher discriminant analysis and category manifold preserving regularity during the feature learning process, and a novel binary clustering method based on K-means clustering is designed to generate binary codebook. Experimental results on recognition and retrieval tasks using two public finger vein databases are presented and demonstrate the effectiveness and efficiency of the proposed method over the state-of-the-art finger vein methods and a finger vein retrieval method.},
  archive      = {J_IJMLC},
  author       = {Liu, Haiying and Yang, Gongping and Yin, Yilong},
  doi          = {10.1007/s13042-020-01143-1},
  journal      = {International Journal of Machine Learning and Cybernetics},
  number       = {11},
  pages        = {2573-2586},
  shortjournal = {Int. J. Mach. Learn. Cybern.},
  title        = {Category-preserving binary feature learning and binary codebook learning for finger vein recognition},
  volume       = {11},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A novel classification algorithm based on kernelized fuzzy
rough sets. <em>IJMLC</em>, <em>11</em>(11), 2565–2572. (<a
href="https://doi.org/10.1007/s13042-020-01142-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fuzzy kernels are a special kind of kernels which are usually employed to calculate the upper and lower approximations, as well as the positive region in kernelized fuzzy rough sets, and the positive region characterizes the degree of consistency between conditional attributes and decision attributes. When the classification hyperplane exists between two classes of samples, the positive region is transformed into the sum of the distances from the samples to classification hyperplane. The larger the positive region, the higher the degree of consistency. In this paper, we construct a novel model to solve the classification hyperplane from the geometric meaning of the positive region in kernelized fuzzy rough sets. Then, a classification model is developed through maximizing the sum of the distances from the samples to classification hyperplane, and this optimization problem that addresses this objective function is transformed to its dual problem. Experimental results show that the proposed classification algorithm is effective.},
  archive      = {J_IJMLC},
  author       = {Chen, Linlin and Chen, Qingjiu},
  doi          = {10.1007/s13042-020-01142-2},
  journal      = {International Journal of Machine Learning and Cybernetics},
  number       = {11},
  pages        = {2565-2572},
  shortjournal = {Int. J. Mach. Learn. Cybern.},
  title        = {A novel classification algorithm based on kernelized fuzzy rough sets},
  volume       = {11},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Simultaneous feature selection and clustering of micro-array
and RNA-sequence gene expression data using multiobjective optimization.
<em>IJMLC</em>, <em>11</em>(11), 2541–2563. (<a
href="https://doi.org/10.1007/s13042-020-01139-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we have devised a multiobjective optimization solution framework for solving the problem of gene expression data clustering in reduced feature space. Here clustering problem is viewed from two different aspects: clustering of genes in reduced sample space or clustering of samples in reduced gene space. Three objective functions: two internal cluster validity indices and the count on the number of features are optimized simultaneously by a popular multiobjective simulated annealing based approach, namely AMOSA. Here, point symmetry based distance is used for the assignment of gene data points to different clusters. Seven publicly available benchmark gene expression data sets are used for experimental purpose. Both aspects of clustering in reduced feature space is demonstrated. The proposed gene expression clustering technique outperforms the existing nine clustering techniques. Apart from this, also some statistical and biological significant tests have been carried out to show that the proposed FSC-MOO technique is more statistically and biologically enriched},
  archive      = {J_IJMLC},
  author       = {Alok, Abhay Kumar and Gupta, Pooja and Saha, Sriparna and Sharma, Vineet},
  doi          = {10.1007/s13042-020-01139-x},
  journal      = {International Journal of Machine Learning and Cybernetics},
  number       = {11},
  pages        = {2541-2563},
  shortjournal = {Int. J. Mach. Learn. Cybern.},
  title        = {Simultaneous feature selection and clustering of micro-array and RNA-sequence gene expression data using multiobjective optimization},
  volume       = {11},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Human posture recognition based on multiple features and
rule learning. <em>IJMLC</em>, <em>11</em>(11), 2529–2540. (<a
href="https://doi.org/10.1007/s13042-020-01138-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The use of skeleton data for human posture recognition is a key research topic in the human-computer interaction field. To improve the accuracy of human posture recognition, a new algorithm based on multiple features and rule learning is proposed in this paper. Firstly, a 219-dimensional vector that includes angle features and distance features is defined. Specifically, the angle and distance features are defined in terms of the local relationship between joints and the global spatial location of joints. Then, during human posture classification, the rule learning method is used together with the Bagging and random subspace methods to create different samples and features for improved classification performance of sub-classifiers for different samples. Finally, the performance of our proposed algorithm is evaluated on four human posture datasets. The experimental results show that our algorithm can recognize many kinds of human postures effectively, and the results obtained by the rule-based learning method are of higher interpretability than those by traditional machine learning methods and CNNs.},
  archive      = {J_IJMLC},
  author       = {Ding, Weili and Hu, Bo and Liu, Han and Wang, Xinming and Huang, Xiangsheng},
  doi          = {10.1007/s13042-020-01138-y},
  journal      = {International Journal of Machine Learning and Cybernetics},
  number       = {11},
  pages        = {2529-2540},
  shortjournal = {Int. J. Mach. Learn. Cybern.},
  title        = {Human posture recognition based on multiple features and rule learning},
  volume       = {11},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Robust metric learning based on the rescaled hinge loss.
<em>IJMLC</em>, <em>11</em>(11), 2515–2528. (<a
href="https://doi.org/10.1007/s13042-020-01137-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Distance/Similarity learning is a fundamental problem in machine learning. For example, kNN classifier or clustering methods are based on a distance/similarity measure. Metric learning algorithms enhance the efficiency of these methods by learning an optimal distance function from data. Most metric learning methods need training information in the form of pair or triplet sets. Nowadays, this training information often is obtained from the Internet via crowdsourcing methods. Therefore, this information may contain label noise or outliers leading to the poor performance of the learned metric. It is even possible that the learned metric functions perform worse than the general metrics such as Euclidean distance. To address this challenge, this paper presents a new robust metric learning method based on the Rescaled Hinge loss. This loss function is a general case of the popular Hinge loss and initially introduced in Xu et al. (Pattern Recogn 63:139–148, 2017) to develop a new robust SVM algorithm. In this paper, we formulate the metric learning problem using the Rescaled Hinge loss function and then develop an efficient algorithm based on HQ (Half-Quadratic) to solve the problem. Experimental results on a variety of both real and synthetic datasets confirm that our new robust algorithm considerably outperforms state-of-the-art metric learning methods in the presence of label noise and outliers.},
  archive      = {J_IJMLC},
  author       = {Al-Obaidi, Sumia Abdulhussien Razooqi and Zabihzadeh, Davood and Hajiabadi, Hamideh},
  doi          = {10.1007/s13042-020-01137-z},
  journal      = {International Journal of Machine Learning and Cybernetics},
  number       = {11},
  pages        = {2515-2528},
  shortjournal = {Int. J. Mach. Learn. Cybern.},
  title        = {Robust metric learning based on the rescaled hinge loss},
  volume       = {11},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Flat random forest: A new ensemble learning method towards
better training efficiency and adaptive model size to deep forest.
<em>IJMLC</em>, <em>11</em>(11), 2501–2513. (<a
href="https://doi.org/10.1007/s13042-020-01136-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The known deficiencies of deep neural networks include inferior training efficiency, weak parallelization capability, too many hyper-parameters etc. To address these issues, some researchers presented deep forest, a special deep learning model, which achieves some significant improvements but remain poor training efficiency, inflexible model size and weak interpretability. This paper endeavors to solve the issues in a new way. Firstly, deep forest is extended to the densely connected deep forest to enhance the prediction accuracy. Secondly, to perform parallel training with adaptive model size, the flat random forest is proposed by achieving the balance between the width and depth of densely connected deep forest. Finally, two core algorithms are respectively presented for the forward output weights computation and output weights updating. The experimental results show, compared with deep forest, the proposed flat random forest acquires competitive prediction accuracy, higher training efficiency, less hyper-parameters and adaptive model size.},
  archive      = {J_IJMLC},
  author       = {Liu, Peng and Wang, Xuekui and Yin, Liangfei and Liu, Bing},
  doi          = {10.1007/s13042-020-01136-0},
  journal      = {International Journal of Machine Learning and Cybernetics},
  number       = {11},
  pages        = {2501-2513},
  shortjournal = {Int. J. Mach. Learn. Cybern.},
  title        = {Flat random forest: A new ensemble learning method towards better training efficiency and adaptive model size to deep forest},
  volume       = {11},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Multi-view semi-supervised least squares twin support vector
machines with manifold-preserving graph reduction. <em>IJMLC</em>,
<em>11</em>(11), 2489–2499. (<a
href="https://doi.org/10.1007/s13042-020-01134-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-view semi-supervised support vector machines consider learning with multi-view unlabeled data to boost the learning performance. However, they have several defects. They need to solve the quadratic programming problem and the time complexity is quite high. Moreover, when a large number of multi-view unlabeled examples exist, it can generate more outliers and noisy examples and influence the performance. Therefore, in this paper, we propose two novel multi-view semi-supervised support vector machines called multi-view Laplacian least squares twin support vector machine and its improved version with the manifold-preserving graph reduction which can enhance the robustness of the algorithm. They can reduce the time complexity by changing the constraints to a series of equality constraints and lead to a pair of linear equations. The linear multi-view Laplacian least squares twin support vector machine and its improved version with manifold-preserving graph reduction are further generalized to the nonlinear case via the kernel trick. Experimental results demonstrate that our proposed methods are effective.},
  archive      = {J_IJMLC},
  author       = {Xie, Xijiong},
  doi          = {10.1007/s13042-020-01134-2},
  journal      = {International Journal of Machine Learning and Cybernetics},
  number       = {11},
  pages        = {2489-2499},
  shortjournal = {Int. J. Mach. Learn. Cybern.},
  title        = {Multi-view semi-supervised least squares twin support vector machines with manifold-preserving graph reduction},
  volume       = {11},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Topic discovery by spectral decomposition and clustering
with coordinated global and local contexts. <em>IJMLC</em>,
<em>11</em>(11), 2475–2487. (<a
href="https://doi.org/10.1007/s13042-020-01133-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Topic modeling is an active research field due to its broad applications such as information retrieval, opinion extraction and authorship identification. It aims to discover topic structures from a collection of documents. Significant progress have been made by the latent dirichlet allocation (LDA) and its variants. However, the “bag-of-words” assumption is usually made for the whole document by conventional methods, which ignores the semantics of local context that play crucial roles in topic modeling and document understanding. In this paper, we propose a novel coordinated embedding topic model (CETM), which incorporates spectral decomposition and clustering technique by leveraging both global and local context information to discover topics. In particular, CETM learns coordinated embeddings by using spectral decomposition, capturing the word semantic relations effectively. To infer the topic distribution, we employ a clustering algorithm to capture semantic centroids of coordinated embeddings and derive a fast algorithm to obtain the topic structures. We conduct extensive experiments on three real-world datasets to evaluate the effectiveness of CETM. Quantitatively, compared to state-of-the-art topic modeling approaches, CETM achieves significantly better performance in terms of topic coherence and text classification. Qualitatively, CETM is able to learn more coherent topics and more accurate word distributions for each topic.},
  archive      = {J_IJMLC},
  author       = {Wang, Jian and He, Kejing and Yang, Min},
  doi          = {10.1007/s13042-020-01133-3},
  journal      = {International Journal of Machine Learning and Cybernetics},
  number       = {11},
  pages        = {2475-2487},
  shortjournal = {Int. J. Mach. Learn. Cybern.},
  title        = {Topic discovery by spectral decomposition and clustering with coordinated global and local contexts},
  volume       = {11},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Attentive multi-view reinforcement learning. <em>IJMLC</em>,
<em>11</em>(11), 2461–2474. (<a
href="https://doi.org/10.1007/s13042-020-01130-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The reinforcement learning process usually takes millions of steps from scratch, due to the limited observation experience. More precisely, the representation approximated by a single deep network is usually limited for reinforcement learning agents. In this paper, we propose a novel multi-view deep attention network (MvDAN), which introduces multi-view representation learning into the reinforcement learning framework for the first time. Based on the multi-view scheme of function approximation, the proposed model approximates multiple view-specific policy or value functions in parallel by estimating the middle-level representation and integrates these functions based on attention mechanisms to generate a comprehensive strategy. Furthermore, we develop the multi-view generalized policy improvement to jointly optimize all policies instead of a single one. Compared with the single-view function approximation scheme in reinforcement learning methods, experimental results on eight Atari benchmarks show that MvDAN outperforms the state-of-the-art methods and has faster convergence and training stability.},
  archive      = {J_IJMLC},
  author       = {Hu, Yueyue and Sun, Shiliang and Xu, Xin and Zhao, Jing},
  doi          = {10.1007/s13042-020-01130-6},
  journal      = {International Journal of Machine Learning and Cybernetics},
  number       = {11},
  pages        = {2461-2474},
  shortjournal = {Int. J. Mach. Learn. Cybern.},
  title        = {Attentive multi-view reinforcement learning},
  volume       = {11},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Partial label metric learning by collapsing classes.
<em>IJMLC</em>, <em>11</em>(11), 2453–2460. (<a
href="https://doi.org/10.1007/s13042-020-01129-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Partial label learning (PLL) is a weakly supervised learning framework proposed recently, in which the ground-truth label of training sample is not precisely annotated but concealed in a set of candidate labels, which makes the accuracy of the existing PLL algorithms is usually lower than that of the traditional supervised learning algorithms. Since the accuracy of a learning algorithm is usually closely related to its distance metric, the metric learning technologies can be employed to improve the accuracy of the existing PLL algorithms. However, only a few PLL metric learning algorithms have been proposed up to the present. In view of this, a novel PLL metric learning algorithm is proposed by using the collapsing classes model in this paper. The basic idea is first to take each training sample and its neighbor with shared candidate labels as a similar pair, while each training sample and its neighbor without shared candidate labels as a dissimilar pair, then two probability distributions are defined based on the distance and label similarity of these pairs, respectively, finally the metric matrix is obtained via minimizing the Kullback–Leibler divergence of these two probability distributions. Experimental results on six UCI data sets and four real-world PLL data sets show that the proposed algorithm can obviously improve the accuracy of the existing PLL algorithms.},
  archive      = {J_IJMLC},
  author       = {Xu, Shuang and Yang, Min and Zhou, Yu and Zheng, Ruirui and Liu, Wenpeng and He, Jianjun},
  doi          = {10.1007/s13042-020-01129-z},
  journal      = {International Journal of Machine Learning and Cybernetics},
  number       = {11},
  pages        = {2453-2460},
  shortjournal = {Int. J. Mach. Learn. Cybern.},
  title        = {Partial label metric learning by collapsing classes},
  volume       = {11},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A new BAT optimization algorithm based feature selection
method for electrocardiogram heartbeat classification using empirical
wavelet transform and fisher ratio. <em>IJMLC</em>, <em>11</em>(11),
2439–2452. (<a
href="https://doi.org/10.1007/s13042-020-01128-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, a novel feature selection method is proposed for the categorization of electrocardiogram (ECG) heartbeats. The proposed technique uses the Fisher ratio and BAT optimization algorithm to obtain the best feature set for ECG classification. The MIT-BIH arrhythmia database contains sixteen classes of the ECG heartbeats. The MIT-BIH ECG arrhythmia database divided into intra-patient and inter-patient schemes to be used in this study. The proposed feature selection methodology works in following steps: firstly, features are extracted using empirical wavelet transform (EWT) and then higher-order statistics, as well as symbolic features, are computed for each decomposed mode of EWT. Thereafter, the complete feature vector is obtained by the conjunction of EWT based features and RR interval features. Secondly, for feature selection, the Fisher ratio is utilized. It is optimized by using BAT algorithm so as to have maximal discrimination of the between classes. Finally, in the classification step, the k-nearest neighbor classifier is used to classify the heartbeats. The performance measures i.e., accuracy, sensitivity, positive predictivity, specificity for intra-patient scheme are 99.80\%, 99.80\%, 99.80\%, 99.987\% and for inter-patient scheme are 97.59\%, 97.589\%, 97.589\%, 99.196\% respectively. The proposed feature selection technique outperforms the other state of art feature selection methods.},
  archive      = {J_IJMLC},
  author       = {Verma, Atul Kumar and Saini, Indu and Saini, Barjinder Singh},
  doi          = {10.1007/s13042-020-01128-0},
  journal      = {International Journal of Machine Learning and Cybernetics},
  number       = {11},
  pages        = {2439-2452},
  shortjournal = {Int. J. Mach. Learn. Cybern.},
  title        = {A new BAT optimization algorithm based feature selection method for electrocardiogram heartbeat classification using empirical wavelet transform and fisher ratio},
  volume       = {11},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Generalized two-dimensional PCA based on <span
class="math display"><em>ℓ</em><sub>2, <em>p</em></sub></span> -norm
minimization. <em>IJMLC</em>, <em>11</em>(11), 2421–2438. (<a
href="https://doi.org/10.1007/s13042-020-01127-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To exploit the information from two-dimensional structured data, two-dimensional principal component analysis (2-DPCA) has been widely used for dimensionality reduction and feature extraction. However, 2-DPCA is sensitive to outliers which are common in real applications. Therefore, many robust 2-DPCA methods have been proposed to improve the robustness of 2-DPCA. But existing robust 2-DPCAs have several weaknesses. First, these methods cannot be robust enough to outliers. Second, to center a sample set mixed with outliers using the L2-norm distance is usually biased. Third, most methods do not preserve the nice property of 2-DPCA (rotational invariance), which is important for learning algorithm. To alleviate these issues, we present a generalized robust 2-DPCA, which is named as 2-DPCA with $$\ell _{2,p}$$ -norm minimization ( $$\ell _{2,p}$$ -2-DPCA), for image representation and recognition. In $$\ell _{2,p}$$ -2-DPCA, $$\ell _{2,p}$$ -norm is employed as the distance metric to measure the reconstruction error, which can alleviate the effect of outliers. Therefore, the proposed method is robust to outliers and preserves the desirable property of 2-DPCA which is invariant to rotational and well characterizes the geometric structure of samples. Moreover, most existing robust PCA methods estimate sample mean from database with outliers by averaging, which is usually biased. Sample mean are treated as an unknown variable to remedy the bias of computing sample mean in $$\ell _{2,p}$$ -2-DPCA. To solve $$\ell _{2,p}$$ -2-DPCA, we propose an iterative algorithm, which has a closed-form solution in each iteration. Experimental results on several benchmark databases demonstrate the effectiveness and advantages of our method.},
  archive      = {J_IJMLC},
  author       = {Mi, Jian-Xun and Zhang, Ya-Nan and Li, Yong and Shu, Yucheng},
  doi          = {10.1007/s13042-020-01127-1},
  journal      = {International Journal of Machine Learning and Cybernetics},
  number       = {11},
  pages        = {2421-2438},
  shortjournal = {Int. J. Mach. Learn. Cybern.},
  title        = {Generalized two-dimensional PCA based on $$\ell _{2,p}$$ -norm minimization},
  volume       = {11},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Evaluating skills in hierarchical reinforcement learning.
<em>IJMLC</em>, <em>11</em>(10), 2407–2420. (<a
href="https://doi.org/10.1007/s13042-020-01141-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Despite the benefits mentioned in previous works of automatically acquiring skills for using them in hierarchical reinforcement learning algorithms such as solving the curse of dimensionality, improving exploration, and speeding up value propagation, they have not paid much attention to evaluating the effect of each skill on these factors. In this paper, we show that depending on the given task, a skill may be useful for learning it or not. In addition, the focus of the related work of automatically acquiring skills is on detecting subgoals, i.e., the skill termination condition, but there is not a precise method for extracting the initiation set of skills. In this paper, we propose not only two methods for evaluating skills but also two other methods for pruning the initiation set of them. Experimental results show significant improvements in learning different test domains after evaluating and pruning skills.},
  archive      = {J_IJMLC},
  author       = {Davoodabadi Farahani, Marzieh and Mozayani, Nasser},
  doi          = {10.1007/s13042-020-01141-3},
  journal      = {International Journal of Machine Learning and Cybernetics},
  number       = {10},
  pages        = {2407-2420},
  shortjournal = {Int. J. Mach. Learn. Cybern.},
  title        = {Evaluating skills in hierarchical reinforcement learning},
  volume       = {11},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A novel-designed fuzzy logic control structure for control
of distinct chaotic systems. <em>IJMLC</em>, <em>11</em>(10), 2391–2406.
(<a href="https://doi.org/10.1007/s13042-020-01125-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, a Lyapunov-based fuzzy logic control (FLC) system is developed for controlling of complicated as well as distinct nonlinear systems. According to Lyapunov Stability Theory, a candidate Lyapunov function with simple quadratic form is designed. Via constructing the fuzzy IF–THEN rules through referencing the statuses of errors states and error derivatives in each sub-space of the Lyapunov function derivatives, the proposed structure of FLC system is able to appropriately as well as flexibly adjust the control forces with minimum magnitude compensating the nonlinear systems in real-time. In addition, the designed FLC system can be applied to different kinds of control systems without further information and operation experience. Two nonlinear systems with distinct structures, classical Lorentz system and Mathieu-van der Pol system, are illustrated for simulation examples. In comparison of the previous research work, the simulation results reveal the effectiveness, flexibility and convenient-design of the proposed method.},
  archive      = {J_IJMLC},
  author       = {Li, Shih-Yu and Tam, Lap-Mou and Chen, Hsien-Keng and Chen, Chin-Sheng},
  doi          = {10.1007/s13042-020-01125-3},
  journal      = {International Journal of Machine Learning and Cybernetics},
  number       = {10},
  pages        = {2391-2406},
  shortjournal = {Int. J. Mach. Learn. Cybern.},
  title        = {A novel-designed fuzzy logic control structure for control of distinct chaotic systems},
  volume       = {11},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Twin support vector machine based on adjustable large margin
distribution for pattern classification. <em>IJMLC</em>,
<em>11</em>(10), 2371–2389. (<a
href="https://doi.org/10.1007/s13042-020-01124-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper researches the value of the margin distribution in binary classifier. The central idea of large margin distribution machine (LDM) is to optimize the margin distribution, such as maximizing the margin mean and minimizing the margin variance. Compared to support vector machine (SVM), LDM demonstrates the good generalization performance. In order to improve the generalization performance of twin support vector machine (TSVM), a twin support vector machine based on adjustable large margin distribution (ALD-TSVM) is proposed in this paper. Firstly, the margin distribution is redefined to construct a pair of adjustable supporting hyperplanes. Then, the redefined margin distribution is introduced onto TSVM to obtain the models of ALD-TSVM, including linear case and nonlinear case. ALD-TSVM is a general learning method which can be used in any place where TSVM and LDM can be applied. Finally, the novel method is compared with other classification algorithms by doing experiments on toy dataset, UCI datasets and image datasets. The experimental results show that ALD-TSVM obtains better classification performance.},
  archive      = {J_IJMLC},
  author       = {Liu, Liming and Chu, Maoxiang and Yang, Yonghui and Gong, Rongfen},
  doi          = {10.1007/s13042-020-01124-4},
  journal      = {International Journal of Machine Learning and Cybernetics},
  number       = {10},
  pages        = {2371-2389},
  shortjournal = {Int. J. Mach. Learn. Cybern.},
  title        = {Twin support vector machine based on adjustable large margin distribution for pattern classification},
  volume       = {11},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Multiplication fusion of sparse and
collaborative-competitive representation for image classification.
<em>IJMLC</em>, <em>11</em>(10), 2357–2369. (<a
href="https://doi.org/10.1007/s13042-020-01123-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Representation based classification methods have become a hot research topic during the past few years, and the two most prominent approaches are sparse representation based classification (SRC) and collaborative representation based classification (CRC). CRC reveals that it is the collaborative representation rather than the sparsity that makes SRC successful. Nevertheless, the dense representation of CRC may not be discriminative which will degrade its performance for classification tasks. To alleviate this problem to some extent, we propose a new method called sparse and collaborative-competitive representation based classification (SCCRC) for image classification. Firstly, the coefficients of the test sample are obtained by SRC and CCRC, respectively. Then the fused coefficient is derived by multiplying the coefficients of SRC and CCRC. Finally, the test sample is designated to the class that has the minimum residual. Experimental results on several benchmark databases demonstrate the efficacy of our proposed SCCRC. The source code of SCCRC is accessible at https://github.com/li-zi-qi/SCCRC .},
  archive      = {J_IJMLC},
  author       = {Li, Zi-Qi and Sun, Jun and Wu, Xiao-Jun and Yin, He-Feng},
  doi          = {10.1007/s13042-020-01123-5},
  journal      = {International Journal of Machine Learning and Cybernetics},
  number       = {10},
  pages        = {2357-2369},
  shortjournal = {Int. J. Mach. Learn. Cybern.},
  title        = {Multiplication fusion of sparse and collaborative-competitive representation for image classification},
  volume       = {11},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Automatic extraction of named entities of cyber threats
using a deep bi-LSTM-CRF network. <em>IJMLC</em>, <em>11</em>(10),
2341–2355. (<a
href="https://doi.org/10.1007/s13042-020-01122-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Countless cyber threat intelligence (CTI) reports are used by companies around the world on a daily basis for security reasons. To secure critical cybersecurity information, analysts and individuals should accordingly analyze information on threats and vulnerabilities. However, analyzing such overwhelming volumes of reports requires considerable time and effort. In this study, we propose a novel approach that automatically extracts core information from CTI reports using a named entity recognition (NER) system. During the process of constructing our proposed NER system, we defined meaningful keywords in the security domain as entities, including malware, domain/URL, IP address, Hash, and Common Vulnerabilities and Exposures. Furthermore, we linked these keywords with the words extracted from the text data of the report. To achieve a higher performance, we utilized the character-level feature vector as an input to bidirectional long-short-term memory using a conditional random field network. We finally achieved an average F1-score of 75.05\%. We release 498,000 tag datasets created during our research.},
  archive      = {J_IJMLC},
  author       = {Kim, Gyeongmin and Lee, Chanhee and Jo, Jaechoon and Lim, Heuiseok},
  doi          = {10.1007/s13042-020-01122-6},
  journal      = {International Journal of Machine Learning and Cybernetics},
  number       = {10},
  pages        = {2341-2355},
  shortjournal = {Int. J. Mach. Learn. Cybern.},
  title        = {Automatic extraction of named entities of cyber threats using a deep bi-LSTM-CRF network},
  volume       = {11},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Accelerated inexact matrix completion algorithm via
closed-form q-thresholding <span
class="math display">(<em>q</em> = 1/2, 2/3)</span> operator.
<em>IJMLC</em>, <em>11</em>(10), 2327–2339. (<a
href="https://doi.org/10.1007/s13042-020-01121-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {$$l_{q}$$ ( $$0&lt; q &lt; 1$$ ) regularization is a dominating strategy for matrix completion problems. The main goal of nonconvex $$l_{q}$$ regularization based algorithm is to find a so-called low-rank solution.Unfortunately, most existing algorithms suffer from full singular value decomposition (SVD), and thus become inefficient for large-scale matrix completion problems. To alleviate this limitation, in this paper we propose an accelerated inexact algorithm to handle such problem. The key idea is to employ the closed-form q-thresholding ( $$q = 1/2, 2/3$$ ) operator to approximate the rank of a matrix. The power method and the special “sparse plus low-rank” structure of the matrix iterates are adopted to allow efficient SVD. Besides, we employ Nesterov’s accelerated gradient method and continuation technique to further accelerate the convergence speed of our proposed algorithm. A convergence analysis shows that the sequence $${X_{t}}$$ generated by our proposed algorithm is bounded and has at least one accumulation point. Extensive experiments have been conducted to study its recovery performance on synthetic data, image recovery and recommendation problems. All results demonstrate that our proposed algorithm is able to achieve comparable recovery performance, while being faster and more efficient than state-of-the-art methods.},
  archive      = {J_IJMLC},
  author       = {Wang, Zhi and Gao, Chao and Luo, Xiaohu and Tang, Ming and Wang, Jianjun and Chen, Wu},
  doi          = {10.1007/s13042-020-01121-7},
  journal      = {International Journal of Machine Learning and Cybernetics},
  number       = {10},
  pages        = {2327-2339},
  shortjournal = {Int. J. Mach. Learn. Cybern.},
  title        = {Accelerated inexact matrix completion algorithm via closed-form q-thresholding $$(q = 1/2, 2/3)$$ operator},
  volume       = {11},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Training error and sensitivity-based ensemble feature
selection. <em>IJMLC</em>, <em>11</em>(10), 2313–2326. (<a
href="https://doi.org/10.1007/s13042-020-01120-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ensemble feature selection combines feature selection and ensemble learning to improve the generalization capability of ensemble systems. However, current methods minimizing only the training error may not generalize well on future unseen samples. In this paper, we propose a training error and sensitivity-based ensemble feature selection method. The NSGA-III is applied to find optimal feature subsets by minimizing two objective functions of the whole ensemble system simultaneously: the training error and the sensitivity of the ensemble. With this scheme, the ensemble system maintains both high accuracy and high stability which is expected to achieve a high generalization capability. Experimental results on 18 datasets show that the proposed method significantly outperforms state-of-the-art methods.},
  archive      = {J_IJMLC},
  author       = {Ng, Wing W. Y. and Tuo, Yuxi and Zhang, Jianjun and Kwong, Sam},
  doi          = {10.1007/s13042-020-01120-8},
  journal      = {International Journal of Machine Learning and Cybernetics},
  number       = {10},
  pages        = {2313-2326},
  shortjournal = {Int. J. Mach. Learn. Cybern.},
  title        = {Training error and sensitivity-based ensemble feature selection},
  volume       = {11},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Bipolar fuzzy petri nets for knowledge representation and
acquisition considering non-cooperative behaviors. <em>IJMLC</em>,
<em>11</em>(10), 2297–2311. (<a
href="https://doi.org/10.1007/s13042-020-01118-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fuzzy Petri nets (FPNs) are a promising modeling tool for knowledge representation and reasoning. As a new type of FPNs, bipolar fuzzy Petri nets (BFPNs) are developed in this article to overcome the shortcomings and improve the performance of traditional FPNs. In order to depict expert knowledge more accurately, the BFPN model adopts bipolar fuzzy sets (BFSs), which are characterized by the satisfaction degree to property and the satisfaction degree to its counter property, to represent knowledge parameters. Because of the increasing scale of expert systems, a concurrent hierarchical reasoning algorithm is introduced to simplify the structure of BFPNs and reduce the computation complexity of knowledge reasoning algorithm. In addition, a large group expert weighting method is proposed for knowledge acquisition by taking experts’ non-cooperative behaviors into account. A realistic case of risk index evaluation system is presented to show the effectiveness and practicality of the proposed BFPNs. The result shows that the new BFPN model is feasible and efficient for knowledge representation and acquisition.},
  archive      = {J_IJMLC},
  author       = {Xu, Xue-Guo and Xiong, Yun and Xu, Dong-Hui and Liu, Hu-Chen},
  doi          = {10.1007/s13042-020-01118-2},
  journal      = {International Journal of Machine Learning and Cybernetics},
  number       = {10},
  pages        = {2297-2311},
  shortjournal = {Int. J. Mach. Learn. Cybern.},
  title        = {Bipolar fuzzy petri nets for knowledge representation and acquisition considering non-cooperative behaviors},
  volume       = {11},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Exploring of alternative representations of facial images
for face recognition. <em>IJMLC</em>, <em>11</em>(10), 2289–2295. (<a
href="https://doi.org/10.1007/s13042-020-01116-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Description and classification of face images is a significant task of computer vision, machine learning and pattern recognition communities. In the past, researchers have made tremendous efforts in this task. Previous researchers always seek high-resolution face images for better image classification. However, with this paper, we present and demonstrate a new opinion that in some cases the use of alternative representations of facial images are very useful for face recognition and properly reducing the image resolution might be beneficial to better classification of face images. This may be attributed to the deformable property of faces and the fact that the proposed alternative representations can in some extent reduce the within-class difference of facial images. Also, the presented idea appear to be useful for helping people to improve face recognition techniques in real worlds.},
  archive      = {J_IJMLC},
  author       = {Qin, Yongbin and Sun, Lilei and Xu, Yong},
  doi          = {10.1007/s13042-020-01116-4},
  journal      = {International Journal of Machine Learning and Cybernetics},
  number       = {10},
  pages        = {2289-2295},
  shortjournal = {Int. J. Mach. Learn. Cybern.},
  title        = {Exploring of alternative representations of facial images for face recognition},
  volume       = {11},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Gradient boosting in crowd ensembles for q-learning using
weight sharing. <em>IJMLC</em>, <em>11</em>(10), 2275–2287. (<a
href="https://doi.org/10.1007/s13042-020-01115-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Reinforcement learning (RL) is a double-edged sword: it frees the human trainer from having to provide voluminous supervised training data or from even knowing a solution. On the other hand, a common complaint about RL is that learning is slow. Deep Q-learning (DQN), a somewhat recent development, has allowed practitioners and scientists to solve tasks previously thought unsolvable by a reinforcement learning approach. However DQN has resulted in an explosion in the number of model parameters which has further exasperated the computational needs of Q-learning during training. In this work, an ensemble approach which improves the training time, in terms of the number of interactions with the training environment, is proposed. In the presented experiments, it is shown that the proposed approach improves stability of during training, results in improved average performance, results in more reliable training, and faster learning of features in convolutional layers.},
  archive      = {J_IJMLC},
  author       = {Elliott, D. L. and Santosh, K. C. and Anderson, Charles},
  doi          = {10.1007/s13042-020-01115-5},
  journal      = {International Journal of Machine Learning and Cybernetics},
  number       = {10},
  pages        = {2275-2287},
  shortjournal = {Int. J. Mach. Learn. Cybern.},
  title        = {Gradient boosting in crowd ensembles for Q-learning using weight sharing},
  volume       = {11},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). ELM-MC: Multi-label classification framework based on
extreme learning machine. <em>IJMLC</em>, <em>11</em>(10), 2261–2274.
(<a href="https://doi.org/10.1007/s13042-020-01114-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-label classification methods aim to a class of application problems where each individual contains a single instance while associates with a set of labels simultaneously. In this paper, we formulate a novel multi-label classification method based on extreme learning machine framework, named ELM-MC algorithm. The essence of ELM-MC algorithm is to convert the multi-label classification problem into some single-label classifications, and fully considers the relationship among different labels. After the classification of one label, the associations with next label are applied to update the learning parameters in ELM-MC algorithm. In addition, we design a backup pool for the hidden nodes. It can help to select relatively suitable hidden nodes to the corresponding label classification case. In the simulation part, six famous databases are applied to demonstrate the satisfied classification accuracy of the proposed method.},
  archive      = {J_IJMLC},
  author       = {Zhang, Haigang and Yang, Jinfeng and Jia, Guimin and Han, Shaocheng and Zhou, Xinran},
  doi          = {10.1007/s13042-020-01114-6},
  journal      = {International Journal of Machine Learning and Cybernetics},
  number       = {10},
  pages        = {2261-2274},
  shortjournal = {Int. J. Mach. Learn. Cybern.},
  title        = {ELM-MC: Multi-label classification framework based on extreme learning machine},
  volume       = {11},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Discriminative low-rank projection for robust subspace
learning. <em>IJMLC</em>, <em>11</em>(10), 2247–2260. (<a
href="https://doi.org/10.1007/s13042-020-01113-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The robustness to outliers, noises, and corruptions has been paid more attention recently to increase the performance in linear feature extraction and image classification. As one of the most effective subspace learning methods, low-rank representation (LRR) can improve the robustness of an algorithm by exploring the global representative structure information among the samples. However, the traditional LRR cannot project the training samples into low-dimensional subspace with supervised information. Thus, in this paper, we integrate the properties of LRR with supervised dimensionality reduction techniques to obtain optimal low-rank subspace and discriminative projection at the same time. To achieve this goal, we proposed a novel model named Discriminative Low-Rank Projection (DLRP). Furthermore, DLRP can break the limitation of the small class problem which means the number of projections is bound by the number of classes. Our model can be solved by alternatively linearized alternating direction method with adaptive penalty and the singular value decomposition. Besides, the analyses of differences between DLRP and previous related models are shown. Extensive experiments conducted on various contaminated databases have confirmed the superiority of the proposed method.},
  archive      = {J_IJMLC},
  author       = {Lai, Zhihui and Bao, Jiaqi and Kong, Heng and Wan, Minghua and Yang, Guowei},
  doi          = {10.1007/s13042-020-01113-7},
  journal      = {International Journal of Machine Learning and Cybernetics},
  number       = {10},
  pages        = {2247-2260},
  shortjournal = {Int. J. Mach. Learn. Cybern.},
  title        = {Discriminative low-rank projection for robust subspace learning},
  volume       = {11},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Gain ratio weighted inverted specific-class distance measure
for nominal attributes. <em>IJMLC</em>, <em>11</em>(10), 2237–2246. (<a
href="https://doi.org/10.1007/s13042-020-01112-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Enhancing distance measures is key to improving the performances of many machine learning algorithms, such as instance-based learning algorithms. Although the inverted specific-class distance measure (ISCDM) is among the top performing distance measures addressing nominal attributes with the presence of missing values and non-class attribute noise in the training set, this still requires the attribute independence assumption. It is obvious that the attribute independence assumption required by the ISCDM is rarely true in reality, which harms its performance in applications with complex attribute dependencies. Thus, in this study we propose an improved ISCDM by utilizing attribute weighting to circumvent the attribute independence assumption. In our improved ISCDM, we simply define the weight of each attribute as its gain ratio. Thus, we denote our improved ISCDM as the gain ratio weighted ISCDM (GRWISCDM for short). We tested the GRWISCDM experimentally on 29 University of California at Irvine datasets, and found that it significantly outperforms the original ISCDM and some other state-of-the-art competitors in terms of the negative conditional log likelihood and root relative squared error.},
  archive      = {J_IJMLC},
  author       = {Gong, Fang and Jiang, Liangxiao and Zhang, Huan and Wang, Dianhong and Guo, Xingfeng},
  doi          = {10.1007/s13042-020-01112-8},
  journal      = {International Journal of Machine Learning and Cybernetics},
  number       = {10},
  pages        = {2237-2246},
  shortjournal = {Int. J. Mach. Learn. Cybern.},
  title        = {Gain ratio weighted inverted specific-class distance measure for nominal attributes},
  volume       = {11},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). PRF-RW: A progressive random forest-based random walk
approach for interactive semi-automated pulmonary lobes segmentation.
<em>IJMLC</em>, <em>11</em>(10), 2221–2235. (<a
href="https://doi.org/10.1007/s13042-020-01111-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The computational detection of lung lobes from computed tomography images is a challenging segmentation problem with important respiratory healthcare applications, including emphysema, chronic bronchitis, and asthma. This paper proposes a progressive random forest-based random walk approach for interactive semi-automated pulmonary lobes segmentation. First, our model performs automated segmentation of the lung lobes in a progressive random forest network, eliminating the need for prior segmentation of lungs, vessels, or airways. Then, an interactive lobes segmentation approach based on random walk mechanism is designed for improving auto-segmentation accuracy. Furthermore, we annotate a new dataset which contains 93 scans (57 men, 36 women; age range: 40–90 years) from the Central Hospital Affiliated with Shenyang Medical College (CHASMC). We evaluate the model on our annotated dataset, LIDC ( https://wiki.cancerimagingarchive.net ) and LOLA11 ( http://lolall.com/ ) datasets. The proposed model achieved a Dice score of $$0.906 \pm 0.106$$ for LIDC, $$0.898 \pm 0.113$$ for LOLA11, and $$0.921 \pm 0.101$$ for our dataset. Experimental results show the accuracy of the proposed approach, which consistently improves performance across different datasets by a maximum of 8.2\% as compared to baselines model.},
  archive      = {J_IJMLC},
  author       = {Li, Qiang and Chen, Lei and Li, Xiangju and Lv, Xiaofeng and Xia, Shuyue and Kang, Yan},
  doi          = {10.1007/s13042-020-01111-9},
  journal      = {International Journal of Machine Learning and Cybernetics},
  number       = {10},
  pages        = {2221-2235},
  shortjournal = {Int. J. Mach. Learn. Cybern.},
  title        = {PRF-RW: A progressive random forest-based random walk approach for interactive semi-automated pulmonary lobes segmentation},
  volume       = {11},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). An adaptive kernel sparse representation-based
classification. <em>IJMLC</em>, <em>11</em>(10), 2209–2219. (<a
href="https://doi.org/10.1007/s13042-020-01110-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, scholars have attached increasing attention to sparse representation. Based on compressed sensing and machine learning, sparse representation-based classification (SRC) has been extensively in classification. However, SRC is not suitable for samples with non-linear structures which arise in many practical applications. Meanwhile, sparsity is overemphasized by SRC, but the correlation information which is of great importance in classification is overlooked. To address these shortcomings, this study puts forward an adaptive kernel sparse representation-based classification (AKSRC). First, the samples were mapped to a high-dimensional feature space from the original feature space. Second, after selecting a suitable kernel function, a sample is represented as the linear combination of training samples of same class. Further more, the trace norm is adopted in AKSRC which is different from general approaches. It’s adaptive to the structure of dictionary which means that a better linear representation which has the most discriminative samples can be obtained. Therefore, AKSRC has more powerful classification ability. Finally, the advancement and effectiveness of the proposed AKSRC are verified by carrying out experiments on benchmark data sets.},
  archive      = {J_IJMLC},
  author       = {Wang, Xuejun and Wang, Wenjian and Men, Changqian},
  doi          = {10.1007/s13042-020-01110-w},
  journal      = {International Journal of Machine Learning and Cybernetics},
  number       = {10},
  pages        = {2209-2219},
  shortjournal = {Int. J. Mach. Learn. Cybern.},
  title        = {An adaptive kernel sparse representation-based classification},
  volume       = {11},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). An MADM approach to covering-based variable precision fuzzy
rough sets: An application to medical diagnosis. <em>IJMLC</em>,
<em>11</em>(9), 2181–2207. (<a
href="https://doi.org/10.1007/s13042-020-01109-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In medical diagnosis, how to select an optimal medicine from some medicines with similar efficacy values to treat diseases has become common problems between doctors and patients. To solve this problem, we describe it as a multi-attribute decision-making (MADM) in a finite fuzzy covering approximation space. This paper aims to propose two pairs of covering-based variable precision fuzzy rough sets. By combining the proposed rough set model with the VIKOR method, we construct a novel method to medicine selection MADM problems in the context of medical diagnosis. A real-life case study of selecting a proper medicine to treat Alzheimer’s disease is given to demonstrate the practicality of our proposed method. Through a comparative analysis and an experimental analysis, we further explore the effectiveness and stability of the established method.},
  archive      = {J_IJMLC},
  author       = {Jiang, Haibo and Zhan, Jianming and Sun, Bingzhen and Alcantud, José Carlos R.},
  doi          = {10.1007/s13042-020-01109-3},
  journal      = {International Journal of Machine Learning and Cybernetics},
  number       = {9},
  pages        = {2181-2207},
  shortjournal = {Int. J. Mach. Learn. Cybern.},
  title        = {An MADM approach to covering-based variable precision fuzzy rough sets: An application to medical diagnosis},
  volume       = {11},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Unsupervised and supervised methods for the detection of
hurriedly created profiles in recommender systems. <em>IJMLC</em>,
<em>11</em>(9), 2165–2179. (<a
href="https://doi.org/10.1007/s13042-020-01108-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recommender systems try to provide users with accurate personalized suggestions for items based on an analysis of previous user decisions and the decisions made by other users. These systems suffer from profile injection attacks, where malicious profiles are generated in order to promote or demote a particular item introducing abnormal ratings. The problem of automatic detection of such malicious profiles has been recently addressed by a great number of authors in the literature using supervised and unsupervised approaches. In this paper, we propose a framework to identify anomalous rating profiles, where each attacker (outlier) hurriedly creates profiles that inject into the system an unspecified combination of random ratings and specific ratings, without any prior knowledge of the existing ratings. This attack is a superset of the two different attacks (Uniform and Delta) proposed in Harper et al. (ACM Trans Interact Intell Syst 5(4):19, 2016) making the attack model more realistic and its detection more challenging. The proposed detection method is based on several attributes related to the unpredictable behavior of the outliers in a validation set, on the user-item rating matrix, on the similarity between users and on the filler items. In this work, we propose a new attribute (RIS) to capture the randomness in item selection of the abnormal profiles. In this work, three different systems are proposed: (1) a probabilistic framework that estimates the probability of a user to be an outlier by combining several features in a completely unsupervised way. (2) An unsupervised clustering system based on the k-means algorithm that automatically spots the spurious profiles. (3) A supervised framework that uses a random forest classifier for cases where labeling sample data is available. Experimental results on the MovieLens and the Small Netflix datasets demonstrate the high performance of the proposed methods as well as the discrimination accuracy of the proposed features.},
  archive      = {J_IJMLC},
  author       = {Panagiotakis, Costas and Papadakis, Harris and Fragopoulou, Paraskevi},
  doi          = {10.1007/s13042-020-01108-4},
  journal      = {International Journal of Machine Learning and Cybernetics},
  number       = {9},
  pages        = {2165-2179},
  shortjournal = {Int. J. Mach. Learn. Cybern.},
  title        = {Unsupervised and supervised methods for the detection of hurriedly created profiles in recommender systems},
  volume       = {11},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Supervised information granulation strategy for attribute
reduction. <em>IJMLC</em>, <em>11</em>(9), 2149–2163. (<a
href="https://doi.org/10.1007/s13042-020-01107-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In rough set based Granular Computing, neighborhood relation has been widely accepted as one of the most popular approaches for realizing information granulation. Such approach is to group samples in terms of their similarities without the consideration of their labels. Therefore, it can be referred to as an unsupervised information granulation strategy. Nevertheless, it is obvious that such unsupervised mechanism may generate imprecise neighborhoods by comparing the actual labels of samples. It follows that it is not good enough for classification-oriented attribute reduction to select qualified attributes. To fill such a gap, a novel supervised information granulation strategy is proposed. Different from the unsupervised information granulation, samples are grouped by using not only the similarities over conditional attributes but also the labels. For such a purpose, our mechanism mainly contains two aspects: (1) intra-class radius, which aims to add samples with the same label into neighborhood; (2) extra-class radius, which aims to delete samples with different labels from the neighborhood. The experimental results over 12 UCI data sets demonstrate that, compared with previous researches, the reducts derived by our supervised information granulation may contribute to superior classification performances. This study suggests new trends and applications of considering information granulation from the viewpoint of supervised learning.},
  archive      = {J_IJMLC},
  author       = {Liu, Keyu and Yang, Xibei and Yu, Hualong and Fujita, Hamido and Chen, Xiangjian and Liu, Dun},
  doi          = {10.1007/s13042-020-01107-5},
  journal      = {International Journal of Machine Learning and Cybernetics},
  number       = {9},
  pages        = {2149-2163},
  shortjournal = {Int. J. Mach. Learn. Cybern.},
  title        = {Supervised information granulation strategy for attribute reduction},
  volume       = {11},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Selection of relevant texture descriptors for recognition of
HEp-2 cell staining patterns. <em>IJMLC</em>, <em>11</em>(9), 2127–2147.
(<a href="https://doi.org/10.1007/s13042-020-01106-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {One of the important applications of computer-aided diagnosis is the detection of connective tissue disorders through automatic classification of antinuclear autoantibodies (ANAs). The recognition of ANAs is primarily done by analyzing indirect immunofluorescence (IIF) images of human epithelial type 2 (HEp-2) cells. In this regard, the paper introduces a novel approach for automatic classification of ANAs by staining pattern recognition of HEp-2 cell IIF images. Considering a set of HEp-2 cell images, the proposed method selects a set of relevant local texture descriptors for a pair of staining pattern classes, as well as identifies a set of important features corresponding to each relevant descriptor. The set of features for multiple classes is obtained from each of the important feature sets selected under various relevant local texture descriptors for all possible class-pairs. The relevance of a descriptor is evaluated based on the theory of rough hypercuboid approach, while the important feature set of a local descriptor is formed by reducing the impact of both noisy pixels present in an HEp-2 cell image and noisy HEp-2 cell images in a staining pattern class. Finally, the support vector machine is used to recognize one of the known staining patterns present in IIF images. The effectiveness of the proposed staining pattern recognition method, along with a comparison with related approaches, is illustrated on two benchmark databases of HEp-2 cell images using different classifiers and experimental set-up. The results show that the proposed approach performs significantly better than existing methods, with respect to both classification accuracy and F1 score, irrespective of the databases and classifiers used.},
  archive      = {J_IJMLC},
  author       = {Kumar, Debamita and Maji, Pradipta},
  doi          = {10.1007/s13042-020-01106-6},
  journal      = {International Journal of Machine Learning and Cybernetics},
  number       = {9},
  pages        = {2127-2147},
  shortjournal = {Int. J. Mach. Learn. Cybern.},
  title        = {Selection of relevant texture descriptors for recognition of HEp-2 cell staining patterns},
  volume       = {11},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Multiple-instance learning via multiple-point concept based
instance selection. <em>IJMLC</em>, <em>11</em>(9), 2113–2126. (<a
href="https://doi.org/10.1007/s13042-020-01105-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multiple-instance learning (MIL) is a kind of weakly supervised learning where a single label is assigned to a bag of instances. To solve MIL problems, researchers have presented an effective embedding based framework that projects bags into a new feature space, which is constructed from some selected instances that can represent target concepts to some extent. Most previous studies use single-point concepts for the instance selection, where every possible concept is represented by only a single point (i.e., instance). However, multiple points may be more powerful for the same concept than a single. In this paper, we propose the notion of multiple-point concept, jointly represented by a group of similar points, and then build an iterative instance-selection method for MIL upon Multiple-Point Concepts. The proposed algorithm is thus named MILMPC, and its main difference from other MIL algorithms is selecting instances via multiple-point concept rather than single-point concept. The experimental results on five data sets have validated the convergence of the iterative instance-selection method, and the generality of the resulting MIL model in that it performs consistently well under three different kinds of relevance evaluation criteria (used to measure the relevance of a candidate concept to the target). Furthermore, compared to other MIL algorithms, the proposed model has been demonstrated not only suitable for common MIL problems, but more suitable for hybrid problems.},
  archive      = {J_IJMLC},
  author       = {Yuan, Liming and Xu, Guangping and Zhao, Lu and Wen, Xianbin and Xu, Haixia},
  doi          = {10.1007/s13042-020-01105-7},
  journal      = {International Journal of Machine Learning and Cybernetics},
  number       = {9},
  pages        = {2113-2126},
  shortjournal = {Int. J. Mach. Learn. Cybern.},
  title        = {Multiple-instance learning via multiple-point concept based instance selection},
  volume       = {11},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A novel learning-based approach for efficient dismantling of
networks. <em>IJMLC</em>, <em>11</em>(9), 2101–2111. (<a
href="https://doi.org/10.1007/s13042-020-01104-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dismantling of complex networks is a problem to find a minimal set of nodes in which the removal leaves the network broken into connected components of sub-extensive size. It has a wide spectrum of important applications, including network immunization and network destruction. Due to its NP-hard computational complexity, this problem cannot be solved exactly with polynomial time. Traditional solutions, including manually-designed and considerably sub-optimal heuristic algorithms, and accurate message-passing ones, all suffer from low efficiency in large-scale problems. In this paper, we introduce a simple learning-based approach, CoreGQN, which seeks to train an agent that is able to smartly choose nodes that would accumulate the maximum rewards. CoreGQN is trained by hundreds of thousands self-plays on small synthetic graphs, and can then be able to generalize well on real-world networks across different types with different scales. Extensive experiments demonstrate that CoreGQN performs on par with the state-of-art algorithms at greatly reduced computational costs, suggesting that CoreGQN should be the better choice for practical network dismantling purposes.},
  archive      = {J_IJMLC},
  author       = {Fan, Changjun and Zeng, Li and Feng, Yanghe and Cheng, Guangquan and Huang, Jincai and Liu, Zhong},
  doi          = {10.1007/s13042-020-01104-8},
  journal      = {International Journal of Machine Learning and Cybernetics},
  number       = {9},
  pages        = {2101-2111},
  shortjournal = {Int. J. Mach. Learn. Cybern.},
  title        = {A novel learning-based approach for efficient dismantling of networks},
  volume       = {11},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Hesitant fuzzy psychological distance measure.
<em>IJMLC</em>, <em>11</em>(9), 2089–2100. (<a
href="https://doi.org/10.1007/s13042-020-01102-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Distance is an indispensable measure in many fields such as clustering analysis, decision making and pattern recognition, etc. When calculating the distance of hesitant fuzzy information, the existing methods normally only take the values of the attributes into consideration while ignore the preferential relationship between the options, which may not meet some actual situations. Thus, it is necessary to propose a new distance measure for hesitant fuzzy information considering both the two aspects. In order to realize this in our paper, firstly, a multi-attribute space is built, in which each attribute is given a unique weight from the experts to show the subjective importance; secondly, the distance vector between the hesitant fuzzy sets (HFSs) is constructed and a balancing coefficient is proposed; thirdly, a novel distance measure for HFS, called the hesitant fuzzy psychological distance measure is developed. In view of the experts’ preferences for the options, the proposed hesitant fuzzy psychological distance between the alternatives can be enlarged relative to the traditional hesitant fuzzy distance measures, which shows a good reasonability in reflecting the experts’ subjective preferences for different alternatives. Furthermore, two numerical examples are used to illustrate the effectiveness and feasibility of the hesitant fuzzy psychological distance measure.},
  archive      = {J_IJMLC},
  author       = {Li, Chaoqun and Zhao, Hua and Xu, Zeshui},
  doi          = {10.1007/s13042-020-01102-w},
  journal      = {International Journal of Machine Learning and Cybernetics},
  number       = {9},
  pages        = {2089-2100},
  shortjournal = {Int. J. Mach. Learn. Cybern.},
  title        = {Hesitant fuzzy psychological distance measure},
  volume       = {11},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Unsupervised image-to-image translation using intra-domain
reconstruction loss. <em>IJMLC</em>, <em>11</em>(9), 2077–2088. (<a
href="https://doi.org/10.1007/s13042-020-01098-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Generative adversarial networks (GANs) have been successfully used for considerable computer vision tasks, especially the image-to-image translation. However, GANs are often accompanied by training instability and mode collapse in the process of image-to-image translation, which leads to the generation of low-quality images. To address the aforementioned problem, by combining CycleGAN and intra-domain reconstruction loss (IDRL), we propose an unsupervised image-to-image translation network named “Cycle-IDRL”. Specifically, the generator adopts the U-Net network with skip connections, which merges the coarse-grained and fine-grained features and the least squares loss in LSGAN is used to improve the stability of training process. Especially, the target domain features extracted from the discriminator are used as input of generator to generate reconstructed samples. Then, we construct the IDRL between the target domain samples and the reconstructed samples by using L1 norm. The experimental results on multiple datasets show that the proposed method performs better than the existing methods.},
  archive      = {J_IJMLC},
  author       = {Fan, Yuan and Shao, Mingwen and Zuo, Wangmeng and Li, Qingyun},
  doi          = {10.1007/s13042-020-01098-3},
  journal      = {International Journal of Machine Learning and Cybernetics},
  number       = {9},
  pages        = {2077-2088},
  shortjournal = {Int. J. Mach. Learn. Cybern.},
  title        = {Unsupervised image-to-image translation using intra-domain reconstruction loss},
  volume       = {11},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). An improved artificial bee colony algorithm for balancing
local and global search behaviors in continuous optimization.
<em>IJMLC</em>, <em>11</em>(9), 2051–2076. (<a
href="https://doi.org/10.1007/s13042-020-01094-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The artificial bee colony, ABC for short, algorithm is population-based iterative optimization algorithm proposed for solving the optimization problems with continuously-structured solution space. Although ABC has been equipped with powerful global search capability, this capability can cause poor intensification on found solutions and slow convergence problem. The occurrence of these issues is originated from the search equations proposed for employed and onlooker bees, which only updates one decision variable at each trial. In order to address these drawbacks of the basic ABC algorithm, we introduce six search equations for the algorithm and three of them are used by employed bees and the rest of equations are used by onlooker bees. Moreover, each onlooker agent can modify three dimensions or decision variables of a food source at each attempt, which represents a possible solution for the optimization problems. The proposed variant of ABC algorithm is applied to solve basic, CEC2005, CEC2014 and CEC2015 benchmark functions. The obtained results are compared with results of the state-of-art variants of the basic ABC algorithm, artificial algae algorithm, particle swarm optimization algorithm and its variants, gravitation search algorithm and its variants and etc. Comparisons are conducted for measurement of the solution quality, robustness and convergence characteristics of the algorithms. The obtained results and comparisons show the experimentally validation of the proposed ABC variant and success in solving the continuous optimization problems dealt with the study.},
  archive      = {J_IJMLC},
  author       = {Hakli, Huseyin and Kiran, Mustafa Servet},
  doi          = {10.1007/s13042-020-01094-7},
  journal      = {International Journal of Machine Learning and Cybernetics},
  number       = {9},
  pages        = {2051-2076},
  shortjournal = {Int. J. Mach. Learn. Cybern.},
  title        = {An improved artificial bee colony algorithm for balancing local and global search behaviors in continuous optimization},
  volume       = {11},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Legal public opinion news abstractive summarization by
incorporating topic information. <em>IJMLC</em>, <em>11</em>(9),
2039–2050. (<a
href="https://doi.org/10.1007/s13042-020-01093-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Automatically generate accurate summaries from legal public opinion news can help readers to grasp the main ideas of news quickly. Although many improved sequence-to-sequence models have been proposed for the abstractive text summarization task, these approaches confront two challenges when addressing domain-specific summarization task: (1) the appropriate selection of domain knowledge; (2) the effective manner of integrating domain knowledge into summarization model. In order to tackle the above challenges, this paper selects the pre-training topic information as the legal domain knowledge, which is then integrated into the sequence-to-sequence model to improve the performance of public opinion news summarization. Concretely, two kinds of topic information are utilized: first, the topic words which denote the main aspects of the source document are encoded to guide the decoding process. Furthermore, the predicted output is forced to have a similar topic probability distribution with the source document. We evaluate our model on a large dataset of legal public opinion news collected from micro-blog, and the experimental results show that the proposed model outperforms existing baseline systems under the rouge metrics. To the best of our knowledge, this work represents the first attempt in the legal public opinion domain for text summarization task.},
  archive      = {J_IJMLC},
  author       = {Huang, Yuxin and Yu, Zhengtao and Guo, Junjun and Yu, Zhiqiang and Xian, Yantuan},
  doi          = {10.1007/s13042-020-01093-8},
  journal      = {International Journal of Machine Learning and Cybernetics},
  number       = {9},
  pages        = {2039-2050},
  shortjournal = {Int. J. Mach. Learn. Cybern.},
  title        = {Legal public opinion news abstractive summarization by incorporating topic information},
  volume       = {11},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Unsupervised attribute reduction based on <span
class="math display"><em>α</em></span> -approximate equal relation in
interval-valued information systems. <em>IJMLC</em>, <em>11</em>(9),
2021–2038. (<a
href="https://doi.org/10.1007/s13042-020-01091-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As generalizations of single-valued information systems, interval-valued information systems (IVISs) can better express real data. At present, numerous unsupervised attribute reduction approaches for single-valued information systems have been considered, but there are few researches on unsupervised attribute reduction for IVISs. In this article, we investigate a new fuzzy relation by means of similarity between interval values, and propose the concept of $$\alpha $$ -approximate equal relation in view of the fuzzy similarity class. Then the equivalence relation induced by $$\alpha $$ -approximate equal relation is used to define the information entropy, which is used to construct the unsupervised attribute reduction method together with mutual information for IVISs. Finally, experiments demonstrate that the advanced unsupervised attribute reduction method is effective and feasible in IVISs.},
  archive      = {J_IJMLC},
  author       = {Liu, Xiaofeng and Dai, Jianhua and Chen, Jiaolong and Zhang, Chucai},
  doi          = {10.1007/s13042-020-01091-w},
  journal      = {International Journal of Machine Learning and Cybernetics},
  number       = {9},
  pages        = {2021-2038},
  shortjournal = {Int. J. Mach. Learn. Cybern.},
  title        = {Unsupervised attribute reduction based on $$\alpha $$ -approximate equal relation in interval-valued information systems},
  volume       = {11},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Extreme learning machine with hybrid cost function of g-mean
and probability for imbalance learning. <em>IJMLC</em>, <em>11</em>(9),
2007–2020. (<a
href="https://doi.org/10.1007/s13042-020-01090-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Extreme learning machine(ELM) is a simple and fast machine learning algorithm. However, similar to other conventional learning algorithms, the classical ELM can not well process the problem of imbalanced data distribution. In this paper, in order to improve the learning performance of classical ELM for imbalanced data learning, we present a novel variant of the ELM algorithm based on a hybrid cost function which employs the probability that given training sample belong in each class to calculate the G-mean. We perform comparable experiments for our approach and the state-of-the-arts methods on standard classification datasets which consist of 58 binary datasets and 9 multiclass datasets under different degrees of imbalance ratio. Experimental results show that our proposed algorithm can improve the classification performance significantly compared with other state-of-the-art methods.},
  archive      = {J_IJMLC},
  author       = {Ri, Jong-Hyok and Tian, Guanzhong and Liu, Yong and Xu, Wei-hua and Lou, Jun-gang},
  doi          = {10.1007/s13042-020-01090-x},
  journal      = {International Journal of Machine Learning and Cybernetics},
  number       = {9},
  pages        = {2007-2020},
  shortjournal = {Int. J. Mach. Learn. Cybern.},
  title        = {Extreme learning machine with hybrid cost function of G-mean and probability for imbalance learning},
  volume       = {11},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). An evolution strategy based approach for cover scheduling
problem in wireless sensor networks. <em>IJMLC</em>, <em>11</em>(9),
1981–2006. (<a
href="https://doi.org/10.1007/s13042-020-01088-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cover scheduling problem in wireless sensor networks (WSN-CSP) aims to find a schedule of covers which minimizes the longest continuous duration of time for which no sensor in the network is able to monitor a target. This problem arises in those sensing environments which permit the coverage breach, i.e., at any instant of time, all targets need not be monitored. The coverage breach may occur owing to either technical restrictions or intentionally. It is an $$\mathcal {NP}$$ -hard problem. This paper presents a $$(1 + 1)$$ -evolution strategy based approach to address WSN-CSP problem. We have compared our approach with the state-of-art approaches available in literature. Computational results show that our approach is significantly superior in comparison to the existing approaches for WSN-CSP.},
  archive      = {J_IJMLC},
  author       = {Srivastava, Gaurav and Venkatesh, Pandiri and Singh, Alok},
  doi          = {10.1007/s13042-020-01088-5},
  journal      = {International Journal of Machine Learning and Cybernetics},
  number       = {9},
  pages        = {1981-2006},
  shortjournal = {Int. J. Mach. Learn. Cybern.},
  title        = {An evolution strategy based approach for cover scheduling problem in wireless sensor networks},
  volume       = {11},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Fully-connected LSTM–CRF on medical concept extraction.
<em>IJMLC</em>, <em>11</em>(9), 1971–1979. (<a
href="https://doi.org/10.1007/s13042-020-01087-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Patient symptoms, test results, and treatment information have been taking down in extensive electronic records. Specifically, the named entity recognition of these medical concepts has high application value in the clinical field. However, due to issues like patient privacy, labeled data is expensive and difficult to find. In 2010, the i2b2/VA Natural Language Processing Challenge started a conceptual extraction task for electronic medical records. One of the task requirements is to classify natural language descriptions as corresponding concept types. In this paper, we proposed a new fully-connected LSTM network, while the LSTM + CRF model is used as the framework to test the effects of various LSTM structures. The real-data experiments demonstrate that the proposed fully-connected LSTM outperforms many of the mainstream LSTM structures in the quantitative evaluation. It is confirmed that the multi-layer bidirectional fully-connected LSTM cooperates with the character level word vector and the pre-trained word embedding, which achieves similar performance compared with the state-of-the-art methods, avoiding the using of prior knowledge data and ultra-high dimensional feature representation. Moreover, this end-to-end training method saves a lot of feature engineering work and storage spaces.},
  archive      = {J_IJMLC},
  author       = {Ji, Jie and Chen, Bairui and Jiang, Hongcheng},
  doi          = {10.1007/s13042-020-01087-6},
  journal      = {International Journal of Machine Learning and Cybernetics},
  number       = {9},
  pages        = {1971-1979},
  shortjournal = {Int. J. Mach. Learn. Cybern.},
  title        = {Fully-connected LSTM–CRF on medical concept extraction},
  volume       = {11},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Enhancing a machine learning binarization framework by
perturbation operators: Analysis on the multidimensional knapsack
problem. <em>IJMLC</em>, <em>11</em>(9), 1951–1970. (<a
href="https://doi.org/10.1007/s13042-020-01085-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Solving combinatorial optimization problems is of great interest in the areas of computer science and operations research. Optimization algorithms and particularly metaheuristics are constantly improved in order to reduce execution times, increase the quality of solutions and address larger instances. In this work, an improvement of the binarization framework which uses the K-means technique is developed. To achieve this, a perturbation operator based on the K-nearest neighbor technique is incorporated into the framework with the aim of generating more robust binarized algorithms. The technique of K-nearest neighbors is used for improving the properties of diversification and intensification of metaheuristics in its binary version. The contribution of the K-nearest neighbors perturbation operator to the final results is systematically analyzed. Particle Swarm Optimization and Cuckoo Search are used as metaheuristic techniques. To verify the results, the well-known multidimensional knapsack problem is tackled. A computational comparison is made with the state-of-the-art of metaheuristic techniques that use general mechanisms of binarization. The results show that our improved framework produces consistently better results. In this sense, the contribution of the operator which uses the K-nearest neighbors technique is investigated finding that this operator contributes significantly to the quality of the results.},
  archive      = {J_IJMLC},
  author       = {García, José and Lalla-Ruiz, Eduardo and Voß, Stefan and Droguett, Enrique López},
  doi          = {10.1007/s13042-020-01085-8},
  journal      = {International Journal of Machine Learning and Cybernetics},
  number       = {9},
  pages        = {1951-1970},
  shortjournal = {Int. J. Mach. Learn. Cybern.},
  title        = {Enhancing a machine learning binarization framework by perturbation operators: Analysis on the multidimensional knapsack problem},
  volume       = {11},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Character-level text classification via convolutional neural
network and gated recurrent unit. <em>IJMLC</em>, <em>11</em>(8),
1939–1949. (<a
href="https://doi.org/10.1007/s13042-020-01084-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Text categorization, or text classification, is one of key tasks for representing the semantic information of documents. Traditional deep leaning models for text categorization are generally time-consuming on large scale datasets due to slow convergence rate or heavily rely on the pre-trained word vectors. Motivated by fully convolutional networks in the field of image processing, we introduce fully convolutional layers to substantially reduce the number of parameters in the text classification model. A character-level model for short text classification, integrating convolutional neural network, bidirectional gated recurrent unit, highway network with the fully connected layers, is proposed to capture both the global and the local textual semantics at the fast convergence speed. Furthermore, In addition, error minimization extreme learning machine is incorporated into the proposed model to improve the classification accuracy further. Extensive experiments show that our approach achieves the state-of-the-art performance compared with the existing methods on the large scale text datasets.},
  archive      = {J_IJMLC},
  author       = {Liu, Bing and Zhou, Yong and Sun, Wei},
  doi          = {10.1007/s13042-020-01084-9},
  journal      = {International Journal of Machine Learning and Cybernetics},
  number       = {8},
  pages        = {1939-1949},
  shortjournal = {Int. J. Mach. Learn. Cybern.},
  title        = {Character-level text classification via convolutional neural network and gated recurrent unit},
  volume       = {11},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Deep quantification down-plain-upsampling residual learning
for single image super-resolution. <em>IJMLC</em>, <em>11</em>(8),
1923–1937. (<a
href="https://doi.org/10.1007/s13042-020-01083-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep convolutional neural networks have been widely used in single image super-resolution (SISR) with great success. However, the performance and efficiency of such models need to be improved for practical applications. In this paper, a novel deep quantification down-plain-upsampling (QDPU) network for SISR is proposed. In the framework, a down-plain-upsampling (DPU) residual block based on U-Net is firstly designed to reduce the computational cost by transforming the spatial scale of feature maps without sacrificing the reconstruction performance. Then, to better transmit low-level features to the reconstruction layer, we construct quantification skip-connection modules to integrate the outputs of the DPU residual blocks. Finally, QDPU is developed by stacking the DPU residual blocks with multiple skip-connections to reconstruct high-resolution images and reduce the computational burden. Quantitative and qualitative evaluations of the reconstruction results on four benchmark datasets show that the proposed method can achieve better performance compared with several state-of-the-art SISR methods.},
  archive      = {J_IJMLC},
  author       = {Huang, Shuying and Zhu, Haijun and Yang, Yong and Zuo, Yifan and Tang, Yingjun},
  doi          = {10.1007/s13042-020-01083-w},
  journal      = {International Journal of Machine Learning and Cybernetics},
  number       = {8},
  pages        = {1923-1937},
  shortjournal = {Int. J. Mach. Learn. Cybern.},
  title        = {Deep quantification down-plain-upsampling residual learning for single image super-resolution},
  volume       = {11},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Least squares support vector machines with fast
leave-one-out AUC optimization on imbalanced prostate cancer data.
<em>IJMLC</em>, <em>11</em>(8), 1909–1922. (<a
href="https://doi.org/10.1007/s13042-020-01081-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Quite often, the available pre-biopsy data for early prostate cancer detection are imbalanced. When the least squares support vector machines (LS-SVMs) are applied to such scenarios, it becomes naturally desirable for us to introduce the well-known AUC performance index into the LS-SVMs framework to avoid bias towards majority classes. However, this may result in high computational complexity for the minimal leave-one-out error. In this paper, by introducing the parameter $$\lambda $$ , a generalized Area under the ROC curve (AUC) performance index $$R_{AUCLS}$$ is developed to theoretically guarantee that $$R_{AUCLS}$$ linearly depends on the classical AUC performance index $$R_{AUC}$$ . Based on both $$R_{AUCLS}$$ and the classical LS-SVM, a new AUC-based least squares support vector machine called AUC-LS-SVMs is proposed for directly and effectively classifying imbalanced prostate cancer data. The distinctive advantage of the proposed classifier AUC-LS-SVMs exists in that it can achieve the minimal leave-one-out error by quickly optimizing the parameter $$\lambda $$ in $$R_{AUCLS}$$ using the proposed fast leave-one-out cross validation (LOOCV) strategy. The proposed classifier is first evaluated using generic public datasets. Further experiments are then conducted on a real-world prostate cancer dataset to demonstrate the efficacy of our proposed classifier for early prostate cancer detection.},
  archive      = {J_IJMLC},
  author       = {Wang, Guanjin and Teoh, Jeremy Yuen-Chun and Lu, Jie and Choi, Kup-Sze},
  doi          = {10.1007/s13042-020-01081-y},
  journal      = {International Journal of Machine Learning and Cybernetics},
  number       = {8},
  pages        = {1909-1922},
  shortjournal = {Int. J. Mach. Learn. Cybern.},
  title        = {Least squares support vector machines with fast leave-one-out AUC optimization on imbalanced prostate cancer data},
  volume       = {11},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Double feature selection algorithm based on low-rank sparse
non-negative matrix factorization. <em>IJMLC</em>, <em>11</em>(8),
1891–1908. (<a
href="https://doi.org/10.1007/s13042-020-01079-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, many feature selection algorithms based on non-negative matrix factorization have been proposed. However, many of these algorithms only consider unilateral information about global or local geometric structure normally. To this end, this paper proposes a new feature selection algorithm called double feature selection algorithm based on low-rank sparse non-negative matrix factorization (NMF-LRSR). Firstly, to reduce the dimensions effectively, NMF-LRSR uses non-negative matrix factorization as the framework to further reduce the dimension of the feature selection which is originally a dimension reduction problem. Secondly, the low-rank sparse representation with the self-representation is used to construct the graph, so both the global and intrinsic geometric structure information of the data could be taken into account in the process of feature selection, which makes full use of the information and makes the feature selection more accurate. In addition, the double feature selection theory is used to this paper, which makes the result of feature selection more accurate. NMF-LRSR is tested on the baseline and the other six algorithms in the literature and evaluated them on 11 publicly available benchmark datasets. Experimental results show that NMF-LRSR is more effective than the other six feature selection algorithms.},
  archive      = {J_IJMLC},
  author       = {Shang, Ronghua and Song, Jiuzheng and Jiao, Licheng and Li, Yangyang},
  doi          = {10.1007/s13042-020-01079-6},
  journal      = {International Journal of Machine Learning and Cybernetics},
  number       = {8},
  pages        = {1891-1908},
  shortjournal = {Int. J. Mach. Learn. Cybern.},
  title        = {Double feature selection algorithm based on low-rank sparse non-negative matrix factorization},
  volume       = {11},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). An actor-critic reinforcement learning-based resource
management in mobile edge computing systems. <em>IJMLC</em>,
<em>11</em>(8), 1875–1889. (<a
href="https://doi.org/10.1007/s13042-020-01077-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Reinforcement learning (RL) as an effective tool has attracted great attention in wireless communication field nowadays. In this paper, we investigate the offloading decision and resource allocation problem in mobile edge computing (MEC) systems based on RL methods. Different from existing literature, our research focuses on improving mobile operators’ revenue by maximizing the amount of the offloaded tasks while decreasing the energy expenditure and time-delays. Considering the dynamic characteristics of wireless environment, the above problem is modeled as a Markov decision process (MDP). Since the action space of the MDP is multidimensional continuous variables mixed with discrete variables, traditional RL algorithms are powerless. Therefore, an actor-critic (AC) with eligibility traces algorithm is proposed to resolve the problem. The actor part introduces the parameterized normal distribution to generate the probabilities of continuous stochastic actions, and the critic part employs a linear approximator to estimate the value of states, based on which the actor part updates policy parameters in the direction of performance improvement. Furthermore, an advantage function is designed to reduce the variance of the learning process. Simulation results indicate that the proposed algorithm can find the best strategy to maximize the amount of the tasks executed by the MEC server while decreasing the energy consumption and time-delays.},
  archive      = {J_IJMLC},
  author       = {Fu, Fang and Zhang, Zhicai and Yu, Fei Richard and Yan, Qiao},
  doi          = {10.1007/s13042-020-01077-8},
  journal      = {International Journal of Machine Learning and Cybernetics},
  number       = {8},
  pages        = {1875-1889},
  shortjournal = {Int. J. Mach. Learn. Cybern.},
  title        = {An actor-critic reinforcement learning-based resource management in mobile edge computing systems},
  volume       = {11},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). W-Metagraph2Vec: A novel approval of enriched schematic
topic-driven heterogeneous information network embedding.
<em>IJMLC</em>, <em>11</em>(8), 1855–1874. (<a
href="https://doi.org/10.1007/s13042-020-01076-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, heterogeneous information network (HIN) embedding is wide studied due to its various applications. In general, network embedding is a way of representation network’s nodes into a low-dimensional space. Most of previous embedding techniques concentrate on the homogeneous networks only in which all nodes are considered as a single type. Heterogeneous network embedding is a challenging problem due to the complexity of different node’s types and link’s types. Recent heterogeneous network embedding studies are based on meta-path and meta-graph to guide the random walks over the networks. These heterogeneous network embedding approaches outperform state-of-the-art homogeneous embedding models in multiple heterogeneous network mining tasks. However, recent meta-graph-based approaches are ineffective in capturing topic similarity between nodes. There is no doubt that most of common HINs (DBLP, Facebook, etc.) are rich-text which contain many text-based nodes, such as paper, comment, post, etc. In this paper, we propose a novel embedding approach, namely W-MetaGraph2Vec. The W-MetaGraph2Vec uses the topic-driven meta-graph-based random walk mechanism in weighted HIN to guide the generation of heterogeneous neighborhood of a node. Extensive experiments on real-world datasets demonstrate that our proposed model not only leverage HIN mining tasks, such as node similarity search, clustering, classification, etc. in performance accuracy but also discern the problems of topic relevance between text-based nodes.},
  archive      = {J_IJMLC},
  author       = {Pham, Phu and Do, Phuc},
  doi          = {10.1007/s13042-020-01076-9},
  journal      = {International Journal of Machine Learning and Cybernetics},
  number       = {8},
  pages        = {1855-1874},
  shortjournal = {Int. J. Mach. Learn. Cybern.},
  title        = {W-Metagraph2Vec: A novel approval of enriched schematic topic-driven heterogeneous information network embedding},
  volume       = {11},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Rotation invariant descriptors for galaxy morphological
classification. <em>IJMLC</em>, <em>11</em>(8), 1839–1853. (<a
href="https://doi.org/10.1007/s13042-020-01075-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The detection of multi-oriented objects is a difficult pattern recognition problem. In this paper, we propose to evaluate the performance of different families of descriptors for the classification of galaxy morphologies. We investigate the performance of the Hu moments, Flusser moments, Zernike moments, Fourier–Mellin moments, and ring projection techniques based on 1D moment and the Fourier transform. We consider two main datasets for the performance evaluation. The first dataset is an artificial dataset based on representative templates from 11 types of galaxies, which are evaluated with different transformations (noise, smoothing), alone or combined. The evaluation is based on image retrieval performance to estimate the robustness of the rotation invariant descriptors with this type of images. The second dataset is composed of real images extracted from the Galaxy Zoo 2 project. The binary classification of elliptical and spiral galaxies is achieved with pre-processing steps including morphological filtering and a Laplacian pyramid. For the binary classification, we compare the different set of features with Support Vector Machines, Extreme Learning Machine, and different types of linear discriminant analysis techniques. The results support the conclusion that the proposed framework for the binary classification of elliptical and spiral galaxies provides an area under the receiver operating characteristic curve reaching 99.54\%, proving the robustness of the approach for helping astronomers to study galaxies.},
  archive      = {J_IJMLC},
  author       = {Cecotti, Hubert},
  doi          = {10.1007/s13042-020-01075-w},
  journal      = {International Journal of Machine Learning and Cybernetics},
  number       = {8},
  pages        = {1839-1853},
  shortjournal = {Int. J. Mach. Learn. Cybern.},
  title        = {Rotation invariant descriptors for galaxy morphological classification},
  volume       = {11},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Respiratory signal and human stress: Non-contact detection
of stress with a low-cost depth sensing camera. <em>IJMLC</em>,
<em>11</em>(8), 1825–1837. (<a
href="https://doi.org/10.1007/s13042-020-01074-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Psychological stress may cause various health problems. To prevent the potential chronic illness that long-term psychological stress could cause, it is important to detect and monitor the psychological stress at its initial stage. In this paper, we present a framework for remotely detecting and classifying human stress by using a KINECT sensor that is portable and affordable enough for ordinary users in everyday life. Unlike most of emotion recognition tasks in which respiratory signals (RSPS) are usually used only as an aiding analysis, the whole task proposed is based on RSPS. Thus, the main contribution of this paper is that not only the non-contact devices is used to identify human stress, but also the relationship between RSPS and stress recognition is analyzed in detail. Experimental results on 84 volunteers show that the recognition accuracy for recognizing psychological stress, physical stress, and relaxing state are 93.90\%, 93.40\%, and 89.05\% respectively. These results suggest that the proposed framework is effective for monitoring human stress, and RSPS could be used for stress recognition.},
  archive      = {J_IJMLC},
  author       = {Shan, Yuhao and Li, Shigang and Chen, Tong},
  doi          = {10.1007/s13042-020-01074-x},
  journal      = {International Journal of Machine Learning and Cybernetics},
  number       = {8},
  pages        = {1825-1837},
  shortjournal = {Int. J. Mach. Learn. Cybern.},
  title        = {Respiratory signal and human stress: Non-contact detection of stress with a low-cost depth sensing camera},
  volume       = {11},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A competitive swarm optimizer with hybrid encoding for
simultaneously optimizing the weights and structure of extreme learning
machines for classification problems. <em>IJMLC</em>, <em>11</em>(8),
1801–1823. (<a
href="https://doi.org/10.1007/s13042-020-01073-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Extreme Learning Machine (ELM) is a learning algorithm proposed recently to train single hidden layer feed forward networks (SLFN). It has many attractive properties that include better generalization performance and very fast learning. ELM starts by assigning random values to the input weights and hidden biases and then in one step it determines the output weights using Moore-Penrose generalized inverse. Despite the aforementioned advantages, ELM performance might be affected by the random initialization of weights and biases or by the large generated network which might contain unnecessary number of neurons. In order to increase the generalization performance and to produce more compact networks, a hybrid model that combines ELM with competitive swarm optimizer (CSO) is proposed in this paper. The proposed model (CSONN-ELM) optimizes the weights and biases and dynamically determines the most appropriate number of neurons. To evaluate the effectiveness of the CSONN-ELM, it is experimented using 23 benchmark datasets, and compared to a set of static rules extracted from literature that are used to determine the number of neurons of SLFN. Moreover, it is compared to two dynamic methods that are used to enhance the performance of ELM, that are Optimally pruned ELM (OP-ELM) and metaheuristic based ELMs (Particle Swarm Optimization-ELM and Differential Evolution-ELM). The obtained results show that the proposed method enhances the generalization performance of ELM and overcomes the static and dynamic methods.},
  archive      = {J_IJMLC},
  author       = {Eshtay, Mohammed and Faris, Hossam and Obeid, Nadim},
  doi          = {10.1007/s13042-020-01073-y},
  journal      = {International Journal of Machine Learning and Cybernetics},
  number       = {8},
  pages        = {1801-1823},
  shortjournal = {Int. J. Mach. Learn. Cybern.},
  title        = {A competitive swarm optimizer with hybrid encoding for simultaneously optimizing the weights and structure of extreme learning machines for classification problems},
  volume       = {11},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Dynamic uncertain causality graph based on cloud model
theory for knowledge representation and reasoning. <em>IJMLC</em>,
<em>11</em>(8), 1781–1799. (<a
href="https://doi.org/10.1007/s13042-020-01072-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The dynamic uncertain causality graph (DUCG), which has been widely applied in many fields, is an important modelling technique for knowledge representation and reasoning. However, the extant DUCG models have been criticized because they cannot precisely represent experts’ knowledge owing to the ignorance of the fuzziness and randomness of uncertain knowledge. In response, we propose a new type of DUCG model called the cloud reasoning dynamic uncertain causality graph (CDUCG). The CDUCG model, which is based on cloud model theory, can handle with the fuzziness and randomness of uncertain information simultaneously. Moreover, an inference algorithm based on the combination of CDUCG and the Technique for Order Preference by Similarity to an Ideal Solution (TOPSIS) is proposed to implement fuzzy knowledge inference effectively and thus make the expert systems more dependable and intelligent. Finally, illustrative examples and an industrial application concerning root cause analysis of aluminum electrolysis are provided to demonstrate the proposed CDUCG model. And experimental results show that the new CDUCG model is flexible and reliable for knowledge representation and reasoning.},
  archive      = {J_IJMLC},
  author       = {Li, Li and Xie, Yongfang and Chen, Xiaofang and Yue, Weichao and Zeng, Zhaohui},
  doi          = {10.1007/s13042-020-01072-z},
  journal      = {International Journal of Machine Learning and Cybernetics},
  number       = {8},
  pages        = {1781-1799},
  shortjournal = {Int. J. Mach. Learn. Cybern.},
  title        = {Dynamic uncertain causality graph based on cloud model theory for knowledge representation and reasoning},
  volume       = {11},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A q-rung orthopair fuzzy multi-criteria group decision
making method for supplier selection based on a novel distance measure.
<em>IJMLC</em>, <em>11</em>(8), 1749–1780. (<a
href="https://doi.org/10.1007/s13042-020-01070-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Supplier selection and evaluation is a crucial decision-making issue to establish an effective supply chain. Higher-order fuzzy decision-making methods have become powerful tools to support decision-makers in solving their problems effectively by reflecting uncertainty in calculations better than crisp sets in the last 3 decades. The q-rung orthopair fuzzy (q-ROF) sets which are the general form of both intuitionistic and Pythagorean fuzzy sets, have been recently introduced to provide decision-makers more freedom of expression than other fuzzy sets. In this paper, we introduce q-ROF TOPSIS and q-ROF ELECTRE as two separate methods and new approaches for group decision making to select the best supplier. As the existing distance measures in q-rung orthopair fuzzy environment have some drawbacks and generate counter-intuitive results, we propose a new distance measure along with its proofs to use in both q-ROF TOPSIS and q-ROF ELECTRE methods. Moreover, a comparison study is conducted to illustrate the superiority of the proposed distance measure. Subsequently, a comprehensive case study is performed with q-ROF TOPSIS and q-ROF ELECTRE methods separately to choose the best supplier for a construction company by rating the importance of criteria and alternatives under q-ROF environment. Finally, a comparison and parameter analysis are performed among the proposed q-ROF TOPSIS and q-ROF ELECTRE methods and existing q-ROF decision-making methods to demonstrate the effectiveness of our proposed methods.},
  archive      = {J_IJMLC},
  author       = {Pinar, Adem and Boran, Fatih Emre},
  doi          = {10.1007/s13042-020-01070-1},
  journal      = {International Journal of Machine Learning and Cybernetics},
  number       = {8},
  pages        = {1749-1780},
  shortjournal = {Int. J. Mach. Learn. Cybern.},
  title        = {A q-rung orthopair fuzzy multi-criteria group decision making method for supplier selection based on a novel distance measure},
  volume       = {11},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). An adaptive kernelized rank-order distance for clustering
non-spherical data with high noise. <em>IJMLC</em>, <em>11</em>(8),
1735–1747. (<a
href="https://doi.org/10.1007/s13042-020-01068-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Clustering is a fundamental research topic in unsupervised learning. Similarity measure is a key factor for clustering. However, it is still challenging for existing similarity measures to cluster non-spherical data with high noise levels. Rank-order distance is proposed to well capture the structures of non-spherical data by sharing the neighboring information of the samples, but it cannot well tolerate high noise. In order to address above issue, we propose KROD, a new similarity measure incorporating rank-order distance with Gaussian kernel. By reducing the noise in the neighboring information of samples, KROD improves rank-order distance to tolerate high noise, thus the structures of non-spherical data with high noise levels can be well captured. Then, KROD strengthens these captured structures by Gaussian kernel so that the samples in the same cluster are closer to each other and can be easily clustered correctly. Experiment illustrates that KROD can effectively improve existing methods for discovering non-spherical clusters with high noise levels. The source code can be downloaded from https://github.com/grcai .},
  archive      = {J_IJMLC},
  author       = {Huang, Tianyi and Wang, Shiping and Zhu, William},
  doi          = {10.1007/s13042-020-01068-9},
  journal      = {International Journal of Machine Learning and Cybernetics},
  number       = {8},
  pages        = {1735-1747},
  shortjournal = {Int. J. Mach. Learn. Cybern.},
  title        = {An adaptive kernelized rank-order distance for clustering non-spherical data with high noise},
  volume       = {11},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Dual sparse learning via data augmentation for robust facial
image classification. <em>IJMLC</em>, <em>11</em>(8), 1717–1734. (<a
href="https://doi.org/10.1007/s13042-020-01067-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data augmentation has been utilized to improve the accuracy and robustness of face recognition algorithms. However, most of the previous studies focused on using the augmentation techniques to enlarge the feature set, while the diversity produced by the virtual samples lacked sufficient attention. In sparse dictionary learning-based face recognition, $$l_1$$ -based sparse representation (SR) and SVD-based dictionary learning (DL) both have shown promising performance. How to utilize both of them in an enhanced training process by data augmentation is still unclear. This paper proposes a novel method that utilizes the sample diversity generated by data augmentation and integrates sparse representation with dictionary learning, to learn dual sparse features for robust face recognition. An additional feature set is created by applying sample augmentation via simply horizontal flipping of face images. The two sparse models, $$l_1$$ -based SR and SVD-based DL, are integrated together using our new proposed objective function. Under two-level fusion of both data and classifiers, the diversity between two training sets is well learned and utilized, in three implementations, to obtain a robust face recognition. After conducting extensive experiments on some popular facial datasets, we demonstrate the proposed method can produce a higher classification accuracy than many state-of-the-art algorithms, and it can be considered as a promising option for image-based face recognition. Our code is released at GitHub.},
  archive      = {J_IJMLC},
  author       = {Zeng, Shaoning and Zhang, Bob and Zhang, Yanghao and Gou, Jianping},
  doi          = {10.1007/s13042-020-01067-w},
  journal      = {International Journal of Machine Learning and Cybernetics},
  number       = {8},
  pages        = {1717-1734},
  shortjournal = {Int. J. Mach. Learn. Cybern.},
  title        = {Dual sparse learning via data augmentation for robust facial image classification},
  volume       = {11},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A self-adaptive exhaustive search optimization-based method
for restoration of bridge defects images. <em>IJMLC</em>,
<em>11</em>(8), 1659–1716. (<a
href="https://doi.org/10.1007/s13042-020-01066-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing bridges are aging and deteriorating. Furthermore, large number of bridges exist in transportation networks meanwhile maintenance budgets are being squeezed. This state of affairs necessities the development of automatic bridge defects evaluation model using computer vision technologies to overcome the limitations of visual inspection. The digital images are prone to degradation by noises during the image acquisition phase. The absence of efficient bridge defects image restoration method results in inaccurate condition assessment models and unreliable bridge management systems. The present study introduces a self-adaptive two-tier method for detection of noises and restoration of bridge defects images. The first model adopts Elman neural network coupled with invasive weed optimization algorithm to identify the type of noise that corrupts images. In the second model, moth-flame optimization algorithm is utilized to design a hybrid image filtering protocol that involves an integration of spatial domain and frequency domain filters. The proposed detection model was assessed through comparisons with other machine learning models as per split validation and tenfold cross validation. It attained the highest classification accuracies, whereas the accuracy, sensitivity, specificity, precision, F-measure and Kappa coefficient are 95.28\%, 95.24\%, 98.07\%, 95.25\%, 95.34\%. 95.43\% and 0.935, respectively in the separate noise recognition module. The capabilities of the proposed restoration model were evaluated against some well-known good-performing optimization algorithms in addition to some conventional restoration models. Moth-flame optimization algorithm outperformed other restoration models, whereas peak signal to noise ratio, mean-squared error, normalized absolute error and image enhancement factor are 25.359, 176.319, 0.0585 and 7.182, respectively.},
  archive      = {J_IJMLC},
  author       = {Mohammed Abdelkader, Eslam and Marzouk, Mohamed and Zayed, Tarek},
  doi          = {10.1007/s13042-020-01066-x},
  journal      = {International Journal of Machine Learning and Cybernetics},
  number       = {8},
  pages        = {1659-1716},
  shortjournal = {Int. J. Mach. Learn. Cybern.},
  title        = {A self-adaptive exhaustive search optimization-based method for restoration of bridge defects images},
  volume       = {11},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A novel hybrid time series forecasting model based on
neutrosophic-PSO approach. <em>IJMLC</em>, <em>11</em>(8), 1643–1658.
(<a href="https://doi.org/10.1007/s13042-020-01064-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article proposed a new time series forecasting model based on neutrosophic set (NS) theory and particle swarm optimization (PSO) algorithm. The proposed model initiated with the representation of time series dataset into NS using three different memberships of NS, i.e., truth-membership, indeterminacy-membership and falsity-membership. This NS representation of time series was referred to as neutrosophic time series (NTS). It was observed that the forecasting accuracy of the proposed model was highly relied on the optimal selection of the universe of discourse of time series dataset. In this study, this problem was resolved by using the PSO algorithm. The proposed model was verified and validated with three different datasets that included the university enrollments dataset of Alabama, TAIFEX index and TSEC weighted index. Experimental results showed that the proposed model outperformed existing benchmark models with average forecasting error rates of 0.80\%, 0.015\% and 0.90\% for the university enrollments, TAIFEX and TSEC, respectively.},
  archive      = {J_IJMLC},
  author       = {Singh, Pritpal},
  doi          = {10.1007/s13042-020-01064-z},
  journal      = {International Journal of Machine Learning and Cybernetics},
  number       = {8},
  pages        = {1643-1658},
  shortjournal = {Int. J. Mach. Learn. Cybern.},
  title        = {A novel hybrid time series forecasting model based on neutrosophic-PSO approach},
  volume       = {11},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Fine-tuning of line and slope based on evolutionary
mechanism. <em>IJMLC</em>, <em>11</em>(7), 1631–1641. (<a
href="https://doi.org/10.1007/s13042-020-01071-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The line and slope fine-tuning is the process of optimizing the horizontal and longitudinal sections to meet the requirements of building clearance, which is an indispensable step in the building engineering. The traditional line and slope fine-tuning, which is manually completed by designers, depends heavily on the domain knowledge of designers. The more experienced the designer is, the better the effect of line and slope fine-tuning will be. This paper makes a first attempt to apply the evolutionary algorithm to the process of line and slope fine-tuning. The main work includes: a new denoising method for tunnel point cloud data is proposed to remove noisy and redundant data from point cloud; an objective function is given to measure the deviation between the design tunnel and the real tunnel; and a learning model of the line and slope fine-tuning is built based on the point cloud data and evolutionary algorithm. A dataset from a length of the real metro tunnel is used for model testing. The testing results show that, in comparison with the traditional manually-adjusting method, our approach based on the evolutionary algorithm can significantly reduce the deviation between the adjusted tunnel and the real tunnel.},
  archive      = {J_IJMLC},
  author       = {Chen, Shuyue and Wang, Hongjie and Wang, Qin and Zhang, Guohua},
  doi          = {10.1007/s13042-020-01071-0},
  journal      = {International Journal of Machine Learning and Cybernetics},
  number       = {7},
  pages        = {1631-1641},
  shortjournal = {Int. J. Mach. Learn. Cybern.},
  title        = {Fine-tuning of line and slope based on evolutionary mechanism},
  volume       = {11},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). From static to dynamic word representations: A survey.
<em>IJMLC</em>, <em>11</em>(7), 1611–1630. (<a
href="https://doi.org/10.1007/s13042-020-01069-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the history of natural language processing (NLP) development, the representation of words has always been a significant research topic. In this survey, we provide a comprehensive typology of word representation models from a novel perspective that the development from static to dynamic embeddings can effectively address the polysemy problem, which has been a great challenge in this field. Then the survey covers the main evaluation metrics and applications of these word embeddings. And, we further discuss the development of word embeddings from static to dynamic in cross-lingual scenario. Finally, we point out some open issues and future works.},
  archive      = {J_IJMLC},
  author       = {Wang, Yuxuan and Hou, Yutai and Che, Wanxiang and Liu, Ting},
  doi          = {10.1007/s13042-020-01069-8},
  journal      = {International Journal of Machine Learning and Cybernetics},
  number       = {7},
  pages        = {1611-1630},
  shortjournal = {Int. J. Mach. Learn. Cybern.},
  title        = {From static to dynamic word representations: A survey},
  volume       = {11},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). An ORESTE approach for multi-criteria decision-making with
probabilistic hesitant fuzzy information. <em>IJMLC</em>,
<em>11</em>(7), 1591–1609. (<a
href="https://doi.org/10.1007/s13042-020-01060-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As an important extension of fuzzy number, the probabilistic hesitant fuzzy element (PHFE) shows the flexibility of decision makers in expressing hesitant information in multi-criteria decision-making (MCDM) processes. Accordingly, numerous research findings have been obtained since PHFE introduction. However, a few important issues in PHFE utilization remain to be addressed. This study introduces the French organization Rangement Et Synthese De Ronnees Relationnelles’ (ORESTE) approach for MCDM with probabilistic hesitant fuzzy information. First, the limitations of normalized PHFE (NPHFE), Euclidean distance, and several operations in previous studies are discussed. Subsequently, an algorithm is designed to derive the new NPHFE. A new Euclidean distance and several operations are developed on the basis of the proposed NPHFE. Second, the ORESTE approach is extended to probabilistic hesitant fuzzy environments. Lastly, the problem of selecting best research topic is presented to demonstrate that the proposed approach is effective. A comparative study with other approaches is conducted with identical illustrative example.},
  archive      = {J_IJMLC},
  author       = {Li, Jian and Chen, Qiongxia and Niu, Li-li and Wang, Zhong-xing},
  doi          = {10.1007/s13042-020-01060-3},
  journal      = {International Journal of Machine Learning and Cybernetics},
  number       = {7},
  pages        = {1591-1609},
  shortjournal = {Int. J. Mach. Learn. Cybern.},
  title        = {An ORESTE approach for multi-criteria decision-making with probabilistic hesitant fuzzy information},
  volume       = {11},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Regularization on a rapidly varying manifold.
<em>IJMLC</em>, <em>11</em>(7), 1571–1590. (<a
href="https://doi.org/10.1007/s13042-019-01059-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The presence of inflection points on data manifold or rapid variation makes it difficult for the second order derivative based graph Laplacian and Hessian regularization techniques to accurately approximate the marginal distribution parameters. Moreover, in general, function over-fitting on seen unlabeled instances due to lack of extrapolation power which makes graph Laplacian regularization based solution biased towards constant. Hessian solves this problem by opting a generic function based on the function’s divergence in more than one direction. However, due to the presence of inflection points in the dense region, the function remains unpenalized by Hessian manifold regularization. We propose a Jerk based manifold regularization (JR) for dense, oscillating and manifolds with inflection points. JR approximates the rate of change of curvature from the underlying manifold which appropriately identifies the unpenalized geodesic deviating functions and accurately penalizes them. It also helps in identifying the optimal function in the presence of inflection points. Extensive experiments on synthetic and real-world datasets show that the proposed JR technique approximates accurate and generic input space geometrical constraints to outperform existing state-of-the-art manifold regularization techniques by a significant margin.},
  archive      = {J_IJMLC},
  author       = {Yadav, Rakesh Kumar and Abhishek and Verma, Shekhar and Venkatesan, S.},
  doi          = {10.1007/s13042-019-01059-5},
  journal      = {International Journal of Machine Learning and Cybernetics},
  number       = {7},
  pages        = {1571-1590},
  shortjournal = {Int. J. Mach. Learn. Cybern.},
  title        = {Regularization on a rapidly varying manifold},
  volume       = {11},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Unsupervised feature learning with sparse bayesian
auto-encoding based extreme learning machine. <em>IJMLC</em>,
<em>11</em>(7), 1557–1569. (<a
href="https://doi.org/10.1007/s13042-019-01057-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Extreme learning machine (ELM) is a popular method in machine learning with extremely few parameters, fast learning speed and model efficiency. Unsupervised feature learning based ELM receives rising research focus. Recently the ELM auto-encoder (ELM-AE) was proposed for this task, which develops the ELM based compact feature learning without sacrificing elegant solution. Compared with ELM-AE and following $$\ell _1$$-regularized ELM-AE, we introduce a sparse Bayesian learning scheme into ELM-AE for better generalization capability. A parallel training strategy is also integrated to improve time-efficiency of multi-output sparse Bayesian learning. Furthermore, pruning hidden nodes for better performance and efficiency according to estimated variances of prior distribution of output weights is achieved. Experiments on several datasets verify the effectiveness and efficiency of our proposed ELM-AE for unsupervised feature learning, compared with PCA, NMF, ELM-AE and $$\ell _1$$-regularized ELM-AE.},
  archive      = {J_IJMLC},
  author       = {Zhang, Guanghao and Cui, Dongshun and Mao, Shangbo and Huang, Guang-Bin},
  doi          = {10.1007/s13042-019-01057-7},
  journal      = {International Journal of Machine Learning and Cybernetics},
  number       = {7},
  pages        = {1557-1569},
  shortjournal = {Int. J. Mach. Learn. Cybern.},
  title        = {Unsupervised feature learning with sparse bayesian auto-encoding based extreme learning machine},
  volume       = {11},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Stochastic trust region inexact newton method for
large-scale machine learning. <em>IJMLC</em>, <em>11</em>(7), 1541–1555.
(<a href="https://doi.org/10.1007/s13042-019-01055-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nowadays stochastic approximation methods are one of the major research direction to deal with the large-scale machine learning problems. From stochastic first order methods, now the focus is shifting to stochastic second order methods due to their faster convergence and availability of computing resources. In this paper, we have proposed a novel stochastic trust region inexact Newton method, called as STRON, to solve large-scale learning problems which uses conjugate gradient to inexactly solve trust region subproblem. The method uses progressive subsampling in the calculation of gradient and Hessian values to take the advantage of both, stochastic and full-batch regimes. We have extended STRON using existing variance reduction techniques to deal with the noisy gradients and using preconditioned conjugate gradient as subproblem solver, and empirically proved that they do not work as expected, for the large-scale learning problems. Finally, our empirical results prove efficacy of the proposed method against existing methods with bench marked datasets.},
  archive      = {J_IJMLC},
  author       = {Chauhan, Vinod Kumar and Sharma, Anuj and Dahiya, Kalpana},
  doi          = {10.1007/s13042-019-01055-9},
  journal      = {International Journal of Machine Learning and Cybernetics},
  number       = {7},
  pages        = {1541-1555},
  shortjournal = {Int. J. Mach. Learn. Cybern.},
  title        = {Stochastic trust region inexact newton method for large-scale machine learning},
  volume       = {11},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Homo-ELM: Fully homomorphic extreme learning machine.
<em>IJMLC</em>, <em>11</em>(7), 1531–1540. (<a
href="https://doi.org/10.1007/s13042-019-01054-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Extreme learning machine (ELM) as a machine learning method has been successfully applied to many classification problems. However, when applying ELM to classification tasks on the encrypted data in cloud, the classification performance is extremely low. Due to the data encryption, ELM is hard to extract informative features from the encrypted data for correct classification. Moreover, the trained neural network is un-protected on the cloud environments, that makes cloud service highly risky to the attackers. In this paper, we propose a novel fully homomorphic ELM (Homo-ELM), which makes cloud searching tasks under a fully protected environment without compromising the privacy of users. To demonstrate the effectiveness of our approach, we conduct a comprehensive experiment on both cloud and local environments. The experiment results show that Homo-ELM can achieve high accuracy on the local environments as well as cloud environments than other machine learning methods.},
  archive      = {J_IJMLC},
  author       = {Wang, Weiru and Gan, Yanfen and Vong, Chi-Man and Chen, Chuangquan},
  doi          = {10.1007/s13042-019-01054-w},
  journal      = {International Journal of Machine Learning and Cybernetics},
  number       = {7},
  pages        = {1531-1540},
  shortjournal = {Int. J. Mach. Learn. Cybern.},
  title        = {Homo-ELM: Fully homomorphic extreme learning machine},
  volume       = {11},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Gaining-sharing knowledge based algorithm for solving
optimization problems: A novel nature-inspired algorithm.
<em>IJMLC</em>, <em>11</em>(7), 1501–1529. (<a
href="https://doi.org/10.1007/s13042-019-01053-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes a novel nature-inspired algorithm called Gaining Sharing Knowledge based Algorithm (GSK) for solving optimization problems over continuous space. The GSK algorithm mimics the process of gaining and sharing knowledge during the human life span. It is based on two vital stages, junior gaining and sharing phase and senior gaining and sharing phase. The present work mathematically models these two phases to achieve the process of optimization. In order to verify and analyze the performance of GSK, numerical experiments on a set of 30 test problems from the CEC2017 benchmark for 10, 30, 50 and 100 dimensions. Besides, the GSK algorithm has been applied to solve the set of real world optimization problems proposed for the IEEE-CEC2011 evolutionary algorithm competition. A comparison with 10 state-of-the-art and recent metaheuristic algorithms are executed. Experimental results indicate that in terms of robustness, convergence and quality of the solution obtained, GSK is significantly better than, or at least comparable to state-of-the-art approaches with outstanding performance in solving optimization problems especially with high dimensions.},
  archive      = {J_IJMLC},
  author       = {Mohamed, Ali Wagdy and Hadi, Anas A. and Mohamed, Ali Khater},
  doi          = {10.1007/s13042-019-01053-x},
  journal      = {International Journal of Machine Learning and Cybernetics},
  number       = {7},
  pages        = {1501-1529},
  shortjournal = {Int. J. Mach. Learn. Cybern.},
  title        = {Gaining-sharing knowledge based algorithm for solving optimization problems: A novel nature-inspired algorithm},
  volume       = {11},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). An online PLA algorithm with maximum error bound for
generating optimal mixed-segments. <em>IJMLC</em>, <em>11</em>(7),
1483–1499. (<a
href="https://doi.org/10.1007/s13042-019-01052-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Piecewise Linear Approximation (PLA) is an effective method used to represent and compress a time series. It divides a time series into a number of segments, each of which is approximated by a straight line. This division and approximation is done under a metric enforcing optimized storage and compressed data quality criteria. In this article, we propose a new optimal linear-time PLA algorithm (SemiMixedAlg) for generating a set of mixed-connected (continue and disconnected segments) with guaranteed maximum error and minimized storage. An efficient “k-length” strategy is designed to determine the location of mixed segments in order to minimize the storage of mixed-connected segments. Our experiments on 43 real-world data sets show that SemiMixedAlg achieves exactly the same results as that of PipeMixedAlg (Luo et al. in Piecewise linear approximation of streaming time series data with max-error guarantees. In: IEEE international conference on data engineering, pp 173—184); the only state of the art algorithm, but with much lower time and memory costs.},
  archive      = {J_IJMLC},
  author       = {Zhao, Huanyu and Li, Tongliang and Chen, Genlang and Dong, Zhaowei and Bo, Mengya and Pang, Chaoyi},
  doi          = {10.1007/s13042-019-01052-y},
  journal      = {International Journal of Machine Learning and Cybernetics},
  number       = {7},
  pages        = {1483-1499},
  shortjournal = {Int. J. Mach. Learn. Cybern.},
  title        = {An online PLA algorithm with maximum error bound for generating optimal mixed-segments},
  volume       = {11},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A systematic review of the research trends of machine
learning in supply chain management. <em>IJMLC</em>, <em>11</em>(7),
1463–1482. (<a
href="https://doi.org/10.1007/s13042-019-01050-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Research interests in machine learning (ML) and supply chain management (SCM) have yielded an enormous amount of publications during the last two decades. However, in the literature, there was no systematic examination on the research development in the discipline of ML application, in particular in SCM. Therefore, this study was carried out to present the latest research trends in the discipline by analyzing the publications between 1998/01/01 and 2018/12/31 in five major databases. The quantitative analysis of 123 shortlisted articles showed that ML applications in SCM were still in a developmental stage since there were not enough high-yielding authors to form a strong group force in the research of ML applications in SCM and their publications were still at a low level; even though 10 ML algorithms were found to be frequently used in SCM, the use of these algorithms were unevenly distributed across the SCM activities most frequently reported in the articles of the literature. The aim of this study is to provide a comprehensive view of ML applications in SCM, working as a reference for future research directions for SCM researchers and application insight for SCM practitioners.},
  archive      = {J_IJMLC},
  author       = {Ni, Du and Xiao, Zhi and Lim, Ming K.},
  doi          = {10.1007/s13042-019-01050-0},
  journal      = {International Journal of Machine Learning and Cybernetics},
  number       = {7},
  pages        = {1463-1482},
  shortjournal = {Int. J. Mach. Learn. Cybern.},
  title        = {A systematic review of the research trends of machine learning in supply chain management},
  volume       = {11},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Orientation space code and multi-feature two-phase sparse
representation for palmprint recognition. <em>IJMLC</em>,
<em>11</em>(7), 1453–1461. (<a
href="https://doi.org/10.1007/s13042-019-01049-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Orientation based coding method is one of the most important approaches in palmprint recognition, which achieves impressive performance by extracting one or more dominant orientation features, and calculating the distance between features of two palmprints. However, simply using the orientation features may be vulnerable to noisy and rotation. In this paper, we proposed a novel orientation-space code scheme to represent the orientation space feature of palmprint and designed a novel multi-feature two-phase sparse representation (MTPSR) scheme for feature matching. Extensive experiments on three benchmark palmprint databases are conducted to demonstrate the high effectiveness of the proposed method},
  archive      = {J_IJMLC},
  author       = {Liang, Lu and Chen, Tao and Fei, Lunke},
  doi          = {10.1007/s13042-019-01049-7},
  journal      = {International Journal of Machine Learning and Cybernetics},
  number       = {7},
  pages        = {1453-1461},
  shortjournal = {Int. J. Mach. Learn. Cybern.},
  title        = {Orientation space code and multi-feature two-phase sparse representation for palmprint recognition},
  volume       = {11},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Multiobjective hybrid monarch butterfly optimization for
imbalanced disease classification problem. <em>IJMLC</em>,
<em>11</em>(7), 1423–1451. (<a
href="https://doi.org/10.1007/s13042-019-01047-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Datasets obtained from the real world are far from balanced, particularly for disease datasets, since such datasets are usually highly skewed having a few minority classes apart from one or more prominent majority classes. In this research, we put forward the novel hybrid architecture to handle imbalanced binary disease datasets that arrives upon the efficient combination of Support vector machine (SVM) classifier’s sensitive parameter values for improved performance of SVM by means of an Evolutionary algorithm (EA), namely monarch butterfly optimization (MBO). In this paper, MBO is used to enumerate three objectives, namely prediction accuracy (PAC), sensitivity (SEN), specificity (SPE). Additionally, we propose a Totally uni-modular matrix (TUM) and limit points based non-dominated solutions selection for deciding local and global search and to generate an efficient initial population respectively. Since these two greatly affect the performance of EAs, the performance of the proposed hybrid architecture is tested on 18 disease datasets having binary class labels and the results obtained demonstrate improvements using the proposed method. For the majority of the datasets, either 100\% sensitivity and/or specificity were attained. Moreover, pertinent statistical tests were carried out to ascertain the performances obtained.},
  archive      = {J_IJMLC},
  author       = {Nalluri, MadhuSudana Rao and Kannan, Krithivasan and Gao, Xiao-Zhi and Roy, Diptendu Sinha},
  doi          = {10.1007/s13042-019-01047-9},
  journal      = {International Journal of Machine Learning and Cybernetics},
  number       = {7},
  pages        = {1423-1451},
  shortjournal = {Int. J. Mach. Learn. Cybern.},
  title        = {Multiobjective hybrid monarch butterfly optimization for imbalanced disease classification problem},
  volume       = {11},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Supervised feature selection by constituting a basis for the
original space of features and matrix factorization. <em>IJMLC</em>,
<em>11</em>(7), 1405–1421. (<a
href="https://doi.org/10.1007/s13042-019-01046-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most of existing research works in the field of feature selection via matrix factorization techniques have been employed for unsupervised learning problems. This paper introduces a new framework for the supervised feature selection, called supervised feature selection by constituting a basis for the original space of features and matrix factorization (SFS-BMF). To this end, SFS-BMF is a guided search to find a basis for the original space of features that inherently contains linearly independent features and can be replaced with the original space. For finding the best subset of features regarding the class attribute, information gain is utilized for the process of constructing a basis. In fact, a basis for the original features is constructed according to the most informative features in terms of the information gain. Then, this basis is decomposed through a matrix factorization form in order to select a subset of features. Our proposed method guarantees the maximum relevancy of selected features to the output by using the information gain while simultaneously secures the minimum redundancy among them based on the linear independence property. Several experiments on high-dimensional microarray datasets are conducted for illustrating the efficiency of SFS-BMF. The experimental results show that the proposed SFS-BMF method outperforms some state-of-the-art feature selection methods with respect to classification performance and also according to the computational complexity.},
  archive      = {J_IJMLC},
  author       = {Saberi-Movahed, Farid and Eftekhari, Mahdi and Mohtashami, Mohammad},
  doi          = {10.1007/s13042-019-01046-w},
  journal      = {International Journal of Machine Learning and Cybernetics},
  number       = {7},
  pages        = {1405-1421},
  shortjournal = {Int. J. Mach. Learn. Cybern.},
  title        = {Supervised feature selection by constituting a basis for the original space of features and matrix factorization},
  volume       = {11},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Parameter self-tuning schemes for the two phase test sample
sparse representation classifier. <em>IJMLC</em>, <em>11</em>(7),
1387–1403. (<a
href="https://doi.org/10.1007/s13042-019-01045-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sparse Representation Classifier (SRC) and its variants were considered as powerful classifiers in the domains of computer vision and pattern recognition. However, classifying test samples is computationally expensive due to the $$\ell _1$$ norm minimization problem that should be solved in order to get the sparse code. Therefore, these classifiers could not be the right choice for scenarios requiring fast classification. In order to overcome the expensive computational cost of SRC, a two-phase coding classifier based on classic Regularized Least Square was proposed. This classifier is more efficient than SRC. A significant limitation of this classifier is the fact that the number of the samples that should be handed over to the next coding phase should be specified a priori. This paper overcomes this main limitation and proposes five data-driven schemes allowing an automatic estimation of the optimal size of the local samples. These schemes handle the three cases that are encountered in any learning system: supervised, unsupervised, and semi-supervised. Experiments are conducted on five image datasets. These experiments show that the introduced learning schemes can improve the performance of the two-phase linear coding classifier adopting ad-hoc choices for the number of local samples.},
  archive      = {J_IJMLC},
  author       = {Dornaika, F. and El Traboulsi, Y. and Ruichek, Y.},
  doi          = {10.1007/s13042-019-01045-x},
  journal      = {International Journal of Machine Learning and Cybernetics},
  number       = {7},
  pages        = {1387-1403},
  shortjournal = {Int. J. Mach. Learn. Cybern.},
  title        = {Parameter self-tuning schemes for the two phase test sample sparse representation classifier},
  volume       = {11},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A survey of robust optimization based machine learning with
special reference to support vector machines. <em>IJMLC</em>,
<em>11</em>(7), 1359–1385. (<a
href="https://doi.org/10.1007/s13042-019-01044-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper gives an overview of developments in the field of robust optimization in machine learning (ML) in general and Support Vector Machine (SVM)/Support Vector Regression (SVR) models in particular. This survey comprises of researches in which robustness is sought against uncertainty. This uncertainty is in the values of parameters of the given model or it can be in the data. In this work, we have discussed how robust optimization has entered in the field of machine learning. Here, we investigate the contribution of various researchers in dealing with different types of uncertainties arising in the problem of maximizing or minimizing the objective function. We deal with the variants of SVM/SVR in more detail, although we have also covered supervised, unsupervised ML and various other aspects of ML. Also, we present an extensive study of research carried out in the applications of robust optimization in other areas like energy and power systems, networking, transportation etc. as well.},
  archive      = {J_IJMLC},
  author       = {Singla, Manisha and Ghosh, Debdas and Shukla, K. K.},
  doi          = {10.1007/s13042-019-01044-y},
  journal      = {International Journal of Machine Learning and Cybernetics},
  number       = {7},
  pages        = {1359-1385},
  shortjournal = {Int. J. Mach. Learn. Cybern.},
  title        = {A survey of robust optimization based machine learning with special reference to support vector machines},
  volume       = {11},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Correction to: A robust self-weighted SELO regression model.
<em>IJMLC</em>, <em>11</em>(6), 1357. (<a
href="https://doi.org/10.1007/s13042-020-01092-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the print published article, the reference 19 was published incorrectly and the correct reference is given below.},
  archive      = {J_IJMLC},
  author       = {Su, Meihong and Guo, Yaqing and Men, Changqian and Wang, Wenjian},
  doi          = {10.1007/s13042-020-01092-9},
  journal      = {International Journal of Machine Learning and Cybernetics},
  number       = {6},
  pages        = {1357},
  shortjournal = {Int. J. Mach. Learn. Cybern.},
  title        = {Correction to: A robust self-weighted SELO regression model},
  volume       = {11},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Incremental attribute reduction with rough set for dynamic
datasets with simultaneously increasing samples and attributes.
<em>IJMLC</em>, <em>11</em>(6), 1339–1355. (<a
href="https://doi.org/10.1007/s13042-020-01065-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Attribute reduction with rough set is a popular data analysis methodology for data dimensionality reduction. For dynamic datasets, the existing research has mainly focused on incremental attribute reduction with increasing samples (rows) or attributes (columns), but there is hardly any further research on attribute reduction for dynamic datasets with simultaneously increasing samples and attributes. This paper presents a novel incremental algorithm for attribute reduction with rough set. Firstly, the definition of discernibility relation is proposed based on the improved discernibility matrix. Then, the incremental mechanisms of samples and attributes are studied in terms of discernibility relation under a unified framework. On the basis of two incremental mechanisms, a unified incremental mechanism is introduced for dynamic datasets with simultaneously increasing samples and attributes, and the incremental algorithm is developed according to the unified incremental mechanism. The proposed algorithm has the solid mathematical foundation, which is also suitable for datasets with massive samples and attributes. Finally, compared experimentally with other algorithms, the efficiency of the developed incremental algorithm is demonstrated in terms of running time.},
  archive      = {J_IJMLC},
  author       = {Dong, Lianjie and Chen, Degang},
  doi          = {10.1007/s13042-020-01065-y},
  journal      = {International Journal of Machine Learning and Cybernetics},
  number       = {6},
  pages        = {1339-1355},
  shortjournal = {Int. J. Mach. Learn. Cybern.},
  title        = {Incremental attribute reduction with rough set for dynamic datasets with simultaneously increasing samples and attributes},
  volume       = {11},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Selecting park locations using a genetic algorithm and
comprehensive satisfaction. <em>IJMLC</em>, <em>11</em>(6), 1331–1338.
(<a href="https://doi.org/10.1007/s13042-019-01043-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Parks play an important role in beautifying the urban environment in cities and in improving the quality of life of citizens. The foremost step in park construction is solving the location problem. Government and resident satisfaction are proposed, and a linear programming model is suggested herein to study the park location problem. Furthermore, government and resident satisfaction are considered together in a metric we call comprehensive satisfaction, and a special solution model called the park location (PL) model is obtained; the properties of the PL model are also investigated. To solve the PL model, a special genetic algorithm in which random simulation is embedded is proposed. Finally, an application of the proposed approach for solving the park location problem is provided as an illustration. This study is expected to help readers to better compare the performance of the proposed method to alternatives.},
  archive      = {J_IJMLC},
  author       = {Ge, Yunyu and Xin, Boyu and Zhou, Lei and Li, Xiong},
  doi          = {10.1007/s13042-019-01043-z},
  journal      = {International Journal of Machine Learning and Cybernetics},
  number       = {6},
  pages        = {1331-1338},
  shortjournal = {Int. J. Mach. Learn. Cybern.},
  title        = {Selecting park locations using a genetic algorithm and comprehensive satisfaction},
  volume       = {11},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Graph-based label propagation algorithm for community
detection. <em>IJMLC</em>, <em>11</em>(6), 1319–1329. (<a
href="https://doi.org/10.1007/s13042-019-01042-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Community detection is one of the most important topics in complex network analysis. Among a variety of approaches for detecting communities, the label propagation algorithm (LPA) is the simplest and time-efficient approach. However, the original label propagation algorithm is not stable due to the randomness in its propagation process. In this paper, we propose a graph-based label propagation algorithm (GLPA) to detect communities incorporating the node similarity and connectivity information during the propagation of the labels. First, we define node similarity between adjacent nodes, and change each node’s label to that of its most similar neighbor node. Based on the label propagation process, GLPA constructs a label propagation graph to get candidate communities. Then, GLPA calculates the connected components of the label propagation graph. Each connected component is treated as a candidate community in the next step. Second, GLPA constructs a weighted graph to obtain final communities, in which each connected component are treated as a super-node, and the number of edges lying between the corresponding components as the weight of edges. We compute the merging factor for each node in the weighted graph and merge super nodes with higher merging factor to its most similar node iteratively to reach the maximum complementary entropy. Compared with 8 other classical community detection algorithms on LFR artificial networks and 12 real world networks, the proposed algorithm GLPA shows preferable performance on stability, NMI, ARI, modularity.},
  archive      = {J_IJMLC},
  author       = {Yang, Gui and Zheng, Wenping and Che, Chenhao and Wang, Wenjian},
  doi          = {10.1007/s13042-019-01042-0},
  journal      = {International Journal of Machine Learning and Cybernetics},
  number       = {6},
  pages        = {1319-1329},
  shortjournal = {Int. J. Mach. Learn. Cybern.},
  title        = {Graph-based label propagation algorithm for community detection},
  volume       = {11},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Study on the prediction of stock price based on the
associated network model of LSTM. <em>IJMLC</em>, <em>11</em>(6),
1307–1317. (<a
href="https://doi.org/10.1007/s13042-019-01041-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Stock market has received widespread attention from investors. It has always been a hot spot for investors and investment companies to grasp the change regularity of the stock market and predict its trend. Currently, there are many methods for stock price prediction. The prediction methods can be roughly divided into two categories: statistical methods and artificial intelligence methods. Statistical methods include logistic regression model, ARCH model, etc. Artificial intelligence methods include multi-layer perceptron, convolutional neural network, naive Bayes network, back propagation network, single-layer LSTM, support vector machine, recurrent neural network, etc. But these studies predict only one single value. In order to predict multiple values in one model, it need to design a model which can handle multiple inputs and produces multiple associated output values at the same time. For this purpose, it is proposed an associated deep recurrent neural network model with multiple inputs and multiple outputs based on long short-term memory network. The associated network model can predict the opening price, the lowest price and the highest price of a stock simultaneously. The associated network model was compared with LSTM network model and deep recurrent neural network model. The experiments show that the accuracy of the associated model is superior to the other two models in predicting multiple values at the same time, and its prediction accuracy is over 95\%.},
  archive      = {J_IJMLC},
  author       = {Ding, Guangyu and Qin, Liangxi},
  doi          = {10.1007/s13042-019-01041-1},
  journal      = {International Journal of Machine Learning and Cybernetics},
  number       = {6},
  pages        = {1307-1317},
  shortjournal = {Int. J. Mach. Learn. Cybern.},
  title        = {Study on the prediction of stock price based on the associated network model of LSTM},
  volume       = {11},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). GRPCA21 for recovering a corrupted low-rank matrix.
<em>IJMLC</em>, <em>11</em>(6), 1293–1305. (<a
href="https://doi.org/10.1007/s13042-019-01039-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Robust principal component analysis (RPCA) based methods via decomposition into low-rank plus sparse matrices offer a wide range of applications for image processing, video processing and 3D computer vision. Most of the time the observed imagery data is often arbitrarily corrupted by anything such as large sparse noise, small dense noise and other unknown fraction, which we call mixed noise in this paper. However, low rank matrix recovery by RPCA is born for the existence of large sparse noise, so its performance and applicability are limited in the presence of mixed noise. In this paper, a generalized robust principal component analysis with norm $$l_{2,1}$$ model is proposed to solve the problem of low-rank matrix recovery under mixed large sparse noise and small dense noise. The corrupted matrix is written as a combination that minimizes the nuclear norm, the 1-norm and the norm $$l_{2,1}$$, which has high efficiency, flexibility and robustness for low rank matrix recovery from mixed noise. Then a novel and efficient algorithm called random permutation alternative direction of multiplier method is applied to solve the model. Experiments with simulations and real datasets demonstrate efficiency and robustness of this model and algorithm.},
  archive      = {J_IJMLC},
  author       = {Zhao, Lina and Hou, Xuke and Yang, Hongwei and Li, Ji},
  doi          = {10.1007/s13042-019-01039-9},
  journal      = {International Journal of Machine Learning and Cybernetics},
  number       = {6},
  pages        = {1293-1305},
  shortjournal = {Int. J. Mach. Learn. Cybern.},
  title        = {GRPCA21 for recovering a corrupted low-rank matrix},
  volume       = {11},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Object-based feature extraction for hyperspectral data using
firefly algorithm. <em>IJMLC</em>, <em>11</em>(6), 1277–1291. (<a
href="https://doi.org/10.1007/s13042-019-01038-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Object-based classification methods can improve the accuracy of hyperspectral image classification due to the fact that they incorporate spatial information into the classification procedure by assigning neighboring pixels into the same class. In this paper, a new object-based feature extraction method is proposed which makes use of information theory to reduce the Bayes error. In this way, the proposed method exploits higher order statistics for feature extraction which are very effective for non Gaussian data such as hyperspectral images. The criterion to be minimized is composed of three mutual information terms. The first and second terms, consider the maximal relevance and minimal redundancy, respectively, while the third term takes into account the segmentation map containing disjoint spatial regions. To obtain the segmentation map, we apply the firefly clustering algorithm whose fitness function simultaneously considers the intra-distance between samples and their cluster centroids, and inter-distance between centroids of any two clusters. Our experimental results, performed using a variety of hyperspectral scenes, indicate that the proposed framework gives better classification results than some state-of the-art spectral–spatial feature extraction methods.},
  archive      = {J_IJMLC},
  author       = {Shahdoosti, Hamid Reza and Tabatabaei, Zahra},
  doi          = {10.1007/s13042-019-01038-w},
  journal      = {International Journal of Machine Learning and Cybernetics},
  number       = {6},
  pages        = {1277-1291},
  shortjournal = {Int. J. Mach. Learn. Cybern.},
  title        = {Object-based feature extraction for hyperspectral data using firefly algorithm},
  volume       = {11},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Adaptive random-based self-organizing background subtraction
for moving detection. <em>IJMLC</em>, <em>11</em>(6), 1267–1276. (<a
href="https://doi.org/10.1007/s13042-019-01037-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The adaptability plays a significant role in moving detection. The diverse scenarios in real world still challenge this problem. Therefore, in this paper, we proposed an adaptive moving detection method, namely Adaptive Random-based Self-Organizing back- ground subtraction (ABSOBS) method. This method can adaptively extract the moving objects in various conditions and eliminate the “ghost” pixels simultaneously. Therefore, a robust initialization strategy is proposed to remove the noise pixels caused by the initialized frames. The proposed method uses a random- based scheme which allows the foreground pixels to up- date the neural network with a small probability. This strategy allows our algorithm to efficiently handle scene changes. Moreover, a foreground filter based on random rule is designed to eliminate the “ghost” pixel. More importantly, ABSOBS adopts a regulator to control the updating rate in different conditions. It makes our method easy-to-used and need not to set the parameters manually. The experiment results on various scenarios show that our method improves the detection accuracy for the SOBS and outperforms other state-of- the-art methods.},
  archive      = {J_IJMLC},
  author       = {Lu, Shan and Ma, Xianmin},
  doi          = {10.1007/s13042-019-01037-x},
  journal      = {International Journal of Machine Learning and Cybernetics},
  number       = {6},
  pages        = {1267-1276},
  shortjournal = {Int. J. Mach. Learn. Cybern.},
  title        = {Adaptive random-based self-organizing background subtraction for moving detection},
  volume       = {11},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A self-adaptive preference model based on dynamic feature
analysis for interactive portfolio optimization. <em>IJMLC</em>,
<em>11</em>(6), 1253–1266. (<a
href="https://doi.org/10.1007/s13042-019-01036-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In financial markets, there are various assets to invest in. Recognizing an investor’s preferences is key to selecting a combination of assets that best serves his or her needs. Considering the mean–variance model for the portfolio optimization problem, this paper proposes an interactive multicriteria decision-making method and explores a self-adaptive preference model based on dynamic feature analysis (denoted RFFS-DT) to capture the decision maker (DM)’s complex preferences in the decision-making process. RFFS-DT recognizes the DM’s preference impact factor and constructs a preference model. To recognize the impact factors of the DM’s preferences, which could change during the decision-making process, three categories of possible features involved in three aspects of the mean–variance model are defined, and a feature selection method based on random forest is designed. Because the DM’s preference structure could be unknown a priori, a decision-tree-based preference model is built and updated adaptively according to the DM’s preference feedback and the selected features. The effectiveness of RFFS-DT for interactive multicriteria decision making is verified by a series of deliberately designed comparative experiments.},
  archive      = {J_IJMLC},
  author       = {Hu, Shicheng and Li, Fang and Liu, Yang and Wang, Song},
  doi          = {10.1007/s13042-019-01036-y},
  journal      = {International Journal of Machine Learning and Cybernetics},
  number       = {6},
  pages        = {1253-1266},
  shortjournal = {Int. J. Mach. Learn. Cybern.},
  title        = {A self-adaptive preference model based on dynamic feature analysis for interactive portfolio optimization},
  volume       = {11},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Sparse graphs using global and local smoothness constraints.
<em>IJMLC</em>, <em>11</em>(6), 1241–1251. (<a
href="https://doi.org/10.1007/s13042-019-01035-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph-based learning methods are very useful for many machine learning approaches and classification tasks. Constructing an informative graph is one of the most important steps since a graph can significantly affect the final performance of the learning algorithms. Sparse representation is a useful tool in machine learning and pattern recognition area. Recently, it was shown that sparse graphs (sparse representation based graphs) provide a powerful approach to graph-based semi-supervised classification. In this paper, we introduce a new graph construction method that simultaneously provides a sparse graph and integrates manifold constraints on the sparse coefficients without any prior knowledge on the graph or on its similarity matrix. Furthermore, we propose an efficient solution to the optimization problem. The proposed method imposes that the sparse coding vectors of similar samples should be also similar. Different from existing graph construction methods that are based on the use of explicit constraints or a predefined graph matrix, the proposed smoothness constraints on the graph weights implicitly adapt data to the global structure of the estimated graph. A series of experiments conducted on several public image databases shows that the proposed method can outperform many state-of-the-art methods when applied to the problem of graph-based label propagation.},
  archive      = {J_IJMLC},
  author       = {Dornaika, F.},
  doi          = {10.1007/s13042-019-01035-z},
  journal      = {International Journal of Machine Learning and Cybernetics},
  number       = {6},
  pages        = {1241-1251},
  shortjournal = {Int. J. Mach. Learn. Cybern.},
  title        = {Sparse graphs using global and local smoothness constraints},
  volume       = {11},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Drug sensitivity prediction framework using ensemble and
multi-task learning. <em>IJMLC</em>, <em>11</em>(6), 1231–1240. (<a
href="https://doi.org/10.1007/s13042-019-01034-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Radiation and hormone level targeted drug therapy are one of the most widely adopted treatment options for different types of cancer. But, due to genetic variations, cancer patients shows heterogeneity towards targeted drug therapies. In such a scenario precision medication necessitates the design of targeted drug therapy for each individual based on their genetic structure. Predictive modeling and drug response data of cancer cells are imperative in designing personalized cancer treatment. Recent advancement in cancer research has produced various pharmacogenomic databases, which further encourages ongoing research in precision medication. In this paper, we have proposed the drug sensitivity prediction framework using ensemble and multi-task learning. The proposed framework successfully maps non-linear relationships among anti-cancer drugs and have modeled their dependency. Further, the proposed framework is validated using publicly available real data sets-GDSC, CCLE, NCI-Dream. The proposed ensemble model shows quite promise in predicting anti-cancer drug response and has achieved lesser mean square error 3.28 (CGP), 0.49 (CCLE) and 0.54 (NCI-DREAM) in comparison to other existing counterparts.},
  archive      = {J_IJMLC},
  author       = {Sharma, Aman and Rani, Rinkle},
  doi          = {10.1007/s13042-019-01034-0},
  journal      = {International Journal of Machine Learning and Cybernetics},
  number       = {6},
  pages        = {1231-1240},
  shortjournal = {Int. J. Mach. Learn. Cybern.},
  title        = {Drug sensitivity prediction framework using ensemble and multi-task learning},
  volume       = {11},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Integrating MTS with bagging strategy for class imbalance
problems. <em>IJMLC</em>, <em>11</em>(6), 1217–1230. (<a
href="https://doi.org/10.1007/s13042-019-01033-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Class imbalance is a common problem in classification tasks. The learning schemes of most classification algorithms tend to optimize the overall accuracy, and thus, identification of important but rarely occurring examples is ignored. The Mahalanobis–Taguchi system (MTS) has been shown to be robust in addressing class imbalance problems owing to its inherent properties of classification model construction. The bagging learning approach often has been applied as a superior strategy to reduce the learning bias of classification algorithms. In this study, we propose MTSbag, which integrates the MTS and the bagging-based ensemble learning approaches to enhance the ability of conventional MTS in handling imbalanced data. We perform numerical experiments involving multiple datasets with various class imbalance levels to demonstrate the effectiveness of MTSbag, especially for datasets with high imbalance levels. Finally, as a healthcare application, an early warning system for in-hospital cardiac arrest, was successfully implemented by leveraging the minority class identification ability of MTSbag.},
  archive      = {J_IJMLC},
  author       = {Hsiao, Yu-Hsiang and Su, Chao-Ton and Fu, Pin-Cheng},
  doi          = {10.1007/s13042-019-01033-1},
  journal      = {International Journal of Machine Learning and Cybernetics},
  number       = {6},
  pages        = {1217-1230},
  shortjournal = {Int. J. Mach. Learn. Cybern.},
  title        = {Integrating MTS with bagging strategy for class imbalance problems},
  volume       = {11},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A global and local feature weighted method for ancient
murals inpainting. <em>IJMLC</em>, <em>11</em>(6), 1197–1216. (<a
href="https://doi.org/10.1007/s13042-019-01032-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ancient murals have been haunted by various problems such as color fading, surface layer turning crisp and even large-area peeling off. Virtually inpainting technologies are widely used to restore these damages. In general, when structure information are blurred or completely missing within a large region, the image inpainting would be more thorny. In this paper, we study mural image inpainting by incorporating structure information collected from the limners guidance or the line drawings, and propose a global and local feature weighted method based on structure guidance to repair the damaged murals of Yulin Grottoes and Mogao Grottoes, Gansu. Unlike traditional methods, a novel sparse representation model with elastic net regularization based on similarity-preserving overcomplete dictionary is formulated to enhance the global feature consistency, and then an estimated method of neighborhood similarity is presented to guarantee local feature consistency, finally, we apply a global feature patch and local feature patch weighted method to obtain the target patch. Experimental results on damaged murals demonstrate the proposed method outperforms state-of-the-art inpainting methods.},
  archive      = {J_IJMLC},
  author       = {Wang, Huan and Li, Qingquan and Jia, Sen},
  doi          = {10.1007/s13042-019-01032-2},
  journal      = {International Journal of Machine Learning and Cybernetics},
  number       = {6},
  pages        = {1197-1216},
  shortjournal = {Int. J. Mach. Learn. Cybern.},
  title        = {A global and local feature weighted method for ancient murals inpainting},
  volume       = {11},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Mk-NNG-DPC: Density peaks clustering based on improved
mutual k-nearest-neighbor graph. <em>IJMLC</em>, <em>11</em>(6),
1179–1195. (<a
href="https://doi.org/10.1007/s13042-019-01031-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Clustering by fast search and detection of density peaks (DPC, Density Peaks Clustering) is a relatively novel clustering algorithm published in the Science journal. As a density-based clustering algorithm, DPC produces better clustering results while using less parameters than other relevant algorithms. However, we found that the DPC algorithm does not perform well if clusters with different densities are very close. To address this problem, we propose a new DPC algorithm by incorporating an improved mutual k-nearest-neighbor graph (Mk-NNG) into DPC. Our Mk-NNG-DPC algorithm leverages the distance matrix of data samples to improve the Mk-NNG, and then utilizes DPC to constrain and select cluster centers. The proposed Mk-NNG-DPC algorithm ensures an instance to be allocated to the fittest cluster. Experimental results on synthetic and real world datasets show that our Mk-NNG-DPC algorithm can effectively and efficiently improve clustering performance, even for clusters with arbitrary shapes.},
  archive      = {J_IJMLC},
  author       = {Fan, Jian-cong and Jia, Pei-ling and Ge, Linqiang},
  doi          = {10.1007/s13042-019-01031-3},
  journal      = {International Journal of Machine Learning and Cybernetics},
  number       = {6},
  pages        = {1179-1195},
  shortjournal = {Int. J. Mach. Learn. Cybern.},
  title        = {Mk-NNG-DPC: Density peaks clustering based on improved mutual K-nearest-neighbor graph},
  volume       = {11},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Data mining and machine learning approaches for prediction
modelling of schistosomiasis disease vectors. <em>IJMLC</em>,
<em>11</em>(6), 1159–1178. (<a
href="https://doi.org/10.1007/s13042-019-01029-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This research presents viable solutions for prediction modelling of schistosomiasis disease based on vector density. Novel training models proposed in this work aim to address various aspects of interest in the artificial intelligence applications domain. Topics discussed include data imputation, semi-supervised labelling and synthetic instance simulation when using sparse training data. Innovative semi-supervised ensemble learning paradigms are proposed focusing on labelling threshold selection and stringency of classification confidence levels. A regression-correlation combination (RCC) data imputation method is also introduced for handling of partially complete training data. Results presented in this work show data imputation precision improvement over benchmark value replacement using proposed RCC on 70\% of test cases. Proposed novel incremental transductive models such as ITSVM have provided interesting findings based on threshold constraints outperforming standard SVM application on 21\% of test cases and can be applied with alternative environment-based epidemic disease domains. The proposed incremental transductive ensemble approach model enables the combination of complimentary algorithms to provide labelling for unlabelled vector density instances. Liberal (LTA) and strict training approaches provided varied results with LTA outperforming Stacking ensemble on 29.1\% of test cases. Proposed novel synthetic minority over-sampling technique (SMOTE) equilibrium approach has yielded subtle classification performance increases which can be further interrogated to assess classification performance and efficiency relationships with synthetic instance generation.},
  archive      = {J_IJMLC},
  author       = {Fusco, Terence and Bi, Yaxin and Wang, Haiying and Browne, Fiona},
  doi          = {10.1007/s13042-019-01029-x},
  journal      = {International Journal of Machine Learning and Cybernetics},
  number       = {6},
  pages        = {1159-1178},
  shortjournal = {Int. J. Mach. Learn. Cybern.},
  title        = {Data mining and machine learning approaches for prediction modelling of schistosomiasis disease vectors},
  volume       = {11},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Knowledge granularity based incremental attribute reduction
for incomplete decision systems. <em>IJMLC</em>, <em>11</em>(5),
1141–1157. (<a
href="https://doi.org/10.1007/s13042-020-01089-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Attribute reduction is an important application of rough set theory. With the dynamic changes of data becoming more and more common, traditional attribute reduction, also called static attribute reduction, is no longer efficient. How to update attribute reducts efficiently gets more and more attention. In the light of the variation about the number of objects, we focus on incremental attribute reduction approaches based on knowledge granularity which can be used to measure the uncertainty in incomplete decision systems. We first introduce incremental mechanisms to calculate knowledge granularity for incomplete decision systems when multiple objects vary dynamically. Then, incremental attribute reduction algorithms for incomplete decision systems when adding multiple objects and when deleting multiple objects are proposed respectively. Finally, comparative experiments on different real-life data sets are conducted to demonstrate the effectiveness and efficiency of the proposed incremental algorithms for updating attribute reducts with the variation of multiple objects in incomplete decision systems.},
  archive      = {J_IJMLC},
  author       = {Zhang, Chucai and Dai, Jianhua and Chen, Jiaolong},
  doi          = {10.1007/s13042-020-01089-4},
  journal      = {International Journal of Machine Learning and Cybernetics},
  number       = {5},
  pages        = {1141-1157},
  shortjournal = {Int. J. Mach. Learn. Cybern.},
  title        = {Knowledge granularity based incremental attribute reduction for incomplete decision systems},
  volume       = {11},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A co-training approach for sequential three-way decisions.
<em>IJMLC</em>, <em>11</em>(5), 1129–1139. (<a
href="https://doi.org/10.1007/s13042-020-01086-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, three-way decisions have received much attention in uncertain decision and cost-sensitive learning communities. However, in many real applications, labeled samples are usually far from sufficient. In this case, it is a reasonable choice to defer the decision rather than make an immediate decision without sufficient supported information, thus it constructs a boundary region. In order to label more available samples, a traditional co-training method employs two classifiers on two complementary views to extend the existing training sets. However, the wrong predictions of new labels may lead to a high misclassification cost, especially when few labeled samples are available. To address this problem, a co-training method is incorporated into three-way decisions, which can label new samples with higher confidence. When we obtain sufficient labeled samples, the non-commitment decisions are directly decided to a positive or a negative region, which finally generates a two-way decisions result. Experiments on several face databases are conducted to validate the effectiveness of the proposed approach.},
  archive      = {J_IJMLC},
  author       = {Dai, Di and Li, Huaxiong and Jia, Xiuyi and Zhou, Xianzhong and Huang, Bing and Liang, Sunning},
  doi          = {10.1007/s13042-020-01086-7},
  journal      = {International Journal of Machine Learning and Cybernetics},
  number       = {5},
  pages        = {1129-1139},
  shortjournal = {Int. J. Mach. Learn. Cybern.},
  title        = {A co-training approach for sequential three-way decisions},
  volume       = {11},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Discernible neighborhood counting based incremental feature
selection for heterogeneous data. <em>IJMLC</em>, <em>11</em>(5),
1115–1127. (<a
href="https://doi.org/10.1007/s13042-019-00997-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Incremental feature selection refreshes a subset of information-rich features from added-in samples without forgetting the previously learned knowledge. However, most existing algorithms for incremental feature selection have no explicit mechanisms to handle heterogeneous data with symbolic and real-valued features. Therefore, this paper presents an incremental feature selection method for heterogeneous data with the sequential arrival of samples in group. Discernible neighborhood counting that measures different types of features, is first introduced to establish a framework for feature selection from heterogeneous data. With the arrival of new samples, the discernible neighborhood counting of a feature subset is then updated to reveal the incremental feature selection scheme. This scheme determines the criterion for efficiently adding informative features and deleting redundant features. Based on the incremental scheme, our incremental feature selection algorithm is further formulated to select valuable features from heterogeneous data. Extensive experiments are finally conducted to demonstrate the effectiveness and the efficiency of the proposed incremental feature selection algorithm.},
  archive      = {J_IJMLC},
  author       = {Yang, Yanyan and Song, Shiji and Chen, Degang and Zhang, Xiao},
  doi          = {10.1007/s13042-019-00997-4},
  journal      = {International Journal of Machine Learning and Cybernetics},
  number       = {5},
  pages        = {1115-1127},
  shortjournal = {Int. J. Mach. Learn. Cybern.},
  title        = {Discernible neighborhood counting based incremental feature selection for heterogeneous data},
  volume       = {11},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Optimal scale selection by integrating uncertainty and
cost-sensitive learning in multi-scale decision tables. <em>IJMLC</em>,
<em>11</em>(5), 1095–1114. (<a
href="https://doi.org/10.1007/s13042-020-01101-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Optimal scale selection is an important issue in the study of multi-scale decision tables. Most existing optimal scale selection methods have been designed from the perspective of consistency or uncertainty, and cost as well as user requirements or preferences in practical applications has not been considered. It is well known that the uncertainty of decision making in different levels of scale varies in sequential three-way decision models. Furthermore, test cost depends on the scale, and delayed decisions may cause delay cost. In practical applications, both uncertainty and cost are supposed to be considered. Therefore, it is worthwhile to introduce cost-sensitive learning into multi-scale decision tables and select the optimal scale by comprehensively considering uncertainty and cost. In this study, uncertainty is firstly quantified, and a novel cost constitution is defined in sequential three-way decision models. In addition, a multi-scale decision information system based on test cost and delay cost is proposed. Then, to obtain the optimal scale with the minimum uncertainty and cost, an optimal scale selection model is established with the constraint of user requirements. Furthermore, an improved optimal scale selection model considering user preferences is proposed by introducing the ideal solution to resolve conflicts among objectives. Finally, the effectiveness of the optimal scale selection model is verified through experiments, and a comparative experimental analysis demonstrates that the proposed model is more consistent with actual user requirements than existing models.},
  archive      = {J_IJMLC},
  author       = {Zhang, Xueqiu and Zhang, Qinghua and Cheng, Yunlong and Wang, Guoyin},
  doi          = {10.1007/s13042-020-01101-x},
  journal      = {International Journal of Machine Learning and Cybernetics},
  number       = {5},
  pages        = {1095-1114},
  shortjournal = {Int. J. Mach. Learn. Cybern.},
  title        = {Optimal scale selection by integrating uncertainty and cost-sensitive learning in multi-scale decision tables},
  volume       = {11},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A general conflict analysis model based on three-way
decision. <em>IJMLC</em>, <em>11</em>(5), 1083–1094. (<a
href="https://doi.org/10.1007/s13042-020-01100-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In a three-valued situation table, researchers have investigated the relationship between two agents by using auxiliary and distance functions, and proposed several models of conflict analysis for trisecting the set of all pairs of agents. But we have not observed a unification of these conflict analysis models. In this paper, we introduce the concepts of conditional alliance and conflict evaluations of two agents towards a subset of issues. In the framework of three-way decision, we propose a general model of three-way conflict analysis, and study three levels of conflict between two agents. We illustrate that five models of conflict analysis for trisecting the set of all pairs of agents are special cases of the general model, and provide a deep insight into the models of conflict analysis based on three-way decision.},
  archive      = {J_IJMLC},
  author       = {Lang, Guangming},
  doi          = {10.1007/s13042-020-01100-y},
  journal      = {International Journal of Machine Learning and Cybernetics},
  number       = {5},
  pages        = {1083-1094},
  shortjournal = {Int. J. Mach. Learn. Cybern.},
  title        = {A general conflict analysis model based on three-way decision},
  volume       = {11},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Using single axioms to characterize l-rough approximate
operators with respect to various types of l-relations. <em>IJMLC</em>,
<em>11</em>(5), 1061–1082. (<a
href="https://doi.org/10.1007/s13042-019-01051-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Considering L being a complete Heyting algebra, this paper mainly proposes a general framework of L-rough approximate operators in which constructive and axiomatic approaches are used. In the constructive approach, upper and lower L-rough approximate operators are introduced and their connections with L-relations are investigated. In the axiomatic approach, various types of set-theoretic L-operators are defined. It is shown that each type of L-rough approximate operators corresponding to special kind of L-relations, including serial, reflexive, symmetric, transitive, mediate, Euclidean and adjoint L-relations as well as their compositions, can be characterized by single axioms.},
  archive      = {J_IJMLC},
  author       = {Pang, Bin and Mi, Ju-Sheng},
  doi          = {10.1007/s13042-019-01051-z},
  journal      = {International Journal of Machine Learning and Cybernetics},
  number       = {5},
  pages        = {1061-1082},
  shortjournal = {Int. J. Mach. Learn. Cybern.},
  title        = {Using single axioms to characterize L-rough approximate operators with respect to various types of L-relations},
  volume       = {11},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Similarity-based attribute reduction in rough set theory: A
clustering perspective. <em>IJMLC</em>, <em>11</em>(5), 1047–1060. (<a
href="https://doi.org/10.1007/s13042-019-00959-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Attribute reduction is one of the most important research issues in the rough set theory. The purpose of attribute reduction is to find a minimal attribute subset that satisfies some specific criteria, while the minimal attribute subset is called attribute reduct. In this paper, we define a similarity-based attribute reduct based on a clustering perspective. Each decision class is treated as a cluster, and the defined similarity-based attribute reduct can maintain or increase the discriminating ability of different clusters in the case of removing redundant attributes. In view of this, firstly, we define the intra-class similarity for objects in the same decision class and the inter-class similarity for objects between different decision classes. Secondly, we define a similarity-based attribute reduct by maximizing intra-class similarity and minimizing inter-class similarity in the rough set model. Thirdly, by considering the heuristic search strategy, we also design a corresponding reduction method for the proposed attribute reduct. The experimental results indicate that compared with other representative attribute reducts, our proposed attribute reduct can significantly improve the classification performance.},
  archive      = {J_IJMLC},
  author       = {Jia, Xiuyi and Rao, Ya and Shang, Lin and Li, Tongjun},
  doi          = {10.1007/s13042-019-00959-w},
  journal      = {International Journal of Machine Learning and Cybernetics},
  number       = {5},
  pages        = {1047-1060},
  shortjournal = {Int. J. Mach. Learn. Cybern.},
  title        = {Similarity-based attribute reduction in rough set theory: A clustering perspective},
  volume       = {11},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Three-way active learning through clustering selection.
<em>IJMLC</em>, <em>11</em>(5), 1033–1046. (<a
href="https://doi.org/10.1007/s13042-020-01099-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In clustering-based active learning, the performance of the learner relies heavily on the quality of clustering results. Empirical studies have shown that different clustering techniques are applicable to different data. In this paper, we propose the three-way active learning through clustering selection (TACS) algorithm to dynamically select the appropriate techniques during the learning process. The algorithm follows the coarse-to-fine scheme of granular computing coupled with three-way instance processing. For label query, we select both representative instances with density peaks, and informative instances with the maximal total distance. For block partition, we revise six popular clustering techniques to speed up learning and accommodate binary splitting. For clustering evaluation, we define weighted entropy with 1-nearest-neighbor. For insufficient labels, we design tree pruning techniques with the use of a block queue. Experiments are undertaken on twelve UCI datasets. The results show that TACS is superior to single clustering technique based algorithms and other state-of-the-art active learning algorithms.},
  archive      = {J_IJMLC},
  author       = {Min, Fan and Zhang, Shi-Ming and Ciucci, Davide and Wang, Min},
  doi          = {10.1007/s13042-020-01099-2},
  journal      = {International Journal of Machine Learning and Cybernetics},
  number       = {5},
  pages        = {1033-1046},
  shortjournal = {Int. J. Mach. Learn. Cybern.},
  title        = {Three-way active learning through clustering selection},
  volume       = {11},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). The construction of attribute (object)-oriented
multi-granularity concept lattices. <em>IJMLC</em>, <em>11</em>(5),
1017–1032. (<a
href="https://doi.org/10.1007/s13042-019-00955-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {How to reduce the complexity of lattice construction is an important research topic in formal concept analysis. Based on granularity tree, the relationship between the extent and the intent of the attribute (object)-oriented concept before and after granularity transformation are investigated. Then, zoom algorithms for attribute (object)-oriented concept lattices are proposed. Specifically, zoom-in algorithm is applied to change the attribute granularity from coarse-granularity to fine-granularity, and zoom-out algorithm achieves changing the attribute granularity from fine-granularity to coarse-granularity. Zoom algorithms deal with the problems of fast construction of the attribute (object)-oriented multi-granularity concept lattices. By using zoom algorithms, the attribute (object)-oriented concept lattice based on different attribute granularity can be directly generated through the existing attribute (object)-oriented concept lattice. The proposed algorithms not only reduce the computational complexity of concept lattice construction, but also facilitate further data mining and knowledge discovery in formal contexts. Furthermore, the transformation algorithms among three kinds of concept lattice are proposed.},
  archive      = {J_IJMLC},
  author       = {Shao, Ming-Wen and Lv, Meng-Meng and Li, Ken-Wen and Wang, Chang-Zhong},
  doi          = {10.1007/s13042-019-00955-0},
  journal      = {International Journal of Machine Learning and Cybernetics},
  number       = {5},
  pages        = {1017-1032},
  shortjournal = {Int. J. Mach. Learn. Cybern.},
  title        = {The construction of attribute (object)-oriented multi-granularity concept lattices},
  volume       = {11},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). An efficient three-way clustering algorithm based on
gravitational search. <em>IJMLC</em>, <em>11</em>(5), 1003–1016. (<a
href="https://doi.org/10.1007/s13042-019-00988-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {There are three types of relationships between an object and a cluster, namely, belong-to definitely, uncertain and not belong-to definitely. Most of the existing clustering algorithms represent a cluster with a single set and they are the two-way clustering algorithms since they just reflect two relationships. By contrast, the three-way clustering can reflect intuitively the three types of relationships with a pair of sets. However, the three-way clustering algorithms usually need to know the thresholds in advance in order to obtain the three types of relationships. To address the problem, we propose an efficient three-way clustering algorithm based on the idea of universal gravitation in this paper. The proposed method can adjust the thresholds automatically in the process of clustering and obtain more detailed ascription relation between objects and clusters. Furthermore, to guarantee the integrity of the work, we also put forward a two-way clustering algorithm to obtain the conventional two-way result. The experimental results show that the proposed algorithm is not only effective to obtain the three-way clustering result from the two-way clustering result automatically, but also it is in a better performance at the accuracy, F-measure, NMI and RI than the compared algorithms in most cases.},
  archive      = {J_IJMLC},
  author       = {Yu, Hong and Chang, Zhihua and Wang, Guoyin and Chen, Xiaofang},
  doi          = {10.1007/s13042-019-00988-5},
  journal      = {International Journal of Machine Learning and Cybernetics},
  number       = {5},
  pages        = {1003-1016},
  shortjournal = {Int. J. Mach. Learn. Cybern.},
  title        = {An efficient three-way clustering algorithm based on gravitational search},
  volume       = {11},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Three-way decisions: Beyond rough sets and granular
computing. <em>IJMLC</em>, <em>11</em>(5), 989–1002. (<a
href="https://doi.org/10.1007/s13042-020-01095-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the fast developments of three-way decisions (3WD), this paper systematically summarizes the development track and evolution process of 3WD in recent decades. Firstly, the historical context, internal connections and relations between 3WD and rough sets are carefully investigated. Then, we discuss the methodology of 3WD via granular computing with “multi-level” strategy and “multi-view” strategy. Two novel 3WD generalized models with multilevel structure and multiview structure, as well as an integration framework of multilevel and multiview, are analyzed and discussed detailedly. Finally, this paper presents the research status and future research topics of 3WD.},
  archive      = {J_IJMLC},
  author       = {Liu, Dun and Yang, Xin and Li, Tianrui},
  doi          = {10.1007/s13042-020-01095-6},
  journal      = {International Journal of Machine Learning and Cybernetics},
  number       = {5},
  pages        = {989-1002},
  shortjournal = {Int. J. Mach. Learn. Cybern.},
  title        = {Three-way decisions: Beyond rough sets and granular computing},
  volume       = {11},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Complex network analysis of three-way decision researches.
<em>IJMLC</em>, <em>11</em>(5), 973–987. (<a
href="https://doi.org/10.1007/s13042-020-01082-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, complex networks are used to analyze the dataset of three-way decision articles published before December 18, 2019 and downloaded from ISI Web of Science. The scientific collaboration network, university collaboration network, networks of scientific papers (i.e., citation network, bibliographic coupling network, co-citation network) and keywords network are constructed to reveal the relationships between authors, affiliations, papers and keywords, respectively. Some interesting results are obtained and used to answer the following questions: (1) which authors play a key role in developing three-way decision; (2) which affiliations actively promote the development of three-way decision; (3) which papers are important or influential in the field of three-way decision; (4) what are the closely related research issues around three-way decision.},
  archive      = {J_IJMLC},
  author       = {Yang, Bo and Li, Jinhai},
  doi          = {10.1007/s13042-020-01082-x},
  journal      = {International Journal of Machine Learning and Cybernetics},
  number       = {5},
  pages        = {973-987},
  shortjournal = {Int. J. Mach. Learn. Cybern.},
  title        = {Complex network analysis of three-way decision researches},
  volume       = {11},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A comparison study of optimal scale combination selection in
generalized multi-scale decision tables. <em>IJMLC</em>, <em>11</em>(5),
961–972. (<a href="https://doi.org/10.1007/s13042-019-00954-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traditional rough set approach is mainly used to unravel rules from a decision table in which objects can possess a unique attribute-value. In a real world data set, for the same attribute objects are usually measured at different scales. The main objective of this paper is to study optimal scale combinations in generalized multi-scale decision tables. A generalized multi-scale information table is an attribute-value system in which different attributes are measured at different levels of scales. With the aim of investigating knowledge representation and knowledge acquisition in inconsistent generalized multi-scale decision tables, we first introduce the notion of scale combinations in a generalized multi-scale information table. We then formulate information granules with different scale combinations in multi-scale information systems and discuss their relationships. Furthermore, we define lower and upper approximations of sets with different scale combinations and examine their properties. Finally, we examine optimal scale combinations in inconsistent generalized multi-scale decision tables. We clarify relationships among different concepts of optimal scale combinations in inconsistent generalized multi-scale decision tables.},
  archive      = {J_IJMLC},
  author       = {Wu, Wei-Zhi and Leung, Yee},
  doi          = {10.1007/s13042-019-00954-1},
  journal      = {International Journal of Machine Learning and Cybernetics},
  number       = {5},
  pages        = {961-972},
  shortjournal = {Int. J. Mach. Learn. Cybern.},
  title        = {A comparison study of optimal scale combination selection in generalized multi-scale decision tables},
  volume       = {11},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Tri-level thinking: Models of three-way decision.
<em>IJMLC</em>, <em>11</em>(5), 947–959. (<a
href="https://doi.org/10.1007/s13042-019-01040-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The underlying philosophy of three-way decision is thinking in threes, namely, understanding and processing a whole through three distinct and related parts. One can formulate many concrete models of three-way decision to account for different interpretations of the three parts. By interpreting the three parts as three levels, this paper investigates tri-level thinking to build concrete models of three-way decision. We examine some fundamental issues and basic ingredients of tri-level thinking. In accordance with the data–information–knowledge–wisdom (DIKW) hierarchy, we present a perception–cognition–action (PCA) tri-level conceptual model that is applicable to studying intelligent data analytics, intelligent systems, and human understanding.},
  archive      = {J_IJMLC},
  author       = {Yao, Yiyu},
  doi          = {10.1007/s13042-019-01040-2},
  journal      = {International Journal of Machine Learning and Cybernetics},
  number       = {5},
  pages        = {947-959},
  shortjournal = {Int. J. Mach. Learn. Cybern.},
  title        = {Tri-level thinking: Models of three-way decision},
  volume       = {11},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). New advances in three-way decision, granular computing and
concept lattice. <em>IJMLC</em>, <em>11</em>(5), 945–946. (<a
href="https://doi.org/10.1007/s13042-020-01117-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_IJMLC},
  author       = {Wang, Xizhao and Li, Jinhai},
  doi          = {10.1007/s13042-020-01117-3},
  journal      = {International Journal of Machine Learning and Cybernetics},
  number       = {5},
  pages        = {945-946},
  shortjournal = {Int. J. Mach. Learn. Cybern.},
  title        = {New advances in three-way decision, granular computing and concept lattice},
  volume       = {11},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Robustness to adversarial examples can be improved with
overfitting. <em>IJMLC</em>, <em>11</em>(4), 935–944. (<a
href="https://doi.org/10.1007/s13042-020-01097-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning (henceforth DL) has become most powerful machine learning methodology. Under specific circumstances recognition rates even surpass those obtained by humans. Despite this, several works have shown that deep learning produces outputs that are very far from human responses when confronted with the same task. This the case of the so-called “adversarial examples” (henceforth AE). The fact that such implausible misclassifications exist points to a fundamental difference between machine and human learning. This paper focuses on the possible causes of this intriguing phenomenon. We first argue that the error in adversarial examples is caused by high bias, i.e. by regularization that has local negative effects. This idea is supported by our experiments in which the robustness to adversarial examples is measured with respect to the level of fitting to training samples. Higher fitting was associated to higher robustness to adversarial examples. This ties the phenomenon to the trade-off that exists in machine learning between fitting and generalization.},
  archive      = {J_IJMLC},
  author       = {Deniz, Oscar and Pedraza, Anibal and Vallez, Noelia and Salido, Jesus and Bueno, Gloria},
  doi          = {10.1007/s13042-020-01097-4},
  journal      = {International Journal of Machine Learning and Cybernetics},
  number       = {4},
  pages        = {935-944},
  shortjournal = {Int. J. Mach. Learn. Cybern.},
  title        = {Robustness to adversarial examples can be improved with overfitting},
  volume       = {11},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Emotion recognition using multimodal deep learning in
multiple psychophysiological signals and video. <em>IJMLC</em>,
<em>11</em>(4), 923–934. (<a
href="https://doi.org/10.1007/s13042-019-01056-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Emotion recognition has attracted great interest. Numerous emotion recognition approaches have been proposed, most of which focus on visual, acoustic or psychophysiological information individually. Although more recent research has considered multimodal approaches, individual modalities are often combined only by simple fusion or are directly fused with deep learning networks at the feature level. In this paper, we propose an approach to training several specialist networks that employs deep learning techniques to fuse the features of individual modalities. This approach includes a multimodal deep belief network (MDBN), which optimizes and fuses unified psychophysiological features derived from the features of multiple psychophysiological signals, a bimodal deep belief network (BDBN) that focuses on representative visual features among the features of a video stream, and another BDBN that focuses on the high multimodal features in the unified features obtained from two modalities. Experiments are conducted on the BioVid Emo DB database and 80.89\% accuracy is achieved, which outperforms the state-of-the-art approaches. The results demonstrate that the proposed approach can solve the problems of feature redundancy and lack of key features caused by multimodal fusion.},
  archive      = {J_IJMLC},
  author       = {Wang, Zhongmin and Zhou, Xiaoxiao and Wang, Wenlang and Liang, Chen},
  doi          = {10.1007/s13042-019-01056-8},
  journal      = {International Journal of Machine Learning and Cybernetics},
  number       = {4},
  pages        = {923-934},
  shortjournal = {Int. J. Mach. Learn. Cybern.},
  title        = {Emotion recognition using multimodal deep learning in multiple psychophysiological signals and video},
  volume       = {11},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). An adversarial non-volume preserving flow model with
boltzmann priors. <em>IJMLC</em>, <em>11</em>(4), 913–921. (<a
href="https://doi.org/10.1007/s13042-019-01048-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Flow-based generative models (flow models) are conceptually attractive due to tractability of the exact log-likelihood and the exact latent-variable inference. In order to generate sharper images and extend the Gaussian prior of Flow models to other discrete forms, we propose an adversarial non-volume preserving flow model with Boltzmann priors (ANVP) for modeling complex high-dimensional densities. In order to generate sharper images, an ANVP model introduces an adversarial regularizer into the loss function to penalize the condition that it places a high probability in regions where the training data distribution has a low density. Moreover, we show that the Gaussian prior can be extended to other forms such as the Boltzmann prior in the proposed ANVP model, and we use multi-scale transformations and Boltzmann priors to model the data distribution. The experiments show that proposed model is effective in image generation task.},
  archive      = {J_IJMLC},
  author       = {Zhang, Jian and Ding, Shifei and Jia, Weikuan},
  doi          = {10.1007/s13042-019-01048-8},
  journal      = {International Journal of Machine Learning and Cybernetics},
  number       = {4},
  pages        = {913-921},
  shortjournal = {Int. J. Mach. Learn. Cybern.},
  title        = {An adversarial non-volume preserving flow model with boltzmann priors},
  volume       = {11},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Pothole detection using location-aware convolutional neural
networks. <em>IJMLC</em>, <em>11</em>(4), 899–911. (<a
href="https://doi.org/10.1007/s13042-020-01078-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Poor road conditions, such as potholes, are a nuisance to society, which would annoy passengers, damage vehicles, and even cause accidents. Thus, detecting potholes is an important step toward pavement maintenance and rehabilitation to improve road conditions. Potholes have different shapes, scales, shadows, and illumination effects, and highly complicated backgrounds can be involved. Therefore, detection of potholes in road images is still a challenging task. In this study, we focus on pothole detection in 2D vision and present a new method to detect potholes based on location-aware convolutional neural networks, which focuses on the discriminative regions in the road instead of the global context. It consists of two main subnetworks: the first localization subnetwork employs a high recall network model to find as many candidate regions as possible, and the second part-based subnetwork performs classification on the candidates on which the network is expected to focus. The experiments using the public pothole dataset show that the proposed method could achieve high precision (95.2\%), recall (92.0\%) simultaneously, and outperform the most existing methods. The results also demonstrate that accurate part localization considerably increases classification performance while maintains high computational efficiency. The source code is available at https://github.com/hanshenchen/pothole-detection.},
  archive      = {J_IJMLC},
  author       = {Chen, Hanshen and Yao, Minghai and Gu, Qinlong},
  doi          = {10.1007/s13042-020-01078-7},
  journal      = {International Journal of Machine Learning and Cybernetics},
  number       = {4},
  pages        = {899-911},
  shortjournal = {Int. J. Mach. Learn. Cybern.},
  title        = {Pothole detection using location-aware convolutional neural networks},
  volume       = {11},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Weighted multi-deep ranking supervised hashing for efficient
image retrieval. <em>IJMLC</em>, <em>11</em>(4), 883–897. (<a
href="https://doi.org/10.1007/s13042-019-01026-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep hashing has proven to be efficient and effective for large-scale image retrieval due to the strong representation capability of deep networks. Existing deep hashing methods only utilize a single deep hash table. In order to achieve both higher retrieval recall and precision, longer hash codes can be used but at the expense of higher space usage. To address this issue, a novel deep hashing method is proposed in this paper, weighted multi-deep ranking supervised hashing (WMDRH), which employs multiple weighted deep hash tables to improve precision/recall without increasing space usage. The hash table is constructed as an additional layer in a deep network. Hash codes are generated by minimizing the loss function that contains two terms: (1) the ranking pairwise loss and (2) the classification loss. The ranking pairwise loss ensures to generate discriminative hash codes by penalizing more for the (dis)similar image pairs with (small)large Hamming distances. The classification loss guarantees the hash codes to be effective for category prediction. Different hash bits in each individual hash table are treated differently by assigning corresponding weights based on information preservation and bit diversity. Moreover, multiple hash tables are integrated by assigning the appropriate weight to each table according to its mean average precision (MAP) score for image retrieval. Experiments on three widely-used image databases show the proposed method outperforms state-of-the-art hashing methods.},
  archive      = {J_IJMLC},
  author       = {Li, Jiayong and Ng, Wing W. Y. and Tian, Xing and Kwong, Sam and Wang, Hui},
  doi          = {10.1007/s13042-019-01026-0},
  journal      = {International Journal of Machine Learning and Cybernetics},
  number       = {4},
  pages        = {883-897},
  shortjournal = {Int. J. Mach. Learn. Cybern.},
  title        = {Weighted multi-deep ranking supervised hashing for efficient image retrieval},
  volume       = {11},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Learning deep hierarchical and temporal recurrent neural
networks with residual learning. <em>IJMLC</em>, <em>11</em>(4),
873–882. (<a href="https://doi.org/10.1007/s13042-020-01063-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Learning both hierarchical and temporal dependencies can be crucial for recurrent neural networks (RNNs) to deeply understand sequences. To this end, a unified RNN framework is required that can ease the learning of both the deep hierarchical and temporal structures by allowing gradients to propagate back from both ends without being vanished. The residual learning (RL) has appeared as an effective and less-costly method to facilitate backward propagation of gradients. The significance of the RL is exclusively shown for learning deep hierarchical representations and temporal dependencies. Nevertheless, there is lack of efforts to unify these finding into a single framework for learning deep RNNs. In this study, we aim to prove that approximating identity mapping is crucial for optimizing both hierarchical and temporal structures. We propose a framework called hierarchical and temporal residual RNNs, to learn RNNs by approximating identity mappings across hierarchical and temporal structures. To validate the proposed method, we explore the efficacy of employing shortcut connections for training deep RNNs structures for sequence learning problems. Experiments are performed on Penn Treebank, Hutter Prize and IAM-OnDB datasets and results demonstrate the utility of the framework in terms of accuracy and computational complexity. We demonstrate that even for large datasets exploiting parameters for increasing network depth can gain computational benefits with reduced size of the RNN &quot;state&quot;.},
  archive      = {J_IJMLC},
  author       = {Zia, Tehseen and Abbas, Assad and Habib, Usman and Khan, Muhammad Sajid},
  doi          = {10.1007/s13042-020-01063-0},
  journal      = {International Journal of Machine Learning and Cybernetics},
  number       = {4},
  pages        = {873-882},
  shortjournal = {Int. J. Mach. Learn. Cybern.},
  title        = {Learning deep hierarchical and temporal recurrent neural networks with residual learning},
  volume       = {11},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Single image rain streaks removal: A review and an
exploration. <em>IJMLC</em>, <em>11</em>(4), 853–872. (<a
href="https://doi.org/10.1007/s13042-020-01061-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, rain streaks removal from a single image has attracted much research attention to alleviate the degenerated performance of computer vision tasks implemented on rainy images. In this paper, we provide a thorough review for current single-image-based rain removal techniques, which can be mainly categorized into three classes: early filter-based, conventional prior-based, and recent deep learning-based approaches. Furthermore, inspired by the rationality of current deep learning-based methods and insightful characteristics underlying rain shapes, we build a specific coarse-to-fine deraining network architecture, which can finely deliver the rain structures and progressively removes rain streaks from the input image, accordingly. The superiority of the proposed network is substantiated by experiments implemented on synthetic and real rainy images both visually and quantitatively, as compared with comprehensive state-of-the-art methods along this line. Especially, it is verified that the proposed network possesses better generalization capability on real rainy images, implying its potential usefulness for this task.},
  archive      = {J_IJMLC},
  author       = {Wang, Hong and Xie, Qi and Wu, Yichen and Zhao, Qian and Meng, Deyu},
  doi          = {10.1007/s13042-020-01061-2},
  journal      = {International Journal of Machine Learning and Cybernetics},
  number       = {4},
  pages        = {853-872},
  shortjournal = {Int. J. Mach. Learn. Cybern.},
  title        = {Single image rain streaks removal: A review and an exploration},
  volume       = {11},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). DeepSite: Bidirectional LSTM and CNN models for predicting
DNA–protein binding. <em>IJMLC</em>, <em>11</em>(4), 841–851. (<a
href="https://doi.org/10.1007/s13042-019-00990-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Transcription factors are cis-regulatory molecules that bind to specific sub-regions of DNA promoters and initiate transcription, the process that regulates the conversion of genetic information from DNA to RNA. Several computational methods have been developed to predict DNA–protein binding sites in DNA sequence using convolutional neural network (CNN). However, these techniques could indicate the dependency information of DNA sequence information in the framework of CNN. In addition, these methods are not accurate enough in prediction of the DNA–protein binding sites from the DNA sequence. In this study, we employ the bidirectional long short-term memory (BLSTM) and CNN to capture long-term dependencies between the sequence motifs in DNA, which is called DeepSite. Apart from traditional CNN, which includes six layers: input layer, BLSTM layer, CNN layer, pooling layer, full connection layer and output layer, DeepSite approach can predict DNA–protein binding sites with 87.12\% sensitivity, 91.06\% specificity, 89.19\% accuracy and 0.783 MCC, when tested on the 690 Chip-seq experiments from ENCODE. Lastly, we conclude that our proposed method can also be applied to find DNA–protein binding sites in different DNA sequences.},
  archive      = {J_IJMLC},
  author       = {Zhang, Yongqing and Qiao, Shaojie and Ji, Shengjie and Li, Yizhou},
  doi          = {10.1007/s13042-019-00990-x},
  journal      = {International Journal of Machine Learning and Cybernetics},
  number       = {4},
  pages        = {841-851},
  shortjournal = {Int. J. Mach. Learn. Cybern.},
  title        = {DeepSite: Bidirectional LSTM and CNN models for predicting DNA–protein binding},
  volume       = {11},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). DeepCascade-WR: A cascading deep architecture based on weak
results for time series prediction. <em>IJMLC</em>, <em>11</em>(4),
825–840. (<a href="https://doi.org/10.1007/s13042-019-00994-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Noisy and nonstationary real-world time series predictions (TSPs) are challenging tasks. Confronted with these challenging tasks, the predictive power of traditional shallow models is commonly not satisfactory enough. While the research on deep learning (DL) has made milestone breakthrough in recent years, and DL paradigm has gradually become indispensable for accomplishing these complex tasks. In this work, a cascading deep architecture based on weak results (DeepCascade-WR) is established, which possesses deep models’ marked capability of feature representation learning based on complex data. In DeepCascade-WR, weak prediction results are defined, innovating the forecasting mode of traditional TSP. The original data will be properly reconstituted with prior knowledge, generating attribute vectors with valid predictive information. DeepCascade-WR possesses online learning ability and effectively avoids the retraining problem, owing to the property of OS-ELM, one base model of DeepCascade-WR. Besides, ELM is exploited as another base model of DeepCascade-WR, therefore, DeepCascade-WR naturally inherits some valuable virtues from ELM, including faster training speed, better generalization ability and the avoidance of being fallen into local optima. Ultimately, in the empirical results, DeepCascade-WR demonstrates its superior predictive performance on five benchmark financial datasets, i.e., ^DJI, ^GSK, ^HSI, JOUT, and S&amp;P 500 Index, compared with its base learners and other state-of-the-art algorithms.},
  archive      = {J_IJMLC},
  author       = {Zhang, Chunyang and Dai, Qun and Song, Gang},
  doi          = {10.1007/s13042-019-00994-7},
  journal      = {International Journal of Machine Learning and Cybernetics},
  number       = {4},
  pages        = {825-840},
  shortjournal = {Int. J. Mach. Learn. Cybern.},
  title        = {DeepCascade-WR: A cascading deep architecture based on weak results for time series prediction},
  volume       = {11},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Cross-modal learning for material perception using deep
extreme learning machine. <em>IJMLC</em>, <em>11</em>(4), 813–823. (<a
href="https://doi.org/10.1007/s13042-019-00962-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The material property of an object’s surface is critical for the tasks of robotic manipulation or interaction with its surrounding environment. Tactile sensing can provide rich information about the material characteristics of an object’s surface. Hence, it is important to convey and interpret tactile information of material properties to the users during interaction. In this paper, we propose a visual-tactile cross-modal retrieval framework to convey tactile information of surface material for perceptual estimation. In particular, we use tactile information of a new unknown surface material to retrieve perceptually similar surface from an available surface visual sample set. For the proposed framework, we develop a deep cross-modal correlation learning method, which incorporates the high-level nonlinear representation of deep extreme learning machine and class-paired correlation learning of cluster canonical correlation analysis. Experimental results on the publicly available dataset validate the effectiveness of the proposed framework and the method.},
  archive      = {J_IJMLC},
  author       = {Zheng, Wendong and Liu, Huaping and Wang, Bowen and Sun, Fuchun},
  doi          = {10.1007/s13042-019-00962-1},
  journal      = {International Journal of Machine Learning and Cybernetics},
  number       = {4},
  pages        = {813-823},
  shortjournal = {Int. J. Mach. Learn. Cybern.},
  title        = {Cross-modal learning for material perception using deep extreme learning machine},
  volume       = {11},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A technical view on neural architecture search.
<em>IJMLC</em>, <em>11</em>(4), 795–811. (<a
href="https://doi.org/10.1007/s13042-020-01062-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to the discovery of innovative and practical neural architectures, deep learning has achieved bright successes in many fields, such as computer vision, natural language processing, recommendation systems, etc. To reach high performance, researchers have to adjust neural architectures and choose training tricks very carefully. The manual trial-and-error process for discovering the best neural network configuration consumes plenty of manpower. The neural architecture search (NAS) aims to alleviate this issue by automatically configuring neural networks. Recently, the rapid development of NAS has shown significant achievements. Novel neural network architectures that outperform the state-of-the-art handcrafted networks have been discovered in image classification benchmarks. In this paper, we survey NAS from a technical view. By summarizing the previous NAS approaches, we drew a picture of NAS for readers including problem definition, search approaches, progress towards practical applications and possible future directions. We hope that this paper can help beginners start their researches on NAS.},
  archive      = {J_IJMLC},
  author       = {Hu, Yi-Qi and Yu, Yang},
  doi          = {10.1007/s13042-020-01062-1},
  journal      = {International Journal of Machine Learning and Cybernetics},
  number       = {4},
  pages        = {795-811},
  shortjournal = {Int. J. Mach. Learn. Cybern.},
  title        = {A technical view on neural architecture search},
  volume       = {11},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A discriminative deep association learning for facial
expression recognition. <em>IJMLC</em>, <em>11</em>(4), 779–793. (<a
href="https://doi.org/10.1007/s13042-019-01024-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning based facial expression recognition becomes more successful in many applications. However, the lack of labeled data is still a bottleneck for better recognition performance. Thus, it is of practical significance to exploit the rich unlabeled data for training deep neural networks (DNNs). In this paper, we propose a novel discriminative deep association learning (DDAL) framework. The unlabeled data is provided to train the DNNs with the labeled data simultaneously, in a multi-loss deep network based on association learning. Moreover, the discrimination loss is also utilized to ensure intra-class clustering and inter-class centers separating. Furthermore, a large synthetic facial expression dataset is generated and used as unlabeled data. By exploiting association learning mechanism on two facial expression datasets, competitive results are obtained. By utilizing synthetic data, the performance is increased clearly.},
  archive      = {J_IJMLC},
  author       = {Jin, Xing and Sun, Wenyun and Jin, Zhong},
  doi          = {10.1007/s13042-019-01024-2},
  journal      = {International Journal of Machine Learning and Cybernetics},
  number       = {4},
  pages        = {779-793},
  shortjournal = {Int. J. Mach. Learn. Cybern.},
  title        = {A discriminative deep association learning for facial expression recognition},
  volume       = {11},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A deep neural networks based recommendation algorithm using
user and item basic data. <em>IJMLC</em>, <em>11</em>(4), 763–777. (<a
href="https://doi.org/10.1007/s13042-019-00981-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {User basic data (e.g. user gender, user age and user ID, etc.) and item basic data (e.g. item name, item category, etc.) are important side data that can be used to enhance the performance of recommendation algorithms, whereas attempts concerning this issue are still relatively scarce. In this study, a deep neural networks based recommendation algorithm is proposed where user average rating, user basic data (user gender, user age, user occupation, user ID), item basic data (item name, item category, item ID) and item average rating are used. The main idea of the algorithm is to build a regression model for predicting user ratings based on deep neural networks. For this, according to the user data (user average rating and user basic data) and the item data (items basic data and item average rating), a user feature matrix and an item feature matrix are respectively constructed using the four types of neural network layers [i.e., embedding layer (EL), convolution layer (CL), pooling layer (PL) and fully connected layer (FCL)]. Then, based on the obtained user feature matrix and item feature matrix, a user-item feature matrix is further constructed using a FCL. On this basis, a regression model for predicting user ratings can be trained, and a recommendation list can be generated according to the predicted user ratings. To verify the effectiveness of the proposed algorithm, three experiments are conducted using the real data from the MovieLens website. The results of experiments show that the proposed algorithm not only outperforms the state-of-the-art collaborative filtering (CF) recommendation algorithms but also alleviates the data sparsity problem and cold-start problem that would occur when the state-of-the-art CF recommendation algorithms are used.},
  archive      = {J_IJMLC},
  author       = {Bi, Jian-Wu and Liu, Yang and Fan, Zhi-Ping},
  doi          = {10.1007/s13042-019-00981-y},
  journal      = {International Journal of Machine Learning and Cybernetics},
  number       = {4},
  pages        = {763-777},
  shortjournal = {Int. J. Mach. Learn. Cybern.},
  title        = {A deep neural networks based recommendation algorithm using user and item basic data},
  volume       = {11},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Combination of loss functions for deep text classification.
<em>IJMLC</em>, <em>11</em>(4), 751–761. (<a
href="https://doi.org/10.1007/s13042-019-00982-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ensemble methods have shown to improve the results of statistical classifiers by combining multiple single learners into a strong one. In this paper, we explore the use of ensemble methods at the level of the objective function of a deep neural network. We propose a novel objective function that is a linear combination of single losses and integrate the proposed objective function into a deep neural network. By doing so, the weights associated with the linear combination of losses are learned by back propagation during the training stage. We study the impact of such an ensemble loss function on the state-of-the-art convolutional neural networks for text classification. We show the effectiveness of our approach through comprehensive experiments on text classification. The experimental results demonstrate a significant improvement compared with the conventional state-of-the-art methods in the literature.},
  archive      = {J_IJMLC},
  author       = {Hajiabadi, Hamideh and Molla-Aliod, Diego and Monsefi, Reza and Yazdi, Hadi Sadoghi},
  doi          = {10.1007/s13042-019-00982-x},
  journal      = {International Journal of Machine Learning and Cybernetics},
  number       = {4},
  pages        = {751-761},
  shortjournal = {Int. J. Mach. Learn. Cybern.},
  title        = {Combination of loss functions for deep text classification},
  volume       = {11},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Recent advances in deep learning. <em>IJMLC</em>,
<em>11</em>(4), 747–750. (<a
href="https://doi.org/10.1007/s13042-020-01096-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_IJMLC},
  author       = {Wang, Xizhao and Zhao, Yanxia and Pourpanah, Farhad},
  doi          = {10.1007/s13042-020-01096-5},
  journal      = {International Journal of Machine Learning and Cybernetics},
  number       = {4},
  pages        = {747-750},
  shortjournal = {Int. J. Mach. Learn. Cybern.},
  title        = {Recent advances in deep learning},
  volume       = {11},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Large-scale evolutionary optimization: A survey and
experimental comparative study. <em>IJMLC</em>, <em>11</em>(3), 729–745.
(<a href="https://doi.org/10.1007/s13042-019-01030-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the last decades, global optimization problems are very common in many research fields of science and engineering and lots of evolutionary computation algorithms have been used to deal with such problems, such as differential evolution (DE) and particle swarm optimization (PSO). However, the algorithms performance rapidly decreases as the increasement of the problem dimension. In order to solve large-scale global optimization problems more efficiently, a lot of improved evolutionary computation algorithms, especially the improved DE or improved PSO algorithms have been proposed. In this paper, we want to analyze the differences and characteristics of various large-scale evolutionary optimization (LSEO) algorithms on some benchmark functions. We adopt the CEC2010 and the CEC2013 large-scale optimization benchmark functions to compare the performance of seven well-known LSEO algorithms. Then, we try to figure out which algorithms perform better on different types of benchmark functions based on simulation results. Finally, we give some potential future research directions of LSEO algorithms and make a conclusion.},
  archive      = {J_IJMLC},
  author       = {Jian, Jun-Rong and Zhan, Zhi-Hui and Zhang, Jun},
  doi          = {10.1007/s13042-019-01030-4},
  journal      = {International Journal of Machine Learning and Cybernetics},
  number       = {3},
  pages        = {729-745},
  shortjournal = {Int. J. Mach. Learn. Cybern.},
  title        = {Large-scale evolutionary optimization: A survey and experimental comparative study},
  volume       = {11},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Bibliometric analysis of support vector machines research
trend: A case study in china. <em>IJMLC</em>, <em>11</em>(3), 715–728.
(<a href="https://doi.org/10.1007/s13042-019-01028-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Support vector machine (SVM) is a widely used algorithm in the field of machine learning, and it is a research hotspot in the field of data mining. In order to fully understand the historical progress and current situation of SVM researches, as well as its future development trend in China, this paper conducts a comprehensive bibliometric study based on the publications from web of science database by Chinese scholars in this field. First, this paper focuses on some of the basic characteristics of the research publications of SVM in China, including important journals, research institutions and countries/regions, most cited publications, and so on. Then, based on the knowledge mapping software VOSviewer, the cooperation between other countries and China as well as the cooperation between research institutions in China are explored. Finally, VOSviewer based bibliometric visualization graphics are used to identify the changes of the research hotspots in the SVM field. This paper provides a relatively broad perspective for the evaluation of SVM scientific researches, and reveals the development trend in this field.},
  archive      = {J_IJMLC},
  author       = {Yu, Dejian and Xu, Zeshui and Wang, Xizhao},
  doi          = {10.1007/s13042-019-01028-y},
  journal      = {International Journal of Machine Learning and Cybernetics},
  number       = {3},
  pages        = {715-728},
  shortjournal = {Int. J. Mach. Learn. Cybern.},
  title        = {Bibliometric analysis of support vector machines research trend: A case study in china},
  volume       = {11},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). An efficient clustering algorithm based on the k-nearest
neighbors with an indexing ratio. <em>IJMLC</em>, <em>11</em>(3),
675–714. (<a href="https://doi.org/10.1007/s13042-019-01027-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Clustering is a challenging problem that is commonly used for many applications. It aims at finding the similarity between data points and grouping similar ones into the same cluster. In this paper, we introduce a new clustering algorithm named Nearest Point with Indexing Ratio (NPIR). The algorithm tries to solve the clustering problem based on the nearest neighbor search technique by finding the nearest neighbors for the points that are already clustered based on the distance between them and cluster them accordingly. The algorithm does not consider all the nearest points at once to cluster a single point but iteratively considers only one nearest point based on an election operation using a distance vector. NPIR tries to solve some limitations of other clustering algorithms. It tries to cluster arbitrary shapes which have non-spherical clusters, clusters with unusual shapes, or clusters with different densities. NPIR is evaluated using 20 real and artificial data sets of different levels of complexity with different number of clusters and points. Results are compared with those obtained for other well-known and common clustering algorithms. The comparative study demonstrates that NPIR outperforms the other algorithms for the majority of the data sets in terms of different evaluation measures including Homogeneity Score, Completeness Score, V-measure, Adjusted Mutual Information, and Adjusted Rand Index. Furthermore, NPIR is experimented on a real-life application for segmenting mall customers for effective decision making. The source code of NPIR is available at http://evo-ml.com/2019/10/28/npir/.},
  archive      = {J_IJMLC},
  author       = {Qaddoura, Raneem and Faris, Hossam and Aljarah, Ibrahim},
  doi          = {10.1007/s13042-019-01027-z},
  journal      = {International Journal of Machine Learning and Cybernetics},
  number       = {3},
  pages        = {675-714},
  shortjournal = {Int. J. Mach. Learn. Cybern.},
  title        = {An efficient clustering algorithm based on the k-nearest neighbors with an indexing ratio},
  volume       = {11},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Three-way decisions in fuzzy incomplete information systems.
<em>IJMLC</em>, <em>11</em>(3), 667–674. (<a
href="https://doi.org/10.1007/s13042-019-01025-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The intuitionistic fuzzy set is introduced to fuzzy incomplete information systems, the membership and non-membership degrees that an object belongs to a concept are constructed based on the similarity relation. By combining the fuzzy rough set and intuitionistic fuzzy set, we make three-way decisions. Various situations in fuzzy incomplete information systems are discussed.},
  archive      = {J_IJMLC},
  author       = {Yang, Xiaoping and Li, Tongjun and Tan, Anhui},
  doi          = {10.1007/s13042-019-01025-1},
  journal      = {International Journal of Machine Learning and Cybernetics},
  number       = {3},
  pages        = {667-674},
  shortjournal = {Int. J. Mach. Learn. Cybern.},
  title        = {Three-way decisions in fuzzy incomplete information systems},
  volume       = {11},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Three-way decision based on decision-theoretic rough sets
with single-valued neutrosophic information. <em>IJMLC</em>,
<em>11</em>(3), 657–665. (<a
href="https://doi.org/10.1007/s13042-019-01023-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose two new three-way decision models based on decision-theoretic rough sets with single-valued neutrosophic information. These two models adopt different ranking methods, one of which is the ranking method based on cosine similarity measure. The other is the ranking method based on Euclidean distance. The key steps of the proposed models were shown by an algorithm and we present the breakfast shop siting problem to demonstrate the rationality and practicability of the proposed models. Finally, we make comparison analysis between the two models, and compare our models with other existing related models.},
  archive      = {J_IJMLC},
  author       = {Jiao, Li and Yang, Hai-Long and Li, Sheng-Gang},
  doi          = {10.1007/s13042-019-01023-3},
  journal      = {International Journal of Machine Learning and Cybernetics},
  number       = {3},
  pages        = {657-665},
  shortjournal = {Int. J. Mach. Learn. Cybern.},
  title        = {Three-way decision based on decision-theoretic rough sets with single-valued neutrosophic information},
  volume       = {11},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Granular matrix-based knowledge reductions of formal fuzzy
contexts. <em>IJMLC</em>, <em>11</em>(3), 643–656. (<a
href="https://doi.org/10.1007/s13042-019-01022-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Knowledge reduction is an important issue in formal fuzzy contexts, which can simplify the structure of concept lattices. In this paper, a novel granular matrix-based for knowledge reduction of crisp-fuzzy concept is investigated. Firstly, matrix representations of extents and intents of concepts are defined, respectively, which are used to characterize the join-irreducible elements and propose the corresponding algorithm. In this framework, granular consistent set and granular reduct are developed. Then the judgement theorem of reduction and its corresponding algorithm in formal fuzzy context are proposed. Furthermore, we generalize the matrix approach to formal fuzzy decision contexts. Finally, numerical experiments are conducted to evaluate the effectiveness of the proposed approaches. Our methods present a new framework for knowledge reduction in formal fuzzy contexts.},
  archive      = {J_IJMLC},
  author       = {Lin, Yidong and Li, Jinjin and Tan, Anhui and Zhang, Jia},
  doi          = {10.1007/s13042-019-01022-4},
  journal      = {International Journal of Machine Learning and Cybernetics},
  number       = {3},
  pages        = {643-656},
  shortjournal = {Int. J. Mach. Learn. Cybern.},
  title        = {Granular matrix-based knowledge reductions of formal fuzzy contexts},
  volume       = {11},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Multiple instance learning for sequence data with across bag
dependencies. <em>IJMLC</em>, <em>11</em>(3), 629–642. (<a
href="https://doi.org/10.1007/s13042-019-01021-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In Multiple Instance Learning (MIL) problem for sequence data, the instances inside the bags are sequences. In some real world applications such as bioinformatics, comparing a random couple of sequences makes no sense. In fact, each instance may have structural and/or functional relations with instances of other bags. Thus, the classification task should take into account this across bag relation. In this work, we present two novel MIL approaches for sequence data classification named ABClass and ABSim. ABClass extracts motifs from related instances and use them to encode sequences. A discriminative classifier is then applied to compute a partial classification result for each set of related sequences. ABSim uses a similarity measure to discriminate the related instances and to compute a scores matrix. For both approaches, an aggregation method is applied in order to generate the final classification result. We applied both approaches to solve the problem of bacterial Ionizing Radiation Resistance prediction. The experimental results of the presented approaches are satisfactory.},
  archive      = {J_IJMLC},
  author       = {Zoghlami, Manel and Aridhi, Sabeur and Maddouri, Mondher and Mephu Nguifo, Engelbert},
  doi          = {10.1007/s13042-019-01021-5},
  journal      = {International Journal of Machine Learning and Cybernetics},
  number       = {3},
  pages        = {629-642},
  shortjournal = {Int. J. Mach. Learn. Cybern.},
  title        = {Multiple instance learning for sequence data with across bag dependencies},
  volume       = {11},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Unsupervised learning of monocular depth and ego-motion with
space–temporal-centroid loss. <em>IJMLC</em>, <em>11</em>(3), 615–627.
(<a href="https://doi.org/10.1007/s13042-019-01020-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose DPCNN (Depth and Pose Convolutional Network), a novel framework for monocular depth with absolute scale and camera motion estimation from videos. DPCNN uses our proposed stereo training examples, in which the spatial and temporal images can be combined more closely, thus providing more priori constraint relationships. In addition, there are two significant features existing in DPCNN: One is that the entire space–temporal-centroid model is established to independently constrain the rotation matrix and the translation vector, so that the spatial and temporal images are collectively limited in a common, real-world scale. The other is to use the triangulation principle to establish a two-channel depth consistency loss, which penalizes inconsistency of the depths estimated from the spatial images and inconsecutive temporal images, respectively. Experiments on the KITTI datasets show that DPCNN achieves the most advanced results in both tasks and outperforms the current monocular methods.},
  archive      = {J_IJMLC},
  author       = {Zhang, Junning and Su, Qunxing and Liu, Pengyuan and Xu, Chao and Chen, Yanlong},
  doi          = {10.1007/s13042-019-01020-6},
  journal      = {International Journal of Machine Learning and Cybernetics},
  number       = {3},
  pages        = {615-627},
  shortjournal = {Int. J. Mach. Learn. Cybern.},
  title        = {Unsupervised learning of monocular depth and ego-motion with space–temporal-centroid loss},
  volume       = {11},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Flexible robust principal component analysis.
<em>IJMLC</em>, <em>11</em>(3), 603–613. (<a
href="https://doi.org/10.1007/s13042-019-00999-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The error correction problem is a very important topic in machine learning. However, existing methods only focus on data recovery and ignore data compact representation. In this paper, we propose a flexible robust principal component analysis (FRPCA) method in which two different matrices are used to perform error correction and the data compact representation can be obtained by using one of matrices. Moreover, FRPCA selects the most relevant features to guarantee that the recovered data can faithfully preserve the original data semantics. The learning is done by solving a nuclear-norm regularized minimization problem, which is convex and can be solved in polynomial time. Experiments were conducted on image sequences containing targets of interest in a variety of environments, e.g., offices, campuses. We also compare our method with existing method in recovering the face images from corruptions. Experimental results show that the proposed method achieves better performances and it is more practical than the existing approaches.},
  archive      = {J_IJMLC},
  author       = {He, Zinan and Wu, Jigang and Han, Na},
  doi          = {10.1007/s13042-019-00999-2},
  journal      = {International Journal of Machine Learning and Cybernetics},
  number       = {3},
  pages        = {603-613},
  shortjournal = {Int. J. Mach. Learn. Cybern.},
  title        = {Flexible robust principal component analysis},
  volume       = {11},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Feature selection based on rough set approach, wrapper
approach, and binary whale optimization algorithm. <em>IJMLC</em>,
<em>11</em>(3), 573–602. (<a
href="https://doi.org/10.1007/s13042-019-00996-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The principle of any approach for solving feature selection problem is to find a subset of the original features. Since finding a minimal subset of the features is an NP-hard problem, it is necessary to develop and propose practical and efficient heuristic algorithms. The whale optimization algorithm is a recently developed nature-inspired meta-heuristic optimization algorithm that imitates the hunting behavior of humpback whales to solve continuous optimization problems. In this paper, we propose a novel binary whale optimization algorithm (BWOA) to solve feature selection problem. BWOA is especially desirable and appealing for feature selection problem whenever there is no heuristic information that can lead the search to the optimal minimal subset. Nonetheless, whales can find the best features as they hunt the prey. Rough set theory (RST) is one of the effective algorithms for feature selection. We use RST with BWOA as the first experiment, and in the second experiment, we use a wrapper approach with BWOA on three different classifiers for feature selection. Also, we verify the performance and the effectiveness of the proposed algorithm by performing our experiments using 32 datasets from the UCI machine learning repository and comparing the proposed algorithm with some powerful existing algorithms in the literature. Furthermore, we employ two nonparametric statistical tests, Wilcoxon Signed-Rank test, and Friedman test, at 5\% significance level. Our results show that the proposed algorithm can provide an efficient tool to find a minimal subset of the features.},
  archive      = {J_IJMLC},
  author       = {Tawhid, Mohamed A. and Ibrahim, Abdelmonem M.},
  doi          = {10.1007/s13042-019-00996-5},
  journal      = {International Journal of Machine Learning and Cybernetics},
  number       = {3},
  pages        = {573-602},
  shortjournal = {Int. J. Mach. Learn. Cybern.},
  title        = {Feature selection based on rough set approach, wrapper approach, and binary whale optimization algorithm},
  volume       = {11},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A survey on analysis of human faces and facial expressions
datasets. <em>IJMLC</em>, <em>11</em>(3), 553–571. (<a
href="https://doi.org/10.1007/s13042-019-00995-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Facial expressions are the basic input for visual emotion detection. They are of great importance in computer vision society. In last decade, substantial amount of work has been done in the field of facial expressions datasets. This survey covers all of the publically available databases in detail and provides necessary information about these sets. This review delivers comprehensive support to researchers in selection of their desired dataset. The datasets are organized in decreasing order of their importance with respect to diversity in expressions, poses, number of images and resolution. This survey also provides comprehensive tabular comparison of different face based databases.},
  archive      = {J_IJMLC},
  author       = {Khan, Gulraiz and Samyan, Sahar and Khan, Muhammad Usman Ghani and Shahid, Muhammad and Wahla, Samyan Qayyum},
  doi          = {10.1007/s13042-019-00995-6},
  journal      = {International Journal of Machine Learning and Cybernetics},
  number       = {3},
  pages        = {553-571},
  shortjournal = {Int. J. Mach. Learn. Cybern.},
  title        = {A survey on analysis of human faces and facial expressions datasets},
  volume       = {11},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Improved fuzzy c-means algorithm based on density peak.
<em>IJMLC</em>, <em>11</em>(3), 545–552. (<a
href="https://doi.org/10.1007/s13042-019-00993-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fuzzy C-means (FCM) algorithm is a fuzzy clustering algorithm based on objective function compared with typical “hard clustering” such as k-means algorithm. FCM algorithm calculates the membership degree of each sample to all classes and obtain more reliable and accurate classification results. However, in the process of clustering, FCM algorithm needs to determine the number of clusters manually, and is sensitive to the initial clustering center. It is easy to generate problems such as multiple clustering iterations, slow convergence speed and local optimal solution. To address those problems, we propose to combine the FCM algorithm and DPC (Clustering by fast search and find of density peaks) algorithm. First, DPC algorithm is used to automatically select the center and number of clusters, and then FCM algorithm is used to realize clustering. The comparison experiments show that the improved FCM algorithm has a faster convergence speed and higher accuracy.},
  archive      = {J_IJMLC},
  author       = {Liu, Xiang-yi and Fan, Jian-cong and Chen, Zi-wen},
  doi          = {10.1007/s13042-019-00993-8},
  journal      = {International Journal of Machine Learning and Cybernetics},
  number       = {3},
  pages        = {545-552},
  shortjournal = {Int. J. Mach. Learn. Cybern.},
  title        = {Improved fuzzy C-means algorithm based on density peak},
  volume       = {11},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Multi-view local linear KNN classification: Theoretical and
experimental studies on image classification. <em>IJMLC</em>,
<em>11</em>(3), 525–543. (<a
href="https://doi.org/10.1007/s13042-019-00992-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {When handling special multi-view scenarios where data from each view keep the same features, we may perhaps encounter two serious challenges: (1) samples from different views of the same class are less similar than those from the same view but different class, which sometimes happen in local way in both training and/or testing phases; (2) training an explicit prediction model becomes unreliable and even infeasible for test samples in multi-view scenarios. In this study, we prefer the philosophy of the k nearest neighbor method (KNN) to circumvent the second challenge. Without an explicit prediction model trained directly from the above multi-view data, a new multi-view local linear k nearest neighbor method (MV-LLKNN) is then developed to circumvent the two challenges so as to predict the label of each test sample. MV-LLKNN has its two reliable assumptions. One is the theoretically and experimentally provable assumption that any test sample can be well approximated by a linear combination of its neighbors in the multi-view training dataset. The other assumes that these neighbors should demonstrate their clustering property according to certain commonality-based similarity measure between the multi-view test sample and these multi-view neighbors so as to avoid the first challenge. MV-LLKNN can realize its effective prediction for a test multi-view sample by cheaply using both on-hand fast iterative shrinkage thresholding algorithm (FISTA) and KNN. Our theoretical analysis and experimental results about real multi-view face datasets indicate the effectiveness of MV-LLKNN.},
  archive      = {J_IJMLC},
  author       = {Jiang, Zhibin and Bian, Zekang and Wang, Shitong},
  doi          = {10.1007/s13042-019-00992-9},
  journal      = {International Journal of Machine Learning and Cybernetics},
  number       = {3},
  pages        = {525-543},
  shortjournal = {Int. J. Mach. Learn. Cybern.},
  title        = {Multi-view local linear KNN classification: Theoretical and experimental studies on image classification},
  volume       = {11},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). An active multi-class classification using privileged
information and belief function. <em>IJMLC</em>, <em>11</em>(3),
511–524. (<a href="https://doi.org/10.1007/s13042-019-00991-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many classification models, based on support vector machine, have been designed so far to improve classification performance in both supervised and semi-supervised learning. One of the studies which is done in this case is about the use of privileged information that is hidden in training data. However, the challenge is how to find the privileged information. In most researches, experts have defined privileged information, but in this paper, it has been tried to automatically select a feature as privileged information and classify training data into several groups. This grouping has been used to correct the decision function of classifier. Moreover, the proposed classifier has been used in one-against-all (OAA) approach for semi-supervised datasets. To overcome uncertain areas in OAA, belief function and active learning techniques are applied to extract the most informative samples. The experimental results indicate the superiority of the proposed method among the other state-of-the-art methods in terms of classification accuracy.},
  archive      = {J_IJMLC},
  author       = {Javid, Mitra and Hamidzadeh, Javad},
  doi          = {10.1007/s13042-019-00991-w},
  journal      = {International Journal of Machine Learning and Cybernetics},
  number       = {3},
  pages        = {511-524},
  shortjournal = {Int. J. Mach. Learn. Cybern.},
  title        = {An active multi-class classification using privileged information and belief function},
  volume       = {11},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Clustering data with the presence of attribute noise: A
study of noise completely at random and ensemble of multiple k-means
clusterings. <em>IJMLC</em>, <em>11</em>(3), 491–509. (<a
href="https://doi.org/10.1007/s13042-019-00989-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In general practice, the perception of noise has been inevitably negative. Specific to data analytic, most of the existing techniques developed thus far comply with a noise-free assumption. Without an assistance of data pre-processing, it is hard for those models to discover reliable patterns. This is also true for k-means, one of the most well known algorithms for cluster analysis. Based on several works in the literature, they suggest that the ensemble approach can deliver accurate results from multiple clusterings of data with noise completely at random. Provided this motivation, the paper presents the study of using different consensus clustering techniques to analyze noisy data, with k-means being exploited as base clusterings. The empirical investigation reveals that the ensemble approach can be robust to low level of noise, while some exhibit improvement over the noise-free cases. This finding is in line with the recent published work that underlines the benefit of small noise to centroid-based clustering methods. In addition, the outcome of this research provides a guideline to analyzing a new data collection of uncertain quality level.},
  archive      = {J_IJMLC},
  author       = {Iam-On, Natthakan},
  doi          = {10.1007/s13042-019-00989-4},
  journal      = {International Journal of Machine Learning and Cybernetics},
  number       = {3},
  pages        = {491-509},
  shortjournal = {Int. J. Mach. Learn. Cybern.},
  title        = {Clustering data with the presence of attribute noise: A study of noise completely at random and ensemble of multiple k-means clusterings},
  volume       = {11},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Unlabelled text mining methods based on two extension models
of concept lattices. <em>IJMLC</em>, <em>11</em>(2), 475–490. (<a
href="https://doi.org/10.1007/s13042-019-00987-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Concept lattice is a useful tool for text extraction. The common text clustering method fails to generate hierarchical relationships among categories and realize soft clustering simultaneously, while the concept lattice ignores the negative correlation between an object subset and an attribute subset. Motivated by the problems, we propose unlabelled text mining methods based on fuzzy concept lattice and three-way concept lattice. Firstly, we excavate hierarchical text categories to construct a classification system based on fuzzy concept lattice, and the labelled samples are obtained by the word matching method. Then, we construct a three-way concept lattice to get positive and negative classification rules based on the labelled samples, and the classifier is constructed to predict the new samples. Finally, Sogou laboratory news corpus is used to evaluate the efficiency of text clustering and classification methods. The results demonstrate that the improved clustering method has a higher average cluster goodness than earlier procedures and the classification model based on three-way concept lattice achieves a higher accuracy.},
  archive      = {J_IJMLC},
  author       = {Chen, Xiaoyu and Qi, Jianjun and Zhu, Xiaomin and Wang, Xin and Wang, Zhen},
  doi          = {10.1007/s13042-019-00987-6},
  journal      = {International Journal of Machine Learning and Cybernetics},
  number       = {2},
  pages        = {475-490},
  shortjournal = {Int. J. Mach. Learn. Cybern.},
  title        = {Unlabelled text mining methods based on two extension models of concept lattices},
  volume       = {11},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Adaptive feature weighting for robust lp-norm sparse
representation with application to biometric image classification.
<em>IJMLC</em>, <em>11</em>(2), 463–474. (<a
href="https://doi.org/10.1007/s13042-019-00986-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sparse representation has attracted much attention in the field of biometrics, such as face recognition and palmprint recognition. Although the $$l_{p}$$-norm $$(0 &lt; p &lt; 1)$$ based sparse representation can obtain more sparse solution than the widely used $$l_{1}$$-norm based method, it needs to solve a non-convex optimization problem, which leads to poor robustness in real application. In this paper, we propose a robust $$l_{p}$$-norm sparse representation method with adaptive feature weighting. We derive the adaptive feature weighting method by self-paced learning (SPL), and utilize it to guide the features of $$l_{p}$$-norm sparse representation in the easy-to-hard learning process. Differing from existing SPL methods, feature weighted SPL in our method dynamically evaluates the learning difficulty of each feature rather than sample. For the advantages of the proposed method, it can avoid $$l_{p}$$-norm sparse minimization failing into bad local minima and reduce the effects of noise feature in the early learning stage. Experiments on several biometric image datasets show that our proposed method is superior to conventional $$l_{p}$$-norm based method and the state-of-the-art classification methods.},
  archive      = {J_IJMLC},
  author       = {Zhu, Qi and Xu, Nuoya and Huang, Sheng-Jun and Qian, Jianjun and Zhang, Daoqiang},
  doi          = {10.1007/s13042-019-00986-7},
  journal      = {International Journal of Machine Learning and Cybernetics},
  number       = {2},
  pages        = {463-474},
  shortjournal = {Int. J. Mach. Learn. Cybern.},
  title        = {Adaptive feature weighting for robust lp-norm sparse representation with application to biometric image classification},
  volume       = {11},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Interactive learning for joint event and relation
extraction. <em>IJMLC</em>, <em>11</em>(2), 449–461. (<a
href="https://doi.org/10.1007/s13042-019-00985-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We tackle the problems of both event and entity relation extraction, and come up with a novel method to implement joint extraction: iteratively interactive learning. This method is motivated by the empirical findings as below: the extracted event attributes (e.g., trigger and event type) can be used as the reliable features for the recognition of entity relation types, and vice versa. Accordingly, on one hand, we utilize the predicted event attributes (by a certain event extraction system) to remodel the distributed representations of features for entity relation extraction, and on the other hand, we use entity relations (recognized by a certain relation extraction system) to remodel the features for event extraction. This enables a double-channel task-independent joint model with an interactive learning: learning events for relation extraction, and meanwhile learning relations for event extraction. In practice, we perform the interactive learning in an iterative manner, so as to boost the joint model progressively. Methodologically, we take the neural network of bidirectional long short-term memory (Bi-LSTM) for learning event and relation respectively. And as usual, the attention mechanism is used. In our experiments, the automatic content extraction corpus is used for the evaluation of the proposed method. Such a corpus consists of event, entity and relation samples with gold-standard attribute tags. Experimental results show that our method outperforms the baselines (Bi-LSTMs with attention without interactive learning) in both event and relation extraction tasks, yielding performance gains of about 1.6\% and 1.8\% F-scores respectively, at the condition of low-resource setting.},
  archive      = {J_IJMLC},
  author       = {Zhang, Jingli and Hong, Yu and Zhou, Wenxuan and Yao, Jianmin and Zhang, Min},
  doi          = {10.1007/s13042-019-00985-8},
  journal      = {International Journal of Machine Learning and Cybernetics},
  number       = {2},
  pages        = {449-461},
  shortjournal = {Int. J. Mach. Learn. Cybern.},
  title        = {Interactive learning for joint event and relation extraction},
  volume       = {11},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). DCSVM: Fast multi-class classification using support vector
machines. <em>IJMLC</em>, <em>11</em>(2), 433–447. (<a
href="https://doi.org/10.1007/s13042-019-00984-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Using binary classification techniques to perform multi-class classification of data is still of great practical interest due to the robustness and simplicity of binary classifiers. These techniques produce a single multi-class classification decision based on many binary decisions. Our work relies on the simple observation that as dimensionality increases so does the data sparsity and, consequently, a single binary classifier may separate multiple classes. Therefore, we claim that the number of binary decisions can be significantly reduced. We present Divide and Conquer Support Vector Machines (DCSVM), an efficient algorithm for multi-class classification using Support Vector Machines. DCSVM is a divide and conquer algorithm which relies on data sparsity in high dimensional space and performs a smart partitioning of the whole training data set into disjoint subsets that are easily separable. A single prediction performed between two partitions eliminates at once one or more classes in one partition, leaving only a reduced number of candidate classes for subsequent steps. The algorithm continues recursively, reducing the number of classes at each step, until a final binary decision is made between the last two classes left in the competition. In the best case scenario, our algorithm makes a final decision between k classes in $$O(\log k)$$ decision steps and in the worst case scenario DCSVM makes a final decision in $$k{-}1$$ steps, which is not worse than the existent techniques.},
  archive      = {J_IJMLC},
  author       = {Don, Duleep Rathgamage and Iacob, Ionut E.},
  doi          = {10.1007/s13042-019-00984-9},
  journal      = {International Journal of Machine Learning and Cybernetics},
  number       = {2},
  pages        = {433-447},
  shortjournal = {Int. J. Mach. Learn. Cybern.},
  title        = {DCSVM: Fast multi-class classification using support vector machines},
  volume       = {11},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A two-stage hybrid probabilistic topic model for refining
image annotation. <em>IJMLC</em>, <em>11</em>(2), 417–431. (<a
href="https://doi.org/10.1007/s13042-019-00983-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Refining image annotation has become one of the core research topics in computer vision and pattern recognition due to its great potentials in image retrieval. However, it is still in its infancy and is not sophisticated enough to extract perfect semantic concepts just according to the image low-level features. In this paper, we propose a two-stage hybrid probabilistic topic model to improve the quality of automatic image annotation. To start with, a probabilistic latent semantic analysis model with asymmetric modalities is learned to estimate the posterior probabilities of each annotation keyword, during which the image-to-word relation can be well established. Next, a label similarity graph is constructed by a weighted linear combination of label similarity and visual similarity of images associated with the corresponding labels. By this way, the information from image low-level visual features and high-level semantic concepts can be seamlessly integrated by fully taking into account the word-to-word and image-to-image relations. Finally, the rank-two relaxation heuristics is exploited to further mine the correlation of the candidate annotations so as to capture the refining results, which plays a critical role in semantic based image retrieval. Extensive experiments show that the proposed model achieves not only superior annotation accuracy but also better retrieval performance.},
  archive      = {J_IJMLC},
  author       = {Tian, Dongping and Shi, Zhongzhi},
  doi          = {10.1007/s13042-019-00983-w},
  journal      = {International Journal of Machine Learning and Cybernetics},
  number       = {2},
  pages        = {417-431},
  shortjournal = {Int. J. Mach. Learn. Cybern.},
  title        = {A two-stage hybrid probabilistic topic model for refining image annotation},
  volume       = {11},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). An augmented lagrangian alternating direction method for
overlapping community detection based on symmetric nonnegative matrix
factorization. <em>IJMLC</em>, <em>11</em>(2), 403–415. (<a
href="https://doi.org/10.1007/s13042-019-00980-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we present an augmented Lagrangian alternating direction algorithm for symmetric nonnegative matrix factorization. The convergence of the algorithm is also proved in detail and strictly. Then we present a modified overlapping community detection method which is based on the presented symmetric nonnegative matrix factorization algorithm. We apply the modified community detection method to several real world networks. The obtained results show the capability of our method in detecting overlapping communities, hubs and outliers. We find that our experimental results have better quality than several competing methods for identifying communities.},
  archive      = {J_IJMLC},
  author       = {Hu, Liying and Guo, Gongde},
  doi          = {10.1007/s13042-019-00980-z},
  journal      = {International Journal of Machine Learning and Cybernetics},
  number       = {2},
  pages        = {403-415},
  shortjournal = {Int. J. Mach. Learn. Cybern.},
  title        = {An augmented lagrangian alternating direction method for overlapping community detection based on symmetric nonnegative matrix factorization},
  volume       = {11},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A hybrid optimization approach based on clustering and
chaotic sequences. <em>IJMLC</em>, <em>11</em>(2), 359–401. (<a
href="https://doi.org/10.1007/s13042-019-00979-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Evolutionary computation algorithms represent a class of stochastic methods that can be applied to a wide set of different complex optimization problems. Recently, the combination of approaches extracted from different computation techniques represents one of the most successful trends in evolutionary optimization. With this integration, the idea is to overcome the limitations of each single method and to reach a synergetic effect through their integration. In this paper, a hybrid optimization algorithm for solving optimization problems is introduced. The approach, called cluster–chaotic-optimization, combines the classification characteristics of a clustering method with the randomness of chaotic sequences to conduct its search strategy. Under the proposed method, at each generation, the population is divided into different clusters according to its space distribution. Then, individuals are modified considering two kinds of operators: intra-cluster and extra-cluster. In the intra-cluster operation, individuals of the same cluster are locally adjusted considering the position of the best element of the cluster in terms of its fitness value. On the other hand, in the extra-cluster operation, the best individual of each cluster is globally attracted to the best element of the complete population. In both operations, the adjustment on each individual position is produced by using deterministic rules and chaotic sequences. With such mechanisms, the proposed method efficiently examines the search space based on the spatial associations produced by the individuals during the optimization process. To exhibit the performance and robustness of the proposed method, different comparisons to other well-known evolutionary methods and hybrid approaches are conducted. The comparison considers several standard benchmark functions and real-world engineering problems which are typically found in the literature of evolutionary algorithms. The results suggest a high performance of the proposed methodology.},
  archive      = {J_IJMLC},
  author       = {Gálvez, Jorge and Cuevas, Erik and Becerra, Héctor and Avalos, Omar},
  doi          = {10.1007/s13042-019-00979-6},
  journal      = {International Journal of Machine Learning and Cybernetics},
  number       = {2},
  pages        = {359-401},
  shortjournal = {Int. J. Mach. Learn. Cybern.},
  title        = {A hybrid optimization approach based on clustering and chaotic sequences},
  volume       = {11},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). An empirical study to estimate the stability of random
forest classifier on the hybrid features recommended by filter based
feature selection technique. <em>IJMLC</em>, <em>11</em>(2), 339–358.
(<a href="https://doi.org/10.1007/s13042-019-00978-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The emergence of advanced malware is a serious threat to information security. A prominent technique that identifies sophisticated malware should consider the runtime behaviour of the source file to detect malicious intent. Although the behaviour-based malware detection technique is a substantial improvement over the traditional signature-based detection technique, current malware employs code obfuscation techniques to elude detection. This paper presents the Hybrid Features-based malware detection system (HFMDS) that integrates static and dynamic features of the portable executable (PE) files to discern malware. The HFMDS is trained with prominent features advised by the filter-based feature selection technique (FST). The detection ability of the proposed HFMDS has evaluated with the random forest (RF) classifier by considering two different datasets that consist of real-world Windows malware samples. In-depth analysis is carried out to determine the optimal number of decision trees (DTs) required by the RF classifier to achieve consistent accuracy. Besides, four popular FSTs performance is also analyzed to determine which FST recommends the best features. From the experimental analysis, we can infer that increasing the number of DTs after 160 within the RF classifier does not make a significant difference in attaining better detection accuracy.},
  archive      = {J_IJMLC},
  author       = {Darshan, S. L. Shiva and Jaidhar, C. D.},
  doi          = {10.1007/s13042-019-00978-7},
  journal      = {International Journal of Machine Learning and Cybernetics},
  number       = {2},
  pages        = {339-358},
  shortjournal = {Int. J. Mach. Learn. Cybern.},
  title        = {An empirical study to estimate the stability of random forest classifier on the hybrid features recommended by filter based feature selection technique},
  volume       = {11},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Non-fragile control protocol for finite-time consensus of
stochastic multi-agent systems with input time-varying delay.
<em>IJMLC</em>, <em>11</em>(2), 325–337. (<a
href="https://doi.org/10.1007/s13042-019-00976-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The existence of gain variations in control design often disrupts most of the control system performance. By considering this point, as a first attempt in the literature, a robust non-fragile state feedback control design is proposed in this paper for achieving finite-time consensus in a class of stochastic nonlinear multi-agent systems with randomly occurring uncertainty and randomly occurring nonlinearity. Specifically, the randomness phenomena are characterized with the aid of stochastic variables that satisfy the Bernoulli distribution properties. To design the non-fragile control protocol, the communication graph is chosen to be directed and connected subject to switching topologies. On the basis of the Lyapunov–Krasovskii stability theory and stochastic analysis techniques, a new set of sufficient conditions is established to guarantee that the states of all agents can reach an agreement over a given finite-time period via the proposed non-fragile switched control law. The effectiveness of the designed consensus protocol is demonstrated through an academic example.},
  archive      = {J_IJMLC},
  author       = {Kaviarasan, B. and Sakthivel, R. and Li, Y. and Zhao, D. and Ren, Y.},
  doi          = {10.1007/s13042-019-00976-9},
  journal      = {International Journal of Machine Learning and Cybernetics},
  number       = {2},
  pages        = {325-337},
  shortjournal = {Int. J. Mach. Learn. Cybern.},
  title        = {Non-fragile control protocol for finite-time consensus of stochastic multi-agent systems with input time-varying delay},
  volume       = {11},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Selectivity analysis of parameters in soft set and its
effect on decision making. <em>IJMLC</em>, <em>11</em>(2), 313–324. (<a
href="https://doi.org/10.1007/s13042-019-00975-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Binary relations between alternatives and parameters in a soft set have a vital role in the analysis of configuration of this soft set. All of the studies on the soft set approach focus on the parameters and their images under the approximate function. Recently, by approaching the soft set from a different perspective, a pseudo soft set has been introduced. This approach associates the alternatives and their images under the approximate function. In this paper, we first discuss the transition from a soft set to a pseudo soft set and vice versa. Later on, we endeavor to analyze the selectivity of parameters in the structure of a (fuzzy parameterized) soft set. This analysis focuses on the binary relations with boolean values between a parameter and its associated alternatives rather than on the specific values of alternatives with respect to a parameter. Relatedly, the concepts of selectivity ratio and coverage selectivity ratio of parameters under the (fuzzy parameterized) soft sets are introduced and some basic properties are presented. Also, by using the transition between the soft set and the pseudo soft set, it is investigated the effect of selectivity analysis of parameters on decision making. As a result of this attempt, new decision making algorithms are proposed. The outputs of these algorithms are compared with those of some of the existing decision making algorithms based on the (fuzzy parameterized) soft sets. Thus, the performance of each of the proposed algorithms is displayed.},
  archive      = {J_IJMLC},
  author       = {Kamacı, Hüseyin},
  doi          = {10.1007/s13042-019-00975-w},
  journal      = {International Journal of Machine Learning and Cybernetics},
  number       = {2},
  pages        = {313-324},
  shortjournal = {Int. J. Mach. Learn. Cybern.},
  title        = {Selectivity analysis of parameters in soft set and its effect on decision making},
  volume       = {11},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Improved variational inference with dynamic routing flow.
<em>IJMLC</em>, <em>11</em>(2), 301–312. (<a
href="https://doi.org/10.1007/s13042-019-00974-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {How to transform a family of simple distributions to approximate an intractable posterior distribution in a scalable manner is a key problem in variational inference. Recent researches have been studied to generate a flexible approximate posterior distribution by utilizing a flow-based model with a long flow structure. However, when the dimension of the data increases, a long flow structure brings the problem of computational complexity and large variance. Therefore, we propose a variational inference with dynamic routing flow (DRF), which ensures the multiformity of the flows with shorter flow structure in this paper. The proposed model consists of a series of iterative sub-modules transformations, and each sub-module is enabled with a greater expression power by routing-by-agreement to achieve a group of weighted mixture of invertible transformations. These sub-modules route can be computed parallelly and they share the same group of invertible functions, which makes the inference more efficiency. The experimental results show that the proposed DRF model achieves significant performance on the posterior distribution estimation both in accuracy and precision.},
  archive      = {J_IJMLC},
  author       = {Hua, Qiang and Wei, Liangliang and Dong, Chunru and Zhang, Feng},
  doi          = {10.1007/s13042-019-00974-x},
  journal      = {International Journal of Machine Learning and Cybernetics},
  number       = {2},
  pages        = {301-312},
  shortjournal = {Int. J. Mach. Learn. Cybern.},
  title        = {Improved variational inference with dynamic routing flow},
  volume       = {11},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Effective spatio-temporal semantic trajectory generation for
similar pattern group identification. <em>IJMLC</em>, <em>11</em>(2),
287–300. (<a href="https://doi.org/10.1007/s13042-019-00973-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The daily trajectories of individual movements convey a concise overview of their behaviors, with different social roles having different trajectory patterns. Therefore, we can identify users or groups based on the similar of their trajectory patterns. However, most existing trajectory analysis focuses only on the spatial and temporal analyses of the raw trajectory data and misses essential semantic information concerning behaviors. In this paper, we propose a new trajectory semantics calculation method to identify groups with similar behaviors. We first propose a fast and efficient two-phase method for identifying stay regions within daily trajectories and enriching the stay regions with semantic labels based on points of interest to generate semantic trajectories. Furthermore, we design a semantic similarity measure model using geographic and semantic similarity factors to measure the similarity between semantic trajectories. We also propose a pruning strategy using time entropy to decrease the number of complex calculations and comparisons to improve performance. The results of our extensive experiments on the real trajectory dataset of the Geolife project show that our proposed method is both effective and efficient.},
  archive      = {J_IJMLC},
  author       = {Cao, Yang and Xue, Fei and Chi, Yuanying and Ding, Zhiming and Guo, Limin and Cai, Zhi and Tang, Hengliang},
  doi          = {10.1007/s13042-019-00973-y},
  journal      = {International Journal of Machine Learning and Cybernetics},
  number       = {2},
  pages        = {287-300},
  shortjournal = {Int. J. Mach. Learn. Cybern.},
  title        = {Effective spatio-temporal semantic trajectory generation for similar pattern group identification},
  volume       = {11},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Type-2 fuzzy cerebellar model articulation control system
design for MIMO uncertain nonlinear systems. <em>IJMLC</em>,
<em>11</em>(2), 269–286. (<a
href="https://doi.org/10.1007/s13042-019-00972-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper aims to propose a more efficient neural network and applies it as an adaptive controller for the multi-input multi-output (MIMO) uncertain nonlinear systems. First, a more efficient fuzzy neural network named as fuzzy cerebellar model articulation controller (CMAC) is introduced, then an adaptive controller is proposed using a novel interval type-2 fuzzy CMAC (T2FCMAC). The T2FCMAC realizes an interval type-2 fuzzy logic system based on the structure of the CMAC. Due to the better ability of handling uncertainties provided by type-2 fuzzy sets, it can solve some complicated problems with outstanding effectiveness than type-1 fuzzy sets. In addition, an intelligent control system is proposed; this control system is comprised of a T2FCMAC and an auxiliary compensation controller. The T2FCMAC is utilized to approximate a perfect controller and the parameters of T2FCMAC are on-line tuned by the derived adaptive laws based on a Lyapunov function. The auxiliary compensation controller is designed to suppress the influence of residual approximation error between the perfect controller and the T2FCMAC. Finally, two MIMO uncertain nonlinear systems, a Chua’s chaotic circuit and a mass-spring-damper mechanical system, are performed to verify the effectiveness of the proposed control scheme. The simulation results confirm that the proposed intelligent adaptive control system can achieve favorable tracking performance with desired robustness.},
  archive      = {J_IJMLC},
  author       = {Lin, Chih-Min and Yang, Ming-Shu},
  doi          = {10.1007/s13042-019-00972-z},
  journal      = {International Journal of Machine Learning and Cybernetics},
  number       = {2},
  pages        = {269-286},
  shortjournal = {Int. J. Mach. Learn. Cybern.},
  title        = {Type-2 fuzzy cerebellar model articulation control system design for MIMO uncertain nonlinear systems},
  volume       = {11},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Integration search strategies in tree seed algorithm for
high dimensional function optimization. <em>IJMLC</em>, <em>11</em>(2),
249–267. (<a href="https://doi.org/10.1007/s13042-019-00970-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The tree-seed algorithm, TSA for short, is a new population-based intelligent optimization algorithm developed for solving continuous optimization problems by inspiring the relationship between trees and their seeds. The locations of trees and seeds correspond to the possible solutions of the optimization problem on the search space. By using this model, the continuous optimization problems with lower dimensions are solved effectively, but its performance dramatically decreases on solving higher dimensional optimization problems. In order to address this issue in the basic TSA, an integration of different solution update rules are proposed in this study for solving high dimensional continuous optimization problems. Based on the search tendency parameter, which is a peculiar control parameter of TSA, five update rules and a withering process are utilized for obtaining seeds for the trees. The performance of the proposed method is investigated on basic 30-dimensional twelve numerical benchmark functions and CEC (congress on evolutionary computation) 2015 test suite. The performance of the proposed approach is also compared with the artificial bee colony algorithm, particle swarm optimization algorithm, genetic algorithm, pure random search algorithm and differential evolution variants. Experimental comparisons show that the proposed method is better than the basic method in terms of solution quality, robustness and convergence characteristics.},
  archive      = {J_IJMLC},
  author       = {Gungor, Imral and Emiroglu, Bulent Gursel and Cinar, Ahmet Cevahir and Kiran, Mustafa Servet},
  doi          = {10.1007/s13042-019-00970-1},
  journal      = {International Journal of Machine Learning and Cybernetics},
  number       = {2},
  pages        = {249-267},
  shortjournal = {Int. J. Mach. Learn. Cybern.},
  title        = {Integration search strategies in tree seed algorithm for high dimensional function optimization},
  volume       = {11},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Ensemble with estimation: Seeking for optimization in class
noisy data. <em>IJMLC</em>, <em>11</em>(2), 231–248. (<a
href="https://doi.org/10.1007/s13042-019-00969-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Class noise, as know as the mislabeled data in training set, can lead to poor accuracy in classification no matter what machine learning methods are used. A reasonable estimation of class noise has a significant impact on the performance of learning methods. However, the error in existing estimation is inevitable theoretically and infer the performance of optimal classifier trained on noisy data. Instead of seeking a single optimal classifier on noisy data, in this work, we use a set of weak classifiers, which are caused by negative impacts of noisy data, to learn an ensemble strong classifier which is based on the training error and estimation of class noise. By this strategy, the proposed ensemble with estimation method overcomes the gap between the estimation and true distribution of class noise. Our proposed method does not require any a priori knowledge about class noises. We prove that the optimal ensemble classifier on the noisy distribution can approximate the optimal classifier on the clean distribution when the training set grows. Comparisons with existing algorithms show that our methods outperform state-of-the-art approaches on a large number of benchmark datasets in different domains. Both the theoretical analysis and the experimental result reveal that our method can improve the performance, works well on clean data and is robust on the algorithm parameter.},
  archive      = {J_IJMLC},
  author       = {Xu, Ruifeng and Wen, Zhiyuan and Gui, Lin and Lu, Qin and Li, Binyang and Wang, Xizhao},
  doi          = {10.1007/s13042-019-00969-8},
  journal      = {International Journal of Machine Learning and Cybernetics},
  number       = {2},
  pages        = {231-248},
  shortjournal = {Int. J. Mach. Learn. Cybern.},
  title        = {Ensemble with estimation: Seeking for optimization in class noisy data},
  volume       = {11},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). On selective learning in stochastic stepwise ensembles.
<em>IJMLC</em>, <em>11</em>(1), 217–230. (<a
href="https://doi.org/10.1007/s13042-019-00968-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ensemble learning has attracted much attention of researchers studying variable selection due to its great power in improving selection accuracy and stabilizing selection results. In this paper, we present a novel ensemble pruning technique called Pruned-ST2E to obtain more effective variable selection ensembles. The order to aggregate the individuals generated by the ST2E algorithm (Xin and Zhu in J Comput Graph Stat 21(2):275–294, 2012) is rearranged. To estimate the importance of each candidate variable, only some members ranked ahead are remained. Experiments with simulated and real-world data show that the performance of Pruned-ST2E is comparable or superior to several other benchmark methods. Through analyzing the accuracy–diversity pattern in both ST2E and Pruned-ST2E, it is revealed that the inserted pruning step excludes less accurate members. The reserved members also become more concentrated on the true importance vector. Moreover, Pruned-ST2E is easy to implement. Therefore, Pruned-ST2E can be considered as an alternative for tackling variable selection tasks in practice.},
  archive      = {J_IJMLC},
  author       = {Zhang, Chun-Xia and Kim, Sang-Woon and Zhang, Jiang-She},
  doi          = {10.1007/s13042-019-00968-9},
  journal      = {International Journal of Machine Learning and Cybernetics},
  number       = {1},
  pages        = {217-230},
  shortjournal = {Int. J. Mach. Learn. Cybern.},
  title        = {On selective learning in stochastic stepwise ensembles},
  volume       = {11},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A robust multilayer extreme learning machine using kernel
risk-sensitive loss criterion. <em>IJMLC</em>, <em>11</em>(1), 197–216.
(<a href="https://doi.org/10.1007/s13042-019-00967-w">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {More recently, extreme learning machine (ELM) has emerged as a novel computing paradigm that enables the neural network (NN) based learning to be achieved with fast training speed and good generalization performance. However, the single hidden layer NN using ELM may be not effective in addressing some large-scale problems with more computational efforts. To avoid such limitation, we utilize the multilayer ELM architecture in this article to reduce the computational complexity, without the physical memory limitation. Meanwhile, it is known to us all that there are a lot of noises in the practical applications, and the traditional ELM may not perform well in this instance. Considering the existence of noises or outliers in training dataset, we develop a more practical approach by incorporating the kernel risk-sensitive loss (KRSL) criterion into ELM, on the basis of the efficient performance surface of KRSL with high accuracy while still maintaining the robustness to outliers. A robust multilayer ELM, i.e., the stacked ELM using the minimum KRSL criterion (SELM-MKRSL), is accordingly proposed in this article to enhance the outlier robustness on large-scale and complicated dataset. The simulation results on some synthetic datasets indicate that the proposed approach SELM-MKRSL can achieve higher classification accuracy and is more robust to the noises compared with other state-of-the-art algorithms related to multilayer ELM.},
  archive      = {J_IJMLC},
  author       = {Luo, Xiong and Li, Ying and Wang, Weiping and Ban, Xiaojuan and Wang, Jenq-Haur and Zhao, Wenbing},
  doi          = {10.1007/s13042-019-00967-w},
  journal      = {International Journal of Machine Learning and Cybernetics},
  number       = {1},
  pages        = {197-216},
  shortjournal = {Int. J. Mach. Learn. Cybern.},
  title        = {A robust multilayer extreme learning machine using kernel risk-sensitive loss criterion},
  volume       = {11},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Surface electromyography feature extraction via
convolutional neural network. <em>IJMLC</em>, <em>11</em>(1), 185–196.
(<a href="https://doi.org/10.1007/s13042-019-00966-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Although a large number of surface electromyography (sEMG) features have been proposed to improve hand gesture recognition accuracy, it is still hard to achieve acceptable performance in inter-session and inter-subject tests. To promote the application of sEMG-based human machine interaction, a convolutional neural network based feature extraction approach (CNNFeat) is proposed to improve hand gesture recognition accuracy. A sEMG database is recorded from eight subjects while performing ten hand gestures. Three classic classifiers, including linear discriminant analysis (LDA), support vector machine (SVM) and K nearest neighbor (KNN), are employed to compare the CNNFeat with 25 traditional features. This work concentrates on the analysis of CNNFeat through accuracy, safety index and repeatability index. The experimental results show that CNNFeat outperforms all the tested traditional features in inter-subject test and is listed as the best three features in inter-session test. Besides, it is also found that combining CNNFeat with traditional features can further improve the accuracy by 4.35\%, 3.62\% and 4.7\% for SVM, LDA and KNN, respectively. Additionally, this work also demonstrates that CNNFeat can be potentially enhanced with more data for model training.},
  archive      = {J_IJMLC},
  author       = {Chen, Hongfeng and Zhang, Yue and Li, Gongfa and Fang, Yinfeng and Liu, Honghai},
  doi          = {10.1007/s13042-019-00966-x},
  journal      = {International Journal of Machine Learning and Cybernetics},
  number       = {1},
  pages        = {185-196},
  shortjournal = {Int. J. Mach. Learn. Cybern.},
  title        = {Surface electromyography feature extraction via convolutional neural network},
  volume       = {11},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Knowledge reasoning approach with linguistic-valued
intuitionistic fuzzy credibility. <em>IJMLC</em>, <em>11</em>(1),
169–184. (<a href="https://doi.org/10.1007/s13042-019-00965-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Linguistic term evaluations are always collected from two opposite sides at the same time in an assessment system. To process the linguistic knowledge, we propose an approximate reasoning approach with linguistic-valued intuitionistic fuzzy credibility based on linguistic-valued intuitionistic fuzzy lattice implication algebra and apply it to the assessment system. Firstly, we give a knowledge representation model with linguistic-valued intuitionistic fuzzy credibility. Based on the representation model, the forms and patterns of linguistic intuitionistic fuzzy modus ponens (LI-FMP) and linguistic intuitionistic fuzzy modus tollens (LI-FMT) are defined. Then there are three main phases of the knowledge reasoning with linguistic-valued intuitionistic fuzzy credibility. For a single rule, the similarity-based algorithms for LI-FMP and LI-FMT are given to get the sub-conclusion and the properties of similarity-based algorithms are discussed. For the multi-rule, we propose a rule aggregation operator to get the final conclusion by combining all the sub-conclusions. Some incomparable results are further processed if it is necessary. An intuitionistic linguistic-real valuation function is defined implying a linguistic intuitionistic fuzzy distance which is proved to be a positive valuation function. The ranking method of the incomparable results utilizes the linguistic intuitionistic distance. Lastly, the example about individual credit risk assessment shows how the proposed approach work and the contrast example illustrates that the proposed approach is rational and applied.},
  archive      = {J_IJMLC},
  author       = {Zhang, Yunxia and Huang, Degen and Lin, Hongmei and Zou, Li},
  doi          = {10.1007/s13042-019-00965-y},
  journal      = {International Journal of Machine Learning and Cybernetics},
  number       = {1},
  pages        = {169-184},
  shortjournal = {Int. J. Mach. Learn. Cybern.},
  title        = {Knowledge reasoning approach with linguistic-valued intuitionistic fuzzy credibility},
  volume       = {11},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Double quantitative fuzzy rough set-based improved AHP
method and application to supplier selection decision making.
<em>IJMLC</em>, <em>11</em>(1), 153–167. (<a
href="https://doi.org/10.1007/s13042-019-00964-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Selecting excellent supplier is the foundation of establishing efficient supply chain. This paper presents a novel hybrid model for supplier selection decision making problem by combining double quantitative fuzzy rough set and analytic hierarchy process (AHP), namely $$Fuzzy-AHP$$. we first transform the supplier decision-making into a rough-approximation problem in the double quantization decision approximation space with fuzzy decision objects, and then construct a new supplier selection decision model and method based on double-quantitative fuzzy rough sets. Then, the double quantitative fuzzy rough set is utilized to calculate the upper and lower approximations of the fuzzy decision object in the quantitative approximation space. Furthermore, the lower and upper approximations are applied to establish the pairwise comparison matrix and analytic hierarchy process is employed to rank these suppliers comprehensively. Finally, an experiment study with six ERP bidder in an Indian mining behemoth is carried out in this paper. The results reveal that the proposed technique can select effective suppliers, but also realize a comprehensive ranking. This research has enriched the methodology of supplier evaluation and selection, as well as owns theoretical value in exploring the coordinated development of supply chain to some extent.},
  archive      = {J_IJMLC},
  author       = {Hu, Xiaoyuan and Sun, Bingzhen and Chen, Xiangtang},
  doi          = {10.1007/s13042-019-00964-z},
  journal      = {International Journal of Machine Learning and Cybernetics},
  number       = {1},
  pages        = {153-167},
  shortjournal = {Int. J. Mach. Learn. Cybern.},
  title        = {Double quantitative fuzzy rough set-based improved AHP method and application to supplier selection decision making},
  volume       = {11},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Fine-art painting classification via two-channel dual path
networks. <em>IJMLC</em>, <em>11</em>(1), 137–152. (<a
href="https://doi.org/10.1007/s13042-019-00963-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fine-art painting expresses the state of mind and social culture of mankind. Automatic fine-art painting classification is an important task to assist the analysis of fine-art paintings. In this paper, we propose a novel two-channel dual path networks for the task of style, artist and genre classification on fine-art painting image. It includes the RGB and the brush stroke information channels. Besides the RGB information channel is used to represent the color information in fine-art painting images, the brush stroke information channel is used to extract brush stroke information from fine-art painting images. And the four-directional gray-level co-occurrence matrix is used in deep learning to detect the brush stroke information, which has never been considered in the task of fine-art painting classification. Experiments on two datasets demonstrate that the four-directional gray-level co-occurrence matrix is effective in feature representation of fine-art painting images. And the proposed model achieves best classification accuracy and good generalization performance when compared with other methods.},
  archive      = {J_IJMLC},
  author       = {Zhong, Sheng-hua and Huang, Xingsheng and Xiao, Zhijiao},
  doi          = {10.1007/s13042-019-00963-0},
  journal      = {International Journal of Machine Learning and Cybernetics},
  number       = {1},
  pages        = {137-152},
  shortjournal = {Int. J. Mach. Learn. Cybern.},
  title        = {Fine-art painting classification via two-channel dual path networks},
  volume       = {11},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). An uncertainty based incremental learning for identifying
the severity of bug report. <em>IJMLC</em>, <em>11</em>(1), 123–136. (<a
href="https://doi.org/10.1007/s13042-019-00961-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To ensure the reliability of software system, software developers have to keep track of the severity of bug reports, and fix critical bugs as soon as possible. Recently, automatic methods to identify the severity of bug reports have emerged as a promising tool to lessen the work burden of software developers. However, most of such methods are supervised and data-driven models which fail to provide favorable performance in the presence of insufficient labeled sample or limited training data. In order to tackle with these issues, we propose an incremental learning for bug reports recognition. According to this framework of incremental learning, one active learning method is developed for tagging unlabeled bug reports, meanwhile, a sample augmentation method is utilized for sufficient training data. Both of these methods are based on uncertainty which is correlated to the informativeness and the classification risk of samples. Moreover, different types of connectionist models are employed to identify bug reports, and comprehensive experiments on real bug report datasets demonstrate that the generalization abilities of these models can be improved by this proposed incremental learning.},
  archive      = {J_IJMLC},
  author       = {Zhang, Tian-Lun and Chen, Rong and Yang, Xi and Zhu, Hong-Yu},
  doi          = {10.1007/s13042-019-00961-2},
  journal      = {International Journal of Machine Learning and Cybernetics},
  number       = {1},
  pages        = {123-136},
  shortjournal = {Int. J. Mach. Learn. Cybern.},
  title        = {An uncertainty based incremental learning for identifying the severity of bug report},
  volume       = {11},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Incomplete label distribution learning based on supervised
neighborhood information. <em>IJMLC</em>, <em>11</em>(1), 111–121. (<a
href="https://doi.org/10.1007/s13042-019-00958-x">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Label distribution learning (LDL) assumes labels are associated with each instance to some degree and tries to model the relationship between labels and instances. LDL has achieved great success in many applications, but most existing LDL methods are designed for data with complete annotation information. However, in reality, supervised information often be incomplete due to the huge costs of data collection. In this paper, we propose a novel incomplete label distribution learning method based on supervised neighborhood information (IncomLDL-SNI). The proposed method uses partial least squares to project the original data into a supervised feature space where instances with similar labels are likely to be projected together. Then, IncomLDL-SNI utilizes the Euclidean distance to find the nearest neighbors for target samples in the supervised feature space and recovers the missing annotations from the neighborhood label Information. Extensive experiments on various data sets validate the effectiveness of our proposal.},
  archive      = {J_IJMLC},
  author       = {Zeng, Xue-Qiang and Chen, Su-Fen and Xiang, Run and Li, Guo-Zheng and Fu, Xue-Feng},
  doi          = {10.1007/s13042-019-00958-x},
  journal      = {International Journal of Machine Learning and Cybernetics},
  number       = {1},
  pages        = {111-121},
  shortjournal = {Int. J. Mach. Learn. Cybern.},
  title        = {Incomplete label distribution learning based on supervised neighborhood information},
  volume       = {11},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Wavelet transform-based weighted <span
class="math display"><em>ν</em></span>-twin support vector regression.
<em>IJMLC</em>, <em>11</em>(1), 95–110. (<a
href="https://doi.org/10.1007/s13042-019-00957-y">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, an efficient wavelet transform-based weighted $$\nu$$-twin support vector regression (WTWTSVR) is proposed, inspired by twin support vector regression (TSVR) and $$\nu$$-twin support vector machine-based regression. TSVR and its improved models work faster than support vector regression because they solve a pair of smaller-sized quadratic programming problems. However, they give the same emphasis to the training samples, i.e., all of the training samples share the same weights, and prior information is not used, which leads to the degradation of performance. Motivated by this, samples in different positions in the proposed WTWTSVR model are given different penalty weights determined by the wavelet transform. The weights are applied to both the quadratic empirical risk term and the first-degree empirical risk term to reduce the influence of outliers. The final regressor can avoid the overfitting problem to a certain extent and yield great generalization ability. Numerical experiments on artificial datasets and benchmark datasets demonstrate the feasibility and validity of our proposed algorithm.},
  archive      = {J_IJMLC},
  author       = {Wang, Lidong and Gao, Chuang and Zhao, Nannan and Chen, Xuebo},
  doi          = {10.1007/s13042-019-00957-y},
  journal      = {International Journal of Machine Learning and Cybernetics},
  number       = {1},
  pages        = {95-110},
  shortjournal = {Int. J. Mach. Learn. Cybern.},
  title        = {Wavelet transform-based weighted $$\nu$$-twin support vector regression},
  volume       = {11},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Local attribute reductions of formal contexts.
<em>IJMLC</em>, <em>11</em>(1), 81–93. (<a
href="https://doi.org/10.1007/s13042-019-00956-z">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Attribute reductions is an important topic in formal concept analysis. The existing attribute reduction approaches are dominated by global reductions and there is very limited investigation on local reductions. This paper is devoted to the study of decision rule specific reduction for formal decision context and concept specific reduction for formal context. The notion of decision rule specific reduction is proposed and the related reduction methods are presented. The relationships between existing reduction approaches and decision rule specific reduction approaches are analyzed. Accordingly, we make an analysis of attributes based on three-way classification by using reductions. Furthermore, the notion of concept specific reduction for formal context is proposed and the concept specific reduction methods are examined. The relationship between concept specific reduction and decision rule specific reduction is surveyed.},
  archive      = {J_IJMLC},
  author       = {Qin, Keyun and Lin, Hong and Jiang, Yuting},
  doi          = {10.1007/s13042-019-00956-z},
  journal      = {International Journal of Machine Learning and Cybernetics},
  number       = {1},
  pages        = {81-93},
  shortjournal = {Int. J. Mach. Learn. Cybern.},
  title        = {Local attribute reductions of formal contexts},
  volume       = {11},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Image set face recognition based on extended low rank
recovery and collaborative representation. <em>IJMLC</em>,
<em>11</em>(1), 71–80. (<a
href="https://doi.org/10.1007/s13042-019-00941-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the real-world face recognition problems, the collected query set images often suffer serious disturbances. To address the problem, we propose an image set face recognition method based on extended low rank recovery and collaborative representation. By exploiting a Frobenius norm term, an extended low rank representation model is firstly developed to remove all possible disturbances from the query set and reconstruct the rank-one query set. To improve the computational efficiency, a compact and discriminative dictionary is learned from the large gallery set, and the closed form solutions for both the dictionary atom and the coding coefficient are straightway derived. The final classification is performed by using any frame in the reconstructed query set instead of using the whole set, which can further improve the running efficiency. Extensive experiments are conducted on the benchmark Honda/USCD and Youtube Celebrities database to verify that the proposed method outperforms significantly the state-of-the-art methods in terms of robustness and efficiency.},
  archive      = {J_IJMLC},
  author       = {Song, Zhanjie and Cui, Kaiyan and Cheng, Guangtao},
  doi          = {10.1007/s13042-019-00941-6},
  journal      = {International Journal of Machine Learning and Cybernetics},
  number       = {1},
  pages        = {71-80},
  shortjournal = {Int. J. Mach. Learn. Cybern.},
  title        = {Image set face recognition based on extended low rank recovery and collaborative representation},
  volume       = {11},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Automatic optic disc detection using low-rank representation
based semi-supervised extreme learning machine. <em>IJMLC</em>,
<em>11</em>(1), 55–69. (<a
href="https://doi.org/10.1007/s13042-019-00939-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Optic disc detection plays an important role in developing automatic screening systems for diabetic retinopathy. Several supervised learning-based approaches have been proposed for optic disc detection. However, these approaches demand that the input training examples are completely labelled. Essentially, in medical image analysis, it is difficult to prepare several training samples which were given reliable class labels due to the fact that manually labelling data is very expensive. Moreover, retinal images such as complex vessels structures in the optic disc constituting nonlinear relationships in high-dimensional observation space, which cannot work well by traditional linear classifiers. In this study, a novel approach named low-rank representation based semi-supervised extreme learning machine (LRR-SSELM) is proposed for automated optic disc detection. Our model has the following advantages. First, it detects the optic disc from the viewpoint of semi-supervised learning and overcomes the problem there are small portion of labelled samples. Second, a nonlinear classifier is introduced into our model to fully explore the nonlinear data. Third, the local and global structures of original data can be greatly persevered by low-rank representation (LRR). The performance of the proposed method is validated on three publicly available databases, DIARETDB0, DIARETDB1 and Messidor. The experimental results indicate the advantages and effectiveness of the proposed approach.},
  archive      = {J_IJMLC},
  author       = {Zhou, Wei and Qiao, Shaojie and Yi, Yugen and Han, Nan and Chen, Yuqi and Lei, Gang},
  doi          = {10.1007/s13042-019-00939-0},
  journal      = {International Journal of Machine Learning and Cybernetics},
  number       = {1},
  pages        = {55-69},
  shortjournal = {Int. J. Mach. Learn. Cybern.},
  title        = {Automatic optic disc detection using low-rank representation based semi-supervised extreme learning machine},
  volume       = {11},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Extreme vector machine for fast training on large data.
<em>IJMLC</em>, <em>11</em>(1), 33–53. (<a
href="https://doi.org/10.1007/s13042-019-00936-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Quite often, different types of loss functions are adopted in SVM or its variants to meet practical requirements. How to scale up the corresponding SVMs for large datasets are becoming more and more important in practice. In this paper, extreme vector machine (EVM) is proposed to realize fast training of SVMs with different yet typical loss functions on large datasets. EVM begins with a fast approximation of the convex hull, expressed by extreme vectors, of the training data in the feature space, and then completes the corresponding SVM optimization over the extreme vector set. When hinge loss function is adopted, EVM is the same as the approximate extreme points support vector machine (AESVM) for classification. When square hinge loss function, least squares loss function and Huber loss function are adopted, EVM corresponds to three versions, namely, L2-EVM, LS-EVM and Hub-EVM, respectively, for classification or regression. In contrast to the most related machine AESVM, with the retainment of its theoretical advantage, EVM is distinctive in its applicability to a wide variety of loss functions to meet practical requirements. Compared with the other state-of-the-art fast training algorithms CVM and FastKDE of SVMs, EVM indeed relaxes the limitation of least squares loss functions, and experimentally exhibits its superiority in training time, robustness capability and number of support vectors.},
  archive      = {J_IJMLC},
  author       = {Gu, Xiaoqing and Chung, Fu-lai and Wang, Shitong},
  doi          = {10.1007/s13042-019-00936-3},
  journal      = {International Journal of Machine Learning and Cybernetics},
  number       = {1},
  pages        = {33-53},
  shortjournal = {Int. J. Mach. Learn. Cybern.},
  title        = {Extreme vector machine for fast training on large data},
  volume       = {11},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Symmetric uncertainty class-feature association map for
feature selection in microarray dataset. <em>IJMLC</em>, <em>11</em>(1),
15–32. (<a href="https://doi.org/10.1007/s13042-019-00932-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For a huge number of features versus a small size of samples, feature selection methods are useful preprocessing approaches that could eliminate the irrelevant and redundant features from the final feature subset. One of the recent research areas in feature selection is DNA microarray that the number of dimensions increase fast and requires further research in the field of feature selection. Modeling the feature search space as a graph leads to improving the visualizing of features and using graph theoretic concepts in the feature selection process. In this paper, a filer-based feature selection algorithm using graph technique is proposed for reducing the dimension of dataset named as Symmetric Uncertainty Class-Feature Association Map feature selection (SU-CFAM). In the first step, it uses the Symmetric Uncertainty concept for visualizing the feature search space as a graph. After clustering the graph into several clusters using a community detection algorithm, SU-CFAM constructs an adjacency matrix for each cluster and the final subset is selected by using the concept of maximal independent set. The performance of SU-CFAM has been compared with five well-known feature selection approaches using three classifiers including SVM, DT, NB. Experiments on fifteen public DNA microarray datasets show that SU-CFAM can achieve a better classification performance compared with other methods.},
  archive      = {J_IJMLC},
  author       = {Bakhshandeh, Soodeh and Azmi, Reza and Teshnehlab, Mohammad},
  doi          = {10.1007/s13042-019-00932-7},
  journal      = {International Journal of Machine Learning and Cybernetics},
  number       = {1},
  pages        = {15-32},
  shortjournal = {Int. J. Mach. Learn. Cybern.},
  title        = {Symmetric uncertainty class-feature association map for feature selection in microarray dataset},
  volume       = {11},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A lazy learning-based language identification from speech
using MFCC-2 features. <em>IJMLC</em>, <em>11</em>(1), 1–14. (<a
href="https://doi.org/10.1007/s13042-019-00928-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Developing an automatic speech recognition system for multilingual countries like India is a challenging task due to the fact that the people are inured to using multiple languages while talking. This makes language identification from speech an important and essential task prior to recognition of the same. In this paper a system is proposed towards language identification from multilingual speech signals. A new second level Mel frequency cepstral coefficient-based feature named MFCC-2 that handles the large and uneven dimensionality of MFCC has been used to characterize languages in the thick of English, Bangla and Hindi. The system has been tested with recordings of as many as 12,000 utterances of numerals and 41,884 clips extracted from YouTube videos considering background music, data from multiple environments, avoidance of noise suppression and use of keywords from different languages in a single phrase. The highest and average accuracies (for Top-3 classifiers from a pool of nine classifiers) of 98.09\% and 95.54\%, respectively were achieved for YouTube data.},
  archive      = {J_IJMLC},
  author       = {Mukherjee, Himadri and Obaidullah, Sk Md and Santosh, K. C. and Phadikar, Santanu and Roy, Kaushik},
  doi          = {10.1007/s13042-019-00928-3},
  journal      = {International Journal of Machine Learning and Cybernetics},
  number       = {1},
  pages        = {1-14},
  shortjournal = {Int. J. Mach. Learn. Cybern.},
  title        = {A lazy learning-based language identification from speech using MFCC-2 features},
  volume       = {11},
  year         = {2020},
}
</textarea>
</details></li>
</ul>

</body>
</html>
