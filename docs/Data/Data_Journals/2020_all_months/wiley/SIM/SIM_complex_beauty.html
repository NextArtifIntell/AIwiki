<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>SIM_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="sim---308">SIM - 308</h2>
<ul>
<li><details>
<summary>
(2020). Formulating causal questions and principled statistical
answers. <em>SIM</em>, <em>39</em>(30), 4922–4948. (<a
href="https://doi.org/10.1002/sim.8741">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Although review papers on causal inference methods are now available, there is a lack of introductory overviews on what they can render and on the guiding criteria for choosing one particular method. This tutorial gives an overview in situations where an exposure of interest is set at a chosen baseline (“point exposure”) and the target outcome arises at a later time point. We first phrase relevant causal questions and make a case for being specific about the possible exposure levels involved and the populations for which the question is relevant. Using the potential outcomes framework, we describe principled definitions of causal effects and of estimation approaches classified according to whether they invoke the no unmeasured confounding assumption (including outcome regression and propensity score-based methods) or an instrumental variable with added assumptions. We mainly focus on continuous outcomes and causal average treatment effects. We discuss interpretation, challenges, and potential pitfalls and illustrate application using a “simulation learner,” that mimics the effect of various breastfeeding interventions on a child&#39;s later development. This involves a typical simulation component with generated exposure, covariate, and outcome data inspired by a randomized intervention study. The simulation learner further generates various (linked) exposure types with a set of possible values per observation unit, from which observed as well as potential outcome data are generated. It thus provides true values of several causal effects. R code for data generation and analysis is available on www.ofcaus.org , where SAS and Stata code for analysis is also provided.},
  archive      = {J_SIM},
  author       = {Els Goetghebeur and Saskia le Cessie and Bianca De Stavola and Erica EM Moodie and Ingeborg Waernbaum},
  doi          = {10.1002/sim.8741},
  journal      = {Statistics in Medicine},
  month        = {12},
  number       = {30},
  pages        = {4922-4948},
  shortjournal = {Stat. Med.},
  title        = {Formulating causal questions and principled statistical answers},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Optimal multiwave sampling for regression modeling in
two-phase designs. <em>SIM</em>, <em>39</em>(30), 4912–4921. (<a
href="https://doi.org/10.1002/sim.8760">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Two-phase designs involve measuring extra variables on a subset of the cohort where some variables are already measured. The goal of two-phase designs is to choose a subsample of individuals from the cohort and analyse that subsample efficiently. It is of interest to obtain an optimal design that gives the most efficient estimates of regression parameters. In this article, we propose a multiwave sampling design to approximate the optimal design for design-based estimators. Influence functions are used to compute the optimal sampling allocations. We propose to use informative priors on regression parameters to derive the wave-1 sampling probabilities because any prespecified sampling probabilities may be far from optimal and decrease the design efficiency. The posterior distributions of the regression parameters derived from the current wave will then be used as priors for the next wave. Generalized raking is used in the final statistical analysis. We show that a two-wave sampling with reasonable informative priors will end up with a highly efficient estimation for the parameter of interest and be close to the underlying optimal design.},
  archive      = {J_SIM},
  author       = {Tong Chen and Thomas Lumley},
  doi          = {10.1002/sim.8760},
  journal      = {Statistics in Medicine},
  month        = {12},
  number       = {30},
  pages        = {4912-4921},
  shortjournal = {Stat. Med.},
  title        = {Optimal multiwave sampling for regression modeling in two-phase designs},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Assessing the performance of population adjustment methods
for anchored indirect comparisons: A simulation study. <em>SIM</em>,
<em>39</em>(30), 4885–4911. (<a
href="https://doi.org/10.1002/sim.8759">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Standard network meta-analysis and indirect comparisons combine aggregate data from multiple studies on treatments of interest, assuming that any factors that interact with treatment effects (effect modifiers) are balanced across populations. Population adjustment methods such as multilevel network meta-regression (ML-NMR), matching-adjusted indirect comparison (MAIC), and simulated treatment comparison (STC) relax this assumption using individual patient data from one or more studies, and are becoming increasingly prevalent in health technology appraisals and the applied literature. Motivated by an applied example and two recent reviews of applications, we undertook an extensive simulation study to assess the performance of these methods in a range of scenarios under various failures of assumptions. We investigated the impact of varying sample size, missing effect modifiers, strength of effect modification and validity of the shared effect modifier assumption, validity of extrapolation and varying between-study overlap, and different covariate distributions and correlations. ML-NMR and STC performed similarly, eliminating bias when the requisite assumptions were met. Serious concerns are raised for MAIC, which performed poorly in nearly all simulation scenarios and may even increase bias compared with standard indirect comparisons. All methods incur bias when an effect modifier is missing, highlighting the necessity of careful selection of potential effect modifiers prior to analysis. When all effect modifiers are included, ML-NMR and STC are robust techniques for population adjustment. ML-NMR offers additional advantages over MAIC and STC, including extending to larger treatment networks and producing estimates in any target population, making this an attractive choice in a variety of scenarios.},
  archive      = {J_SIM},
  author       = {David M. Phillippo and Sofia Dias and A. E. Ades and Nicky J. Welton},
  doi          = {10.1002/sim.8759},
  journal      = {Statistics in Medicine},
  month        = {12},
  number       = {30},
  pages        = {4885-4911},
  shortjournal = {Stat. Med.},
  title        = {Assessing the performance of population adjustment methods for anchored indirect comparisons: A simulation study},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). High-dimensional integrative copula discriminant analysis
for multiomics data. <em>SIM</em>, <em>39</em>(30), 4869–4884. (<a
href="https://doi.org/10.1002/sim.8758">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multiomics or integrative omics data have been increasingly common in biomedical studies, holding a promise in better understanding human health and disease. In this article, we propose an integrative copula discrimination analysis classifier in the context of two-class classification, which relaxes the common Gaussian assumption and gains power by borrowing information from multiple omics data types in discriminant analysis. Numerical studies are conducted to assess the finite sample performance of the new classifier. We apply our model to the Religious Orders Study and Memory and Aging Project (ROSMAP) Study, integrating gene expression and DNA methylation data for better prediction.},
  archive      = {J_SIM},
  author       = {Yong He and Hao Chen and Hao Sun and Jiadong Ji and Yufeng Shi and Xinsheng Zhang and Lei Liu},
  doi          = {10.1002/sim.8758},
  journal      = {Statistics in Medicine},
  month        = {12},
  number       = {30},
  pages        = {4869-4884},
  shortjournal = {Stat. Med.},
  title        = {High-dimensional integrative copula discriminant analysis for multiomics data},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Bayesian inference using hamiltonian monte-carlo algorithm
for nonlinear joint modeling in the context of cancer immunotherapy.
<em>SIM</em>, <em>39</em>(30), 4853–4868. (<a
href="https://doi.org/10.1002/sim.8756">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Treatment evaluation in advanced cancer mainly relies on overall survival and tumor size dynamics. Both markers and their association can be simultaneously analyzed by using joint models, and these approaches are supported by many softwares or packages. However, these approaches are essentially limited to linear models for the longitudinal part, which limit their biological interpretation. More biological models of tumor dynamics can be obtained by using nonlinear models, but they are limited by the fact that parameter identifiability require rich dataset. In that context Bayesian approaches are particularly suited to incorporate the biological knowledge and increase the information available, but they are limited by the high computing cost of Monte-Carlo by Markov Chains algorithms. Here, we aimed to assess the performances of the Hamiltonian Monte-Carlo (HMC) algorithm implemented in Stan for inference in a nonlinear joint model. The method was validated on simulated data where HMC provided proper posterior distributions and credibility intervals in a reasonable computational time. Then the association between tumor size dynamics and survival was assessed in patients with advanced or metastatic bladder cancer treated with atezolizumab, an immunotherapy agent. HMC confirmed limited sensitivity to prior distributions. A cross-validation approach was developed and identified the current slope of tumor size dynamics as the most relevant driver of survival. In summary, HMC is an efficient approach to perform nonlinear joint models in a Bayesian framework, and opens the way for the use of nonlinear models to characterize both the rapid dynamics and the intersubject variability observed during cancer immunotherapy treatment.},
  archive      = {J_SIM},
  author       = {Marion Kerioui and Francois Mercier and Julie Bertrand and Coralie Tardivon and René Bruno and Jérémie Guedj and Solène Desmée},
  doi          = {10.1002/sim.8756},
  journal      = {Statistics in Medicine},
  month        = {12},
  number       = {30},
  pages        = {4853-4868},
  shortjournal = {Stat. Med.},
  title        = {Bayesian inference using hamiltonian monte-carlo algorithm for nonlinear joint modeling in the context of cancer immunotherapy},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Bayesian decision-theoretic methods for survival data using
stochastic optimization. <em>SIM</em>, <em>39</em>(30), 4841–4852. (<a
href="https://doi.org/10.1002/sim.8755">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce a principled method for Bayesian subgroup analysis. The approach is based on casting subgroup analysis as a Bayesian decision problem. The two main innovations are: (1) the explicit consideration of a “subgroup report,” comprising multiple subpopulations; and (2) adapting an inhomogeneous Markov chain Monte Carlo simulation scheme to implement stochastic optimization. The latter makes the search for “subgroup reports” practically feasible.},
  archive      = {J_SIM},
  author       = {Riten Mitra and Peter Müller and Arinjita Bhattacharyya},
  doi          = {10.1002/sim.8755},
  journal      = {Statistics in Medicine},
  month        = {12},
  number       = {30},
  pages        = {4841-4852},
  shortjournal = {Stat. Med.},
  title        = {Bayesian decision-theoretic methods for survival data using stochastic optimization},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A pilot design for observational studies: Using abundant
data thoughtfully. <em>SIM</em>, <em>39</em>(30), 4821–4840. (<a
href="https://doi.org/10.1002/sim.8754">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Observational studies often benefit from an abundance of observational units. This can lead to studies that—while challenged by issues of internal validity—have inferences derived from sample sizes substantially larger than randomized controlled trials. But is the information provided by an observational unit best used in the analysis phase? We propose the use of a “pilot design,” in which observations are expended in the design phase of the study, and the posttreatment information from these observations is used to improve study design. In modern observational studies, which are data rich but control poor, pilot designs can be used to gain information about the structure of posttreatment variation. This information can then be used to improve instrumental variable designs, propensity score matching, doubly robust estimation, and other observational study designs. We illustrate one version of a pilot design, which aims to reduce within-set heterogeneity and improve performance in sensitivity analyses. This version of a pilot design expends observational units during the design phase to fit a prognostic model, avoiding concerns of overfitting. In addition, it enables the construction of “assignment-control plots,” which visualize the relationship between propensity and prognostic scores. We first show some examples of these plots, then we demonstrate in a simulation setting how this alternative use of the observations can lead to gains in terms of both treatment effect estimation and sensitivity analyses of unobserved confounding.},
  archive      = {J_SIM},
  author       = {Rachael C. Aikens and Dylan Greaves and Michael Baiocchi},
  doi          = {10.1002/sim.8754},
  journal      = {Statistics in Medicine},
  month        = {12},
  number       = {30},
  pages        = {4821-4840},
  shortjournal = {Stat. Med.},
  title        = {A pilot design for observational studies: Using abundant data thoughtfully},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Direct estimation of the area under the receiver operating
characteristic curve with verification biased data. <em>SIM</em>,
<em>39</em>(30), 4789–4820. (<a
href="https://doi.org/10.1002/sim.8753">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In medical diagnostic studies, verification of the true disease status might be partially missing based on results of diagnostic tests and other characteristics of subjects. Because estimates of area under the ROC curve (AUC) based on partially validated subjects are usually biased, it is usually necessary to estimate AUC from a bias-corrected ROC curve. In this article, various direct estimation methods of the AUC based on hybrid imputation [full imputations and mean score imputation (MSI)], inverse probability weighting, and the semiparametric efficient (SPE) approach are proposed and compared in the presence of verification bias when the test result is continuous under the assumption that the true disease status, if missing, is missing at random. Simulation results show that the proposed estimators are accurate for the biased sampling if the disease and verification models are correctly specified. The SPE and MSI based estimators perform well even under the misspecified disease/verification models. Numerical studies are performed to compare the finite sample performance of the proposed approaches with existing methods. A real dataset of neonatal hearing screening study is analyzed.},
  archive      = {J_SIM},
  author       = {Yan Hai and Gengsheng Qin},
  doi          = {10.1002/sim.8753},
  journal      = {Statistics in Medicine},
  month        = {12},
  number       = {30},
  pages        = {4789-4820},
  shortjournal = {Stat. Med.},
  title        = {Direct estimation of the area under the receiver operating characteristic curve with verification biased data},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Bayesian estimation of multivariate gaussian markov random
fields with constraint. <em>SIM</em>, <em>39</em>(30), 4767–4788. (<a
href="https://doi.org/10.1002/sim.8752">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article concerns with conditionally formulated multivariate Gaussian Markov random fields (MGMRF) for modeling multivariate local dependencies with unknown dependence parameters subject to positivity constraint. In the context of Bayesian hierarchical modeling of lattice data in general and Bayesian disease mapping in particular, analytic and simulation studies provide new insights into various approaches to posterior estimation of dependence parameters under “hard” or “soft” positivity constraint, including the well-known strictly diagonal dominance criterion and options of hierarchical priors. Hierarchical centering is examined as a means to gain computational efficiency in Bayesian estimation of multivariate generalized linear mixed effects models in the presence of spatial confounding and weakly identified model parameters. Simulated data on irregular or regular lattice, and three datasets from the multivariate and spatiotemporal disease mapping literature, are used for illustration. The present investigation also sheds light on the use of deviance information criterion for model comparison, choice, and interpretation in the context of posterior risk predictions judged by borrowing-information and bias-precision tradeoff. The article concludes with a summary discussion and directions of future work. Potential applications of MGMRF in spatial information fusion and image analysis are briefly mentioned.},
  archive      = {J_SIM},
  author       = {Ying C. MacNab},
  doi          = {10.1002/sim.8752},
  journal      = {Statistics in Medicine},
  month        = {12},
  number       = {30},
  pages        = {4767-4788},
  shortjournal = {Stat. Med.},
  title        = {Bayesian estimation of multivariate gaussian markov random fields with constraint},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Bayesian learning of multiple directed networks from
observational data. <em>SIM</em>, <em>39</em>(30), 4745–4766. (<a
href="https://doi.org/10.1002/sim.8751">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graphical modeling represents an established methodology for identifying complex dependencies in biological networks, as exemplified in the study of co-expression, gene regulatory, and protein interaction networks. The available observations often exhibit an intrinsic heterogeneity, which impacts on the network structure through the modification of specific pathways for distinct groups, such as disease subtypes. We propose to infer the resulting multiple graphs jointly in order to benefit from potential similarities across groups; on the other hand our modeling framework is able to accommodate group idiosyncrasies. We consider directed acyclic graphs (DAGs) as network structures, and develop a Bayesian method for structural learning of multiple DAGs. We explicitly account for Markov equivalence of DAGs, and propose a suitable prior on the collection of graph spaces that induces selective borrowing strength across groups. The resulting inference allows in particular to compute the posterior probability of edge inclusion, a useful summary for representing flow directions within the network. Finally, we detail a simulation study addressing the comparative performance of our method, and present an analysis of two protein networks together with a substantive interpretation of our findings.},
  archive      = {J_SIM},
  author       = {Federico Castelletti and Luca La Rocca and Stefano Peluso and Francesco C. Stingo and Guido Consonni},
  doi          = {10.1002/sim.8751},
  journal      = {Statistics in Medicine},
  month        = {12},
  number       = {30},
  pages        = {4745-4766},
  shortjournal = {Stat. Med.},
  title        = {Bayesian learning of multiple directed networks from observational data},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Survival analysis using a 5-step stratified testing and
amalgamation routine (5-STAR) in randomized clinical trials.
<em>SIM</em>, <em>39</em>(30), 4724–4744. (<a
href="https://doi.org/10.1002/sim.8750">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Randomized clinical trials are often designed to assess whether a test treatment prolongs survival relative to a control treatment. Increased patient heterogeneity, while desirable for generalizability of results, can weaken the ability of common statistical approaches to detect treatment differences, potentially hampering the regulatory approval of safe and efficacious therapies. A novel solution to this problem is proposed. A list of baseline covariates that have the potential to be prognostic for survival under either treatment is pre-specified in the analysis plan. At the analysis stage, using all observed survival times but blinded to patient-level treatment assignment, “noise” covariates are removed with elastic net Cox regression. The shortened covariate list is used by a conditional inference tree algorithm to segment the heterogeneous trial population into subpopulations of prognostically homogeneous patients (risk strata). After patient-level treatment unblinding, a treatment comparison is done within each formed risk stratum and stratum-level results are combined for overall statistical inference. The impressive power-boosting performance of our proposed 5-step stratified testing and amalgamation routine (5-STAR), relative to that of the logrank test and other common approaches that do not leverage inherently structured patient heterogeneity, is illustrated using a hypothetical and two real datasets along with simulation results. Furthermore, the importance of reporting stratum-level comparative treatment effects (time ratios from accelerated failure time model fits in conjunction with model averaging and, as needed, hazard ratios from Cox proportional hazard model fits) is highlighted as a potential enabler of personalized medicine. An R package is available at https://github.com/rmarceauwest/fiveSTAR .},
  archive      = {J_SIM},
  author       = {Devan V. Mehrotra and Rachel Marceau West},
  doi          = {10.1002/sim.8750},
  journal      = {Statistics in Medicine},
  month        = {12},
  number       = {30},
  pages        = {4724-4744},
  shortjournal = {Stat. Med.},
  title        = {Survival analysis using a 5-step stratified testing and amalgamation routine (5-STAR) in randomized clinical trials},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Tumor heterogeneity estimation for radiomics in cancer.
<em>SIM</em>, <em>39</em>(30), 4704–4723. (<a
href="https://doi.org/10.1002/sim.8749">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Radiomics is an emerging field of medical image analysis research where quantitative measurements are obtained from radiological images that can be utilized to predict patient outcomes and inform treatment decisions. Cancer patients routinely undergo radiological evaluations when images of various modalities including computed tomography, positron emission tomography, and magnetic resonance images are collected for diagnosis and for evaluation of disease progression. Tumor characteristics, often referred to as measures of tumor heterogeneity , can be computed using these clinical images and used as predictors of disease progression and patient survival. Several approaches for quantifying tumor heterogeneity have been proposed, including intensity histogram-based measures, shape and volume-based features, and texture analysis. Taking into account the topology of the tumors we propose a statistical framework for estimating tumor heterogeneity using clustering based on Markov random field theory. We model the voxel intensities using a Gaussian mixture model using a Gibbs prior to incorporate voxel neighborhood information. We propose a novel approach to choosing the number of mixture components. Subsequently, we show that the proposed procedure outperforms the existing approaches when predicting lung cancer survival.},
  archive      = {J_SIM},
  author       = {Ani Eloyan and Mun Sang Yue and Davit Khachatryan},
  doi          = {10.1002/sim.8749},
  journal      = {Statistics in Medicine},
  month        = {12},
  number       = {30},
  pages        = {4704-4723},
  shortjournal = {Stat. Med.},
  title        = {Tumor heterogeneity estimation for radiomics in cancer},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Modeling daily and weekly moderate and vigorous physical
activity using zero-inflated mixture poisson distribution. <em>SIM</em>,
<em>39</em>(30), 4687–4703. (<a
href="https://doi.org/10.1002/sim.8748">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently developed accelerometer devices have been used in large epidemiological studies for continuous and objective monitoring of physical activities. Typically, physical movements are summarized as minutes in light, moderate, and vigorous physical activities in each wearing day. Because of preponderance of zeros, zero-inflated distributions have been used for modeling the daily moderate or higher levels of physical activity. Yet, these models do not fully account for variations in daily physical activity and cannot be extended to model weekly physical activity explicitly, while the weekly physical activity is considered as an indicator for a subject&#39;s average level of physical activity. To overcome these limitations, we propose to use a zero-inflated Poisson mixture distribution that can model daily and weekly physical activity in same family of mixture distributions. Under this method, the likelihood of an inactive day and the amount of exercise in an active day are simultaneously modeled by a joint random effects model to incorporate heterogeneity across participants. If needed, the method has the flexibility to include an additional random effect to address extra variations in daily physical activity. Maximum likelihood estimation can be obtained through Gaussian quadrature technique, which is implemented conveniently in an R package GLMMadaptive . Method performances are examined using simulation studies. The method is applied to data from the Hispanic Community Health Study/Study of Latinos to examine the relationship between physical activity and BMI groups and within a participant the difference in physical activity between weekends and weekdays.},
  archive      = {J_SIM},
  author       = {Xiaonan Xue and Qibin Qi and Daniela Sotres-Alvarez and Scott C. Roesch and Maria M. Llabre and Sierra A. Bainter and Yasmin Mossavar-Rahmani and Robert Kaplan and Tao Wang},
  doi          = {10.1002/sim.8748},
  journal      = {Statistics in Medicine},
  month        = {12},
  number       = {30},
  pages        = {4687-4703},
  shortjournal = {Stat. Med.},
  title        = {Modeling daily and weekly moderate and vigorous physical activity using zero-inflated mixture poisson distribution},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Estimating the sample variance from the sample size and
range. <em>SIM</em>, <em>39</em>(30), 4667–4686. (<a
href="https://doi.org/10.1002/sim.8747">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For meta-analysis studies and systematic reviews, it is important to pool the data from a set of similar clinical trials. To pool the data, one needs to know their SD. Many trial reports, however, contain only the median, the minimum and maximum values, and the sample size. It is therefore important to be able to estimate the SD S from the sample size n and range r . For small n ≤ 100 , we improve existing estimators of r / S , the “divisor,” denoted by ξ ⁡ ( n ) . This in turn yields improved estimators of the SD in the form S ^ = r / ξ ^ ⁡ ( n ) on simulated as well as real datasets. We provide numerical values of the proposed estimator as well as approximation by a simple formula 3 ⁢ ln ⁡ ( n ) − 1 . 4025 . Furthermore, for large n , we provide estimators ξ ^ ⁡ ( n ) of the divisor ξ ⁡ ( n ) for the normal, exponential, and other bounded and unbounded distributions.},
  archive      = {J_SIM},
  author       = {Jan Rychtář and Dewey T. Taylor},
  doi          = {10.1002/sim.8747},
  journal      = {Statistics in Medicine},
  month        = {12},
  number       = {30},
  pages        = {4667-4686},
  shortjournal = {Stat. Med.},
  title        = {Estimating the sample variance from the sample size and range},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A simulation-free approach to assessing the performance of
the continual reassessment method. <em>SIM</em>, <em>39</em>(30),
4651–4666. (<a href="https://doi.org/10.1002/sim.8746">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The continual reassessment method (CRM) is an adaptive design for Phase I trials whose operating characteristics, including appropriate sample size, probability of correctly identifying the maximum tolerated dose, and the expected proportion of participants assigned to each dose, can only be determined via simulation. The actual time to determine a final “best” design can take several hours or days, depending on the number of scenarios that are examined. The computational cost increases as the kernel of the one-parameter CRM design is expanded to other settings, including additional parameters, monitoring of both toxicity and efficacy, and studies of combinations of two agents. For a given vector of true DLT probabilities, we have developed an approach that replaces a simulation study of thousands of hypothetical trials with a single simulation. Our approach, which is founded on the consistency of the CRM, very accurately reflects the results produced by the simulation study, but does so in a fraction of time required by the simulation study. Relative to traditional simulations, we extensively examine how our method is able to assess the operating characteristics of a CRM design for a hypothetical trial whose characteristics are based upon a previously published Phase I trial. We also provide a metric of nonconsistency and demonstrate that although nonconsistency can impact the operating characteristics of our method, the degree of over- or under-estimation is unpredictable. As a solution, we provide an algorithm for maintaining the consistency of a chosen CRM design so that our method is applicable for any trial.},
  archive      = {J_SIM},
  author       = {Thomas M. Braun},
  doi          = {10.1002/sim.8746},
  journal      = {Statistics in Medicine},
  month        = {12},
  number       = {30},
  pages        = {4651-4666},
  shortjournal = {Stat. Med.},
  title        = {A simulation-free approach to assessing the performance of the continual reassessment method},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Extended multi-item gamma poisson shrinker methods based on
the zero-inflated poisson model for postmarket drug safety surveillance.
<em>SIM</em>, <em>39</em>(30), 4636–4650. (<a
href="https://doi.org/10.1002/sim.8745">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Bayesian signal detection methods, including the multiitem gamma Poisson shrinker (MGPS), assume a Poisson distribution for the number of reports. However, the database of the adverse event reporting system often has a large number of zero-count cells. A zero-inflated Poisson (ZIP) distribution can be more appropriate in this situation than a Poisson distribution. Few studies have considered ZIP-based models for Bayesian signal detection. In addition, most studies on Bayesian signal detection methods include simulation studies conducted assuming a gamma distribution for the prior. Herein, we extend the MGPS method using the ZIP model and apply various prior distributions. We evaluated the extended methods through an extensive simulation using more varied settings for the model and prior than existing methods. We varied the total number of reports, the number of true signals, the relative reporting rate, and the probability of observing a true zero. The results show that as the probability of observing a zero count increased, methods based on the ZIP model outperformed the Poisson model in most cases. We also found that using the mixture log-normal prior resulted in more conservative detection than other priors when the relative reporting rate is high. Conversely, more signals were found when using the mixture truncated normal distributions. We applied the Bayesian signal detection methods to data from the Korea Adverse Event Reporting System from 2012 to 2016.},
  archive      = {J_SIM},
  author       = {Seok-Jae Heo and Inkyung Jung},
  doi          = {10.1002/sim.8745},
  journal      = {Statistics in Medicine},
  month        = {12},
  number       = {30},
  pages        = {4636-4650},
  shortjournal = {Stat. Med.},
  title        = {Extended multi-item gamma poisson shrinker methods based on the zero-inflated poisson model for postmarket drug safety surveillance},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Comparing kaplan-meier curves with the probability of
agreement. <em>SIM</em>, <em>39</em>(30), 4621–4635. (<a
href="https://doi.org/10.1002/sim.8744">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The probability of agreement has been used as an effective strategy for quantifying the similarity between the reliability of two populations. By contrast to hypothesis testing approaches based on P -values, the probability of agreement provides a more realistic assessment of similarity by emphasizing practically important differences. In this article, we propose the use of the probability of agreement to evaluate the similarity of two Kaplan-Meier curves, which estimate the survival functions in two populations. This article extends the probability of agreement paradigm to right censored data and explores three different methods of quantifying uncertainty in the probability of agreement estimate. The first approach provides a convenient assessment based on large-sample normal-theory (LSNT), while the other two approaches are nonparametric alternatives based on ordinary and fractional random-weight bootstrap (FRWB) techniques. All methods are illustrated with examples for which comparing the survival curves of related populations is of interest and the efficacy of the methods are also evaluated through simulation studies. Based on these simulations we recommend point estimation using the proposed LSNT calculation and confidence interval estimation via the FRWB approach. We also provide a Shiny app that facilitates an automated implementation of the methodology.},
  archive      = {J_SIM},
  author       = {Nathaniel T. Stevens and Lu Lu},
  doi          = {10.1002/sim.8744},
  journal      = {Statistics in Medicine},
  month        = {12},
  number       = {30},
  pages        = {4621-4635},
  shortjournal = {Stat. Med.},
  title        = {Comparing kaplan-meier curves with the probability of agreement},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Genome-wide association study-based deep learning for
survival prediction. <em>SIM</em>, <em>39</em>(30), 4605–4620. (<a
href="https://doi.org/10.1002/sim.8743">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Informative and accurate survival prediction with individualized dynamic risk profiles over time is critical for personalized disease prevention and clinical management. The massive genetic data, such as SNPs from genome-wide association studies (GWAS), together with well-characterized time-to-event phenotypes provide unprecedented opportunities for developing effective survival prediction models. Recent advances in deep learning have made extraordinary achievements in establishing powerful prediction models in the biomedical field. However, the applications of deep learning approaches in survival prediction are limited, especially with utilizing the wealthy GWAS data. Motivated by developing powerful prediction models for the progression of an eye disease, age-related macular degeneration (AMD), we develop and implement a multilayer deep neural network (DNN) survival model to effectively extract features and make accurate and interpretable predictions. Various simulation studies are performed to compare the prediction performance of the DNN survival model with several other machine learning-based survival models. Finally, using the GWAS data from two large-scale randomized clinical trials in AMD with over 7800 observations, we show that the DNN survival model not only outperforms several existing survival prediction models in terms of prediction accuracy (eg, c-index =0.76 ), but also successfully detects clinically meaningful risk subgroups by effectively learning the complex structures among genetic variants. Moreover, we obtain a subject-specific importance measure for each predictor from the DNN survival model, which provides valuable insights into the personalized early prevention and clinical management for this disease.},
  archive      = {J_SIM},
  author       = {Tao Sun and Yue Wei and Wei Chen and Ying Ding},
  doi          = {10.1002/sim.8743},
  journal      = {Statistics in Medicine},
  month        = {12},
  number       = {30},
  pages        = {4605-4620},
  shortjournal = {Stat. Med.},
  title        = {Genome-wide association study-based deep learning for survival prediction},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Understanding and adjusting for the selection bias from a
proof-of-concept study to a more confirmatory study. <em>SIM</em>,
<em>39</em>(30), 4593–4604. (<a
href="https://doi.org/10.1002/sim.8740">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {It has long been noticed that the efficacy observed in small early phase studies is generally better than that observed in later larger studies. Historically, the inflation of the efficacy results from early proof-of-concept studies is either ignored, or adjusted empirically using a frequentist or Bayesian approach. In this article, we systematically explained the underlying reason for the inflation of efficacy results in small early phase studies from the perspectives of measurement error models and selection bias. A systematic method was built to adjust the early phase study results from both frequentist and Bayesian perspectives. A hierarchical model was proposed to estimate the distribution of the efficacy for a portfolio of compounds, which can serve as the prior distribution for the Bayesian approach. We showed through theory that the systematic adjustment provides an unbiased estimator for the true mean efficacy for a portfolio of compounds. The adjustment was applied to paired data for the efficacy in early small and later larger studies for a set of compounds in diabetes and immunology. After the adjustment, the bias in the early phase small studies seems to be diminished.},
  archive      = {J_SIM},
  author       = {Yongming Qu and Yu Du and Ying Zhang and Lei Shen},
  doi          = {10.1002/sim.8740},
  journal      = {Statistics in Medicine},
  month        = {12},
  number       = {30},
  pages        = {4593-4604},
  shortjournal = {Stat. Med.},
  title        = {Understanding and adjusting for the selection bias from a proof-of-concept study to a more confirmatory study},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Two-part models for repeatedly measured ordinal data with
“don’t know” category. <em>SIM</em>, <em>39</em>(30), 4574–4592. (<a
href="https://doi.org/10.1002/sim.8739">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ordinal data (eg, “low,” “medium,” “high”; graded response on a Likert scale) with an additional “don&#39;t know” category are frequently encountered in the medical, social, and behavioral science literature. The handling of a “don&#39;t know” option presents unique challenges as it often “destroys” the ordinal nature of the data. Commonly, nominal models are employed which ignore the partial ordering and have a complicated interpretation, especially in situations with repeatedly measured outcomes. We propose two-part models that easily accommodate longitudinal partially ordered (semiordinal) data. The most easily interpretable formulation consists of a random effect logistic submodel for “don&#39;t know” vs all the other categories combined, and a random effect ordinal submodel for the ordered categories. Correlated random effects account for statistical dependence within individual. An extension allowing for nonproportionality of odds for the predictor effects in the ordinal submodel is also considered. Maximum likelihood estimation is performed using adaptive Gaussian quadrature in SAS PROC NLMIXED. A simulation study is performed to evaluate the performance of the estimation algorithm in terms of bias and efficiency, and to compare the results of joint and separate models of the two parts, and of proportional and nonproportional model formulations. The methods are motivated and illustrated on a dataset from a study of adolescents&#39; perceptions of nicotine strength of JUUL e-cigarettes. Using the proposed approach we show that adolescents perceive 5% nicotine content as relatively low, a misconception more pronounced among past month nonusers than among past month users of JUUL e-cigarettes.},
  archive      = {J_SIM},
  author       = {Ralitza Gueorguieva and Eugenia Buta and Meghan Morean and Suchitra Krishnan-Sarin},
  doi          = {10.1002/sim.8739},
  journal      = {Statistics in Medicine},
  month        = {12},
  number       = {30},
  pages        = {4574-4592},
  shortjournal = {Stat. Med.},
  title        = {Two-part models for repeatedly measured ordinal data with “don&#39;t know” category},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Assessing consistency in clinical trials with two subgroups
and binary endpoints: A new test within the logistic regression model.
<em>SIM</em>, <em>39</em>(30), 4551–4573. (<a
href="https://doi.org/10.1002/sim.8719">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In late stage drug development, the experimental drug is tested in a diverse study population within the relevant indication. In order to receive marketing authorization, robust evidence for the therapeutic efficacy is crucial requiring investigation of treatment effects in well-defined subgroups. Conventionally, consistency analyses in subgroups have been performed by means of interaction tests. However, the interaction test can only reject the null hypothesis of equivalence and not confirm consistency. Simulation studies suggest that the interaction test has low power but can also be oversensitive depending on sample size—leading in combination with the actually ill-posed null hypothesis to findings regardless of clinical relevance. In order to overcome these disadvantages in the setup of binary endpoints, we propose to use a consistency test based on the interval inclusion principle, which is able to reject heterogeneity and confirm consistency of subgroup-specific treatment effects while controlling the type I error. This homogeneity test is based upon the deviation between overall treatment effect and subgroup-specific effects on the odds ratio scale and is compared with an equivalence test based on the ratio of both subgroup-specific effects. Performance of these consistency tests is assessed in a simulation study. In addition, the consistency tests are outlined for the relative risk regression. The proposed homogeneity test reaches sufficient power in realistic scenarios with small interactions. As expected, power decreases for unbalanced subgroups, lower sample sizes, and narrower margins. Severe interactions are covered by the null hypothesis and are more likely to be rejected the stronger they are.},
  archive      = {J_SIM},
  author       = {Susann Grill and Arne Ring and Werner Brannath and Martin Scharpenberg},
  doi          = {10.1002/sim.8719},
  journal      = {Statistics in Medicine},
  month        = {12},
  number       = {30},
  pages        = {4551-4573},
  shortjournal = {Stat. Med.},
  title        = {Assessing consistency in clinical trials with two subgroups and binary endpoints: A new test within the logistic regression model},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A tutorial on dealing with time-varying eligibility for
treatment: Comparing the risk of major bleeding with direct-acting oral
anticoagulants vs warfarin. <em>SIM</em>, <em>39</em>(29), 4538–4550.
(<a href="https://doi.org/10.1002/sim.8715">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this tutorial, we focus on the problem of how to define and estimate treatment effects when some patients develop a contraindication and are thus ineligible to receive a treatment of interest during follow-up. We first describe the concept of positivity, which is the requirement that all subjects in an analysis be eligible for all treatments of interest conditional on their baseline covariates, and the extension of this concept in the longitudinal treatment setting. We demonstrate using simulated datasets and regression analysis that under violations of longitudinal positivity, typical associational estimates between treatment over time and the outcome of interest may be misleading depending on the data-generating structure. Finally, we explain how one may define “treatment strategies,” such as “treat with medication unless contraindicated,” to overcome the problems linked to time-varying eligibility. Finally, we show how contrasts between the expected potential outcomes under these strategies may be consistently estimated with inverse probability weighting methods. We provide R code for all the analyses described.},
  archive      = {J_SIM},
  author       = {Mireille E. Schnitzer and Robert W. Platt and Madeleine Durand},
  doi          = {10.1002/sim.8715},
  journal      = {Statistics in Medicine},
  month        = {12},
  number       = {29},
  pages        = {4538-4550},
  shortjournal = {Stat. Med.},
  title        = {A tutorial on dealing with time-varying eligibility for treatment: Comparing the risk of major bleeding with direct-acting oral anticoagulants vs warfarin},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Meta-analysis of quantile intervals from different studies
with an application to a pulmonary tuberculosis data. <em>SIM</em>,
<em>39</em>(29), 4519–4537. (<a
href="https://doi.org/10.1002/sim.8738">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {After the completion of many studies, experimental results are reported in terms of distribution-free confidence intervals that may involve pairs of order statistics. This article considers a meta-analysis procedure to combine these confidence intervals from independent studies to estimate or construct a confidence interval for the true quantile of the population distribution. Data synthesis is made under both fixed-effect and random-effect meta-analysis models. We show that mean square error (MSE) of the combined quantile estimator is considerably smaller than that of the best individual quantile estimator. We also show that the coverage probability of the meta-analysis confidence interval is quite close to the nominal confidence level. The random-effect meta-analysis model yields a better coverage probability and smaller MSE than the fixed-effect meta-analysis model. The meta-analysis method is then used to synthesize medians of patient delays in pulmonary tuberculosis diagnosis in China to provide an illustration of the proposed methodology.},
  archive      = {J_SIM},
  author       = {Omer Ozturk and Narayanaswamy Balakrishnan},
  doi          = {10.1002/sim.8738},
  journal      = {Statistics in Medicine},
  month        = {12},
  number       = {29},
  pages        = {4519-4537},
  shortjournal = {Stat. Med.},
  title        = {Meta-analysis of quantile intervals from different studies with an application to a pulmonary tuberculosis data},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Bayesian adaptive n-of-1 trials for estimating population
and individual treatment effects. <em>SIM</em>, <em>39</em>(29),
4499–4518. (<a href="https://doi.org/10.1002/sim.8737">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article proposes a novel adaptive design algorithm that can be used to find optimal treatment allocations in N-of-1 clinical trials. This new methodology uses two Laplace approximations to provide a computationally efficient estimate of population and individual random effects within a repeated measures, adaptive design framework. Given the efficiency of this approach, it is also adopted for treatment selection to target the collection of data for the precise estimation of treatment effects. To evaluate this approach, we consider both a simulated and motivating N-of-1 clinical trial from the literature. For each trial, our methods were compared with the multiarmed bandit approach and a randomized N-of-1 trial design in terms of identifying the best treatment for each patient and the information gained about the model parameters. The results show that our new approach selects designs that are highly efficient in achieving each of these objectives. As such, we propose our Laplace-based algorithm as an efficient approach for designing adaptive N-of-1 trials.},
  archive      = {J_SIM},
  author       = {Siththara Gedara J. Senarathne and Antony M. Overstall and James M. McGree},
  doi          = {10.1002/sim.8737},
  journal      = {Statistics in Medicine},
  month        = {12},
  number       = {29},
  pages        = {4499-4518},
  shortjournal = {Stat. Med.},
  title        = {Bayesian adaptive N-of-1 trials for estimating population and individual treatment effects},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Poisson item count techniques with noncompliance.
<em>SIM</em>, <em>39</em>(29), 4480–4498. (<a
href="https://doi.org/10.1002/sim.8736">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Poisson item count technique (PICT) is a survey method that was recently developed to elicit respondents&#39; truthful answers to sensitive questions. It simplifies the well-known item count technique (ICT) by replacing a list of independent innocuous questions in known proportions with a single innocuous counting question. However, ICT and PICT both rely on the strong “no design effect assumption” (ie, respondents give the same answers to the innocuous items regardless of the absence or presence of the sensitive item in the list) and “no liar” (ie, all respondents give truthful answers) assumptions. To address the problem of self-protective behavior and provide more reliable analyses, we introduced a noncompliance parameter into the existing PICT. Based on the survey design of PICT, we considered more practical model assumptions and developed the corresponding statistical inferences. Simulation studies were conducted to evaluate the performance of our method. Finally, a real example of automobile insurance fraud was used to demonstrate our method.},
  archive      = {J_SIM},
  author       = {Qin Wu and Man-Lai Tang and Derrick Wing-Hong Fung and Guo-Liang Tian},
  doi          = {10.1002/sim.8736},
  journal      = {Statistics in Medicine},
  month        = {12},
  number       = {29},
  pages        = {4480-4498},
  shortjournal = {Stat. Med.},
  title        = {Poisson item count techniques with noncompliance},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Finite sample variance estimation for optimal dynamic
treatment regimes of survival outcomes. <em>SIM</em>, <em>39</em>(29),
4466–4479. (<a href="https://doi.org/10.1002/sim.8735">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deriving valid confidence intervals for complex estimators is a challenging task in practice. Estimators of dynamic weighted survival modeling (DWSurv), a method to estimate an optimal dynamic treatment regime of censored outcomes, are asymptotically normal and consistent for their target parameters when at least a subset of the nuisance models is correctly specified. However, their behavior in finite samples and the impact of model misspecification on inferences remain unclear. In addition, the estimators&#39; nonregularity may negatively affect the inferences under some specific data generating mechanisms. Our objective was to compare five methods, two asymptotic variance formulas (adjusting or not for the estimation of nuisance parameters) to three bootstrap approaches, to construct confidence intervals for the DWSurv parameters in finite samples. Via simulations, we considered practical scenarios, for example, when some nuisance models are misspecified or when nonregularity is problematic. We also compared the five methods in an application about the treatment of rheumatoid arthritis. We found that the bootstrap approaches performed consistently well at the cost of longer computational times. The asymptotic variance with adjustments generally yielded conservative confidence intervals. The asymptotic variance without adjustments yielded nominal coverages for large sample sizes. We recommend using the asymptotic variance with adjustments in small samples and the bootstrap if computationally feasible. Caution should be taken when nonregularity may be an issue.},
  archive      = {J_SIM},
  author       = {Gabrielle Simoneau and Erica E. M. Moodie and Jagtar S. Nijjar and Robert W. Platt},
  doi          = {10.1002/sim.8735},
  journal      = {Statistics in Medicine},
  month        = {12},
  number       = {29},
  pages        = {4466-4479},
  shortjournal = {Stat. Med.},
  title        = {Finite sample variance estimation for optimal dynamic treatment regimes of survival outcomes},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Bayesian enhancement two-stage design with error control for
phase II clinical trials. <em>SIM</em>, <em>39</em>(29), 4452–4465. (<a
href="https://doi.org/10.1002/sim.8734">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Phase II clinical trials make a critical decision of go or no-go to a subsequent phase III studies. A considerable proportion of promising drugs identified in phase II trials fail the confirmative efficacy test in phase III. Recognizing the low posterior probabilities of H 1 when accepting the drug under Simon&#39;s two-stage design, the Bayesian enhancement two-stage (BET) design is proposed to strengthen the passing criterion. Under the BET design, the lengths of highest posterior density (HPD) intervals, posterior probabilities of H 0 and H 1 are computed to calibrate the design parameters, aiming to improve the stability of the trial characteristics and strengthen the evidence for proceeding the drug development forward. However, from a practical perspective, the HPD interval length lacks transparency and interpretability. To circumvent this problem, we propose the BET design with error control (BETEC) by replacing the HPD interval length with the posterior error rate. The BETEC design can achieve a balance between the posterior false positive rate and false negative rate and, more importantly, it has an intuitive and clear interpretation. We compare our method with the BET design and Simon&#39;s design through extensive simulation studies. As an illustration, we further apply BETEC to two recent clinical trials, and investigate its performance in comparison with other competitive designs. Being both efficient and intuitive, the BETEC design can serve as an alternative toolbox for implementing phase II single-arm trials.},
  archive      = {J_SIM},
  author       = {Huaqing Jin and Guosheng Yin},
  doi          = {10.1002/sim.8734},
  journal      = {Statistics in Medicine},
  month        = {12},
  number       = {29},
  pages        = {4452-4465},
  shortjournal = {Stat. Med.},
  title        = {Bayesian enhancement two-stage design with error control for phase II clinical trials},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). BILITE: A bayesian randomized phase II design for
immunotherapy by jointly modeling the longitudinal immune response and
time-to-event efficacy. <em>SIM</em>, <em>39</em>(29), 4439–4451. (<a
href="https://doi.org/10.1002/sim.8733">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Immunotherapy—treatments that target a patient&#39;s immune system—has attracted considerable attention in cancer research. Its recent success has led to generation of novel immunotherapeutic agents that need to be evaluated in clinical trials. Two unique features of immunotherapy are the immune response and the fact that some patients may achieve long-term durable response. In this article, we propose a two-arm Bayesian adaptive randomized phase II clinical trial design for immunotherapy that jointly models the longitudinal immune response and time-to-event efficacy (BILITE), with a fraction of patients assumed to be cured by the treatment. The longitudinal immune response is modeled using hierarchical nonlinear mixed-effects models with possibly different trajectory patterns for the cured and susceptible groups. Conditional on the immune response trajectory, the time-to-event efficacy data for patients in the susceptible group is modeled via a time-dependent Cox-type regression model. We quantify the desirability of the treatment using a utility function and propose a two-stage design to adaptively randomize patients to treatments and make treatment recommendations at the end of the trial. Simulation studies show that compared with a conventional design that ignores the immune response, BILITE yields superior operating characteristics in terms of the ability to identify promising agents and terminate the trial early for futility.},
  archive      = {J_SIM},
  author       = {Beibei Guo and Yong Zang},
  doi          = {10.1002/sim.8733},
  journal      = {Statistics in Medicine},
  month        = {12},
  number       = {29},
  pages        = {4439-4451},
  shortjournal = {Stat. Med.},
  title        = {BILITE: A bayesian randomized phase II design for immunotherapy by jointly modeling the longitudinal immune response and time-to-event efficacy},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Robust estimation for linear panel data models.
<em>SIM</em>, <em>39</em>(29), 4421–4438. (<a
href="https://doi.org/10.1002/sim.8732">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In different fields of applications including, but not limited to, behavioral, environmental, medical sciences, and econometrics, the use of panel data regression models has become increasingly popular as a general framework for making meaningful statistical inferences. However, when the ordinary least squares (OLS) method is used to estimate the model parameters, presence of outliers may significantly alter the adequacy of such models by producing biased and inefficient estimates. In this work, we propose a new, weighted likelihood based robust estimation procedure for linear panel data models with fixed and random effects. The finite sample performances of the proposed estimators have been illustrated through an extensive simulation study as well as with an application to blood pressure dataset. Our thorough study demonstrates that the proposed estimators show significantly better performances over the traditional methods in the presence of outliers and produce competitive results to the OLS based estimates when no outliers are present in the dataset.},
  archive      = {J_SIM},
  author       = {Beste Hamiye Beyaztas and Soutir Bandyopadhyay},
  doi          = {10.1002/sim.8732},
  journal      = {Statistics in Medicine},
  month        = {12},
  number       = {29},
  pages        = {4421-4438},
  shortjournal = {Stat. Med.},
  title        = {Robust estimation for linear panel data models},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Statistical approaches using longitudinal biomarkers for
disease early detection: A comparison of methodologies. <em>SIM</em>,
<em>39</em>(29), 4405–4420. (<a
href="https://doi.org/10.1002/sim.8731">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Early detection of clinical outcomes such as cancer may be predicted using longitudinal biomarker measurements. Tracking longitudinal biomarkers as a way to identify early disease onset may help to reduce mortality from diseases like ovarian cancer that are more treatable if detected early. Two disease risk prediction frameworks, the shared random effects model (SREM) and the pattern mixture model (PMM) could be used to assess longitudinal biomarkers on disease early detection. In this article, we studied the discrimination and calibration performances of SREM and PMM on disease early detection through an application to ovarian cancer, where early detection using the risk of ovarian cancer algorithm (ROCA) has been evaluated. Comparisons of the above three approaches were performed via analyses of the ovarian cancer data from the Prostate, Lung, Colorectal, and Ovarian Cancer Screening Trial. Discrimination was evaluated by the time-dependent receiver operating characteristic curve and its area, while calibration was assessed using calibration plot and the ratio of observed to expected number of diseased subjects. The out-of-sample performances were calculated via using leave-one-out cross-validation, aiming to minimize potential model overfitting. A careful analysis of using the biomarker cancer antigen 125 for ovarian cancer early detection showed significantly improved discrimination performance of PMM as compared with SREM and ROCA, nevertheless all approaches were generally well calibrated. Robustness of all approaches was further investigated in extensive simulation studies. The improved performance of PMM relative to ROCA is in part due to the fact that the biomarker measurements were taken at a yearly interval, which is not frequent enough to reliably estimate the changepoint or the slope after changepoint in cases under ROCA.},
  archive      = {J_SIM},
  author       = {Yongli Han and Paul S. Albert and Christine D. Berg and Nicolas Wentzensen and Hormuzd A. Katki and Danping Liu},
  doi          = {10.1002/sim.8731},
  journal      = {Statistics in Medicine},
  month        = {12},
  number       = {29},
  pages        = {4405-4420},
  shortjournal = {Stat. Med.},
  title        = {Statistical approaches using longitudinal biomarkers for disease early detection: A comparison of methodologies},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Bias testing, bias correction, and confounder selection
using an instrumental variable model. <em>SIM</em>, <em>39</em>(29),
4386–4404. (<a href="https://doi.org/10.1002/sim.8730">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Instrumental variable (IV) analysis can be used to address bias due to unobserved confounding when estimating the causal effect of a treatment on an outcome of interest. However, if a proposed IV is correlated with unmeasured confounders and/or weakly correlated with the treatment, the standard IV estimator may be more biased than an ordinary least squares (OLS) estimator. Several methods have been proposed that compare the bias of the IV and OLS estimators relying on the belief that measured covariates can be used as proxies for the unmeasured confounder. Despite these developments, there is lack of discussion about approaches that can be used to formally test whether the IV estimator may be less biased than the OLS estimator. Thus, we have developed a testing framework to compare the bias and a criterion to select informative measured covariates for bias comparison and regression adjustment. We also have developed a bias-correction method, which allows one to use an invalid IV to correct the bias of the OLS or IV estimator. Numerical studies demonstrate that the proposed methods perform well with realistic sample sizes.},
  archive      = {J_SIM},
  author       = {Byeong Yeob Choi and Jason P. Fine and M. Alan Brookhart},
  doi          = {10.1002/sim.8730},
  journal      = {Statistics in Medicine},
  month        = {12},
  number       = {29},
  pages        = {4386-4404},
  shortjournal = {Stat. Med.},
  title        = {Bias testing, bias correction, and confounder selection using an instrumental variable model},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Discrete-time survival data with longitudinal covariates.
<em>SIM</em>, <em>39</em>(29), 4372–4385. (<a
href="https://doi.org/10.1002/sim.8729">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Survival analysis has been conventionally performed on a continuous time scale. In practice, the survival time is often recorded or handled on a discrete scale; when this is the case, the discrete-time survival analysis would provide analysis results more relevant to the actual data scale. Besides, data on time-dependent covariates in the survival analysis are usually collected through intermittent follow-ups, resulting in the missing and mismeasured covariate data. In this work, we propose the sufficient discrete hazard (SDH) approach to discrete-time survival analysis with longitudinal covariates that are subject to missingness and mismeasurement. The SDH method employs the conditional score idea available for dealing with mismeasured covariates, and the penalized least squares for estimating the missing covariate value using the regression spline basis. The SDH method is developed for the single event analysis with the logistic discrete hazard model, and for the competing risks analysis with the multinomial logit model. Simulation results revel good finite-sample performances of the proposed estimator and the associated asymptotic theory. The proposed SDH method is applied to the scleroderma lung study data, where the time to medication withdrawal and time to death were recorded discretely in months, for illustration.},
  archive      = {J_SIM},
  author       = {Chi-Chung Wen and Yi-Hau Chen},
  doi          = {10.1002/sim.8729},
  journal      = {Statistics in Medicine},
  month        = {12},
  number       = {29},
  pages        = {4372-4385},
  shortjournal = {Stat. Med.},
  title        = {Discrete-time survival data with longitudinal covariates},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Model-assisted estimators for time-to-event data from
complex surveys. <em>SIM</em>, <em>39</em>(29), 4351–4371. (<a
href="https://doi.org/10.1002/sim.8728">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We develop model-assisted estimators for complex survey data for the proportion of a population that experienced some event by a specified time t . Theory for the new estimators uses time-to-event models as the underlying framework but have both good model-based and design-based properties. The estimators are compared in a simulation to traditional survey estimation methods and are also applied to a study of nurses&#39; health. The new estimators take advantage of covariates predictive of the event and reduce standard errors compared to conventional alternatives.},
  archive      = {J_SIM},
  author       = {Benjamin M. Reist and Richard Valliant},
  doi          = {10.1002/sim.8728},
  journal      = {Statistics in Medicine},
  month        = {12},
  number       = {29},
  pages        = {4351-4371},
  shortjournal = {Stat. Med.},
  title        = {Model-assisted estimators for time-to-event data from complex surveys},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). To tolerate or to agree: A tutorial on tolerance intervals
in method comparison studies with BivRegBLS r package. <em>SIM</em>,
<em>39</em>(28), 4334–4349. (<a
href="https://doi.org/10.1002/sim.8709">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The well-known agreement interval by Bland and Altman is extensively applied in method comparison studies. Two clinical measurement methods are considered interchangeable if their differences are not clinically significant. The agreement interval is commonly applied to assess the spread of the differences. However, this interval is approximate (too narrow) and several authors propose calculating a confidence interval around each bound. This article demonstrates that this approach is misleading, awkward, and confusing. On the other hand, tolerance intervals are exact and can include a confidence level if needed. Tolerance intervals are also easier to calculate and to interpret. Real data sets are used to illustrate the tolerance intervals with the R package BivRegBLS under normal or log-normal assumptions. Furthermore, it is also explained how to assess the coverage probabilities of the tolerance intervals with simulations.},
  archive      = {J_SIM},
  author       = {Bernard G. Francq and Marion Berger and Charles Boachie},
  doi          = {10.1002/sim.8709},
  journal      = {Statistics in Medicine},
  month        = {12},
  number       = {28},
  pages        = {4334-4349},
  shortjournal = {Stat. Med.},
  title        = {To tolerate or to agree: A tutorial on tolerance intervals in method comparison studies with BivRegBLS r package},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Statistical inference for missing data mechanisms.
<em>SIM</em>, <em>39</em>(28), 4325–4333. (<a
href="https://doi.org/10.1002/sim.8727">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the literature of statistical analysis with missing data there is a significant gap in statistical inference for missing data mechanisms especially for nonmonotone missing data, which has essentially restricted the use of the estimation methods which require estimating the missing data mechanisms. For example, the inverse probability weighting methods (Horvitz &amp; Thompson, 1952; Little &amp; Rubin, 2002), including the popular augmented inverse probability weighting (Robins et al, 1994), depend on sufficient models for the missing data mechanisms to reduce estimation bias while improving estimation efficiency. This research proposes a semiparametric likelihood method for estimating missing data mechanisms where an EM algorithm with closed form expressions for both E-step and M-step is used in evaluating the estimate (Zhao et al, 2009; Zhao, 2020). The asymptotic variance of the proposed estimator is estimated from the profile score function. The methods are general and robust. Simulation studies in various missing data settings are performed to examine the finite sample performance of the proposed method. Finally, we analysis the missing data mechanism of Duke cardiac catheterization coronary artery disease diagnostic data to illustrate the method.},
  archive      = {J_SIM},
  author       = {Yang Zhao},
  doi          = {10.1002/sim.8727},
  journal      = {Statistics in Medicine},
  month        = {12},
  number       = {28},
  pages        = {4325-4333},
  shortjournal = {Stat. Med.},
  title        = {Statistical inference for missing data mechanisms},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Generalized finite mixture of multivariate regressions with
applications to therapeutic biomarker identification. <em>SIM</em>,
<em>39</em>(28), 4301–4324. (<a
href="https://doi.org/10.1002/sim.8726">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Finite mixtures of regressions have been used to analyze data that come from a heterogeneous population. When more than one response is observed, accommodating a multivariate response can be useful. In this article, we go a step further and introduce a multivariate extension that includes a latent overlapping cluster indicator variable that allows for potential overdispersion. A generalized mixture of multivariate regressions in connection with the proposed model and a new EM algorithm for fitting are provided. In addition, we allow for high-dimensional predictors via shrinkage estimation. This model proves particularly useful in the analysis of complex data like the search for cancer therapeutic biomarkers. We demonstrate this using the genomics of drug sensitivity in cancer resource.},
  archive      = {J_SIM},
  author       = {Hongmei Liu and J. Sunil Rao},
  doi          = {10.1002/sim.8726},
  journal      = {Statistics in Medicine},
  month        = {12},
  number       = {28},
  pages        = {4301-4324},
  shortjournal = {Stat. Med.},
  title        = {Generalized finite mixture of multivariate regressions with applications to therapeutic biomarker identification},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). An efficient variance estimator of AUC and its applications
to binary classification. <em>SIM</em>, <em>39</em>(28), 4281–4300. (<a
href="https://doi.org/10.1002/sim.8725">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The area under the ROC (receiver operating characteristic) curve, AUC, is one of the most commonly used measures to evaluate the performance of a binary classifier. Due to sampling variation, the model with the largest observed AUC score is not necessarily optimal, so it is crucial to assess the variation of AUC estimate. We extend the proposal by Wang and Lindsay and devise an unbiased variance estimator of AUC estimate that is of a two-sample U-statistic form. The proposal can be easily generalized to estimate the variance of a K -sample U-statistic ( K ≥ 2 ). To make our developed variance estimator more applicable, we employ a partition-resampling scheme that is computationally efficient. Simulation studies suggest that the developed AUC variance estimator yields much better or comparable performance to jackknife and bootstrap variance estimators, and computational times that are about 10 to 30 times faster than the times of its counterparts. In practice, the proposal can be used in the one-standard-error rule for model selection, or to construct an asymptotic confidence interval of AUC in binary classification. In addition to conducting simulation studies, we illustrate its practical applications using two real datasets in medical sciences.},
  archive      = {J_SIM},
  author       = {Qing Wang and Alexandria Guo},
  doi          = {10.1002/sim.8725},
  journal      = {Statistics in Medicine},
  month        = {12},
  number       = {28},
  pages        = {4281-4300},
  shortjournal = {Stat. Med.},
  title        = {An efficient variance estimator of AUC and its applications to binary classification},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A prediction-based test for multiple endpoints.
<em>SIM</em>, <em>39</em>(28), 4267–4280. (<a
href="https://doi.org/10.1002/sim.8724">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article introduces a global hypothesis test intended for studies with multiple endpoints. Our test makes use of a priori predictions about the direction of the result of each endpoint and we weight these predictions using the sample correlation matrix. The global alternative hypothesis concerns a parameter, ϕ , defined as the researcher&#39;s ability to correctly predict the direction of each measure, essentially a binomial parameter. This allows for the test to include expected effects that are all positive, all negative or both while still using the cumulative information across those endpoints. A rejection of the null hypothesis ( ) provides evidence that the researcher&#39;s underlying theory about the natural process provides a better prediction of the observed results relative to the null hypothesized predictive ability, thus indicating the theory is worthy of further study. We compare our test to O&#39;Brien&#39;s ordinary least squares (OLS) test and show that for small samples and situations where the effect is not in the same direction across all endpoints our approach has better power, while if the effect is equidirectional across all endpoints the OLS test can have greater power.},
  archive      = {J_SIM},
  author       = {Robert N. Montgomery and Jonathan D. Mahnken},
  doi          = {10.1002/sim.8724},
  journal      = {Statistics in Medicine},
  month        = {12},
  number       = {28},
  pages        = {4267-4280},
  shortjournal = {Stat. Med.},
  title        = {A prediction-based test for multiple endpoints},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Copula modeling of receiver operating characteristic and
predictiveness curves. <em>SIM</em>, <em>39</em>(28), 4252–4266. (<a
href="https://doi.org/10.1002/sim.8723">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Receiver operating characteristic (ROC) and predictiveness curves are graphical tools to study the discriminative and predictive power of a continuous-valued marker in a binary outcome. In this paper, a copula-based construction of the joint density of the marker and the outcome is developed for plotting and analyzing both curves. The methodology only requires a copula function, the marginal distribution of the marker, and the prevalence rate for the model to be characterized. The adoption of the Gaussian copula and the customization of the margin for the marker are proposed for such characterization. The computation of both curves is numerically more feasible than methods that attempt to obtain one curve in terms of the other. Estimation is carried out using maximum likelihood and resampling-based methods. Randomized quantile residuals from each conditional distribution are employed for both assessing the adequacy of the model and identifying outliers. The performance of the estimators of both curves and their underlying quantities is evaluated in simulation studies that assume different dependence structures and sample sizes. The methods are illustrated with an analysis of the level of progesterone receptor gene expression for the diagnosis and prediction of estrogen receptor-positive breast cancer.},
  archive      = {J_SIM},
  author       = {Gabriel Escarela and Carlos Erwin Rodríguez and Gabriel Núñez-Antonio},
  doi          = {10.1002/sim.8723},
  journal      = {Statistics in Medicine},
  month        = {12},
  number       = {28},
  pages        = {4252-4266},
  shortjournal = {Stat. Med.},
  title        = {Copula modeling of receiver operating characteristic and predictiveness curves},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Multiply robust estimation of causal quantile treatment
effects. <em>SIM</em>, <em>39</em>(28), 4238–4251. (<a
href="https://doi.org/10.1002/sim.8722">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In causal inference, often the interest lies in the estimation of the average causal effect. Other quantities such as the quantile treatment effect may be of interest as well. In this article, we propose a multiply robust method for estimating the marginal quantiles of potential outcomes by achieving mean balance in (a) the propensity score, and (b) the conditional distributions of potential outcomes. An empirical likelihood or entropy measure approach can be utilized for estimation instead of inverse probability weighting, which is known to be sensitive to the misspecification of the propensity score model. Simulation studies are conducted across different scenarios of correctness in both the propensity score models and the outcome models. Both simulation results and theoretical development indicate that our proposed estimator is consistent if any of the models are correctly specified. In the data analysis, we investigate the quantile treatment effect of mothers&#39; smoking status on infants&#39; birthweight.},
  archive      = {J_SIM},
  author       = {Yuying Xie and Cecilia Cotton and Yeying Zhu},
  doi          = {10.1002/sim.8722},
  journal      = {Statistics in Medicine},
  month        = {12},
  number       = {28},
  pages        = {4238-4251},
  shortjournal = {Stat. Med.},
  title        = {Multiply robust estimation of causal quantile treatment effects},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Sample size requirements for detecting treatment effect
heterogeneity in cluster randomized trials. <em>SIM</em>,
<em>39</em>(28), 4218–4237. (<a
href="https://doi.org/10.1002/sim.8721">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cluster randomized trials (CRTs) refer to experiments with randomization carried out at the cluster or the group level. While numerous statistical methods have been developed for the design and analysis of CRTs, most of the existing methods focused on testing the overall treatment effect across the population characteristics, with few discussions on the differential treatment effect among subpopulations. In addition, the sample size and power requirements for detecting differential treatment effect in CRTs remain unclear, but are helpful for studies planned with such an objective. In this article, we develop a new sample size formula for detecting treatment effect heterogeneity in two-level CRTs for continuous outcomes, continuous or binary covariates measured at cluster or individual level. We also investigate the roles of two intraclass correlation coefficients (ICCs): the adjusted ICC for the outcome of interest and the marginal ICC for the covariate of interest. We further derive a closed-form design effect formula to facilitate the application of the proposed method, and provide extensions to accommodate multiple covariates. Extensive simulations are carried out to validate the proposed formula in finite samples. We find that the empirical power agrees well with the prediction across a range of parameter constellations, when data are analyzed by a linear mixed effects model with a treatment-by-covariate interaction. Finally, we use data from the HF-ACTION study to illustrate the proposed sample size procedure for detecting heterogeneous treatment effects.},
  archive      = {J_SIM},
  author       = {Siyun Yang and Fan Li and Monique A. Starks and Adrian F. Hernandez and Robert J. Mentz and Kingshuk R. Choudhury},
  doi          = {10.1002/sim.8721},
  journal      = {Statistics in Medicine},
  month        = {12},
  number       = {28},
  pages        = {4218-4237},
  shortjournal = {Stat. Med.},
  title        = {Sample size requirements for detecting treatment effect heterogeneity in cluster randomized trials},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Classification algorithm for high-dimensional protein
markers in time-course data. <em>SIM</em>, <em>39</em>(28), 4201–4217.
(<a href="https://doi.org/10.1002/sim.8720">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Identification of biomarkers is an emerging area in oncology. In this article, we develop an efficient statistical procedure for the classification of protein markers according to their effect on cancer progression. A high-dimensional time-course dataset of protein markers for 80 patients motivates us for developing the model. The threshold value is formulated as a level of a marker having maximum impact on cancer progression. The classification algorithm technique for high-dimensional time-course data is developed and the algorithm is validated by comparing random components using both proportional hazard and accelerated failure time frailty models. The study elucidates the application of two separate joint modeling techniques using auto regressive-type model and mixed effect model for time-course data and proportional hazard model for survival data with proper utilization of Bayesian methodology. Also, a prognostic score is developed on the basis of few selected genes with application on patients. This study facilitates to identify relevant biomarkers from a set of markers.},
  archive      = {J_SIM},
  author       = {Gajendra K. Vishwakarma and Atanu Bhattacharjee and Souvik Banerjee and Benoit Liquet},
  doi          = {10.1002/sim.8720},
  journal      = {Statistics in Medicine},
  month        = {12},
  number       = {28},
  pages        = {4201-4217},
  shortjournal = {Stat. Med.},
  title        = {Classification algorithm for high-dimensional protein markers in time-course data},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A stratified generalized additive model and permutation test
for temporal heterogeneity of smoothed bivariate spatial effects.
<em>SIM</em>, <em>39</em>(28), 4187–4200. (<a
href="https://doi.org/10.1002/sim.8718">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Generalized additive models (GAMs) with bivariate smoothers are frequently used to map geographic disease risks in epidemiology studies. A challenge in identifying health disparities has been the lack of intuitive and computationally feasible methods to assess whether the pattern of spatial effects varies over time. In this research, we accommodate time-stratified smoothers into the GAM framework to estimate time-specific spatial risk patterns while borrowing information from confounding effects across time. A backfitting algorithm for model estimation is proposed along with a permutation testing framework for assessing temporal heterogeneity of geospatial risk patterns across two or more time points. Simulation studies show that our proposed permuted mean squared difference (PMSD) test performs well with respect to type I error and power in various settings when compared with existing methods. The proposed model and PMSD test are used geospatial risk patterns of patent ductus arteriosus (PDA) in the state of Massachusetts over 2003-2009. We show that there is variation over time in spatial patterns of PDA risk, adjusting for other known risk factors, suggesting the presence of potential time-varying and space-related risk factors other than the adjusted ones.},
  archive      = {J_SIM},
  author       = {Yannan Tang and Veronica M. Vieira and Scott M. Bartell and Daniel L. Gillen},
  doi          = {10.1002/sim.8718},
  journal      = {Statistics in Medicine},
  month        = {12},
  number       = {28},
  pages        = {4187-4200},
  shortjournal = {Stat. Med.},
  title        = {A stratified generalized additive model and permutation test for temporal heterogeneity of smoothed bivariate spatial effects},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Inference without randomization or ignorability: A
stability-controlled quasi-experiment on the prevention of tuberculosis.
<em>SIM</em>, <em>39</em>(28), 4169–4186. (<a
href="https://doi.org/10.1002/sim.8717">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The stability-controlled quasi-experiment (SCQE) is an approach to study the effects of nonrandomized, newly adopted treatments. While covariate adjustment techniques rely on a “no unobserved confounding” assumption, SCQE imposes an assumption on the change in the average nontreatment outcome between successive cohorts (the “baseline trend”). We provide inferential tools for SCQE and its first application, examining whether isoniazid preventive therapy (IPT) reduced tuberculosis (TB) incidence among 26 715 HIV patients in Tanzania. After IPT became available, 16% of untreated patients developed TB within a year, compared with only 0.5% of patients under treatment. Thus, a simple difference in means suggests a 15.5 percentage point (pp) lower risk (p ≪ .001). Adjusting for covariates using numerous techniques leaves this effectively unchanged. Yet, due to confounding biases, such estimates can be misleading regardless of their statistical strength. By contrast, SCQE reveals valid causal effect estimates for any chosen assumption on the baseline trend. For example, assuming a baseline trend near 0 (no change in TB incidence over time, absent this treatment) implies a small and insignificant effect. To argue IPT was beneficial requires arguing that the nontreatment incidence would have risen by at least 0.7 pp per year, which is plausible but far from certain. SCQE may produce narrow estimates when the plausible range of baseline trends can be sufficiently constrained, while in every case it tells us what baseline trends must be believed in order to sustain a given conclusion, protecting against inferences that rely upon infeasible assumptions.},
  archive      = {J_SIM},
  author       = {Chad Hazlett and Werner Maokola and David Ami Wulf},
  doi          = {10.1002/sim.8717},
  journal      = {Statistics in Medicine},
  month        = {12},
  number       = {28},
  pages        = {4169-4186},
  shortjournal = {Stat. Med.},
  title        = {Inference without randomization or ignorability: A stability-controlled quasi-experiment on the prevention of tuberculosis},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Variance formulae for multiphase stepped wedge cluster
randomized trial. <em>SIM</em>, <em>39</em>(28), 4147–4168. (<a
href="https://doi.org/10.1002/sim.8716">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In a multiphase stepped wedge cluster randomized trial (MSW-CRT), more than one intervention will be initiated on each sequence in a fixed order. Hence, with the MSW-CRT design, the effect of the first intervention can be evaluated when compared to control, as well as the added-on effects of the subsequent interventions. Studies that use MSW-CRT have been proposed, but properties of this design have not been explicitly studied. We derive closed-form variance formulae to test the interventions&#39; effects, which can be readily used for sample size and power calculation. Additionally, we provide relationships between variances to test the interventions&#39; effects and design parameters. Under special conditions, some important properties include: (i) the variances to test different interventions&#39; effects (ie, the first intervention effect and the second intervention effect) may be same; (ii) as the cluster-period mean autocorrelation increases, the variance to test an intervention effect may first increase and then decrease; (iii) as the amount of periods between the initiations of two interventions (ie, lag) increases, the variance to test an intervention effect may remain unchanged. We illustrate the relationships between power and design parameters using the variance formulae. From a few illustrative examples, we observe that the statistical test that uses data only relevant to a specific intervention has inferior power (relative power loss &lt;15%) compared to the test when using all the study data. Also, power is reduced when both the total number of periods and lag are decreased simultaneously (relative power loss &lt;20%).},
  archive      = {J_SIM},
  author       = {Pengyue Zhang and Abigail Shoben and Rebecca Jackson and Soledad Fernandez},
  doi          = {10.1002/sim.8716},
  journal      = {Statistics in Medicine},
  month        = {12},
  number       = {28},
  pages        = {4147-4168},
  shortjournal = {Stat. Med.},
  title        = {Variance formulae for multiphase stepped wedge cluster randomized trial},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A nonparametric method for value function guided subgroup
identification via gradient tree boosting for censored survival data.
<em>SIM</em>, <em>39</em>(28), 4133–4146. (<a
href="https://doi.org/10.1002/sim.8714">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In randomized clinical trials with survival outcome, there has been an increasing interest in subgroup identification based on baseline genomic, proteomic markers, or clinical characteristics. Some of the existing methods identify subgroups that benefit substantially from the experimental treatment by directly modeling outcomes or treatment effect. When the goal is to find an optimal treatment for a given patient rather than finding the right patient for a given treatment, methods under the individualized treatment regime framework estimate an individualized treatment rule that would lead to the best expected clinical outcome as measured by a value function. Connecting the concept of value function to subgroup identification, we propose a nonparametric method that searches for subgroup membership scores by maximizing a value function that directly reflects the subgroup-treatment interaction effect based on restricted mean survival time. A gradient tree boosting algorithm is proposed to search for the individual subgroup membership scores. We conduct simulation studies to evaluate the performance of the proposed method and an application to an AIDS clinical trial is performed for illustration.},
  archive      = {J_SIM},
  author       = {Pingye Zhang and Junshui Ma and Xinqun Chen and Yue Shentu},
  doi          = {10.1002/sim.8714},
  journal      = {Statistics in Medicine},
  month        = {12},
  number       = {28},
  pages        = {4133-4146},
  shortjournal = {Stat. Med.},
  title        = {A nonparametric method for value function guided subgroup identification via gradient tree boosting for censored survival data},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A marginal estimate for the overall treatment effect on a
survival outcome within the joint modeling framework. <em>SIM</em>,
<em>39</em>(28), 4120–4132. (<a
href="https://doi.org/10.1002/sim.8713">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Joint models for longitudinal and survival data are increasingly used and enjoy a wide range of application areas. In this article, we focus on the application of joint models on clinical trial data with special interest in the treatment effect on the survival outcome. Within a joint model, the estimated treatment effect on the survival outcome is an aggregate comprising the indirect treatment effect through the longitudinal outcome and the direct treatment effect on the survival outcome. This overall treatment effect is, however, conditional on random effects, and therefore has a subject-specific interpretation. The conditional interpretation arises from the shared random effects between the longitudinal and survival process in combination with the nonlinear link function of the survival model. The overall treatment effect is, therefore, not valid for population-based inference, which is the goal for most clinical trials. We propose a method to obtain a marginal estimate of the overall treatment effect on the survival outcome in a joint model. Additionally, we extend our proposal to allow for different parameterizations for the association between the longitudinal and survival outcome. The proposed method is demonstrated on data of a clinical study on the effect of synbiotic on the gut microbiota of cesarean delivered infants, where we estimate the marginal overall treatment effect on the risk of eczema or atopic dermatitis using longitudinal information on fecal bifidobacteria.},
  archive      = {J_SIM},
  author       = {Floor M. van Oudenhoven and Sophie H. N. Swinkels and Joseph G. Ibrahim and Dimitris Rizopoulos},
  doi          = {10.1002/sim.8713},
  journal      = {Statistics in Medicine},
  month        = {12},
  number       = {28},
  pages        = {4120-4132},
  shortjournal = {Stat. Med.},
  title        = {A marginal estimate for the overall treatment effect on a survival outcome within the joint modeling framework},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Synthesizing independent stagewise trials for optimal
dynamic treatment regimes. <em>SIM</em>, <em>39</em>(28), 4107–4119. (<a
href="https://doi.org/10.1002/sim.8712">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dynamic treatment regimes (DTRs) adaptively prescribe treatments based on patients&#39; intermediate responses and evolving health status over multiple treatment stages. Data from sequential multiple assignment randomization trials (SMARTs) are recommended to be used for learning DTRs. However, due to re-randomization of the same patients over multiple treatment stages and a prolonged follow-up period, SMARTs are often difficult to implement and costly to manage, and patient adherence is always a concern in practice. To lessen such practical challenges, we propose an alternative approach to learn optimal DTRs by synthesizing independent trials over different stages. Specifically, at each stage, data from a single randomized trial along with patients&#39; natural medical history and health status in previous stages are used. We use a backward learning method to estimate optimal treatment decisions at a particular stage, where patients&#39; future optimal outcome increments are estimated using data observed from independent trials with future stages&#39; information. Under some conditions, we show that the proposed method yields consistent estimation of the optimal DTRs and we obtain the same learning rates as those from SMARTs. We conduct simulation studies to demonstrate the advantage of the proposed method. Finally, we learn optimal DTRs for treating major depressive disorder (MDD) by stagewise synthesis of two randomized trials. We perform a validation study on independent subjects and show that the synthesized DTRs lead to the greatest MDD symptom reduction compared to alternative methods.},
  archive      = {J_SIM},
  author       = {Yuan Chen and Yuanjia Wang and Donglin Zeng},
  doi          = {10.1002/sim.8712},
  journal      = {Statistics in Medicine},
  month        = {12},
  number       = {28},
  pages        = {4107-4119},
  shortjournal = {Stat. Med.},
  title        = {Synthesizing independent stagewise trials for optimal dynamic treatment regimes},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Correction: Adjusted restricted mean survival times in
observational studies. <em>SIM</em>, <em>39</em>(27), 4105. (<a
href="https://doi.org/10.1002/sim.8742">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_SIM},
  author       = {Sarah C. Conner and Ludovic Trinquart},
  doi          = {10.1002/sim.8742},
  journal      = {Statistics in Medicine},
  month        = {11},
  number       = {27},
  pages        = {4105},
  shortjournal = {Stat. Med.},
  title        = {Correction: Adjusted restricted mean survival times in observational studies},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Inconsistencies with formulas for the standard error of the
standardized mean difference of repeated measures experiments.
<em>SIM</em>, <em>39</em>(27), 4101–4104. (<a
href="https://doi.org/10.1002/sim.8669">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {There are inconsistencies between the formulas for the variance of standardized mean difference ( SMD ) in the Cochrane Handbook for Systematic Reviews and the variance reported in other sources. Instead of the variance appropriate for the SMD of a crossover experiment, the Cochrane Handbook uses the variance appropriate for a pre-test post-test experiment. This means that if there is a non-negligible time period effect, the formula reported by the Handbook will underestimate both the effect size and its variance. In addition, the formula for the standard error of SMD reported in the Cochrane Handbook (in section 23.2.7.2) is inconsistent with the variance derived from the variance of the related t − test. Even if the period effect is negligible, the Cochrane Handbook formula is biased toward underestimates. The difference between the estimates from the two formulas will be small if either the correlation between the repeated measures, or the magnitude of the SMD estimate, is small, or if the sample size is large. However, it can be can be quite substantial in other circumstances.},
  archive      = {J_SIM},
  author       = {Barbara Kitchenham and Lech Madeyski},
  doi          = {10.1002/sim.8669},
  journal      = {Statistics in Medicine},
  month        = {11},
  number       = {27},
  pages        = {4101-4104},
  shortjournal = {Stat. Med.},
  title        = {Inconsistencies with formulas for the standard error of the standardized mean difference of repeated measures experiments},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Comments on “critical aspects of the bayesian approach to
phase i cancer trials.” <em>SIM</em>, <em>39</em>(27), 4100. (<a
href="https://doi.org/10.1002/sim.8483">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_SIM},
  author       = {Xiang Li and Kevin Liu and Hong Tian},
  doi          = {10.1002/sim.8483},
  journal      = {Statistics in Medicine},
  month        = {11},
  number       = {27},
  pages        = {4100},
  shortjournal = {Stat. Med.},
  title        = {Comments on “Critical aspects of the bayesian approach to phase i cancer trials”},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Semiparametric mixture cure model analysis with competing
risks data: Application to vascular access thrombosis data.
<em>SIM</em>, <em>39</em>(27), 4086–4099. (<a
href="https://doi.org/10.1002/sim.8711">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The article is motivated by a nephrology study in Taiwan, which enrolled hemodialysis patients who suffered from vascular access thrombosis. After treatment, some patients were cured of thrombosis, while some may experience recurrence of either type (acute or nonacute) of vascular access thrombosis. Our major interest is to estimate the cumulative incidence probability of time to the first recurrence of acute thrombosis after therapy. Since the occurrence of one type of vascular access thrombosis precludes occurrence of the other type, patients are subject to competing risks. To account for the presence of competing risks and cured patients, we develop a mixture model approach to the regression analysis of competing-risks data with a cure fraction. We make inference about the effects of factors on both the cure rate and cumulative incidence function (CIF) for a failure of interest, which are separately specified in the logistic regression model and semiparametric regression model with time-varying and time-invariant effects. Based on two-stage method, we develop novel estimation equations using the inverse probability censoring weight techniques. The asymptotic properties of the estimators are rigorously studied and the plug-in variance estimators can be obtained for constructing interval estimators. We also propose a lack-of-fit test for assessing the adequacy of the proposed model and several tests for time-varying effects. The simulation studies and vascular access thrombosis data analysis are conducted to illustrate the proposed method.},
  archive      = {J_SIM},
  author       = {Chyong-Mei Chen and Pao-sheng Shen and Chih-Ching Lin and Chih-Cheng Wu},
  doi          = {10.1002/sim.8711},
  journal      = {Statistics in Medicine},
  month        = {11},
  number       = {27},
  pages        = {4086-4099},
  shortjournal = {Stat. Med.},
  title        = {Semiparametric mixture cure model analysis with competing risks data: Application to vascular access thrombosis data},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Impact of discretization of the timeline for longitudinal
causal inference methods. <em>SIM</em>, <em>39</em>(27), 4069–4085. (<a
href="https://doi.org/10.1002/sim.8710">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In longitudinal settings, causal inference methods usually rely on a discretization of the patient timeline that may not reflect the underlying data generation process. This article investigates the estimation of causal parameters under discretized data. It presents the implicit assumptions practitioners make but do not acknowledge when discretizing data to assess longitudinal causal parameters. We illustrate that differences in point estimates under different discretizations are due to the data coarsening resulting in both a modified definition of the parameter of interest and loss of information about time-dependent confounders. We further investigate several tools to advise analysts in selecting a timeline discretization for use with pooled longitudinal targeted maximum likelihood estimation for the estimation of the parameters of a marginal structural model. We use a simulation study to empirically evaluate bias at different discretizations and assess the use of the cross-validated variance as a measure of data support to select a discretization under a chosen data coarsening mechanism. We then apply our approach to a study on the relative effect of alternative asthma treatments during pregnancy on pregnancy duration. The results of the simulation study illustrate how coarsening changes the target parameter of interest as well as how it may create bias due to a lack of appropriate control for time-dependent confounders. We also observe evidence that the cross-validated variance acts well as a measure of support in the data, by being minimized at finer discretizations as the sample size increases.},
  archive      = {J_SIM},
  author       = {Steve Ferreira Guerra and Mireille E. Schnitzer and Amélie Forget and Lucie Blais},
  doi          = {10.1002/sim.8710},
  journal      = {Statistics in Medicine},
  month        = {11},
  number       = {27},
  pages        = {4069-4085},
  shortjournal = {Stat. Med.},
  title        = {Impact of discretization of the timeline for longitudinal causal inference methods},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Six-way decomposition of causal effects: Unifying mediation
and mechanistic interaction. <em>SIM</em>, <em>39</em>(27), 4051–4068.
(<a href="https://doi.org/10.1002/sim.8708">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The sufficient component cause (SCC) model and counterfactual model are two common methods for causal inference, each with their own advantages: the SCC model allows the mechanistic interaction to be detailed, whereas the counterfactual model features a systemic framework for quantifying causal effects. Hence, integrating the SCC and counterfactual models may facilitate the conceptualization of causation. Based on the marginal SCC (mSCC) model, we propose a novel counterfactual mSCC framework that includes the steps of definition, identification, and estimation. We further propose a six-way effect decomposition for assessing mediation and the mechanistic interaction. The results demonstrate that when all variables are binary, the six-way decomposition is an extension of four-way decomposition and that without agonism, the six-way decomposition is reduced to four-way decomposition. To illustrate the utility of the proposed decomposition, we apply it to a Taiwanese cohort to examine the mechanism of hepatitis C virus (HCV)-induced hepatocellular carcinoma (HCC) with liver inflammation measured by alanine aminotransferase (ALT) as a mediator. Among the HCV-induced HCC cases, 62.27% are not explained by either mediation or interaction in relation to ALT; 9.32% are purely mediated by ALT; 16.53% are caused by the synergistic effect of HCV and ALT; and 9.31% are due to the mediated synergistic effect of HCV and ALT. In summary, we introduce an SCC model framework based on counterfactual theory and detail the required identification assumptions and estimation procedures; we also propose a six-way effect decomposition to unify mediation and mechanistic interaction analyses.},
  archive      = {J_SIM},
  author       = {Yen-Tsung Huang and An-Shun Tai and Meng-Ying Chou and Geng-Xian Lin and Sheng-Hsuan Lin},
  doi          = {10.1002/sim.8708},
  journal      = {Statistics in Medicine},
  month        = {11},
  number       = {27},
  pages        = {4051-4068},
  shortjournal = {Stat. Med.},
  title        = {Six-way decomposition of causal effects: Unifying mediation and mechanistic interaction},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Incorporating pragmatic features into power analysis for
cluster randomized trials with a count outcome. <em>SIM</em>,
<em>39</em>(27), 4037–4050. (<a
href="https://doi.org/10.1002/sim.8707">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cluster randomized designs are frequently employed in pragmatic clinical trials which test interventions in the full spectrum of everyday clinical settings in order to maximize applicability and generalizability. In this study, we propose to directly incorporate pragmatic features into power analysis for cluster randomized trials with count outcomes. The pragmatic features considered include arbitrary randomization ratio, overdispersion, random variability in cluster size, and unequal lengths of follow-up over which the count outcome is measured. The proposed method is developed based on generalized estimating equation (GEE) and it is advantageous in that the sample size formula retains a closed form, facilitating its implementation in pragmatic trials. We theoretically explore the impact of various pragmatic features on sample size requirements. An efficient Jackknife algorithm is presented to address the problem of underestimated variance by the GEE sandwich estimator when the number of clusters is small. We assess the performance of the proposed sample size method through extensive simulation and an application example to a real clinical trial is presented.},
  archive      = {J_SIM},
  author       = {Dateng Li and Song Zhang and Jing Cao},
  doi          = {10.1002/sim.8707},
  journal      = {Statistics in Medicine},
  month        = {11},
  number       = {27},
  pages        = {4037-4050},
  shortjournal = {Stat. Med.},
  title        = {Incorporating pragmatic features into power analysis for cluster randomized trials with a count outcome},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Multiple imputation score tests and an application to
cochran-mantel-haenszel statistics. <em>SIM</em>, <em>39</em>(27),
4025–4036. (<a href="https://doi.org/10.1002/sim.8706">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The standard multiple imputation technique focuses on parameter estimation. In this study, we describe a method for conducting score tests following multiple imputation. As an important application, we use the Cochran-Mantel-Haenszel (CMH) test as a score test and compare the proposed multiple imputation method with a method based on the Wilson-Hilferty transformation of the CMH statistic. We show that the proposed multiple imputation method preserves the nominal significance level for three types of alternative hypotheses, whereas that based on the Wilson-Hilferty transformation inflates type I error for the “row means differ” and “general association” alternative hypotheses. Moreover, we find that this type I error inflation worsens as the amount of missing data increases.},
  archive      = {J_SIM},
  author       = {Kaifeng Lu},
  doi          = {10.1002/sim.8706},
  journal      = {Statistics in Medicine},
  month        = {11},
  number       = {27},
  pages        = {4025-4036},
  shortjournal = {Stat. Med.},
  title        = {Multiple imputation score tests and an application to cochran-mantel-haenszel statistics},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). The minimum intensity of a mixed exposure that increases the
risk of an outcome. <em>SIM</em>, <em>39</em>(27), 4016–4024. (<a
href="https://doi.org/10.1002/sim.8705">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Semi-quantitative exposures, such as smoking and alcohol consumption, are common in clinical studies. Their association with outcomes is captured using either a single quantitative variable that includes nonexposed with a value of zero, or using two variables by adding an additional binary variable exposed versus not exposed. Herein, we propose two approaches to determine a lower bound on the amount of such an exposure (eg, number of cigarettes smoked per day) that significantly increases the risk of outcomes. Using smoking as illustration, the first approach consists of sequentially testing the effect of 1, 2, and so on, cigarettes per day, which requires an adjustment for multiplicity to protect the overall type-I error. An alternative gatekeeping approach is also described. The proposed methods are illustrated for the association of smoking with clinically confirmed neuropathy in a logistic regression model, and for the association of smoking with the risk of CVD in a Cox PH regression model.},
  archive      = {J_SIM},
  author       = {Ionut Bebu and Barbara H. Braffett and John M. Lachin},
  doi          = {10.1002/sim.8705},
  journal      = {Statistics in Medicine},
  month        = {11},
  number       = {27},
  pages        = {4016-4024},
  shortjournal = {Stat. Med.},
  title        = {The minimum intensity of a mixed exposure that increases the risk of an outcome},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Regression with a right-censored predictor using inverse
probability weighting methods. <em>SIM</em>, <em>39</em>(27), 4001–4015.
(<a href="https://doi.org/10.1002/sim.8704">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In a longitudinal study, measures of key variables might be incomplete or partially recorded due to drop-out, loss to follow-up, or early termination of the study occurring before the advent of the event of interest. In this paper, we focus primarily on the implementation of a regression model with a randomly censored predictor. We examine, particularly, the use of inverse probability weighting methods in a generalized linear model (GLM), when the predictor of interest is right-censored, to adjust for censoring. To improve the performance of the complete-case analysis and prevent selection bias, we consider three different weighting schemes: inverse censoring probability weights, Kaplan-Meier weights, and Cox proportional hazards weights. We use Monte Carlo simulation studies to evaluate and compare the empirical properties of different weighting estimation methods. Finally, we apply these methods to the Framingham Heart Study data as an illustrative example to estimate the relationship between age of onset of a clinically diagnosed cardiovascular event and low-density lipoprotein among cigarette smokers.},
  archive      = {J_SIM},
  author       = {Roland A. Matsouaka and Folefac D. Atem},
  doi          = {10.1002/sim.8704},
  journal      = {Statistics in Medicine},
  month        = {11},
  number       = {27},
  pages        = {4001-4015},
  shortjournal = {Stat. Med.},
  title        = {Regression with a right-censored predictor using inverse probability weighting methods},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A bayesian time-to-event pharmacokinetic model for phase i
dose-escalation trials with multiple schedules. <em>SIM</em>,
<em>39</em>(27), 3986–4000. (<a
href="https://doi.org/10.1002/sim.8703">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Phase I dose-escalation trials must be guided by a safety model in order to avoid exposing patients to unacceptably high risk of toxicities. Traditionally, these trials are based on one type of schedule. In more recent practice, however, there is often a need to consider more than one schedule, which means that in addition to the dose itself, the schedule needs to be varied in the trial. Hence, the aim is finding an acceptable dose-schedule combination. However, most established methods for dose-escalation trials are designed to escalate the dose only and ad hoc choices must be made to adapt these to the more complicated setting of finding an acceptable dose-schedule combination. In this article, we introduce a Bayesian time-to-event model which takes explicitly the dose amount and schedule into account through the use of pharmacokinetic principles. The model uses a time-varying exposure measure to account for the risk of a dose-limiting toxicity over time. The dose-schedule decisions are informed by an escalation with overdose control criterion. The model is formulated using interpretable parameters which facilitates the specification of priors. In a simulation study, we compared the proposed method with an existing method. The simulation study demonstrates that the proposed method yields similar or better results compared with an existing method in terms of recommending acceptable dose-schedule combinations, yet reduces the number of patients enrolled in most of scenarios. The R and Stan code to implement the proposed method is publicly available from Github ( https://github.com/gunhanb/TITEPK_code ).},
  archive      = {J_SIM},
  author       = {Burak Kürsad Günhan and Sebastian Weber and Tim Friede},
  doi          = {10.1002/sim.8703},
  journal      = {Statistics in Medicine},
  month        = {11},
  number       = {27},
  pages        = {3986-4000},
  shortjournal = {Stat. Med.},
  title        = {A bayesian time-to-event pharmacokinetic model for phase i dose-escalation trials with multiple schedules},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Blinded continuous information monitoring of recurrent event
endpoints with time trends in clinical trials. <em>SIM</em>,
<em>39</em>(27), 3968–3985. (<a
href="https://doi.org/10.1002/sim.8702">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Blinded sample size re-estimation and information monitoring based on blinded data has been suggested to mitigate risks due to planning uncertainties regarding nuisance parameters. Motivated by a randomized controlled trial in pediatric multiple sclerosis (MS), a continuous monitoring procedure for overdispersed count data was proposed recently. However, this procedure assumed constant event rates, an assumption often not met in practice. Here we extend the procedure to accommodate time trends in the event rates considering two blinded approaches: (a) the mixture approach modeling the number of events by a mixture of two negative binomial distributions and (b) the lumping approach approximating the marginal distribution of the event counts by a negative binomial distribution. Through simulations the operating characteristics of the proposed procedures are investigated under decreasing event rates. We find that the type I error rate is not inflated relevantly by either of the monitoring procedures, with the exception of strong time dependencies where the procedure assuming constant rates exhibits some inflation. Furthermore, the procedure accommodating time trends has generally favorable power properties compared with the procedure based on constant rates which stops often too late. The proposed method is illustrated by the clinical trial in pediatric MS.},
  archive      = {J_SIM},
  author       = {Tobias Mütze and Susanna Salem and Norbert Benda and Heinz Schmidli and Tim Friede},
  doi          = {10.1002/sim.8702},
  journal      = {Statistics in Medicine},
  month        = {11},
  number       = {27},
  pages        = {3968-3985},
  shortjournal = {Stat. Med.},
  title        = {Blinded continuous information monitoring of recurrent event endpoints with time trends in clinical trials},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Performance of variable and function selection methods for
estimating the nonlinear health effects of correlated chemical mixtures:
A simulation study. <em>SIM</em>, <em>39</em>(27), 3947–3967. (<a
href="https://doi.org/10.1002/sim.8701">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Statistical methods for identifying harmful chemicals in a correlated mixture often assume linearity in exposure-response relationships. Nonmonotonic relationships are increasingly recognized (eg, for endocrine-disrupting chemicals); however, the impact of nonmonotonicity on exposure selection has not been evaluated. In a simulation study, we assessed the performance of Bayesian kernel machine regression (BKMR), Bayesian additive regression trees (BART), Bayesian structured additive regression with spike-slab priors (BSTARSS), generalized additive models with double penalty (GAMDP) and thin plate shrinkage smoothers (GAMTS), multivariate adaptive regression splines (MARS), and lasso penalized regression. We simulated realistic exposure data based on pregnancy exposure to 17 phthalates and phenols in the US National Health and Nutrition Examination Survey using a multivariate copula. We simulated data sets of size N = 250 and compared methods across 32 scenarios, varying by model size and sparsity, signal-to-noise ratio, correlation structure, and exposure-response relationship shapes. We compared methods in terms of their sensitivity, specificity, and estimation accuracy. In most scenarios, BKMR, BSTARSS, GAMDP, and GAMTS achieved moderate to high sensitivity (0.52-0.98) and specificity (0.21-0.99). BART and MARS achieved high specificity (≥0.90), but low sensitivity in low signal-to-noise ratio scenarios (0.20-0.51). Lasso was highly sensitive (0.71-0.99), except for quadratic relationships (≤0.27). Penalized regression methods that assume linearity, such as lasso, may not be suitable for studies of environmental chemicals hypothesized to have nonmonotonic relationships with outcomes. Instead, BKMR, BSTARSS, GAMDP, and GAMTS are attractive methods for flexibly estimating the shapes of exposure-response relationships and selecting among correlated exposures.},
  archive      = {J_SIM},
  author       = {Nina Lazarevic and Luke D. Knibbs and Peter D. Sly and Adrian G. Barnett},
  doi          = {10.1002/sim.8701},
  journal      = {Statistics in Medicine},
  month        = {11},
  number       = {27},
  pages        = {3947-3967},
  shortjournal = {Stat. Med.},
  title        = {Performance of variable and function selection methods for estimating the nonlinear health effects of correlated chemical mixtures: A simulation study},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). True verification probabilities should not be used in
estimating the area under receiver operating characteristic curve.
<em>SIM</em>, <em>39</em>(27), 3937–3946. (<a
href="https://doi.org/10.1002/sim.8700">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In medical research, a two-phase study is often used for the estimation of the area under the receiver operating characteristic curve (AUC) of a diagnostic test. However, such a design introduces verification bias. One of the methods to correct verification bias is inverse probability weighting (IPW). Since the probability a subject is selected into phase 2 of the study for disease verification is known, both true and estimated verification probabilities can be used to form an IPW estimator for AUC. In this article, we derive explicit variance formula for both IPW AUC estimators and show that the IPW AUC estimator using the true values of verification probabilities even when they are known are less efficient than its counterpart using the estimated values. Our simulation results show that the efficiency loss can be substantial especially when the variance of test result in disease population is small relative to its counterpart in nondiseased population.},
  archive      = {J_SIM},
  author       = {Yougui Wu},
  doi          = {10.1002/sim.8700},
  journal      = {Statistics in Medicine},
  month        = {11},
  number       = {27},
  pages        = {3937-3946},
  shortjournal = {Stat. Med.},
  title        = {True verification probabilities should not be used in estimating the area under receiver operating characteristic curve},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Design for immuno-oncology clinical trials enrolling both
responders and nonresponders. <em>SIM</em>, <em>39</em>(27), 3914–3936.
(<a href="https://doi.org/10.1002/sim.8694">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A typical challenge facing the design and analysis of immuno-oncology (IO) trials is the prevalence of nonproportional hazards (NPH) patterns manifested in Kaplan-Meier curves under time-to-event endpoints. The NPH patterns would violate the proportional hazards assumption, and yet conventional design and analysis strategies often ignore such a violation, resulting in underpowered or even falsely negative IO studies. In this article, we show, both empirically and analytically, that treating nonresponders in IO studies of inadequate size would give rise to a variety of NPH patterns; we then present a novel design and analysis strategy, P %- r esponder i nformation e m b e dded (PRIME), to properly incorporate the dichotomized response incurred from treating nonresponders. Empirical studies demonstrate that the proposed strategy can achieve desirable power, whereas the conventional alternative leads to a severe power loss. The PRIME strategy allows us to quantify the impact of treating nonresponders on study efficiency, thereby enabling a proper design of IO trials with an adequate power. More importantly, it pinpoints a solution to enhance the study efficiency and alleviates the NPH patterns by enrolling more prospective responders. An R package (Immunotherapy.Design) is developed for implementation.},
  archive      = {J_SIM},
  author       = {Zhenzhen Xu and Bin Zhu and Yongsoek Park},
  doi          = {10.1002/sim.8694},
  journal      = {Statistics in Medicine},
  month        = {11},
  number       = {27},
  pages        = {3914-3936},
  shortjournal = {Stat. Med.},
  title        = {Design for immuno-oncology clinical trials enrolling both responders and nonresponders},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A robust and unified framework for estimating heritability
in twin studies using generalized estimating equations. <em>SIM</em>,
<em>39</em>(27), 3897–3913. (<a
href="https://doi.org/10.1002/sim.8564">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The ‘heritability’ of a phenotype measures the proportion of trait variance due to genetic factors in a population. In the past 50 years, studies with monozygotic and dizygotic twins have estimated heritability for 17,804 traits; 1 thus twin studies are popular for estimating heritability. Researchers are often interested in estimating heritability for non-normally distributed outcomes such as binary, counts, skewed or heavy-tailed continuous traits. In these settings, the traditional normal ACE model (NACE) and Falconer&#39;s method can produce poor coverage of the true heritability. Therefore, we propose a robust generalized estimating equations (GEE2) framework for estimating the heritability of non-normally distributed outcomes. The traditional NACE and Falconer&#39;s method are derived within this unified GEE2 framework, which additionally provides robust standard errors. Although the traditional Falconer&#39;s method cannot adjust for covariates, the corresponding ‘GEE2-Falconer’ can incorporate mean and variance-level covariate effects (e.g. let heritability vary by sex or age). Given a non-normally distributed outcome, the GEE2 models are shown to attain better coverage of the true heritability compared to traditional methods. Finally, a scenario is demonstrated where NACE produces biased estimates of heritability while Falconer remains unbiased. Therefore, we recommend GEE2-Falconer for estimating the heritability of non-normally distributed outcomes in twin studies.},
  archive      = {J_SIM},
  author       = {Jaron Arbet and Matt McGue and Saonli Basu},
  doi          = {10.1002/sim.8564},
  journal      = {Statistics in Medicine},
  month        = {11},
  number       = {27},
  pages        = {3897-3913},
  shortjournal = {Stat. Med.},
  title        = {A robust and unified framework for estimating heritability in twin studies using generalized estimating equations},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Restricted mean survival time for interval-censored data.
<em>SIM</em>, <em>39</em>(26), 3879–3895. (<a
href="https://doi.org/10.1002/sim.8699">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Restricted mean survival time (RMST) evaluates the mean event-free survival time up to a prespecified time point. It has been used as an alternative measure of treatment effect owing to its model-free structure and clinically meaningful interpretation of treatment benefit for right-censored data. In clinical trials, another type of censoring called interval censoring may occur if subjects are examined at several discrete time points and the survival time falls into an interval rather than being exactly observed. The missingness of exact observations under interval-censored cases makes the nonparametric measure of treatment effect more challenging. Employing the linear smoothing technique to overcome the ambiguity, we propose a new model-free measure for the interval-censored RMST. As an alternative to the commonly used log-rank test, we further construct a hypothesis testing procedure to assess the survival difference between two groups. Simulation studies show that the bias of our proposed interval-censored RMST estimator is negligible and the testing procedure delivers promising performance in detecting between-group difference with regard to size and power under various configurations of survival curves. The proposed method is illustrated by reanalyzing two real datasets containing interval-censored observations.},
  archive      = {J_SIM},
  author       = {Chenyang Zhang and Yuanshan Wu and Guosheng Yin},
  doi          = {10.1002/sim.8699},
  journal      = {Statistics in Medicine},
  month        = {11},
  number       = {26},
  pages        = {3879-3895},
  shortjournal = {Stat. Med.},
  title        = {Restricted mean survival time for interval-censored data},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). On the relationship between association and surrogacy when
both the surrogate and true endpoint are binary outcomes. <em>SIM</em>,
<em>39</em>(26), 3867–3878. (<a
href="https://doi.org/10.1002/sim.8698">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The relationship between association and surrogacy has been the focus of much debate in the surrogate marker literature. Recently, the individual causal association (ICA) has been introduced as a metric of surrogacy in the causal inference framework, when both the surrogate and the true endpoint are normally distributed and when both are binary. Earlier work on the normal case has demonstrated that, although the ICA and the adjusted association are related metrics, their relationship strongly depends on unidentifiable parameters and, consequently, the association between both endpoints conveys little information on the validity of the surrogate. In addition, in the normal setting, the magnitude of the ICA does not depend on the mean of the outcomes. The latter implies that identifiable parameters such as mean responses and treatment effects provide no information on the validity of the surrogate. In the present work it is shown that this is fundamentally different in the binary case. We demonstrate that the observed association between the outcomes as well as the success rates in both treatment groups are quite predictive for the ICA. It is shown that finding a good surrogate will be more likely when the association between the endpoints is large, there are sizeable treatment effects and the success rates for both endpoints are similar in both treatment groups. These results are demonstrated using extensive simulations and illustrated on a case study in multi-drug resistant tuberculosis.},
  archive      = {J_SIM},
  author       = {Paul Meyvisch and Ariel Alonso and Wim Van der Elst and Geert Molenberghs},
  doi          = {10.1002/sim.8698},
  journal      = {Statistics in Medicine},
  month        = {11},
  number       = {26},
  pages        = {3867-3878},
  shortjournal = {Stat. Med.},
  title        = {On the relationship between association and surrogacy when both the surrogate and true endpoint are binary outcomes},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). On the impact of residential history in the spatial analysis
of diseases with a long latency period: A study of mesothelioma in
belgium. <em>SIM</em>, <em>39</em>(26), 3840–3866. (<a
href="https://doi.org/10.1002/sim.8697">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mesothelioma is a rare cancer caused by exposure to asbestos. Belgium has a known long history of asbestos production, resulting in one of the highest mesothelioma mortality rates worldwide. While the production of asbestos has stopped completely, the long latency period of mesothelioma, which can fluctuate between 20 and 40 years after exposure, causes incidences still to be frequent. Mesothelioma&#39;s long incubation time affects our assessment of its geographical distribution as well. Since patients&#39; residential locations are likely to change a number of times throughout their lives, the location where the patients develop the disease is often far from the location where they were exposed to asbestos. Using the residential history of patients, we propose the use of a convolution multiple membership model (MMM), which includes both a spatial conditional autoregressive and an unstructured random effect. Pancreatic cancer patients are used as a control population, reflecting the population at risk for mesothelioma. Results show the impact of the residential mobility on the geographical risk estimation, as well as the importance of acknowledging the latency period of a disease. A simulation study was conducted to investigate the properties of the convolution MMM. The robustness of the results for the convolution MMM is assessed via a sensitivity analysis.},
  archive      = {J_SIM},
  author       = {Oana Petrof and Thomas Neyens and Valerie Nuyts and Kristiaan Nackaerts and Benoit Nemery and Christel Faes},
  doi          = {10.1002/sim.8697},
  journal      = {Statistics in Medicine},
  month        = {11},
  number       = {26},
  pages        = {3840-3866},
  shortjournal = {Stat. Med.},
  title        = {On the impact of residential history in the spatial analysis of diseases with a long latency period: A study of mesothelioma in belgium},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Group sequential designs for clinical trials with bivariate
endpoints. <em>SIM</em>, <em>39</em>(26), 3823–3839. (<a
href="https://doi.org/10.1002/sim.8696">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Although all clinical trials are designed and monitored using more than one endpoint, methods are needed to assure that decision criteria are chosen to reflect the clinically relevant tradeoffs that assure the trial&#39;s scientific integrity. This article presents a framework for the design and monitoring clinical trials in a bivariate outcome space. The framework uses a rectangular hyperbola to define a bivariate null curve that divides outcome space into regions of benefit and lack of benefit. The curve is shown to be a flexible mapping of bivariate space that allows a continuous tradeoff between the two endpoints in a manner that captures many previous bivariate designs. The curve is extended to a distance function in bivariate space that allows different decisions in each of the four quadrants that comprise bivariate space. The distance function forms a statistic ( ); the distribution of its estimate is derived and used as a basis for trial design and group sequential monitoring plans in bivariate space. A recursive form of the bivariate group sequential density is used to evaluate and control operating characteristics for the proposed design. The bivariate designs are shown to meet or exceed the usual standards for size and power. The proposed design is illustrated in the ongoing NHLBI-sponsored Kids-DOTT multinational randomized controlled trial comparing shortened versus conventional anticoagulation for the treatment of venous thromboembolism in patients less than 21 years of age. The proposed methods are broadly applicable to a wide range of clinical settings and trial designs.},
  archive      = {J_SIM},
  author       = {Junxiao Hu and Patrick J. Blatchford and Neil A. Goldenberg and John M. Kittelson},
  doi          = {10.1002/sim.8696},
  journal      = {Statistics in Medicine},
  month        = {11},
  number       = {26},
  pages        = {3823-3839},
  shortjournal = {Stat. Med.},
  title        = {Group sequential designs for clinical trials with bivariate endpoints},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Use of tolerance intervals for assessing biosimilarity.
<em>SIM</em>, <em>39</em>(26), 3806–3822. (<a
href="https://doi.org/10.1002/sim.8695">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A biosimilar is a biological product that is highly similar to an existing approved reference drug and has no clinically meaningful difference from it. Biosimilars are composed of or derived from living cells or organisms. Therefore, they are often sensitive to slight variations in the manufacturing process. Consequently, in demonstrating biosimilarity, it might be inappropriate to focus solely on the mean difference, or ratio of means, while ignoring the variabilities associated with the test and reference products. It is important to account for the entire population of clinical outcomes. Thus, we propose using the concept of tolerance intervals and related hypothesis testing for assessing biosimilarity. Our approach has the advantage of considering entire populations associated with both groups. A real example is used to illustrate our proposed method, and our approach is more stringent than those that employ confidence intervals. This is specifically the case when the mean difference of two drugs is not sufficiently large, but the biosimilar has a higher variability than that in the reference drug.},
  archive      = {J_SIM},
  author       = {Chian Chen and Chin-Fu Hsiao},
  doi          = {10.1002/sim.8695},
  journal      = {Statistics in Medicine},
  month        = {11},
  number       = {26},
  pages        = {3806-3822},
  shortjournal = {Stat. Med.},
  title        = {Use of tolerance intervals for assessing biosimilarity},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Semiparametric estimation of the cure fraction in
population-based cancer survival analysis. <em>SIM</em>,
<em>39</em>(26), 3787–3805. (<a
href="https://doi.org/10.1002/sim.8693">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With rapid development in medical research, the treatment of diseases including cancer has progressed dramatically and those survivors may die from causes other than the one under study, especially among elderly patients. Motivated by the Surveillance, Epidemiology, and End Results (SEER) female breast cancer study, background mortality is incorporated into the mixture cure proportional hazards (MCPH) model to improve the cure fraction estimation in population-based cancer studies. Here, that patients are “cured” is defined as when the mortality rate of the individuals in diseased group returns to the same level as that expected in the general population, where the population level mortality is presented by the mortality table of the United States. The semiparametric estimation method based on the EM algorithm for the MCPH model with background mortality (MCPH+BM) is further developed and validated via comprehensive simulation studies. Real data analysis shows that the proposed semiparametric MCPH+BM model may provide more accurate estimation in population-level cancer study.},
  archive      = {J_SIM},
  author       = {Ennan Gu and Jiajia Zhang and Wenbin Lu and Lianming Wang and Federico Felizzi},
  doi          = {10.1002/sim.8693},
  journal      = {Statistics in Medicine},
  month        = {11},
  number       = {26},
  pages        = {3787-3805},
  shortjournal = {Stat. Med.},
  title        = {Semiparametric estimation of the cure fraction in population-based cancer survival analysis},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). An improved closed procedure for testing multiple
hypotheses. <em>SIM</em>, <em>39</em>(26), 3772–3786. (<a
href="https://doi.org/10.1002/sim.8692">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Clinical trials routinely involve multiple hypothesis testing. The closed testing procedure (CTP) is a fundamental principle in testing multiple hypotheses. This article presents an improved CTP in which intersection hypotheses can be tested at a level greater than α such that the control of the familywise error rate at level α remains. Consequently, our method uniformly improves the power of discovering false hypotheses over the original CTP. We illustrate that an improvement by our method exists for many commonly used tests. An empirical study on the effectiveness of a glucose-lowering drug is provided.},
  archive      = {J_SIM},
  author       = {Zeng-Hua Lu},
  doi          = {10.1002/sim.8692},
  journal      = {Statistics in Medicine},
  month        = {11},
  number       = {26},
  pages        = {3772-3786},
  shortjournal = {Stat. Med.},
  title        = {An improved closed procedure for testing multiple hypotheses},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A multiple imputation-based sensitivity analysis approach
for data subject to missing not at random. <em>SIM</em>,
<em>39</em>(26), 3756–3771. (<a
href="https://doi.org/10.1002/sim.8691">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Missingness mechanism is in theory unverifiable based only on observed data. If there is a suspicion of missing not at random, researchers often perform a sensitivity analysis to evaluate the impact of various missingness mechanisms. In general, sensitivity analysis approaches require a full specification of the relationship between missing values and missingness probabilities. Such relationship can be specified based on a selection model, a pattern-mixture model or a shared parameter model. Under the selection modeling framework, we propose a sensitivity analysis approach using a nonparametric multiple imputation strategy. The proposed approach only requires specifying the correlation coefficient between missing values and selection (response) probabilities under a selection model. The correlation coefficient is a standardized measure and can be used as a natural sensitivity analysis parameter. The sensitivity analysis involves multiple imputations of missing values, yet the sensitivity parameter is only used to select imputing/donor sets. Hence, the proposed approach might be more robust against misspecifications of the sensitivity parameter. For illustration, the proposed approach is applied to incomplete measurements of level of preoperative Hemoglobin A1c, for patients who had high-grade carotid artery stenosisa and were scheduled for surgery. A simulation study is conducted to evaluate the performance of the proposed approach.},
  archive      = {J_SIM},
  author       = {Chiu-Hsieh Hsu and Yulei He and Chengcheng Hu and Wei Zhou},
  doi          = {10.1002/sim.8691},
  journal      = {Statistics in Medicine},
  month        = {11},
  number       = {26},
  pages        = {3756-3771},
  shortjournal = {Stat. Med.},
  title        = {A multiple imputation-based sensitivity analysis approach for data subject to missing not at random},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Measurement error and precision medicine: Error-prone
tailoring covariates in dynamic treatment regimes. <em>SIM</em>,
<em>39</em>(26), 3732–3755. (<a
href="https://doi.org/10.1002/sim.8690">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Precision medicine incorporates patient-level covariates to tailor treatment decisions, seeking to improve outcomes. In longitudinal studies with time-varying covariates and sequential treatment decisions, precision medicine can be formalized with dynamic treatment regimes (DTRs): sequences of covariate-dependent treatment rules. To date, the precision medicine literature has not addressed a ubiquitous concern in health research—measurement error—where observed data deviate from the truth. We discuss the consequences of ignoring measurement error in the context of DTRs, focusing on challenges unique to precision medicine. We show—through simulation and theoretical results—that relatively simple measurement error correction techniques can lead to substantial improvements over uncorrected analyses, and apply these findings to the sequenced treatment alternatives to relieve depression study.},
  archive      = {J_SIM},
  author       = {Dylan Spicker and Michael P. Wallace},
  doi          = {10.1002/sim.8690},
  journal      = {Statistics in Medicine},
  month        = {11},
  number       = {26},
  pages        = {3732-3755},
  shortjournal = {Stat. Med.},
  title        = {Measurement error and precision medicine: Error-prone tailoring covariates in dynamic treatment regimes},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Dismantling the fragility index: A demonstration of
statistical reasoning. <em>SIM</em>, <em>39</em>(26), 3720–3731. (<a
href="https://doi.org/10.1002/sim.8689">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Fragility Index has been introduced as a complement to the P -value to summarize the statistical strength of evidence for a trial&#39;s result. The Fragility Index (FI) is defined in trials with two equal treatment group sizes, with a dichotomous or time-to-event outcome, and is calculated as the minimum number of conversions from nonevent to event in the treatment group needed to shift the P -value from Fisher&#39;s exact test over the .05 threshold. As the index lacks a well-defined probability motivation, its interpretation is challenging for consumers. We clarify what the FI may be capturing by separately considering two scenarios: (a) what the FI is capturing mathematically when the probability model is correct and (b) how well the FI captures violations of probability model assumptions. By calculating the posterior probability of a treatment effect, we show that when the probability model is correct, the FI inappropriately penalizes small trials for using fewer events than larger trials to achieve the same significance level. The analysis shows that for experiments conducted without bias, the FI promotes an incorrect intuition of probability, which has not been noted elsewhere and must be dispelled. We illustrate shortcomings of the FI&#39;s ability to quantify departures from model assumptions and contextualize the FI concept within current debate around the null hypothesis significance testing paradigm. Altogether, the FI creates more confusion than it resolves and does not promote statistical thinking. We recommend against its use. Instead, sensitivity analyses are recommended to quantify and communicate robustness of trial results.},
  archive      = {J_SIM},
  author       = {Gail E. Potter},
  doi          = {10.1002/sim.8689},
  journal      = {Statistics in Medicine},
  month        = {11},
  number       = {26},
  pages        = {3720-3731},
  shortjournal = {Stat. Med.},
  title        = {Dismantling the fragility index: A demonstration of statistical reasoning},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Genetic association studies with bivariate mixed responses
subject to measurement error and misclassification. <em>SIM</em>,
<em>39</em>(26), 3700–3719. (<a
href="https://doi.org/10.1002/sim.8688">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In genetic association studies, mixed effects models have been widely used in detecting the pleiotropy effects which occur when one gene affects multiple phenotype traits. In particular, bivariate mixed effects models are useful for describing the association of a gene with a continuous trait and a binary trait. However, such models are inadequate to feature the data with response mismeasurement, a characteristic that is often overlooked. It has been well studied that in univariate settings, ignorance of mismeasurement in variables usually results in biased estimation. In this paper, we consider the setting with a bivariate outcome vector which contains a continuous component and a binary component both subject to mismeasurement. We propose an induced likelihood approach and an EM algorithm method to handle measurement error in continuous response and misclassification in binary response simultaneously. Simulation studies confirm that the proposed methods successfully remove the bias induced from the response mismeasurement.},
  archive      = {J_SIM},
  author       = {Qihuang Zhang and Grace Y. Yi},
  doi          = {10.1002/sim.8688},
  journal      = {Statistics in Medicine},
  month        = {11},
  number       = {26},
  pages        = {3700-3719},
  shortjournal = {Stat. Med.},
  title        = {Genetic association studies with bivariate mixed responses subject to measurement error and misclassification},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Incorporating longitudinal biomarkers for dynamic risk
prediction in the era of big data: A pseudo-observation approach.
<em>SIM</em>, <em>39</em>(26), 3685–3699. (<a
href="https://doi.org/10.1002/sim.8687">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Longitudinal biomarker data are often collected in studies, providing important information regarding the probability of an outcome of interest occurring at a future time. With many new and evolving technologies for biomarker discovery, the number of biomarker measurements available for analysis of disease progression has increased dramatically. A large amount of data provides a more complete picture of a patient&#39;s disease progression, potentially allowing us to make more accurate and reliable predictions, but the magnitude of available data introduces challenges to most statistical analysts. Existing approaches suffer immensely from the curse of dimensionality. In this article, we propose methods for making dynamic risk predictions using repeatedly measured biomarkers of a large dimension, including cases when the number of biomarkers is close to the sample size. The proposed methods are computationally simple, yet sufficiently flexible to capture complex relationships between longitudinal biomarkers and potentially censored events times. The proposed approaches are evaluated by extensive simulation studies and are further illustrated by an application to a data set from the Nephrotic Syndrome Study Network.},
  archive      = {J_SIM},
  author       = {Lili Zhao and Susan Murray and Laura H. Mariani and Wenjun Ju},
  doi          = {10.1002/sim.8687},
  journal      = {Statistics in Medicine},
  month        = {11},
  number       = {26},
  pages        = {3685-3699},
  shortjournal = {Stat. Med.},
  title        = {Incorporating longitudinal biomarkers for dynamic risk prediction in the era of big data: A pseudo-observation approach},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Two-stage randomized trial design for testing treatment,
preference, and self-selection effects for count outcomes. <em>SIM</em>,
<em>39</em>(25), 3653–3683. (<a
href="https://doi.org/10.1002/sim.8686">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {While the traditional clinical trial design lays emphasis on testing the treatment effect between randomly assigned groups, it ignores the role of patient preference for a particular treatment in the trial. Yet, for healthcare providers who seek to optimize the patient-centered treatment strategy, the evaluation of a patient&#39;s psychology toward each treatment could be a key consideration. The two-stage randomized trial design allows researchers to test patient&#39;s preference and selection effects, in addition to the treatment effect. The current methodology for the two-stage design is limited to continuous and binary outcomes; this article extends the model to include count outcomes. The test statistics for preference, selection, and treatment effects are derived. Closed-form sample size formulae are presented for each effect. Simulations are presented to demonstrate the properties of the unstratified and stratified designs. Finally, we apply methods to the use of antimicrobials at the end of life to demonstrate the applicability of the methods.},
  archive      = {J_SIM},
  author       = {Yu Shi and Briana Cameron and Xian Gu and Michael Kane and Peter Peduzzi and Denise A. Esserman},
  doi          = {10.1002/sim.8686},
  journal      = {Statistics in Medicine},
  month        = {11},
  number       = {25},
  pages        = {3653-3683},
  shortjournal = {Stat. Med.},
  title        = {Two-stage randomized trial design for testing treatment, preference, and self-selection effects for count outcomes},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Parameter-expanded data augmentation for analyzing
correlated binary data using multivariate probit models. <em>SIM</em>,
<em>39</em>(25), 3637–3652. (<a
href="https://doi.org/10.1002/sim.8685">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data augmentation has been commonly utilized to analyze correlated binary data using multivariate probit models in Bayesian analysis. However, the identification issue in the multivariate probit models necessitates a rigorous Metropolis-Hastings algorithm for sampling a correlation matrix, which may cause slow convergence and inefficiency of Markov chains. It is well-known that the parameter-expanded data augmentation, by introducing a working/artificial parameter or parameter vector, makes an identifiable model be non-identifiable and improves the mixing and convergence of data augmentation components. Therefore, we motivate to develop efficient parameter-expanded data augmentations to analyze correlated binary data using multivariate probit models. We investigate both the identifiable and non-identifiable multivariate probit models and develop the corresponding parameter-expanded data augmentation algorithms. We point out that the approaches, based on one non-identifiable model, circumvent a Metropolis-Hastings algorithm for sampling a correlation matrix and improve the convergence and mixing of correlation parameters; the identifiable model may produce the estimated regression parameters with smaller standard errors than the non-identifiable model does. We illustrate our proposed approaches using simulation studies and through the application to a longitudinal dataset from the Six Cities study.},
  archive      = {J_SIM},
  author       = {Xiao Zhang},
  doi          = {10.1002/sim.8685},
  journal      = {Statistics in Medicine},
  month        = {11},
  number       = {25},
  pages        = {3637-3652},
  shortjournal = {Stat. Med.},
  title        = {Parameter-expanded data augmentation for analyzing correlated binary data using multivariate probit models},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). The timing of geographic power. <em>SIM</em>,
<em>39</em>(25), 3624–3636. (<a
href="https://doi.org/10.1002/sim.8684">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In many studies on the spatial risk of disease, investigators use geographic locations at the time of disease diagnosis in spatial models to search for individual areas of elevated risk. However, these studies often fail to find a significant spatial signal. This may be due to the misspecification of the timing and location of pertinent exposures. Environmental exposures related to cancer risk vary over space and time, and many cancers have long latencies. When these factors are considered in conjunction with a mobile population, it is likely that the spatial signal related to relevant historic environmental exposures is obscured. To investigate this hypothesis, we conducted simulation studies to characterize the effect of residential mobility on the ability of generalized additive models to detect areas of significantly elevated historic environmental exposure. We generated data based on the residential histories of participants in the National Cancer Institute Surveillance, Epidemiology, and End Results non-Hodgkin lymphoma study, and varied the duration and intensity of the environmental exposure. Results showed that the probability of detection, mean spatial sensitivity, and mean spatial specificity of models decreased steadily as the time since relevant exposure increased. This suggests that for diseases with long latencies, spatial areas of high risk due to high-intensity exposure of relatively short duration will be difficult to detect over time when using residential locations at the time of diagnosis in mobile study populations.},
  archive      = {J_SIM},
  author       = {Anny-Claude Joseph and Catherine A. Calder and David C. Wheeler},
  doi          = {10.1002/sim.8684},
  journal      = {Statistics in Medicine},
  month        = {11},
  number       = {25},
  pages        = {3624-3636},
  shortjournal = {Stat. Med.},
  title        = {The timing of geographic power},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Sample size calculation for cluster randomization trials
with a time-to-event endpoint. <em>SIM</em>, <em>39</em>(25), 3608–3623.
(<a href="https://doi.org/10.1002/sim.8683">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cluster randomization trials randomize groups (called clusters) of subjects (called subunits) between intervention arms, and observations are collected from each subject. In this case, subunits within each cluster share common frailties, so that the observations from subunits of each cluster tend to be correlated. Oftentimes, the outcome of a cluster randomization trial is a time-to-event endpoint with censoring. In this article, we propose a closed form sample size formula for weighted rank tests to compare the marginal survival distributions between intervention arms under cluster randomization with possibly variable cluster sizes. Extensive simulation studies are conducted to evaluate the performance of our sample size formula under various design settings. Real study examples are taken to demonstrate our method.},
  archive      = {J_SIM},
  author       = {Jianghao Li and Sin-Ho Jung},
  doi          = {10.1002/sim.8683},
  journal      = {Statistics in Medicine},
  month        = {11},
  number       = {25},
  pages        = {3608-3623},
  shortjournal = {Stat. Med.},
  title        = {Sample size calculation for cluster randomization trials with a time-to-event endpoint},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Handling missing predictor values when validating and
applying a prediction model to new patients. <em>SIM</em>,
<em>39</em>(25), 3591–3607. (<a
href="https://doi.org/10.1002/sim.8682">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Missing data present challenges for development and real-world application of clinical prediction models. While these challenges have received considerable attention in the development setting, there is only sparse research on the handling of missing data in applied settings. The main unique feature of handling missing data in these settings is that missing data methods have to be performed for a single new individual, precluding direct application of mainstay methods used during model development. Correspondingly, we propose that it is desirable to perform model validation using missing data methods that transfer to practice in single new patients. This article compares existing and new methods to account for missing data for a new individual in the context of prediction. These methods are based on (i) submodels based on observed data only, (ii) marginalization over the missing variables, or (iii) imputation based on fully conditional specification (also known as chained equations). They were compared in an internal validation setting to highlight the use of missing data methods that transfer to practice while validating a model. As a reference, they were compared to the use of multiple imputation by chained equations in a set of test patients, because this has been used in validation studies in the past. The methods were evaluated in a simulation study where performance was measured by means of optimism corrected C-statistic and mean squared prediction error. Furthermore, they were applied in data from a large Dutch cohort of prophylactic implantable cardioverter defibrillator patients.},
  archive      = {J_SIM},
  author       = {Jeroen Hoogland and Marit van Barreveld and Thomas P. A. Debray and Johannes B. Reitsma and Tom E. Verstraelen and Marcel G. W. Dijkgraaf and Aeilko H. Zwinderman},
  doi          = {10.1002/sim.8682},
  journal      = {Statistics in Medicine},
  month        = {11},
  number       = {25},
  pages        = {3591-3607},
  shortjournal = {Stat. Med.},
  title        = {Handling missing predictor values when validating and applying a prediction model to new patients},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A hidden markov model for population-level cervical cancer
screening data. <em>SIM</em>, <em>39</em>(25), 3569–3590. (<a
href="https://doi.org/10.1002/sim.8681">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Cancer Registry of Norway has been administrating a national cervical cancer screening program since 1992 by coordinating triennial cytology exam screenings for the female population between 25 and 69 years of age. Up to 80% of cancers are prevented through mass screening, but this comes at the expense of considerable screening activity and leads to overtreatment of clinically asymptomatic precancers. In this article, we present a continuous-time, time-inhomogeneous hidden Markov model which was developed to understand the screening process and cervical cancer carcinogenesis in detail. By leveraging 1.7 million individual&#39;s multivariate time-series of medical exams performed over a 25-year period, we simultaneously estimate all model parameters. We show that an age-dependent model reflects the Norwegian screening program by comparing empirical survival curves from observed registry data and data simulated from the proposed model. The model can be generalized to include more detailed individual-level covariates as well as new types of screening exams. By utilizing individual screening histories and covariate data, the proposed model shows potential for improving strategies for cancer screening programs by personalizing recommended screening intervals.},
  archive      = {J_SIM},
  author       = {Braden C. Soper and Mari Nygård and Ghaleb Abdulla and Rui Meng and Jan F. Nygård},
  doi          = {10.1002/sim.8681},
  journal      = {Statistics in Medicine},
  month        = {11},
  number       = {25},
  pages        = {3569-3590},
  shortjournal = {Stat. Med.},
  title        = {A hidden markov model for population-level cervical cancer screening data},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Partially linear monotone methods with automatic variable
selection and monotonicity direction discovery. <em>SIM</em>,
<em>39</em>(25), 3549–3568. (<a
href="https://doi.org/10.1002/sim.8680">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In many statistical regression and prediction problems, it is reasonable to assume monotone relationships between certain predictor variables and the outcome. Genomic effects on phenotypes are, for instance, often assumed to be monotone. However, in some settings, it may be reasonable to assume a partially linear model, where some of the covariates can be assumed to have a linear effect. One example is a prediction model using both high-dimensional gene expression data, and low-dimensional clinical data, or when combining continuous and categorical covariates. We study methods for fitting the partially linear monotone model, where some covariates are assumed to have a linear effect on the response, and some are assumed to have a monotone (potentially nonlinear) effect. Most existing methods in the literature for fitting such models are subject to the limitation that they have to be provided the monotonicity directions a priori for the different monotone effects. We here present methods for fitting partially linear monotone models which perform both automatic variable selection, and monotonicity direction discovery. The proposed methods perform comparably to, or better than, existing methods, in terms of estimation, prediction, and variable selection performance, in simulation experiments in both classical and high-dimensional data settings.},
  archive      = {J_SIM},
  author       = {Solveig Engebretsen and Ingrid K. Glad},
  doi          = {10.1002/sim.8680},
  journal      = {Statistics in Medicine},
  month        = {11},
  number       = {25},
  pages        = {3549-3568},
  shortjournal = {Stat. Med.},
  title        = {Partially linear monotone methods with automatic variable selection and monotonicity direction discovery},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Active learning for efficiently training emulators of
computationally expensive mathematical models. <em>SIM</em>,
<em>39</em>(25), 3521–3548. (<a
href="https://doi.org/10.1002/sim.8679">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {An emulator is a fast-to-evaluate statistical approximation of a detailed mathematical model (simulator). When used in lieu of simulators, emulators can expedite tasks that require many repeated evaluations, such as sensitivity analyses, policy optimization, model calibration, and value-of-information analyses. Emulators are developed using the output of simulators at specific input values (design points). Developing an emulator that closely approximates the simulator can require many design points, which becomes computationally expensive. We describe a self-terminating active learning algorithm to efficiently develop emulators tailored to a specific emulation task, and compare it with algorithms that optimize geometric criteria (random latin hypercube sampling and maximum projection designs) and other active learning algorithms (treed Gaussian Processes that optimize typical active learning criteria). We compared the algorithms&#39; root mean square error (RMSE) and maximum absolute deviation from the simulator (MAX) for seven benchmark functions and in a prostate cancer screening model. In the empirical analyses, in simulators with greatly varying smoothness over the input domain, active learning algorithms resulted in emulators with smaller RMSE and MAX for the same number of design points. In all other cases, all algorithms performed comparably. The proposed algorithm attained satisfactory performance in all analyses, had smaller variability than the treed Gaussian Processes, and, on average, had similar or better performance as the treed Gaussian Processes in six out of seven benchmark functions and in the prostate cancer model.},
  archive      = {J_SIM},
  author       = {Alexandra G. Ellis and Rowan Iskandar and Christopher H. Schmid and John B. Wong and Thomas A. Trikalinos},
  doi          = {10.1002/sim.8679},
  journal      = {Statistics in Medicine},
  month        = {11},
  number       = {25},
  pages        = {3521-3548},
  shortjournal = {Stat. Med.},
  title        = {Active learning for efficiently training emulators of computationally expensive mathematical models},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Ascertaining properties of weighting in the estimation of
optimal treatment regimes under monotone missingness. <em>SIM</em>,
<em>39</em>(25), 3503–3520. (<a
href="https://doi.org/10.1002/sim.8678">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dynamic treatment regimes operationalize precision medicine as a sequence of decision rules, one per stage of clinical intervention, that map up-to-date patient information to a recommended intervention. An optimal treatment regime maximizes the mean utility when applied to the population of interest. Methods for estimating an optimal treatment regime assume the data to be fully observed, which rarely occurs in practice. A common approach is to first use multiple imputation and then pool the estimators across imputed datasets. However, this approach requires estimating the joint distribution of patient trajectories, which can be high-dimensional, especially when there are multiple stages of intervention. We examine the application of inverse probability weighted estimating equations as an alternative to multiple imputation in the context of monotonic missingness. This approach applies to a broad class of estimators of an optimal treatment regime including both Q-learning and a generalization of outcome weighted learning. We establish consistency under mild regularity conditions and demonstrate its advantages in finite samples using a series of simulation experiments and an application to a schizophrenia study.},
  archive      = {J_SIM},
  author       = {Lin Dong and Eric Laber and Yair Goldberg and Rui Song and Shu Yang},
  doi          = {10.1002/sim.8678},
  journal      = {Statistics in Medicine},
  month        = {11},
  number       = {25},
  pages        = {3503-3520},
  shortjournal = {Stat. Med.},
  title        = {Ascertaining properties of weighting in the estimation of optimal treatment regimes under monotone missingness},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A segmented measurement error model for modeling and
analysis of method comparison data. <em>SIM</em>, <em>39</em>(25),
3491–3502. (<a href="https://doi.org/10.1002/sim.8677">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Method comparison studies are concerned with estimating relationship between two clinical measurement methods. The methods often exhibit a structural change in the relationship over the measurement range. Ignoring this change would lead to an inaccurate estimate of the relationship. Motivated by a study of two digoxin assays where such a change occurs, this article develops a statistical methodology for appropriately analyzing such studies. Specifically, it proposes a segmented extension of the classical measurement error model to allow a piecewise linear relationship between the methods. The changepoint at which the transition takes place is treated as an unknown parameter in the model. An expectation-maximization-type algorithm is developed to fit the model and appropriate extensions of the existing measures are proposed for segment-specific evaluation of similarity and agreement. Bootstrapping and large-sample theory of maximum likelihood estimators are employed to perform the relevant inferences. The proposed methodology is evaluated by simulation and is illustrated by analyzing the digoxin data.},
  archive      = {J_SIM},
  author       = {Lak N. Kotinkaduwa and Pankaj K. Choudhary},
  doi          = {10.1002/sim.8677},
  journal      = {Statistics in Medicine},
  month        = {11},
  number       = {25},
  pages        = {3491-3502},
  shortjournal = {Stat. Med.},
  title        = {A segmented measurement error model for modeling and analysis of method comparison data},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Flexible bivariate correlated count data regression.
<em>SIM</em>, <em>39</em>(25), 3476–3490. (<a
href="https://doi.org/10.1002/sim.8676">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multivariate count data are common in many disciplines. The variables in such data often exhibit complex positive or negative dependency structures. We propose three Bayesian approaches to modeling bivariate count data by simultaneously considering covariate-dependent means and correlation. A direct approach utilizes a bivariate negative binomial probability mass function developed in Famoye (2010, Journal of Applied Statistics ). The second approach fits bivariate count data indirectly using a bivariate Poisson-gamma mixture model. The third approach is a bivariate Gaussian copula model. Based on the results from simulation analyses, the indirect and copula approaches perform better overall than the direct approach in terms of model fitting and identifying covariate-dependent association. The proposed approaches are applied to two RNA-sequencing data sets for studying breast cancer and melanoma (BRCA-US and SKCM-US), respectively, obtained through the International Cancer Genome Consortium.},
  archive      = {J_SIM},
  author       = {Zichen Ma and Timothy E. Hanson and Yen-Yi Ho},
  doi          = {10.1002/sim.8676},
  journal      = {Statistics in Medicine},
  month        = {11},
  number       = {25},
  pages        = {3476-3490},
  shortjournal = {Stat. Med.},
  title        = {Flexible bivariate correlated count data regression},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Bayesian methods for the analysis of early-phase oncology
basket trials with information borrowing across cancer types.
<em>SIM</em>, <em>39</em>(25), 3459–3475. (<a
href="https://doi.org/10.1002/sim.8675">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Research in oncology has changed the focus from histological properties of tumors in a specific organ to a specific genomic aberration potentially shared by multiple cancer types. This motivates the basket trial, which assesses the efficacy of treatment simultaneously on multiple cancer types that have a common aberration. Although the assumption of homogeneous treatment effects seems reasonable given the shared aberration, in reality, the treatment effect may vary by cancer type, and potentially only a subgroup of the cancer types respond to the treatment. Various approaches have been proposed to increase the trial power by borrowing information across cancer types, which, however, tend to inflate the type I error rate. In this article, we review some representative Bayesian information borrowing methods for the analysis of early-phase basket trials. We then propose a novel method called the Bayesian hierarchical model with a correlated prior (CBHM), which conducts more flexible borrowing across cancer types according to sample similarity. We did simulation studies to compare CBHM with independent analysis and three information borrowing approaches: the conventional Bayesian hierarchical model, the EXNEX approach, and Liu&#39;s two-stage approach. Simulation results show that all information borrowing approaches substantially improve the power of independent analysis if a large proportion of the cancer types truly respond to the treatment. Our proposed CBHM approach shows an advantage over the existing information borrowing approaches, with a power similar to that of EXNEX or Liu&#39;s approach, but the potential to provide substantially better control of type I error rate.},
  archive      = {J_SIM},
  author       = {Jin Jin and Marie-Karelle Riviere and Xiaodong Luo and Yingwen Dong},
  doi          = {10.1002/sim.8675},
  journal      = {Statistics in Medicine},
  month        = {11},
  number       = {25},
  pages        = {3459-3475},
  shortjournal = {Stat. Med.},
  title        = {Bayesian methods for the analysis of early-phase oncology basket trials with information borrowing across cancer types},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Score confidence intervals and sample sizes for stratified
comparisons of binomial proportions. <em>SIM</em>, <em>39</em>(24),
3427–3457. (<a href="https://doi.org/10.1002/sim.8674">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In a series of articles, Gart and Nam construct the efficient score tests and confidence intervals with or without skewness correction for stratified comparisons of binomial proportions on the risk difference, relative risk, and odds ratio effect metrics. However, the stratified score methods and their properties are not well understood. We rederive the efficient score tests, which reveals their theoretical relationship with the contrast-based score tests, and provides a basis for adapting the method by using other weighting schemes. The inverse variance weight is optimal for a common treatment effect in large samples. We explore the behavior of the score approach in the presence of extreme outcomes when either no or all subjects in some strata are responders, and provide guidance on the choice of weights in the analysis of rare events. The score method is recommended for studies with a small number of moderate or large sized strata. A general framework is proposed to calculate the asymptotic power and sample size for the score test in superiority, noninferiority and equivalence clinical trials, or case-control studies. We also describe a nearly exact procedure that underestimates the exact power, but the degree of underestimation can be controlled to a negligible level. The proposed methods are illustrated by numerical examples.},
  archive      = {J_SIM},
  author       = {Yongqiang Tang},
  doi          = {10.1002/sim.8674},
  journal      = {Statistics in Medicine},
  month        = {10},
  number       = {24},
  pages        = {3427-3457},
  shortjournal = {Stat. Med.},
  title        = {Score confidence intervals and sample sizes for stratified comparisons of binomial proportions},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Developing biomarker combinations in multicenter studies via
direct maximization and penalization. <em>SIM</em>, <em>39</em>(24),
3412–3426. (<a href="https://doi.org/10.1002/sim.8673">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Motivated by a study of acute kidney injury, we consider the setting of biomarker studies involving patients at multiple centers where the goal is to develop a biomarker combination for diagnosis, prognosis, or screening. As biomarker studies become larger, this type of data structure will be encountered more frequently. In the presence of multiple centers, one way to assess the predictive capacity of a given combination is to consider the center-adjusted area under the receiver operating characteristic curve (aAUC), a summary of the ability of the combination to discriminate between cases and controls in each center. Rather than using a general method, such as logistic regression, to construct the biomarker combination, we propose directly maximizing the aAUC. Furthermore, it may be desirable to have a biomarker combination with similar performance across centers. To that end, we allow for penalization of the variability in the center-specific AUCs. We demonstrate desirable asymptotic properties of the resulting combinations. Simulations provide small-sample evidence that maximizing the aAUC can lead to combinations with improved performance. We also use simulated data to illustrate the utility of constructing combinations by maximizing the aAUC while penalizing variability. Finally, we apply these methods to data from the study of acute kidney injury.},
  archive      = {J_SIM},
  author       = {Allison Meisner and Chirag R. Parikh and Kathleen F. Kerr},
  doi          = {10.1002/sim.8673},
  journal      = {Statistics in Medicine},
  month        = {10},
  number       = {24},
  pages        = {3412-3426},
  shortjournal = {Stat. Med.},
  title        = {Developing biomarker combinations in multicenter studies via direct maximization and penalization},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Sensitivity analysis of treatment effect to unmeasured
confounding in observational studies with survival and competing risks
outcomes. <em>SIM</em>, <em>39</em>(24), 3397–3411. (<a
href="https://doi.org/10.1002/sim.8672">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {No unmeasured confounding is often assumed in estimating treatment effects in observational data, whether using classical regression models or approaches such as propensity scores and inverse probability weighting. However, in many such studies collection of confounders cannot possibly be exhaustive in practice, and it is crucial to examine the extent to which the resulting estimate is sensitive to the unmeasured confounders. We consider this problem for survival and competing risks data. Due to the complexity of models for such data, we adapt the simulated potential confounder approach of Carnegie et al (2016), which provides a general tool for sensitivity analysis due to unmeasured confounding. More specifically, we specify one sensitivity parameter to quantify the association between an unmeasured confounder and the exposure or treatment received, and another set of parameters to quantify the association between the confounder and the time-to-event outcomes. By varying the magnitudes of the sensitivity parameters, we estimate the treatment effect of interest using the stochastic expectation-maximization (EM) and the EM algorithms. We demonstrate the performance of our methods on simulated data, and apply them to a comparative effectiveness study in inflammatory bowel disease. An R package “survSens” is available on CRAN that implements the proposed methodology.},
  archive      = {J_SIM},
  author       = {Rong Huang and Ronghui Xu and Parambir S. Dulai},
  doi          = {10.1002/sim.8672},
  journal      = {Statistics in Medicine},
  month        = {10},
  number       = {24},
  pages        = {3397-3411},
  shortjournal = {Stat. Med.},
  title        = {Sensitivity analysis of treatment effect to unmeasured confounding in observational studies with survival and competing risks outcomes},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Smoothed time-dependent receiver operating characteristic
curve for right censored survival data. <em>SIM</em>, <em>39</em>(24),
3373–3396. (<a href="https://doi.org/10.1002/sim.8671">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The prediction reliability is of primary concern in many clinical studies when the objective is to develop new predictive models or improve existing risk scores. In fact, before using a model in any clinical decision making, it is very important to check its ability to discriminate between subjects who are at risk of, for example, developing certain disease in a near future from those who will not. To that end, the time-dependent receiver operating characteristic (ROC) curve is the most commonly used method in practice. Several approaches have been proposed in the literature to estimate the ROC nonparametrically in the context of survival data. But, except one recent approach, all the existing methods provide a nonsmooth ROC estimator whereas, by definition, the ROC curve is smooth. In this article we propose and study a new nonparametric smooth ROC estimator based on a weighted kernel smoother. More precisely, our approach relies on a well-known kernel method used to estimate cumulative distribution functions of random variables with bounded supports. We derived some asymptotic properties for the proposed estimator. As bandwidth is the main parameter to be set, we present and study different methods to appropriately select one. A simulation study is conducted, under different scenarios, to prove the consistency of the proposed method and to compare its finite sample performance with a competitor. The results show that the proposed method performs better and appear to be quite robust to bandwidth choice. As for inference purposes, our results also reveal the good performances of a proposed nonparametric bootstrap procedure. Furthermore, we illustrate the method using a real data example.},
  archive      = {J_SIM},
  author       = {Kassu Mehari Beyene and Anouar El Ghouch},
  doi          = {10.1002/sim.8671},
  journal      = {Statistics in Medicine},
  month        = {10},
  number       = {24},
  pages        = {3373-3396},
  shortjournal = {Stat. Med.},
  title        = {Smoothed time-dependent receiver operating characteristic curve for right censored survival data},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Sample size calculation in three-level cluster randomized
trials using generalized estimating equation models. <em>SIM</em>,
<em>39</em>(24), 3347–3372. (<a
href="https://doi.org/10.1002/sim.8670">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Three-level cluster randomized trials (CRTs) are increasingly used in implementation science, where 2fold-nested-correlated data arise. For example, interventions are randomly assigned to practices, and providers within the same practice who provide care to participants are trained with the assigned intervention. Teerenstra et al proposed a nested exchangeable correlation structure that accounts for two levels of clustering within the generalized estimating equations (GEE) approach. In this article, we utilize GEE models to test the treatment effect in a two-group comparison for continuous, binary, or count data in three-level CRTs. Given the nested exchangeable correlation structure, we derive the asymptotic variances of the estimator of the treatment effect for different types of outcomes. When the number of clusters is small, researchers have proposed bias-corrected sandwich estimators to improve performance in two-level CRTs. We extend the variances of two bias-corrected sandwich estimators to three-level CRTs. The equal provider and practice sizes were assumed to calculate number of practices for simplicity. However, they are not guaranteed in practice. Relative efficiency (RE) is defined as the ratio of variance of the estimator of the treatment effect for equal to unequal provider and practice sizes. The expressions of REs are obtained from both asymptotic variance estimation and bias-corrected sandwich estimators. Their performances are evaluated for different scenarios of provider and practice size distributions through simulation studies. Finally, a percentage increase in the number of practices is proposed due to efficiency loss from unequal provider and/or practice sizes.},
  archive      = {J_SIM},
  author       = {Jingxia Liu and Graham A. Colditz},
  doi          = {10.1002/sim.8670},
  journal      = {Statistics in Medicine},
  month        = {10},
  number       = {24},
  pages        = {3347-3372},
  shortjournal = {Stat. Med.},
  title        = {Sample size calculation in three-level cluster randomized trials using generalized estimating equation models},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Exploiting relationships between outcomes in bayesian
multivariate network meta-analysis with an application to
relapsing-remitting multiple sclerosis. <em>SIM</em>, <em>39</em>(24),
3329–3346. (<a href="https://doi.org/10.1002/sim.8668">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In multivariate network meta-analysis (NMA), the piecemeal nature of the evidence base means that there may be treatment-outcome combinations for which no data is available. Most existing multivariate evidence synthesis models are either unable to estimate the missing treatment-outcome combinations, or can only do so under particularly strong assumptions, such as perfect between-study correlations between outcomes or constant effect size across outcomes. Many existing implementations are also limited to two treatments or two outcomes, or rely on model specification that is heavily tailored to the dimensions of the dataset. We present a Bayesian multivariate NMA model that estimates the missing treatment-outcome combinations via mappings between the population mean effects, while allowing the study-specific effects to be imperfectly correlated. The method is designed for aggregate-level data (rather than individual patient data) and is likely to be useful when modeling multiple sparsely reported outcomes, or when varying definitions of the same underlying outcome are adopted by different studies. We implement the model via a novel decomposition of the treatment effect variance, which can be specified efficiently for an arbitrary dataset given some basic assumptions regarding the correlation structure. The method is illustrated using data concerning the efficacy and liver-related safety of eight active treatments for relapsing-remitting multiple sclerosis. The results indicate that fingolimod and interferon beta-1b are the most efficacious treatments but also have some of the worst effects on liver safety. Dimethyl fumarate and glatiramer acetate perform reasonably on all of the efficacy and safety outcomes in the model.},
  archive      = {J_SIM},
  author       = {Ed Waddingham and Paul M. Matthews and Deborah Ashby},
  doi          = {10.1002/sim.8668},
  journal      = {Statistics in Medicine},
  month        = {10},
  number       = {24},
  pages        = {3329-3346},
  shortjournal = {Stat. Med.},
  title        = {Exploiting relationships between outcomes in bayesian multivariate network meta-analysis with an application to relapsing-remitting multiple sclerosis},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Longitudinal partially ordered data analysis for preclinical
sarcopenia. <em>SIM</em>, <em>39</em>(24), 3313–3328. (<a
href="https://doi.org/10.1002/sim.8667">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sarcopenia is a geriatric syndrome characterized by significant loss of muscle mass. Based on a commonly used definition of the condition that involves three measurements, different subclinical and clinical states of sarcopenia are formed. These states constitute a partially ordered set (poset). This article focuses on the analysis of longitudinal poset in the context of sarcopenia. We propose an extension of the generalized linear mixed model and a recoding scheme for poset analysis such that two submodels—one for ordered categories and one for nominal categories—that include common random effects can be jointly estimated. The new poset model postulates random effects conceptualized as latent variables that represent an underlying construct of interest, that is, susceptibility to sarcopenia over time. We demonstrate how information can be gleaned from nominal sarcopenic states for strengthening statistical inference on a person&#39;s susceptibility to sarcopenia.},
  archive      = {J_SIM},
  author       = {Edward H. Ip and Shyh-Huei Chen and Karen Bandeen-Roche and Jaime L. Speiser and Li Cai and Denise K. Houston},
  doi          = {10.1002/sim.8667},
  journal      = {Statistics in Medicine},
  month        = {10},
  number       = {24},
  pages        = {3313-3328},
  shortjournal = {Stat. Med.},
  title        = {Longitudinal partially ordered data analysis for preclinical sarcopenia},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Multinomial logistic regression with missing outcome data:
An application to cancer subtypes. <em>SIM</em>, <em>39</em>(24),
3299–3312. (<a href="https://doi.org/10.1002/sim.8666">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many diseases such as cancer and heart diseases are heterogeneous and it is of great interest to study the disease risk specific to the subtypes in relation to genetic and environmental risk factors. However, due to logistic and cost reasons, the subtype information for the disease is missing for some subjects. In this article, we investigate methods for multinomial logistic regression with missing outcome data, including a bootstrap hot deck multiple imputation (BHMI), simple inverse probability weighted (SIPW), augmented inverse probability weighted (AIPW), and expected estimating equation (EEE) estimators. These methods are important approaches for missing data regression. The BHMI modifies the standard hot deck multiple imputation method such that it can provide valid confidence interval estimation. Under the situation when the covariates are discrete, the SIPW, AIPW, and EEE estimators are numerically identical. When the covariates are continuous, nonparametric smoothers can be applied to estimate the selection probabilities and the estimating scores. These methods perform similarly. Extensive simulations show that all of these methods yield unbiased estimators while the complete-case (CC) analysis can be biased if the missingness depends on the observed data. Our simulations also demonstrate that these methods can gain substantial efficiency compared with the CC analysis. The methods are applied to a colorectal cancer study in which cancer subtype data are missing among some study individuals.},
  archive      = {J_SIM},
  author       = {Ching-Yun Wang and Li Hsu},
  doi          = {10.1002/sim.8666},
  journal      = {Statistics in Medicine},
  month        = {10},
  number       = {24},
  pages        = {3299-3312},
  shortjournal = {Stat. Med.},
  title        = {Multinomial logistic regression with missing outcome data: An application to cancer subtypes},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Developing and testing high-efficacy patient subgroups
within a clinical trial using risk scores. <em>SIM</em>,
<em>39</em>(24), 3285–3298. (<a
href="https://doi.org/10.1002/sim.8665">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {There is the potential for high-dimensional information about patients collected in clinical trials (such as genomic, imaging, and data from wearable technologies) to be informative for the efficacy of a new treatment in situations where only a subset of patients benefits from the treatment. The adaptive signature design (ASD) method has been proposed for developing and testing the efficacy of a treatment in a high-efficacy patient group (the sensitive group) using genetic data. The method requires selection of three tuning parameters which may be highly computationally expensive. We propose a variation to the ASD method, the cross-validated risk scores (CVRS) design method, that does not require selection of any tuning parameters. The method is based on computing a risk score for each patient and dividing them into clusters using a nonparametric clustering procedure. We assess the properties of CVRS against the originally proposed cross-validated ASD using simulation data and a real psychiatry trial. CVRS, as assessed for various sample sizes and response rates, has a substantial reduction in the computational time required. In many simulation scenarios, there is a substantial improvement in the ability to correctly identify the sensitive group and the power of the design to detect a treatment effect in the sensitive group. We illustrate the application of the CVRS method on the psychiatry trial.},
  archive      = {J_SIM},
  author       = {Svetlana Cherlin and James M. S. Wason},
  doi          = {10.1002/sim.8665},
  journal      = {Statistics in Medicine},
  month        = {10},
  number       = {24},
  pages        = {3285-3298},
  shortjournal = {Stat. Med.},
  title        = {Developing and testing high-efficacy patient subgroups within a clinical trial using risk scores},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A new cure rate model with flexible competing causes with
applications to melanoma and transplantation data. <em>SIM</em>,
<em>39</em>(24), 3272–3284. (<a
href="https://doi.org/10.1002/sim.8664">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we introduce a long-term survival model in which the number of competing causes of the event of interest follows the zero-modified geometric (ZMG) distribution. Such distribution accommodates equidispersion, underdispersion, and overdispersion and captures deflation or inflation of zeros in the number of lesions or initiated cells after the treatment. The ZMG distribution is also an appropriate alternative for modeling clustered samples when the number of competing causes of the event of interest consists of two subpopulations, one containing only zeros (cure proportion), while in the other (noncure proportion) the number of competing causes of the event of interest follows a geometric distribution. The advantage of this assumption is that we can measure the cure proportion in the initiated cells. Furthermore, the proposed model can yield greater or lower cure proportion than that of the geometric distribution when modeling the number of competing causes. In this article, we present some statistical properties of the proposed model and use the maximum likelihood method to estimate the model parameters. We also conduct a Monte Carlo simulation study to evaluate the performance of the estimators. We present and discuss two applications using real-world medical data to assess the practical usefulness of the proposed model.},
  archive      = {J_SIM},
  author       = {Jeremias Leão and Marcelo Bourguignon and Diego I. Gallardo and Ricardo Rocha and Vera Tomazella},
  doi          = {10.1002/sim.8664},
  journal      = {Statistics in Medicine},
  month        = {10},
  number       = {24},
  pages        = {3272-3284},
  shortjournal = {Stat. Med.},
  title        = {A new cure rate model with flexible competing causes with applications to melanoma and transplantation data},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Efficient estimation of human immunodeficiency virus
incidence rate using a pooled cross-sectional cohort study design.
<em>SIM</em>, <em>39</em>(24), 3255–3271. (<a
href="https://doi.org/10.1002/sim.8661">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Development of methods to accurately estimate human immunodeficiency virus (HIV) incidence rate remains a challenge. Ideally, one would follow a random sample of HIV-negative individuals under a longitudinal study design and identify incident cases as they arise. Such designs can be prohibitively resource intensive and therefore alternative designs may be preferable. We propose such a simple, less resource-intensive study design and develop a weighted log likelihood approach which simultaneously accounts for selection bias and outcome misclassification error. The design is based on a cross-sectional survey which queries individuals&#39; time since last HIV-negative test, validates their test results with formal documentation whenever possible, and tests all persons who do not have documentation of being HIV-positive. To gain efficiency, we update the weighted log likelihood function with potentially misclassified self-reports from individuals who could not produce documentation of a prior HIV-negative test and investigate large sample properties of validated sub-sample only versus pooled sample estimators through extensive Monte Carlo simulations. We illustrate our method by estimating incidence rate for individuals who tested HIV-negative within 1.5 and 5 years prior to Botswana Combination Prevention Project enrolment. This article establishes that accurate estimates of HIV incidence rate can be obtained from individuals&#39; history of testing in a cross-sectional cohort study design by appropriately accounting for selection bias and misclassification error. Moreover, this approach is notably less resource-intensive compared to longitudinal and laboratory-based methods.},
  archive      = {J_SIM},
  author       = {Kesaobaka Molebatsi and Lesego Gabaitiri and Lucky Mokgatlhe and Sikhulile Moyo and Simani Gaseitsiwe and Kathleen E. Wirth and Victor DeGruttola and Eric Tchetgen Tchetgen},
  doi          = {10.1002/sim.8661},
  journal      = {Statistics in Medicine},
  month        = {10},
  number       = {24},
  pages        = {3255-3271},
  shortjournal = {Stat. Med.},
  title        = {Efficient estimation of human immunodeficiency virus incidence rate using a pooled cross-sectional cohort study design},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Balancing vs modeling approaches to weighting in practice.
<em>SIM</em>, <em>39</em>(24), 3227–3254. (<a
href="https://doi.org/10.1002/sim.8659">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {There are two seemingly unrelated approaches to weighting in observational studies. One of them maximizes the fit of a model for treatment assignment to then derive weights—we call this the modeling approach. The other directly optimizes certain features of the weights—we call this the balancing approach. The implementations of these two approaches are related: the balancing approach implicitly models the propensity score, while instances of the modeling approach impose balance conditions on the covariates used to estimate the propensity score. In this article, we review and compare these two approaches to weighting. Previous review papers have focused on the modeling approach, emphasizing the importance of checking covariate balance. However, as we discuss, the dispersion of the weights is another important aspect of the weights to consider, in addition to the representativeness of the weighted sample and the sample boundedness of the weighted estimator. In particular, the dispersion of the weights is important because it translates into a measure of effective sample size, which can be used to select between alternative weighting schemes. In this article, we examine the balancing approach to weighting, discuss recent methodological developments, and compare instances of the balancing and modeling approaches in a simulation study and an empirical study. In practice, unless the treatment assignment model is known, we recommend using the balancing approach to weighting, as it systematically results in better covariate balance with weights that are minimally dispersed. As a result, effect estimates tend to be more accurate and stable.},
  archive      = {J_SIM},
  author       = {Ambarish Chattopadhyay and Christopher H. Hase and José R. Zubizarreta},
  doi          = {10.1002/sim.8659},
  journal      = {Statistics in Medicine},
  month        = {10},
  number       = {24},
  pages        = {3227-3254},
  shortjournal = {Stat. Med.},
  title        = {Balancing vs modeling approaches to weighting in practice},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A general presentation on how to carry out a CHARMS analysis
for prognostic multivariate models. <em>SIM</em>, <em>39</em>(23),
3207–3225. (<a href="https://doi.org/10.1002/sim.8660">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The CHARMS (critical appraisal and data extraction for systematic reviews of prediction modelling studies) checklist was created to provide methodological appraisals of predictive models, based on the best available scientific evidence and through systematic reviews. Our purpose is to give a general presentation on how to carry out a CHARMS analysis for prognostic multivariate models, making clear what the steps are and how they are applied individually to the studies included in the systematic review. This tutorial is aimed at providing such a resource. In addition to this explanation, we will apply the method to a real case: predictive models of atrial fibrillation in the community. This methodology could be applied to other predictive models using the steps provided in our review so as to have complete information for each included model and determine whether it can be implemented in daily clinical practice.},
  archive      = {J_SIM},
  author       = {Antonio Palazón-Bru and Francisco Martín-Pérez and Emma Mares-García and Concepción Beneyto-Ripoll and Vicente Francisco Gil-Guillén and Ángel Pérez-Sempere and María Ángeles Carbonell-Torregrosa},
  doi          = {10.1002/sim.8660},
  journal      = {Statistics in Medicine},
  month        = {10},
  number       = {23},
  pages        = {3207-3225},
  shortjournal = {Stat. Med.},
  title        = {A general presentation on how to carry out a CHARMS analysis for prognostic multivariate models},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A multistate joint model for interval-censored event-history
data subject to within-unit clustering and informative missingness, with
application to neurocysticercosis research. <em>SIM</em>,
<em>39</em>(23), 3195–3206. (<a
href="https://doi.org/10.1002/sim.8663">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a multistate joint model to analyze interval-censored event-history data subject to within-unit clustering and nonignorable missing data. The model is motivated by a study of the neurocysticercosis (NC) cyst evolution at the cyst-level, taking into account the multiple cysts phases with intermittent missing data and loss to follow-up, as well as the intra-brain clustering of observations made on a predefined data collection schedule. Of particular interest in this study is the description of the process leading to cyst resolution, and whether this process varies by antiparasitic treatment. The model uses shared random effects to account for within-brain correlation and to explain the hidden heterogeneity governing the missing data mechanism. We developed a likelihood-based method using a Monte Carlo EM algorithm for the inference. The practical utility of the methods is illustrated using data from a randomized controlled trial on the effect of antiparasitic treatment with albendazole on NC cysts among patients from six hospitals in Ecuador. Simulation results demonstrate that the proposed methods perform well in the finite sample and misspecified models that ignore the data complexities could lead to substantial biases.},
  archive      = {J_SIM},
  author       = {Hongbin Zhang and Elizabeth A. Kelvin and Arturo Carpio and W. Allen Hauser},
  doi          = {10.1002/sim.8663},
  journal      = {Statistics in Medicine},
  month        = {10},
  number       = {23},
  pages        = {3195-3206},
  shortjournal = {Stat. Med.},
  title        = {A multistate joint model for interval-censored event-history data subject to within-unit clustering and informative missingness, with application to neurocysticercosis research},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A bivariate autoregressive poisson model and its application
to asthma-related emergency room visits. <em>SIM</em>, <em>39</em>(23),
3184–3194. (<a href="https://doi.org/10.1002/sim.8662">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {There are no gold standard methods that perform well in every situation when it comes to the analysis of multiple time series of counts. In this paper, we consider a positively correlated bivariate time series of counts and propose a parameter-driven Poisson regression model for its analysis. In our proposed model, we employ a latent autoregressive process, AR ( p ) to accommodate the temporal correlations in the two series. We compute the familiar maximum likelihood estimators of the model parameters and their standard errors via a Bayesian data cloning approach. We apply the model to the analysis of a bivariate time series arising from asthma-related visits to emergency rooms across the Canadian province of Ontario.},
  archive      = {J_SIM},
  author       = {Huda Al-Wahsh and Abdulkadir Hussein},
  doi          = {10.1002/sim.8662},
  journal      = {Statistics in Medicine},
  month        = {10},
  number       = {23},
  pages        = {3184-3194},
  shortjournal = {Stat. Med.},
  title        = {A bivariate autoregressive poisson model and its application to asthma-related emergency room visits},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). The contemplated average success probability for normally
distributed models with an application to optimal sample sizes
selection. <em>SIM</em>, <em>39</em>(23), 3173–3183. (<a
href="https://doi.org/10.1002/sim.8658">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We analytically obtain the average success probability (ASP) and the contemplated average success probability (CASP) for normally distributed observed differences in the treatment group and the placebo group means of the early trial and the confirmatory trial, assuming a uniform noninformative prior for the population treatment effect and a common known variance of the observations from both groups. For the CASP optimization problem with a fixed subtotal sample size of the early trial and the confirmatory trial of one arm larger than a threshold, we obtain the optimal plan of the sample sizes in a theorem. Moreover, in the theorem, we obtain the analytical formula of the optimal CASP as an increasing function of the subtotal sample size. After that, we calculate and compare the numerical values of the ASP with those in Table 1 of Chuang-Stein (2006). Finally, we investigate the numerical features of the CASP and find the optimal plan of the sample sizes for a given subtotal sample size.},
  archive      = {J_SIM},
  author       = {Ying-Ying Zhang and Teng-Zhong Rong and Man-Man Li},
  doi          = {10.1002/sim.8658},
  journal      = {Statistics in Medicine},
  month        = {10},
  number       = {23},
  pages        = {3173-3183},
  shortjournal = {Stat. Med.},
  title        = {The contemplated average success probability for normally distributed models with an application to optimal sample sizes selection},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Constructing inverse probability weights for institutional
comparisons in healthcare. <em>SIM</em>, <em>39</em>(23), 3156–3172. (<a
href="https://doi.org/10.1002/sim.8657">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In comparing quality of care between hospitals, disease-specific quality indicators measure structural, process, or outcome elements related to the care of a particular condition. Such comparisons can be framed in terms of causal contrasts, answering the question of whether a patient (or a population of patients on average) would receive different care if treated at the care level of a different hospital. Fair comparisons have to be adjusted for patient case-mix, which is equivalent to controlling for confounding by the patient-level factors, including demographic factors, comorbidities, and disease progression. The methodological choice for such comparisons is usually between direct and indirect standardization methods. In this article, we discuss the alternative of inverse probability weighting as a tool for standardization in hospital comparisons. This involves fitting multinomial logistic hospital assignment models and using these to construct the inverse probability weights. The challenge in the present context is the presence of large number of hospitals being compared, many of which have a small patient volume. We propose methods to include small categories in the weighted analysis, as well as metrics and visualizations for checking the positivity/overlap and covariate balance in constructing such weights. The methods are illustrated in a running example using linked administrative data on surgical treatment of kidney cancer patients in Ontario.},
  archive      = {J_SIM},
  author       = {Thai-Son Tang and Peter C. Austin and Keith A. Lawson and Antonio Finelli and Olli Saarela},
  doi          = {10.1002/sim.8657},
  journal      = {Statistics in Medicine},
  month        = {10},
  number       = {23},
  pages        = {3156-3172},
  shortjournal = {Stat. Med.},
  title        = {Constructing inverse probability weights for institutional comparisons in healthcare},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Graphical approaches for the control of generalized error
rates. <em>SIM</em>, <em>39</em>(23), 3135–3155. (<a
href="https://doi.org/10.1002/sim.8595">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {When simultaneously testing multiple hypotheses, the usual approach in the context of confirmatory clinical trials is to control the familywise error rate (FWER), which bounds the probability of making at least one false rejection. In many trial settings, these hypotheses will additionally have a hierarchical structure that reflects the relative importance and links between different clinical objectives. The graphical approach of Bretz et al (2009) is a flexible and easily communicable way of controlling the FWER while respecting complex trial objectives and multiple structured hypotheses. However, the FWER can be a very stringent criterion that leads to procedures with low power, and may not be appropriate in exploratory trial settings. This motivates controlling generalized error rates, particularly when the number of hypotheses tested is no longer small. We consider the generalized familywise error rate ( k -FWER), which is the probability of making k or more false rejections, as well as the tail probability of the false discovery proportion (FDP), which is the probability that the proportion of false rejections is greater than some threshold. We also consider asymptotic control of the false discovery rate, which is the expectation of the FDP. In this article, we show how to control these generalized error rates when using the graphical approach and its extensions. We demonstrate the utility of the resulting graphical procedures on three clinical trial case studies.},
  archive      = {J_SIM},
  author       = {David S. Robertson and James M. S. Wason and Frank Bretz},
  doi          = {10.1002/sim.8595},
  journal      = {Statistics in Medicine},
  month        = {10},
  number       = {23},
  pages        = {3135-3155},
  shortjournal = {Stat. Med.},
  title        = {Graphical approaches for the control of generalized error rates},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Variable selection for high-dimensional partly linear
additive cox model with application to alzheimer’s disease.
<em>SIM</em>, <em>39</em>(23), 3120–3134. (<a
href="https://doi.org/10.1002/sim.8594">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Variable selection has been discussed under many contexts and especially, a large literature has been established for the analysis of right-censored failure time data. In this article, we discuss an interval-censored failure time situation where there exist two sets of covariates with one being low-dimensional and having possible nonlinear effects and the other being high-dimensional. For the problem, we present a penalized estimation procedure for simultaneous variable selection and estimation, and in the method, Bernstein polynomials are used to approximate the involved nonlinear functions. Furthermore, for implementation, a coordinate-wise optimization algorithm, which can accommodate most commonly used penalty functions, is developed. A numerical study is performed for the evaluation of the proposed approach and suggests that it works well in practical situations. Finally the method is applied to an Alzheimer&#39;s disease study that motivated this investigation.},
  archive      = {J_SIM},
  author       = {Qiwei Wu and Hui Zhao and Liang Zhu and Jianguo Sun},
  doi          = {10.1002/sim.8594},
  journal      = {Statistics in Medicine},
  month        = {10},
  number       = {23},
  pages        = {3120-3134},
  shortjournal = {Stat. Med.},
  title        = {Variable selection for high-dimensional partly linear additive cox model with application to alzheimer&#39;s disease},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A bayesian multivariate meta-analysis of prevalence data.
<em>SIM</em>, <em>39</em>(23), 3105–3119. (<a
href="https://doi.org/10.1002/sim.8593">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {When conducting a meta-analysis involving prevalence data for an outcome with several subtypes, each of them is typically analyzed separately using a univariate meta-analysis model. Recently, multivariate meta-analysis models have been shown to correspond to a decrease in bias and variance for multiple correlated outcomes compared with univariate meta-analysis, when some studies only report a subset of the outcomes. In this article, we propose a novel Bayesian multivariate random effects model to account for the natural constraint that the prevalence of any given subtype cannot be larger than that of the overall prevalence. Extensive simulation studies show that this new model can reduce bias and variance when estimating subtype prevalences in the presence of missing data, compared with standard univariate and multivariate random effects models. The data from a rapid review on occupation and lower urinary tract symptoms by the Prevention of Lower Urinary Tract Symptoms Research Consortium are analyzed as a case study to estimate the prevalence of urinary incontinence and several incontinence subtypes among women in suspected high risk work environments.},
  archive      = {J_SIM},
  author       = {Lianne Siegel and Kyle Rudser and Siobhan Sutcliffe and Alayne Markland and Linda Brubaker and Sheila Gahagan and Ann E. Stapleton and Haitao Chu},
  doi          = {10.1002/sim.8593},
  journal      = {Statistics in Medicine},
  month        = {10},
  number       = {23},
  pages        = {3105-3119},
  shortjournal = {Stat. Med.},
  title        = {A bayesian multivariate meta-analysis of prevalence data},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Design and sample size considerations for valuation studies
of multi-attribute utility instruments. <em>SIM</em>, <em>39</em>(23),
3074–3104. (<a href="https://doi.org/10.1002/sim.8592">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The EQ-5D, a widely used multiattribute utility instrument, is commonly used in health economic evaluations where the goal is to decide on which treatments to reimburse. Like other instruments, value sets of the EQ-5D are constructed using valuation studies typically valuing a subset of the health states and using predicted values from a regression model for the unvalued health states. In current practice the prediction errors associated with the value sets are substantial. The goal of this work is 2-fold. First, derive a formula of the mean squared error (MSE) of a value set assuming that the value set is estimated using a linear mixed model with either an independent or a Gaussian spatial correlation on the model misspecification error. Second, explore the effect of the number of health states directly valued, the number of participants and the correlation structure on the MSE. Keeping the total number of participants and the total number of valuations fixed, valuing all 242 health states of the EQ-5D-3L was found to reduce the MSE considerably compared with the common practice of valuing only 42 health states. Furthermore, an independent correlation structure with 3773 participants valuing 42 health states produced the MSE that can be achieved with less than 600 participants valuing all 242 health states under a Gaussian spatial correlation structure. Based on the comparison of the MSE values of some of the well-known designs our suggestion is to value more health states and to use a model with spatially correlated misspecification errors.},
  archive      = {J_SIM},
  author       = {Shahriar Shams and Eleanor Pullenayegum},
  doi          = {10.1002/sim.8592},
  journal      = {Statistics in Medicine},
  month        = {10},
  number       = {23},
  pages        = {3074-3104},
  shortjournal = {Stat. Med.},
  title        = {Design and sample size considerations for valuation studies of multi-attribute utility instruments},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Using electronic health records to identify candidates for
human immunodeficiency virus pre-exposure prophylaxis: An application of
super learning to risk prediction when the outcome is rare.
<em>SIM</em>, <em>39</em>(23), 3059–3073. (<a
href="https://doi.org/10.1002/sim.8591">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Human immunodeficiency virus (HIV) pre-exposure prophylaxis (PrEP) protects high risk patients from becoming infected with HIV. Clinicians need help to identify candidates for PrEP based on information routinely collected in electronic health records (EHRs). The greatest statistical challenge in developing a risk prediction model is that acquisition is extremely rare. Methods : Data consisted of 180 covariates (demographic, diagnoses, treatments, prescriptions) extracted from records on 399 385 patient (150 cases) seen at Atrius Health (2007-2015), a clinical network in Massachusetts. Super learner is an ensemble machine learning algorithm that uses k -fold cross validation to evaluate and combine predictions from a collection of algorithms. We trained 42 variants of sophisticated algorithms, using different sampling schemes that more evenly balanced the ratio of cases to controls. We compared super learner&#39;s cross validated area under the receiver operating curve (cv-AUC) with that of each individual algorithm. Results : The least absolute shrinkage and selection operator (lasso) using a 1:20 class ratio outperformed the super learner (cv-AUC = 0.86 vs 0.84). A traditional logistic regression model restricted to 23 clinician-selected main terms was slightly inferior (cv-AUC = 0.81). Conclusion : Machine learning was successful at developing a model to predict 1-year risk of acquiring HIV based on a physician-curated set of predictors extracted from EHRs.},
  archive      = {J_SIM},
  author       = {Susan Gruber and Douglas Krakower and John T. Menchaca and Katherine Hsu and Rebecca Hawrusik and Judith C. Maro and Noelle M. Cocoros and Benjamin A. Kruskal and Ira B. Wilson and Kenneth H. Mayer and Michael Klompas},
  doi          = {10.1002/sim.8591},
  journal      = {Statistics in Medicine},
  month        = {10},
  number       = {23},
  pages        = {3059-3073},
  shortjournal = {Stat. Med.},
  title        = {Using electronic health records to identify candidates for human immunodeficiency virus pre-exposure prophylaxis: An application of super learning to risk prediction when the outcome is rare},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). On shared gamma-frailty conditional markov model for
semicompeting risks data. <em>SIM</em>, <em>39</em>(23), 3042–3058. (<a
href="https://doi.org/10.1002/sim.8590">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Semicompeting risks data are a mixture of competing risks data and progressive state data. This type of data occurs when a nonterminal event is subject to truncation by a well-defined terminal event, but not vice versa. The shared gamma-frailty conditional Markov model (GFCMM) has been used to analyze semicompeting risks data because of its flexibility. There are two versions of this model: the restricted and the unrestricted model. Maximum likelihood estimation methodology has been proposed in the literature. However, we found through numerical experiments that the unrestricted model sometimes yields nonparametrically biased estimation. In this article, we provide a practical guideline for using the GFCMM in the analysis of semicompeting risk data that includes: (a) a score test to assess if the restricted model, which does not exhibit estimation problems, is reasonable under a proportional hazards assumption, and (b) a graphical illustration to justify whether the unrestricted model yields nonparametric estimation with substantial bias for cases where the test provides a statistical significant result against the restricted model. This guideline was applied to the Indianapolis-Ibadan Dementia Project data as an illustration to explore how dementia occurrence changes mortality risk.},
  archive      = {J_SIM},
  author       = {Jing Li and Ying Zhang and Giorgos Bakoyannis and Sujuan Gao},
  doi          = {10.1002/sim.8590},
  journal      = {Statistics in Medicine},
  month        = {10},
  number       = {23},
  pages        = {3042-3058},
  shortjournal = {Stat. Med.},
  title        = {On shared gamma-frailty conditional markov model for semicompeting risks data},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Misspecifying the covariance structure in a linear mixed
model under MAR drop-out. <em>SIM</em>, <em>39</em>(23), 3027–3041. (<a
href="https://doi.org/10.1002/sim.8589">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Misspecification of the covariance structure in a linear mixed model (LMM) can lead to biased population parameters&#39; estimates under MAR drop-out. In our motivating example of modeling CD4 cell counts during untreated HIV infection, random intercept and slope LMMs are frequently used. In this article, we evaluate the performance of LMMs with specific covariance structures, in terms of bias in the fixed effects estimates, under specific MAR drop-out mechanisms, and adopt a Bayesian model comparison criterion to discriminate between the examined approaches in real-data applications. We analytically show that using a random intercept and slope structure when the true one is more complex can lead to seriously biased estimates, with the degree of bias depending on the magnitude of the MAR drop-out. Under misspecified covariance structure, we compare in terms of induced bias the approach of adding a fractional Brownian motion (BM) process on top of random intercepts and slopes with the approach of using splines for the random effects. In general, the performance of both approaches was satisfactory, with the BM model leading to smaller bias in most cases. A simulation study is carried out to evaluate the performance of the proposed Bayesian criterion in identifying the model with the correct covariance structure. Overall, the proposed method performs better than the AIC and BIC criteria under our specific simulation setting. The models under consideration are applied to real data from the CASCADE study; the most plausible model is identified by all examined criteria.},
  archive      = {J_SIM},
  author       = {Christos Thomadakis and Loukia Meligkotsidou and Nikos Pantazis and Giota Touloumi},
  doi          = {10.1002/sim.8589},
  journal      = {Statistics in Medicine},
  month        = {10},
  number       = {23},
  pages        = {3027-3041},
  shortjournal = {Stat. Med.},
  title        = {Misspecifying the covariance structure in a linear mixed model under MAR drop-out},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Response to the letter by guogen shan, hua zhang, and tao
jiang. <em>SIM</em>, <em>39</em>(22), 3024–3025. (<a
href="https://doi.org/10.1002/sim.8583">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_SIM},
  author       = {Jessica Gronsbell and Lu Tian},
  doi          = {10.1002/sim.8583},
  journal      = {Statistics in Medicine},
  month        = {9},
  number       = {22},
  pages        = {3024-3025},
  shortjournal = {Stat. Med.},
  title        = {Response to the letter by guogen shan, hua zhang, and tao jiang},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Comments on “exact inference for the random-effect model for
meta-analyses with rare events.” <em>SIM</em>, <em>39</em>(22),
3022–3023. (<a href="https://doi.org/10.1002/sim.8500">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_SIM},
  author       = {Guogen Shan and Hua Zhang and Tao Jiang},
  doi          = {10.1002/sim.8500},
  journal      = {Statistics in Medicine},
  month        = {9},
  number       = {22},
  pages        = {3022-3023},
  shortjournal = {Stat. Med.},
  title        = {Comments on “Exact inference for the random-effect model for meta-analyses with rare events”},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Empirical use of causal inference methods to evaluate
survival differences in a real-world registry vs those found in
randomized clinical trials. <em>SIM</em>, <em>39</em>(22), 3003–3021.
(<a href="https://doi.org/10.1002/sim.8581">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With heighted interest in causal inference based on real-world evidence, this empirical study sought to understand differences between the results of observational analyses and long-term randomized clinical trials. We hypothesized that patients deemed “eligible” for clinical trials would follow a different survival trajectory from those deemed “ineligible” and that this factor could partially explain results. In a large observational registry dataset, we estimated separate survival trajectories for hypothetically trial-eligible vs ineligible patients under both coronary artery bypass surgery (CABG) and percutaneous coronary intervention (PCI). We also explored whether results would depend on the causal inference method (inverse probability of treatment weighting vs optimal full propensity matching) or the approach to combine propensity scores from multiple imputations (the “across” vs “within” approaches). We found that, in this registry population of PCI/CABG multivessel patients, 32.5% would have been eligible for contemporaneous RCTs, suggesting that RCTs enroll selected populations. Additionally, we found treatment selection bias with different distributions of propensity scores between PCI and CABG patients. The different methodological approaches did not result in different conclusions. Overall, trial-eligible patients appeared to demonstrate at least marginally better survival than ineligible patients. Treatment comparisons by eligibility depended on disease severity. Among trial-eligible three-vessel diseased and trial-ineligible two-vessel diseased patients, CABG appeared to have at least a slight advantage with no treatment difference otherwise. In conclusion, our analyses suggest that RCTs enroll highly selected populations, and our findings are generally consistent with RCTs but less pronounced than major registry findings.},
  archive      = {J_SIM},
  author       = {Hui-Jie Lee and John B. Wong and Beilin Jia and Xinyue Qi and Elizabeth R. DeLong},
  doi          = {10.1002/sim.8581},
  journal      = {Statistics in Medicine},
  month        = {9},
  number       = {22},
  pages        = {3003-3021},
  shortjournal = {Stat. Med.},
  title        = {Empirical use of causal inference methods to evaluate survival differences in a real-world registry vs those found in randomized clinical trials},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Statistical inference for decision curve analysis, with
applications to cataract diagnosis. <em>SIM</em>, <em>39</em>(22),
2980–3002. (<a href="https://doi.org/10.1002/sim.8588">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Statistical learning methods are widely used in medical literature for the purpose of diagnosis or prediction. Conventional accuracy assessment via sensitivity, specificity, and ROC curves does not fully account for clinical utility of a specific model. Decision curve analysis (DCA) becomes a novel complement as it incorporates a clinical judgment of the relative value of benefits (treating a true positive case) and harms (treating a false positive case) associated with prediction models. The preference of a patient or a policy-maker is formulated statistically as the underlying threshold probability, above which the patient would choose to be treated. Net benefit is then calculated for possible threshold probability, which places benefits and harms on the same scale. We consider the inference problems for DCA in this paper. Interval estimation procedure and inference methodology are provided after we derive the relevant asymptotic properties. Our formulation can accommodate the classification problems with multiple categories. We carry out numerical studies to assess the performance of the proposed methods. An eye disease dataset is analyzed to illustrate our proposals.},
  archive      = {J_SIM},
  author       = {Sumaiya Z. Sande and Jialiang Li and Ralph D&#39;Agostino and Tien Yin Wong and Ching-Yu Cheng},
  doi          = {10.1002/sim.8588},
  journal      = {Statistics in Medicine},
  month        = {9},
  number       = {22},
  pages        = {2980-3002},
  shortjournal = {Stat. Med.},
  title        = {Statistical inference for decision curve analysis, with applications to cataract diagnosis},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A bivariate logistic regression model based on latent
variables. <em>SIM</em>, <em>39</em>(22), 2962–2979. (<a
href="https://doi.org/10.1002/sim.8587">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Bivariate observations of binary and ordinal data arise frequently and require a bivariate modeling approach in cases where one is interested in aspects of the marginal distributions as separate outcomes along with the association between the two. We consider methods for constructing such bivariate models based on latent variables with logistic marginals and propose a model based on the Ali-Mikhail-Haq bivariate logistic distribution. We motivate the model as an extension of that based on the Gumbel type 2 distribution as considered by other authors and as a bivariate extension of the logistic distribution, which preserves certain natural characteristics. Basic properties of the obtained model are studied and the proposed methods are illustrated through analysis of two data sets: a basic science cognitive experiment of visual recognition and awareness and a clinical data set describing assessments of walking disability among multiple sclerosis patients.},
  archive      = {J_SIM},
  author       = {Simon Bang Kristensen and Bo Martin Bibby},
  doi          = {10.1002/sim.8587},
  journal      = {Statistics in Medicine},
  month        = {9},
  number       = {22},
  pages        = {2962-2979},
  shortjournal = {Stat. Med.},
  title        = {A bivariate logistic regression model based on latent variables},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Regression models using parametric pseudo-observations.
<em>SIM</em>, <em>39</em>(22), 2949–2961. (<a
href="https://doi.org/10.1002/sim.8586">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Pseudo-observations based on the nonparametric Kaplan-Meier estimator of the survival function have been proposed as an alternative to the widely used Cox model for analyzing censored time-to-event data. Using a spline-based estimator of the survival has some potential benefits over the nonparametric approach in terms of less variability. We propose to define pseudo-observations based on a flexible parametric estimator and use these for analysis in regression models to estimate parameters related to the cumulative risk. We report the results of a simulation study that compares the empirical standard errors of estimates based on parametric and nonparametric pseudo-observations in various settings. Our simulations show that in some situations there is a substantial gain in terms of reduced variability using the proposed parametric pseudo-observations compared with the nonparametric pseudo-observations. The gain can be measured as a reduction of the empirical standard error by up to about one third; corresponding to an additional 125% larger sample size. We illustrate the use of the proposed method in a brief data example.},
  archive      = {J_SIM},
  author       = {Martin Nygård Johansen and Søren Lundbye-Christensen and Erik Thorlund Parner},
  doi          = {10.1002/sim.8586},
  journal      = {Statistics in Medicine},
  month        = {9},
  number       = {22},
  pages        = {2949-2961},
  shortjournal = {Stat. Med.},
  title        = {Regression models using parametric pseudo-observations},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Semiparametric copula-based analysis for treatment effects
in the presence of treatment switching. <em>SIM</em>, <em>39</em>(22),
2936–2948. (<a href="https://doi.org/10.1002/sim.8585">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In controlled trials, “treatment switching” occurs when patients in one treatment group switch to alternative treatments during the trial, and poses challenges to treatment effect evaluation owing to crossover of the treatments groups. In this work, we assume that treatment switching can occur after some disease progression event and view the progression and death events as two semicompeting risks. The proposed model consists of a copula model for the joint distribution of time-to-progression (TTP) and overall survival (OS) up to the earlier of the two events, as well as a conditional hazard model for OS subsequent to progression. The copula model facilitates assessing the marginal distributions of TTP and OS separately from the association between the two events, and, in particular, the treatment effect on OS in the absence of treatment switching. The proposed conditional hazard model for death subsequent to progression allows us to assess the treatment switching (crossover) effect on OS given occurrence of progression and covariates. Semiparametric proportional hazards models are employed in the marginal models for TTP and OS. A nonparametric maximum likelihood procedure is developed for model inference, which is verified through asymptotic theory and simulation studies. The proposed analysis is applied to a lung cancer dataset to illustrate its real utility.},
  archive      = {J_SIM},
  author       = {Chia-Hui Huang and Yi-Hau Chen and Jinn-Li Wang and Mey Wang},
  doi          = {10.1002/sim.8585},
  journal      = {Statistics in Medicine},
  month        = {9},
  number       = {22},
  pages        = {2936-2948},
  shortjournal = {Stat. Med.},
  title        = {Semiparametric copula-based analysis for treatment effects in the presence of treatment switching},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A general method for elicitation, imputation, and
sensitivity analysis for incomplete repeated binary data. <em>SIM</em>,
<em>39</em>(22), 2921–2935. (<a
href="https://doi.org/10.1002/sim.8584">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We develop and demonstrate methods to perform sensitivity analyses to assess sensitivity to plausible departures from missing at random in incomplete repeated binary outcome data. We use multiple imputation in the not at random fully conditional specification framework, which includes one or more sensitivity parameters (SPs) for each incomplete variable. The use of an online elicitation questionnaire is demonstrated to obtain expert opinion on the SPs, and highest prior density regions are used alongside opinion pooling methods to display credible regions for SPs. We demonstrate that substantive conclusions can be far more sensitive to departures from the missing at random assumption (MAR) when control and intervention nonresponders depart from MAR differently, and show that the correlation of arm specific SPs in expert opinion is particularly important. We illustrate these methods on the iQuit in Practice smoking cessation trial, which compared the impact of a tailored text messaging system versus standard care on smoking cessation. We show that conclusions about the effect of intervention on smoking cessation outcomes at 8 week and 6 months are broadly insensitive to departures from MAR, with conclusions significantly affected only when the differences in behavior between the nonresponders in the two trial arms is larger than expert opinion judges to be realistic.},
  archive      = {J_SIM},
  author       = {Daniel Tompsett and Stephen Sutton and Shaun R. Seaman and Ian R. White},
  doi          = {10.1002/sim.8584},
  journal      = {Statistics in Medicine},
  month        = {9},
  number       = {22},
  pages        = {2921-2935},
  shortjournal = {Stat. Med.},
  title        = {A general method for elicitation, imputation, and sensitivity analysis for incomplete repeated binary data},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Classification of human physical activity based on raw
accelerometry data via spherical coordinate transformation.
<em>SIM</em>, <em>39</em>(22), 2901–2920. (<a
href="https://doi.org/10.1002/sim.8582">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Human health is strongly associated with person&#39;s lifestyle and levels of physical activity. Therefore, characterization of daily human activity is an important task. Accelerometers have been used to obtain precise measurements of body acceleration. Wearable accelerometers collect data as a three-dimensional time series with frequencies up to 100 Hz. Using such accelerometry signal, we are able to classify different types of physical activity. In our work, we present a novel procedure for physical activity classification based on the raw accelerometry signal. Our proposal is based on the spherical representation of the data. We classify four activity types: resting, upper body activities (sitting), upper body activities (standing), and lower body activities. The classifier is constructed using decision trees with extracted features consisting of spherical coordinates summary statistics, moving averages of the radius and the angles, radius variance, and spherical variance. The classification accuracy of our method has been tested on data collected on a sample of 47 elderly individuals who performed a series of activities in laboratory settings. The achieved classification accuracy is over 90% when the subject-specific data are used and 84% when the group data are used. Main contributor to the classification accuracy is the angular part of the collected signal, especially spherical variance. To the best of our knowledge, spherical variance has never been previously used in the analysis of the raw accelerometry data. Its major advantage over other angular measures is its invariance to the accelerometer location shifts.},
  archive      = {J_SIM},
  author       = {Michał Kos and Małgorzata Bogdan and Nancy W. Glynn and Jaroslaw Harezlak},
  doi          = {10.1002/sim.8582},
  journal      = {Statistics in Medicine},
  month        = {9},
  number       = {22},
  pages        = {2901-2920},
  shortjournal = {Stat. Med.},
  title        = {Classification of human physical activity based on raw accelerometry data via spherical coordinate transformation},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). The impact of covariance priors on arm-based bayesian
network meta-analyses with binary outcomes. <em>SIM</em>,
<em>39</em>(22), 2883–2900. (<a
href="https://doi.org/10.1002/sim.8580">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Bayesian analyses with the arm-based (AB) network meta-analysis (NMA) model require researchers to specify a prior distribution for the covariance matrix of the treatment-specific event rates in a transformed scale, for example, the treatment-specific log-odds when a logit transformation is used. The commonly used conjugate prior for the covariance matrix, the inverse-Wishart (IW) distribution, has several limitations. For example, although the IW distribution is often described as noninformative or weakly informative, it may in fact provide strong information when some variance components are small (eg, when the standard deviation of study-specific log-odds of a treatment is smaller than 1/2), as is common in NMAs with binary outcomes. In addition, the IW prior generally leads to underestimation of correlations between treatment-specific log-odds, which are critical for borrowing strength across treatment arms to estimate treatment effects efficiently and to reduce potential bias. Alternatively, several separation strategies (ie, separate priors on variances and correlations) can be considered. To study the IW prior&#39;s impact on NMA results and compare it with separation strategies, we did simulation studies under different missing-treatment mechanisms. A separation strategy with appropriate priors for the correlation matrix and variances performs better than the IW prior, and should be recommended as the default vague prior in the AB NMA approach. Finally, we reanalyzed three case studies and illustrated the importance, when performing AB-NMA, of sensitivity analyses with different prior specifications on variances.},
  archive      = {J_SIM},
  author       = {Zhenxun Wang and Lifeng Lin and James S. Hodges and Haitao Chu},
  doi          = {10.1002/sim.8580},
  journal      = {Statistics in Medicine},
  month        = {9},
  number       = {22},
  pages        = {2883-2900},
  shortjournal = {Stat. Med.},
  title        = {The impact of covariance priors on arm-based bayesian network meta-analyses with binary outcomes},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Secondary analysis of case-control association studies:
Insights on weighting-based inference motivate a new specification test.
<em>SIM</em>, <em>39</em>(22), 2869–2882. (<a
href="https://doi.org/10.1002/sim.8579">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Case-control sampling is frequently used in genetic association studies to examine the relationship between disease and genetic exposures. Such designs usually collect extensive information on phenotypes beyond the primary disease, whose associations with the genetic exposures are also of great interest. Because the cases are over-sampled, appropriate analysis of secondary phenotypes should take into account this biased sampling design. We previously introduced a weighting-based estimator for appropriate secondary analysis, but have not thoroughly explored its statistical properties. In this article, we revisit our previous estimator to offer new insights and methodological extensions. Specifically, we extend our previous estimator and construct its more general form based on generalized least squares (GLS). Such an extension allows us to connect the GLS estimator with the generalized method of moments and motivates a new specification test designed to assess the adequacy of the disease model or the weights. The specification test statistic measures the weighted discrepancy between the case and control subsample estimators, and asymptotically follows a central Chi-squared distribution under correct disease model specification. We illustrate the GLS estimator and specification test using a case-control sample of peripheral arterial disease, and use simulations to further shed light on the operating characteristics of the specification test.},
  archive      = {J_SIM},
  author       = {Fan Li and Andrew S. Allen},
  doi          = {10.1002/sim.8579},
  journal      = {Statistics in Medicine},
  month        = {9},
  number       = {22},
  pages        = {2869-2882},
  shortjournal = {Stat. Med.},
  title        = {Secondary analysis of case-control association studies: Insights on weighting-based inference motivate a new specification test},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Efficient interaction selection for clustered data via
stagewise generalized estimating equations. <em>SIM</em>,
<em>39</em>(22), 2855–2868. (<a
href="https://doi.org/10.1002/sim.8574">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Model selection in the presence of interaction terms is challenging as the final model must maintain a hierarchy between main effects and interaction terms. This work presents two stagewise estimation approaches to appropriately select models with interaction terms that can utilize generalized estimating equations to model clustered data. The first proposed technique is a hierarchical lasso stagewise estimating equations approach, which is shown to directly correspond to the hierarchical lasso penalized regression. The second is a stagewise active set approach, which enforces the variable hierarchy by conforming the selection to a properly growing active set in each stagewise estimation step. The effectiveness in interaction selection and the superior computational efficiency of the proposed techniques are assessed in simulation studies. The new methods are applied to a study of hospitalization rates attributed to suicide attempts among 15 to 19 year old at the school district level in Connecticut.},
  archive      = {J_SIM},
  author       = {Gregory Vaughan and Robert Aseltine and Kun Chen and Jun Yan},
  doi          = {10.1002/sim.8574},
  journal      = {Statistics in Medicine},
  month        = {9},
  number       = {22},
  pages        = {2855-2868},
  shortjournal = {Stat. Med.},
  title        = {Efficient interaction selection for clustered data via stagewise generalized estimating equations},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Randomization-based interval estimation in randomized
clinical trials. <em>SIM</em>, <em>39</em>(21), 2843–2854. (<a
href="https://doi.org/10.1002/sim.8577">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Randomization-based interval estimation takes into account the particular randomization procedure in the analysis and preserves the confidence level even in the presence of heterogeneity. It is distinguished from population-based confidence intervals with respect to three aspects: definition, computation, and interpretation. The article contributes to the discussion of how to construct a confidence interval for a treatment difference from randomization tests when analyzing data from randomized clinical trials. The discussion covers (i) the definition of a confidence interval for a treatment difference in randomization-based inference, (ii) computational algorithms for efficiently approximating the endpoints of an interval, and (iii) evaluation of statistical properties (ie, coverage probability and interval length) of randomization-based and population-based confidence intervals under a selected set of randomization procedures when assuming heterogeneity in patient outcomes. The method is illustrated with a case study.},
  archive      = {J_SIM},
  author       = {Yanying Wang and William F. Rosenberger},
  doi          = {10.1002/sim.8577},
  journal      = {Statistics in Medicine},
  month        = {9},
  number       = {21},
  pages        = {2843-2854},
  shortjournal = {Stat. Med.},
  title        = {Randomization-based interval estimation in randomized clinical trials},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Sensitivity analysis for clinical trials with missing
continuous outcome data using controlled multiple imputation: A
practical guide. <em>SIM</em>, <em>39</em>(21), 2815–2842. (<a
href="https://doi.org/10.1002/sim.8569">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Missing data due to loss to follow-up or intercurrent events are unintended, but unfortunately inevitable in clinical trials. Since the true values of missing data are never known, it is necessary to assess the impact of untestable and unavoidable assumptions about any unobserved data in sensitivity analysis. This tutorial provides an overview of controlled multiple imputation (MI) techniques and a practical guide to their use for sensitivity analysis of trials with missing continuous outcome data. These include δ - and reference-based MI procedures. In δ -based imputation, an offset term, δ , is typically added to the expected value of the missing data to assess the impact of unobserved participants having a worse or better response than those observed. Reference-based imputation draws imputed values with some reference to observed data in other groups of the trial, typically in other treatment arms. We illustrate the accessibility of these methods using data from a pediatric eczema trial and a chronic headache trial and provide Stata code to facilitate adoption. We discuss issues surrounding the choice of δ in δ -based sensitivity analysis. We also review the debate on variance estimation within reference-based analysis and justify the use of Rubin&#39;s variance estimator in this setting, since as we further elaborate on within, it provides information anchored inference.},
  archive      = {J_SIM},
  author       = {Suzie Cro and Tim P. Morris and Michael G. Kenward and James R. Carpenter},
  doi          = {10.1002/sim.8569},
  journal      = {Statistics in Medicine},
  month        = {9},
  number       = {21},
  pages        = {2815-2842},
  shortjournal = {Stat. Med.},
  title        = {Sensitivity analysis for clinical trials with missing continuous outcome data using controlled multiple imputation: A practical guide},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Estimation of seroconversion rates for infectious diseases:
Effects of age and noise. <em>SIM</em>, <em>39</em>(21), 2799–2814. (<a
href="https://doi.org/10.1002/sim.8578">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The presence of serum antibodies is a biomarker of past infection. Instead of seroclassification aimed at measuring seroprevalence a population sample of serum antibody levels may be used to estimate the incidence of seroconversion. This article expands an earlier study into seroincidence estimation, employing models of the seroresponse that include probability of escaping infection, as well as nonexponential decay kinetics and different sources of noise. As previously, a constant force of infection is assumed. When the seroconversion rate is low, a substantial fraction of the population may not be old enough to have experienced any seroconversions, causing underestimation of seroconversion rates that may be substantial at young ages. A correction is given that can be shown to remove such age dependent bias. Simulation studies show that the updated models provide accurate estimates of seroconversion rates, but also that the presence of noise, when unaccounted for, may introduce considerable bias, especially at low (&lt; 0.1/yr) seroconversion rates and young ages. The revised serocalculator scripts can be used to update the R package “seroincidence.”},
  archive      = {J_SIM},
  author       = {P. F. M. Teunis and J. C. H. van Eijkeren},
  doi          = {10.1002/sim.8578},
  journal      = {Statistics in Medicine},
  month        = {9},
  number       = {21},
  pages        = {2799-2814},
  shortjournal = {Stat. Med.},
  title        = {Estimation of seroconversion rates for infectious diseases: Effects of age and noise},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Identifying density-based local outliers in medical
multivariate circular data. <em>SIM</em>, <em>39</em>(21), 2793–2798.
(<a href="https://doi.org/10.1002/sim.8576">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article is considered to be the first to deal with the problem of outlier-detection in multivariate circular data. The proposed algorithm is an extension of the Local Outlier Factor (LOF) method. Two different circular distances are used; taking into account the close bounded range of circular variables, and testing all possible permutations. The performance of the algorithm is investigated via an extensive simulation study. The performance of the LOF algorithm has a direct relationship with concentration parameter, while it has an inverse relationship with the sample size. For illustrative purposes, the algorithm has been implemented on two medical multivariate circular data, namely, X-ray beam projectors data and eye data. The extension of the LOF algorithm for other types of directional data such as spherical and cylindrical datasets is worth to be investigated.},
  archive      = {J_SIM},
  author       = {Ali H. Abuzaid},
  doi          = {10.1002/sim.8576},
  journal      = {Statistics in Medicine},
  month        = {9},
  number       = {21},
  pages        = {2793-2798},
  shortjournal = {Stat. Med.},
  title        = {Identifying density-based local outliers in medical multivariate circular data},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Maintaining the validity of inference in small-sample
stepped wedge cluster randomized trials with binary outcomes when using
generalized estimating equations. <em>SIM</em>, <em>39</em>(21),
2779–2792. (<a href="https://doi.org/10.1002/sim.8575">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Stepped wedge cluster trials are an increasingly popular alternative to traditional parallel cluster randomized trials. Such trials often utilize a small number of clusters and numerous time intervals, and these components must be considered when choosing an analysis method. A generalized linear mixed model containing a random intercept and fixed time and intervention covariates is the most common analysis approach. However, the sole use of a random intercept applies a constant intraclass correlation coefficient structure, which is an assumption that is likely to be violated given stepped wedge trials (SWTs) have multiple time intervals. Alternatively, generalized estimating equations (GEE) are robust to the misspecification of the working correlation structure, although it has been shown that small-sample adjustments to standard error estimates and the use of appropriate degrees of freedom are required to maintain the validity of inference when the number of clusters is small. In this article, we show, using an extensive simulation study based on a motivating example and a more general design, the use of GEE can maintain the validity of inference in small-sample SWTs with binary outcomes. Furthermore, we show which combinations of bias corrections to standard error estimates and degrees of freedom work best in terms of attaining nominal type I error rates.},
  archive      = {J_SIM},
  author       = {Whitney P. Ford and Philip M. Westgate},
  doi          = {10.1002/sim.8575},
  journal      = {Statistics in Medicine},
  month        = {9},
  number       = {21},
  pages        = {2779-2792},
  shortjournal = {Stat. Med.},
  title        = {Maintaining the validity of inference in small-sample stepped wedge cluster randomized trials with binary outcomes when using generalized estimating equations},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Random cancers as supported by registry data. <em>SIM</em>,
<em>39</em>(21), 2767–2778. (<a
href="https://doi.org/10.1002/sim.8573">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {There has been considerable interest in recent years in quantifying the rate of unavoidable or so-called random cancers, as opposed to cancers linked to environmental, genetic or other factors. We propose a data-based approach to estimate an upper limit to this probability, based on an analysis of multiple registry data. The argument is that the cumulative hazards for random cancers cannot exceed the minimum reliable cumulative hazard observed across the registries. We propose a Monte Carlo method to identify this upper limit and apply the method to data on nine different cancers recorded by 423 registries. We compare our values with estimates obtained from a random mutations argument.},
  archive      = {J_SIM},
  author       = {Janez Stare and Robin Henderson and Nina Ružić Gorenjec},
  doi          = {10.1002/sim.8573},
  journal      = {Statistics in Medicine},
  month        = {9},
  number       = {21},
  pages        = {2767-2778},
  shortjournal = {Stat. Med.},
  title        = {Random cancers as supported by registry data},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). An empirical saddlepoint approximation method for producing
smooth survival and hazard functions under interval-censoring.
<em>SIM</em>, <em>39</em>(21), 2755–2766. (<a
href="https://doi.org/10.1002/sim.8572">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We devise a new method to produce smooth estimates of baseline survival and hazard functions for incomplete data observed subject to interval-censoring, that can in principle be viewed as being nonparametric. The key idea is to start from the nonparametric maximum likelihood estimate, and to then construct an empirical moment generating function for the underlying data generating mechanism, which is subsequently inverted via a saddlepoint approximation in order to obtain smooth distributional estimates. Unlike the typical spline-based and other semiparametric methods that have thus far been devised for the same purpose, the proposed approach is unencumbered by the choice of tuning parameters. Simulation studies show that in terms of integrated squared error, the method is very close in performance to the parametric gold standard, and should generally be preferred over the well-established spline-based approach implemented in R package logspline . The methodology is illustrated on some publicly available real datasets, and its implications and limitations are discussed.},
  archive      = {J_SIM},
  author       = {Manjari Dissanayake and A. Alexandre Trindade},
  doi          = {10.1002/sim.8572},
  journal      = {Statistics in Medicine},
  month        = {9},
  number       = {21},
  pages        = {2755-2766},
  shortjournal = {Stat. Med.},
  title        = {An empirical saddlepoint approximation method for producing smooth survival and hazard functions under interval-censoring},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). High-dimensional single-index models with censored
responses. <em>SIM</em>, <em>39</em>(21), 2743–2754. (<a
href="https://doi.org/10.1002/sim.8571">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we study the estimation of high-dimensional single index models when the response variable is censored. We hybrid the estimation methods for high-dimensional single-index models (but without censorship) and univariate nonparametric models with randomly censored responses to estimate the index parameters and the link function and apply the proposed methods to analyze a genomic dataset from a study of diffuse large B-cell lymphoma. We evaluate the finite sample performance of the proposed procedures via simulation studies and establish large sample theories for the proposed estimators of the index parameter and the nonparametric link function under certain regularity conditions.},
  archive      = {J_SIM},
  author       = {Hailin Huang and Jizi Shangguan and Xinmin Li and Hua Liang},
  doi          = {10.1002/sim.8571},
  journal      = {Statistics in Medicine},
  month        = {9},
  number       = {21},
  pages        = {2743-2754},
  shortjournal = {Stat. Med.},
  title        = {High-dimensional single-index models with censored responses},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Graphical calibration curves and the integrated calibration
index (ICI) for survival models. <em>SIM</em>, <em>39</em>(21),
2714–2742. (<a href="https://doi.org/10.1002/sim.8570">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the context of survival analysis, calibration refers to the agreement between predicted probabilities and observed event rates or frequencies of the outcome within a given duration of time. We aimed to describe and evaluate methods for graphically assessing the calibration of survival models. We focus on hazard regression models and restricted cubic splines in conjunction with a Cox proportional hazards model. We also describe modifications of the Integrated Calibration Index, of E50 and of E90. In this context, this is the average (respectively, median or 90th percentile) absolute difference between predicted survival probabilities and smoothed survival frequencies. We conducted a series of Monte Carlo simulations to evaluate the performance of these calibration measures when the underlying model has been correctly specified and under different types of model mis-specification. We illustrate the utility of calibration curves and the three calibration metrics by using them to compare the calibration of a Cox proportional hazards regression model with that of a random survival forest for predicting mortality in patients hospitalized with heart failure. Under a correctly specified regression model, differences between the two methods for constructing calibration curves were minimal, although the performance of the method based on restricted cubic splines tended to be slightly better. In contrast, under a mis-specified model, the smoothed calibration curved constructed using hazard regression tended to be closer to the true calibration curve. The use of calibration curves and of these numeric calibration metrics permits for a comprehensive comparison of the calibration of competing survival models.},
  archive      = {J_SIM},
  author       = {Peter C. Austin and Frank E. Harrell and David van Klaveren},
  doi          = {10.1002/sim.8570},
  journal      = {Statistics in Medicine},
  month        = {9},
  number       = {21},
  pages        = {2714-2742},
  shortjournal = {Stat. Med.},
  title        = {Graphical calibration curves and the integrated calibration index (ICI) for survival models},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Relative rate of change in cognitive score network dynamics
via bayesian hierarchical models reveal spatial patterns of
neurodegeneration. <em>SIM</em>, <em>39</em>(21), 2695–2713. (<a
href="https://doi.org/10.1002/sim.8568">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The degeneration of the human brain is a complex process, which often affects certain brain regions due to healthy aging or disease. This degeneration can be evaluated on regions of interest (ROI) in the brain through probabilistic networks and morphological estimates. Current approaches for finding such networks are limited to analyses at discrete neuropsychological stages, which cannot appropriately account for connectivity dynamics over the onset of cognitive deterioration, and morphological changes are seldom unified with connectivity networks, despite known dependencies. To overcome these limitations, a probabilistic wombling model is proposed to simultaneously estimate ROI cortical thickness and covariance networks contingent on rates of change in cognitive decline. This proposed model was applied to analyze longitudinal data from healthy control (HC) and Alzheimer&#39;s disease (AD) groups and found connection differences pertaining to regions, which play a crucial role in lasting cognitive impairment, such as the entorhinal area and temporal regions. Moreover, HC cortical thickness estimates were significantly higher than those in the AD group across all ROIs. The analyses presented in this work will help practitioners jointly analyze brain tissue atrophy at the ROI-level conditional on neuropsychological networks, which could potentially allow for more targeted therapeutic interventions.},
  archive      = {J_SIM},
  author       = {Marcela I. Cespedes and James M. McGree and Christopher C. Drovandi and Kerrie Mengersen and Jurgen Fripp and James D. Doecke},
  doi          = {10.1002/sim.8568},
  journal      = {Statistics in Medicine},
  month        = {9},
  number       = {21},
  pages        = {2695-2713},
  shortjournal = {Stat. Med.},
  title        = {Relative rate of change in cognitive score network dynamics via bayesian hierarchical models reveal spatial patterns of neurodegeneration},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Correction. <em>SIM</em>, <em>39</em>(20), 2693. (<a
href="https://doi.org/10.1002/sim.8567">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_SIM},
  author       = {Mike Baiocchi and Jing Cheng and Dylan Small},
  doi          = {10.1002/sim.8567},
  journal      = {Statistics in Medicine},
  month        = {9},
  number       = {20},
  pages        = {2693},
  shortjournal = {Stat. Med.},
  title        = {Correction},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Authors’ response. <em>SIM</em>, <em>39</em>(20), 2692. (<a
href="https://doi.org/10.1002/sim.8559">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_SIM},
  author       = {Peter C. Austin and Aurélien Latouche},
  doi          = {10.1002/sim.8559},
  journal      = {Statistics in Medicine},
  month        = {9},
  number       = {20},
  pages        = {2692},
  shortjournal = {Stat. Med.},
  title        = {Authors&#39; response},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Comment on: A review of the use of time-varying covariates
in the fine-gray subdistribution hazard competing risk regression model
by peter c. Austin, aurélien latouche, and jason p. fine. <em>SIM</em>,
<em>39</em>(20), 2689–2691. (<a
href="https://doi.org/10.1002/sim.8558">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_SIM},
  author       = {Shunsuke Mori},
  doi          = {10.1002/sim.8558},
  journal      = {Statistics in Medicine},
  month        = {9},
  number       = {20},
  pages        = {2689-2691},
  shortjournal = {Stat. Med.},
  title        = {Comment on: A review of the use of time-varying covariates in the fine-gray subdistribution hazard competing risk regression model by peter c. austin, aurélien latouche, and jason p. fine},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Note on the role of the placebo group in the short-term and
long-term hazard ratio model. <em>SIM</em>, <em>39</em>(20), 2685–2688.
(<a href="https://doi.org/10.1002/sim.8424">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_SIM},
  author       = {Philippe Flandre and John O&#39;Quigley},
  doi          = {10.1002/sim.8424},
  journal      = {Statistics in Medicine},
  month        = {9},
  number       = {20},
  pages        = {2685-2688},
  shortjournal = {Stat. Med.},
  title        = {Note on the role of the placebo group in the short-term and long-term hazard ratio model},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A numerical strategy to evaluate performance of predictive
scores via a copula-based approach. <em>SIM</em>, <em>39</em>(20),
2671–2684. (<a href="https://doi.org/10.1002/sim.8566">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Assessing and comparing the performance of correlated predictive scores are of current interest in precision medicine. Given the limitations of available theoretical approaches for assessing and comparing the predictive accuracy, numerical methods are highly desired which, however, have not been systematically developed due to technical challenges. The main challenges include the lack of a general strategy on effectively simulating many kinds of correlated predictive scores each with some given level of predictive accuracy in either concordance index or the area under a receiver operating characteristic curve area under the curves (AUC). To fill in this important knowledge gap, this paper is to provide a general copula-based numeric framework for assessing and comparing predictive performance of correlated predictive or risk scores. The new algorithms are designed to effectively simulate correlated predictive scores with given levels of predictive accuracy as measured in terms of concordance indices or time-dependent AUC for predicting survival outcomes. The copula-based numerical strategy is convenient for numerically evaluating and comparing multiple measures of predictive accuracy of correlated risk scores and for investigating finite-sample properties of test statistics and confidence intervals as well as assessing for optimism of given performance measures using cross-validation or bootstrap.},
  archive      = {J_SIM},
  author       = {Yilong Zhang and Yongzhao Shao},
  doi          = {10.1002/sim.8566},
  journal      = {Statistics in Medicine},
  month        = {9},
  number       = {20},
  pages        = {2671-2684},
  shortjournal = {Stat. Med.},
  title        = {A numerical strategy to evaluate performance of predictive scores via a copula-based approach},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). On permutation tests for comparing restricted mean survival
time with small sample from randomized trials. <em>SIM</em>,
<em>39</em>(20), 2655–2670. (<a
href="https://doi.org/10.1002/sim.8565">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Between-group comparison based on the restricted mean survival time (RMST) is getting attention as an alternative to the conventional logrank/hazard ratio approach for time-to-event outcomes in randomized controlled trials (RCTs). The validity of the commonly used nonparametric inference procedure for RMST has been well supported by large sample theories. However, we sometimes encounter cases with a small sample size in practice, where we cannot rely on the large sample properties. Generally, the permutation approach can be useful to handle these situations in RCTs. However, a numerical issue arises when implementing permutation tests for difference or ratio of RMST from two groups. In this article, we discuss the numerical issue and consider six permutation methods for comparing survival time distributions between two groups using RMST in RCTs setting. We conducted extensive numerical studies and assessed type I error rates of these methods. Our numerical studies demonstrated that the inflation of the type I error rate of the asymptotic methods is not negligible when sample size is small, and that all of the six permutation methods are workable solutions. Although some permutation methods became a little conservative, no remarkable inflation of the type I error rates were observed. We recommend using permutation tests instead of the asymptotic tests, especially when the sample size is less than 50 per arm.},
  archive      = {J_SIM},
  author       = {Miki Horiguchi and Hajime Uno},
  doi          = {10.1002/sim.8565},
  journal      = {Statistics in Medicine},
  month        = {9},
  number       = {20},
  pages        = {2655-2670},
  shortjournal = {Stat. Med.},
  title        = {On permutation tests for comparing restricted mean survival time with small sample from randomized trials},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A bayesian hierarchical approach for multiple outcomes in
routinely collected healthcare data. <em>SIM</em>, <em>39</em>(20),
2639–2654. (<a href="https://doi.org/10.1002/sim.8563">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Clinical trials are the standard approach for evaluating new treatments, but may lack the power to assess rare outcomes. Trial results are also necessarily restricted to the population considered in the study. The availability of routinely collected healthcare data provides a source of information on the performance of treatments beyond that offered by clinical trials, but the analysis of this type of data presents a number of challenges. Hierarchical methods, which take advantage of known relationships between clinical outcomes, while accounting for bias, may be a suitable statistical approach for the analysis of this data. A study of direct oral anticoagulants in Scotland is discussed and used to motivate a modeling approach. A Bayesian hierarchical model, which allows a stratification of the population into clusters with similar characteristics, is proposed and applied to the direct oral anticoagulant study data. A simulation study is used to assess its performance in terms of outcome detection and error rates.},
  archive      = {J_SIM},
  author       = {Raymond Carragher and Tanja Mueller and Marion Bennie and Chris Robertson},
  doi          = {10.1002/sim.8563},
  journal      = {Statistics in Medicine},
  month        = {9},
  number       = {20},
  pages        = {2639-2654},
  shortjournal = {Stat. Med.},
  title        = {A bayesian hierarchical approach for multiple outcomes in routinely collected healthcare data},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Confidence intervals of the difference between areas under
two ROC curves in matched-pair experiments. <em>SIM</em>,
<em>39</em>(20), 2621–2638. (<a
href="https://doi.org/10.1002/sim.8562">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In a matched-pair study, when outcomes of two diagnostic tests are ordinal/continuous, the difference between two correlated areas under ROC curves (AUCs) is usually used to compare the overall discriminatory ability of two diagnostic tests. This article considers confidence interval (CI) construction problems of difference between two correlated AUCs in a matched-pair experiment, and proposes 13 hybrid CIs based on variance estimates recovery with the maximum likelihood estimation, Delong&#39;s statistic, Wilson score statistic (WS) and WS with continuity correction, the modified Wald statistic (MW) and MW with continuity correction and Agresti-Coull statistic, and three Bootstrap-resampling-based CIs. For comparison, we present traditional parametric and nonparametric CIs. Simulation studies are conducted to assess the performance of the proposed CIs in terms of empirical coverage probabilities, empirical interval widths, and ratios of the mesial noncoverage probabilities to the noncoverage probabilities. Two examples from clinical studies are illustrated by the proposed methodologies. Empirical results evidence that the hybrid Agresti-Coull CI with the empirical estimation (EAC) behaved most satisfactorily because its coverage probability was quite close to the prespecified confidence level with short interval width. Hence, we recommend the usage of the EAC CI in applications.},
  archive      = {J_SIM},
  author       = {Yunqi Zhang and Niansheng Tang},
  doi          = {10.1002/sim.8562},
  journal      = {Statistics in Medicine},
  month        = {9},
  number       = {20},
  pages        = {2621-2638},
  shortjournal = {Stat. Med.},
  title        = {Confidence intervals of the difference between areas under two ROC curves in matched-pair experiments},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Excess cumulative incidence estimation for matched cohort
survival studies. <em>SIM</em>, <em>39</em>(20), 2606–2620. (<a
href="https://doi.org/10.1002/sim.8561">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We suggest a regression approach to estimate the excess cumulative incidence function (CIF) when matched data are available. In a competing risk setting, we define the excess risk as the difference between the CIF in the exposed group and the background CIF observed in the unexposed group. We show that the excess risk can be estimated through an extended binomial regression model that actively uses the matched structure of the data, avoiding further estimation of both the exposed and the unexposed CIFs. The method naturally deals with two time scales, age and time since exposure and simplifies how to deal with the left truncation on the age time-scale. The model makes it easy to predict individual excess risk scenarios and allows for a direct interpretation of the covariate effects on the cumulative incidence scale. After introducing the model and some theory to justify the approach, we show via simulations that our model works well in practice. We conclude by applying the excess risk model to data from the ALiCCS study to investigate the excess risk of late events in childhood cancer survivors.},
  archive      = {J_SIM},
  author       = {Cristina Boschini and Klaus K. Andersen and Hélène Jacqmin-Gadda and Pierre Joly and Thomas H. Scheike},
  doi          = {10.1002/sim.8561},
  journal      = {Statistics in Medicine},
  month        = {9},
  number       = {20},
  pages        = {2606-2620},
  shortjournal = {Stat. Med.},
  title        = {Excess cumulative incidence estimation for matched cohort survival studies},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A tractable method to account for high-dimensional
nonignorable missing data in intensive longitudinal data. <em>SIM</em>,
<em>39</em>(20), 2589–2605. (<a
href="https://doi.org/10.1002/sim.8560">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Despite the need for sensitivity analysis to nonignorable missingness in intensive longitudinal data (ILD), such analysis is greatly hindered by novel ILD features, such as large data volume and complex nonmonotonic missing-data patterns. Likelihood of alternative models permitting nonignorable missingness often involves very high-dimensional integrals, causing curse of dimensionality and rendering solutions computationally prohibitive to obtain. We aim to overcome this challenge by developing a computationally feasible method, nonlinear indexes of local sensitivity to nonignorability (NISNI). We use linear mixed effects models for the incomplete outcome and covariates. We use Markov multinomial models to describe complex missing-data patterns and mechanisms in ILD, thereby permitting missingness probabilities to depend directly on missing data. Using a second-order Taylor series to approximate likelihood under nonignorability, we develop formulas and closed-form expressions for NISNI. Our approach permits the outcome and covariates to be missing simultaneously, as is often the case in ILD, and can capture U-shaped impact of nonignorability in the neighborhood of the missing at random model without fitting alternative models or evaluating integrals. We evaluate performance of this method using simulated data and real ILD collected by the ecological momentary assessment method.},
  archive      = {J_SIM},
  author       = {Chengbo Yuan and Donald Hedeker and Robin Mermelstein and Hui Xie},
  doi          = {10.1002/sim.8560},
  journal      = {Statistics in Medicine},
  month        = {9},
  number       = {20},
  pages        = {2589-2605},
  shortjournal = {Stat. Med.},
  title        = {A tractable method to account for high-dimensional nonignorable missing data in intensive longitudinal data},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Correction for “optimal unidirectional switch designs” by
zhuozhao zhan, geertruida h. De bock, and edwin r. Van den heuvel.
<em>SIM</em>, <em>39</em>(19), 2587. (<a
href="https://doi.org/10.1002/sim.8553">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_SIM},
  author       = {Zhuozhao Zhan and Edwin R. van den Heuvel},
  doi          = {10.1002/sim.8553},
  journal      = {Statistics in Medicine},
  month        = {8},
  number       = {19},
  pages        = {2587},
  shortjournal = {Stat. Med.},
  title        = {Correction for “Optimal unidirectional switch designs” by zhuozhao zhan, geertruida h. de bock, and edwin r. van den heuvel},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Point and interval estimation in two-stage adaptive designs
with time to event data and biomarker-driven subpopulation selection.
<em>SIM</em>, <em>39</em>(19), 2568–2586. (<a
href="https://doi.org/10.1002/sim.8557">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In personalized medicine, it is often desired to determine if all patients or only a subset of them benefit from a treatment. We consider estimation in two-stage adaptive designs that in stage 1 recruit patients from the full population. In stage 2, patient recruitment is restricted to the part of the population, which, based on stage 1 data, benefits from the experimental treatment. Existing estimators, which adjust for using stage 1 data for selecting the part of the population from which stage 2 patients are recruited, as well as for the confirmatory analysis after stage 2, do not consider time to event patient outcomes. In this work, for time to event data, we have derived a new asymptotically unbiased estimator for the log hazard ratio and a new interval estimator with good coverage probabilities and probabilities that the upper bounds are below the true values. The estimators are appropriate for several selection rules that are based on a single or multiple biomarkers, which can be categorical or continuous.},
  archive      = {J_SIM},
  author       = {Peter K. Kimani and Susan Todd and Lindsay A. Renfro and Ekkehard Glimm and Josephine N. Khan and John A. Kairalla and Nigel Stallard},
  doi          = {10.1002/sim.8557},
  journal      = {Statistics in Medicine},
  month        = {8},
  number       = {19},
  pages        = {2568-2586},
  shortjournal = {Stat. Med.},
  title        = {Point and interval estimation in two-stage adaptive designs with time to event data and biomarker-driven subpopulation selection},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Selective recruitment designs for improving observational
studies using electronic health records. <em>SIM</em>, <em>39</em>(19),
2556–2567. (<a href="https://doi.org/10.1002/sim.8556">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Large-scale electronic health records (EHRs) present an opportunity to quickly identify suitable individuals in order to directly invite them to participate in an observational study. EHRs can contain data from millions of individuals, raising the question of how to optimally select a cohort of size n from a larger pool of size N . In this article, we propose a simple selective recruitment protocol that selects a cohort in which covariates of interest tend to have a uniform distribution. We show that selectively recruited cohorts potentially offer greater statistical power and more accurate parameter estimates than randomly selected cohorts. Our protocol can be applied to studies with multiple categorical and continuous covariates. We apply our protocol to a numerically simulated prospective observational study using an EHR database of stable acute coronary disease patients from 82 089 individuals in the U.K. Selective recruitment designs require a smaller sample size, leading to more efficient and cost-effective studies.},
  archive      = {J_SIM},
  author       = {James E. Barrett and Aylin Cakiroglu and Catey Bunce and Anoop Shah and Spiros Denaxas},
  doi          = {10.1002/sim.8556},
  journal      = {Statistics in Medicine},
  month        = {8},
  number       = {19},
  pages        = {2556-2567},
  shortjournal = {Stat. Med.},
  title        = {Selective recruitment designs for improving observational studies using electronic health records},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). One-stage individual participant data meta-analysis models
for continuous and binary outcomes: Comparison of treatment coding
options and estimation methods. <em>SIM</em>, <em>39</em>(19),
2536–2555. (<a href="https://doi.org/10.1002/sim.8555">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A one-stage individual participant data (IPD) meta-analysis synthesizes IPD from multiple studies using a general or generalized linear mixed model. This produces summary results (eg, about treatment effect) in a single step, whilst accounting for clustering of participants within studies (via a stratified study intercept, or random study intercepts) and between-study heterogeneity (via random treatment effects). We use simulation to evaluate the performance of restricted maximum likelihood (REML) and maximum likelihood (ML) estimation of one-stage IPD meta-analysis models for synthesizing randomized trials with continuous or binary outcomes. Three key findings are identified. First, for ML or REML estimation of stratified intercept or random intercepts models, a t-distribution based approach generally improves coverage of confidence intervals for the summary treatment effect, compared with a z-based approach. Second, when using ML estimation of a one-stage model with a stratified intercept, the treatment variable should be coded using “study-specific centering” (ie, 1/0 minus the study-specific proportion of participants in the treatment group), as this reduces the bias in the between-study variance estimate (compared with 1/0 and other coding options). Third, REML estimation reduces downward bias in between-study variance estimates compared with ML estimation, and does not depend on the treatment variable coding; for binary outcomes, this requires REML estimation of the pseudo-likelihood, although this may not be stable in some situations (eg, when data are sparse). Two applied examples are used to illustrate the findings.},
  archive      = {J_SIM},
  author       = {Richard D. Riley and Amardeep Legha and Dan Jackson and Tim P. Morris and Joie Ensor and Kym I.E. Snell and Ian R. White and Danielle L. Burke},
  doi          = {10.1002/sim.8555},
  journal      = {Statistics in Medicine},
  month        = {8},
  number       = {19},
  pages        = {2536-2555},
  shortjournal = {Stat. Med.},
  title        = {One-stage individual participant data meta-analysis models for continuous and binary outcomes: Comparison of treatment coding options and estimation methods},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Bayesian analysis of multivariate linear mixed models with
censored and intermittent missing responses. <em>SIM</em>,
<em>39</em>(19), 2518–2535. (<a
href="https://doi.org/10.1002/sim.8554">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multivariate longitudinal data usually exhibit complex features such as the presence of censored responses due to detection limits of the assay and unavoidable missing values arising when participants make irregular visits that lead to intermittently recorded characteristics. A generalization of the multivariate linear mixed model constructed by taking into account impacts of censored and intermittent missing responses simultaneously, which is named as the MLMM-CM, has been recently proposed for more precisely analyzing such kinds of data. This paper aims at presenting a fully Bayesian sampling-based approach to the MLMM-CM for addressing the uncertainties of censored and missing responses as well as unknown parameters. Two widely accepted Bayesian computational techniques based on the Markov chain Monte Carlo and the inverse Bayes formulas coupled with the Gibbs (IBF-Gibbs) schemes are developed for carrying out posterior inference of the model. The proposed methodology is illustrated through a simulation study and a real-data example from the Adult AIDS Clinical Trials Group 388 study. Numerical results show empirically that the proposed Bayesian methodology performs satisfactorily and offers reliable posterior inference.},
  archive      = {J_SIM},
  author       = {Wan-Lun Wang},
  doi          = {10.1002/sim.8554},
  journal      = {Statistics in Medicine},
  month        = {8},
  number       = {19},
  pages        = {2518-2535},
  shortjournal = {Stat. Med.},
  title        = {Bayesian analysis of multivariate linear mixed models with censored and intermittent missing responses},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Statistical properties of minimal sufficient balance and
minimization as methods for controlling baseline covariate imbalance at
the design stage of sequential clinical trials. <em>SIM</em>,
<em>39</em>(19), 2506–2517. (<a
href="https://doi.org/10.1002/sim.8552">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {When the number of baseline covariates whose imbalance needs to be controlled in a sequential randomized controlled trial is large, minimization is the most commonly used method for randomizing treatment assignments. The lack of allocation randomness associated with the minimization method has been the source of controversy, and the need to reduce even minor imbalances inherent in the minimization method has been challenged. The minimal sufficient balance (MSB) method is an alternative to the minimization method. It prevents serious imbalance from a large number of covariates while maintaining a high level of allocation randomness. In this study, the two treatment allocation methods are compared with regards to the effectiveness of balancing covariates across treatment arms and allocation randomness in equal allocation clinical trials. The MSB method proves to be equal or superior in both respects. In addition, type I error rate is preserved in analyses for both balancing methods, when using a binary endpoint.},
  archive      = {J_SIM},
  author       = {Steven D. Lauzon and Viswanathan Ramakrishnan and Paul J. Nietert and Jody D. Ciolino and Michael D. Hill and Wenle Zhao},
  doi          = {10.1002/sim.8552},
  journal      = {Statistics in Medicine},
  month        = {8},
  number       = {19},
  pages        = {2506-2517},
  shortjournal = {Stat. Med.},
  title        = {Statistical properties of minimal sufficient balance and minimization as methods for controlling baseline covariate imbalance at the design stage of sequential clinical trials},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A test for differential ascertainment in case-control
studies with application to child maltreatment. <em>SIM</em>,
<em>39</em>(19), 2490–2505. (<a
href="https://doi.org/10.1002/sim.8551">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a method to test for the presence of differential ascertainment in case-control studies, when data are collected by multiple sources. We show that, when differential ascertainment is present, the use of only the observed cases leads to severe bias in the computation of the odds ratio. We can alleviate the effect of such bias using the estimates that our method of testing for differential ascertainment naturally provides. We apply it to a dataset obtained from the National Violent Death Reporting System, with the goal of checking for the presence of differential ascertainment by race in the count of deaths caused by child maltreatment.},
  archive      = {J_SIM},
  author       = {Matteo Sordello and Dylan S. Small},
  doi          = {10.1002/sim.8551},
  journal      = {Statistics in Medicine},
  month        = {8},
  number       = {19},
  pages        = {2490-2505},
  shortjournal = {Stat. Med.},
  title        = {A test for differential ascertainment in case-control studies with application to child maltreatment},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A covariate-specific time-dependent receiver operating
characteristic curve for correlated survival data. <em>SIM</em>,
<em>39</em>(19), 2477–2489. (<a
href="https://doi.org/10.1002/sim.8550">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Several studies for the clinical validity of circulating tumor cells (CTCs) in metastatic breast cancer were conducted showing that it is a prognostic biomarker of overall survival. In this work, we consider an individual patient data meta-analysis for nonmetastatic breast cancer to assess the discrimination of CTCs regarding the risk of death. Data are collected in several centers and present correlated failure times for subjects of the same center. However, although the covariate-specific time-dependent receiver operating characteristic (ROC) curve has been widely used for assessing the performance of a biomarker, there is no methodology yet that can handle this specific setting with clustered censored failure times. We propose an estimator for the covariate-specific time-dependent ROC curves and area under the ROC curve when clustered failure times are detected. We discuss the assumptions under which the estimators are consistent and their interpretations. We assume a shared frailty model for modeling the effect of the covariates and the biomarker on the outcome in order to account for the cluster effect. A simulation study was conducted and it shows negligible bias for the proposed estimator and a nonparametric one based on inverse probability censoring weighting, while a semiparametric estimator, ignoring the clustering, is markedly biased. Finally, in our application to breast cancer data, the estimation of the covariate-specific area under the curves illustrates that the CTCs discriminate better patients with inflammatory tumor than patients with noninflammatory tumor, with respect to their risk of death.},
  archive      = {J_SIM},
  author       = {Alessandra Meddis and Paul Blanche and François C Bidard and Aurélien Latouche},
  doi          = {10.1002/sim.8550},
  journal      = {Statistics in Medicine},
  month        = {8},
  number       = {19},
  pages        = {2477-2489},
  shortjournal = {Stat. Med.},
  title        = {A covariate-specific time-dependent receiver operating characteristic curve for correlated survival data},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Quantifying the bias due to observed individual confounders
in causal treatment effect estimates. <em>SIM</em>, <em>39</em>(18),
2447–2476. (<a href="https://doi.org/10.1002/sim.8549">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {It is often of interest to use observational data to estimate the causal effect of a target exposure or treatment on an outcome. When estimating the treatment effect, it is essential to appropriately adjust for selection bias due to observed confounders using, for example, propensity score weighting. Selection bias due to confounders occurs when individuals who are treated are substantially different from those who are untreated with respect to covariates that are also associated with the outcome. A comparison of the unadjusted, naive treatment effect estimate with the propensity score adjusted treatment effect estimate provides an estimate of the selection bias due to these observed confounders. In this article, we propose methods to identify the observed covariate that explains the largest proportion of the estimated selection bias. Identification of the most influential observed covariate or covariates is important in resource-sensitive settings where the number of covariates obtained from individuals needs to be minimized due to cost and/or patient burden and in settings where this covariate can provide actionable information to healthcare agencies, providers, and stakeholders. We propose straightforward parametric and nonparametric procedures to examine the role of observed covariates and quantify the proportion of the observed selection bias explained by each covariate. We demonstrate good finite sample performance of our proposed estimates using a simulation study and use our procedures to identify the most influential covariates that explain the observed selection bias in estimating the causal effect of alcohol use on progression of Huntington&#39;s disease, a rare neurological disease.},
  archive      = {J_SIM},
  author       = {Layla Parast and Beth Ann Griffin},
  doi          = {10.1002/sim.8549},
  journal      = {Statistics in Medicine},
  month        = {8},
  number       = {18},
  pages        = {2447-2476},
  shortjournal = {Stat. Med.},
  title        = {Quantifying the bias due to observed individual confounders in causal treatment effect estimates},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Predictive accuracy of markers or risk scores for interval
censored survival data. <em>SIM</em>, <em>39</em>(18), 2437–2446. (<a
href="https://doi.org/10.1002/sim.8547">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Methods for the evaluation of the predictive accuracy of biomarkers with respect to survival outcomes subject to right censoring have been discussed extensively in the literature. In cancer and other diseases, survival outcomes are commonly subject to interval censoring by design or due to the follow up schema. In this article, we present an estimator for the area under the time-dependent receiver operating characteristic (ROC) curve for interval censored data based on a nonparametric sieve maximum likelihood approach. We establish the asymptotic properties of the proposed estimator and illustrate its finite-sample properties using a simulation study. The application of our method is illustrated using data from a cancer clinical study. An open-source R package to implement the proposed method is available on Comprehensive R Archive Network.},
  archive      = {J_SIM},
  author       = {Yuan Wu and Xiaofei Wang and Jiaxing Lin and Beilin Jia and Kouros Owzar},
  doi          = {10.1002/sim.8547},
  journal      = {Statistics in Medicine},
  month        = {8},
  number       = {18},
  pages        = {2437-2446},
  shortjournal = {Stat. Med.},
  title        = {Predictive accuracy of markers or risk scores for interval censored survival data},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Group testing in mediation analysis. <em>SIM</em>,
<em>39</em>(18), 2423–2436. (<a
href="https://doi.org/10.1002/sim.8546">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider the scenario where there is an exposure, multiple biologically defined sets of biomarkers, and an outcome. We propose a new two-step procedure that tests if any of the sets of biomarkers mediate the exposure/outcome relationship, while maintaining a prespecified familywise error rate. The first step of the proposed procedure is a screening step that removes all groups that are unlikely to be strongly associated with both the exposure and the outcome. The second step adapts recent advances in postselection inference to test if there are true mediators in each of the remaining candidate sets. We use simulation to show that this simple two-step procedure has higher statistical power to detect true mediating sets when compared with existing procedures. We then use our two-step procedure to identify a set of Lysine-related metabolites that potentially mediate the known relationship between increased body mass index and the increased risk of estrogen-receptor positive breast cancer in postmenopausal women.},
  archive      = {J_SIM},
  author       = {Andriy Derkach and Steven C. Moore and Simina M. Boca and Joshua N. Sampson},
  doi          = {10.1002/sim.8546},
  journal      = {Statistics in Medicine},
  month        = {8},
  number       = {18},
  pages        = {2423-2436},
  shortjournal = {Stat. Med.},
  title        = {Group testing in mediation analysis},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Learning genetic and environmental graphical models from
family data. <em>SIM</em>, <em>39</em>(18), 2403–2422. (<a
href="https://doi.org/10.1002/sim.8545">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many challenging problems in biomedical research rely on understanding how variables are associated with each other and influenced by genetic and environmental factors. Probabilistic graphical models (PGMs) are widely acknowledged as a very natural and formal language to describe relationships among variables and have been extensively used for studying complex diseases and traits. In this work, we propose methods that leverage observational Gaussian family data for learning a decomposition of undirected and directed acyclic PGMs according to the influence of genetic and environmental factors. Many structure learning algorithms are strongly based on a conditional independence test. For independent measurements of normally distributed variables, conditional independence can be tested through standard tests for zero partial correlation. In family data, the assumption of independent measurements does not hold since related individuals are correlated due to mainly genetic factors. Based on univariate polygenic linear mixed models, we propose tests that account for the familial dependence structure and allow us to assess the significance of the partial correlation due to genetic (between-family) factors and due to other factors, denoted here as environmental (within-family) factors, separately. Then, we extend standard structure learning algorithms, including the IC/PC and the really fast causal inference (RFCI) algorithms, to Gaussian family data. The algorithms learn the most likely PGM and its decomposition into two components, one explained by genetic factors and the other by environmental factors. The proposed methods are evaluated by simulation studies and applied to the Genetic Analysis Workshop 13 simulated dataset, which captures significant features of the Framingham Heart Study.},
  archive      = {J_SIM},
  author       = {Adèle H. Ribeiro and Júlia Maria Pavan Soler},
  doi          = {10.1002/sim.8545},
  journal      = {Statistics in Medicine},
  month        = {8},
  number       = {18},
  pages        = {2403-2422},
  shortjournal = {Stat. Med.},
  title        = {Learning genetic and environmental graphical models from family data},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Sample-weighted semiparametric estimation of cause-specific
cumulative risk and incidence using left- or interval-censored data from
electronic health records. <em>SIM</em>, <em>39</em>(18), 2387–2402. (<a
href="https://doi.org/10.1002/sim.8544">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Electronic health records (EHRs) can be a cost-effective data source for forming cohorts and developing risk models in the context of disease screening. However, important issues need to be handled: competing outcomes, left-censoring of prevalent disease, interval-censoring of incident disease, and uncertainty of prevalent disease when accurate disease ascertainment is not conducted at baseline. Furthermore, novel tests that are costly and limited in availability can be conducted on stored biospecimens selected as samples from EHRs by using different sampling fractions. We extend sample-weighted semiparametric marginal mixture models to estimating competing risks. For flexible modeling of relative risks, a general transformation of the subdistribution hazard function and regression parameters is used. We propose a numerical algorithm for nonparametrically calculating the maximum likelihood estimates for subdistribution hazard functions and regression parameters. Methods for calculating the consistent confidence intervals for relative and absolute risk estimates are presented. The proposed algorithm and methods show reliable finite sample performance through simulation studies. We apply our methods to a cohort assembled from EHRs at a health maintenance organization where we estimate cumulative risk of cervical precancer/cancer and incidence of infection-clearance by HPV genotype among human papillomavirus (HPV) positive women. There is no significant difference in 3-year HPV-clearance rates across different HPV types, but 3-year cumulative risk of progression-to-precancer/cancer from HPV-16 is relatively higher than the other HPV genotypes.},
  archive      = {J_SIM},
  author       = {Noorie Hyun and Hormuzd A. Katki and Barry I. Graubard},
  doi          = {10.1002/sim.8544},
  journal      = {Statistics in Medicine},
  month        = {8},
  number       = {18},
  pages        = {2387-2402},
  shortjournal = {Stat. Med.},
  title        = {Sample-weighted semiparametric estimation of cause-specific cumulative risk and incidence using left- or interval-censored data from electronic health records},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Seamless phase 2/3 oncology trial design with flexible
sample size determination. <em>SIM</em>, <em>39</em>(18), 2373–2386. (<a
href="https://doi.org/10.1002/sim.8543">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Conventional seamless phase 2/3 design with fixed sample size determination (SSD) has gained its popularity in oncology drug development due to attractive features such as significantly shortening the development timeline, minimizing sample size, as well as early decision making. However, this design is not immune to inaccurate treatment effect assumption when only limited efficacy data are available at study design stage. We propose an innovative seamless phase 2/3 study design with flexible SSD for oncology trials, in which the trial is designed under a distribution of treatment effect instead of one single assumption due to huge uncertainty of treatment effect at design stage and the sample size for end of phase 3 analysis is not predetermined at design stage, but rather dynamically determined based on observed treatment effect at phase 2 portion. Some practical sample size determination rules for end of phase 3 analysis will be discussed. The proposed design can lead to reduced sample size or/and improved power compared with conventional seamless phase 2/3 design with fixed SSD. This innovative study design can be especially useful for programs with aggressive development strategy to expedite the process in delivering efficacious treatment to patients.},
  archive      = {J_SIM},
  author       = {Zhaoyang Teng and Yuan Tian and Yi Liu and Guohui Liu},
  doi          = {10.1002/sim.8543},
  journal      = {Statistics in Medicine},
  month        = {8},
  number       = {18},
  pages        = {2373-2386},
  shortjournal = {Stat. Med.},
  title        = {Seamless phase 2/3 oncology trial design with flexible sample size determination},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Correction. <em>SIM</em>, <em>39</em>(17), 2371–2372. (<a
href="https://doi.org/10.1002/sim.8527">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_SIM},
  author       = {Rui Zhu},
  doi          = {10.1002/sim.8527},
  journal      = {Statistics in Medicine},
  month        = {7},
  number       = {17},
  pages        = {2371-2372},
  shortjournal = {Stat. Med.},
  title        = {Correction},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Matching with time-dependent treatments: A review and look
forward. <em>SIM</em>, <em>39</em>(17), 2350–2370. (<a
href="https://doi.org/10.1002/sim.8533">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Observational studies of treatment effects attempt to mimic a randomized experiment by balancing the covariate distribution in treated and control groups, thus removing biases related to measured confounders. Methods such as weighting, matching, and stratification, with or without a propensity score, are common in cross-sectional data. When treatments are initiated over longitudinal follow-up, a target pragmatic trial can be emulated using appropriate matching methods. The ideal experiment of interest is simple; patients would be enrolled sequentially, randomized to one or more treatments and followed subsequently. This tutorial defines a class of longitudinal matching methods that emulate this experiment and provides a review of existing variations, with guidance regarding study design, execution, and analysis. These principles are illustrated in application to the study of statins on cardiovascular outcomes in the Framingham Offspring cohort. We identify avenues for future research and highlight the relevance of this methodology to high-quality comparative effectiveness studies in the era of big data.},
  archive      = {J_SIM},
  author       = {Laine E. Thomas and Siyun Yang and Daniel Wojdyla and Douglas E. Schaubel},
  doi          = {10.1002/sim.8533},
  journal      = {Statistics in Medicine},
  month        = {7},
  number       = {17},
  pages        = {2350-2370},
  shortjournal = {Stat. Med.},
  title        = {Matching with time-dependent treatments: A review and look forward},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Deep learning for survival outcomes. <em>SIM</em>,
<em>39</em>(17), 2339–2349. (<a
href="https://doi.org/10.1002/sim.8542">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning is a class of machine learning algorithms that are popular for building risk prediction models. When observations are censored, the outcomes are only partially observed and standard deep learning algorithms cannot be directly applied. We develop a new class of deep learning algorithms for outcomes that are potentially censored. To account for censoring, the unobservable loss function used in the absence of censoring is replaced by a censoring unbiased transformation. The resulting class of algorithms can be used to estimate both survival probabilities and restricted mean survival. We show how the deep learning algorithms can be implemented by adapting software for uncensored data by using a form of response transformation. We provide comparisons of the proposed deep learning algorithms to existing risk prediction algorithms for predicting survival probabilities and restricted mean survival through both simulated datasets and analysis of data from breast cancer patients.},
  archive      = {J_SIM},
  author       = {Jon Arni Steingrimsson and Samantha Morrison},
  doi          = {10.1002/sim.8542},
  journal      = {Statistics in Medicine},
  month        = {7},
  number       = {17},
  pages        = {2339-2349},
  shortjournal = {Stat. Med.},
  title        = {Deep learning for survival outcomes},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Doubly robust estimation and causal inference for recurrent
event data. <em>SIM</em>, <em>39</em>(17), 2324–2338. (<a
href="https://doi.org/10.1002/sim.8541">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many longitudinal databases record the occurrence of recurrent events over time. In this article, we propose a new method to estimate the average causal effect of a binary treatment for recurrent event data in the presence of confounders. We propose a doubly robust semiparametric estimator based on a weighted version of the Nelson-Aalen estimator and a conditional regression estimator under an assumed semiparametric multiplicative rate model for recurrent event data. We show that the proposed doubly robust estimator is consistent and asymptotically normal. In addition, a model diagnostic plot of residuals is presented to assess the adequacy of our proposed semiparametric model. We then evaluate the finite sample behavior of the proposed estimators under a number of simulation scenarios. Finally, we illustrate the proposed methodology via a database of circus artist injuries.},
  archive      = {J_SIM},
  author       = {Chien-Lin Su and Russell Steele and Ian Shrier},
  doi          = {10.1002/sim.8541},
  journal      = {Statistics in Medicine},
  month        = {7},
  number       = {17},
  pages        = {2324-2338},
  shortjournal = {Stat. Med.},
  title        = {Doubly robust estimation and causal inference for recurrent event data},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A novel approach for propensity score matching and
stratification for multiple treatments: Application to an electronic
health record–derived study. <em>SIM</em>, <em>39</em>(17), 2308–2323.
(<a href="https://doi.org/10.1002/sim.8540">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Currently, methods for conducting multiple treatment propensity scoring in the presence of high-dimensional covariate spaces that result from “big data” are lacking—the most prominent method relies on inverse probability treatment weighting (IPTW). However, IPTW only utilizes one element of the generalized propensity score (GPS) vector, which can lead to a loss of information and inadequate covariate balance in the presence of multiple treatments. This limitation motivates the development of a novel propensity score method that uses the entire GPS vector to establish a scalar balancing score that, when adjusted for, achieves covariate balance in the presence of potentially high-dimensional covariates. Specifically, the generalized propensity score cumulative distribution function (GPS-CDF) method is introduced. A one-parameter power function fits the CDF of the GPS vector and a resulting scalar balancing score is used for matching and/or stratification. Simulation results show superior performance of the new method compared to IPTW both in achieving covariate balance and estimating average treatment effects in the presence of multiple treatments. The proposed approach is applied to a study derived from electronic medical records to determine the causal relationship between three different vasopressors and mortality in patients with non-traumatic aneurysmal subarachnoid hemorrhage. Results suggest that the GPS-CDF method performs well when applied to large observational studies with multiple treatments that have large covariate spaces.},
  archive      = {J_SIM},
  author       = {Derek W. Brown and Stacia M. DeSantis and Thomas J. Greene and Vahed Maroufy and Ashraf Yaseen and Hulin Wu and George Williams and Michael D. Swartz},
  doi          = {10.1002/sim.8540},
  journal      = {Statistics in Medicine},
  month        = {7},
  number       = {17},
  pages        = {2308-2323},
  shortjournal = {Stat. Med.},
  title        = {A novel approach for propensity score matching and stratification for multiple treatments: Application to an electronic health record–derived study},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Nonparametric covariate hypothesis tests for the cure rate
in mixture cure models. <em>SIM</em>, <em>39</em>(17), 2291–2307. (<a
href="https://doi.org/10.1002/sim.8530">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In lifetime data, like cancer studies, there may be long term survivors, which lead to heavy censoring at the end of the follow-up period. Since a standard survival model is not appropriate to handle these data, a cure model is needed. In the literature, covariate hypothesis tests for cure models are limited to parametric and semiparametric methods. We fill this important gap by proposing a nonparametric covariate hypothesis test for the probability of cure in mixture cure models. A bootstrap method is proposed to approximate the null distribution of the test statistic. The procedure can be applied to any type of covariate, and could be extended to the multivariate setting. Its efficiency is evaluated in a Monte Carlo simulation study. Finally, the method is applied to a colorectal cancer dataset.},
  archive      = {J_SIM},
  author       = {Ana López-Cheda and Maria Amalia Jácome and Ingrid Van Keilegom and Ricardo Cao},
  doi          = {10.1002/sim.8530},
  journal      = {Statistics in Medicine},
  month        = {7},
  number       = {17},
  pages        = {2291-2307},
  shortjournal = {Stat. Med.},
  title        = {Nonparametric covariate hypothesis tests for the cure rate in mixture cure models},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Uncertainty in the design stage of two-stage bayesian
propensity score analysis. <em>SIM</em>, <em>39</em>(17), 2265–2290. (<a
href="https://doi.org/10.1002/sim.8486">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The two-stage process of propensity score analysis (PSA) includes a design stage where propensity scores (PSs) are estimated and implemented to approximate a randomized experiment and an analysis stage where treatment effects are estimated conditional on the design. This article considers how uncertainty associated with the design stage impacts estimation of causal effects in the analysis stage. Such design uncertainty can derive from the fact that the PS itself is an estimated quantity, but also from other features of the design stage tied to choice of PS implementation. This article offers a procedure for obtaining the posterior distribution of causal effects after marginalizing over a distribution of design-stage outputs, lending a degree of formality to Bayesian methods for PSA that have gained attention in recent literature. Formulation of a probability distribution for the design-stage output depends on how the PS is implemented in the design stage, and propagation of uncertainty into causal estimates depends on how the treatment effect is estimated in the analysis stage. We explore these differences within a sample of commonly used PS implementations (quantile stratification, nearest-neighbor matching, caliper matching, inverse probability of treatment weighting, and doubly robust estimation) and investigate in a simulation study the impact of statistician choice in PS model and implementation on the degree of between- and within-design variability in the estimated treatment effect. The methods are then deployed in an investigation of the association between levels of fine particulate air pollution and elevated exposure to emissions from coal-fired power plants.},
  archive      = {J_SIM},
  author       = {Shirley X. Liao and Corwin M. Zigler},
  doi          = {10.1002/sim.8486},
  journal      = {Statistics in Medicine},
  month        = {7},
  number       = {17},
  pages        = {2265-2290},
  shortjournal = {Stat. Med.},
  title        = {Uncertainty in the design stage of two-stage bayesian propensity score analysis},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). STRATOS guidance document on measurement error and
misclassification of variables in observational epidemiology: Part
2—more complex methods of adjustment and advanced topics. <em>SIM</em>,
<em>39</em>(16), 2232–2263. (<a
href="https://doi.org/10.1002/sim.8531">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We continue our review of issues related to measurement error and misclassification in epidemiology. We further describe methods of adjusting for biased estimation caused by measurement error in continuous covariates, covering likelihood methods, Bayesian methods, moment reconstruction, moment-adjusted imputation, and multiple imputation. We then describe which methods can also be used with misclassification of categorical covariates. Methods of adjusting estimation of distributions of continuous variables for measurement error are then reviewed. Illustrative examples are provided throughout these sections. We provide lists of available software for implementing these methods and also provide the code for implementing our examples in the Supporting Information. Next, we present several advanced topics, including data subject to both classical and Berkson error, modeling continuous exposures with measurement error, and categorical exposures with misclassification in the same model, variable selection when some of the variables are measured with error, adjusting analyses or design for error in an outcome variable, and categorizing continuous variables measured with error. Finally, we provide some advice for the often met situations where variables are known to be measured with substantial error, but there is only an external reference standard or partial (or no) information about the type or magnitude of the error.},
  archive      = {J_SIM},
  author       = {Pamela A. Shaw and Paul Gustafson and Raymond J. Carroll and Veronika Deffner and Kevin W. Dodd and Ruth H. Keogh and Victor Kipnis and Janet A. Tooze and Michael P. Wallace and Helmut Küchenhoff and Laurence S. Freedman},
  doi          = {10.1002/sim.8531},
  journal      = {Statistics in Medicine},
  month        = {7},
  number       = {16},
  pages        = {2232-2263},
  shortjournal = {Stat. Med.},
  title        = {STRATOS guidance document on measurement error and misclassification of variables in observational epidemiology: Part 2—More complex methods of adjustment and advanced topics},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). STRATOS guidance document on measurement error and
misclassification of variables in observational epidemiology: Part
1—basic theory and simple methods of adjustment. <em>SIM</em>,
<em>39</em>(16), 2197–2231. (<a
href="https://doi.org/10.1002/sim.8532">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Measurement error and misclassification of variables frequently occur in epidemiology and involve variables important to public health. Their presence can impact strongly on results of statistical analyses involving such variables. However, investigators commonly fail to pay attention to biases resulting from such mismeasurement. We provide, in two parts, an overview of the types of error that occur, their impacts on analytic results, and statistical methods to mitigate the biases that they cause. In this first part, we review different types of measurement error and misclassification, emphasizing the classical, linear, and Berkson models, and on the concepts of nondifferential and differential error. We describe the impacts of these types of error in covariates and in outcome variables on various analyses, including estimation and testing in regression models and estimating distributions. We outline types of ancillary studies required to provide information about such errors and discuss the implications of covariate measurement error for study design. Methods for ascertaining sample size requirements are outlined, both for ancillary studies designed to provide information about measurement error and for main studies where the exposure of interest is measured with error. We describe two of the simpler methods, regression calibration and simulation extrapolation (SIMEX), that adjust for bias in regression coefficients caused by measurement error in continuous covariates, and illustrate their use through examples drawn from the Observing Protein and Energy (OPEN) dietary validation study. Finally, we review software available for implementing these methods. The second part of the article deals with more advanced topics.},
  archive      = {J_SIM},
  author       = {Ruth H. Keogh and Pamela A. Shaw and Paul Gustafson and Raymond J. Carroll and Veronika Deffner and Kevin W. Dodd and Helmut Küchenhoff and Janet A. Tooze and Michael P. Wallace and Victor Kipnis and Laurence S. Freedman},
  doi          = {10.1002/sim.8532},
  journal      = {Statistics in Medicine},
  month        = {7},
  number       = {16},
  pages        = {2197-2231},
  shortjournal = {Stat. Med.},
  title        = {STRATOS guidance document on measurement error and misclassification of variables in observational epidemiology: Part 1—Basic theory and simple methods of adjustment},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Imbalanced randomization in clinical trials. <em>SIM</em>,
<em>39</em>(16), 2185–2196. (<a
href="https://doi.org/10.1002/sim.8539">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Randomization is a common technique used in clinical trials to eliminate potential bias and confounders in a patient population. Equal allocation to treatment groups is the standard due to its optimal efficiency in many cases. However, in certain scenarios, unequal allocation can improve efficiency. In superiority trials with more than two groups, the optimal randomization is not always a balanced randomization. In noninferiority (NI) trials, additive margin with equal variance is the only instance with balanced randomization. Optimal randomization for NI trials can be far from 1:1 and can greatly improve efficiency, a fact which is commonly overlooked. A tool for sample size calculation for NI trials with additive or multiplicative margin with normal, binomial, or Poisson distribution is available at http://www.statlab.wisc.edu/shiny/SSNI/ .},
  archive      = {J_SIM},
  author       = {Thevaa Chandereng and Xiaodan Wei and Rick Chappell},
  doi          = {10.1002/sim.8539},
  journal      = {Statistics in Medicine},
  month        = {7},
  number       = {16},
  pages        = {2185-2196},
  shortjournal = {Stat. Med.},
  title        = {Imbalanced randomization in clinical trials},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Optimized variable selection via repeated data splitting.
<em>SIM</em>, <em>39</em>(16), 2167–2184. (<a
href="https://doi.org/10.1002/sim.8538">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Model selection in high-dimensional settings has received substantial attention in recent years, however, similar advancements in the low-dimensional setting have been lacking. In this article, we introduce a new variable selection procedure for low to moderate scale regressions ( n &gt; p ). This method repeatedly splits the data into two sets, one for estimation and one for validation, to obtain an empirically optimized threshold which is then used to screen for variables to include in the final model. In an extensive simulation study, we show that the proposed variable selection technique enjoys superior performance compared with candidate methods (backward elimination via repeated data splitting, univariate screening at 0.05 level, adaptive LASSO, SCAD), being amongst those with the lowest inclusion of noisy predictors while having the highest power to detect the correct model and being unaffected by correlations among the predictors. We illustrate the methods by applying them to a cohort of patients undergoing hepatectomy at our institution.},
  archive      = {J_SIM},
  author       = {Marinela Capanu and Mihai Giurcanu and Colin B. Begg and Mithat Gönen},
  doi          = {10.1002/sim.8538},
  journal      = {Statistics in Medicine},
  month        = {7},
  number       = {16},
  pages        = {2167-2184},
  shortjournal = {Stat. Med.},
  title        = {Optimized variable selection via repeated data splitting},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Improving coverage probabilities for parametric tolerance
intervals via bootstrap calibration. <em>SIM</em>, <em>39</em>(16),
2152–2166. (<a href="https://doi.org/10.1002/sim.8537">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Statistical tolerance intervals are commonly employed in biomedical and pharmaceutical research, such as in lifetime analysis, the assessment of biosimilarity of branded and generic versions of biopharmaceutical drugs, and in quality control of drug products to ensure that a specified proportion of the products are covered within established acceptance limits. Exact two-sided parametric tolerance intervals are only available for the normal distribution, while exact one-sided parametric tolerance limits are available for a limited number of distributions. Approximations to two-sided parametric tolerance intervals often use the Bonferroni correction on the one-sided tolerance interval calculation; however, this often incurs a higher coverage probability than the nominal level. Recently, the usage of a bootstrap calibration has been demonstrated as a way to improve coverage probabilities of tolerance intervals for very specific and complex distributional settings. We present a focused treatment on using a single-layer bootstrap calibration to improve the coverage probabilities of two-sided parametric tolerance intervals. Simulation results clearly demonstrate the improved coverage probabilities towards the nominal level over the uncalibrated setting. Applications to medical data for various parametric distributions also highlight the utility of constructing these calibrated tolerance intervals.},
  archive      = {J_SIM},
  author       = {Yixuan Zou and Derek S. Young},
  doi          = {10.1002/sim.8537},
  journal      = {Statistics in Medicine},
  month        = {7},
  number       = {16},
  pages        = {2152-2166},
  shortjournal = {Stat. Med.},
  title        = {Improving coverage probabilities for parametric tolerance intervals via bootstrap calibration},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). BAREB: A bayesian repulsive biclustering model for
periodontal data. <em>SIM</em>, <em>39</em>(16), 2139–2151. (<a
href="https://doi.org/10.1002/sim.8536">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Preventing periodontal diseases (PD) and maintaining the structure and function of teeth are important goals for personal oral care. To understand the heterogeneity in patients with diverse PD patterns, we develop a Bayesian repulsive biclustering method that can simultaneously cluster the PD patients and their tooth sites after taking the patient- and site-level covariates into consideration. BAREB uses the determinantal point process prior to induce diversity among different biclusters to facilitate parsimony and interpretability. Since PD progression is hypothesized to be spatially referenced, BAREB factors in the spatial dependence among tooth sites. In addition, since PD is the leading cause for tooth loss, the missing data mechanism is nonignorable. Such nonrandom missingness is incorporated into BAREB. For the posterior inference, we design an efficient reversible jump Markov chain Monte Carlo sampler. Simulation studies show that BAREB is able to accurately estimate the biclusters, and compares favorably to alternatives. For real world application, we apply BAREB to a dataset from a clinical PD study, and obtain desirable and interpretable results. A major contribution of this article is the Rcpp implementation of our methodology, available in the R package BAREB .},
  archive      = {J_SIM},
  author       = {Yuliang Li and Dipankar Bandyopadhyay and Fangzheng Xie and Yanxun Xu},
  doi          = {10.1002/sim.8536},
  journal      = {Statistics in Medicine},
  month        = {7},
  number       = {16},
  pages        = {2139-2151},
  shortjournal = {Stat. Med.},
  title        = {BAREB: A bayesian repulsive biclustering model for periodontal data},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Individual participant data meta-analysis to examine
interactions between treatment effect and participant-level covariates:
Statistical recommendations for conduct and planning. <em>SIM</em>,
<em>39</em>(15), 2115–2137. (<a
href="https://doi.org/10.1002/sim.8516">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Precision medicine research often searches for treatment-covariate interactions, which refers to when a treatment effect (eg, measured as a mean difference, odds ratio, hazard ratio) changes across values of a participant-level covariate (eg, age, gender, biomarker). Single trials do not usually have sufficient power to detect genuine treatment-covariate interactions, which motivate the sharing of individual participant data (IPD) from multiple trials for meta-analysis. Here, we provide statistical recommendations for conducting and planning an IPD meta-analysis of randomized trials to examine treatment-covariate interactions. For conduct, two-stage and one-stage statistical models are described, and we recommend: (i) interactions should be estimated directly, and not by calculating differences in meta-analysis results for subgroups; (ii) interaction estimates should be based solely on within-study information; (iii) continuous covariates and outcomes should be analyzed on their continuous scale; (iv) nonlinear relationships should be examined for continuous covariates, using a multivariate meta-analysis of the trend (eg, using restricted cubic spline functions); and (v) translation of interactions into clinical practice is nontrivial, requiring individualized treatment effect prediction. For planning, we describe first why the decision to initiate an IPD meta-analysis project should not be based on between-study heterogeneity in the overall treatment effect; and second, how to calculate the power of a potential IPD meta-analysis project in advance of IPD collection, conditional on characteristics (eg, number of participants, standard deviation of covariates) of the trials (potentially) promising their IPD. Real IPD meta-analysis projects are used for illustration throughout.},
  archive      = {J_SIM},
  author       = {Richard D. Riley and Thomas P.A. Debray and David Fisher and Miriam Hattle and Nadine Marlin and Jeroen Hoogland and Francois Gueyffier and Jan A. Staessen and Jiguang Wang and Karel G.M. Moons and Johannes B. Reitsma and Joie Ensor},
  doi          = {10.1002/sim.8516},
  journal      = {Statistics in Medicine},
  month        = {7},
  number       = {15},
  pages        = {2115-2137},
  shortjournal = {Stat. Med.},
  title        = {Individual participant data meta-analysis to examine interactions between treatment effect and participant-level covariates: Statistical recommendations for conduct and planning},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Early dengue outbreak detection modeling based on dengue
incidences in singapore during 2012 to 2017. <em>SIM</em>,
<em>39</em>(15), 2101–2114. (<a
href="https://doi.org/10.1002/sim.8535">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dengue has been as an endemic with year-round presence in Singapore. In the recent years 2013, 2014, and 2016, there were several severe dengue outbreaks, posing serious threat to the public health. To proactively control and mitigate the disease spread, early warnings of dengue outbreaks, at which there are rapid and large-scale spread of dengue incidences, are extremely helpful. In this study, a two-step framework is proposed to predict dengue outbreaks and it is evaluated based on the dengue incidences in Singapore during 2012 to 2017. First, a generalized additive model (GAM) is trained based on the weekly dengue incidence data during 2006 to 2011. The proposed GAM is a one-week-ahead forecasting model, and it inherently accounts for the possible correlation among the historical incidence data, making the residuals approximately normally distributed. Then, an exponentially weighted moving average (EWMA) control chart is proposed to sequentially monitor the weekly residuals during 2012 to 2017. Our investigation shows that the proposed two-step framework is able to give persistent signals at the early stage of the outbreaks in 2013, 2014, and 2016, which provides early alerts of outbreaks and wins time for the early interventions and the preparation of necessary public health resources. In addition, extensive simulations show that the proposed method is comparable to other potential outbreak detection methods and it is robust to the underlying data-generating mechanisms.},
  archive      = {J_SIM},
  author       = {Piao Chen and Xiuju Fu and Stefan Ma and Hai-Yan Xu and Wanbing Zhang and Gaoxi Xiao and Rick Siow Mong Goh and George Xu and Lee Ching Ng},
  doi          = {10.1002/sim.8535},
  journal      = {Statistics in Medicine},
  month        = {7},
  number       = {15},
  pages        = {2101-2114},
  shortjournal = {Stat. Med.},
  title        = {Early dengue outbreak detection modeling based on dengue incidences in singapore during 2012 to 2017},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A new conditional performance score for the evaluation of
adaptive group sequential designs with sample size recalculation.
<em>SIM</em>, <em>39</em>(15), 2067–2100. (<a
href="https://doi.org/10.1002/sim.8534">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In standard clinical trial designs, the required sample size is fixed in the planning stage based on initial parameter assumptions. It is intuitive that the correct choice of the sample size is of major importance for an ethical justification of the trial. The required parameter assumptions should be based on previously published results from the literature. In clinical practice, however, historical data often do not exist or show highly variable results. Adaptive group sequential designs allow a sample size recalculation after a planned unblinded interim analysis in order to adjust the sample size during the ongoing trial. So far, there exist no unique standards to assess the performance of sample size recalculation rules. Single performance criteria commonly reported are given by the power and the average sample size; the variability of the recalculated sample size and the conditional power distribution are usually ignored. Therefore, the need for an adequate performance score combining these relevant performance criteria is evident. To judge the performance of an adaptive design, there exist two possible perspectives, which might also be combined: Either the global performance of the design can be addressed, which averages over all possible interim results, or the conditional performance is addressed, which focuses on the remaining performance conditional on a specific interim result. In this work, we give a compact overview of sample size recalculation rules and performance measures. Moreover, we propose a new conditional performance score and apply it to various standard recalculation rules by means of Monte-Carlo simulations.},
  archive      = {J_SIM},
  author       = {Carolin Herrmann and Maximilian Pilz and Meinhard Kieser and Geraldine Rauch},
  doi          = {10.1002/sim.8534},
  journal      = {Statistics in Medicine},
  month        = {7},
  number       = {15},
  pages        = {2067-2100},
  shortjournal = {Stat. Med.},
  title        = {A new conditional performance score for the evaluation of adaptive group sequential designs with sample size recalculation},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A flexible nonlinear mixed effects model for HIV viral load
rebound after treatment interruption. <em>SIM</em>, <em>39</em>(15),
2051–2066. (<a href="https://doi.org/10.1002/sim.8529">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Characterization of HIV viral rebound after the discontinuation of antiretroviral therapy is central to HIV cure research. We propose a parametric nonlinear mixed effects model for the viral rebound trajectory, which often has a rapid rise to a peak value followed by a decrease to a viral load set point. We choose a flexible functional form that captures the shapes of viral rebound trajectories and can also provide biological insights regarding the rebound process. Each parameter can incorporate a random effect to allow for variation in parameters across individuals. Key features of viral rebound trajectories such as viral set points are represented by the parameters in the model, which facilitates assessment of intervention effects and identification of important pretreatment interruption predictors for these features. We employ a stochastic expectation-maximization (StEM) algorithm to incorporate HIV-1 RNA values that are below the lower limit of assay quantification. We evaluate the performance of our model in simulation studies and apply the proposed model to longitudinal HIV-1 viral load data from five AIDS Clinical Trials Group treatment interruption studies.},
  archive      = {J_SIM},
  author       = {Rui Wang and Ante Bing and Cathy Wang and Yuchen Hu and Ronald J. Bosch and Victor DeGruttola},
  doi          = {10.1002/sim.8529},
  journal      = {Statistics in Medicine},
  month        = {7},
  number       = {15},
  pages        = {2051-2066},
  shortjournal = {Stat. Med.},
  title        = {A flexible nonlinear mixed effects model for HIV viral load rebound after treatment interruption},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A phase i-II design based on periodic and continuous
monitoring of disease status and the times to toxicity and death.
<em>SIM</em>, <em>39</em>(15), 2035–2050. (<a
href="https://doi.org/10.1002/sim.8528">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A Bayesian phase I-II dose-finding design is presented for a clinical trial with four coprimary outcomes that reflect the actual clinical observation process. During a prespecified fixed follow-up period, the times to disease progression, toxicity, and death are monitored continuously, and an ordinal disease status variable, including progressive disease (PD) as one level, is evaluated repeatedly by scheduled imaging. We assume a proportional hazards model with piecewise constant baseline hazard for each continuous variable and a longitudinal multinomial probit model for the ordinal disease status process and include multivariate patient frailties to induce association among the outcomes. A finite partition of the nonfatal outcome combinations during the follow-up period is constructed, and the utility of each set in the partition is elicited. Posterior mean utility is used to optimize each patient&#39;s dose, subject to a safety rule excluding doses with an unacceptably high rate of PD, severe toxicity, or death. A simulation study shows that, compared with the proposed design, a simpler design based on commonly used efficacy and toxicity outcomes obtained by combining the four variables described above performs poorly and has substantially smaller probabilities of correctly choosing truly optimal doses and excluding truly unsafe doses.},
  archive      = {J_SIM},
  author       = {Juhee Lee and Peter F. Thall and Pavlos Msaouel},
  doi          = {10.1002/sim.8528},
  journal      = {Statistics in Medicine},
  month        = {7},
  number       = {15},
  pages        = {2035-2050},
  shortjournal = {Stat. Med.},
  title        = {A phase I-II design based on periodic and continuous monitoring of disease status and the times to toxicity and death},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Flexible regression approach to propensity score analysis
and its relationship with matching and weighting. <em>SIM</em>,
<em>39</em>(15), 2017–2034. (<a
href="https://doi.org/10.1002/sim.8526">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In propensity score analysis, the frequently used regression adjustment involves regressing the outcome on the estimated propensity score and treatment indicator. This approach can be highly efficient when model assumptions are valid, but can lead to biased results when the assumptions are violated. We extend the simple regression adjustment to a varying coefficient regression model that allows for nonlinear association between outcome and propensity score. We discuss its connection with some propensity score matching and weighting methods, and show that the proposed analytical framework can shed light on the intrinsic connection among some mainstream propensity score approaches (stratification, regression, kernel matching, and inverse probability weighting) and handle commonly used causal estimands. We derive analytic point and variance estimators that properly take into account the sampling variability in the estimated propensity score. Extensive simulations show that the proposed approach possesses desired finite sample properties and demonstrates competitive performance in comparison with other methods estimating the same causal estimand. The proposed methodology is illustrated with a study on right heart catheterization.},
  archive      = {J_SIM},
  author       = {Huzhang Mao and Liang Li},
  doi          = {10.1002/sim.8526},
  journal      = {Statistics in Medicine},
  month        = {7},
  number       = {15},
  pages        = {2017-2034},
  shortjournal = {Stat. Med.},
  title        = {Flexible regression approach to propensity score analysis and its relationship with matching and weighting},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Confidence, prediction, and tolerance in linear mixed
models. <em>SIM</em>, <em>39</em>(14), 2015. (<a
href="https://doi.org/10.1002/sim.8548">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_SIM},
  doi          = {10.1002/sim.8548},
  journal      = {Statistics in Medicine},
  month        = {6},
  number       = {14},
  pages        = {2015},
  shortjournal = {Stat. Med.},
  title        = {Confidence, prediction, and tolerance in linear mixed models},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Extending inferences from a randomized trial to a new target
population. <em>SIM</em>, <em>39</em>(14), 1999–2014. (<a
href="https://doi.org/10.1002/sim.8426">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {When treatment effect modifiers influence the decision to participate in a randomized trial, the average treatment effect in the population represented by the randomized individuals will differ from the effect in other populations. In this tutorial, we consider methods for extending causal inferences about time-fixed treatments from a trial to a new target population of nonparticipants, using data from a completed randomized trial and baseline covariate data from a sample from the target population. We examine methods based on modeling the expectation of the outcome, the probability of participation, or both (doubly robust). We compare the methods in a simulation study and show how they can be implemented in software. We apply the methods to a randomized trial nested within a cohort of trial-eligible patients to compare coronary artery surgery plus medical therapy versus medical therapy alone for patients with chronic coronary artery disease. We conclude by discussing issues that arise when using the methods in applied analyses.},
  archive      = {J_SIM},
  author       = {Issa J. Dahabreh and Sarah E. Robertson and Jon A. Steingrimsson and Elizabeth A. Stuart and Miguel A. Hernán},
  doi          = {10.1002/sim.8426},
  journal      = {Statistics in Medicine},
  month        = {6},
  number       = {14},
  pages        = {1999-2014},
  shortjournal = {Stat. Med.},
  title        = {Extending inferences from a randomized trial to a new target population},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Blinded sample size reestimation for negative binomial
regression with baseline adjustment. <em>SIM</em>, <em>39</em>(14),
1980–1998. (<a href="https://doi.org/10.1002/sim.8525">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In randomized clinical trials, it is standard to include baseline variables in the primary analysis as covariates, as it is recommended by international guidelines. For the study design to be consistent with the analysis, these variables should also be taken into account when calculating the sample size to appropriately power the trial. Because assumptions made in the sample size calculation are always subject to some degree of uncertainty, a blinded sample size reestimation (BSSR) is recommended to adjust the sample size when necessary. In this article, we introduce a BSSR approach for count data outcomes with baseline covariates. Count outcomes are common in clinical trials and examples include the number of exacerbations in asthma and chronic obstructive pulmonary disease, relapses, and scan lesions in multiple sclerosis and seizures in epilepsy. The introduced methods are based on Wald and likelihood ratio test statistics. The approaches are illustrated by a clinical trial in epilepsy. The BSSR procedures proposed are compared in a Monte Carlo simulation study and shown to yield power values close to the target while not inflating the type I error rate.},
  archive      = {J_SIM},
  author       = {Antonia Zapf and Thomas Asendorf and Christoph Anten and Tobias Mütze and Tim Friede},
  doi          = {10.1002/sim.8525},
  journal      = {Statistics in Medicine},
  month        = {6},
  number       = {14},
  pages        = {1980-1998},
  shortjournal = {Stat. Med.},
  title        = {Blinded sample size reestimation for negative binomial regression with baseline adjustment},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). An analytic framework for exploring sampling and observation
process biases in genome and phenome-wide association studies using
electronic health records. <em>SIM</em>, <em>39</em>(14), 1965–1979. (<a
href="https://doi.org/10.1002/sim.8524">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Large-scale association analyses based on observational health care databases such as electronic health records have been a topic of increasing interest in the scientific community. However, challenges due to nonprobability sampling and phenotype misclassification associated with the use of these data sources are often ignored in standard analyses. The extent of the bias introduced by ignoring these factors is not well-characterized. In this paper, we develop an analytic framework for characterizing the bias expected in disease-gene association studies based on electronic health records when disease status misclassification and the sampling mechanism are ignored. Through a sensitivity analysis approach, this framework can be used to obtain plausible values for parameters of interest given summary results from standard analysis. We develop an online tool for performing this sensitivity analysis. Simulations demonstrate promising properties of the proposed method. We apply our approach to study bias in disease-gene association studies using electronic health record data from the Michigan Genomics Initiative, a longitudinal biorepository effort within The University Michigan health system.},
  archive      = {J_SIM},
  author       = {Lauren J. Beesley and Lars G. Fritsche and Bhramar Mukherjee},
  doi          = {10.1002/sim.8524},
  journal      = {Statistics in Medicine},
  month        = {6},
  number       = {14},
  pages        = {1965-1979},
  shortjournal = {Stat. Med.},
  title        = {An analytic framework for exploring sampling and observation process biases in genome and phenome-wide association studies using electronic health records},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Nonparametric estimation of broad sense agreement between
ordinal and censored continuous outcomes. <em>SIM</em>, <em>39</em>(14),
1952–1964. (<a href="https://doi.org/10.1002/sim.8523">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The concept of broad sense agreement (BSA) has recently been proposed for studying the relationship between a continuous measurement and an ordinal measurement. They developed a nonparametric procedure for estimating the BSA index, which is only applicable to completely observed data. In this work, we consider the problem of evaluating BSA index when the continuous measurement is subject to censoring. We propose a nonparametric estimation method built upon a derivation of a new functional representation of the BSA index, which allows for accommodating censoring by plugging in the nonparametric survival function estimators. We establish the consistency and asymptotic normality for the proposed BSA estimator. We also investigate an alternative approach based on the strategy of multiple imputation, which is shown to have better empirical performance with small sample sizes than the plug-in method. Extensive simulation studies are conducted to evaluate our proposals. We illustrate our methods via an application to a Surgical Intensive Care Unit study.},
  archive      = {J_SIM},
  author       = {Tian Dai and Ying Guo and Limin Peng and Amita Manatunga},
  doi          = {10.1002/sim.8523},
  journal      = {Statistics in Medicine},
  month        = {6},
  number       = {14},
  pages        = {1952-1964},
  shortjournal = {Stat. Med.},
  title        = {Nonparametric estimation of broad sense agreement between ordinal and censored continuous outcomes},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Variance estimation for the kappa statistic in the presence
of clustered data and heterogeneous observations. <em>SIM</em>,
<em>39</em>(14), 1941–1951. (<a
href="https://doi.org/10.1002/sim.8522">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a methodology motivated by a controlled trial designed to validate SPOT GRADE, a novel surgical bleeding severity scale. Briefly, the study was designed to quantify inter- and intra-surgeon agreement for characterizing the severity of surgical bleeds via a Kappa statistic. Multiple surgeons were presented with a randomized sequence of controlled bleeding videos and asked to apply the rating system to characterize each wound. Each video was shown multiple times to quantify intra-surgeon reliability, creating clustered data. In addition, videos within the same category may have had different classification probabilities due to changes in blood flow rates and wound sizes. In this work, we propose a new variance estimator for the Kappa statistic, for use in clustered data as well as heterogeneity among items within the same classification category. We then apply this methodology to data from the SPOT GRADE trial.},
  archive      = {J_SIM},
  author       = {Mary M. Ryan and William D. Spotnitz and Daniel L. Gillen},
  doi          = {10.1002/sim.8522},
  journal      = {Statistics in Medicine},
  month        = {6},
  number       = {14},
  pages        = {1941-1951},
  shortjournal = {Stat. Med.},
  title        = {Variance estimation for the kappa statistic in the presence of clustered data and heterogeneous observations},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Generalized estimating equations to estimate the ordered
stereotype logit model for panel data. <em>SIM</em>, <em>39</em>(14),
1919–1940. (<a href="https://doi.org/10.1002/sim.8520">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {By modeling the effects of predictor variables as a multiplicative function of regression parameters being invariant over categories, and category-specific scalar effects, the ordered stereotype logit model is a flexible regression model for ordinal response variables. In this article, we propose a generalized estimating equations (GEE) approach to estimate the ordered stereotype logit model for panel data based on working covariance matrices, which are not required to be correctly specified. A simulation study compares the performance of GEE estimators based on various working correlation matrices and working covariance matrices using local odds ratios. Estimation of the model is illustrated using a real-world dataset. The results from the simulation study suggest that GEE estimation of this model is feasible in medium-sized and large samples and that estimators based on local odds ratios as realized in this study tend to be less efficient compared with estimators based on a working correlation matrix. For low true correlations, the efficiency gains seem to be rather small and if the working covariance structure is too flexible, the corresponding estimator may even be less efficient compared with the GEE estimator assuming independence. Like for GEE estimators more generally, if the true correlations over time are high, then a working covariance structure which is close to the true structure can lead to considerable efficiency gains compared with assuming independence.},
  archive      = {J_SIM},
  author       = {Martin Spiess and Daniel Fernández and Thuong Nguyen and Ivy Liu},
  doi          = {10.1002/sim.8520},
  journal      = {Statistics in Medicine},
  month        = {6},
  number       = {14},
  pages        = {1919-1940},
  shortjournal = {Stat. Med.},
  title        = {Generalized estimating equations to estimate the ordered stereotype logit model for panel data},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A multiparameter regression model for interval-censored
survival data. <em>SIM</em>, <em>39</em>(14), 1903–1918. (<a
href="https://doi.org/10.1002/sim.8508">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We develop flexible multiparameter regression (MPR) survival models for interval-censored survival data arising in longitudinal prospective studies and longitudinal randomised controlled clinical trials. A multiparameter Weibull regression survival model, which is wholly parametric, and has nonproportional hazards, is the main focus of the article. We describe the basic model, develop the interval-censored likelihood, and extend the model to include gamma frailty and a dispersion model. We evaluate the models by means of a simulation study and a detailed reanalysis of data from the Signal Tandmobiel study. The results demonstrate that the MPR model with frailty is computationally efficient and provides an excellent fit to the data.},
  archive      = {J_SIM},
  author       = {Defen Peng and Gilbert MacKenzie and Kevin Burke},
  doi          = {10.1002/sim.8508},
  journal      = {Statistics in Medicine},
  month        = {6},
  number       = {14},
  pages        = {1903-1918},
  shortjournal = {Stat. Med.},
  title        = {A multiparameter regression model for interval-censored survival data},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Correction. <em>SIM</em>, <em>39</em>(13), 1901–1902. (<a
href="https://doi.org/10.1002/sim.8515">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_SIM},
  author       = {Laure Wynants},
  doi          = {10.1002/sim.8515},
  journal      = {Statistics in Medicine},
  month        = {6},
  number       = {13},
  pages        = {1901-1902},
  shortjournal = {Stat. Med.},
  title        = {Correction},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Doubly robust inference procedure for relative survival
ratio in population-based cancer registry data. <em>SIM</em>,
<em>39</em>(13), 1884–1900. (<a
href="https://doi.org/10.1002/sim.8521">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cancer registry system has been playing important roles in research and policy making in cancer control. In general, information on cause of death is not available in cancer registry data. To make inference on survival of cancer patients in the absence of cause of death information, the relative survival ratio is widely used in the population-based cancer research utilizing external life tables for the general population. Another difficulty arising in analyzing cancer registry data is informative censoring. In this article, we propose a doubly robust inference procedure for the relative survival ratio under a certain type of informative censoring, called the covariate-dependent censoring. The proposed estimator is doubly robust in the sense that it is consistent if at least one of the regression models for the time-to-death and for the censoring time is correctly specified. Furthermore, we introduced a doubly robust test assessing underlying conditional independence assumption between the time-to-death and the censoring time. This test is model based, but is doubly robust in the sense that at least one of the models for the time to event and for the censoring time is correctly specified, it maintains its nominal significance level. This notable feature entails us to make inference on cancer registry data relying on assumptions, which are much weaker than the existing methods and are verifiable empirically. We examine the theoretical and empirical properties of our proposed methods by asymptotic theory and simulation studies. We illustrate the proposed method with cancer registry data in Osaka, Japan.},
  archive      = {J_SIM},
  author       = {Sho Komukai and Satoshi Hattori},
  doi          = {10.1002/sim.8521},
  journal      = {Statistics in Medicine},
  month        = {6},
  number       = {13},
  pages        = {1884-1900},
  shortjournal = {Stat. Med.},
  title        = {Doubly robust inference procedure for relative survival ratio in population-based cancer registry data},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Sample size and power calculations for open cohort
longitudinal cluster randomized trials. <em>SIM</em>, <em>39</em>(13),
1871–1883. (<a href="https://doi.org/10.1002/sim.8519">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {When calculating sample size or power for stepped wedge or other types of longitudinal cluster randomized trials, it is critical that the planned sampling structure be accurately specified. One common assumption is that participants will provide measurements in each trial period, that is, a closed cohort, and another is that each participant provides only one measurement during the course of the trial. However some studies have an “open cohort” sampling structure, where participants may provide measurements in variable numbers of periods. To date, sample size calculations for longitudinal cluster randomized trials have not accommodated open cohorts. Feldman and McKinlay (1994) provided some guidance, stating that the participant-level autocorrelation could be varied to account for the degree of overlap in different periods of the study, but did not indicate precisely how to do so. We present sample size and power formulas that allow for open cohorts and discuss the impact of the degree of “openness” on sample size and power. We consider designs where the number of participants in each cluster will be maintained throughout the trial, but individual participants may provide differing numbers of measurements. Our results are a unification of closed cohort and repeated cross-sectional sample results of Hooper et al (2016), and indicate precisely how participant autocorrelation of Feldman and McKinlay should be varied to account for an open cohort sampling structure. We discuss different types of open cohort sampling schemes and how open cohort sampling structure impacts on power in the presence of decaying within-cluster correlations and autoregressive participant-level errors.},
  archive      = {J_SIM},
  author       = {Jessica Kasza and Richard Hooper and Andrew Copas and Andrew B. Forbes},
  doi          = {10.1002/sim.8519},
  journal      = {Statistics in Medicine},
  month        = {6},
  number       = {13},
  pages        = {1871-1883},
  shortjournal = {Stat. Med.},
  title        = {Sample size and power calculations for open cohort longitudinal cluster randomized trials},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Network meta-regression for ordinal outcomes: Applications
in comparing crohn’s disease treatments. <em>SIM</em>, <em>39</em>(13),
1846–1870. (<a href="https://doi.org/10.1002/sim.8518">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Crohn&#39;s disease (CD) is a life-long condition associated with recurrent relapses characterized by abdominal pain, weight loss, anemia, and persistent diarrhea. In the US, there are approximately 780 000 CD patients and 33 000 new cases added each year. In this article, we propose a new network meta-regression approach for modeling ordinal outcomes in order to assess the efficacy of treatments for CD. Specifically, we develop regression models based on aggregate covariates for the underlying cut points of the ordinal outcomes as well as for the variances of the random effects to capture heterogeneity across trials. Our proposed models are particularly useful for indirect comparisons of multiple treatments that have not been compared head-to-head within the network meta-analysis framework. Moreover, we introduce Pearson residuals and construct an invariant test statistic to evaluate goodness-of-fit in the setting of ordinal outcome data. A detailed case study demonstrating the usefulness of the proposed methodology is carried out using aggregate ordinal outcome data from 16 clinical trials for treating CD.},
  archive      = {J_SIM},
  author       = {Yeongjin Gwon and May Mo and Ming-Hui Chen and Zhiyi Chi and Juan Li and Amy H. Xia and Joseph G. Ibrahim},
  doi          = {10.1002/sim.8518},
  journal      = {Statistics in Medicine},
  month        = {6},
  number       = {13},
  pages        = {1846-1870},
  shortjournal = {Stat. Med.},
  title        = {Network meta-regression for ordinal outcomes: Applications in comparing crohn&#39;s disease treatments},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Mitigating bias from intermittent measurement of
time-dependent covariates in failure time analysis. <em>SIM</em>,
<em>39</em>(13), 1833–1845. (<a
href="https://doi.org/10.1002/sim.8517">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cox regression models are routinely fitted to examine the association between time-dependent markers and a failure time when analyzing data from clinical registries. Typically, the marker values are measured periodically at clinic visits with the recorded value carried forward until the next assessment. We examine the asymptotic behavior of estimators from Cox regression models under this observation and data handling scheme when the true relationship is based on a Cox model using the current value of the marker. Specifically, we explore the impact of the marker process dynamics, the clinic visit intensity, and the marginal failure rate on the limiting value of the estimator of the marker effect from the Cox model. We also illustrate how a joint multistate model that accommodates intermittent observation of the time-varying marker can be formulated. Simulation studies demonstrate that the finite sample performance of the naive estimator aligns with the asymptotic results and shows good performance of the estimators from the joint model. We apply both methods to data from a study of bone markers and their effect on the development of skeletal complications in metastatic cancer.},
  archive      = {J_SIM},
  author       = {Shu Jiang and Richard J. Cook and Leilei Zeng},
  doi          = {10.1002/sim.8517},
  journal      = {Statistics in Medicine},
  month        = {6},
  number       = {13},
  pages        = {1833-1845},
  shortjournal = {Stat. Med.},
  title        = {Mitigating bias from intermittent measurement of time-dependent covariates in failure time analysis},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Spatiotemporal signal detection using continuous shrinkage
priors. <em>SIM</em>, <em>39</em>(13), 1817–1832. (<a
href="https://doi.org/10.1002/sim.8514">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Periodontal disease (PD) is a chronic inflammatory disease that affects the gum tissue and bone supporting the teeth. Although tooth-site level PD progression is believed to be spatio-temporally referenced, the whole-mouth average periodontal pocket depth (PPD) has been commonly used as an indicator of the current/active status of PD. This leads to imminent loss of information, and imprecise parameter estimates. Despite availability of statistical methods that accommodates spatiotemporal information for responses collected at the tooth-site level, the enormity of longitudinal databases derived from oral health practice-based settings render them unscalable for application. To mitigate this, we introduce a Bayesian spatiotemporal model to detect problematic/diseased tooth-sites dynamically inside the mouth for any subject obtained from large databases. This is achieved via a spatial continuous sparsity-inducing shrinkage prior on spatially varying linear-trend regression coefficients. A low-rank representation captures the nonstationary covariance structure of the PPD outcomes, and facilitates the relevant Markov chain Monte Carlo computing steps applicable to thousands of study subjects. Application of our method to both simulated data and to a rich database of electronic dental records from the HealthPartners Institute reveal improved prediction performances, compared with alternative models with usual Gaussian priors for regression parameters and conditionally autoregressive specification of the covariance structure.},
  archive      = {J_SIM},
  author       = {An-Ting Jhuang and Montserrat Fuentes and Dipankar Bandyopadhyay and Brian J. Reich},
  doi          = {10.1002/sim.8514},
  journal      = {Statistics in Medicine},
  month        = {6},
  number       = {13},
  pages        = {1817-1832},
  shortjournal = {Stat. Med.},
  title        = {Spatiotemporal signal detection using continuous shrinkage priors},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Two-part hidden markov models for semicontinuous
longitudinal data with nonignorable missing covariates. <em>SIM</em>,
<em>39</em>(13), 1801–1816. (<a
href="https://doi.org/10.1002/sim.8513">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study develops a two-part hidden Markov model (HMM) for analyzing semicontinuous longitudinal data in the presence of missing covariates. The proposed model manages a semicontinuous variable by splitting it into two random variables: a binary indicator for determining the occurrence of excess zeros at all occasions and a continuous random variable for examining its actual level. For the continuous longitudinal response, an HMM is proposed to describe the relationship between the observation and unobservable finite-state transition processes. The HMM consists of two major components. The first component is a transition model for investigating how potential covariates influence the probabilities of transitioning from one hidden state to another. The second component is a conditional regression model for examining the state-specific effects of covariates on the response. A shared random effect is introduced to each part of the model to accommodate possible unobservable heterogeneity among observation processes and the nonignorability of missing covariates. A Bayesian adaptive least absolute shrinkage and selection operator (lasso) procedure is developed to conduct simultaneous variable selection and estimation. The proposed methodology is applied to a study on the Alzheimer&#39;s Disease Neuroimaging Initiative dataset. New insights into the pathology of Alzheimer&#39;s disease and its potential risk factors are obtained.},
  archive      = {J_SIM},
  author       = {Xiaoxiao Zhou and Kai Kang and Xinyuan Song},
  doi          = {10.1002/sim.8513},
  journal      = {Statistics in Medicine},
  month        = {6},
  number       = {13},
  pages        = {1801-1816},
  shortjournal = {Stat. Med.},
  title        = {Two-part hidden markov models for semicontinuous longitudinal data with nonignorable missing covariates},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Modified models and simulations for estimating dynamic
functional connectivity in resting state functional magnetic resonance
imaging. <em>SIM</em>, <em>39</em>(12), 1781–1800. (<a
href="https://doi.org/10.1002/sim.8512">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As understanding the nature of brain networks through dynamic functional connectivity (dFC) estimation is of paramount significant, the introduction and revision of blood-oxygen-level dependent (BOLD) signal simulation methods in brain regions and dFC estimation methods have gained significant ground in recent years. Based on the observation of BOLD signals with multivariate nonnormal distribution in functional magnetic resonance imaging (fMRI) images, we first propose a copula-based method for the production of these signals, in which nonnormal data are generated with a selected time-varying covariance matrix. Therefore, we can compare the performance of models in the cases where brain signals have a multivariate nonnormal distribution. Then, two kendallized exponentially weighted moving average (KEWMA) and kendallized dynamic conditional correlation (KDCC) multivariate volatility models are introduced which are based on two well-known and commonly used exponentially weighted moving average (EMWA) and dynamic conditional correlation (DCC) models. The results show that KDCC model can estimate conditional correlation significantly far better than the former ones (ie, DCC, standardized dynamic conditional correlation, EWMA, and standardized exponentially weighted moving average) on both types of data (ie, multivariate normal and nonnormal). In the next step, the bivariate normal distribution in Iranian resting state fMRI data is confirmed by using statistical tests, and it is shown that the dynamic nature of FC is not optimally detected using prevalent methods. Two alternative Portmanteau and rank-based tests are proposed for the examination of conditional heteroscedasticity in data. Finally, dFC in these data is estimated by employing the KDCC model.},
  archive      = {J_SIM},
  author       = {Maryam Behboudi and Rahman Farnoosh},
  doi          = {10.1002/sim.8512},
  journal      = {Statistics in Medicine},
  month        = {5},
  number       = {12},
  pages        = {1781-1800},
  shortjournal = {Stat. Med.},
  title        = {Modified models and simulations for estimating dynamic functional connectivity in resting state functional magnetic resonance imaging},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A novel bayesian continuous piecewise linear log-hazard
model, with estimation and inference via reversible jump markov chain
monte carlo. <em>SIM</em>, <em>39</em>(12), 1766–1780. (<a
href="https://doi.org/10.1002/sim.8511">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a reversible jump Bayesian piecewise log-linear hazard model that extends the Bayesian piecewise exponential hazard to a continuous function of piecewise linear log hazards. A simulation study encompassing several different hazard shapes, accrual rates, censoring proportion, and sample sizes showed that the Bayesian piecewise linear log-hazard model estimated the true mean survival time and survival distributions better than the piecewsie exponential hazard. Survival data from Wake Forest Baptist Medical Center is analyzed by both methods and the posterior results are compared.},
  archive      = {J_SIM},
  author       = {Andrew G. Chapple and Taylor Peak and Ashok Hemal},
  doi          = {10.1002/sim.8511},
  journal      = {Statistics in Medicine},
  month        = {5},
  number       = {12},
  pages        = {1766-1780},
  shortjournal = {Stat. Med.},
  title        = {A novel bayesian continuous piecewise linear log-hazard model, with estimation and inference via reversible jump markov chain monte carlo},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Modelling, bayesian inference, and model assessment for
nosocomial pathogens using whole-genome-sequence data. <em>SIM</em>,
<em>39</em>(12), 1746–1765. (<a
href="https://doi.org/10.1002/sim.8510">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Whole-genome sequencing of pathogens in outbreaks of infectious disease provides the potential to reconstruct transmission pathways and enhance the information contained in conventional epidemiological data. In recent years, there have been numerous new methods and models developed to exploit such high-resolution genetic data. However, corresponding methods for model assessment have been largely overlooked. In this article, we develop both new modelling methods and new model assessment methods, specifically by building on the work of Worby et al. Although the methods are generic in nature, we focus specifically on nosocomial pathogens and analyze a dataset collected during an outbreak of MRSA in a hospital setting.},
  archive      = {J_SIM},
  author       = {Rosanna Cassidy and Theodore Kypraios and Philip D. O&#39;Neill},
  doi          = {10.1002/sim.8510},
  journal      = {Statistics in Medicine},
  month        = {5},
  number       = {12},
  pages        = {1746-1765},
  shortjournal = {Stat. Med.},
  title        = {Modelling, bayesian inference, and model assessment for nosocomial pathogens using whole-genome-sequence data},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Group sequential comparison of positive predictive value
curves for correlated biomarker data. <em>SIM</em>, <em>39</em>(12),
1732–1745. (<a href="https://doi.org/10.1002/sim.8509">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Clinical studies of predictive diagnostic tests consider the evaluation of a single test and comparison of two tests regarding their predictive accuracy of disease status. The positive predictive value (PPV) curve is used for assessing the probability of predicting the disease given a positive test result. The sequential property of one PPV curve had been studied. However, in later stages of diagnostic test development, it is more interesting to compare predictive accuracy of two tests. In this article, we propose a group sequential test for the comparison of PPV curves for paired designs when both diagnostic tests are applied to the same subject. We first derive asymptotic properties of the sequential differences of two correlated empirical PPV curves under the common case-control sampling. We then apply these results to develop a group sequential test procedure. The asymptotic results are also critical for deriving both the optimal sample size ratio and minimal required sample sizes for the proposed procedure. Our simulation studies show that the proposed sequential testing maintains the nominal type I error rate in finite samples. The proposed design is illustrated in a hypothetical lung cancer predictive trial and in a cancer diagnostic trial.},
  archive      = {J_SIM},
  author       = {Xuan Ye and Larry L. Tang and Xiaochen Zhu},
  doi          = {10.1002/sim.8509},
  journal      = {Statistics in Medicine},
  month        = {5},
  number       = {12},
  pages        = {1732-1745},
  shortjournal = {Stat. Med.},
  title        = {Group sequential comparison of positive predictive value curves for correlated biomarker data},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A bootstrap semiparametric homogeneity test for the
distributions of multigroup proportional data, with applications to
analysis of quality of life outcomes in clinical trials. <em>SIM</em>,
<em>39</em>(12), 1715–1731. (<a
href="https://doi.org/10.1002/sim.8507">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article is concerned about the test for the difference in the distributions of multigroup proportional data, which is motivated by the problem of comparing the distributions of quality of life (QoL) outcomes among different treatment groups in clinical trials. The proportional data, such as QoL outcomes assessed by answers to questions on a questionnaire, are bounded in a closed interval such as [0,1] with continuous observations in (0,1) and, in addition, excess observations taking the boundary values 0 and/or 1. Common statistical procedures used in practice, such as t - and rank-based tests, may not be very powerful since they ignore the specific feature of the proportional data. In this article, we propose a three-component mixture model for the proportional data and a density ratio model for the distributions of continuous observations in (0,1). A semiparametric test statistic for the homogeneity of distributions of multigroup proportional data is derived based on the empirical likelihood ratio principle and shown to be asymptotically distributed as a chi-squared random variable under null hypothesis. A nonparametric bootstrap procedure is proposed to further improve the performance of the semiparametric test. Simulation studies are performed to evaluate the empirical type I error and power of the proposed test procedure and compare it with likelihood ratio tests (LRTs) under parametric distribution assumptions, rank-based Kruskal-Wallis test, and Wald-type test. The proposed test procedure is also applied to the analysis of QoL outcomes from a clinical trial on colorectal cancer that motivated our study.},
  archive      = {J_SIM},
  author       = {Chunlin Wang and Dongsheng Tu},
  doi          = {10.1002/sim.8507},
  journal      = {Statistics in Medicine},
  month        = {5},
  number       = {12},
  pages        = {1715-1731},
  shortjournal = {Stat. Med.},
  title        = {A bootstrap semiparametric homogeneity test for the distributions of multigroup proportional data, with applications to analysis of quality of life outcomes in clinical trials},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A general frailty model to accommodate individual
heterogeneity in the acquisition of multiple infections: An application
to bivariate current status data. <em>SIM</em>, <em>39</em>(12),
1695–1714. (<a href="https://doi.org/10.1002/sim.8506">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The analysis of multivariate time-to-event (TTE) data can become complicated due to the presence of clustering, leading to dependence between multiple event times. For a long time, (conditional) frailty models and (marginal) copula models have been used to analyze clustered TTE data. In this article, we propose a general frailty model employing a copula function between the frailty terms to construct flexible (bivariate) frailty distributions with the application to current status data. The model has the advantage to impose a less restrictive correlation structure among latent frailty variables as compared to traditional frailty models. Specifically, our model uses a copula function to join the marginal distributions of the frailty vector. In this article, we considered different copula functions, and we relied on marginal gamma distributions due to their mathematical convenience. Based on a simulation study, our novel model outperformed the commonly used additive correlated gamma frailty model, especially in the case of a negative association between the frailties. At the end of the article, the new methodology is illustrated on real-life data applications entailing bivariate serological survey data.},
  archive      = {J_SIM},
  author       = {Thao M.P. Tran and Steven Abrams and Roel Braekers},
  doi          = {10.1002/sim.8506},
  journal      = {Statistics in Medicine},
  month        = {5},
  number       = {12},
  pages        = {1695-1714},
  shortjournal = {Stat. Med.},
  title        = {A general frailty model to accommodate individual heterogeneity in the acquisition of multiple infections: An application to bivariate current status data},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Interaction analysis under misspecification of main effects:
Some common mistakes and simple solutions. <em>SIM</em>,
<em>39</em>(11), 1675–1694. (<a
href="https://doi.org/10.1002/sim.8505">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The statistical practice of modeling interaction with two linear main effects and a product term is ubiquitous in the statistical and epidemiological literature. Most data modelers are aware that the misspecification of main effects can potentially cause severe type I error inflation in tests for interactions, leading to spurious detection of interactions. However, modeling practice has not changed. In this article, we focus on the specific situation where the main effects in the model are misspecified as linear terms and characterize its impact on common tests for statistical interaction. We then propose some simple alternatives that fix the issue of potential type I error inflation in testing interaction due to main effect misspecification. We show that when using the sandwich variance estimator for a linear regression model with a quantitative outcome and two independent factors, both the Wald and score tests asymptotically maintain the correct type I error rate. However, if the independence assumption does not hold or the outcome is binary, using the sandwich estimator does not fix the problem. We further demonstrate that flexibly modeling the main effect under a generalized additive model can largely reduce or often remove bias in the estimates and maintain the correct type I error rate for both quantitative and binary outcomes regardless of the independence assumption. We show, under the independence assumption and for a continuous outcome, overfitting and flexibly modeling the main effects does not lead to power loss asymptotically relative to a correctly specified main effect model. Our simulation study further demonstrates the empirical fact that using flexible models for the main effects does not result in a significant loss of power for testing interaction in general. Our results provide an improved understanding of the strengths and limitations for tests of interaction in the presence of main effect misspecification. Using data from a large biobank study “ The Michigan Genomics Initiative ”, we present two examples of interaction analysis in support of our results.},
  archive      = {J_SIM},
  author       = {Min Zhang and Youfei Yu and Shikun Wang and Maxwell Salvatore and Lars G. Fritsche and Zihuai He and Bhramar Mukherjee},
  doi          = {10.1002/sim.8505},
  journal      = {Statistics in Medicine},
  month        = {5},
  number       = {11},
  pages        = {1675-1694},
  shortjournal = {Stat. Med.},
  title        = {Interaction analysis under misspecification of main effects: Some common mistakes and simple solutions},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Estimating treatment effects under untestable assumptions
with nonignorable missing data. <em>SIM</em>, <em>39</em>(11),
1658–1674. (<a href="https://doi.org/10.1002/sim.8504">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nonignorable missing data poses key challenges for estimating treatment effects because the substantive model may not be identifiable without imposing further assumptions. For example, the Heckman selection model has been widely used for handling nonignorable missing data but requires the study to make correct assumptions, both about the joint distribution of the missingness and outcome and that there is a valid exclusion restriction. Recent studies have revisited how alternative selection model approaches, for example estimated by multiple imputation (MI) and maximum likelihood, relate to Heckman-type approaches in addressing the first hurdle. However, the extent to which these different selection models rely on the exclusion restriction assumption with nonignorable missing data is unclear. Motivated by an interventional study (REFLUX) with nonignorable missing outcome data in half of the sample, this article critically examines the role of the exclusion restriction in Heckman, MI, and full-likelihood selection models when addressing nonignorability. We explore the implications of the different methodological choices concerning the exclusion restriction for relative bias and root-mean-squared error in estimating treatment effects. We find that the relative performance of the methods differs in practically important ways according to the relevance and strength of the exclusion restriction. The full-likelihood approach is less sensitive to alternative assumptions about the exclusion restriction than Heckman-type models and appears an appropriate method for handling nonignorable missing data. We illustrate the implications of method choice for inference in the REFLUX study, which evaluates the effect of laparoscopic surgery on long-term quality of life for patients with gastro-oseophageal reflux disease.},
  archive      = {J_SIM},
  author       = {Manuel Gomes and Michael G. Kenward and Richard Grieve and James Carpenter},
  doi          = {10.1002/sim.8504},
  journal      = {Statistics in Medicine},
  month        = {5},
  number       = {11},
  pages        = {1658-1674},
  shortjournal = {Stat. Med.},
  title        = {Estimating treatment effects under untestable assumptions with nonignorable missing data},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Propensity scores using missingness pattern information: A
practical guide. <em>SIM</em>, <em>39</em>(11), 1641–1657. (<a
href="https://doi.org/10.1002/sim.8503">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Electronic health records are a valuable data source for investigating health-related questions, and propensity score analysis has become an increasingly popular approach to address confounding bias in such investigations. However, because electronic health records are typically routinely recorded as part of standard clinical care, there are often missing values, particularly for potential confounders. In our motivating study—using electronic health records to investigate the effect of renin-angiotensin system blockers on the risk of acute kidney injury—two key confounders, ethnicity and chronic kidney disease stage, have 59% and 53% missing data, respectively. The missingness pattern approach (MPA), a variant of the missing indicator approach, has been proposed as a method for handling partially observed confounders in propensity score analysis. In the MPA, propensity scores are estimated separately for each missingness pattern present in the data. Although the assumptions underlying the validity of the MPA are stated in the literature, it can be difficult in practice to assess their plausibility. In this article, we explore the MPA&#39;s underlying assumptions by using causal diagrams to assess their plausibility in a range of simple scenarios, drawing general conclusions about situations in which they are likely to be violated. We present a framework providing practical guidance for assessing whether the MPA&#39;s assumptions are plausible in a particular setting and thus deciding when the MPA is appropriate. We apply our framework to our motivating study, showing that the MPA&#39;s underlying assumptions appear reasonable, and we demonstrate the application of MPA to this study.},
  archive      = {J_SIM},
  author       = {Helen A. Blake and Clémence Leyrat and Kathryn E. Mansfield and Shaun Seaman and Laurie A. Tomlinson and James Carpenter and Elizabeth J. Williamson},
  doi          = {10.1002/sim.8503},
  journal      = {Statistics in Medicine},
  month        = {5},
  number       = {11},
  pages        = {1641-1657},
  shortjournal = {Stat. Med.},
  title        = {Propensity scores using missingness pattern information: A practical guide},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Variance estimation when using propensity-score matching
with replacement with survival or time-to-event outcomes. <em>SIM</em>,
<em>39</em>(11), 1623–1640. (<a
href="https://doi.org/10.1002/sim.8502">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Propensity-score matching is a popular analytic method to estimate the effects of treatments when using observational data. Matching on the propensity score typically requires a pool of potential controls that is larger than the number of treated or exposed subjects. The most common approach to matching on the propensity score is matching without replacement, in which each control subject is matched to at most one treated subject. Failure to find a matched control for each treated subject can lead to “bias due to incomplete matching.” To avoid this bias, it is important to identify a matched control subject for each treated subject. An alternative to matching without replacement is matching with replacement, in which control subjects are allowed to be matched to multiple treated subjects. A limitation to the use of matching with replacement is that variance estimation must account for both the matched nature of the sample and for some control subjects being included in multiple matched sets. While a variance estimator has been proposed for when outcomes are continuous, no such estimator has been proposed for use with time-to-event outcomes, which are common in medical and epidemiological research. We propose a variance estimator for the hazard ratio when matching with replacement. We conducted a series of Monte Carlo simulations to examine the performance of this estimator. We illustrate the utility of matching with replacement to estimate the effect of smoking cessation counseling on survival in smokers discharged from hospital with a heart attack.},
  archive      = {J_SIM},
  author       = {Peter C. Austin and Guy Cafri},
  doi          = {10.1002/sim.8502},
  journal      = {Statistics in Medicine},
  month        = {5},
  number       = {11},
  pages        = {1623-1640},
  shortjournal = {Stat. Med.},
  title        = {Variance estimation when using propensity-score matching with replacement with survival or time-to-event outcomes},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). The impact of population mobility on estimates of
environmental exposure effects in a case-control study. <em>SIM</em>,
<em>39</em>(11), 1610–1622. (<a
href="https://doi.org/10.1002/sim.8501">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In many studies of environmental risk factors for disease, researchers use the location at diagnosis as a geographic reference for environmental exposures. However, many environmental pollutants change continuously over space and time. The dynamic characteristics of these pollutants coupled with population mobility in the United States suggest that for diseases with long latencies like cancer, historic exposures may be more relevant than exposure at the time of diagnosis. In this article, we evaluated to what extent the commonly used assumption of no population mobility results in increased bias in the estimates of the relationship between environmental exposures and long-latency health outcomes disease in a case-control study. We conducted a simulation study using the residential histories of a random sample from the National Institutes of Health-AARP (formerly American Association of Retired Persons) Diet and Health Study. We simulated case-control status based on subject exposure and true exposure effects that varied temporally. We compared estimates from models using only subject location at diagnosis to estimates where subjects were assumed to be mobile. Ignoring population mobility resulted in underestimates of subject exposure, with largest deviations observed at time points further away from study enrollment. In general, the effect of population mobility on the bias of the estimates of the relationship between the exposure and the outcome was more prominent with exposures that showed substantial spatial and temporal variability. Based on our results, we recommend using residential histories when environmental exposures and disease latencies span a long enough time period that mobility is important.},
  archive      = {J_SIM},
  author       = {Anny-Claude Joseph and Montserrat Fuentes and David C. Wheeler},
  doi          = {10.1002/sim.8501},
  journal      = {Statistics in Medicine},
  month        = {5},
  number       = {11},
  pages        = {1610-1622},
  shortjournal = {Stat. Med.},
  title        = {The impact of population mobility on estimates of environmental exposure effects in a case-control study},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Estimation of treatment effects following a sequential trial
of multiple treatments. <em>SIM</em>, <em>39</em>(11), 1593–1609. (<a
href="https://doi.org/10.1002/sim.8497">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {When a clinical trial is subject to a series of interim analyses as a result of which the study may be terminated or modified, final frequentist analyses need to take account of the design used. Failure to do so may result in overstated levels of significance, biased effect estimates and confidence intervals with inadequate coverage probabilities. A wide variety of valid methods of frequentist analysis have been devised for sequential designs comparing a single experimental treatment with a single control treatment. It is less clear how to perform the final analysis of a sequential or adaptive design applied in a more complex setting, for example, to determine which treatment or set of treatments amongst several candidates should be recommended. This article has been motivated by consideration of a trial in which four treatments for sepsis are to be compared, with interim analyses allowing the dropping of treatments or termination of the trial to declare a single winner or to conclude that there is little difference between the treatments that remain. The approach taken is based on the method of Rao-Blackwellization which enhances the accuracy of unbiased estimates available from the first interim analysis by taking their conditional expectations given final sufficient statistics. Analytic approaches to determine such expectations are difficult and specific to the details of the design: instead “reverse simulations” are conducted to construct replicate realizations of the first interim analysis from the final test statistics. The method also provides approximate confidence intervals for the differences between treatments.},
  archive      = {J_SIM},
  author       = {John Whitehead and Yasin Desai and Thomas Jaki},
  doi          = {10.1002/sim.8497},
  journal      = {Statistics in Medicine},
  month        = {5},
  number       = {11},
  pages        = {1593-1609},
  shortjournal = {Stat. Med.},
  title        = {Estimation of treatment effects following a sequential trial of multiple treatments},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Correction to walter SD, turner RM, macaskill p. Optimising
the two-stage randomised trial design when some participants are
indifferent in their treatment preferences (2019). Statistics in
medicine 38, 2317-2331. <em>SIM</em>, <em>39</em>(10), 1591. (<a
href="https://doi.org/10.1002/sim.8487">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_SIM},
  author       = {Stephen D. Walter},
  doi          = {10.1002/sim.8487},
  journal      = {Statistics in Medicine},
  month        = {5},
  number       = {10},
  pages        = {1591},
  shortjournal = {Stat. Med.},
  title        = {Correction to walter SD, turner RM, macaskill p. optimising the two-stage randomised trial design when some participants are indifferent in their treatment preferences (2019). statistics in medicine 38, 2317-2331},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A unified approach for synthesizing population-level
covariate effect information in semiparametric estimation with survival
data. <em>SIM</em>, <em>39</em>(10), 1573–1590. (<a
href="https://doi.org/10.1002/sim.8499">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {There has been a growing interest in developing methodologies to combine information from public domains to improve efficiency in the analysis of relatively small-scale studies that collect more detailed patient-level information. The auxiliary information is usually given in the form of summary statistics or regression coefficients. Thus, the question arises as to how to incorporate the summary information in the model estimation procedure. In this article, we consider statistical analysis of right-censored survival data when additional information about the covariate effects evaluated in a reduced Cox model is available. Recognizing that such external information can be summarized using population moments, we present a unified framework by employing the generalized method of moments to combine information from different sources for the analysis of survival data. The proposed estimator can be shown to be consistent and asymptotically normal; moreover, it is more efficient than the maximum partial likelihood estimator. We also consider incorporating uncertainty of the external information in the inference procedure. Simulation studies show that, by incorporating the additional summary information, the proposed estimators enjoy a substantial gain in efficiency over the conventional approach. A data analysis of a pancreatic cancer cohort study is presented to illustrate the methods and theory.},
  archive      = {J_SIM},
  author       = {Chiung-Yu Huang and Jing Qin},
  doi          = {10.1002/sim.8499},
  journal      = {Statistics in Medicine},
  month        = {5},
  number       = {10},
  pages        = {1573-1590},
  shortjournal = {Stat. Med.},
  title        = {A unified approach for synthesizing population-level covariate effect information in semiparametric estimation with survival data},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Adjusted score functions for monotone likelihood in the cox
regression model. <em>SIM</em>, <em>39</em>(10), 1558–1572. (<a
href="https://doi.org/10.1002/sim.8496">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Standard inference procedures for the Cox model involve maximizing the partial likelihood function. Monotone partial likelihood is an issue that frequently happens in the analysis of health science studies. Monotone likelihood mainly occurs in samples with substantial censoring of survival times and is associated with categorical covariates. In particular, and more frequently, it usually happens when one level of a categorical covariate has just experienced censoring times. In order to overcome this problem, Heinze and Schemper proposed an adjusted partial likelihood score function obtained by suitably adapting the general approach of Firth for mean bias reduction. The procedure is effective in preventing infinite estimates. As an alternative solution, we propose an approach based on the adjusted score function recently suggested by Kenne Pagui et al for median bias reduction. This procedure also solves the infinite estimate problem and has an additional advantage of being invariant under componentwise reparameterizations. This latter fact is fundamental under Cox model since hazards ratio interpretation is obtained by exponentiating parameter estimates. Numerical studies of the proposed method suggest better inference properties than those of the mean bias reduction. A real-data application related to a melanoma skin dataset is used as illustration for a comparison basis of the methods.},
  archive      = {J_SIM},
  author       = {Euloge C. Kenne Pagui and Enrico A. Colosimo},
  doi          = {10.1002/sim.8496},
  journal      = {Statistics in Medicine},
  month        = {5},
  number       = {10},
  pages        = {1558-1572},
  shortjournal = {Stat. Med.},
  title        = {Adjusted score functions for monotone likelihood in the cox regression model},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A hierarchical testing approach for detecting safety signals
in clinical trials. <em>SIM</em>, <em>39</em>(10), 1541–1557. (<a
href="https://doi.org/10.1002/sim.8495">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Detecting safety signals in clinical trial safety data is known to be challenging due to high dimensionality, rare occurrence, weak signal, and complex dependence. We propose a new hierarchical testing approach for analyzing safety data from a typical randomized clinical trial. This approach accounts for the hierarchical structure of adverse events (AEs), that is, AEs are categorized by system organ class (SOC). Our approach contains two steps: the first step tests, for each SOC, whether any AEs within this SOC are differently distributed between treatment arms; and the second step identifies signal AEs from SOCs passing the first step tests. We show the superiority, in terms of power of detecting safety signals given controlled false discovery rate, of the new approach comparing with currently available approaches through simulation studies. We also demonstrate this approach with two real data examples.},
  archive      = {J_SIM},
  author       = {Xianming Tan and Bingshu E. Chen and Jianping Sun and Tejendra Patel and Joseph G. Ibrahim},
  doi          = {10.1002/sim.8495},
  journal      = {Statistics in Medicine},
  month        = {5},
  number       = {10},
  pages        = {1541-1557},
  shortjournal = {Stat. Med.},
  title        = {A hierarchical testing approach for detecting safety signals in clinical trials},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Adding noise to markov cohort state-transition model in
decision modeling and cost-effectiveness analysis. <em>SIM</em>,
<em>39</em>(10), 1529–1540. (<a
href="https://doi.org/10.1002/sim.8494">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Following its introduction over 30 years ago, the Markov cohort state-transition model has been used extensively to model population trajectories over time in health decision modeling and cost-effectiveness analysis studies. We recently showed that a cohort model represents the average of a continuous-time stochastic process on a multidimensional integer lattice governed by a master equation, which represents the time-evolution of the probability function of an integer-valued random vector. By leveraging this theoretical connection, this study introduces an alternative modeling method using a stochastic differential equation (SDE) approach, which captures not only the mean behavior but also the variance of the population process. We show the derivation of an SDE model from first principles, describe an algorithm to construct an SDE and solve the SDE via simulation for use in practice, and demonstrate the two applications of an SDE in detail. The first example demonstrates that the population trajectories, and their mean and variance, from the SDE and other commonly used methods in decision modeling match. The second example shows that users can readily apply the SDE method in their existing works without the need for additional inputs beyond those required for constructing a conventional cohort model. In addition, the second example demonstrates that the SDE model is superior to a microsimulation model in terms of computational speed. In summary, an SDE model provides an alternative modeling framework which includes information on variance, can accommodate for time-varying parameters, and is computationally less expensive than a microsimulation for a typical cohort modeling problem.},
  archive      = {J_SIM},
  author       = {Rowan Iskandar},
  doi          = {10.1002/sim.8494},
  journal      = {Statistics in Medicine},
  month        = {5},
  number       = {10},
  pages        = {1529-1540},
  shortjournal = {Stat. Med.},
  title        = {Adding noise to markov cohort state-transition model in decision modeling and cost-effectiveness analysis},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Small area estimation of receiver operating characteristic
curves for ordinal data under stochastic ordering. <em>SIM</em>,
<em>39</em>(10), 1514–1528. (<a
href="https://doi.org/10.1002/sim.8493">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {There has been a recent increase in the diagnosis of diseases through radiographic images such as x-rays, magnetic resonance imaging, and computed tomography. The outcome of a radiological diagnostic test is often in the form of discrete ordinal data, and we usually summarize the performance of the diagnostic test using the receiver operating characteristic (ROC) curve and the area under the curve (AUC). The ROC curve will be concave and called proper when the outcomes of the diagnostic test in the actually positive subjects are higher than in the actually negative subjects. The diagnostic test for disease detection is clinically useful when a ROC curve is proper. In this study, we develop a hierarchical Bayesian model to estimate the proper ROC curve and AUC using stochastic ordering in several domains when the outcome of the diagnostic test is discrete ordinal data and compare it with the model without stochastic ordering. The model without stochastic ordering can estimate the improper ROC curve with a nonconcave shape or a hook when the true ROC curve of the population is a proper ROC curve. Therefore, the model with stochastic ordering is preferable over the model without stochastic ordering to estimate the proper ROC curve with clinical usefulness for ordinal data.},
  archive      = {J_SIM},
  author       = {Eun Jin Jang and Balgobin Nandram and Yousun Ko and Dal Ho Kim},
  doi          = {10.1002/sim.8493},
  journal      = {Statistics in Medicine},
  month        = {5},
  number       = {10},
  pages        = {1514-1528},
  shortjournal = {Stat. Med.},
  title        = {Small area estimation of receiver operating characteristic curves for ordinal data under stochastic ordering},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Sample size estimation for stratified individual and cluster
randomized trials with binary outcomes. <em>SIM</em>, <em>39</em>(10),
1489–1513. (<a href="https://doi.org/10.1002/sim.8492">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Individual randomized trials (IRTs) and cluster randomized trials (CRTs) with binary outcomes arise in a variety of settings and are often analyzed by logistic regression (fitted using generalized estimating equations for CRTs). The effect of stratification on the required sample size is less well understood for trials with binary outcomes than for continuous outcomes. We propose easy-to-use methods for sample size estimation for stratified IRTs and CRTs and demonstrate the use of these methods for a tuberculosis prevention CRT currently being planned. For both IRTs and CRTs, we also identify the ratio of the sample size for a stratified trial vs a comparably powered unstratified trial, allowing investigators to evaluate how stratification will affect the required sample size when planning a trial. For CRTs, these can be used when the investigator has estimates of the within-stratum intracluster correlation coefficients (ICCs) or by assuming a common within-stratum ICC. Using these methods, we describe scenarios where stratification may have a practically important impact on the required sample size. We find that in the two-stratum case, for both IRTs and for CRTs with very small cluster sizes, there are unlikely to be plausible scenarios in which an important sample size reduction is achieved when the overall probability of a subject experiencing the event of interest is low. When the probability of events is not small, or when cluster sizes are large, however, there are scenarios where practically important reductions in sample size result from stratification.},
  archive      = {J_SIM},
  author       = {Lee Kennedy-Shaffer and Michael D. Hughes},
  doi          = {10.1002/sim.8492},
  journal      = {Statistics in Medicine},
  month        = {5},
  number       = {10},
  pages        = {1489-1513},
  shortjournal = {Stat. Med.},
  title        = {Sample size estimation for stratified individual and cluster randomized trials with binary outcomes},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Testing for change-point in the covariate effects based on
the cox regression model. <em>SIM</em>, <em>39</em>(10), 1473–1488. (<a
href="https://doi.org/10.1002/sim.8491">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Models with change-point in covariates have wide applications in cancer research with the response being the time to a certain event. A Cox model with change-point in covariate is considered at which the pattern of the change-point effects can be flexibly specified. To test for the existence of the change-point effects, three statistical tests, namely, the maximal score, maximal normalized score, and maximal Wald tests are proposed. The asymptotic properties of the test statistics are established. Monte Carlo approaches to simulate the critical values are suggested. A large-scale simulation study is carried out to study the finite sample performance of the proposed test statistics under the null hypothesis of no change-points and various alternative hypothesis settings. Each of the proposed methods provides a natural estimate for the location of the change-point, but it is found that the performance of the maximal score test can be sensitive to the true location of the change-point in some cases, while the performance of the maximal Wald test is very satisfactory in general even in cases with moderate sample size. For illustration, the proposed methods are applied to two medical datasets concerning patients with primary biliary cirrhosis and breast cancer, respectively.},
  archive      = {J_SIM},
  author       = {Chun Yin Lee and Xuerong Chen and Kwok Fai Lam},
  doi          = {10.1002/sim.8491},
  journal      = {Statistics in Medicine},
  month        = {5},
  number       = {10},
  pages        = {1473-1488},
  shortjournal = {Stat. Med.},
  title        = {Testing for change-point in the covariate effects based on the cox regression model},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A super-combo-drug test to detect adverse drug events and
drug interactions from electronic health records in the era of
polypharmacy. <em>SIM</em>, <em>39</em>(10), 1458–1472. (<a
href="https://doi.org/10.1002/sim.8490">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Pharmacoinformatics research has experienced a great deal of successes in detecting drug-induced adverse events (AEs) using large-scale health record databases. In the era of polypharmacy, pharmacoinformatics faces many new challenges, and two significant challenges are to detect high-order drug interactions and to handle strongly correlated drugs. In this article, we propose a super-combo-drug test (SupCD-T) to address the aforementioned two challenges. SupCD-T detects drug interactions by identifying optimal drug combinations with increased AE risks. In addition, SupCD-T increases the statistical powers to detect single-drug effects by combining strongly correlated drugs. Although SupCD-T does not distinguish single-drug effects from their combination effects, it is noticeably more powerful in selecting an individual drug effect in the multiple regression analysis, where confounding justification between two correlated drugs reduces the power in testing the individual drug effects on AEs. Our simulation studies demonstrate that SupCD-T has generally better power comparing with the multiple regression analysis. In addition, SupCD-T is able to select meaningful drug combinations (eg, highly coprescribed drugs). Using electronic health record database, we illustrate the utility of SupCD-T and discover a number of drug combinations that have increased risk in myopathy. Some novel drug combinations have not yet been investigated and reported in the pharmacology research.},
  archive      = {J_SIM},
  author       = {Anqi Zhu and Donglin Zeng and Li Shen and Xia Ning and Lang Li and Pengyue Zhang},
  doi          = {10.1002/sim.8490},
  journal      = {Statistics in Medicine},
  month        = {5},
  number       = {10},
  pages        = {1458-1472},
  shortjournal = {Stat. Med.},
  title        = {A super-combo-drug test to detect adverse drug events and drug interactions from electronic health records in the era of polypharmacy},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). On the aggregation of published prognostic scores for causal
inference in observational studies. <em>SIM</em>, <em>39</em>(10),
1440–1457. (<a href="https://doi.org/10.1002/sim.8489">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As real world evidence on drug efficacy involves nonrandomized studies, statistical methods adjusting for confounding are needed. In this context, prognostic score (PGS) analysis has recently been proposed as a method for causal inference. It aims to restore balance across the different treatment groups by identifying subjects with a similar prognosis for a given reference exposure (“control”). This requires the development of a multivariable prognostic model in the control arm of the study sample, which is then extrapolated to the different treatment arms. Unfortunately, large cohorts for developing prognostic models are not always available. Prognostic models are therefore subject to a dilemma between overfitting and parsimony; the latter being prone to a violation of the assumption of no unmeasured confounders when important covariates are ignored. Although it is possible to limit overfitting by using penalization strategies, an alternative approach is to adopt evidence synthesis. Aggregating previously published prognostic models may improve the generalizability of PGS, while taking account of a large set of covariates—even when limited individual participant data are available. In this article, we extend a method for prediction model aggregation to PGS analysis in nonrandomized studies. We conduct extensive simulations to assess the validity of model aggregation, compared with other methods of PGS analysis for estimating marginal treatment effects. We show that aggregating existing PGS into a “meta-score” is robust to misspecification, even when elementary scores wrongfully omit confounders or focus on different outcomes. We illustrate our methods in a setting of treatments for asthma.},
  archive      = {J_SIM},
  author       = {Tri-Long Nguyen and Gary S. Collins and Fabio Pellegrini and Karel G.M. Moons and Thomas P.A. Debray},
  doi          = {10.1002/sim.8489},
  journal      = {Statistics in Medicine},
  month        = {5},
  number       = {10},
  pages        = {1440-1457},
  shortjournal = {Stat. Med.},
  title        = {On the aggregation of published prognostic scores for causal inference in observational studies},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Analysis of counts for cluster randomized trials: Negative
controls and test-negative designs. <em>SIM</em>, <em>39</em>(10),
1429–1439. (<a href="https://doi.org/10.1002/sim.8488">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In cluster randomized trials (CRTs), the outcome of interest is often a count at the cluster level. This occurs, for example, in evaluating an intervention with the outcome being the number of infections of a disease such as HIV or dengue or the number of hospitalizations in the cluster. Standard practice analyzes these counts through cluster outcome rates using an appropriate denominator (eg, population size). However, such denominators are sometimes unknown, particularly when the counts depend on a passive community surveillance system. We consider direct comparison of the counts without knowledge of denominators, relying on randomization to balance denominators. We also focus on permutation tests to allow for small numbers of randomized clusters. However, such approaches are subject to bias when there is differential ascertainment of counts across arms, a situation that may occur in CRTs that cannot implement blinded interventions. We suggest the use of negative control counts as a method to remove, or reduce, this bias, discussing the key properties necessary for an effective negative control. A current example of such a design is the recent extension of test-negative designs to CRTs testing community-level interventions. Via simulation, we compare the performance of new and standard estimators based on CRTs with negative controls to approaches that only use the original counts. When there is no differential ascertainment by intervention arm, the count-only approaches perform comparably to those using debiasing negative controls. However, under even modest differential ascertainment, the count-only estimators are no longer reliable.},
  archive      = {J_SIM},
  author       = {Suzanne M. Dufault and Nicholas P. Jewell},
  doi          = {10.1002/sim.8488},
  journal      = {Statistics in Medicine},
  month        = {5},
  number       = {10},
  pages        = {1429-1439},
  shortjournal = {Stat. Med.},
  title        = {Analysis of counts for cluster randomized trials: Negative controls and test-negative designs},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A mediation analysis for a nonrare dichotomous outcome with
sequentially ordered multiple mediators. <em>SIM</em>, <em>39</em>(10),
1415–1428. (<a href="https://doi.org/10.1002/sim.8485">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mediation analyses can help us to understand the biological mechanism in which an exposure or treatment affects an outcome. Single mediator analyses have been used in various applications, but may not be appropriate for analyzing intricate mechanisms involving multiple mediators that affect each other. Thus, in this article, we studied multiple sequentially ordered mediators for a dichotomous outcome and presented the identifiability assumptions for the path-specific effects on the outcome, that is, the effect of an exposure on the outcome mediated by a specific set of mediators. We proposed a closed-form estimator for the path-specific effects by modeling the dichotomous outcome using a probit model. Asymptotic variance of the proposed estimator is derived and can be approximated via delta method or bootstrapping. Simulations under a finite sample showed the validity of our method in capturing the path-specific effects when the probability of each potential counterfactual outcome is not small and demonstrated the utility of a computationally efficient alternative to bootstrapping for calculating variance. The method is applied to investigate the effects of polycystic ovarian syndrome on live birth rates mediated by estradiol levels and the number of oocytes retrieved in a large electronic in vitro fertilization database. We implemented the method into an R package SOMM , which is available at https://github.com/roqe/SOMM .},
  archive      = {J_SIM},
  author       = {En-Yu Lai and Stephannie Shih and Yen-Tsung Huang and Shunping Wang},
  doi          = {10.1002/sim.8485},
  journal      = {Statistics in Medicine},
  month        = {5},
  number       = {10},
  pages        = {1415-1428},
  shortjournal = {Stat. Med.},
  title        = {A mediation analysis for a nonrare dichotomous outcome with sequentially ordered multiple mediators},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A primer on strong vs weak control of familywise error rate.
<em>SIM</em>, <em>39</em>(9), 1407–1413. (<a
href="https://doi.org/10.1002/sim.8463">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multiple comparison adjustments have a long history, yet confusion remains about which procedures control type 1 error rate in a strong sense and how to show this. Part of the confusion stems from a powerful technique called the closed testing principle, whose statement is deceptively simple, but is sometimes misinterpreted. This primer presents a straightforward way to think about multiplicity adjustment.},
  archive      = {J_SIM},
  author       = {Michael A. Proschan and Erica H. Brittain},
  doi          = {10.1002/sim.8463},
  journal      = {Statistics in Medicine},
  month        = {4},
  number       = {9},
  pages        = {1407-1413},
  shortjournal = {Stat. Med.},
  title        = {A primer on strong vs weak control of familywise error rate},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Comparing a multivariate response bayesian random effects
logistic regression model with a latent variable item response theory
model for provider profiling on multiple binary indicators
simultaneously. <em>SIM</em>, <em>39</em>(9), 1390–1406. (<a
href="https://doi.org/10.1002/sim.8484">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Provider profiling entails comparing the performance of hospitals on indicators of quality of care. Many common indicators of healthcare quality are binary (eg, short-term mortality, use of appropriate medications). Typically, provider profiling examines the variation in each indicator in isolation across hospitals. We developed Bayesian multivariate response random effects logistic regression models that allow one to simultaneously examine variation and covariation in multiple binary indicators across hospitals. Use of this model allows for (i) determining the probability that a hospital has poor performance on a single indicator; (ii) determining the probability that a hospital has poor performance on multiple indicators simultaneously; (iii) determining, by using the Mahalanobis distance, how far the performance of a given hospital is from that of an average hospital. We illustrate the utility of the method by applying it to 10 881 patients hospitalized with acute myocardial infarction at 102 hospitals. We considered six binary patient-level indicators of quality of care: use of reperfusion, assessment of left ventricular ejection fraction, measurement of cardiac troponins, use of acetylsalicylic acid within 6 hours of hospital arrival, use of beta-blockers within 12 hours of hospital arrival, and survival to 30 days after hospital admission. When considering the five measures evaluating processes of care, we found that there was a strong correlation between a hospital&#39;s performance on one indicator and its performance on a second indicator for five of the 10 possible comparisons. We compared inferences made using this approach with those obtained using a latent variable item response theory model.},
  archive      = {J_SIM},
  author       = {Peter C. Austin and Douglas S. Lee and George Leckie},
  doi          = {10.1002/sim.8484},
  journal      = {Statistics in Medicine},
  month        = {4},
  number       = {9},
  pages        = {1390-1406},
  shortjournal = {Stat. Med.},
  title        = {Comparing a multivariate response bayesian random effects logistic regression model with a latent variable item response theory model for provider profiling on multiple binary indicators simultaneously},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Profiling dialysis facilities for adverse recurrent events.
<em>SIM</em>, <em>39</em>(9), 1374–1389. (<a
href="https://doi.org/10.1002/sim.8482">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Profiling analysis aims to evaluate health care providers, such as hospitals, nursing homes, or dialysis facilities, with respect to a patient outcome. Previous profiling methods have considered binary outcomes, such as 30-day hospital readmission or mortality. For the unique population of dialysis patients, regular blood works are required to evaluate effectiveness of treatment and avoid adverse events, including dialysis inadequacy, imbalance mineral levels, and anemia among others. For example, anemic events (when hemoglobin levels exceed normative range) are recurrent and common for patients on dialysis. Thus, we propose high-dimensional Poisson and negative binomial regression models for rate/count outcomes and introduce a standardized event ratio measure to compare the event rate at a specific facility relative to a chosen normative standard, typically defined as an “average” national rate across all facilities. Our proposed estimation and inference procedures overcome the challenge of high-dimensional parameters for thousands of dialysis facilities. Also, we investigate how overdispersion affects inference in the context of profiling analysis. The proposed methods are illustrated with profiling dialysis facilities for recurrent anemia events.},
  archive      = {J_SIM},
  author       = {Jason P. Estes and Yanjun Chen and Damla Şentürk and Connie M. Rhee and Esra Kürüm and Amy S. You and Elani Streja and Kamyar Kalantar-Zadeh and Danh V. Nguyen},
  doi          = {10.1002/sim.8482},
  journal      = {Statistics in Medicine},
  month        = {4},
  number       = {9},
  pages        = {1374-1389},
  shortjournal = {Stat. Med.},
  title        = {Profiling dialysis facilities for adverse recurrent events},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A threshold-free summary index for quantifying the capacity
of covariates to yield efficient treatment rules. <em>SIM</em>,
<em>39</em>(9), 1362–1373. (<a
href="https://doi.org/10.1002/sim.8481">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {When data on treatment assignment, outcomes, and covariates from a randomized trial are available, a question of interest is to what extent covariates can be used to optimize treatment decisions. Statistical hypothesis testing of covariate-by-treatment interaction is ill-suited for this purpose. The application of decision theory results in treatment rules that compare the expected benefit of treatment given the patient&#39;s covariates against a treatment threshold. However, determining treatment threshold is often context-specific, and any given threshold might seem arbitrary when the overall capacity towards predicting treatment benefit is of concern. We propose the Concentration of Benefit index ( C b ), a threshold-free metric that quantifies the combined performance of covariates towards finding individuals who will benefit the most from treatment. The construct of the proposed index is comparing expected treatment outcomes with and without knowledge of covariates when one of a two randomly selected patients is to be treated. We show that the resulting index can also be expressed in terms of the integrated efficiency of individualized treatment decision over the entire range of treatment thresholds. We propose parametric and semiparametric estimators, the latter being suitable for out-of-sample validation and correction for optimism. We used data from a clinical trial to demonstrate the calculations in a step-by-step fashion. The proposed index has intuitive and theoretically sound interpretation and can be estimated with relative ease for a wide class of regression models. Beyond the conceptual developments, various aspects of estimation and inference for such a metric need to be pursued in future research. R code that implements the method for a variety of regression models is provided at ( https://github.com/msadatsafavi/txBenefit ).},
  archive      = {J_SIM},
  author       = {Mohsen Sadatsafavi and Mohammad Ali Mansournia and Paul Gustafson},
  doi          = {10.1002/sim.8481},
  journal      = {Statistics in Medicine},
  month        = {4},
  number       = {9},
  pages        = {1362-1373},
  shortjournal = {Stat. Med.},
  title        = {A threshold-free summary index for quantifying the capacity of covariates to yield efficient treatment rules},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Ordinal outcomes: A cumulative probability model with the
log link and an assumption of proportionality. <em>SIM</em>,
<em>39</em>(9), 1343–1361. (<a
href="https://doi.org/10.1002/sim.8479">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present here a study of ordinal outcomes with a cumulative probability model. In particular, we consider the log link with the assumption of proportionality. The logit link is currently the most widely used link with ordinal outcomes in the health research literature. With the logit link, one obtains regression coefficients that are functions of odds. When the log link is used, one obtains regression coefficients that are functions of probabilities. While odds might be preferred with certain studies that are retrospective, many health researchers may prefer to have direct statements about probabilities. There are two classes of models with the log link analogous to those with the logit link. We will call these two classes the Proportional Probability Model (PPM) and the Log Cumulative Probability Model (LCPM). These models introduce a challenge not seen with the logit link models. The log link models have constraints on the parameter space. We must insist that the maximum likelihood estimate (MLE) satisfy these constraints. We present conditions for the uniqueness of the MLE and we present other features of the MLE. Several examples and several closed form expressions for the MLE are presented. While this paper is primarily about the PPM, our R package lcpm contains functions to fit both the PPM and the LCPM.},
  archive      = {J_SIM},
  author       = {Gurbakhshash Singh and Gordon Hilton Fick},
  doi          = {10.1002/sim.8479},
  journal      = {Statistics in Medicine},
  month        = {4},
  number       = {9},
  pages        = {1343-1361},
  shortjournal = {Stat. Med.},
  title        = {Ordinal outcomes: A cumulative probability model with the log link and an assumption of proportionality},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A fully bayesian mixture model approach for identifying
noncompliance in a regulatory tobacco clinical trial. <em>SIM</em>,
<em>39</em>(9), 1328–1342. (<a
href="https://doi.org/10.1002/sim.8478">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Identifying noncompliance in a randomized trial is challenging, but could be improved by leveraging biomarker data to identify participants that did not comply with their assigned treatment. For randomized trials of very low nicotine content (VLNC) cigarettes, the biomarker of total nicotine equivalents (TNE) could be used to identify noncompliance. Compliant participants should have lower levels of TNEs than participants that did not comply and smoked normal nicotine content cigarettes, resulting in a mixture of compliant and noncompliant participants at each dose level. Thresholds of TNE could then be identified from the compliant groups at each dose level and used to determine which study participants were compliant. Furthermore, proposed biological relationships of TNE with nicotine dose could be incorporated into improve the efficiency of estimation, but may introduce bias if misspecified. To account for multiple modeling assumptions across dose levels, we explore model averaging via reversible jump markov chain monte carlo (MCMC) within each dose level to take advantage of improvements in efficiency when the proposed relationship is true and to downweight the biological model when it is misspecified. In simulation studies, we demonstrate that model averaging in the presence of a correct biological relationship results in a decrease in the mean square error (MSE) of up to 85%, but downweights the model in dose levels where the relationship is not appropriate. We apply our approach to data from a randomized trial of VLNC cigarettes to estimate TNE thresholds and probability of compliance curves as a function of TNEs for each nicotine dose used in the trial.},
  archive      = {J_SIM},
  author       = {Alexander M. Kaizer and Joseph S. Koopmeiners},
  doi          = {10.1002/sim.8478},
  journal      = {Statistics in Medicine},
  month        = {4},
  number       = {9},
  pages        = {1328-1342},
  shortjournal = {Stat. Med.},
  title        = {A fully bayesian mixture model approach for identifying noncompliance in a regulatory tobacco clinical trial},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Multikernel linear mixed model with adaptive lasso for
complex phenotype prediction. <em>SIM</em>, <em>39</em>(9), 1311–1327.
(<a href="https://doi.org/10.1002/sim.8477">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Linear mixed models (LMMs) and their extensions have been widely used for high-dimensional genomic data analyses. While LMMs hold great promise for risk prediction research, the high dimensionality of the data and different effect sizes of genomic regions bring great analytical and computational challenges. In this work, we present a multikernel linear mixed model with adaptive lasso (KLMM-AL) to predict phenotypes using high-dimensional genomic data. We develop two algorithms for estimating parameters from our model and also establish the asymptotic properties of LMM with adaptive lasso when only one dependent observation is available. The proposed KLMM-AL can account for heterogeneous effect sizes from different genomic regions, capture both additive and nonadditive genetic effects, and adaptively and efficiently select predictive genomic regions and their corresponding effects. Through simulation studies, we demonstrate that KLMM-AL outperforms most of existing methods. Moreover, KLMM-AL achieves high sensitivity and specificity of selecting predictive genomic regions. KLMM-AL is further illustrated by an application to the sequencing dataset obtained from the Alzheimer&#39;s disease neuroimaging initiative.},
  archive      = {J_SIM},
  author       = {Yalu Wen and Qing Lu},
  doi          = {10.1002/sim.8477},
  journal      = {Statistics in Medicine},
  month        = {4},
  number       = {9},
  pages        = {1311-1327},
  shortjournal = {Stat. Med.},
  title        = {Multikernel linear mixed model with adaptive lasso for complex phenotype prediction},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Design efficiency in genetic association studies.
<em>SIM</em>, <em>39</em>(9), 1292–1310. (<a
href="https://doi.org/10.1002/sim.8476">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Selecting the best design for genetic association studies requires careful deliberation; different study designs can be used to scan for different genetic effects, and each design has its own set of strengths and limitations. A variety of family and unrelated control configurations are amenable to genetic association analyses, including the case-control design, case-parent triads, and case-parent triads in combination with unrelated controls or control-parent triads. Ultimately, the goal is to choose the design that achieves the highest statistical power using the lowest cost. For given parameter values and genotyped individuals, designs can be compared directly by computing the power. However, a more informative and general design comparison can be achieved by studying the relative efficiency, defined as the ratio of variances of two different parameter estimators, corresponding to two separate designs. Using log-linear modeling, we derive the relative efficiency from the asymptotic variance of the parameter estimators and relate it to the concept of Pitman efficiency. The relative efficiency takes into account the fact that different designs impose different costs relative to the number of genotyped individuals. We show that while optimal efficiency for analyses of regular autosomal effects is achieved using the standard case-control design, the case-parent triad design without unrelated controls is efficient when searching for parent-of-origin effects. Due to the potential loss of efficiency, maternal genes should generally not be adjusted for in an initial genome-wide association study scan of offspring genes but instead checked post hoc. The relative efficiency calculations are implemented in our R package Haplin.},
  archive      = {J_SIM},
  author       = {Miriam Gjerdevik and Håkon K. Gjessing and Julia Romanowska and Øystein A. Haaland and Astanand Jugessur and Nikolai O. Czajkowski and Rolv T. Lie},
  doi          = {10.1002/sim.8476},
  journal      = {Statistics in Medicine},
  month        = {4},
  number       = {9},
  pages        = {1292-1310},
  shortjournal = {Stat. Med.},
  title        = {Design efficiency in genetic association studies},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A robust bayesian mixed effects approach for zero inflated
and highly skewed longitudinal count data emanating from the zero
inflated discrete weibull distribution. <em>SIM</em>, <em>39</em>(9),
1275–1291. (<a href="https://doi.org/10.1002/sim.8475">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article proposes a Bayesian mixed effects zero inflated discrete Weibull (ZIDW) regression model for zero inflated and highly skewed longitudinal count data, as an alternative to mixed effects regression models that are based on the negative binomial, zero inflated negative binomial, and conventional discrete Weibull (DW) distributions. The mixed effects ZIDW regression model is an extension of a recently introduced model based on the DW distribution and uses the log-link function to specify the relationship between the linear predictors and the median counts. The ZIDW approach offers a more robust characteristic of central tendency, compared to the mean count, when there is skewness in the data. A matrix generalized half- t (MGH- t ) prior distribution is specified for the random effects covariance matrix as an alternative to the widely used Wishart prior distribution. The methodology is applied to a longitudinal dataset from an epilepsy clinical trial. In a data contamination simulation study, we show that the mixed effect ZIDW regression model is more robust than the competing mixed effects regression models when the data contain excess zeros or outliers. The performance of the ZIDW regression model is also assessed in a simulation study under the specification of, respectively, the MGH- t and Wishart prior distributions for the random effects covariance matrix. It turns out that the highest posterior density intervals under the MGH- t prior for the fixed effects maintain nominal coverage when the true variability between random slopes over time is small, whereas those under the Wishart prior are generally conservative.},
  archive      = {J_SIM},
  author       = {Divan Aristo Burger and Robert Schall and Johannes Theodorus Ferreira and Ding-Geng Chen},
  doi          = {10.1002/sim.8475},
  journal      = {Statistics in Medicine},
  month        = {4},
  number       = {9},
  pages        = {1275-1291},
  shortjournal = {Stat. Med.},
  title        = {A robust bayesian mixed effects approach for zero inflated and highly skewed longitudinal count data emanating from the zero inflated discrete weibull distribution},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A note on the bias of standard errors when orthogonality of
mean and variance parameters is not satisfied in the mixed model for
repeated measures analysis. <em>SIM</em>, <em>39</em>(9), 1264–1274. (<a
href="https://doi.org/10.1002/sim.8474">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The mixed effect models for repeated measures (MMRM) analysis is sometimes used as a primary analysis in longitudinal randomized clinical trials. The SE for the treatment effect in the MMRM analysis is usually estimated by assuming the orthogonality of the fixed effect and variance-covariance parameters, which is the orthogonality property of a multivariate normal distribution, because of default settings of most standard statistical software. However, this property might be lost when analysis models are misspecified and/or data include missing values with the mechanism of being missing at random. In this study, we investigated the effect of the assumption of the orthogonality property on the estimation of the SE for the MMRM analysis. From simulation and case studies, it was shown that the SE with the assumption of orthogonality property had nonnegligible bias, especially when the analysis models assuming heteroscedasticity between treatment groups were applied. We also introduce the SAS code for the MMRM analysis without assuming the orthogonality property. Assuming the orthogonality property in the MMRM analysis would lead to invalid statistical inference, and it is necessary to be careful when applying the MMRM analysis with most standard software.},
  archive      = {J_SIM},
  author       = {Kazushi Maruo and Ryota Ishii and Yusuke Yamaguchi and Masaaki Doi and Masahiko Gosho},
  doi          = {10.1002/sim.8474},
  journal      = {Statistics in Medicine},
  month        = {4},
  number       = {9},
  pages        = {1264-1274},
  shortjournal = {Stat. Med.},
  title        = {A note on the bias of standard errors when orthogonality of mean and variance parameters is not satisfied in the mixed model for repeated measures analysis},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Constructing dynamic treatment regimes with shared
parameters for censored data. <em>SIM</em>, <em>39</em>(9), 1250–1263.
(<a href="https://doi.org/10.1002/sim.8473">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dynamic treatment regimes are sequential decision rules that adapt throughout disease progression according to a patient&#39;s evolving characteristics. In many clinical applications, it is desirable that the format of the decision rules remains consistent over time. Unlike the estimation of dynamic treatment regimes in regular settings, where decision rules are formed without shared parameters, the derivation of the shared decision rules requires estimating shared parameters indexing the decision rules across different decision points. Estimation of such rules becomes more complicated when the clinical outcome of interest is a survival time subject to censoring. To address these challenges, we propose two novel methods: censored shared-Q-learning and censored shared-O-learning. Both methods incorporate clinical preferences into a qualitative rule, where the parameters indexing the decision rules are shared across different decision points and estimated simultaneously. We use simulation studies to demonstrate the superior performance of the proposed methods. The methods are further applied to the Framingham Heart Study to derive treatment rules for cardiovascular disease.},
  archive      = {J_SIM},
  author       = {Ying-Qi Zhao and Ruoqing Zhu and Guanhua Chen and Yingye Zheng},
  doi          = {10.1002/sim.8473},
  journal      = {Statistics in Medicine},
  month        = {4},
  number       = {9},
  pages        = {1250-1263},
  shortjournal = {Stat. Med.},
  title        = {Constructing dynamic treatment regimes with shared parameters for censored data},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Human disease cost network analysis. <em>SIM</em>,
<em>39</em>(9), 1237–1249. (<a
href="https://doi.org/10.1002/sim.8472">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Diseases can be interconnected. In the recent years, there has been a surge of multidisease studies. Among them, HDN (human disease network) analysis takes a system perspective, examines the interconnections among diseases along with their individual properties, and has demonstrated great potential. Most of the existing HDN analyses are based on either molecular information (which may be unreliable and have limited clinical relevance) or phenotypic measures (which may have limited implications for disease management and not directly reflect disease severity). In this study, we take advantage of the uniquely valuable Taiwan NHIRD (National Health Insurance Research Database) data and conduct an HDN analysis of disease treatment cost. Complementing the existing literature, treatment cost can serve as a surrogate of disease severity (and hence be clinically highly relevant) and also directly describe the financial burden of illness (and hence be uniquely informative for disease management). With inpatient and outpatient treatment data on close to 1 million randomly selected subjects and collected during the period of 2000 to 2013, the human disease cost network is constructed using a novel copula-based approach and the weighted correlation-based network construction technique. Extensive analysis is conducted, and the results are found to be biomedically sensible.},
  archive      = {J_SIM},
  author       = {Chenjin Ma and Yang Li and BenChang Shia and Shuangge Ma},
  doi          = {10.1002/sim.8472},
  journal      = {Statistics in Medicine},
  month        = {4},
  number       = {9},
  pages        = {1237-1249},
  shortjournal = {Stat. Med.},
  title        = {Human disease cost network analysis},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A causal framework for classical statistical estimands in
failure-time settings with competing events. <em>SIM</em>,
<em>39</em>(8), 1199–1236. (<a
href="https://doi.org/10.1002/sim.8471">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In failure-time settings, a competing event is any event that makes it impossible for the event of interest to occur. For example, cardiovascular disease death is a competing event for prostate cancer death because an individual cannot die of prostate cancer once he has died of cardiovascular disease. Various statistical estimands have been defined as possible targets of inference in the classical competing risks literature. Many reviews have described these statistical estimands and their estimating procedures with recommendations about their use. However, this previous work has not used a formal framework for characterizing causal effects and their identifying conditions, which makes it difficult to interpret effect estimates and assess recommendations regarding analytic choices. Here we use a counterfactual framework to explicitly define each of these classical estimands. We clarify that, depending on whether competing events are defined as censoring events, contrasts of risks can define a total effect of the treatment on the event of interest or a direct effect of the treatment on the event of interest not mediated by the competing event. In contrast, regardless of whether competing events are defined as censoring events, counterfactual hazard contrasts cannot generally be interpreted as causal effects. We illustrate how identifying assumptions for all of these counterfactual estimands can be represented in causal diagrams, in which competing events are depicted as time-varying covariates. We present an application of these ideas to data from a randomized trial designed to estimate the effect of estrogen therapy on prostate cancer mortality.},
  archive      = {J_SIM},
  author       = {Jessica G. Young and Mats J. Stensrud and Eric J. Tchetgen Tchetgen and Miguel A. Hernán},
  doi          = {10.1002/sim.8471},
  journal      = {Statistics in Medicine},
  month        = {4},
  number       = {8},
  pages        = {1199-1236},
  shortjournal = {Stat. Med.},
  title        = {A causal framework for classical statistical estimands in failure-time settings with competing events},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Recovery of original individual person data (IPD) inferences
from empirical IPD summaries only: Applications to distributed computing
under disclosure constraints. <em>SIM</em>, <em>39</em>(8), 1183–1198.
(<a href="https://doi.org/10.1002/sim.8470">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {There are many settings where individual person data (IPD) are not available, due to privacy or technical reasons, and one must work with IPD proxies, such as summary statistics, to approximate original IPD inferences, that is, the results of statistical analyses that would ideally have been performed on individual-level data. For instance, in a distributed computing setting, as implemented in the DataSHIELD software framework, different centers can only share IPD proxies to obtain pooled IPD inferences. Such privacy requirements limit the scope of statistical investigation. For example, it can be challenging to perform between-center random-effect regression models. To increase modeling freedom we propose a method that only uses simple nondisclosive summaries of the original IPD as input, such as empirical marginal moments and correlation matrices, and generates artificial data compatible with those summary features. Specifically, data are generated from a Gaussian copula with marginal and joint components specified by the above summaries. The goal is to reproduce original IPD features in the artificial data, such that original IPD inferences are recovered from the artificial data. In an application example, and through simulations, we show that we can recover estimates of a multivariable IPD random-effect logistic regression, from artificial data generated via the Gaussian copula using the above IPD summaries, suggesting the proposed approach provides a generally applicable strategy for distributed computing settings with data protection constraints.},
  archive      = {J_SIM},
  author       = {Federico Bonofiglio and Martin Schumacher and Harald Binder},
  doi          = {10.1002/sim.8470},
  journal      = {Statistics in Medicine},
  month        = {4},
  number       = {8},
  pages        = {1183-1198},
  shortjournal = {Stat. Med.},
  title        = {Recovery of original individual person data (IPD) inferences from empirical IPD summaries only: Applications to distributed computing under disclosure constraints},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Methods for generalized change-point models: With
applications to human immunodeficiency virus surveillance and diabetes
data. <em>SIM</em>, <em>39</em>(8), 1167–1182. (<a
href="https://doi.org/10.1002/sim.8469">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In many epidemiological and biomedical studies, the association between a response variable and some covariates of interest may change at one or several thresholds of the covariates. Change-point models are suitable for investigating the relationship between the response and covariates in such situations. We present change-point models, with at least one unknown change-point occurring with respect to some covariates of a generalized linear model for independent or correlated data. We develop methods for the estimation of the model parameters and investigate their finite-sample performances in simulations. We apply the proposed methods to examine the trends in the reported estimates of the annual percentage of new human immunodeficiency virus (HIV) diagnoses linked to HIV-related medical care within 3 months after diagnosis using HIV surveillance data from the HIV prevention trial network 065 study. We also apply our methods to a dataset from the Pima Indian diabetes study to examine the effects of age and body mass index on the risk of being diagnosed with type 2 diabetes.},
  archive      = {J_SIM},
  author       = {Jean de Dieu Tapsoba and Ching-Yun Wang and Sahar Zangeneh and Ying Qing Chen},
  doi          = {10.1002/sim.8469},
  journal      = {Statistics in Medicine},
  month        = {4},
  number       = {8},
  pages        = {1167-1182},
  shortjournal = {Stat. Med.},
  title        = {Methods for generalized change-point models: With applications to human immunodeficiency virus surveillance and diabetes data},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A fair comparison of tree-based and parametric methods in
multiple imputation by chained equations. <em>SIM</em>, <em>39</em>(8),
1156–1166. (<a href="https://doi.org/10.1002/sim.8468">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multiple imputation by chained equations (MICE) has emerged as a leading strategy for imputing missing epidemiological data due to its ease of implementation and ability to maintain unbiased effect estimates and valid inference. Within the MICE algorithm, imputation can be performed using a variety of parametric or nonparametric methods. Literature has suggested that nonparametric tree-based imputation methods outperform parametric methods in terms of bias and coverage when there are interactions or other nonlinear effects among the variables. However, these studies fail to provide a fair comparison as they do not follow the well-established recommendation that any effects in the final analysis model (including interactions) should be included in the parametric imputation model. We show via simulation that properly incorporating interactions in the parametric imputation model leads to much better performance. In fact, correctly specified parametric imputation and tree-based random forest imputation perform similarly when estimating the interaction effect. Parametric imputation leads to slightly higher coverage for the interaction effect, but it has wider confidence intervals than random forest imputation and requires correct specification of the imputation model. Epidemiologists should take care in specifying MICE imputation models, and this paper assists in that task by providing a fair comparison of parametric and tree-based imputation in MICE.},
  archive      = {J_SIM},
  author       = {Emily Slade and Melissa G. Naylor},
  doi          = {10.1002/sim.8468},
  journal      = {Statistics in Medicine},
  month        = {4},
  number       = {8},
  pages        = {1156-1166},
  shortjournal = {Stat. Med.},
  title        = {A fair comparison of tree-based and parametric methods in multiple imputation by chained equations},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Evaluating the ALERT algorithm for local outbreak onset
detection in seasonal infectious disease surveillance data.
<em>SIM</em>, <em>39</em>(8), 1145–1155. (<a
href="https://doi.org/10.1002/sim.8467">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Estimation of epidemic onset timing is an important component of controlling the spread of seasonal infectious diseases within community healthcare sites. The Above Local Elevated Respiratory Illness Threshold (ALERT) algorithm uses a threshold-based approach to suggest incidence levels that historically have indicated the transition from endemic to epidemic activity. In this paper, we present the first detailed overview of the computational approach underlying the algorithm. In the motivating example section, we evaluate the performance of ALERT in determining the onset of increased respiratory virus incidence using laboratory testing data from the Children&#39;s Hospital of Colorado. At a threshold of 10 cases per week, ALERT-selected intervention periods performed better than the observed hospital site periods (2004/2005–2012/2013) and a CUSUM method. Additional simulation studies show how data properties may effect ALERT performance on novel data. We found that the conditions under which ALERT showed ideal performance generally included high seasonality and low off-season incidence.},
  archive      = {J_SIM},
  author       = {Alexandria C. Brown and Stephen A. Lauer and Christine C. Robinson and Ann-Christine Nyquist and Suchitra Rao and Nicholas G. Reich},
  doi          = {10.1002/sim.8467},
  journal      = {Statistics in Medicine},
  month        = {4},
  number       = {8},
  pages        = {1145-1155},
  shortjournal = {Stat. Med.},
  title        = {Evaluating the ALERT algorithm for local outbreak onset detection in seasonal infectious disease surveillance data},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Modeling peer effect modification by network strength: The
diffusion of implantable cardioverter defibrillators in the US hospital
network. <em>SIM</em>, <em>39</em>(8), 1125–1144. (<a
href="https://doi.org/10.1002/sim.8466">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We develop methodology that allows peer effects (also referred to as social influence and contagion) to be modified by the structural importance of the focal actor&#39;s position in the network. The methodology is first developed for a single peer effect and then extended to simultaneously model multiple peer-effects and their modifications by the structural importance of the focal actor. This work is motivated by the diffusion of implantable cardioverter defibrillators (ICDs) in patients with congestive heart failure across a cardiovascular disease patient-sharing network of United States hospitals. We apply the general methodology to estimate peer effects for the adoption of capability to implant ICDs, the number of ICD implants performed by hospitals that are capable, and the number of patients referred to other hospitals by noncapable hospitals. Applying our novel methodology to study ICD diffusion across hospitals, we find evidence that exposure to ICD-capable peer hospitals is strongly associated with the chance a hospital becomes ICD-capable and that the direction and magnitude of the association is extensively modified by the strength of that hospital&#39;s position in the network, even after controlling for effects of geography. Therefore, interhospital networks, rather than geography per se, may explain key patterns of regional variations in healthcare utilization.},
  archive      = {J_SIM},
  author       = {A. James O&#39;Malley and Erika L. Moen and Julie P. W. Bynum and Andrea M. Austin and Jonathan S. Skinner},
  doi          = {10.1002/sim.8466},
  journal      = {Statistics in Medicine},
  month        = {4},
  number       = {8},
  pages        = {1125-1144},
  shortjournal = {Stat. Med.},
  title        = {Modeling peer effect modification by network strength: The diffusion of implantable cardioverter defibrillators in the US hospital network},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Bayesian hierarchical meta-analytic methods for modeling
surrogate relationships that vary across treatment classes using
aggregate data. <em>SIM</em>, <em>39</em>(8), 1103–1124. (<a
href="https://doi.org/10.1002/sim.8465">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Surrogate endpoints play an important role in drug development when they can be used to measure treatment effect early compared to the final clinical outcome and to predict clinical benefit or harm. Such endpoints are assessed for their predictive value of clinical benefit by investigating the surrogate relationship between treatment effects on the surrogate and final outcomes using meta-analytic methods. When surrogate relationships vary across treatment classes, such validation may fail due to limited data within each treatment class. In this paper, two alternative Bayesian meta-analytic methods are introduced which allow for borrowing of information from other treatment classes when exploring the surrogacy in a particular class. The first approach extends a standard model for the evaluation of surrogate endpoints to a hierarchical meta-analysis model assuming full exchangeability of surrogate relationships across all the treatment classes, thus facilitating borrowing of information across the classes. The second method is able to relax this assumption by allowing for partial exchangeability of surrogate relationships across treatment classes to avoid excessive borrowing of information from distinctly different classes. We carried out a simulation study to assess the proposed methods in nine data scenarios and compared them with subgroup analysis using the standard model within each treatment class. We also applied the methods to an illustrative example in colorectal cancer which led to obtaining the parameters describing the surrogate relationships with higher precision.},
  archive      = {J_SIM},
  author       = {Tasos Papanikos and John R. Thompson and Keith R. Abrams and Nicolas Städler and Oriana Ciani and Rod Taylor and Sylwia Bujkiewicz},
  doi          = {10.1002/sim.8465},
  journal      = {Statistics in Medicine},
  month        = {4},
  number       = {8},
  pages        = {1103-1124},
  shortjournal = {Stat. Med.},
  title        = {Bayesian hierarchical meta-analytic methods for modeling surrogate relationships that vary across treatment classes using aggregate data},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Adaptive multiarm multistage clinical trials. <em>SIM</em>,
<em>39</em>(8), 1084–1102. (<a
href="https://doi.org/10.1002/sim.8464">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Two methods for designing adaptive multiarm multistage (MAMS) clinical trials, originating from conceptually different group sequential frameworks are presented, and their operating characteristics are compared. In both methods pairwise comparisons are made, stage-by-stage, between each treatment arm and a common control arm with the goal of identifying active treatments and dropping inactive ones. At any stage one may alter the future course of the trial through adaptive changes to the prespecified decision rules for treatment selection and sample size reestimation, and notwithstanding such changes, both methods guarantee strong control of the family-wise error rate. The stage-wise MAMS approach was historically the first to be developed and remains the standard method for designing inferentially seamless phase 2-3 clinical trials. In this approach, at each stage, the data from each treatment comparison are summarized by a single multiplicity adjusted P -value. These stage-wise P -values are combined by a prespecified combination function and the resultant test statistic is monitored with respect to the classical two-arm group sequential efficacy boundaries. The cumulative MAMS approach is a more recent development in which a separate test statistic is constructed for each treatment comparison from the cumulative data at each stage. These statistics are then monitored with respect to multiplicity adjusted group sequential efficacy boundaries. We compared the powers of the two methods for designs with two and three active treatment arms, under commonly utilized decision rules for treatment selection, sample size reestimation and early stopping. In our investigations, which were carried out over a reasonably exhaustive exploration of the parameter space, the cumulative MAMS designs were more powerful than the stage-wise MAMS designs, except for the homogeneous case of equal treatment effects, where a small power advantage was discernable for the stage-wise MAMS designs.},
  archive      = {J_SIM},
  author       = {Pranab Ghosh and Lingyun Liu and Cyrus Mehta},
  doi          = {10.1002/sim.8464},
  journal      = {Statistics in Medicine},
  month        = {4},
  number       = {8},
  pages        = {1084-1102},
  shortjournal = {Stat. Med.},
  title        = {Adaptive multiarm multistage clinical trials},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Modeling repeated labor curves in consecutive pregnancies:
Individualized prediction of labor progression from previous pregnancy
data. <em>SIM</em>, <em>39</em>(8), 1068–1083. (<a
href="https://doi.org/10.1002/sim.8462">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The measurement of cervical dilation of a pregnant woman is used to monitor the progression of labor until 10 cm when pushing begins. There is anecdotal evidence that labor tracks across repeated pregnancies; moreover, no statistical methodology has been developed to address this important issue, which can help obstetricians make more informed clinical decisions about an individual woman&#39;s progression. Motivated by the NICHD Consecutive Pregnancies Study (CPS), we propose new methodology for analyzing labor curves across consecutive pregnancies. Our focus is both on studying the correlation between repeated labor curves on the same woman and on using the cervical dilation data from prior pregnancies to predict subsequent labor curves. We propose a hierarchical random effects model with a random change point that characterizes repeated labor curves within and between women to address these issues. We employ Bayesian methodology for parameter estimation and prediction. Model diagnostics to examine the appropriateness of the hierarchical random effects structure for characterizing the dependence structure across consecutive pregnancies are also proposed. The methodology was used in analyzing the CPS data and in developing a predictor for labor progression that can be used in clinical practice.},
  archive      = {J_SIM},
  author       = {Olive D. Buhule and Hyoyoung Choo-Wosoba and Paul S. Albert},
  doi          = {10.1002/sim.8462},
  journal      = {Statistics in Medicine},
  month        = {4},
  number       = {8},
  pages        = {1068-1083},
  shortjournal = {Stat. Med.},
  title        = {Modeling repeated labor curves in consecutive pregnancies: Individualized prediction of labor progression from previous pregnancy data},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Causal data fusion methods using summary-level statistics
for a continuous outcome. <em>SIM</em>, <em>39</em>(8), 1054–1067. (<a
href="https://doi.org/10.1002/sim.8461">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In many empirical studies, there exist rich individual studies to separately estimate causal effect of the treatment or exposure variable on the outcome variable, but incomplete confounders are adjusted in each study. Suppose we are interested in the causal effect of a treatment or exposure on an outcome variable, and we have available rich datasets that contain different confounders. How to integrate summary-level statistics from multiple individual datasets to improve causal inference has become a main challenge in data fusion. We propose a novel method in this article to identify the causal effect of a treatment or exposure on the continuous outcome. We show that the causal effect is identifiable and can be estimated by combining summary-level statistics from multiple datasets containing subsets of confounders and an external dataset only containing complete confounding information. Simulation studies indicate the unbiasedness of causal effect estimate by our method and we apply our method to a study about the effect of body mass index on fasting blood glucose.},
  archive      = {J_SIM},
  author       = {Hongkai Li and Wang Miao and Zheng Cai and Xinhui Liu and Tao Zhang and Fuzhong Xue and Zhi Geng},
  doi          = {10.1002/sim.8461},
  journal      = {Statistics in Medicine},
  month        = {4},
  number       = {8},
  pages        = {1054-1067},
  shortjournal = {Stat. Med.},
  title        = {Causal data fusion methods using summary-level statistics for a continuous outcome},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). An exact test with high power and robustness to unmeasured
confounding effects. <em>SIM</em>, <em>39</em>(8), 1041–1053. (<a
href="https://doi.org/10.1002/sim.8460">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In observational studies, it is agreed that the sensitivity of the findings to unmeasured confounders needs to be assessed. The issue is that a poor choice of test statistic can result in overstated sensitivity to hidden bias of this kind. In this article, a new adaptive test is proposed, guided by considerations of low sensitivity to hidden bias: it is tailored so that its power is greater than other leading tests, both in finite and infinite samples. One way of defining power in case of possible confounders is as the probability of reporting robustness (ie, insensitivity) of a true discovery to potential bias. In case of finite samples, we compute the power by simulations. When sample size approaches infinity, a meaningful indicator of the power is the design sensitivity, which is computed analytically and found to be better in the new test than in existing tests. Another asymptotic criterion for comparing tests when there is concern for confounders is Bahadur efficiency. The proposed test outperforms commonly used tests in terms of Bahadur efficiency in most sampling situations. The advantages of the new test mainly stem from its adaptivity: it combines two test statistics and consequently achieves the best design sensitivity and the best Bahadur efficiency of the two. As a “real-world” examination, we compare 441 daily smokers to 441 nonsmokers, to test the effect of smoking on periodontal disease. The new test is more robust to unmeasured confounders than both the Wilcoxon signed rank test and the paired t- test.},
  archive      = {J_SIM},
  author       = {Michal Shauly-Aharonov},
  doi          = {10.1002/sim.8460},
  journal      = {Statistics in Medicine},
  month        = {4},
  number       = {8},
  pages        = {1041-1053},
  shortjournal = {Stat. Med.},
  title        = {An exact test with high power and robustness to unmeasured confounding effects},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A functional-model-adjusted spatial scan statistic.
<em>SIM</em>, <em>39</em>(8), 1025–1040. (<a
href="https://doi.org/10.1002/sim.8459">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper introduces a new spatial scan statistic designed to adjust cluster detection for longitudinal confounding factors indexed in space. The functional-model-adjusted statistic was developed using generalized functional linear models in which longitudinal confounding factors were considered to be functional covariates. A general framework was developed for application to various probability models. Application to a Poisson model showed that the new method is equivalent to a conventional spatial scan statistic that adjusts the underlying population for covariates. In a simulation study with single and multiple covariate models, we found that our new method adjusts the cluster detection procedure more accurately than other methods. Use of the new spatial scan statistic was illustrated by analyzing data on premature mortality in France over the period from 1998 to 2013, with the quarterly unemployment rate as a longitudinal confounding factor.},
  archive      = {J_SIM},
  author       = {Mohamed-Salem Ahmed and Michaël Genin},
  doi          = {10.1002/sim.8459},
  journal      = {Statistics in Medicine},
  month        = {4},
  number       = {8},
  pages        = {1025-1040},
  shortjournal = {Stat. Med.},
  title        = {A functional-model-adjusted spatial scan statistic},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Survival model methods for analyses of cancer incidence
trends in young adults. <em>SIM</em>, <em>39</em>(7), 1011–1024. (<a
href="https://doi.org/10.1002/sim.8458">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent studies have reported increases in cancer incidence in adults under 50 years. However, there remains uncertainty about whether these are true increases or a result of incidental findings from increased medical imaging. To evaluate these trends, we propose an alternative method to age-period-cohort analyses based on survival modeling. Simulations show that our method is capable of quantifying cohort effects within various backgrounds including increasing medical imaging. We applied the method to analyze the changes in cancer incidence rates for 44 anatomic sites, stratified by sex, by birth cohort for individuals born from 1945 to 1969 in the US based on incidence data from the Surveillance, Epidemiology, and End Results (SEER) program, and tested the validity of our models using later birth cohorts (1970-1974 and 1975-1979). We found that cancer risks have increased significantly in 15 sites (9 in men and 11 in women) for 25-49 year-olds. These results were consistent with previous findings from age-period-cohort analyses. Furthermore, based on our simulations, these increases were independent of increased medical imaging and support substantial, increased extrinsic risks in the identified cancers. Although our approach has several limitations including the restriction to the younger age range and requirement of complete data for all ages of interest, we demonstrate many advantages of our approach including the ease in implementation and interpretation of cohort effects, robustness to various period backgrounds, and ability to make predictions. Our approach should help epidemiologists evaluate cohort effects using incidence data for cancer or other diseases.},
  archive      = {J_SIM},
  author       = {Wan Yang and Rebecca D. Kehm and Mary Beth Terry},
  doi          = {10.1002/sim.8458},
  journal      = {Statistics in Medicine},
  month        = {3},
  number       = {7},
  pages        = {1011-1024},
  shortjournal = {Stat. Med.},
  title        = {Survival model methods for analyses of cancer incidence trends in young adults},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Propensity score-adjusted three-component mixture model for
drug-drug interaction data mining in FDA adverse event reporting system.
<em>SIM</em>, <em>39</em>(7), 996–1010. (<a
href="https://doi.org/10.1002/sim.8457">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With increasing trend of polypharmacy, drug-drug interaction (DDI)-induced adverse drug events (ADEs) are considered as a major challenge for clinical practice. As premarketing clinical trials usually have stringent inclusion/exclusion criteria, limited comedication data capture and often times small sample size have limited values in study DDIs. On the other hand, ADE reports collected by spontaneous reporting system (SRS) become an important source for DDI studies. There are two major challenges in detecting DDI signals from SRS: confounding bias and false positive rate. In this article, we propose a novel approach, propensity score-adjusted three-component mixture model (PS-3CMM). This model can simultaneously adjust for confounding bias and estimate false discovery rate for all drug-drug-ADE combinations in FDA Adverse Event Reporting System (FAERS), which is a preeminent SRS database. In simulation studies, PS-3CMM performs better in detecting true DDIs comparing to the existing approach. It is more sensitive in selecting the DDI signals that have nonpositive individual drug relative ADE risk (NPIRR). The application of PS-3CMM is illustrated in analyzing the FAERS database. Compared to the existing approaches, PS-3CMM prioritizes DDI signals differently. PS-3CMM gives high priorities to DDI signals that have NPIRR. Both simulation studies and FAERS data analysis conclude that our new PS-3CMM is a new method that is complement to the existing DDI signal detection methods.},
  archive      = {J_SIM},
  author       = {Xueying Wang and Lang Li and Lei Wang and Weixing Feng and Pengyue Zhang},
  doi          = {10.1002/sim.8457},
  journal      = {Statistics in Medicine},
  month        = {3},
  number       = {7},
  pages        = {996-1010},
  shortjournal = {Stat. Med.},
  title        = {Propensity score-adjusted three-component mixture model for drug-drug interaction data mining in FDA adverse event reporting system},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Bayesian leveraging of historical control data for a
clinical trial with time-to-event endpoint. <em>SIM</em>,
<em>39</em>(7), 984–995. (<a
href="https://doi.org/10.1002/sim.8456">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The recent 21st Century Cures Act propagates innovations to accelerate the discovery, development, and delivery of 21st century cures. It includes the broader application of Bayesian statistics and the use of evidence from clinical expertise. An example of the latter is the use of trial-external (or historical) data, which promises more efficient or ethical trial designs. We propose a Bayesian meta-analytic approach to leverage historical data for time-to-event endpoints, which are common in oncology and cardiovascular diseases. The approach is based on a robust hierarchical model for piecewise exponential data. It allows for various degrees of between trial-heterogeneity and for leveraging individual as well as aggregate data. An ovarian carcinoma trial and a non-small cell cancer trial illustrate methodological and practical aspects of leveraging historical data for the analysis and design of time-to-event trials.},
  archive      = {J_SIM},
  author       = {Satrajit Roychoudhury and Beat Neuenschwander},
  doi          = {10.1002/sim.8456},
  journal      = {Statistics in Medicine},
  month        = {3},
  number       = {7},
  pages        = {984-995},
  shortjournal = {Stat. Med.},
  title        = {Bayesian leveraging of historical control data for a clinical trial with time-to-event endpoint},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Weighted kaplan-meier estimators motivating to estimate
HIV-1 RNA reduction censored by a limit of detection. <em>SIM</em>,
<em>39</em>(7), 968–983. (<a
href="https://doi.org/10.1002/sim.8455">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Measuring the magnitude of reduction in HIV-1 RNA levels accurately is difficult because many patients have a censored reduction due to the limit of detection (LOD) of the virologic assay being employed. The use of censored methods has improved the analysis of such reductions compared with crude methods but implies independent censoring. For HIV-1 RNA reduction data, the value at which a patient&#39;s HIV-1 RNA reduction becomes censored is mainly determined by the patient&#39;s baseline HIV-1 RNA level. We suggest two possibilities based on modification of the redistribution to the right algorithm to handle the situation of dependence either from a single continuous marker, that is, the baseline HIV-1 RNA level, or from multiple markers. Two series of simulation, one in the HIV-1 RNA setting and one in the classical censoring setting, compared performance of the previous methods with our suggestions. Our proposed estimators show good performances when the dependent censoring is due to LOD. Overall, in the classical censoring setting, our suggestions perform as well as other methods including the Inverse Probability of Censoring Weighted and the Kaplan-Meier imputation with Bootstrap. We applied those estimators to estimate the HIV-1 RNA reduction at week 8 of 502 patients who received a raltegravir-containing regimen and to data from the Mayo Clinic trial in primary biliary cirrhosis.},
  archive      = {J_SIM},
  author       = {Ismaïl Ahmed and Philippe Flandre},
  doi          = {10.1002/sim.8455},
  journal      = {Statistics in Medicine},
  month        = {3},
  number       = {7},
  pages        = {968-983},
  shortjournal = {Stat. Med.},
  title        = {Weighted kaplan-meier estimators motivating to estimate HIV-1 RNA reduction censored by a limit of detection},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Structured sparse logistic regression with application to
lung cancer prediction using breath volatile biomarkers. <em>SIM</em>,
<em>39</em>(7), 955–967. (<a
href="https://doi.org/10.1002/sim.8454">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article is motivated by a study of lung cancer prediction using breath volatile organic compound (VOC) biomarkers, where the challenge is that the predictors include not only high-dimensional time-dependent or functional VOC features but also the time-independent clinical variables. We consider a high-dimensional logistic regression and propose two different penalties: group spline-penalty or group smooth-penalty to handle the group structures of the time-dependent variables in the model. The new methods have the advantage for the situation where the model coefficients are sparse but change smoothly within the group, compared with other existing methods such as the group lasso and the group bridge approaches. Our methods are easy to implement since they can be turned into a group minimax concave penalty problem after certain transformations. We show that our fitting algorithm possesses the descent property and leads to attractive convergence properties. The simulation studies and the lung cancer application are performed to demonstrate the accuracy and stability of the proposed approaches.},
  archive      = {J_SIM},
  author       = {Xiaochen Zhang and Qingzhao Zhang and Xiaofeng Wang and Shuangge Ma and Kuangnan Fang},
  doi          = {10.1002/sim.8454},
  journal      = {Statistics in Medicine},
  month        = {3},
  number       = {7},
  pages        = {955-967},
  shortjournal = {Stat. Med.},
  title        = {Structured sparse logistic regression with application to lung cancer prediction using breath volatile biomarkers},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). An integer-valued time series model for multivariate
surveillance. <em>SIM</em>, <em>39</em>(7), 940–954. (<a
href="https://doi.org/10.1002/sim.8453">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent days, different types of surveillance data are becoming available for public health purposes. In most cases, several variables are monitored and events of different types are reported. As the amount of surveillance data increases, statistical methods that can effectively address multivariate surveillance scenarios are demanded. Even though research activity in this field is increasing rapidly in recent years, only a few approaches have simultaneously addressed the integer-valued property of the data and its correlation (both time correlation and cross-correlation) structure. In this article, we suggest a multivariate integer-valued autoregressive model that allows for both serial and cross-correlations between the series and can easily accommodate overdispersion and covariate information. Moreover, its structure implies a natural decomposition into an endemic and an epidemic component, a common distinction in dynamic models for infectious disease counts. Detection of disease outbreaks is achieved through the comparison of surveillance data with one-step-ahead predictions obtained after fitting the suggested model to a set of clean historical data. The performance of the suggested model is illustrated on a trivariate series of syndromic surveillance data collected during Athens 2004 Olympic Games.},
  archive      = {J_SIM},
  author       = {Xanthi Pedeli and Dimitris Karlis},
  doi          = {10.1002/sim.8453},
  journal      = {Statistics in Medicine},
  month        = {3},
  number       = {7},
  pages        = {940-954},
  shortjournal = {Stat. Med.},
  title        = {An integer-valued time series model for multivariate surveillance},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Exploratory identification of predictive biomarkers in
randomized trials with normal endpoints. <em>SIM</em>, <em>39</em>(7),
923–939. (<a href="https://doi.org/10.1002/sim.8452">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {One of the main endeavours in present-day medicine, especially in oncological research, is to provide evidence for individual treatment decisions (“stratified medicine”). In the pursuit of optimal treatment decision rules, the identification of predictive biomarkers that modify the treatment effect is essential. Proposed methods have often been based on recursive partitioning since a wide variety of interaction patterns can be captured automatically and the results are easily interpretable. Furthermore, these methods are readily extendable to high-dimensional settings by means of ensemble learning. In this article, we present predMOB, an adaptation of the model-based recursive partitioning (MOB) for subgroup analysis approach specifically tailored to the identification of predictive factors. In a simulation study, predMOB outperforms the original MOB with respect to the number of false detections and shows to be more robust in moderately complex settings. Furthermore, we compare the results of predMOB for the application to a public data base of amyotrophic lateral sclerosis patients to those obtained from the original MOB and are able to elucidate the nature of the biomarkers&#39; effects.},
  archive      = {J_SIM},
  author       = {Julia Krzykalla and Axel Benner and Annette Kopp-Schneider},
  doi          = {10.1002/sim.8452},
  journal      = {Statistics in Medicine},
  month        = {3},
  number       = {7},
  pages        = {923-939},
  shortjournal = {Stat. Med.},
  title        = {Exploratory identification of predictive biomarkers in randomized trials with normal endpoints},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Improving safety of the continual reassessment method via a
modified allocation rule. <em>SIM</em>, <em>39</em>(7), 906–922. (<a
href="https://doi.org/10.1002/sim.8450">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article proposes a novel criterion for the allocation of patients in phase I dose-escalation clinical trials, aiming to find the maximum tolerated dose (MTD). Conventionally, using a model-based approach, the next patient is allocated to the dose with the toxicity estimate closest (in terms of the absolute or squared distance) to the maximum acceptable toxicity. This approach, however, ignores the uncertainty in point estimates and ethical concerns of assigning a lot of patients to overly toxic doses. In fact, balancing the trade-off between how accurately the MTD can be estimated and how many patients would experience adverse events is one of the primary challenges in phase I studies. Motivated by recent discussions in the theory of estimation in restricted parameter spaces, we propose a criterion that allows to balance these explicitly. The criterion requires a specification of one additional parameter only that has a simple and intuitive interpretation. We incorporate the proposed criterion into the one-parameter Bayesian continual reassessment method and show, using simulations, that it can result in similar accuracy on average as the original design, but with fewer toxic responses on average. A comparison with other model-based dose-escalation designs, such as escalation with overdose control and its modifications, demonstrates that the proposed design can result in either the same mean accuracy as alternatives but fewer toxic responses or in a higher mean accuracy but the same number of toxic responses. Therefore, the proposed design can provide a better trade-off between the accuracy and the number of patients experiencing adverse events, making the design a more ethical alternative over some of the existing methods for phase I trials.},
  archive      = {J_SIM},
  author       = {Pavel Mozgunov and Thomas Jaki},
  doi          = {10.1002/sim.8450},
  journal      = {Statistics in Medicine},
  month        = {3},
  number       = {7},
  pages        = {906-922},
  shortjournal = {Stat. Med.},
  title        = {Improving safety of the continual reassessment method via a modified allocation rule},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Mendelian randomization using semiparametric linear
transformation models. <em>SIM</em>, <em>39</em>(7), 890–905. (<a
href="https://doi.org/10.1002/sim.8449">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mendelian randomization (MR) uses genetic information as an instrumental variable (IV) to estimate the causal effect of an exposure of interest on an outcome in the presence of unknown confounding. We are interested in the causal effect of cigarette smoking on lung cancer survival, which is subject to confounding by underlying pulmonary functions. Despite the well-developed IV analyses for the continuous and binary outcomes, the scarcity of methodology for the survival outcome limits its utility for the time-to-event data collected in many observational studies. We propose an IV analysis method in the survival context, estimating causal effects on a transformed survival time and survival probabilities using semiparametric linear transformation models. We study the conditions under which hazard ratio and the effect on survival probability can be approximated. For statistical inference, we construct estimating equations to circumvent the difficulty in deriving joint likelihood of the exposure and the outcome, due to the unknown confounding. Asymptotic properties of the proposed estimators are established without parametric assumptions about confounders. We study the finite sample performance in extensive simulation studies. The MR analysis of a lung cancer study suggests a harmful prognostic effect of smoking pack-years that would have been missed by the crude association.},
  archive      = {J_SIM},
  author       = {Yen-Tsung Huang},
  doi          = {10.1002/sim.8449},
  journal      = {Statistics in Medicine},
  month        = {3},
  number       = {7},
  pages        = {890-905},
  shortjournal = {Stat. Med.},
  title        = {Mendelian randomization using semiparametric linear transformation models},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Quicker detection risk-adjusted cumulative sum charting
procedures. <em>SIM</em>, <em>39</em>(7), 875–889. (<a
href="https://doi.org/10.1002/sim.8448">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {When a patient is operated on, the surgical outcome depends on two major factors: (i) the patient&#39;s health condition and (ii) the surgical process comprising the surgeon, the supporting staff, operating environment, and equipment. An outcome is usually represented by one if a patient dies within 30 days of an operation and zero otherwise. Another method of measuring the outcome is to use survival time with truncation on the 30th day for monitoring purposes. In order to monitor a surgical process effectively, the health condition of a patient must be taken into consideration. This is usually done using a log-likelihood ratio statistic based on an outcome, that is, risk adjusted according to the health condition of the patient. The 30-day wait results in delay in signaling when a deterioration occurs. The consequence of having to wait even though a death has occurred is the potential loss of lives because of delay in signaling. Regular updating of patients&#39; information can improve the sensitivity of a charting procedure. The main objective of this article is to develop and study the class of risk-adjusted cumulative sum procedures that are updated on a regular basis based on patients&#39; current conditions, without having to wait 30 days. Our study shows that these charts do in fact signal earlier and there are differences among the various updating techniques and monitoring statistics.},
  archive      = {J_SIM},
  author       = {Fah Fatt Gan and Jing Sheng Yuen and Sven Knoth},
  doi          = {10.1002/sim.8448},
  journal      = {Statistics in Medicine},
  month        = {3},
  number       = {7},
  pages        = {875-889},
  shortjournal = {Stat. Med.},
  title        = {Quicker detection risk-adjusted cumulative sum charting procedures},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). An evaluation of statistical approaches to postmarketing
surveillance. <em>SIM</em>, <em>39</em>(7), 845–874. (<a
href="https://doi.org/10.1002/sim.8447">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Safety of medical products presents a serious concern worldwide. Surveillance systems of postmarket medical products have been established for continual monitoring of adverse events (AEs) in many countries, and the proliferation of electronic health record systems further facilitates continual monitoring for AEs. We review existing statistical methods for signal detection that are mostly in use in postmarketing safety surveillance of spontaneously reported AEs and we study their performance characteristics by simulation. We compare those with the likelihood ratio test (LRT) method (appropriately modified for use in pharmacovigilance) and use three different methods to generate data (AE based, drug based, and a modification of the method of Ahmed et al). Performance metrics include type I error, power, sensitivity, and false discovery rate, among others. The results show superior performance of the LRT method in almost all simulation experiments. An application to the FDA Adverse Event Reporting System database is illustrated using rhabdomyolysis-related preferred terms reported to FDA during the third-quarter of 2014 to the first-quarter of 2017 for statin drugs. We present a critical discussion and recommendations for use of these methods.},
  archive      = {J_SIM},
  author       = {Yuxin Ding and Marianthi Markatou and Robert Ball},
  doi          = {10.1002/sim.8447},
  journal      = {Statistics in Medicine},
  month        = {3},
  number       = {7},
  pages        = {845-874},
  shortjournal = {Stat. Med.},
  title        = {An evaluation of statistical approaches to postmarketing surveillance},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Novel methods for the analysis of stepped wedge cluster
randomized trials. <em>SIM</em>, <em>39</em>(7), 815–844. (<a
href="https://doi.org/10.1002/sim.8451">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Stepped wedge cluster randomized trials (SW-CRTs) have become increasingly popular and are used for a variety of interventions and outcomes, often chosen for their feasibility advantages. SW-CRTs must account for time trends in the outcome because of the staggered rollout of the intervention. Robust inference procedures and nonparametric analysis methods have recently been proposed to handle such trends without requiring strong parametric modeling assumptions, but these are less powerful than model-based approaches. We propose several novel analysis methods that reduce reliance on modeling assumptions while preserving some of the increased power provided by the use of mixed effects models. In one method, we use the synthetic control approach to find the best matching clusters for a given intervention cluster. Another method makes use of within-cluster crossover information to construct an overall estimator. We also consider methods that combine these approaches to further improve power. We test these methods on simulated SW-CRTs, describing scenarios in which these methods have increased power compared with existing nonparametric methods while preserving nominal validity when mixed effects models are misspecified. We also demonstrate theoretical properties of these estimators with less restrictive assumptions than mixed effects models. Finally, we propose avenues for future research on the use of these methods; motivation for such research arises from their flexibility, which allows the identification of specific causal contrasts of interest, their robustness, and the potential for incorporating covariates to further increase power. Investigators conducting SW-CRTs might well consider such methods when common modeling assumptions may not hold.},
  archive      = {J_SIM},
  author       = {Lee Kennedy-Shaffer and Victor de Gruttola and Marc Lipsitch},
  doi          = {10.1002/sim.8451},
  journal      = {Statistics in Medicine},
  month        = {3},
  number       = {7},
  pages        = {815-844},
  shortjournal = {Stat. Med.},
  title        = {Novel methods for the analysis of stepped wedge cluster randomized trials},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A unified method for rare variant analysis of
gene-environment interactions. <em>SIM</em>, <em>39</em>(6), 801–813.
(<a href="https://doi.org/10.1002/sim.8446">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Advanced technology in whole-genome sequencing has offered the opportunity to comprehensively investigate the genetic contribution, particularly rare variants, to complex traits. Several region-based tests have been developed to jointly model the marginal effect of rare variants, but methods to detect gene-environment (GE) interactions are underdeveloped. Identifying the modification effects of environmental factors on genetic risk poses a considerable challenge. To tackle this challenge, we develop a method to detect GE interactions for rare variants using generalized linear mixed effect model. The proposed method can accommodate either binary or continuous traits in related or unrelated samples. Under this model, genetic main effects, GE interactions, and sample relatedness are modeled as random effects. We adopt a kernel-based method to leverage the joint information across rare variants and implement variance component score tests to reduce the computational burden. Our simulation studies of continuous and binary traits show that the proposed method maintains correct type I error rates and appropriate power under various scenarios, such as genotype main effects and GE interaction effects in opposite directions and varying the proportion of causal variants in the model. We apply our method in the Framingham Heart Study to test GE interaction of smoking on body mass index or overweight status and replicate the Cholinergic Receptor Nicotinic Beta 4 gene association reported in previous large consortium meta-analysis of single nucleotide polymorphism-smoking interaction. Our proposed set-based GE test is computationally efficient and is applicable to both binary and continuous phenotypes, while appropriately accounting for familial or cryptic relatedness.},
  archive      = {J_SIM},
  author       = {Elise Lim and Han Chen and Josée Dupuis and Ching-Ti Liu},
  doi          = {10.1002/sim.8446},
  journal      = {Statistics in Medicine},
  month        = {3},
  number       = {6},
  pages        = {801-813},
  shortjournal = {Stat. Med.},
  title        = {A unified method for rare variant analysis of gene-environment interactions},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). The emerging landscape of health research based on biobanks
linked to electronic health records: Existing resources, statistical
challenges, and potential opportunities. <em>SIM</em>, <em>39</em>(6),
773–800. (<a href="https://doi.org/10.1002/sim.8445">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Biobanks linked to electronic health records provide rich resources for health-related research. With improvements in administrative and informatics infrastructure, the availability and utility of data from biobanks have dramatically increased. In this paper, we first aim to characterize the current landscape of available biobanks and to describe specific biobanks, including their place of origin, size, and data types. The development and accessibility of large-scale biorepositories provide the opportunity to accelerate agnostic searches, expedite discoveries, and conduct hypothesis-generating studies of disease-treatment, disease-exposure, and disease-gene associations. Rather than designing and implementing a single study focused on a few targeted hypotheses, researchers can potentially use biobanks&#39; existing resources to answer an expanded selection of exploratory questions as quickly as they can analyze them. However, there are many obvious and subtle challenges with the design and analysis of biobank-based studies. Our second aim is to discuss statistical issues related to biobank research such as study design, sampling strategy, phenotype identification, and missing data. We focus our discussion on biobanks that are linked to electronic health records. Some of the analytic issues are illustrated using data from the Michigan Genomics Initiative and UK Biobank, two biobanks with two different recruitment mechanisms. We summarize the current body of literature for addressing these challenges and discuss some standing open problems. This work complements and extends recent reviews about biobank-based research and serves as a resource catalog with analytical and practical guidance for statisticians, epidemiologists, and other medical researchers pursuing research using biobanks.},
  archive      = {J_SIM},
  author       = {Lauren J. Beesley and Maxwell Salvatore and Lars G. Fritsche and Anita Pandit and Arvind Rao and Chad Brummett and Cristen J. Willer and Lynda D. Lisabeth and Bhramar Mukherjee},
  doi          = {10.1002/sim.8445},
  journal      = {Statistics in Medicine},
  month        = {3},
  number       = {6},
  pages        = {773-800},
  shortjournal = {Stat. Med.},
  title        = {The emerging landscape of health research based on biobanks linked to electronic health records: Existing resources, statistical challenges, and potential opportunities},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Generalized multiple contrast tests in dose-response
studies. <em>SIM</em>, <em>39</em>(6), 757–772. (<a
href="https://doi.org/10.1002/sim.8444">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the process of developing drugs, proof-of-concept studies can be helpful in determining whether there is any evidence of a dose-response relationship. A global test for this purpose that has gained popularity is a component of the multiple comparisons procedure with modeling techniques (MCP-Mod), which involves the specification of a candidate set of several plausible dose-response models. For each model, a test is performed for significance of an optimally chosen contrast among the sample means. An overall P -value is obtained from the distribution of the maximum of the contrast statistics. This is equivalent to basing the test on the minimum of the P -values arising from these contrast statistics and, hence, can be viewed as a method for combining dependent P -values. We generalize this idea to the use of different statistics for combining the dependent P -values, such as Fisher&#39;s combination method or the inverse normal combination method. Simulation studies show that the generalized multiple contrast tests (GMCTs) based on the Fisher and inverse normal methods are generally more powerful than the MCP-Mod procedure based on the minimum of the P -values except for cases where the true dose-response model is, in a sense, near the extremes of the candidate set of dose-response models. The proposed GMCTs can also be used for model selection and dosage selection by employing a closed testing procedure.},
  archive      = {J_SIM},
  author       = {Shiyang Ma and Michael P. McDermott},
  doi          = {10.1002/sim.8444},
  journal      = {Statistics in Medicine},
  month        = {3},
  number       = {6},
  pages        = {757-772},
  shortjournal = {Stat. Med.},
  title        = {Generalized multiple contrast tests in dose-response studies},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Dynamic predictive probabilities to monitor rapid cystic
fibrosis disease progression. <em>SIM</em>, <em>39</em>(6), 740–756. (<a
href="https://doi.org/10.1002/sim.8443">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cystic fibrosis (CF) is a progressive, genetic disease characterized by frequent, prolonged drops in lung function. Accurately predicting rapid underlying lung-function decline is essential for clinical decision support and timely intervention. Determining whether an individual is experiencing a period of rapid decline is complicated due to its heterogeneous timing and extent, and error component of the measured lung function. We construct individualized predictive probabilities for “nowcasting” rapid decline. We assume each patient&#39;s true longitudinal lung function, S ( t ) , follows a nonlinear, nonstationary stochastic process, and accommodate between-patient heterogeneity through random effects. Corresponding lung-function decline at time t is defined as the rate of change, S ′( t ) . We predict S ′( t ) conditional on observed covariate and measurement history by modeling a measured lung function as a noisy version of S ( t ) . The method is applied to data on 30 879 US CF Registry patients. Results are contrasted with a currently employed decision rule using single-center data on 212 individuals. Rapid decline is identified earlier using predictive probabilities than the center&#39;s currently employed decision rule (mean difference: 0.65 years; 95% confidence interval (CI): 0.41, 0.89). We constructed a bootstrapping algorithm to obtain CIs for predictive probabilities. We illustrate real-time implementation with R Shiny. Predictive accuracy is investigated using empirical simulations, which suggest this approach more accurately detects peak decline, compared with a uniform threshold of rapid decline. Median area under the ROC curve estimates (Q1-Q3) were 0.817 (0.814-0.822) and 0.745 (0.741-0.747), respectively, implying reasonable accuracy for both. This article demonstrates how individualized rate of change estimates can be coupled with probabilistic predictive inference and implementation for a useful medical-monitoring approach.},
  archive      = {J_SIM},
  author       = {Rhonda D. Szczesniak and Weiji Su and Cole Brokamp and Ruth H. Keogh and John P. Pestian and Michael Seid and Peter J. Diggle and John P. Clancy},
  doi          = {10.1002/sim.8443},
  journal      = {Statistics in Medicine},
  month        = {3},
  number       = {6},
  pages        = {740-756},
  shortjournal = {Stat. Med.},
  title        = {Dynamic predictive probabilities to monitor rapid cystic fibrosis disease progression},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A bayesian hierarchical variable selection prior for
pathway-based GWAS using summary statistics. <em>SIM</em>,
<em>39</em>(6), 724–739. (<a
href="https://doi.org/10.1002/sim.8442">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {While genome-wide association studies (GWASs) have been widely used to uncover associations between diseases and genetic variants, standard SNP-level GWASs often lack the power to identify SNPs that individually have a moderate effect size but jointly contribute to the disease. To overcome this problem, pathway-based GWASs methods have been developed as an alternative strategy that complements SNP-level approaches. We propose a Bayesian method that uses the generalized fused hierarchical structured variable selection prior to identify pathways associated with the disease using SNP-level summary statistics. Our prior has the flexibility to take in pathway structural information so that it can model the gene-level correlation based on prior biological knowledge, an important feature that makes it appealing compared to existing pathway-based methods. Using simulations, we show that our method outperforms competing methods in various scenarios, particularly when we have pathway structural information that involves complex gene-gene interactions. We apply our method to the Wellcome Trust Case Control Consortium Crohn&#39;s disease GWAS data, demonstrating its practical application to real data.},
  archive      = {J_SIM},
  author       = {Yi Yang and Saonli Basu and Lin Zhang},
  doi          = {10.1002/sim.8442},
  journal      = {Statistics in Medicine},
  month        = {3},
  number       = {6},
  pages        = {724-739},
  shortjournal = {Stat. Med.},
  title        = {A bayesian hierarchical variable selection prior for pathway-based GWAS using summary statistics},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Exact interval estimators for some commonly used measures of
binary agreement. <em>SIM</em>, <em>39</em>(6), 709–723. (<a
href="https://doi.org/10.1002/sim.8441">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We develop exact interval estimators for some commonly used classical measures of agreement in binary responses. We apply Monte Carlo simulation to evaluate the performance of these estimators. When the measure of agreement is homogeneous, we note that extending the results presented here to accommodate stratified analysis is straightforward. We use the data taken from a survey studying the agreement of religious identifications and the data taken from a study assessing the diagnostic performance of Whooley questions for major depression disorder to illustrate the use of these interval estimators.},
  archive      = {J_SIM},
  author       = {Kung-Jong Lui},
  doi          = {10.1002/sim.8441},
  journal      = {Statistics in Medicine},
  month        = {3},
  number       = {6},
  pages        = {709-723},
  shortjournal = {Stat. Med.},
  title        = {Exact interval estimators for some commonly used measures of binary agreement},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Cancer immunotherapy trial design with cure rate and delayed
treatment effect. <em>SIM</em>, <em>39</em>(6), 698–708. (<a
href="https://doi.org/10.1002/sim.8440">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cancer immunotherapy trials have two special features: a delayed treatment effect and a cure rate. Both features violate the proportional hazard model assumption and ignoring either one of the two features in an immunotherapy trial design will result in substantial loss of statistical power. To properly design immunotherapy trials, we proposed a piecewise proportional hazard cure rate model to incorporate both delayed treatment effect and cure rate into the trial design consideration. A sample size formula is derived for a weighted log-rank test under a fixed alternative hypothesis. The accuracy of sample size calculation using the new formula is assessed and compared with the existing methods via simulation studies. A real immunotherapy trial is used to illustrate the study design along with practical consideration of balance between sample size and follow-up time.},
  archive      = {J_SIM},
  author       = {Jing Wei and Jianrong Wu},
  doi          = {10.1002/sim.8440},
  journal      = {Statistics in Medicine},
  month        = {3},
  number       = {6},
  pages        = {698-708},
  shortjournal = {Stat. Med.},
  title        = {Cancer immunotherapy trial design with cure rate and delayed treatment effect},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Incorporating retesting outcomes for estimation of disease
prevalence. <em>SIM</em>, <em>39</em>(6), 687–697. (<a
href="https://doi.org/10.1002/sim.8439">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Group testing has been widely used as a cost-effective strategy to screen for and estimate the prevalence of a rare disease. While it is well-recognized that retesting is necessary for identifying infected subjects, it is not required for estimating the prevalence. For a test without misclassification, gains in statistical efficiency are expected from incorporating retesting results in the estimation of the prevalence. However, when the test is subject to misclassification, it is not clear how much gain should be expected. There are a number of theoretical challenges in addressing this issue, including (1) enumerating the potential test results from retesting individual subjects in a group, (2) the dependence among these test results and the test result from testing at the group level, and (3) differential misclassification due to pooling of biospecimens. Overcoming some of these challenges, we show that retesting subjects in either positive or negative groups can substantially improve the efficiency of the estimation and that retesting positive groups yields higher efficiency than retesting a same number or proportion of negative groups.},
  archive      = {J_SIM},
  author       = {Wei Zhang and Aiyi Liu and Qizhai Li and Paul S. Albert},
  doi          = {10.1002/sim.8439},
  journal      = {Statistics in Medicine},
  month        = {3},
  number       = {6},
  pages        = {687-697},
  shortjournal = {Stat. Med.},
  title        = {Incorporating retesting outcomes for estimation of disease prevalence},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A surrogate ℓ0 sparse cox’s regression with applications to
sparse high-dimensional massive sample size time-to-event data.
<em>SIM</em>, <em>39</em>(6), 675–686. (<a
href="https://doi.org/10.1002/sim.8438">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sparse high-dimensional massive sample size (sHDMSS) time-to-event data present multiple challenges to quantitative researchers as most current sparse survival regression methods and software will grind to a halt and become practically inoperable. This paper develops a scalable ℓ 0 -based sparse Cox regression tool for right-censored time-to-event data that easily takes advantage of existing high performance implementation of ℓ 2 -penalized regression method for sHDMSS time-to-event data. Specifically, we extend the ℓ 0 -based broken adaptive ridge (BAR) methodology to the Cox model, which involves repeatedly performing reweighted ℓ 2 -penalized regression. We rigorously show that the resulting estimator for the Cox model is selection consistent, oracle for parameter estimation, and has a grouping property for highly correlated covariates. Furthermore, we implement our BAR method in an R package for sHDMSS time-to-event data by leveraging existing efficient algorithms for massive ℓ 2 -penalized Cox regression. We evaluate the BAR Cox regression method by extensive simulations and illustrate its application on an sHDMSS time-to-event data from the National Trauma Data Bank with hundreds of thousands of observations and tens of thousands sparsely represented covariates.},
  archive      = {J_SIM},
  author       = {Eric S. Kawaguchi and Marc A. Suchard and Zhenqiu Liu and Gang Li},
  doi          = {10.1002/sim.8438},
  journal      = {Statistics in Medicine},
  month        = {3},
  number       = {6},
  pages        = {675-686},
  shortjournal = {Stat. Med.},
  title        = {A surrogate ℓ0 sparse cox&#39;s regression with applications to sparse high-dimensional massive sample size time-to-event data},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Publisher’s note. <em>SIM</em>, <em>39</em>(5), 674. (<a
href="https://doi.org/10.1002/sim.8498">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_SIM},
  doi          = {10.1002/sim.8498},
  journal      = {Statistics in Medicine},
  month        = {2},
  number       = {5},
  pages        = {674},
  shortjournal = {Stat. Med.},
  title        = {Publisher&#39;s note},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Modeling the overdetection of screen-identified cancers in
population-based cancer screening with the coxian phase-type markov
process. <em>SIM</em>, <em>39</em>(5), 660–673. (<a
href="https://doi.org/10.1002/sim.8437">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Modeling overdetection resulting from screening often uses the conventional competing risk model. This model assigns screen-detected cases dying from other causes as overdetection modeled by a one-jump process, which may not be true for the censored overdetected cases. To relax this restrictive assumption, accommodate a finite Markov process for overdetection, and dispense with long-term follow-up until death, we propose a generalized Coxian phase-type Markov process to distinguish the progressive latent multistate pathway from the nonprogressive (overdetected) latent multistate pathway. Various new likelihood functions were developed to estimate the transition parameters with the available data accrued at the time of diagnosis. The proportion of overdetected cancers by the cured model was further estimated by using parameters with and without distinguishing between the two latent pathways. While perturbation analyses were conducted by changing their parameters to assess their effects on overdetection, the results, including of asymptotic analyses, were very robust for an overdetection rate higher than 20% but not for low overdetection rates. These two scenarios were demonstrated by applying the Coxian phase-type model to prostate cancer and breast cancer screening, yielding a substantial proportion of overdetected prostate cancer (60%) attributed to the prostate specific antigen test and a small fraction of overdetected breast cancer (3%) detected by mammography. This kind of variation in overdetection elucidated by the Coxian phase-type Markov process provides new insights into the quantitative mechanisms producing overdetection, which is informative for evaluating the benefits and risks of various types of population-based cancer screening programs.},
  archive      = {J_SIM},
  author       = {Amy Ming-Fang Yen and Hsiu-Hsi Chen},
  doi          = {10.1002/sim.8437},
  journal      = {Statistics in Medicine},
  month        = {2},
  number       = {5},
  pages        = {660-673},
  shortjournal = {Stat. Med.},
  title        = {Modeling the overdetection of screen-identified cancers in population-based cancer screening with the coxian phase-type markov process},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Assessing the prior event rate ratio method via
probabilistic bias analysis on a bayesian network. <em>SIM</em>,
<em>39</em>(5), 639–659. (<a
href="https://doi.org/10.1002/sim.8435">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Background: Unmeasured confounders are commonplace in observational studies conducted using real-world data. Prior event rate ratio (PERR) adjustment is a technique shown to perform well in addressing such confounding. However, it has been demonstrated that, in some circumstances, the PERR method actually increases rather than decreases bias. In this work, we seek to better understand the robustness of PERR adjustment. Methods: We begin with a Bayesian network representation of a generalized observational study, which is subject to unmeasured confounding. Previous work evaluating PERR performance used Monte Carlo simulation to calculate joint probabilities of interest within the study population. Here, we instead use a Bayesian networks framework. Results: Using this streamlined analytic approach, we are able to conduct probabilistic bias analysis (PBA) using large numbers of combinations of parameters and thus obtain a comprehensive picture of PERR performance. We apply our methodology to a recent study that used the PERR in evaluating elderly-specific high-dose (HD) influenza vaccine in the US Veterans Affairs population. That study obtained an HD relative effectiveness of 25% (95% CI: 2%-43%) against influenza- and pneumonia-associated hospitalization, relative to standard-dose influenza vaccine. In this instance, we find that the PERR-adjusted result is more like to underestimate rather than to overestimate the relative effectiveness of the intervention. Conclusions: Although the PERR is a powerful tool for mitigating the effects of unmeasured confounders, it is not infallible. Here, we develop some general guidance for when a PERR approach is appropriate and when PBA is a safer option.},
  archive      = {J_SIM},
  author       = {Edward W. Thommes and Salaheddin M. Mahmud and Yinong Young-Xu and Julia Thornton Snider and Robertus van Aalst and Jason K.H. Lee and Yuliya Halchenko and Ellyn Russo and Ayman Chit},
  doi          = {10.1002/sim.8435},
  journal      = {Statistics in Medicine},
  month        = {2},
  number       = {5},
  pages        = {639-659},
  shortjournal = {Stat. Med.},
  title        = {Assessing the prior event rate ratio method via probabilistic bias analysis on a bayesian network},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Semiparametric bayesian variable selection for
gene-environment interactions. <em>SIM</em>, <em>39</em>(5), 617–638.
(<a href="https://doi.org/10.1002/sim.8434">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many complex diseases are known to be affected by the interactions between genetic variants and environmental exposures beyond the main genetic and environmental effects. Study of gene-environment (G×E) interactions is important for elucidating the disease etiology. Existing Bayesian methods for G×E interaction studies are challenged by the high-dimensional nature of the study and the complexity of environmental influences. Many studies have shown the advantages of penalization methods in detecting G×E interactions in “large p, small n” settings. However, Bayesian variable selection, which can provide fresh insight into G×E study, has not been widely examined. We propose a novel and powerful semiparametric Bayesian variable selection model that can investigate linear and nonlinear G×E interactions simultaneously. Furthermore, the proposed method can conduct structural identification by distinguishing nonlinear interactions from main-effects-only case within the Bayesian framework. Spike-and-slab priors are incorporated on both individual and group levels to identify the sparse main and interaction effects. The proposed method conducts Bayesian variable selection more efficiently than existing methods. Simulation shows that the proposed model outperforms competing alternatives in terms of both identification and prediction. The proposed Bayesian method leads to the identification of main and interaction effects with important implications in a high-throughput profiling study with high-dimensional SNP data.},
  archive      = {J_SIM},
  author       = {Jie Ren and Fei Zhou and Xiaoxi Li and Qi Chen and Hongmei Zhang and Shuangge Ma and Yu Jiang and Cen Wu},
  doi          = {10.1002/sim.8434},
  journal      = {Statistics in Medicine},
  month        = {2},
  number       = {5},
  pages        = {617-638},
  shortjournal = {Stat. Med.},
  title        = {Semiparametric bayesian variable selection for gene-environment interactions},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Analysis of ordered composite endpoints. <em>SIM</em>,
<em>39</em>(5), 602–616. (<a
href="https://doi.org/10.1002/sim.8431">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Composite endpoints are frequently used in clinical trials, but simple approaches, such as the time to first event, do not reflect any ordering among the endpoints. However, some endpoints, such as mortality, are worse than others. A variety of procedures have been proposed to reflect the severity of the individual endpoints such as pairwise ranking approaches, the win ratio, and the desirability of outcome ranking. When patients have different lengths of follow-up, however, ranking can be difficult and proposed methods do not naturally lead to regression approaches and require specialized software. This paper defines an ordering score O to operationalize the patient ranking implied by hierarchical endpoints. We show how differential right censoring of follow-up corresponds to multiple interval censoring of the ordering score allowing standard software for survival models to be used to calculate the nonparametric maximum likelihood estimators (NPMLEs) of different measures. Additionally, if one assumes that the ordering score is transformable to an exponential random variable, a semiparametric regression is obtained, which is equivalent to the proportional hazards model subject to multiple interval censoring. Standard software can be used for estimation. We show that the NPMLE can be poorly behaved compared to the simple estimators in staggered entry trials. We also show that the semiparametric estimator can be more efficient than simple estimators and explore how standard Cox regression maneuvers can be used to assess model fit, allow for flexible generalizations, and assess interactions of covariates with treatment. We analyze a trial of short versus long-term antiplatelet therapy using our methods.},
  archive      = {J_SIM},
  author       = {Dean Follmann and Michael P Fay and Toshimitsu Hamasaki and Scott Evans},
  doi          = {10.1002/sim.8431},
  journal      = {Statistics in Medicine},
  month        = {2},
  number       = {5},
  pages        = {602-616},
  shortjournal = {Stat. Med.},
  title        = {Analysis of ordered composite endpoints},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Adaptive trial designs in diagnostic accuracy research.
<em>SIM</em>, <em>39</em>(5), 591–601. (<a
href="https://doi.org/10.1002/sim.8430">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The aim of diagnostic accuracy studies is to evaluate how accurately a diagnostic test can distinguish diseased from nondiseased individuals. Depending on the research question, different study designs and accuracy measures are appropriate. As the prior knowledge in the planning phase is often very limited, modifications of design aspects such as the sample size during the ongoing trial could increase the efficiency of diagnostic trials. In intervention studies, group sequential and adaptive designs are well established. Such designs are characterized by preplanned interim analyses, giving the opportunity to stop early for efficacy or futility or to modify elements of the study design. In contrast, in diagnostic accuracy studies, such flexible designs are less common, even if they are as important as for intervention studies. However, diagnostic accuracy studies have specific features, which may require adaptations of the statistical methods or may lead to specific advantages or limitations of sequential and adaptive designs. In this article, we summarize the current status of methodological research and applications of flexible designs in diagnostic accuracy research. Furthermore, we indicate and advocate future development of adaptive design methodology and their use in diagnostic accuracy trials from an interdisciplinary viewpoint. The term “interdisciplinary viewpoint” describes the collaboration of experts of the academic and nonacademic research.},
  archive      = {J_SIM},
  author       = {Antonia Zapf and Maria Stark and Oke Gerke and Christoph Ehret and Norbert Benda and Patrick Bossuyt and Jon Deeks and Johannes Reitsma and Todd Alonzo and Tim Friede},
  doi          = {10.1002/sim.8430},
  journal      = {Statistics in Medicine},
  month        = {2},
  number       = {5},
  pages        = {591-601},
  shortjournal = {Stat. Med.},
  title        = {Adaptive trial designs in diagnostic accuracy research},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Extending the mixed-effects model to consider within-subject
variance for ecological momentary assessment data. <em>SIM</em>,
<em>39</em>(5), 577–590. (<a
href="https://doi.org/10.1002/sim.8429">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ecological Momentary Assessment data present some new modeling opportunities. Typically, there are sufficient data to explicitly model the within-subject (WS) variance, and in many applications, it is of interest to allow the WS variance to depend on covariates as well as random subject effects. We describe a model that allows multiple random effects per subject in the mean model (eg, random location intercept and slopes), as well as random scale in the error variance model. We present an example of the use of this model on a real dataset and a simulation study that shows the benefit of this model, relative to simpler approaches.},
  archive      = {J_SIM},
  author       = {Rachel Nordgren and Donald Hedeker and Genevieve Dunton and Chih-Hsiang Yang},
  doi          = {10.1002/sim.8429},
  journal      = {Statistics in Medicine},
  month        = {2},
  number       = {5},
  pages        = {577-590},
  shortjournal = {Stat. Med.},
  title        = {Extending the mixed-effects model to consider within-subject variance for ecological momentary assessment data},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). An empirical comparison of two novel transformation models.
<em>SIM</em>, <em>39</em>(5), 562–576. (<a
href="https://doi.org/10.1002/sim.8425">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Continuous response variables are often transformed to meet modeling assumptions, but the choice of the transformation can be challenging. Two transformation models have recently been proposed: semiparametric cumulative probability models (CPMs) and parametric most likely transformation models (MLTs). Both approaches model the cumulative distribution function and require specifying a link function, which implicitly assumes that the responses follow a known distribution after some monotonic transformation. However, the two approaches estimate the transformation differently. With CPMs, an ordinal regression model is fit, which essentially treats each continuous response as a unique category and therefore nonparametrically estimates the transformation; CPMs are semiparametric linear transformation models. In contrast, with MLTs, the transformation is parameterized using flexible basis functions. Conditional expectations and quantiles are readily derived from both methods on the response variable&#39;s original scale. We compare the two methods with extensive simulations. We find that both methods generally have good performance with moderate and large sample sizes. MLTs slightly outperformed CPMs in small sample sizes under correct models. CPMs tended to be somewhat more robust to model misspecification and outcome rounding. Except in the simplest situations, both methods outperform basic transformation approaches commonly used in practice. We apply both methods to an HIV biomarker study.},
  archive      = {J_SIM},
  author       = {Yuqi Tian and Torsten Hothorn and Chun Li and Frank E. Harrell and Bryan E. Shepherd},
  doi          = {10.1002/sim.8425},
  journal      = {Statistics in Medicine},
  month        = {2},
  number       = {5},
  pages        = {562-576},
  shortjournal = {Stat. Med.},
  title        = {An empirical comparison of two novel transformation models},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Robust estimation of the causal effect of time-varying
neighborhood factors on health outcomes. <em>SIM</em>, <em>39</em>(5),
544–561. (<a href="https://doi.org/10.1002/sim.8423">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The fundamental difficulty of establishing causal relationships between an exposure and an outcome in observational data involves disentangling causality from confounding factors. This problem underlies much of neighborhoods research, which abounds with studies that consider associations between neighborhood characteristics and health outcomes in longitudinal data. Such analyses are confounded by selection issues; individuals with above average health outcomes (or associated characteristics) may self-select into advantaged neighborhoods. Techniques commonly used to assess causal inferences in observational longitudinal data, such as inverse probability of treatment weighting (IPTW), may be inappropriate in neighborhoods data due to unique characteristics of such data. We advance the IPTW toolkit by introducing a procedure based on a multivariate kernel density function which is more appropriate for neighborhoods data. The proposed weighting method is applied in conjunction with a marginal structural model. Our empirical analyses use longitudinal data from the Health and Retirement Study; our exposure of interest is an index of neighborhood socioeconomic status (NSES), and we examine its influence on cognitive function. Our findings illustrate the importance of the choice of method for IPTW—the comparison weighting methods provide poor balance across the set of covariates (which is not the case for our preferred procedure) and yield misleading results when applied in the outcomes models. The utility of the multivariate kernel is also validated via simulation. In addition, our findings emphasize the importance of IPTW—controlling for covariates within a regression without IPTW indicates that NSES affects cognition, whereas IPTW-weighted models fail to show a statistically significant effect.},
  archive      = {J_SIM},
  author       = {Michael W. Robbins and Beth Ann Griffin and Regina A. Shih and Mary Ellen Slaughter},
  doi          = {10.1002/sim.8423},
  journal      = {Statistics in Medicine},
  month        = {2},
  number       = {5},
  pages        = {544-561},
  shortjournal = {Stat. Med.},
  title        = {Robust estimation of the causal effect of time-varying neighborhood factors on health outcomes},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A flexible copula-based approach for the analysis of
secondary phenotypes in ascertained samples. <em>SIM</em>,
<em>39</em>(5), 517–543. (<a
href="https://doi.org/10.1002/sim.8416">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data collected for a genome-wide association study of a primary phenotype are often used for additional genome-wide association analyses of secondary phenotypes. However, when the primary and secondary traits are dependent, naïve analyses of secondary phenotypes may induce spurious associations in non-randomly ascertained samples. Previously, retrospective likelihood-based methods have been proposed to correct for sampling biases arising in secondary trait association analyses. However, most methods have been introduced to handle studies featuring a case-control design based on a binary primary phenotype. As such, these methods are not directly applicable to more complicated study designs such as multiple-trait studies, where the sampling mechanism also depends on the secondary phenotype, or extreme-trait studies, where individuals with extreme primary phenotype values are selected. To accommodate these more complicated sampling mechanisms, only a few prospective likelihood approaches have been proposed. These approaches assume a normal distribution for the secondary phenotype (or the latent secondary phenotype) and a bivariate normal distribution for the primary-secondary phenotype dependence. In this paper, we propose a unified copula-based approach to appropriately detect genetic variant/secondary phenotype association in the presence of selected samples. Primary phenotype is either binary or continuous and the secondary phenotype is continuous although not necessary normal. We use both prospective and retrospective likelihoods to account for the sampling mechanism and use a copula model to allow for potentially different dependence structures between the primary and secondary phenotypes. We demonstrate the effectiveness of our approach through simulation studies and by analyzing data from the Avon Longitudinal Study of Parents and Children cohort.},
  archive      = {J_SIM},
  author       = {Fodé Tounkara and Geneviève Lefebvre and Celia Greenwood and Karim Oualkacha},
  doi          = {10.1002/sim.8416},
  journal      = {Statistics in Medicine},
  month        = {2},
  number       = {5},
  pages        = {517-543},
  shortjournal = {Stat. Med.},
  title        = {A flexible copula-based approach for the analysis of secondary phenotypes in ascertained samples},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Comments on “two-tailed significance tests for 2 × 2
contingency tables: What is the alternative?” By robin j prescott
statistics in medicine 2019;38:4264-4269. <em>SIM</em>, <em>39</em>(4),
510–513. (<a href="https://doi.org/10.1002/sim.8432">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_SIM},
  author       = {A. Martín Andrés and I. Herranz Tejedor and F. Gayá Moreno},
  doi          = {10.1002/sim.8432},
  journal      = {Statistics in Medicine},
  month        = {2},
  number       = {4},
  pages        = {510-513},
  shortjournal = {Stat. Med.},
  title        = {Comments on “Two-tailed significance tests for 2 × 2 contingency tables: what is the alternative?” by robin j prescott statistics in medicine 2019;38:4264-4269},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Randomization tests for multiarmed randomized clinical
trials. <em>SIM</em>, <em>39</em>(4), 494–509. (<a
href="https://doi.org/10.1002/sim.8418">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We examine the use of randomization-based inference for analyzing multiarmed randomized clinical trials, including the application of conditional randomization tests to multiple comparisons. The view is taken that the linkage of the statistical test to the experimental design (randomization procedure) should be recognized. A selected collection of randomization procedures generalized to multiarmed treatment allocation is summarized, and generalizations for two randomization procedures that heretofore were designed for only two treatments are developed. We explain the process of computing the randomization test and conditional randomization test via Monte Carlo simulation, developing an efficient algorithm that makes multiple comparisons possible that would not be possible using a standard algorithm, demonstrate the preservation of type I error rate, and explore the relationship of statistical power to the randomization procedure in the presence of a time trend and outliers. We distinguish between the interpretation of the p -value in the randomization test and in the population test and verify that the randomization test can be approximated by the population test on some occasions. Data from two multiarmed clinical trials from the literature are reanalyzed to illustrate the methodology.},
  archive      = {J_SIM},
  author       = {Yanying Wang and William F. Rosenberger and Diane Uschner},
  doi          = {10.1002/sim.8418},
  journal      = {Statistics in Medicine},
  month        = {2},
  number       = {4},
  pages        = {494-509},
  shortjournal = {Stat. Med.},
  title        = {Randomization tests for multiarmed randomized clinical trials},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Estimating cumulative incidence functions in competing risks
data with dependent left-truncation. <em>SIM</em>, <em>39</em>(4),
481–493. (<a href="https://doi.org/10.1002/sim.8421">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Both delayed study entry (left-truncation) and competing risks are common phenomena in observational time-to-event studies. For example, in studies conducted by Teratology Information Services (TIS) on adverse drug reactions during pregnancy, the natural time scale is gestational age, but women enter the study after time origin and upon contact with the service. Competing risks are present, because an elective termination may be precluded by a spontaneous abortion. If left-truncation is entirely random, the Aalen-Johansen estimator is the canonical estimator of the cumulative incidence functions of the competing events. If the assumption of random left-truncation is in doubt, we propose a new semiparametric estimator of the cumulative incidence function. The dependence between entry time and time-to-event is modeled using a cause-specific Cox proportional hazards model and the marginal (unconditional) estimates are derived via inverse probability weighting arguments. We apply the new estimator to data about coumarin usage during pregnancy. Here, the concern is that the cause-specific hazard of experiencing an induced abortion may depend on the time when seeking advice by a TIS, which also is the time of left-truncation or study entry. While the aims of counseling by a TIS are to reduce the rate of elective terminations based on irrational overestimation of drug risks and to lead to better and safer medical treatment of maternal disease, it is conceivable that women considering an induced abortion are more likely to seek counseling. The new estimator is also evaluated in extensive simulation studies and found preferable compared to the Aalen-Johansen estimator in non–misspecified scenarios and to at least provide for a sensitivity analysis otherwise.},
  archive      = {J_SIM},
  author       = {Regina Stegherr and Arthur Allignol and Reinhard Meister and Christof Schaefer and Jan Beyersmann},
  doi          = {10.1002/sim.8421},
  journal      = {Statistics in Medicine},
  month        = {2},
  number       = {4},
  pages        = {481-493},
  shortjournal = {Stat. Med.},
  title        = {Estimating cumulative incidence functions in competing risks data with dependent left-truncation},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Assessing predictive accuracy of survival regressions
subject to nonindependent censoring. <em>SIM</em>, <em>39</em>(4),
469–480. (<a href="https://doi.org/10.1002/sim.8420">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Survival regression is commonly applied in biomedical studies or clinical trials, and evaluating their predictive performance plays an essential role for model diagnosis and selection. The presence of censored data, particularly if informative, may pose more challenges for the assessment of predictive accuracy. Existing literature mainly focuses on prediction for survival probabilities with limitation work for survival time. In this work, we focus on accuracy measures of predicted survival times adjusted for a potentially informative censoring mechanism (ie, coarsening at random (CAR); non-CAR) by adopting the technique of inverse probability of censoring weighting. Our proposed predictive metric can be adaptive to various survival regression frameworks including but not limited to accelerated failure time models and proportional hazards models. Moreover, we provide the asymptotic properties of the inverse probability of censoring weighting estimators under CAR. We consider the settings of high-dimensional data under CAR or non-CAR for extensions. The performance of the proposed method is evaluated through extensive simulation studies and analysis of real data from the Critical Assessment of Microarray Data Analysis.},
  archive      = {J_SIM},
  author       = {Ming Wang and Qi Long and Chixiang Chen and Lijun Zhang},
  doi          = {10.1002/sim.8420},
  journal      = {Statistics in Medicine},
  month        = {2},
  number       = {4},
  pages        = {469-480},
  shortjournal = {Stat. Med.},
  title        = {Assessing predictive accuracy of survival regressions subject to nonindependent censoring},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Causal inference with noisy data: Bias analysis and
estimation approaches to simultaneously addressing missingness and
misclassification in binary outcomes. <em>SIM</em>, <em>39</em>(4),
456–468. (<a href="https://doi.org/10.1002/sim.8419">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Causal inference has been widely conducted in various fields and many methods have been proposed for different settings. However, for noisy data with both mismeasurements and missing observations, those methods often break down. In this paper, we consider a problem that binary outcomes are subject to both missingness and misclassification, when the interest is in estimation of the average treatment effects (ATE). We examine the asymptotic biases caused by ignoring missingness and/or misclassification and establish the intrinsic connections between missingness effects and misclassification effects on the estimation of ATE. We develop valid weighted estimation methods to simultaneously correct for missingness and misclassification effects. To provide protection against model misspecification, we further propose a doubly robust correction method which yields consistent estimators when either the treatment model or the outcome model is misspecified. Simulation studies are conducted to assess the performance of the proposed methods. An application to smoking cessation data is reported to illustrate the use of the proposed methods.},
  archive      = {J_SIM},
  author       = {Di Shu and Grace Y. Yi},
  doi          = {10.1002/sim.8419},
  journal      = {Statistics in Medicine},
  month        = {2},
  number       = {4},
  pages        = {456-468},
  shortjournal = {Stat. Med.},
  title        = {Causal inference with noisy data: Bias analysis and estimation approaches to simultaneously addressing missingness and misclassification in binary outcomes},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Design and analysis considerations for cohort stepped wedge
cluster randomized trials with a decay correlation structure.
<em>SIM</em>, <em>39</em>(4), 438–455. (<a
href="https://doi.org/10.1002/sim.8415">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A stepped wedge cluster randomized trial is a type of longitudinal cluster design that sequentially switches clusters to intervention over time until all clusters are treated. While the traditional posttest-only parallel design requires adjustment for a single intraclass correlation coefficient, the stepped wedge design allows multiple outcome measurements from the same cluster and so additional correlation parameters are necessary to characterize the within-cluster correlation structure. Although a number of studies have differentiated between the concepts of within-period and between-period correlations, only a few studies have allowed the between-period correlation to decay over time. In this article, we consider the proportional decay correlation structure for a cohort stepped wedge design, and provide a matrix-adjusted quasi-least squares approach to accurately estimate the correlation parameters along with the marginal intervention effect. We further develop the sample size and power procedures accounting for the correlation decay, and investigate the accuracy of the power procedure with continuous outcomes in a simulation study. We show that the empirical power agrees well with the prediction even with as few as nine clusters, when data are analyzed with matrix-adjusted quasi-least squares concurrently with a suitable bias-corrected sandwich variance. Two trial examples are provided to illustrate the new sample size procedure.},
  archive      = {J_SIM},
  author       = {Fan Li},
  doi          = {10.1002/sim.8415},
  journal      = {Statistics in Medicine},
  month        = {2},
  number       = {4},
  pages        = {438-455},
  shortjournal = {Stat. Med.},
  title        = {Design and analysis considerations for cohort stepped wedge cluster randomized trials with a decay correlation structure},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Optimizing interim analysis timing for bayesian adaptive
commensurate designs. <em>SIM</em>, <em>39</em>(4), 424–437. (<a
href="https://doi.org/10.1002/sim.8414">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In developing products for rare diseases, statistical challenges arise due to the limited number of patients available for participation in drug trials and other clinical research. Bayesian adaptive clinical trial designs offer the possibility of increased statistical efficiency, reduced development cost and ethical hazard prevention via their incorporation of evidence from external sources (historical data, expert opinions, and real-world evidence), and flexibility in the specification of interim looks. In this paper, we propose a novel Bayesian adaptive commensurate design that borrows adaptively from historical information and also uses a particular payoff function to optimize the timing of the study&#39;s interim analysis. The trial payoff is a function of how many samples can be saved via early stopping and the probability of making correct early decisions for either futility or efficacy. We calibrate our Bayesian algorithm to have acceptable long-run frequentist properties (Type I error and power) via simulation at the design stage. We illustrate our approach using a pediatric trial design setting testing the effect of a new drug for a rare genetic disease. The optimIA R package available at https://github.com/wxwx1993/Bayesian_IA_Timing provides an easy-to-use implementation of our approach.},
  archive      = {J_SIM},
  author       = {Xiao Wu and Yi Xu and Bradley P. Carlin},
  doi          = {10.1002/sim.8414},
  journal      = {Statistics in Medicine},
  month        = {2},
  number       = {4},
  pages        = {424-437},
  shortjournal = {Stat. Med.},
  title        = {Optimizing interim analysis timing for bayesian adaptive commensurate designs},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Association measures for clustered competing risks.
<em>SIM</em>, <em>39</em>(4), 409–423. (<a
href="https://doi.org/10.1002/sim.8413">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a semiparameteric model for multivariate clustered competing risks data when the cause-specific failure times and the occurrence of competing risk events among subjects within the same cluster are of interest. The cause-specific hazard functions are assumed to follow Cox proportional hazard models, and the associations between failure times given the same or different cause events and the associations between occurrences of competing risk events within the same cluster are investigated through copula models. A cross-odds ratio measure is explored under our proposed models. Two-stage estimation procedure is proposed in which the marginal models are estimated in the first stage, and the dependence parameters are estimated via an expectation-maximization algorithm in the second stage. The proposed estimators are shown to yield consistent and asymptotically normal under mild regularity conditions. Simulation studies are conducted to assess finite sample performance of the proposed method. The proposed technique is demonstrated through an application to a multicenter Bone Marrow transplantation dataset.},
  archive      = {J_SIM},
  author       = {Chien-Lin Su and Lajmi Lakhal-Chaieb},
  doi          = {10.1002/sim.8413},
  journal      = {Statistics in Medicine},
  month        = {2},
  number       = {4},
  pages        = {409-423},
  shortjournal = {Stat. Med.},
  title        = {Association measures for clustered competing risks},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Multistate analysis from cross-sectional and auxiliary
samples. <em>SIM</em>, <em>39</em>(4), 387–408. (<a
href="https://doi.org/10.1002/sim.8411">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Epidemiological studies routinely involve cross-sectional sampling of a population comprised of individuals progressing through life history processes. We consider features of a cross-sectional sample in terms of the intensity functions of a progressive multistate disease process under stationarity assumptions. The limiting values of estimators for regression coefficients in naive logistic regression models are studied, and simulations confirm the key asymptotic results that are relevant in finite samples. We also consider the need for and the use of data from auxiliary samples, which enable one to fit the full multistate life history process. We conclude with an application to data from a national cross-sectional sample assessing marker effects on psoriatic arthritis among individuals with psoriasis.},
  archive      = {J_SIM},
  author       = {Leilei Zeng and Richard J. Cook and Jooyoung Lee},
  doi          = {10.1002/sim.8411},
  journal      = {Statistics in Medicine},
  month        = {2},
  number       = {4},
  pages        = {387-408},
  shortjournal = {Stat. Med.},
  title        = {Multistate analysis from cross-sectional and auxiliary samples},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Safety surveillance and the estimation of risk in select
populations: Flexible methods to control for confounding while targeting
marginal comparisons via standardization. <em>SIM</em>, <em>39</em>(4),
369–386. (<a href="https://doi.org/10.1002/sim.8410">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider the critical problem of pharmacosurveillance for adverse events once a drug or medical product is incorporated into routine clinical care. When making inference on comparative safety using large-scale electronic health records, we often encounter an extremely rare binary adverse outcome with a large number of potential confounders. In this context, it is challenging to offer flexible methods to adjust for high-dimensional confounders, whereas use of the propensity score (PS) can help address this challenge by providing both confounding control and dimension reduction. Among PS methods, regression adjustment using the PS as a covariate in an outcome model has been incompletely studied and potentially misused. Previous studies have suggested that simple linear adjustment may not provide sufficient control of confounding. Moreover, no formal representation of the statistical procedure and associated inference has been detailed. In this paper, we characterize a three-step procedure, which performs flexible regression adjustment of the estimated PS followed by standardization to estimate the causal effect in a select population. We also propose a simple variance estimation method for performing inference. Through a realistic simulation mimicking data from the Food and Drugs Administration&#39;s Sentinel Initiative comparing the effect of angiotensin-converting enzyme inhibitors and beta blockers on incidence of angioedema, we show that flexible regression on the PS resulted in less bias without loss of efficiency, and can outperform other methods when the PS model is correctly specified. In addition, the direct variance estimation method is a computationally fast and reliable approach for inference.},
  archive      = {J_SIM},
  author       = {Xu Shi and Robert Wellman and Patrick J. Heagerty and Jennifer C. Nelson and Andrea J. Cook},
  doi          = {10.1002/sim.8410},
  journal      = {Statistics in Medicine},
  month        = {2},
  number       = {4},
  pages        = {369-386},
  shortjournal = {Stat. Med.},
  title        = {Safety surveillance and the estimation of risk in select populations: Flexible methods to control for confounding while targeting marginal comparisons via standardization},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Identification of the optimal treatment regimen in the
presence of missing covariates. <em>SIM</em>, <em>39</em>(4), 353–368.
(<a href="https://doi.org/10.1002/sim.8407">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Covariates associated with treatment-effect heterogeneity can potentially be used to make personalized treatment recommendations towards best clinical outcomes. Methods for treatment-selection rule development that directly maximize treatment-selection benefits have attracted much interest in recent years, due to the robustness of these methods to outcome modeling. In practice, the task of treatment-selection rule development can be further complicated by missingness in data. Here, we consider the identification of optimal treatment-selection rules for a binary disease outcome when measurements of an important covariate from study participants are partly missing. Under the missing at random assumption, we develop a robust estimator of treatment-selection rules under the direct-optimization paradigm. This estimator targets the maximum selection benefits to the population under correct specification of at least one mechanism from each of the two sets—missing data or conditional covariate distribution, and treatment assignment or disease outcome model. We evaluate and compare performance of the proposed estimator with alternative direct-optimization estimators through extensive simulation studies. We demonstrate the application of the proposed method through a real data example from an Alzheimer&#39;s disease study for developing covariate combinations to guide the treatment of Alzheimer&#39;s disease.},
  archive      = {J_SIM},
  author       = {Ying Huang and Xiao-Hua Zhou},
  doi          = {10.1002/sim.8407},
  journal      = {Statistics in Medicine},
  month        = {2},
  number       = {4},
  pages        = {353-368},
  shortjournal = {Stat. Med.},
  title        = {Identification of the optimal treatment regimen in the presence of missing covariates},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Exact sequential analysis for multiple weighted binomial end
points. <em>SIM</em>, <em>39</em>(3), 340–351. (<a
href="https://doi.org/10.1002/sim.8405">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sequential analysis is used in clinical trials and postmarket drug safety surveillance to prospectively monitor efficacy and safety to quickly detect benefits and problems, while taking the multiple testing of repeated analyses into account. When there are multiple outcomes, each one may be given a weight corresponding to its severity. This paper introduces an exact sequential analysis procedure for multiple weighted binomial end points; the analysis incorporates a drug&#39;s combined benefit and safety profile. It works with a variety of alpha spending functions for continuous, group, or mixed group-continuous sequential analysis. The binomial probabilities may vary over time and do not need to be known a priori . The new method was implemented in the free R Sequential package for both one- and two-tailed sequential analysis. An example is given examining myocardial infarction and major bleeding events in patients who initiated non-steroidal antiinflammatory drugs.},
  archive      = {J_SIM},
  author       = {Ivair R. Silva and Joshua J. Gagne and Mehdi Najafzadeh and Martin Kulldorff},
  doi          = {10.1002/sim.8405},
  journal      = {Statistics in Medicine},
  month        = {2},
  number       = {3},
  pages        = {340-351},
  shortjournal = {Stat. Med.},
  title        = {Exact sequential analysis for multiple weighted binomial end points},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Joint modeling of binary response and survival for clustered
data in clinical trials. <em>SIM</em>, <em>39</em>(3), 326–339. (<a
href="https://doi.org/10.1002/sim.8403">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In clinical trials, it is often desirable to evaluate the effect of a prognostic factor such as a marker response on a survival outcome. However, the marker response and survival outcome are usually associated with some potentially unobservable factors. In this case, the conventional statistical methods that model these two outcomes separately may not be appropriate. In this paper, we propose a joint model for marker response and survival outcomes for clustered data, providing efficient statistical inference by considering these two outcomes simultaneously. We focus on a special type of marker response: a binary outcome, which is investigated together with survival data using a cluster-specific multivariate random effect variable. A multivariate penalized likelihood method is developed to make statistical inference for the joint model. However, the standard errors obtained from the penalized likelihood method are usually underestimated. This issue is addressed using a jackknife resampling method to obtain a consistent estimate of standard errors. We conduct extensive simulation studies to assess the finite sample performance of the proposed joint model and inference methods in different scenarios. The simulation studies show that the proposed joint model has excellent finite sample properties compared to the separate models when there exists an underlying association between the marker response and survival data. Finally, we apply the proposed method to a symptom control study conducted by Canadian Cancer Trials Group to explore the prognostic effect of covariates on pain control and overall survival.},
  archive      = {J_SIM},
  author       = {Bingshu E. Chen and Jia Wang},
  doi          = {10.1002/sim.8403},
  journal      = {Statistics in Medicine},
  month        = {2},
  number       = {3},
  pages        = {326-339},
  shortjournal = {Stat. Med.},
  title        = {Joint modeling of binary response and survival for clustered data in clinical trials},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Incorporating patient-reported outcomes in dose-finding
clinical trials. <em>SIM</em>, <em>39</em>(3), 310–325. (<a
href="https://doi.org/10.1002/sim.8402">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Oncology dose-finding clinical trials determine the maximum tolerated dose (MTD) based on toxicity outcomes captured by clinicians. With the availability of more rigorous instruments for measuring toxicity directly from patients, there is a growing interest to incorporate patient-reported outcomes (PRO) in clinical trials to inform patient tolerability. This is particularly important for dose-finding trials to ensure the identification of a well-tolerated dose. In this paper, we propose three extensions of the continual reassessment method (CRM), termed PRO-CRMs, that incorporate both clinician and patient outcomes. The first method is a marginal modeling approach whereby clinician and patient toxicity outcomes are modeled separately. The other two methods impose a constraint using a joint outcome defined based on both clinician and patient toxicities and model them either jointly or marginally. Simulation studies show that while all three PRO-CRMs select well-tolerated doses based on clinician&#39;s and patient&#39;s perspectives, the methods using a joint outcome perform better and have similar performance. We also show that the proposed PRO-CRMs are consistent under robust model assumptions.},
  archive      = {J_SIM},
  author       = {Shing M. Lee and Xiaoqi Lu and Bin Cheng},
  doi          = {10.1002/sim.8402},
  journal      = {Statistics in Medicine},
  month        = {2},
  number       = {3},
  pages        = {310-325},
  shortjournal = {Stat. Med.},
  title        = {Incorporating patient-reported outcomes in dose-finding clinical trials},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). High-dimensional regression with ordered multiple
categorical predictors. <em>SIM</em>, <em>39</em>(3), 294–309. (<a
href="https://doi.org/10.1002/sim.8400">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Models for the ordered multiple categorical (OMC) response variable have already been extensively established and widely applied, but few studies have investigated linear regression problems with OMC predictors, especially in high-dimensional situations. In such settings, the pseudocategories of the discrete variable and other irrelevant explanatory variables need to be automatically selected. This paper introduces a transformation method of dummy variables for such OMC predictors, an L 1 penalty regression method is proposed based on the transformation. Model selection consistency of the proposed method is derived under some common assumptions for high-dimensional situation. Both simulation studies and real data analysis present good performance of this method, showing its wide applicability in relevant regression analysis.},
  archive      = {J_SIM},
  author       = {Lei Huang and Weiqiang Hang and Yue Chao},
  doi          = {10.1002/sim.8400},
  journal      = {Statistics in Medicine},
  month        = {2},
  number       = {3},
  pages        = {294-309},
  shortjournal = {Stat. Med.},
  title        = {High-dimensional regression with ordered multiple categorical predictors},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Assessing method agreement for paired repeated binary
measurements administered by multiple raters. <em>SIM</em>,
<em>39</em>(3), 279–293. (<a
href="https://doi.org/10.1002/sim.8398">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Method comparison studies are essential for development in medical and clinical fields. These studies often compare a cheaper, faster, or less invasive measuring method with a widely used one to see if they have sufficient agreement for interchangeable use. Moreover, unlike simply reading measurements from devices, eg, reading body temperature from a thermometer, the response measurement in many clinical and medical assessments is impacted not only by the measuring device but also by the rater. For example, widespread inconsistencies are commonly observed among raters in psychological or cognitive assessment studies due to different characteristics such as rater training and experience, especially in large-scale assessment studies when many raters are employed. This paper proposes a model-based approach to assess agreement of two measuring methods for paired repeated binary measurements under the scenario where the agreement between two measuring methods and the agreement among raters are required to be studied simultaneously. Based upon the generalized linear mixed models (GLMMs), the decision on the adequacy of interchangeable use is made by testing the equality of fixed effects of methods. Approaches for assessing method agreement, such as the Bland-Altman diagram and Cohen&#39;s kappa, are also developed for repeated binary measurements based upon the latent variables in GLMMs. We assess our novel model-based approach by simulation studies and a real clinical application, in which patients are evaluated repeatedly for delirium with two validated screening methods. Both the simulation studies and the real data analyses demonstrate that our proposed approach can effectively assess method agreement.},
  archive      = {J_SIM},
  author       = {Wei Wang and Nan Lin and Jordan D. Oberhaus and Michael S. Avidan},
  doi          = {10.1002/sim.8398},
  journal      = {Statistics in Medicine},
  month        = {2},
  number       = {3},
  pages        = {279-293},
  shortjournal = {Stat. Med.},
  title        = {Assessing method agreement for paired repeated binary measurements administered by multiple raters},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Order restricted inference in chronobiology. <em>SIM</em>,
<em>39</em>(3), 265–278. (<a
href="https://doi.org/10.1002/sim.8397">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper is motivated by applications in oscillatory systems where researchers are typically interested in discovering components of those systems that display rhythmic temporal patterns. The contributions of this paper are twofold. First, a methodology is developed based on a circular signal plus error model that is defined using order restrictions. This mathematical formulation of rhythmicity is simple, easily interpretable and very flexible, with the latter property derived from the nonparametric formulation of the signal. Second, we address various commonly encountered problems in the analysis of oscillatory systems data. Specifically, we propose a methodology for (a) detecting rhythmic signals in an oscillatory system and (b) estimating the unknown sampling time that occurs when tissues are obtained from subjects whose time of death is unknown. The proposed methodology is computationally efficient, outperforms the existing methods, and is broadly applicable to address a wide range of questions related to oscillatory systems.},
  archive      = {J_SIM},
  author       = {Yolanda Larriba and Cristina Rueda and Miguel A. Fernández and Shyamal D. Peddada},
  doi          = {10.1002/sim.8397},
  journal      = {Statistics in Medicine},
  month        = {2},
  number       = {3},
  pages        = {265-278},
  shortjournal = {Stat. Med.},
  title        = {Order restricted inference in chronobiology},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Exact inference for the random-effect model for
meta-analyses with rare events. <em>SIM</em>, <em>39</em>(3), 252–264.
(<a href="https://doi.org/10.1002/sim.8396">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Meta-analysis allows for the aggregation of results from multiple studies to improve statistical inference for the parameter of interest. In recent years, random-effect meta-analysis has been employed to synthesize estimates of incidence rates of adverse events across heterogeneous clinical trials to evaluate treatment safety. However, the validity of existing approaches relies on asymptotic approximation as the number of studies becomes large. In practice, a limited number of trials are typically available for analysis. Moreover, adverse events are typically rare; thus, study-specific incidence rate estimates may be unstable or undefined. In this paper, we present a method for construction of an exact confidence interval for the location parameter of the beta-binomial model through inversion of exact tests. The coverage level of the proposed confidence interval is guaranteed to achieve at least the nominal level, regardless of the number of studies or the with-in study sample size, making it particularly applicable to the study of rare-event data.},
  archive      = {J_SIM},
  author       = {Jessica Gronsbell and Chuan Hong and Lei Nie and Ying Lu and Lu Tian},
  doi          = {10.1002/sim.8396},
  journal      = {Statistics in Medicine},
  month        = {2},
  number       = {3},
  pages        = {252-264},
  shortjournal = {Stat. Med.},
  title        = {Exact inference for the random-effect model for meta-analyses with rare events},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Estimating the intervention effect in calibration
substudies. <em>SIM</em>, <em>39</em>(3), 239–251. (<a
href="https://doi.org/10.1002/sim.8394">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Exposure assessment is often subject to measurement errors. We consider here the analysis of studies aimed at reducing exposure to potential health hazards, in which exposure is the outcome variable. In these studies, the intervention effect may be estimated using either biomarkers or self-report data, but it is not common to combine these measures of exposure. Bias in the self-reported measures of exposure is a well-known fact; however, only few studies attempt to correct it. Recently, Keogh et al addressed this problem, presenting a model for measurement error in this setting and investigating how self-report and biomarker data can be combined. Keogh et al find the maximum likelihood estimate for the intervention effect in their model via direct numerical maximization of the likelihood. Here, we exploit an alternative presentation of the model that leads us to a closed formula for the MLE and also for its variance, when the number of biomarker replicates is the same for all subjects in the substudy. The variance formula enables efficient design of such intervention studies. When the number of biomarker replicates is not constant, our approach can be used along with the EM-algorithm to quickly compute the MLE. We compare the MLE to Buonaccorsi&#39;s method (Buonaccorsi, 1996) and find that they have similar efficiency when most subjects have biomarker data, but that the MLE has clear advantages when only a small fraction of subjects has biomarker data. This conclusion extends the findings of Keogh et al (2016) and has practical importance for efficiently designing studies.},
  archive      = {J_SIM},
  author       = {Michal Talitman and Malka Gorfine and David M. Steinberg},
  doi          = {10.1002/sim.8394},
  journal      = {Statistics in Medicine},
  month        = {2},
  number       = {3},
  pages        = {239-251},
  shortjournal = {Stat. Med.},
  title        = {Estimating the intervention effect in calibration substudies},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Ecological inference for infectious disease data, with
application to vaccination strategies. <em>SIM</em>, <em>39</em>(3),
220–238. (<a href="https://doi.org/10.1002/sim.8390">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Disease surveillance systems provide a rich source of data regarding infectious diseases, aggregated across geographical regions. The analysis of such ecological data is fraught with difficulties, and, unless care and suitable data summaries are available, will lead to biased estimates of individual-level parameters. We consider using surveillance data to study the impacts of vaccination. To catalog the problems of ecological inference, we start with an individual-level model, which contains familiar parameters, and derive an ecologically consistent model for infectious diseases in partially vaccinated populations. We compare with other popular model classes and highlight deficiencies. We explore the properties of the new model through simulation and demonstrate that, under standard assumptions, the ecological model provides less biased estimates. We then fit the new model to data collected on measles outbreaks in Germany from 2005-2007.},
  archive      = {J_SIM},
  author       = {Leigh H. Fisher and Jon Wakefield},
  doi          = {10.1002/sim.8390},
  journal      = {Statistics in Medicine},
  month        = {2},
  number       = {3},
  pages        = {220-238},
  shortjournal = {Stat. Med.},
  title        = {Ecological inference for infectious disease data, with application to vaccination strategies},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Causal inference of latent classes in complex survey data
with the estimating equation framework. <em>SIM</em>, <em>39</em>(3),
207–219. (<a href="https://doi.org/10.1002/sim.8382">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Latent class analysis (LCA) has been effectively used to cluster multiple survey items. However, causal inference with an exposure variable, identified by an LCA model, is challenging because (1) the exposure variable is unobserved and harbors the uncertainty of estimating parameters in the LCA model and (2) confounding bias adjustments need to be done with the unobserved LCA-driven exposure variable. In addition to these challenges, complex survey design features and survey weights must be accounted for if they are present. Our solutions to these issues are to (1) assess point estimates with the expected estimating function approach and (2) modify the survey design weights with LCA-based propensity scores. This paper aims to introduce a statistical procedure to apply the estimating equation approach to assessing the effects of LCA-driven cause in complex survey data using an example of the National Health and Nutrition Examination Survey.},
  archive      = {J_SIM},
  author       = {Joseph Kang and Yulei He and Jaeyoung Hong and Precious Esie and Kyle T. Bernstein},
  doi          = {10.1002/sim.8382},
  journal      = {Statistics in Medicine},
  month        = {2},
  number       = {3},
  pages        = {207-219},
  shortjournal = {Stat. Med.},
  title        = {Causal inference of latent classes in complex survey data with the estimating equation framework},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Correction to “a bayesian latent class approach for
EHR-based phenotyping.” <em>SIM</em>, <em>39</em>(2), 205. (<a
href="https://doi.org/10.1002/sim.8436">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_SIM},
  author       = {Rebecca A. Hubbard},
  doi          = {10.1002/sim.8436},
  journal      = {Statistics in Medicine},
  month        = {1},
  number       = {2},
  pages        = {205},
  shortjournal = {Stat. Med.},
  title        = {Correction to “A bayesian latent class approach for EHR-based phenotyping”},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Missing data and sensitivity analysis for binary data with
implications for sample size and power of randomized clinical trials.
<em>SIM</em>, <em>39</em>(2), 192–204. (<a
href="https://doi.org/10.1002/sim.8428">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Despite our best efforts, missing outcomes are common in randomized controlled clinical trials. The National Research Council&#39;s Committee on National Statistics panel report titled The Prevention and Treatment of Missing Data in Clinical Trials noted that further research is required to assess the impact of missing data on the power of clinical trials and how to set useful target rates and acceptable rates of missing data in clinical trials. In this article, using binary responses for illustration, we establish that conclusions based on statistical analyses that include only complete cases can be seriously misleading, and that the adverse impact of missing data grows not only with increasing rates of missingness but also with increasing sample size. We illustrate how principled sensitivity analysis can be used to assess the robustness of the conclusions. Finally, we illustrate how sample sizes can be adjusted to account for expected rates of missingness. We find that when sensitivity analyses are considered as part of the primary analysis, the required adjustments to the sample size are dramatically larger than those that are traditionally used. Furthermore, in some cases, especially in large trials with small target effect sizes, it is impossible to achieve the desired power.},
  archive      = {J_SIM},
  author       = {Thomas Cook and Ryan Zea},
  doi          = {10.1002/sim.8428},
  journal      = {Statistics in Medicine},
  month        = {1},
  number       = {2},
  pages        = {192-204},
  shortjournal = {Stat. Med.},
  title        = {Missing data and sensitivity analysis for binary data with implications for sample size and power of randomized clinical trials},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Estimation in meta-analyses of mean difference and
standardized mean difference. <em>SIM</em>, <em>39</em>(2), 171–191. (<a
href="https://doi.org/10.1002/sim.8422">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Methods for random-effects meta-analysis require an estimate of the between-study variance, τ 2 . The performance of estimators of τ 2 (measured by bias and coverage) affects their usefulness in assessing heterogeneity of study-level effects and also the performance of related estimators of the overall effect. However, as we show, the performance of the methods varies widely among effect measures. For the effect measures mean difference (MD) and standardized MD (SMD), we use improved effect-measure-specific approximations to the expected value of Q for both MD and SMD to introduce two new methods of point estimation of τ 2 for MD (Welch-type and corrected DerSimonian-Laird) and one WT interval method. We also introduce one point estimator and one interval estimator for τ 2 in SMD. Extensive simulations compare our methods with four point estimators of τ 2 (the popular methods of DerSimonian-Laird, restricted maximum likelihood, and Mandel and Paule, and the less-familiar method of Jackson) and four interval estimators for τ 2 (profile likelihood, Q-profile, Biggerstaff and Jackson, and Jackson). We also study related point and interval estimators of the overall effect, including an estimator whose weights use only study-level sample sizes. We provide measure-specific recommendations from our comprehensive simulation study and discuss an example.},
  archive      = {J_SIM},
  author       = {Ilyas Bakbergenuly and David C. Hoaglin and Elena Kulinskaya},
  doi          = {10.1002/sim.8422},
  journal      = {Statistics in Medicine},
  month        = {1},
  number       = {2},
  pages        = {171-191},
  shortjournal = {Stat. Med.},
  title        = {Estimation in meta-analyses of mean difference and standardized mean difference},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Time-varying feature selection for longitudinal analysis.
<em>SIM</em>, <em>39</em>(2), 156–170. (<a
href="https://doi.org/10.1002/sim.8412">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose time-varying coefficient model selection and estimation based on the spline approach, which is capable of capturing time-dependent covariate effects. The new penalty function utilizes local-region information for varying-coefficient estimation, in contrast to the traditional model selection approach focusing on the entire region. The proposed method is extremely useful when the signals associated with relevant predictors are time-dependent, and detecting relevant covariate effects in the local region is more scientifically relevant than those of the entire region. Our simulation studies indicate that the proposed model selection incorporating local features outperforms the global feature model selection approaches. The proposed method is also illustrated through a longitudinal growth and health study from National Heart, Lung, and Blood Institute.},
  archive      = {J_SIM},
  author       = {Lan Xue and Xinxin Shu and Peibei Shi and Colin O. Wu and Annie Qu},
  doi          = {10.1002/sim.8412},
  journal      = {Statistics in Medicine},
  month        = {1},
  number       = {2},
  pages        = {156-170},
  shortjournal = {Stat. Med.},
  title        = {Time-varying feature selection for longitudinal analysis},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Integrating approximate single factor graphical models.
<em>SIM</em>, <em>39</em>(2), 146–155. (<a
href="https://doi.org/10.1002/sim.8408">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the analysis of complex and high-dimensional data, graphical models have been commonly adopted to describe associations among variables. When common factors exist which make the associations dense, the single factor graphical model has been proposed, which first extracts the common factor and then conducts graphical modeling. Under other simpler contexts, it has been recognized that results generated from analyzing a single dataset are often unsatisfactory, and integrating multiple datasets can effectively improve variable selection and estimation. In graphical modeling, the increased number of parameters makes the “lack of information” problem more severe. In this article, we integrate multiple datasets and conduct the approximate single factor graphical model analysis. A novel penalization approach is developed for the identification and estimation of important loadings and edges. An effective computational algorithm is developed. A wide spectrum of simulations and the analysis of breast cancer gene expression datasets demonstrate the competitive performance of the proposed approach. Overall, this study provides an effective new venue for taking advantage of multiple datasets and improving graphical model analysis.},
  archive      = {J_SIM},
  author       = {Xinyan Fan and Kuangnan Fang and Shuangge Ma and Qingzhao Zhang},
  doi          = {10.1002/sim.8408},
  journal      = {Statistics in Medicine},
  month        = {1},
  number       = {2},
  pages        = {146-155},
  shortjournal = {Stat. Med.},
  title        = {Integrating approximate single factor graphical models},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A new mixed-effects mixture model for constrained
longitudinal data. <em>SIM</em>, <em>39</em>(2), 129–145. (<a
href="https://doi.org/10.1002/sim.8406">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Biomedical research often features continuous responses bounded by the interval [0, 1]. The well-known beta regression model addresses the constrained nature of these data, while its augmented and mixed-effects variants can address the presence of zeros and/or ones and longitudinal or clustered response values, respectively. However, these models are not robust to the presence of outliers and/or excessive number of observations near the tails. We propose a new augmented mixed-effects regression model based on a special beta mixture distribution that is capable of handling these issues. Extensive simulation studies show the superiority of the proposed model to the models most often used in the literature. The proposed model is applied to two real datasets: one taken from a long-term study of Parkinson&#39;s disease and the other taken from a study on reading accuracy.},
  archive      = {J_SIM},
  author       = {Agnese Maria Di Brisco and Sonia Migliorati},
  doi          = {10.1002/sim.8406},
  journal      = {Statistics in Medicine},
  month        = {1},
  number       = {2},
  pages        = {129-145},
  shortjournal = {Stat. Med.},
  title        = {A new mixed-effects mixture model for constrained longitudinal data},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Stochastic approach for mechanistic interaction under
longitudinal studies with noninformative right censoring. <em>SIM</em>,
<em>39</em>(2), 114–128. (<a
href="https://doi.org/10.1002/sim.8401">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Characterizing the mechanistic interactions between exposures and diseases is one of the most critical issues in epidemiologic studies. Previous studies have proposed a stochastic sufficient component cause framework, under which each sufficient cause is treated as a stochastic process instead of a time-invariant random variable. However, different types of mechanistic interactions such as synergism and agonism cannot be further identified. In this study, we proposed a stochastic marginal sufficient component cause model to conceptualize and identify agonism and synergism by exploiting the additional information. We further provided six approaches to identify and estimate agonism and synergism based on an additive hazard model and a complementary log model. Researchers can easily adjust confounding factors by including appropriate covariates into a regression model. Simulations have proven that approaches under three models are all valid tests. The power of an additive hazard model increases as the total follow-up time increases and is higher than that of the other two models. We applied this method to a Taiwanese cohort data set to investigate the mechanistic interaction among hepatitis B and C viruses on the incidence of hepatocellular carcinoma. The hazard of people with agonistic interaction is 1.28 × 10 −5 (95% CI: 6.97 × 10 −6 , 1.87 × 10 −5 ), and the cumulative hazard of those people is 7.41 × 10 −2 (95% CI: 4.09 × 10 −2 , 1.07 × 10 −1 ), which is approximately 3.5 times stronger than that of synergistic interaction. The proposed method makes it possible to quantify different types of mechanistic interactions in longitudinal studies with censored data.},
  archive      = {J_SIM},
  author       = {Jui-Hsiang Lin and Kuan-I Lin and Wen-Chung Lee and Sheng-Hsuan Lin},
  doi          = {10.1002/sim.8401},
  journal      = {Statistics in Medicine},
  month        = {1},
  number       = {2},
  pages        = {114-128},
  shortjournal = {Stat. Med.},
  title        = {Stochastic approach for mechanistic interaction under longitudinal studies with noninformative right censoring},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A review of the use of time-varying covariates in the
fine-gray subdistribution hazard competing risk regression model.
<em>SIM</em>, <em>39</em>(2), 103–113. (<a
href="https://doi.org/10.1002/sim.8399">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In survival analysis, time-varying covariates are covariates whose value can change during follow-up. Outcomes in medical research are frequently subject to competing risks (events precluding the occurrence of the primary outcome). We review the types of time-varying covariates and highlight the effect of their inclusion in the subdistribution hazard model. External time-dependent covariates are external to the subject, can effect the failure process, but are not otherwise involved in the failure mechanism. Internal time-varying covariates are measured on the subject, can effect the failure process directly, and may also be impacted by the failure mechanism. In the absence of competing risks, a consequence of including internal time-dependent covariates in the Cox model is that one cannot estimate the survival function or the effect of covariates on the survival function. In the presence of competing risks, the inclusion of internal time-varying covariates in a subdistribution hazard model results in the loss of the ability to estimate the cumulative incidence function (CIF) or the effect of covariates on the CIF. Furthermore, the definition of the risk set for the subdistribution hazard function can make defining internal time-varying covariates difficult or impossible. We conducted a review of the use of time-varying covariates in subdistribution hazard models in articles published in the medical literature in 2015 and in the first 5 months of 2019. Seven percent of articles published included a time-varying covariate. Several inappropriately described a time-varying covariate as having an association with the risk of the outcome.},
  archive      = {J_SIM},
  author       = {Peter C. Austin and Aurélien Latouche and Jason P. Fine},
  doi          = {10.1002/sim.8399},
  journal      = {Statistics in Medicine},
  month        = {1},
  number       = {2},
  pages        = {103-113},
  shortjournal = {Stat. Med.},
  title        = {A review of the use of time-varying covariates in the fine-gray subdistribution hazard competing risk regression model},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Response to comments on “two-tailed significance tests for 2
× 2 contingency tables: What is the alternative?” <em>SIM</em>,
<em>39</em>(1), 99–101. (<a
href="https://doi.org/10.1002/sim.8433">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_SIM},
  author       = {Robin J Prescott},
  doi          = {10.1002/sim.8433},
  journal      = {Statistics in Medicine},
  month        = {1},
  number       = {1},
  pages        = {99-101},
  shortjournal = {Stat. Med.},
  title        = {Response to comments on “Two-tailed significance tests for 2 × 2 contingency tables: What is the alternative?”},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Comments on “adaptive sample size modification in clinical
trials: Start small then ask for more?” <em>SIM</em>, <em>39</em>(1),
97–98. (<a href="https://doi.org/10.1002/sim.8427">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_SIM},
  author       = {Maximilian Pilz and Meinhard Kieser and Kevin Kunzmann},
  doi          = {10.1002/sim.8427},
  journal      = {Statistics in Medicine},
  month        = {1},
  number       = {1},
  pages        = {97-98},
  shortjournal = {Stat. Med.},
  title        = {Comments on “Adaptive sample size modification in clinical trials: Start small then ask for more?”},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Confidence interval estimation for the changepoint of
treatment stratification in the presence of a qualitative
covariate-treatment interaction. <em>SIM</em>, <em>39</em>(1), 70–96.
(<a href="https://doi.org/10.1002/sim.8404">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The goal in stratified medicine is to administer the “best” treatment to a patient. Not all patients might benefit from the same treatment; the choice of best treatment can depend on certain patient characteristics. In this article, it is assumed that a time-to-event outcome is considered as a patient-relevant outcome and a qualitative interaction between a continuous covariate and treatment exists, ie, that patients with different values of one specific covariate should be treated differently. We suggest and investigate different methods for confidence interval estimation for the covariate value, where the treatment recommendation should be changed based on data collected in a randomized clinical trial. An adaptation of Fieller&#39;s theorem, the delta method, and different bootstrap approaches (normal, percentile-based, wild bootstrap) are investigated and compared in a simulation study. Extensions to multivariable problems are presented and evaluated. We observed appropriate confidence interval coverage following Fieller&#39;s theorem irrespective of sample size but at the cost of very wide or even infinite confidence intervals. The delta method and the wild bootstrap approach provided the smallest intervals but inadequate coverage for small to moderate event numbers, also depending on the location of the true changepoint. For the percentile-based bootstrap, wide intervals were observed, and it was slightly conservative regarding coverage, whereas the normal bootstrap did not provide acceptable results for many scenarios. The described methods were also applied to data from a randomized clinical trial comparing two treatments for patients with symptomatic, severe carotid artery stenosis, considering patient&#39;s age as predictive marker.},
  archive      = {J_SIM},
  author       = {Bernhard Haller and Ulrich Mansmann and Dennis Dobler and Kurt Ulm and Alexander Hapfelmeier},
  doi          = {10.1002/sim.8404},
  journal      = {Statistics in Medicine},
  month        = {1},
  number       = {1},
  pages        = {70-96},
  shortjournal = {Stat. Med.},
  title        = {Confidence interval estimation for the changepoint of treatment stratification in the presence of a qualitative covariate-treatment interaction},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Exploiting gene-environment independence in haplotype-based
inferences for population-based case-control studies with complex
sampling. <em>SIM</em>, <em>39</em>(1), 57–69. (<a
href="https://doi.org/10.1002/sim.8395">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The use of complex sampling in population-based case-control studies is becoming more common. Although most single nucleotide polymorphism-based association studies with complex sampling account for the design complications, many of haplotype-based genetic association studies with complex sampling tend to ignore them when estimating haplotype frequencies, regression coefficients, or both. In this article, we develop innovative one-step and two-step statistical methods that account for the design complications in haplotype-based association studies when cases and/or controls are sampled with complex sampling. Attracted by the efficiency advantage of the retrospective method, we explore the assumptions of Hardy-Weinberg equilibrium and gene-environment independence in the underlying population. Results of our simulation studies demonstrate superior performance of the proposed methods over selected existing methods under various complex sampling designs. An application of the proposed methods is illustrated using a population-based case-control study of kidney cancer.},
  archive      = {J_SIM},
  author       = {Lingxiao Wang and Daoying Lin and Yan Li},
  doi          = {10.1002/sim.8395},
  journal      = {Statistics in Medicine},
  month        = {1},
  number       = {1},
  pages        = {57-69},
  shortjournal = {Stat. Med.},
  title        = {Exploiting gene-environment independence in haplotype-based inferences for population-based case-control studies with complex sampling},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Multistate quantile regression models. <em>SIM</em>,
<em>39</em>(1), 45–56. (<a
href="https://doi.org/10.1002/sim.8393">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We develop regression methods for inference on conditional quantiles of time-to-transition in multistate processes. Special cases include survival, recurrent event, semicompeting, and competing risk data. We use an ad hoc representation of the underlying stochastic process, in conjunction with methods for censored quantile regression. In a simulation study, we demonstrate that the proposed approach has a superior finite sample performance over simple methods for censored quantile regression, which naively assume independence between states, and over methods for competing risks, even when the latter are applied to competing risk data settings. We apply our approach to data on hospital-acquired infections in cirrhotic patients, showing a quantile-dependent effect of catheterization on time to infection.},
  archive      = {J_SIM},
  author       = {Alessio Farcomeni and Marco Geraci},
  doi          = {10.1002/sim.8393},
  journal      = {Statistics in Medicine},
  month        = {1},
  number       = {1},
  pages        = {45-56},
  shortjournal = {Stat. Med.},
  title        = {Multistate quantile regression models},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Combining individual and aggregated data to investigate the
role of socioeconomic disparities on cancer burden in italy.
<em>SIM</em>, <em>39</em>(1), 26–44. (<a
href="https://doi.org/10.1002/sim.8392">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Quantifying socioeconomic disparities and understanding the roots of inequalities are growing topics in cancer research. However, socioeconomic differences are challenging to investigate mainly due to the lack of accurate data at individual-level, while aggregate indicators are only partially informative. We implemented a multiple imputation algorithm within a statistical matching framework that combines diverse sources of data to estimate individual-level associations between income and risk of breast and lung cancer, adjusting for potential confounding factors in Italy. The framework is computationally flexible and can be adapted to similar contexts.},
  archive      = {J_SIM},
  author       = {Maura Mezzetti and Domenico Palli and Francesca Dominici},
  doi          = {10.1002/sim.8392},
  journal      = {Statistics in Medicine},
  month        = {1},
  number       = {1},
  pages        = {26-44},
  shortjournal = {Stat. Med.},
  title        = {Combining individual and aggregated data to investigate the role of socioeconomic disparities on cancer burden in italy},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A model-based approach for clustering of multivariate
semicontinuous data with application to dietary pattern analysis and
intervention. <em>SIM</em>, <em>39</em>(1), 16–25. (<a
href="https://doi.org/10.1002/sim.8391">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Semicontinuous data, characterized by a sizable number of zeros and observations from a continuous distribution, are frequently encountered in health research concerning food consumptions, physical activities, medical and pharmacy claims expenditures, and many others. In analyzing such semicontinuous data, it is imperative that the excessive zeros be adequately accounted for to obtain unbiased and efficient inference. Although many methods have been proposed in the literature for the modeling and analysis of semicontinuous data, little attention has been given to clustering of semicontinuous data to identify important patterns that could be indicative of certain health outcomes or intervention effects. We propose a Bernoulli-normal mixture model for clustering of multivariate semicontinuous data and demonstrate its accuracy as compared to the well-known clustering method with the conventional normal mixture model. The proposed method is illustrated with data from a dietary intervention trial to promote healthy eating behavior among children with type 1 diabetes. In the trial, certain diabetes friendly foods (eg, total fruit, whole fruit, dark green and orange vegetables and legumes, whole grain) were only consumed by a proportion of study participants, yielding excessive zero values due to nonconsumption of the foods. Baseline foods consumptions data in the trial are used to explore preintervention dietary patterns among study participants. While the conventional normal mixture model approach fails to do so, the proposed Bernoulli-normal mixture model approach has shown to be able to identify a dietary profile that significantly differentiates the intervention effects from others, as measured by the popular healthy eating index at the end of the trial.},
  archive      = {J_SIM},
  author       = {Tao Jiang and Yahui Lu and Huimin Duan and Wei Zhang and Aiyi Liu},
  doi          = {10.1002/sim.8391},
  journal      = {Statistics in Medicine},
  month        = {1},
  number       = {1},
  pages        = {16-25},
  shortjournal = {Stat. Med.},
  title        = {A model-based approach for clustering of multivariate semicontinuous data with application to dietary pattern analysis and intervention},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Regression analysis of recurrent-event-free time from
multiple follow-up windows. <em>SIM</em>, <em>39</em>(1), 1–15. (<a
href="https://doi.org/10.1002/sim.8385">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This research develops multivariable restricted time models appropriate for analysis of recurrent events data, where data is repurposed into censored longitudinal time-to-first-event outcomes in τ -length follow-up windows. We develop two approaches for addressing the censored nature of the outcomes: a pseudo-observation (PO) approach and a multiple-imputation (MI) approach. Each of these approaches allows for complete data methods, such as generalized estimating equations, to be used for the analysis of the newly constructed correlated outcomes. Through simulation, this manuscript assesses the performance of the proposed PO and MI methods. Both PO and MI approaches show attractive results with either correlated or independent gap times in an individual. We also demonstrate how to apply the proposed methods in the data from azithromycin in Chronic Obstructive Pulmonary Disease Trial.},
  archive      = {J_SIM},
  author       = {Meng Xia and Susan Murray and Nabihah Tayob},
  doi          = {10.1002/sim.8385},
  journal      = {Statistics in Medicine},
  month        = {1},
  number       = {1},
  pages        = {1-15},
  shortjournal = {Stat. Med.},
  title        = {Regression analysis of recurrent-event-free time from multiple follow-up windows},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
</ul>

</body>
</html>
