<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>BIMJ_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="bimj---150">BIMJ - 150</h2>
<ul>
<li><details>
<summary>
(2020h). Cover picture: Biometrical journal 8’20. <em>BIMJ</em>,
<em>62</em>(8), NA. (<a
href="https://doi.org/10.1002/bimj.202070081">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_BIMJ},
  doi          = {10.1002/bimj.202070081},
  journal      = {Biometrical Journal},
  month        = {12},
  number       = {8},
  pages        = {NA},
  shortjournal = {Bio. J.},
  title        = {Cover picture: Biometrical journal 8&#39;20},
  volume       = {62},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Comments on dr. Aniket biswas’ letter to the editor.
<em>BIMJ</em>, <em>62</em>(8), 2034–2035. (<a
href="https://doi.org/10.1002/bimj.202000224">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_BIMJ},
  author       = {Xiongzhi Chen and Rebecca W. Doerge},
  doi          = {10.1002/bimj.202000224},
  journal      = {Biometrical Journal},
  month        = {12},
  number       = {8},
  pages        = {2034-2035},
  shortjournal = {Bio. J.},
  title        = {Comments on dr. aniket biswas&#39; letter to the editor},
  volume       = {62},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Regarding paper “multiple testing with discrete data:
Proportion of true null hypotheses and two adaptive FDR procedures” by
xiongzhi chen, rebecca w. Doerge, and joseph f. heyse. <em>BIMJ</em>,
<em>62</em>(8), 2032–2033. (<a
href="https://doi.org/10.1002/bimj.202000139">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_BIMJ},
  author       = {Aniket Biswas},
  doi          = {10.1002/bimj.202000139},
  journal      = {Biometrical Journal},
  month        = {12},
  number       = {8},
  pages        = {2032-2033},
  shortjournal = {Bio. J.},
  title        = {Regarding paper “Multiple testing with discrete data: Proportion of true null hypotheses and two adaptive FDR procedures” by xiongzhi chen, rebecca w. doerge, and joseph f. heyse},
  volume       = {62},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). The design heatmap: A simple visualization of -optimality
design problems. <em>BIMJ</em>, <em>62</em>(8), 2013–2031. (<a
href="https://doi.org/10.1002/bimj.202000087">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Optimal experimental designs are often formal and specific, and not intuitively plausible to practical experimenters. However, even in theory, there often are many different possible design points providing identical or nearly identical information compared to the design points of a strictly optimal design. In practical applications, this can be used to find designs that are a compromise between mathematical optimality and practical requirements, including preferences of experimenters. For this purpose, we propose a derivative-based two-dimensional graphical representation of the design space that, given any optimal design is already known, will show which areas of the design space are relevant for good designs and how these areas relate to each other. While existing equivalence theorems already allow such an illustration in regard to the relevance of design points only, our approach also shows whether different design points contribute the same kind of information, and thus allows tweaking of designs for practical applications, especially in regard to the splitting and combining of design points. We demonstrate the approach on a toxicological trial where a -optimal design for a dose–response experiment modeled by a four-parameter log-logistic function was requested. As these designs require a prior estimate of the relevant parameters, which is difficult to obtain in a practical situation, we also discuss an adaption of our representations to the criterion of Bayesian -optimality. While we focus on -optimality, the approach is in principle applicable to different optimality criteria as well. However, much of the computational and graphical simplicity will be lost.},
  archive      = {J_BIMJ},
  author       = {Tim Holland-Letz and Annette Kopp-Schneider},
  doi          = {10.1002/bimj.202000087},
  journal      = {Biometrical Journal},
  month        = {12},
  number       = {8},
  pages        = {2013-2031},
  shortjournal = {Bio. J.},
  title        = {The design heatmap: A simple visualization of -optimality design problems},
  volume       = {62},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Bayesian inference for quantiles of the log-normal
distribution. <em>BIMJ</em>, <em>62</em>(8), 1997–2012. (<a
href="https://doi.org/10.1002/bimj.201900386">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The log-normal distribution is very popular for modeling positive right-skewed data and represents a common distributional assumption in many environmental applications. Here we consider the estimation of quantiles of this distribution from a Bayesian perspective. We show that the prior on the variance of the log of the variable is relevant for the properties of the posterior distribution of quantiles. Popular choices for this prior, such as the inverse gamma, lead to posteriors without finite moments. We propose the generalized inverse Gaussian and show that a restriction on the choice of one of its parameters guarantees the existence of posterior moments up to a prespecified order. In small samples, a careful choice of the prior parameters leads to point and interval estimators of the quantiles with good frequentist properties, outperforming those currently suggested by the frequentist literature. Finally, two real examples from environmental monitoring and occupational health frameworks highlight the improvements of our methodology, especially in a small sample situation.},
  archive      = {J_BIMJ},
  author       = {Aldo Gardini and Carlo Trivisano and Enrico Fabrizi},
  doi          = {10.1002/bimj.201900386},
  journal      = {Biometrical Journal},
  month        = {12},
  number       = {8},
  pages        = {1997-2012},
  shortjournal = {Bio. J.},
  title        = {Bayesian inference for quantiles of the log-normal distribution},
  volume       = {62},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). On the finite sample distribution of the likelihood ratio
statistic for testing heterogeneity in meta-analysis. <em>BIMJ</em>,
<em>62</em>(8), 1986–1996. (<a
href="https://doi.org/10.1002/bimj.201900400">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In meta-analysis, hypothesis testing is one of the commonly used approaches for assessing whether heterogeneity exists in effects between studies. The literature concluded that the Q -statistic is clearly the best choice and criticized the performance of the likelihood ratio test in terms of the type I error control and power. However, all the criticism for the likelihood ratio test is based on the use of a mixture of two chi-square distributions with 0 and 1 degrees of freedom, which is justified only asymptotically. In this study, we develop a novel method to derive the finite sample distribution of the likelihood ratio test and restricted likelihood ratio test statistics for testing the zero variance component in the random effects model for meta-analysis. We also extend this result to the heterogeneity test when metaregression is applied. A numerical study shows that the proposed statistics have superior performance to the Q -statistic, especially when the number of studies collected for meta-analysis is small to moderate.},
  archive      = {J_BIMJ},
  author       = {Sunghee Kuk and Woojoo Lee},
  doi          = {10.1002/bimj.201900400},
  journal      = {Biometrical Journal},
  month        = {12},
  number       = {8},
  pages        = {1986-1996},
  shortjournal = {Bio. J.},
  title        = {On the finite sample distribution of the likelihood ratio statistic for testing heterogeneity in meta-analysis},
  volume       = {62},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Personalized treatment plans with multivariate outcomes.
<em>BIMJ</em>, <em>62</em>(8), 1973–1985. (<a
href="https://doi.org/10.1002/bimj.201800072">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this work, we propose a novel method for individualized treatment selection when the treatment response is multivariate. Our method covers any number of treatments and it can be applied for a broad set of models. The proposed method uses a Mahalanobis-type distance measure to establish an ordering of treatments based on treatment performance measures. Our investigation in this work deals with means of responses conditional on lower dimensional composite scores based on covariates where these scores are built using single index models to approximate mean responses against patient covariates. Smoothed estimates of such conditional means are combined to construct an estimate of the aforementioned distance measure, which is then used to estimate the optimal treatment. An empirical study demonstrates the performance of the proposed method in finite samples. We also present a data analysis using an HIV clinical trial data to show the applicability of the proposed procedure for real data.},
  archive      = {J_BIMJ},
  author       = {Chathura Siriwardhana and Karunarathna B. Kulasekera},
  doi          = {10.1002/bimj.201800072},
  journal      = {Biometrical Journal},
  month        = {12},
  number       = {8},
  pages        = {1973-1985},
  shortjournal = {Bio. J.},
  title        = {Personalized treatment plans with multivariate outcomes},
  volume       = {62},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Information fraction estimation based on the number of
events within the standard treatment regimen. <em>BIMJ</em>,
<em>62</em>(8), 1960–1972. (<a
href="https://doi.org/10.1002/bimj.201900236">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For a Phase III randomized trial that compares survival outcomes between an experimental treatment versus a standard therapy, interim monitoring analysis is used to potentially terminate the study early based on efficacy. To preserve the nominal Type I error rate, alpha spending methods and information fractions are used to compute appropriate rejection boundaries in studies with planned interim analyses. For a one-sided trial design applied to a scenario in which the experimental therapy is superior to the standard therapy, interim monitoring should provide the opportunity to stop the trial prior to full follow-up and conclude that the experimental therapy is superior. This paper proposes a method called total control only (TCO) for estimating the information fraction based on the number of events within the standard treatment regimen. Based on theoretical derivations and simulation studies, for a maximum duration superiority design, the TCO method is not influenced by departure from the designed hazard ratio, is sensitive to detecting treatment differences, and preserves the Type I error rate compared to information fraction estimation methods that are based on total observed events. The TCO method is simple to apply, provides unbiased estimates of the information fraction, and does not rely on statistical assumptions that are impossible to verify at the design stage. For these reasons, the TCO method is a good approach when designing a maximum duration superiority trial with planned interim monitoring analyses.},
  archive      = {J_BIMJ},
  author       = {Ha M. Dang and Todd Alonzo and Meredith Franklin and Wendy J. Mack and Mark D. Krailo and Sandrah P. Eckel},
  doi          = {10.1002/bimj.201900236},
  journal      = {Biometrical Journal},
  month        = {12},
  number       = {8},
  pages        = {1960-1972},
  shortjournal = {Bio. J.},
  title        = {Information fraction estimation based on the number of events within the standard treatment regimen},
  volume       = {62},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Causal mediation analysis in nested case-control studies
using conditional logistic regression. <em>BIMJ</em>, <em>62</em>(8),
1939–1959. (<a href="https://doi.org/10.1002/bimj.201900120">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The paper proposes an approach to causal mediation analysis in nested case-control study designs, often incorporated with countermatching schemes using conditional likelihood, and we compare the method&#39;s performance to that of mediation analysis using the Cox model for the full cohort with a continuous or dichotomous mediator. Simulation studies are conducted to assess our proposed method and investigate the efficiency relative to the cohort. We illustrate the method using actual data from two studies of potential mediation of radiation risk conducted within the Adult Health Study cohort of atomic-bomb survivors. The performance becomes comparable to that based on the full cohort, illustrating the potential for valid mediation analysis based on the reduced data obtained through the nested case-control design.},
  archive      = {J_BIMJ},
  author       = {Young Min Kim and John B. Cologne and Euna Jang and Theis Lange and Yoshimi Tatsukawa and Waka Ohishi and Mai Utada and Harry M. Cullings},
  doi          = {10.1002/bimj.201900120},
  journal      = {Biometrical Journal},
  month        = {12},
  number       = {8},
  pages        = {1939-1959},
  shortjournal = {Bio. J.},
  title        = {Causal mediation analysis in nested case-control studies using conditional logistic regression},
  volume       = {62},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Inverse problem approach to regularized regression models
with application to predicting recovery after stroke. <em>BIMJ</em>,
<em>62</em>(8), 1926–1938. (<a
href="https://doi.org/10.1002/bimj.201900283">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Regression modelling is a powerful statistical tool often used in biomedical and clinical research. It could be formulated as an inverse problem that measures the discrepancy between the target outcome and the data produced by representation of the modelled predictors. This approach could simultaneously perform variable selection and coefficient estimation. We focus particularly on a linear regression issue, where is the parameter of interest and its components are the regression coefficients. The inverse problem finds an estimate for the parameter , which is mapped by the linear operator to the observed outcome data . This problem could be conveyed by finding a solution in the affine subspace . However, in the presence of collinearity, high-dimensional data and high conditioning number of the related covariance matrix, the solution may not be unique, so the introduction of prior information to reduce the subset and regularize the inverse problem is needed. Informed by Huber&#39;s robust statistics framework, we propose an optimal regularizer to the regression problem. We compare results of the proposed method and other penalized regression regularization methods: ridge, lasso, adaptive-lasso and elastic-net under different strong hypothesis such as high conditioning number of the covariance matrix and high error amplitude, on both simulated and real data from the South London Stroke Register. The proposed approach can be extended to mixed regression models. Our inverse problem framework coupled with robust statistics methodology offer new insights in statistical regression and learning. It could open a new research development for model fitting and learning.},
  archive      = {J_BIMJ},
  author       = {Youssef Hbid and Khaladi Mohamed and Charles D.A. Wolfe and Abdel Douiri},
  doi          = {10.1002/bimj.201900283},
  journal      = {Biometrical Journal},
  month        = {12},
  number       = {8},
  pages        = {1926-1938},
  shortjournal = {Bio. J.},
  title        = {Inverse problem approach to regularized regression models with application to predicting recovery after stroke},
  volume       = {62},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Nonparametric screening and feature selection for
ultrahigh-dimensional case II interval-censored failure time data.
<em>BIMJ</em>, <em>62</em>(8), 1909–1925. (<a
href="https://doi.org/10.1002/bimj.201900154">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For the analysis of ultrahigh-dimensional data, the first step is often to perform screening and feature selection to effectively reduce the dimensionality while retaining all the active or relevant variables with high probability. For this, many methods have been developed under various frameworks but most of them only apply to complete data. In this paper, we consider an incomplete data situation, case II interval-censored failure time data, for which there seems to be no screening procedure. Basing on the idea of cumulative residual, a model-free or nonparametric method is developed and shown to have the sure independent screening property. In particular, the approach is shown to tend to rank the active variables above the inactive ones in terms of their association with the failure time of interest. A simulation study is conducted to demonstrate the usefulness of the proposed method and, in particular, indicates that it works well with general survival models and is capable of capturing the nonlinear covariates with interactions. Also the approach is applied to a childhood cancer survivor study that motivated this investigation.},
  archive      = {J_BIMJ},
  author       = {Qiang Hu and Liang Zhu and Yanyan Liu and Jianguo Sun and Deo Kumar Srivastava and Leslie L. Robison},
  doi          = {10.1002/bimj.201900154},
  journal      = {Biometrical Journal},
  month        = {12},
  number       = {8},
  pages        = {1909-1925},
  shortjournal = {Bio. J.},
  title        = {Nonparametric screening and feature selection for ultrahigh-dimensional case II interval-censored failure time data},
  volume       = {62},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Isolating cost drivers in interstitial lung disease
treatment using nonparametric bayesian methods. <em>BIMJ</em>,
<em>62</em>(8), 1896–1908. (<a
href="https://doi.org/10.1002/bimj.202000076">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mixture modeling is a popular approach to accommodate overdispersion, skewness, and multimodality features that are very common for health care utilization data. However, mixture modeling tends to rely on subjective judgment regarding the appropriate number of mixture components or some hypothesis about how to cluster the data. In this work, we adopt a nonparametric, variational Bayesian approach to allow the model to select the number of components while estimating their parameters. Our model allows for a probabilistic classification of observations into clusters and simultaneous estimation of a Gaussian regression model within each cluster. When we apply this approach to data on patients with interstitial lung disease, we find distinct subgroups of patients with differences in means and variances of health care costs, health and treatment covariates, and relationships between covariates and costs. The subgroups identified are readily interpretable, suggesting that this nonparametric variational approach to inference can discover valid insights into the factors driving treatment costs. Moreover, the learning algorithm we employed is very fast and scalable, which should make the technique accessible for a broad range of applications.},
  archive      = {J_BIMJ},
  author       = {Christoph F. Kurz and Seth Stafford},
  doi          = {10.1002/bimj.202000076},
  journal      = {Biometrical Journal},
  month        = {12},
  number       = {8},
  pages        = {1896-1908},
  shortjournal = {Bio. J.},
  title        = {Isolating cost drivers in interstitial lung disease treatment using nonparametric bayesian methods},
  volume       = {62},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Generalized estimating equations approach for spatial
lattice data: A case study in adoption of improved maize varieties in
mozambique. <em>BIMJ</em>, <em>62</em>(8), 1879–1895. (<a
href="https://doi.org/10.1002/bimj.201800360">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Generalized estimating equations (GEE) are extension of generalized linear models (GLM) widely applied in longitudinal data analysis. GEE are also applied in spatial data analysis using geostatistics methods. In this paper, we advocate application of GEE for spatial lattice data by modeling the spatial working correlation matrix using the Moran&#39;s index and the spatial weight matrix. We present theoretical developments and results for simulated and actual data as well. For the former case, 1,000 samples of a random variable (response variable) defined in (0, 1) interval were generated using different values of the Moran&#39;s index. In addition, 1,000 samples of a binary and a continuous variable were also randomly generated as covariates. In each sample, three structures of spatial working correlation matrices were used while modeling: The independent, autoregressive, and the Toeplitz structure. Two measures were used to evaluate the performance of each of the spatial working correlation structures: the asymptotic relative efficiency and the working correlation selection criterions. The results showed that both measures indicated that the autoregressive spatial working correlation matrix proposed in this paper presents the best performance in general. For the actual data case, the proportion of small farmers who used improved maize varieties was considered as the response variable and a set of nine variables were used as covariates. Two structures of spatial working correlation matrices were used and the results showed consistence with those obtained in the simulation study.},
  archive      = {J_BIMJ},
  author       = {Lourenço Manuel and João D. Scalon},
  doi          = {10.1002/bimj.201800360},
  journal      = {Biometrical Journal},
  month        = {12},
  number       = {8},
  pages        = {1879-1895},
  shortjournal = {Bio. J.},
  title        = {Generalized estimating equations approach for spatial lattice data: A case study in adoption of improved maize varieties in mozambique},
  volume       = {62},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Empirical bayes small area prediction under a zero-inflated
lognormal model with correlated random area effects. <em>BIMJ</em>,
<em>62</em>(8), 1859–1878. (<a
href="https://doi.org/10.1002/bimj.202000029">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many variables of interest in agricultural or economical surveys have skewed distributions and can equal zero. Our data are measures of sheet and rill erosion called Revised Universal Soil Loss Equation - 2 (RUSLE2). Small area estimates of mean RUSLE2 erosion are of interest. We use a zero-inflated lognormal mixed effects model for small area estimation. The model combines a unit-level lognormal model for the positive RUSLE2 responses with a unit-level logistic mixed effects model for the binary indicator that the response is nonzero. In the Conservation Effects Assessment Project (CEAP) data, counties with a higher probability of nonzero responses also tend to have a higher mean among the positive RUSLE2 values. We capture this property of the data through an assumption that the pair of random effects for a county are correlated. We develop empirical Bayes (EB) small area predictors and a bootstrap estimator of the mean squared error (MSE). In simulations, the proposed predictor is superior to simpler alternatives. We then apply the method to construct EB predictors of mean RUSLE2 erosion for South Dakota counties. To obtain auxiliary variables for the population of cropland in South Dakota, we integrate a satellite-derived land cover map with a geographic database of soil properties. We provide an R Shiny application called viscover (available at https://lyux.shinyapps.io/viscover/ ) to visualize the overlay operations required to construct the covariates. On the basis of bootstrap estimates of the mean square error, we conclude that the EB predictors of mean RUSLE2 erosion are superior to direct estimators.},
  archive      = {J_BIMJ},
  author       = {Xiaodan Lyu and Emily J. Berg and Heike Hofmann},
  doi          = {10.1002/bimj.202000029},
  journal      = {Biometrical Journal},
  month        = {12},
  number       = {8},
  pages        = {1859-1878},
  shortjournal = {Bio. J.},
  title        = {Empirical bayes small area prediction under a zero-inflated lognormal model with correlated random area effects},
  volume       = {62},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A generalized transition model for grouped longitudinal
categorical data. <em>BIMJ</em>, <em>62</em>(8), 1837–1858. (<a
href="https://doi.org/10.1002/bimj.201900394">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Transition models are an important framework that can be used to model longitudinal categorical data. They are particularly useful when the primary interest is in prediction. The available methods for this class of models are suitable for the cases in which responses are recorded individually over time. However, in many areas, it is common for categorical data to be recorded as groups, that is, different categories with a number of individuals in each. As motivation we consider a study in insect movement and another in pig behaviou. The first study was developed to understand the movement patterns of female adults of Diaphorina citri , a pest of citrus plantations. The second study investigated how hogs behaved under the influence of environmental enrichment. In both studies, the number of individuals in different response categories was observed over time. We propose a new framework for considering the time dependence in the linear predictor of a generalized logit transition model using a quantitative response, corresponding to the number of individuals in each category. We use maximum likelihood estimation and present the results of the fitted models under stationarity and non-stationarity assumptions, and use recently proposed tests to assess non-stationarity. We evaluated the performance of the proposed model using simulation studies under different scenarios, and concluded that our modeling framework represents a flexible alternative to analyze grouped longitudinal categorical data.},
  archive      = {J_BIMJ},
  author       = {Idemauro A. R. Lara and Rafael A. Moral and Cesar A. Taconeli and Carolina Reigada and John Hinde},
  doi          = {10.1002/bimj.201900394},
  journal      = {Biometrical Journal},
  month        = {12},
  number       = {8},
  pages        = {1837-1858},
  shortjournal = {Bio. J.},
  title        = {A generalized transition model for grouped longitudinal categorical data},
  volume       = {62},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020h). Contents: Biometrical journal 8’20. <em>BIMJ</em>,
<em>62</em>(8), 1835–1836. (<a
href="https://doi.org/10.1002/bimj.202070084">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_BIMJ},
  doi          = {10.1002/bimj.202070084},
  journal      = {Biometrical Journal},
  month        = {12},
  number       = {8},
  pages        = {1835-1836},
  shortjournal = {Bio. J.},
  title        = {Contents: Biometrical journal 8&#39;20},
  volume       = {62},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020g). Cover picture: Biometrical journal 7’20. <em>BIMJ</em>,
<em>62</em>(7), NA. (<a
href="https://doi.org/10.1002/bimj.202070071">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_BIMJ},
  doi          = {10.1002/bimj.202070071},
  journal      = {Biometrical Journal},
  month        = {11},
  number       = {7},
  pages        = {NA},
  shortjournal = {Bio. J.},
  title        = {Cover picture: Biometrical journal 7&#39;20},
  volume       = {62},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Rank and pseudo-rank procedures for independent observations
in factorial designs—using r and SAS edgar BrunnerArne c. BathkeFrank
konietschke (2019). Springer series in statistics, springer, heidelberg.
521 pages. ISBN: 9783030029128. <em>BIMJ</em>, <em>62</em>(7), 1827. (<a
href="https://doi.org/10.1002/bimj.202000243">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_BIMJ},
  author       = {George Luta},
  doi          = {10.1002/bimj.202000243},
  journal      = {Biometrical Journal},
  month        = {11},
  number       = {7},
  pages        = {1827},
  shortjournal = {Bio. J.},
  title        = {Rank and pseudo-rank procedures for independent observations in factorial designs—using r and SAS edgar BrunnerArne c. BathkeFrank konietschke (2019). springer series in statistics, springer, heidelberg. 521 pages. ISBN: 9783030029128.},
  volume       = {62},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Introduction to statistical decision theory silvia bacci
bruno chiandotto (2019). New york: Chapman and hall/CRC, 303 pages,
ISBN: 9781315112220. <em>BIMJ</em>, <em>62</em>(7), 1826. (<a
href="https://doi.org/10.1002/bimj.202000229">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_BIMJ},
  author       = {Silvia Calderazzo},
  doi          = {10.1002/bimj.202000229},
  journal      = {Biometrical Journal},
  month        = {11},
  number       = {7},
  pages        = {1826},
  shortjournal = {Bio. J.},
  title        = {Introduction to statistical decision theory silvia bacci bruno chiandotto (2019). new york: chapman and Hall/CRC, 303 pages, ISBN: 9781315112220.},
  volume       = {62},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Design-based mapping of tree attributes by 3P sampling.
<em>BIMJ</em>, <em>62</em>(7), 1810–1825. (<a
href="https://doi.org/10.1002/bimj.201900377">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The estimation of individual values (marks) in a finite population of units (e.g., trees) scattered onto a survey region is considered under 3P sampling. For each unit, the mark is estimated by means of an inverse distance weighting interpolator. Conditions ensuring the design-based consistency of maps are considered under 3P sampling. A computationally simple mean squared error estimator is adopted. Because 3P sampling involves the prediction of marks for each unit in the population, prediction errors rather than marks can be interpolated. Then, marks are estimated by the predictions plus the interpolated errors. If predictions are good, prediction errors are more smoothed than raw marks so that the procedure is likely to better meet consistency requirements. The purpose of this paper is to provide theoretical and empirical evidence on the effectiveness of the interpolation based on prediction errors to prove that the proposed strategy is a tool of general validity for mapping forest stands.},
  archive      = {J_BIMJ},
  author       = {Lorenzo Fattorini and Sara Franceschi and Piermaria Corona},
  doi          = {10.1002/bimj.201900377},
  journal      = {Biometrical Journal},
  month        = {11},
  number       = {7},
  pages        = {1810-1825},
  shortjournal = {Bio. J.},
  title        = {Design-based mapping of tree attributes by 3P sampling},
  volume       = {62},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Parametric mode regression for bounded responses.
<em>BIMJ</em>, <em>62</em>(7), 1791–1809. (<a
href="https://doi.org/10.1002/bimj.202000039">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose new parametric frameworks of regression analysis with the conditional mode of a bounded response as the focal point of interest. Covariate effects estimation and prediction based on the maximum likelihood method under two new classes of regression models are demonstrated. We also develop graphical and numerical diagnostic tools to detect various sources of model misspecification. Predictions based on different central tendency measures inferred using various regression models are compared using synthetic data in simulations. Finally, we conduct regression analysis for data from the Alzheimer&#39;s Disease Neuroimaging Initiative to demonstrate practical implementation of the proposed methods. Supporting Information that contain technical details and additional simulation and data analysis results are available online.},
  archive      = {J_BIMJ},
  author       = {Haiming Zhou and Xianzheng Huang},
  doi          = {10.1002/bimj.202000039},
  journal      = {Biometrical Journal},
  month        = {11},
  number       = {7},
  pages        = {1791-1809},
  shortjournal = {Bio. J.},
  title        = {Parametric mode regression for bounded responses},
  volume       = {62},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Bayesian confidence intervals for the difference between
variances of delta-lognormal distributions. <em>BIMJ</em>,
<em>62</em>(7), 1769–1790. (<a
href="https://doi.org/10.1002/bimj.201900079">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unnatural rainfall fluctuation can result in such severe natural phenomena as drought and floods. This variability not only occurs in areas with unusual natural features such as land formations and drainage but can also be due to human intervention. Since rainfall data often contain zero values, evaluating rainfall change is an important undertaking, which can be estimated via the confidence intervals for the difference between delta-lognormal variances using the highest posterior density–based reference (HPD-ref) and probability-matching (HPD-pm) priors. Simulation results indicate that HPD-pm performances were better than other methods in terms of coverage rates and relative average lengths for the difference in delta-lognormal variances, even with a large difference in variances. To illustrate the efficacy of our proposed methods, we applied them to daily rainfall data sets for the lower and upper regions of northern Thailand.},
  archive      = {J_BIMJ},
  author       = {Patcharee Maneerat and Sa-Aat Niwitpong and Suparat Niwitpong},
  doi          = {10.1002/bimj.201900079},
  journal      = {Biometrical Journal},
  month        = {11},
  number       = {7},
  pages        = {1769-1790},
  shortjournal = {Bio. J.},
  title        = {Bayesian confidence intervals for the difference between variances of delta-lognormal distributions},
  volume       = {62},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A pseudo-likelihood method for estimating misclassification
probabilities in competing-risks settings when true-event data are
partially observed. <em>BIMJ</em>, <em>62</em>(7), 1747–1768. (<a
href="https://doi.org/10.1002/bimj.201900198">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Outcome misclassification occurs frequently in binary-outcome studies and can result in biased estimation of quantities such as the incidence, prevalence, cause-specific hazards, cumulative incidence functions, and so forth. A number of remedies have been proposed to address the potential misclassification of the outcomes in such data. The majority of these remedies lie in the estimation of misclassification probabilities, which are in turn used to adjust analyses for outcome misclassification. A number of authors advocate using a gold-standard procedure on a sample internal to the study to learn about the extent of the misclassification. With this type of internal validation, the problem of quantifying the misclassification also becomes a missing data problem as, by design, the true outcomes are only ascertained on a subset of the entire study sample. Although, the process of estimating misclassification probabilities appears simple conceptually, the estimation methods proposed so far have several methodological and practical shortcomings. Most methods rely on missing outcome data to be missing completely at random (MCAR), a rather stringent assumption which is unlikely to hold in practice. Some of the existing methods also tend to be computationally-intensive. To address these issues, we propose a computationally-efficient, easy-to-implement, pseudo-likelihood estimator of the misclassification probabilities under a missing at random (MAR) assumption, in studies with an available internal-validation sample. We present the estimator through the lens of studies with competing-risks outcomes, though the estimator extends beyond this setting. We describe the consistency and asymptotic distributional properties of the resulting estimator, and derive a closed-form estimator of its variance. The finite-sample performance of this estimator is evaluated via simulations. Using data from a real-world study with competing-risks outcomes, we illustrate how the proposed method can be used to estimate misclassification probabilities. We also show how the estimated misclassification probabilities can be used in an external study to adjust for possible misclassification bias when modeling cumulative incidence functions.},
  archive      = {J_BIMJ},
  author       = {Philani B. Mpofu and Giorgos Bakoyannis and Constantin T. Yiannoutsos and Ann W. Mwangi and Margaret Mburu},
  doi          = {10.1002/bimj.201900198},
  journal      = {Biometrical Journal},
  month        = {11},
  number       = {7},
  pages        = {1747-1768},
  shortjournal = {Bio. J.},
  title        = {A pseudo-likelihood method for estimating misclassification probabilities in competing-risks settings when true-event data are partially observed},
  volume       = {62},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Compound optimal allocations for survival clinical trials.
<em>BIMJ</em>, <em>62</em>(7), 1730–1746. (<a
href="https://doi.org/10.1002/bimj.201900232">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The aim of the present paper is to provide optimal allocations for comparative clinical trials with survival outcomes. The suggested targets are derived adopting a compound optimization strategy based on a subjective weighting of the relative importance of inferential demands and ethical concerns. The ensuing compound optimal targets are continuous functions of the treatment effects, so we provide the conditions under which they can be approached by standard response-adaptive randomization procedures, also guaranteeing the applicability of the classical asymptotic inference. The operating characteristics of the suggested methodology are verified both theoretically and by simulation, including the robustness to model misspecification. With respect to the other available proposals, our strategy always assigns more patients to the best treatment without compromising inference, taking into account estimation efficiency and power as well. We illustrate our procedure by redesigning two real oncological trials.},
  archive      = {J_BIMJ},
  author       = {Alessandro Baldi Antognini and Marco Novelli and Maroussa Zagoraiou and Alessandro Vagheggini},
  doi          = {10.1002/bimj.201900232},
  journal      = {Biometrical Journal},
  month        = {11},
  number       = {7},
  pages        = {1730-1746},
  shortjournal = {Bio. J.},
  title        = {Compound optimal allocations for survival clinical trials},
  volume       = {62},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Using a dose-finding benchmark to quantify the loss incurred
by dichotomization in phase II dose-ranging studies. <em>BIMJ</em>,
<em>62</em>(7), 1717–1729. (<a
href="https://doi.org/10.1002/bimj.201900332">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {While there is recognition that more informative clinical endpoints can support better decision-making in clinical trials, it remains a common practice to categorize endpoints originally measured on a continuous scale. The primary motivation for this categorization (and most commonly dichotomization) is the simplicity of the analysis. There is, however, a long argument that this simplicity can come at a high cost. Specifically, larger sample sizes are needed to achieve the same level of accuracy when using a dichotomized outcome instead of the original continuous endpoint. The degree of “loss of information” has been studied in the contexts of parallel-group designs and two-stage Phase II trials. Limited attention, however, has been given to the quantification of the associated losses in dose-ranging trials. In this work, we propose an approach to estimate the associated losses in Phase II dose-ranging trials that is free of the actual dose-ranging design used and depends on the clinical setting only. The approach uses the notion of a nonparametric optimal benchmark for dose-finding trials, an evaluation tool that facilitates the assessment of a dose-finding design by providing an upper bound on its performance under a given scenario in terms of the probability of the target dose selection. After demonstrating how the benchmark can be applied to Phase II dose-ranging trials, we use it to quantify the dichotomization losses. Using parameters from real clinical trials in various therapeutic areas, it is found that the ratio of sample sizes needed to obtain the same precision using continuous and binary (dichotomized) endpoints varies between 70% and 75% under the majority of scenarios but can drop to 50% in some cases.},
  archive      = {J_BIMJ},
  author       = {Pavel Mozgunov and Thomas Jaki and Xavier Paoletti},
  doi          = {10.1002/bimj.201900332},
  journal      = {Biometrical Journal},
  month        = {11},
  number       = {7},
  pages        = {1717-1729},
  shortjournal = {Bio. J.},
  title        = {Using a dose-finding benchmark to quantify the loss incurred by dichotomization in phase II dose-ranging studies},
  volume       = {62},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Impact of adolescent obesity on middle-age health of women
given data MAR. <em>BIMJ</em>, <em>62</em>(7), 1702–1716. (<a
href="https://doi.org/10.1002/bimj.201900094">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We analyze adolescent BMI and middle-age systolic blood pressure (SBP) repeatedly measured on women enrolled in the Fels Longitudinal Study (FLS) between 1929 and 2010 to address three questions: Do adolescent-specific growth rates in body mass index (BMI) and menarche affect middle-age SBP? Do they moderate the aging effect on middle-age SBP? Have the effects changed over historical time? To address the questions, we propose analyzing a growth curve model (GCM) that controls for age, birth-year cohort, and historical time. However, several complications in the data make the GCM analysis nonstandard. First, the person-specific adolescent BMI and middle-age SBP trajectories are unobservable. Second, missing data are substantial on BMI, SBP, and menarche. Finally, modeling the latent trajectories for BMI and SBP, repeatedly measured on two distinct sets of unbalanced time points, are computationally intensive. We adopt a bivariate GCM for BMI and SBP with correlated random coefficients. To efficiently handle missing values of BMI, SBP, and menarche assumed missing at random, we estimate their joint distribution by maximum likelihood via the EM algorithm where the correlated random coefficients and menarche are multivariate normal. The estimated distribution will be transformed to the desired GCM for SBP that includes the random coefficients of BMI and menarche as covariates. We demonstrate unbiased estimation by simulation. We find that adolescent growth rates in BMI and menarche are positively associated with, and moderate, the aging effect on SBP in middle age, controlling for age, cohort, and historical time, but the effect sizes are at most modest. The aging effect is significant on SBP, controlling for cohort and historical time, but not vice versa.},
  archive      = {J_BIMJ},
  author       = {Yongyun Shin and Shumei Sun and Dipankar Bandyopadhyay},
  doi          = {10.1002/bimj.201900094},
  journal      = {Biometrical Journal},
  month        = {11},
  number       = {7},
  pages        = {1702-1716},
  shortjournal = {Bio. J.},
  title        = {Impact of adolescent obesity on middle-age health of women given data MAR},
  volume       = {62},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Measuring intrarater association between correlated ordinal
ratings. <em>BIMJ</em>, <em>62</em>(7), 1687–1701. (<a
href="https://doi.org/10.1002/bimj.201900177">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Variability between raters&#39; ordinal scores is commonly observed in imaging tests, leading to uncertainty in the diagnostic process. In breast cancer screening, a radiologist visually interprets mammograms and MRIs, while skin diseases, Alzheimer&#39;s disease, and psychiatric conditions are graded based on clinical judgment. Consequently, studies are often conducted in clinical settings to investigate whether a new training tool can improve the interpretive performance of raters. In such studies, a large group of experts each classify a set of patients&#39; test results on two separate occasions, before and after some form of training with the goal of assessing the impact of training on experts&#39; paired ratings. However, due to the correlated nature of the ordinal ratings, few statistical approaches are available to measure association between raters&#39; paired scores. Existing measures are restricted to assessing association at just one time point for a single screening test. We propose here a novel paired kappa to provide a summary measure of association between many raters&#39; paired ordinal assessments of patients&#39; test results before versus after rater training. Intrarater association also provides valuable insight into the consistency of ratings when raters view a patient&#39;s test results on two occasions with no intervention undertaken between viewings. In contrast to existing correlated measures, the proposed kappa is a measure that provides an overall evaluation of the association among multiple raters&#39; scores from two time points and is robust to the underlying disease prevalence. We implement our proposed approach in two recent breast-imaging studies and conduct extensive simulation studies to evaluate properties and performance of our summary measure of association.},
  archive      = {J_BIMJ},
  author       = {Kerrie P. Nelson and Thomas J. Zhou and Don Edwards},
  doi          = {10.1002/bimj.201900177},
  journal      = {Biometrical Journal},
  month        = {11},
  number       = {7},
  pages        = {1687-1701},
  shortjournal = {Bio. J.},
  title        = {Measuring intrarater association between correlated ordinal ratings},
  volume       = {62},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Variable selection with p-splines in functional linear
regression: Application in graft-versus-host disease. <em>BIMJ</em>,
<em>62</em>(7), 1670–1686. (<a
href="https://doi.org/10.1002/bimj.201900189">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper focuses on the problems of estimation and variable selection in the functional linear regression model (FLM) with functional response and scalar covariates. To this end, two different types of regularization ( L 1 and L 2 ) are considered in this paper. On the one hand, a sample approach for functional LASSO in terms of basis representation of the sample values of the response variable is proposed. On the other hand, we propose a penalized version of the FLM by introducing a P-spline penalty in the least squares fitting criterion. But our aim is to propose P-splines as a powerful tool simultaneously for variable selection and functional parameters estimation. In that sense, the importance of smoothing the response variable before fitting the model is also studied. In summary, penalized ( L 1 and L 2 ) and nonpenalized regression are combined with a presmoothing of the response variable sample curves, based on regression splines or P-splines, providing a total of six approaches to be compared in two simulation schemes. Finally, the most competitive approach is applied to a real data set based on the graft-versus-host disease, which is one of the most frequent complications (30% –50%) in allogeneic hematopoietic stem-cell transplantation.},
  archive      = {J_BIMJ},
  author       = {M. Carmen Aguilera-Morillo and Ismael Buño and Rosa E. Lillo and Juan Romo},
  doi          = {10.1002/bimj.201900189},
  journal      = {Biometrical Journal},
  month        = {11},
  number       = {7},
  pages        = {1670-1686},
  shortjournal = {Bio. J.},
  title        = {Variable selection with P-splines in functional linear regression: Application in graft-versus-host disease},
  volume       = {62},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A flexible hierarchical framework for improving inference in
area-referenced environmental health studies. <em>BIMJ</em>,
<em>62</em>(7), 1650–1669. (<a
href="https://doi.org/10.1002/bimj.201900241">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Study designs where data have been aggregated by geographical areas are popular in environmental epidemiology. These studies are commonly based on administrative databases and, providing a complete spatial coverage, are particularly appealing to make inference on the entire population. However, the resulting estimates are often biased and difficult to interpret due to unmeasured confounders, which typically are not available from routinely collected data. We propose a framework to improve inference drawn from such studies exploiting information derived from individual-level survey data. The latter are summarized in an area-level scalar score by mimicking at ecological level the well-known propensity score methodology. The literature on propensity score for confounding adjustment is mainly based on individual-level studies and assumes a binary exposure variable. Here, we generalize its use to cope with area-referenced studies characterized by a continuous exposure. Our approach is based upon Bayesian hierarchical structures specified into a two-stage design: (i) geolocated individual-level data from survey samples are up-scaled at ecological level, then the latter are used to estimate a generalized ecological propensity score (EPS) in the in-sample areas; (ii) the generalized EPS is imputed in the out-of-sample areas under different assumptions about the missingness mechanisms, then it is included into the ecological regression, linking the exposure of interest to the health outcome. This delivers area-level risk estimates, which allow a fuller adjustment for confounding than traditional areal studies. The methodology is illustrated by using simulations and a case study investigating the risk of lung cancer mortality associated with nitrogen dioxide in England (UK).},
  archive      = {J_BIMJ},
  author       = {Monica Pirani and Alexina J. Mason and Anna L. Hansell and Sylvia Richardson and Marta Blangiardo},
  doi          = {10.1002/bimj.201900241},
  journal      = {Biometrical Journal},
  month        = {11},
  number       = {7},
  pages        = {1650-1669},
  shortjournal = {Bio. J.},
  title        = {A flexible hierarchical framework for improving inference in area-referenced environmental health studies},
  volume       = {62},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Bayesian profiling for cost with zeros to decompose total
cost into probability of cost and mean nonzero cost. <em>BIMJ</em>,
<em>62</em>(7), 1631–1649. (<a
href="https://doi.org/10.1002/bimj.201900148">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cost of health care can vary substantially across hospitals, centers, or providers. Data from electronic health records provide information for studying patterns of cost variation and identifying high or low cost centers. Cost data often include zero values when patients receive no care, and joint two-part models have been developed for clustered cost data with zeros. Standard methods for center comparisons, sometimes called profiling, can use these methods to incorporate zero values into total cost. However, zero costs also provide opportunities to further examine sources of cost variation and outliers. For example, a hospital may have high (or low) cost due to frequency of nonzero cost, amount of nonzero cost, or a combination of those. We give methods for decomposing hospital differences in total cost with zeros into components for probability of use (i.e., of nonzero cost) and for cost of use (mean of nonzero cost). The components multiply to total cost and quantify components on the same easily interpreted multiplicative scales. The methods are based on Bayesian hierarchical models and counterfactual arguments, with Markov chain Monte Carlo estimation. We used simulated data to illustrate use, interpretation, and visualization of the methods in diverse situations, and applied the methods to 30,024 patients at 57 US Veterans Administration hospitals to characterize outlier hospitals in one year cost of inpatient care following a cardiac procedure. Twenty eight percent of patients had zero cost. These methods are useful in providing insight into cost variation and outliers for planning future studies or interventions.},
  archive      = {J_BIMJ},
  author       = {Gary K. Grunwald and James A. Arnett and Wenhui Liu and P. Michael Ho},
  doi          = {10.1002/bimj.201900148},
  journal      = {Biometrical Journal},
  month        = {11},
  number       = {7},
  pages        = {1631-1649},
  shortjournal = {Bio. J.},
  title        = {Bayesian profiling for cost with zeros to decompose total cost into probability of cost and mean nonzero cost},
  volume       = {62},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Comparison of random-effects meta-analysis models for the
relative risk in the case of rare events: A simulation study.
<em>BIMJ</em>, <em>62</em>(7), 1597–1630. (<a
href="https://doi.org/10.1002/bimj.201900379">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Pooling the relative risk (RR) across studies investigating rare events, for example, adverse events, via meta-analytical methods still presents a challenge to researchers. The main reason for this is the high probability of observing no events in treatment or control group or both, resulting in an undefined log RR (the basis of standard meta-analysis). Other technical challenges ensue, for example, the violation of normality assumptions, or bias due to exclusion of studies and application of continuity corrections, leading to poor performance of standard approaches. In the present simulation study, we compared three recently proposed alternative models (random-effects [RE] Poisson regression, RE zero-inflated Poisson [ZIP] regression, binomial regression) to the standard methods in conjunction with different continuity corrections and to different versions of beta-binomial regression. Based on our investigation of the models&#39; performance in 162 different simulation settings informed by meta-analyses from the Cochrane database and distinguished by different underlying true effects, degrees of between-study heterogeneity, numbers of primary studies, group size ratios, and baseline risks, we recommend the use of the RE Poisson regression model. The beta-binomial model recommended by Kuss (2015) also performed well. Decent performance was also exhibited by the ZIP models, but they also had considerable convergence issues. We stress that these recommendations are only valid for meta-analyses with larger numbers of primary studies. All models are applied to data from two Cochrane reviews to illustrate differences between and issues of the models. Limitations as well as practical implications and recommendations are discussed; a flowchart summarizing recommendations is provided.},
  archive      = {J_BIMJ},
  author       = {Marie Beisemann and Philipp Doebler and Heinz Holling},
  doi          = {10.1002/bimj.201900379},
  journal      = {Biometrical Journal},
  month        = {11},
  number       = {7},
  pages        = {1597-1630},
  shortjournal = {Bio. J.},
  title        = {Comparison of random-effects meta-analysis models for the relative risk in the case of rare events: A simulation study},
  volume       = {62},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020g). Contents: Biometrical journal 7’20. <em>BIMJ</em>,
<em>62</em>(7), 1595–1596. (<a
href="https://doi.org/10.1002/bimj.202070074">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_BIMJ},
  doi          = {10.1002/bimj.202070074},
  journal      = {Biometrical Journal},
  month        = {11},
  number       = {7},
  pages        = {1595-1596},
  shortjournal = {Bio. J.},
  title        = {Contents: Biometrical journal 7&#39;20},
  volume       = {62},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020f). Cover picture: Biometrical journal 6’20. <em>BIMJ</em>,
<em>62</em>(6), NA. (<a
href="https://doi.org/10.1002/bimj.202070061">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_BIMJ},
  doi          = {10.1002/bimj.202070061},
  journal      = {Biometrical Journal},
  month        = {10},
  number       = {6},
  pages        = {NA},
  shortjournal = {Bio. J.},
  title        = {Cover picture: Biometrical journal 6&#39;20},
  volume       = {62},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Testing against umbrella or tree orderings for binomial
proportions with an adaptation of an insect resistance case.
<em>BIMJ</em>, <em>62</em>(6), 1574–1588. (<a
href="https://doi.org/10.1002/bimj.201900272">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Alternative hypotheses for order restrictions, such as umbrella or inverse umbrella (a.k.a tree) orderings, have been studied extensively in the literature, although less so when the studied response for each individual is the presence or absence of the event of interest. Two families of test statistics for solving the problem of testing against an umbrella or a tree ordering when the responses are binomial proportions are studied in this work and their asymptotic distributions are derived. A simulation study is conducted to compare the empirical power of some members of the derived families of test statistics with competing approaches. The methodology developed here was driven by an applied problem arising in stored products research where despite universal mortality in the case of doses of 1000 ppm of the insecticide phosphine, unexpected survival was noted at higher doses.},
  archive      = {J_BIMJ},
  author       = {A. M. Franco-Pereira and C. T. Nakas and C. G. Athanassiou and M. C. Pardo},
  doi          = {10.1002/bimj.201900272},
  journal      = {Biometrical Journal},
  month        = {10},
  number       = {6},
  pages        = {1574-1588},
  shortjournal = {Bio. J.},
  title        = {Testing against umbrella or tree orderings for binomial proportions with an adaptation of an insect resistance case},
  volume       = {62},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A note on the interpretation of tree-based regression
models. <em>BIMJ</em>, <em>62</em>(6), 1564–1573. (<a
href="https://doi.org/10.1002/bimj.201900195">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Tree-based models are a popular tool for predicting a response given a set of explanatory variables when the regression function is characterized by a certain degree of complexity. Sometimes, they are also used to identify important variables and for variable selection. We show that if the generating model contains chains of direct and indirect effects, then the typical variable importance measures suggest selecting as important mainly the background variables, which have a strong indirect effect, disregarding the variables that directly influence the response. This is attributable mainly to the variable choice in the first steps of the algorithm selecting the splitting variable and to the greedy nature of such search. This pitfall could be relevant when using tree-based algorithms for understanding the underlying generating process, for population segmentation and for causal inference.},
  archive      = {J_BIMJ},
  author       = {Anna Gottard and Giulia Vannucci and Giovanni Maria Marchetti},
  doi          = {10.1002/bimj.201900195},
  journal      = {Biometrical Journal},
  month        = {10},
  number       = {6},
  pages        = {1564-1573},
  shortjournal = {Bio. J.},
  title        = {A note on the interpretation of tree-based regression models},
  volume       = {62},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A weighted FDR procedure under discrete and heterogeneous
null distributions. <em>BIMJ</em>, <em>62</em>(6), 1544–1563. (<a
href="https://doi.org/10.1002/bimj.201900216">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multiple testing (MT) with false discovery rate (FDR) control has been widely conducted in the “discrete paradigm” where p -values have discrete and heterogeneous null distributions. However, in this scenario existing FDR procedures often lose some power and may yield unreliable inference, and for this scenario there does not seem to be an FDR procedure that partitions hypotheses into groups, employs data-adaptive weights and is nonasymptotically conservative. We propose a weighted p -value-based FDR procedure, “weighted FDR (wFDR) procedure” for short, for MT in the discrete paradigm that efficiently adapts to both heterogeneity and discreteness of p -value distributions. We theoretically justify the nonasymptotic conservativeness of the wFDR procedure under independence, and show via simulation studies that, for MT based on p -values of binomial test or Fisher&#39;s exact test, it is more powerful than six other procedures. The wFDR procedure is applied to two examples based on discrete data, a drug safety study, and a differential methylation study, where it makes more discoveries than two existing methods.},
  archive      = {J_BIMJ},
  author       = {Xiongzhi Chen and R. W. Doerge and Sanat K. Sarkar},
  doi          = {10.1002/bimj.201900216},
  journal      = {Biometrical Journal},
  month        = {10},
  number       = {6},
  pages        = {1544-1563},
  shortjournal = {Bio. J.},
  title        = {A weighted FDR procedure under discrete and heterogeneous null distributions},
  volume       = {62},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Allometric analysis using the multivariate shifted
exponential normal distribution. <em>BIMJ</em>, <em>62</em>(6),
1525–1543. (<a href="https://doi.org/10.1002/bimj.201900248">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In allometric studies, the joint distribution of the log-transformed morphometric variables is typically elliptical and with heavy tails. To account for these peculiarities, we introduce the multivariate shifted exponential normal (MSEN) distribution , an elliptical heavy-tailed generalization of the multivariate normal (MN). The MSEN belongs to the family of MN scale mixtures (MNSMs) by choosing a convenient shifted exponential as mixing distribution. The probability density function of the MSEN has a simple closed-form characterized by only one additional parameter, with respect to the nested MN, governing the tail weight. The first four moments exist and the excess kurtosis can assume any positive value. The membership to the family of MNSMs allows us a simple computation of the maximum likelihood (ML) estimates of the parameters via the expectation-maximization (EM) algorithm; advantageously, the M-step is computationally simplified by closed-form updates of all the parameters. We also evaluate the existence of the ML estimates. Since the parameter governing the tail weight is estimated from the data, robust estimates of the mean vector of the nested MN distribution are automatically obtained by downweighting; we show this aspect theoretically but also by means of a simulation study. We fit the MSEN distribution to multivariate allometric data where we show its usefulness also in comparison with other well-established multivariate elliptical distributions.},
  archive      = {J_BIMJ},
  author       = {Antonio Punzo and Luca Bagnato},
  doi          = {10.1002/bimj.201900248},
  journal      = {Biometrical Journal},
  month        = {10},
  number       = {6},
  pages        = {1525-1543},
  shortjournal = {Bio. J.},
  title        = {Allometric analysis using the multivariate shifted exponential normal distribution},
  volume       = {62},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A model with space-varying regression coefficients for
clustering multivariate spatial count data. <em>BIMJ</em>,
<em>62</em>(6), 1508–1524. (<a
href="https://doi.org/10.1002/bimj.201900229">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multivariate spatial count data are often segmented by unobserved space-varying factors that vary across space. In this setting, regression models that assume space-constant covariate effects could be too restrictive. Motivated by the analysis of cause-specific mortality data, we propose to estimate space-varying effects by exploiting a multivariate hidden Markov field. It models the data by a battery of Poisson regressions with spatially correlated regression coefficients, which are driven by an unobserved spatial multinomial process. It parsimoniously describes multivariate count data by means of a finite number of latent classes. Parameter estimation is carried out by composite likelihood methods, that we specifically develop for the proposed model. In a case study of cause-specific mortality data in Italy, the model was capable to capture the spatial variation of gender differences and age effects.},
  archive      = {J_BIMJ},
  author       = {Francesco Lagona and Monia Ranalli and Elisabetta Barbi},
  doi          = {10.1002/bimj.201900229},
  journal      = {Biometrical Journal},
  month        = {10},
  number       = {6},
  pages        = {1508-1524},
  shortjournal = {Bio. J.},
  title        = {A model with space-varying regression coefficients for clustering multivariate spatial count data},
  volume       = {62},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Spatial auto-correlation and auto-regressive models
estimation from sample survey data. <em>BIMJ</em>, <em>62</em>(6),
1494–1507. (<a href="https://doi.org/10.1002/bimj.201800225">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Maximum likelihood estimation of the model parameters for a spatial population based on data collected from a survey sample is usually straightforward when sampling and non-response are both non-informative, since the model can then usually be fitted using the available sample data, and no allowance is necessary for the fact that only a part of the population has been observed. Although for many regression models this naive strategy yields consistent estimates, this is not the case for some models, such as spatial auto-regressive models. In this paper, we show that for a broad class of such models, a maximum marginal likelihood approach that uses both sample and population data leads to more efficient estimates since it uses spatial information from sampled as well as non-sampled units. Extensive simulation experiments based on two well-known data sets are used to assess the impact of the spatial sampling design, the auto-correlation parameter and the sample size on the performance of this approach. When compared to some widely used methods that use only sample data, the results from these experiments show that the maximum marginal likelihood approach is much more precise.},
  archive      = {J_BIMJ},
  author       = {Roberto Benedetti and Thomas Suesse and Federica Piersimoni},
  doi          = {10.1002/bimj.201800225},
  journal      = {Biometrical Journal},
  month        = {10},
  number       = {6},
  pages        = {1494-1507},
  shortjournal = {Bio. J.},
  title        = {Spatial auto-correlation and auto-regressive models estimation from sample survey data},
  volume       = {62},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). The area between ROC curves, a non-parametric method to
evaluate a biomarker for patient treatment selection. <em>BIMJ</em>,
<em>62</em>(6), 1476–1493. (<a
href="https://doi.org/10.1002/bimj.201900171">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Treatment selection markers are generally sought for when the benefit of an innovative treatment in comparison with a reference treatment is considered, and this benefit is suspected to vary according to the characteristics of the patients. Classically, such quantitative markers are detected through testing a marker-by-treatment interaction in a parametric regression model. Most alternative methods rely on modeling the risk of event occurrence in each treatment arm or the benefit of the innovative treatment over the marker values, but with assumptions that may be difficult to verify. Herein, a simple non-parametric approach is proposed to detect and assess the general capacity of a quantitative marker for treatment selection when no overall difference in efficacy could be demonstrated between two treatments in a clinical trial. This graphical method relies on the area between treatment-arm-specific receiver operating characteristic curves (ABC), which reflects the treatment selection capacity of the marker. A simulation study assessed the inference properties of the ABC estimator and compared them with other parametric and non-parametric indicators. The simulations showed that the estimate of the ABC had low bias, power comparable to parametric indicators, and that its confidence interval had a good coverage probability (better than the other non-parametric indicator in some cases). Thus, the ABC is a good alternative to parametric indicators. The ABC method was applied to data of the PETACC-8 trial that investigated FOLFOX4 versus FOLFOX4 + cetuximab in stage III colon adenocarcinoma. It enabled the detection of a treatment selection marker: the DDR2 gene.},
  archive      = {J_BIMJ},
  author       = {Yoann Blangero and Muriel Rabilloud and Pierre Laurent-Puig and Karine Le Malicot and Côme Lepage and René Ecochard and Julien Taieb and Fabien Subtil},
  doi          = {10.1002/bimj.201900171},
  journal      = {Biometrical Journal},
  month        = {10},
  number       = {6},
  pages        = {1476-1493},
  shortjournal = {Bio. J.},
  title        = {The area between ROC curves, a non-parametric method to evaluate a biomarker for patient treatment selection},
  volume       = {62},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Nonparametric confidence regions for the symmetry
point-based optimal cutpoint and associated sensitivity of a
continuous-scale diagnostic test. <em>BIMJ</em>, <em>62</em>(6),
1463–1475. (<a href="https://doi.org/10.1002/bimj.201900222">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In medical research, diagnostic tests with continuous values are widely employed to attempt to distinguish between diseased and non-diseased subjects. The diagnostic accuracy of a test (or a biomarker) can be assessed by using the receiver operating characteristic (ROC) curve of the test. To summarize the ROC curve and primarily to determine an “optimal” threshold for test results to use in practice, several approaches may be considered, such as those based on the Youden index, on the so-called close-to-(0,1) point, on the concordance probability and on the symmetry point. In this paper, we focus on the symmetry point-based approach, that simultaneously controls the probabilities of the two types of correct classifications (healthy as healthy and diseased as diseased), and show how to get joint nonparametric confidence regions for the corresponding optimal cutpoint and the associated sensitivity (= specificity) value. Extensive simulation experiments are conducted to evaluate the finite sample performances of the proposed method. Real datasets are also used to illustrate its application.},
  archive      = {J_BIMJ},
  author       = {Gianfranco Adimari and Andrea Sinigaglia},
  doi          = {10.1002/bimj.201900222},
  journal      = {Biometrical Journal},
  month        = {10},
  number       = {6},
  pages        = {1463-1475},
  shortjournal = {Bio. J.},
  title        = {Nonparametric confidence regions for the symmetry point-based optimal cutpoint and associated sensitivity of a continuous-scale diagnostic test},
  volume       = {62},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Propensity score methods for time-dependent cluster
confounding. <em>BIMJ</em>, <em>62</em>(6), 1443–1462. (<a
href="https://doi.org/10.1002/bimj.201900277">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In observational studies, subjects are often nested within clusters. In medical studies, patients are often treated by doctors and therefore patients are regarded as nested or clustered within doctors. A concern that arises with clustered data is that cluster-level characteristics (e.g., characteristics of the doctor) are associated with both treatment selection and patient outcomes, resulting in cluster-level confounding. Measuring and modeling cluster attributes can be difficult and statistical methods exist to control for all unmeasured cluster characteristics. An assumption of these methods however is that characteristics of the cluster and the effects of those characteristics on the outcome (as well as probability of treatment assignment when using covariate balancing methods) are constant over time. In this paper, we consider methods that relax this assumption and allow for estimation of treatment effects in the presence of unmeasured time-dependent cluster confounding. The methods are based on matching with the propensity score and incorporate unmeasured time-specific cluster effects by performing matching within clusters or using fixed- or random-cluster effects in the propensity score model. The methods are illustrated using data to compare the effectiveness of two total hip devices with respect to survival of the device and a simulation study is performed that compares the proposed methods. One method that was found to perform well is matching within surgeon clusters partitioned by time. Considerations in implementing the proposed methods are discussed.},
  archive      = {J_BIMJ},
  author       = {Guy Cafri and Peter C. Austin},
  doi          = {10.1002/bimj.201900277},
  journal      = {Biometrical Journal},
  month        = {10},
  number       = {6},
  pages        = {1443-1462},
  shortjournal = {Bio. J.},
  title        = {Propensity score methods for time-dependent cluster confounding},
  volume       = {62},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Bayesian interval mapping of count trait loci based on
zero-inflated generalized poisson regression model. <em>BIMJ</em>,
<em>62</em>(6), 1428–1442. (<a
href="https://doi.org/10.1002/bimj.201900274">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Count phenotypes with excessive zeros are often observed in the biological world. Researchers have studied many statistical methods for mapping the quantitative trait loci (QTLs) of zero-inflated count phenotypes. However, most of the existing methods consist of finding the approximate positions of the QTLs on the chromosome by genome-wide scanning. Additionally, most of the existing methods use the EM algorithm for parameter estimation. In this paper, we propose a Bayesian interval mapping scheme of QTLs for zero-inflated count data. The method takes advantage of a zero-inflated generalized Poisson (ZIGP) regression model to study the influence of QTLs on the zero-inflated count phenotype. The MCMC algorithm is used to estimate the effects and position parameters of QTLs. We use the Haldane map function to realize the conversion between recombination rate and map distance. Monte Carlo simulations are conducted to test the applicability and advantage of the proposed method. The effects of QTLs on the formation of mouse cholesterol gallstones were demonstrated by analyzing an mouse data set.},
  archive      = {J_BIMJ},
  author       = {Jinling Chi and Ying Zhou and Lili Chen and Yajing Zhou},
  doi          = {10.1002/bimj.201900274},
  journal      = {Biometrical Journal},
  month        = {10},
  number       = {6},
  pages        = {1428-1442},
  shortjournal = {Bio. J.},
  title        = {Bayesian interval mapping of count trait loci based on zero-inflated generalized poisson regression model},
  volume       = {62},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A bayesian decision-theoretic approach to incorporate
preclinical information into phase i oncology trials. <em>BIMJ</em>,
<em>62</em>(6), 1408–1427. (<a
href="https://doi.org/10.1002/bimj.201900161">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Leveraging preclinical animal data for a phase I oncology trial is appealing yet challenging. In this paper, we use animal data to improve decision-making in a model-based dose-escalation procedure. We make a proposal for how to measure and address a prior-data conflict in a sequential study with a small sample size. Animal data are incorporated via a robust two-component mixture prior for the parameters of the human dose–toxicity relationship. The weights placed on each component of the prior are chosen empirically and updated dynamically as the trial progresses and more data accrue. After completion of each cohort, we use a Bayesian decision-theoretic approach to evaluate the predictive utility of the animal data for the observed human toxicity outcomes, reflecting the degree of agreement between dose–toxicity relationships in animals and humans. The proposed methodology is illustrated through several data examples and an extensive simulation study.},
  archive      = {J_BIMJ},
  author       = {Haiyan Zheng and Lisa V. Hampson},
  doi          = {10.1002/bimj.201900161},
  journal      = {Biometrical Journal},
  month        = {10},
  number       = {6},
  pages        = {1408-1427},
  shortjournal = {Bio. J.},
  title        = {A bayesian decision-theoretic approach to incorporate preclinical information into phase i oncology trials},
  volume       = {62},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Improved confidence intervals for a difference of two
cause-specific cumulative incidence functions estimated in the presence
of competing risks and random censoring. <em>BIMJ</em>, <em>62</em>(6),
1394–1407. (<a href="https://doi.org/10.1002/bimj.201900060">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A cause-specific cumulative incidence function (CIF) is the probability of failure from a specific cause as a function of time. In randomized trials, a difference of cause-specific CIFs (treatment minus control) represents a treatment effect. Cause-specific CIF in each intervention arm can be estimated based on the usual non-parametric Aalen–Johansen estimator which generalizes the Kaplan–Meier estimator of CIF in the presence of competing risks. Under random censoring, asymptotically valid Wald-type confidence intervals (CIs) for a difference of cause-specific CIFs at a specific time point can be constructed using one of the published variance estimators. Unfortunately, these intervals can suffer from substantial under-coverage when the outcome of interest is a rare event, as may be the case for example in the analysis of uncommon adverse events. We propose two new approximate interval estimators for a difference of cause-specific CIFs estimated in the presence of competing risks and random censoring. Theoretical analysis and simulations indicate that the new interval estimators are superior to the Wald CIs in the sense of avoiding substantial under-coverage with rare events, while being equivalent to the Wald CIs asymptotically. In the absence of censoring, one of the two proposed interval estimators reduces to the well-known Agresti–Caffo CI for a difference of two binomial parameters. The new methods can be easily implemented with any software package producing point and variance estimates for the Aalen–Johansen estimator, as illustrated in a real data example.},
  archive      = {J_BIMJ},
  author       = {Emil Scosyrev},
  doi          = {10.1002/bimj.201900060},
  journal      = {Biometrical Journal},
  month        = {10},
  number       = {6},
  pages        = {1394-1407},
  shortjournal = {Bio. J.},
  title        = {Improved confidence intervals for a difference of two cause-specific cumulative incidence functions estimated in the presence of competing risks and random censoring},
  volume       = {62},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Dynamic prediction of time to a clinical event with sparse
and irregularly measured longitudinal biomarkers. <em>BIMJ</em>,
<em>62</em>(6), 1371–1393. (<a
href="https://doi.org/10.1002/bimj.201900112">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In clinical research and practice, landmark models are commonly used to predict the risk of an adverse future event, using patients&#39; longitudinal biomarker data as predictors. However, these data are often observable only at intermittent visits, making their measurement times irregularly spaced and unsynchronized across different subjects. This poses challenges to conducting dynamic prediction at any post-baseline time. A simple solution is the last-value-carry-forward method, but this may result in bias for the risk model estimation and prediction. Another option is to jointly model the longitudinal and survival processes with a shared random effects model. However, when dealing with multiple biomarkers, this approach often results in high-dimensional integrals without a closed-form solution, and thus the computational burden limits its software development and practical use. In this article, we propose to process the longitudinal data by functional principal component analysis techniques, and then use the processed information as predictors in a class of flexible linear transformation models to predict the distribution of residual time-to-event occurrence. The measurement schemes for multiple biomarkers are allowed to be different within subject and across subjects. Dynamic prediction can be performed in a real-time fashion. The advantages of our proposed method are demonstrated by simulation studies. We apply our approach to the African American Study of Kidney Disease and Hypertension, predicting patients&#39; risk of kidney failure or death by using four important longitudinal biomarkers for renal functions.},
  archive      = {J_BIMJ},
  author       = {Yayuan Zhu and Xuelin Huang and Liang Li},
  doi          = {10.1002/bimj.201900112},
  journal      = {Biometrical Journal},
  month        = {10},
  number       = {6},
  pages        = {1371-1393},
  shortjournal = {Bio. J.},
  title        = {Dynamic prediction of time to a clinical event with sparse and irregularly measured longitudinal biomarkers},
  volume       = {62},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020f). Contents: Biometrical journal 6’20. <em>BIMJ</em>,
<em>62</em>(6), 1369–1370. (<a
href="https://doi.org/10.1002/bimj.202070064">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_BIMJ},
  doi          = {10.1002/bimj.202070064},
  journal      = {Biometrical Journal},
  month        = {10},
  number       = {6},
  pages        = {1369-1370},
  shortjournal = {Bio. J.},
  title        = {Contents: Biometrical journal 6&#39;20},
  volume       = {62},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020e). Cover picture: Biometrical journal 5’20. <em>BIMJ</em>,
<em>62</em>(5), NA. (<a
href="https://doi.org/10.1002/bimj.202070051">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_BIMJ},
  doi          = {10.1002/bimj.202070051},
  journal      = {Biometrical Journal},
  month        = {9},
  number       = {5},
  pages        = {NA},
  shortjournal = {Bio. J.},
  title        = {Cover picture: Biometrical journal 5&#39;20},
  volume       = {62},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Biodemography: An introduction to concepts and methods.
James r. Carey and deborah a. Roach. (2020). Princeton university press.
480 pages, ISBN: 9780691129006. <em>BIMJ</em>, <em>62</em>(5), 1359. (<a
href="https://doi.org/10.1002/bimj.202000177">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_BIMJ},
  author       = {Thomas H. G. Ezard},
  doi          = {10.1002/bimj.202000177},
  journal      = {Biometrical Journal},
  month        = {9},
  number       = {5},
  pages        = {1359},
  shortjournal = {Bio. J.},
  title        = {Biodemography: an introduction to concepts and methods. james r. carey and deborah a. roach. (2020). princeton university press. 480 pages, ISBN: 9780691129006.},
  volume       = {62},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Model-based geostatistics for global public health: Methods
and applications peter j. DiggleEmanuele giorgi (2019). CRC press. 248
pages. ISBN: 9781138732353. <em>BIMJ</em>, <em>62</em>(5), 1357–1358.
(<a href="https://doi.org/10.1002/bimj.202000175">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_BIMJ},
  author       = {Kirsty L. Hassall},
  doi          = {10.1002/bimj.202000175},
  journal      = {Biometrical Journal},
  month        = {9},
  number       = {5},
  pages        = {1357-1358},
  shortjournal = {Bio. J.},
  title        = {Model-based geostatistics for global public health: methods and applications peter j. DiggleEmanuele giorgi (2019). CRC press. 248 pages. ISBN: 9781138732353.},
  volume       = {62},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Early detection of high disease activity in juvenile
idiopathic arthritis by sequential monitoring of patients’
health-related quality of life scores. <em>BIMJ</em>, <em>62</em>(5),
1343–1356. (<a href="https://doi.org/10.1002/bimj.201900127">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Juvenile idiopathic arthritis (JIA) is a chronic disease. During its “high disease activity (HDA)” stage, JIA can cause severe pain, and thus could seriously affect patients&#39; physical and psychological health. Early detection of the HDA stage of JIA can reduce the damage of the disease by treating it at an early stage and alleviating the painful experience of the patients. So far, no effective cure of JIA has been found, and one major goal of disease management is to improve patients&#39; quality of life. To this end, patients&#39; health-related quality of life (HRQOL) scores are routinely collected over time from JIA patients. In this paper, we demonstrate that a new statistical methodology called dynamic screening system (DySS) is effective for early detection of the HDA stage of JIA. By this approach, a patient&#39;s HRQOL scores are monitored sequentially, and a signal is given by DySS once the longitudinal pattern of the scores is found to be significantly different from the pattern of patients with low disease activity. Dimension reduction of the observed HRQOL scores and the corresponding impact on the performance of DySS are also discussed.},
  archive      = {J_BIMJ},
  author       = {Lu You and Andrew Qiu and Bin Huang and Peihua Qiu},
  doi          = {10.1002/bimj.201900127},
  journal      = {Biometrical Journal},
  month        = {9},
  number       = {5},
  pages        = {1343-1356},
  shortjournal = {Bio. J.},
  title        = {Early detection of high disease activity in juvenile idiopathic arthritis by sequential monitoring of patients&#39; health-related quality of life scores},
  volume       = {62},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Corrected estimator of sensitive population proportion using
unknown repeated trials in the unrelated question randomized response
model. <em>BIMJ</em>, <em>62</em>(5), 1337–1342. (<a
href="https://doi.org/10.1002/bimj.201900334">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we have pointed out a major mistake in the research paper of Singh and Mathur [(2004). Unknown repeated trials in the unrelated question randomized response model, Biometrical Journal , 46:375–378]. We have corrected this mistake and proposed the corresponding corrected estimator of sensitive population proportion. Furthermore, we have obtained the variance of our proposed estimator. Likewise, Singh and Mathur, we have also compared the variance of our proposed estimator with that of the Greenberg et al.’s estimator theoretically as well as numerically.},
  archive      = {J_BIMJ},
  author       = {Lovleen Kumar Grover and Amanpreet Kaur},
  doi          = {10.1002/bimj.201900334},
  journal      = {Biometrical Journal},
  month        = {9},
  number       = {5},
  pages        = {1337-1342},
  shortjournal = {Bio. J.},
  title        = {Corrected estimator of sensitive population proportion using unknown repeated trials in the unrelated question randomized response model},
  volume       = {62},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Modeling different behaviors in disclosing risk perception.
<em>BIMJ</em>, <em>62</em>(5), 1315–1336. (<a
href="https://doi.org/10.1002/bimj.201900006">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In many fields, people are requested to express their level of awareness about some risk (mainly associated with health, environment, energy, etc.) by selecting a category in an ordered scale. We propose a model for such ordinal data by taking into account that the observed response does not necessarily reflect the respondent&#39;s true opinion since the final answer can be inaccurate or completely random. The proposed model hypothesizes three behaviors in the process of answering: accurate interviewees express their risk perception exactly, uncertain ones randomly select the response according to the uniform distribution, and inaccurate interviewees make evaluation errors but with high probability they choose a rating close to the true one. Statistical inference for the proposed models is addressed without assuming that the model, to be fitted, is correctly specified. Two real case studies on the awareness of geo-hydrological risk and work-related stress risk are considered using the proposed methodology.},
  archive      = {J_BIMJ},
  author       = {Roberto Colombi and Sabrina Giordano},
  doi          = {10.1002/bimj.201900006},
  journal      = {Biometrical Journal},
  month        = {9},
  number       = {5},
  pages        = {1315-1336},
  shortjournal = {Bio. J.},
  title        = {Modeling different behaviors in disclosing risk perception},
  volume       = {62},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A bayesian seamless phase i–II trial design with two stages
for cancer clinical trials with drug combinations. <em>BIMJ</em>,
<em>62</em>(5), 1300–1314. (<a
href="https://doi.org/10.1002/bimj.201900095">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The use of drug combinations in clinical trials is increasingly common during the last years since a more favorable therapeutic response may be obtained by combining drugs. In phase I clinical trials, most of the existing methodology recommends a one unique dose combination as “optimal,” which may result in a subsequent failed phase II clinical trial since other dose combinations may present higher treatment efficacy for the same level of toxicity. We are particularly interested in the setting where it is necessary to wait a few cycles of therapy to observe an efficacy outcome and the phase I and II population of patients are different with respect to treatment efficacy. Under these circumstances, it is common practice to implement two-stage designs where a set of maximum tolerated dose combinations is selected in a first stage, and then studied in a second stage for treatment efficacy. In this article we present a new two-stage design for early phase clinical trials with drug combinations. In the first stage, binary toxicity data is used to guide the dose escalation and set the maximum tolerated dose combinations. In the second stage, we take the set of maximum tolerated dose combinations recommended from the first stage, which remains fixed along the entire second stage, and through adaptive randomization, we allocate subsequent cohorts of patients in dose combinations that are likely to have high posterior median time to progression. The methodology is assessed with extensive simulations and exemplified with a real trial.},
  archive      = {J_BIMJ},
  author       = {José L. Jiménez and Sungjin Kim and Mourad Tighiouart},
  doi          = {10.1002/bimj.201900095},
  journal      = {Biometrical Journal},
  month        = {9},
  number       = {5},
  pages        = {1300-1314},
  shortjournal = {Bio. J.},
  title        = {A bayesian seamless phase I–II trial design with two stages for cancer clinical trials with drug combinations},
  volume       = {62},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Sample size recalculation in multicenter randomized
controlled clinical trials based on noncomparative data. <em>BIMJ</em>,
<em>62</em>(5), 1284–1299. (<a
href="https://doi.org/10.1002/bimj.201900138">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many late-phase clinical trials recruit subjects at multiple study sites. This introduces a hierarchical structure into the data that can result in a power-loss compared to a more homogeneous single-center trial. Building on a recently proposed approach to sample size determination, we suggest a sample size recalculation procedure for multicenter trials with continuous endpoints. The procedure estimates nuisance parameters at interim from noncomparative data and recalculates the sample size required based on these estimates. In contrast to other sample size calculation methods for multicenter trials, our approach assumes a mixed effects model and does not rely on balanced data within centers. It is therefore advantageous, especially for sample size recalculation at interim. We illustrate the proposed methodology by a study evaluating a diabetes management system. Monte Carlo simulations are carried out to evaluate operation characteristics of the sample size recalculation procedure using comparative as well as noncomparative data, assessing their dependence on parameters such as between-center heterogeneity, residual variance of observations, treatment effect size and number of centers. We compare two different estimators for between-center heterogeneity, an unadjusted and a bias-adjusted estimator, both based on quadratic forms. The type 1 error probability as well as statistical power are close to their nominal levels for all parameter combinations considered in our simulation study for the proposed unadjusted estimator, whereas the adjusted estimator exhibits some type 1 error rate inflation. Overall, the sample size recalculation procedure can be recommended to mitigate risks arising from misspecified nuisance parameters at the planning stage.},
  archive      = {J_BIMJ},
  author       = {Markus Harden and Tim Friede},
  doi          = {10.1002/bimj.201900138},
  journal      = {Biometrical Journal},
  month        = {9},
  number       = {5},
  pages        = {1284-1299},
  shortjournal = {Bio. J.},
  title        = {Sample size recalculation in multicenter randomized controlled clinical trials based on noncomparative data},
  volume       = {62},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Adaptive seamless clinical trials using early outcomes for
treatment or subgroup selection: Methods, simulation model and their
implementation in r. <em>BIMJ</em>, <em>62</em>(5), 1264–1283. (<a
href="https://doi.org/10.1002/bimj.201900020">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Adaptive seamless designs combine confirmatory testing, a domain of phase III trials, with features such as treatment or subgroup selection, typically associated with phase II trials. They promise to increase the efficiency of development programmes of new drugs, for example, in terms of sample size and/or development time. It is well acknowledged that adaptive designs are more involved from a logistical perspective and require more upfront planning, often in the form of extensive simulation studies, than conventional approaches. Here, we present a framework for adaptive treatment and subgroup selection using the same notation, which links the somewhat disparate literature on treatment selection on one side and on subgroup selection on the other. Furthermore, we introduce a flexible and efficient simulation model that serves both designs. As primary endpoints often take a long time to observe, interim analyses are frequently informed by early outcomes. Therefore, all methods presented accommodate interim analyses informed by either the primary outcome or an early outcome. The R package asd , previously developed to simulate designs with treatment selection, was extended to include subgroup selection (so-called adaptive enrichment designs). Here, we describe the functionality of the R package asd and use it to present some worked-up examples motivated by clinical trials in chronic obstructive pulmonary disease and oncology. The examples both illustrate various features of the R package and provide insights into the operating characteristics of adaptive seamless studies.},
  archive      = {J_BIMJ},
  author       = {Tim Friede and Nigel Stallard and Nicholas Parsons},
  doi          = {10.1002/bimj.201900020},
  journal      = {Biometrical Journal},
  month        = {9},
  number       = {5},
  pages        = {1264-1283},
  shortjournal = {Bio. J.},
  title        = {Adaptive seamless clinical trials using early outcomes for treatment or subgroup selection: Methods, simulation model and their implementation in r},
  volume       = {62},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Bayesian regression with spatiotemporal varying
coefficients. <em>BIMJ</em>, <em>62</em>(5), 1245–1263. (<a
href="https://doi.org/10.1002/bimj.201900098">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To study the impact of climate variables on morbidity of some diseases in Mexico, we propose a spatiotemporal varying coefficients regression model. For that we introduce a new spatiotemporal-dependent process prior, in a Bayesian context, with identically distributed normal marginal distributions and joint multivariate normal distribution. We study its properties and characterise the dependence induced. Our results show that the effect of climate variables, on the incidence of specific diseases, is not constant across space and time and our proposed model is able to capture and quantify those changes.},
  archive      = {J_BIMJ},
  author       = {Luis E. Nieto-Barajas},
  doi          = {10.1002/bimj.201900098},
  journal      = {Biometrical Journal},
  month        = {9},
  number       = {5},
  pages        = {1245-1263},
  shortjournal = {Bio. J.},
  title        = {Bayesian regression with spatiotemporal varying coefficients},
  volume       = {62},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Skew-normal random-effects model for meta-analysis of
diagnostic test accuracy (DTA) studies. <em>BIMJ</em>, <em>62</em>(5),
1223–1244. (<a href="https://doi.org/10.1002/bimj.201900184">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hierarchical models are recommended for meta-analyzing diagnostic test accuracy (DTA) studies. The bivariate random-effects model is currently widely used to synthesize a pair of test sensitivity and specificity using logit transformation across studies. This model assumes a bivariate normal distribution for the random-effects. However, this assumption is restrictive and can be violated. When the assumption fails, inferences could be misleading. In this paper, we extended the current bivariate random-effects model by assuming a flexible bivariate skew-normal distribution for the random-effects in order to robustly model logit sensitivities and logit specificities. The marginal distribution of the proposed model is analytically derived so that parameter estimation can be performed using standard likelihood methods. The method of weighted-average is adopted to estimate the overall logit-transformed sensitivity and specificity. An extensive simulation study is carried out to investigate the performance of the proposed model compared to other standard models. Overall, the proposed model performs better in terms of confidence interval width of the average logit-transformed sensitivity and specificity compared to the standard bivariate linear mixed model and bivariate generalized linear mixed model. Simulations have also shown that the proposed model performed better than the well-established bivariate linear mixed model in terms of bias and comparable with regards to the root mean squared error (RMSE) of the between-study (co)variances. The proposed method is also illustrated using a published meta-analysis data.},
  archive      = {J_BIMJ},
  author       = {Zelalem F. Negeri and Joseph Beyene},
  doi          = {10.1002/bimj.201900184},
  journal      = {Biometrical Journal},
  month        = {9},
  number       = {5},
  pages        = {1223-1244},
  shortjournal = {Bio. J.},
  title        = {Skew-normal random-effects model for meta-analysis of diagnostic test accuracy (DTA) studies},
  volume       = {62},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). On a class of non-linear transformation cure rate models.
<em>BIMJ</em>, <em>62</em>(5), 1208–1222. (<a
href="https://doi.org/10.1002/bimj.201900005">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose a generalization of the mixture (binary) cure rate model, motivated by the existence of a zero-modified (inflation or deflation) distribution, on the initial number of causes, under a competing cause scenario. This non-linear transformation cure rate model is in the same form of models studied in the past; however, following our approach, we are able to give a realistic interpretation to a specific class of proper transformation functions, for the cure rate modeling. The estimation of the parameters is then carried out using the maximum likelihood method along with a profile approach. A simulation study examines the accuracy of the proposed estimation method and the model discrimination based on the likelihood ratio test. For illustrative purposes, analysis of two real life data-sets, one on recidivism and another on cutaneous melanoma, is also carried out.},
  archive      = {J_BIMJ},
  author       = {Narayanaswamy Balakrishnan and Fotios S. Milienos},
  doi          = {10.1002/bimj.201900005},
  journal      = {Biometrical Journal},
  month        = {9},
  number       = {5},
  pages        = {1208-1222},
  shortjournal = {Bio. J.},
  title        = {On a class of non-linear transformation cure rate models},
  volume       = {62},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Multiply imputing missing values arising by design in
transplant survival data. <em>BIMJ</em>, <em>62</em>(5), 1192–1207. (<a
href="https://doi.org/10.1002/bimj.201800253">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we address a missing data problem that occurs in transplant survival studies. Recipients of organ transplants are followed up from transplantation and their survival times recorded, together with various explanatory variables. Due to differences in data collection procedures in different centers or over time, a particular explanatory variable (or set of variables) may only be recorded for certain recipients, which results in this variable being missing for a substantial number of records in the data. The variable may also turn out to be an important predictor of survival and so it is important to handle this missing-by-design problem appropriately. Consensus in the literature is to handle this problem with complete case analysis, as the missing data are assumed to arise under an appropriate missing at random mechanism that gives consistent estimates here. Specifically, the missing values can reasonably be assumed not to be related to the survival time. In this article, we investigate the potential for multiple imputation to handle this problem in a relevant study on survival after kidney transplantation, and show that it comprehensively outperforms complete case analysis on a range of measures. This is a particularly important finding in the medical context as imputing large amounts of missing data is often viewed with scepticism.},
  archive      = {J_BIMJ},
  author       = {Laura Pankhurst and Robin Mitra and Alan Kimber and Dave Collett},
  doi          = {10.1002/bimj.201800253},
  journal      = {Biometrical Journal},
  month        = {9},
  number       = {5},
  pages        = {1192-1207},
  shortjournal = {Bio. J.},
  title        = {Multiply imputing missing values arising by design in transplant survival data},
  volume       = {62},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A regularized estimation approach for case-cohort periodic
follow-up studies with an application to HIV vaccine trials.
<em>BIMJ</em>, <em>62</em>(5), 1176–1191. (<a
href="https://doi.org/10.1002/bimj.201900180">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper discusses regression analysis of the failure time data arising from case-cohort periodic follow-up studies, and one feature of such data, which makes their analysis much more difficult, is that they are usually interval-censored rather than right-censored. Although some methods have been developed for general failure time data, there does not seem to exist an established procedure for the situation considered here. To address the problem, we present a semiparametric regularized procedure and develop a simple algorithm for the implementation of the proposed method. In addition, unlike some existing procedures for similar situations, the proposed procedure is shown to have the oracle property, and an extensive simulation is conducted and it suggests that the presented approach seems to work well for practical situations. The method is applied to an HIV vaccine trial that motivated this study.},
  archive      = {J_BIMJ},
  author       = {Hui Zhao and Qiwei Wu and Peter B. Gilbert and Ying Q. Chen and Jianguo Sun},
  doi          = {10.1002/bimj.201900180},
  journal      = {Biometrical Journal},
  month        = {9},
  number       = {5},
  pages        = {1176-1191},
  shortjournal = {Bio. J.},
  title        = {A regularized estimation approach for case-cohort periodic follow-up studies with an application to HIV vaccine trials},
  volume       = {62},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Joint analysis of panel count and interval-censored data
using distribution-free frailty analysis. <em>BIMJ</em>, <em>62</em>(5),
1164–1175. (<a href="https://doi.org/10.1002/bimj.201900134">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a joint analysis of recurrent and nonrecurrent event data subject to general types of interval censoring. The proposed analysis allows for general semiparametric models, including the Box–Cox transformation and inverse Box–Cox transformation models for the recurrent and nonrecurrent events, respectively. A frailty variable is used to account for the potential dependence between the recurrent and nonrecurrent event processes, while leaving the distribution of the frailty unspecified. We apply the pseudolikelihood for interval-censored recurrent event data, usually termed as panel count data, and the sufficient likelihood for interval-censored nonrecurrent event data by conditioning on the sufficient statistic for the frailty and using the working assumption of independence over examination times. Large sample theory and a computation procedure for the proposed analysis are established. We illustrate the proposed methodology by a joint analysis of the numbers of occurrences of basal cell carcinoma over time and time to the first recurrence of squamous cell carcinoma based on a skin cancer dataset, as well as a joint analysis of the numbers of adverse events and time to premature withdrawal from study medication based on a scleroderma lung disease dataset.},
  archive      = {J_BIMJ},
  author       = {Chi-Chung Wen and Yi-Hau Chen and Chi-Hong Tseng},
  doi          = {10.1002/bimj.201900134},
  journal      = {Biometrical Journal},
  month        = {9},
  number       = {5},
  pages        = {1164-1175},
  shortjournal = {Bio. J.},
  title        = {Joint analysis of panel count and interval-censored data using distribution-free frailty analysis},
  volume       = {62},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Estimation in the cox survival regression model with
covariate measurement error and a changepoint. <em>BIMJ</em>,
<em>62</em>(5), 1139–1163. (<a
href="https://doi.org/10.1002/bimj.201800085">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Cox regression model is a popular model for analyzing the relationship between a covariate vector and a survival endpoint. The standard Cox model assumes a constant covariate effect across the entire covariate domain. However, in many epidemiological and other applications, the covariate of main interest is subject to a threshold effect: a change in the slope at a certain point within the covariate domain. Often, the covariate of interest is subject to some degree of measurement error. In this paper, we study measurement error correction in the case where the threshold is known. Several bias correction methods are examined: two versions of regression calibration (RC1 and RC2, the latter of which is new), two methods based on the induced relative risk under a rare event assumption (RR1 and RR2, the latter of which is new), a maximum pseudo-partial likelihood estimator (MPPLE), and simulation-extrapolation (SIMEX). We develop the theory, present simulations comparing the methods, and illustrate their use on data concerning the relationship between chronic air pollution exposure to particulate matter PM 10 and fatal myocardial infarction (Nurses Health Study (NHS)), and on data concerning the effect of a subject&#39;s long-term underlying systolic blood pressure level on the risk of cardiovascular disease death (Framingham Heart Study (FHS)). The simulations indicate that the best methods are RR2 and MPPLE.},
  archive      = {J_BIMJ},
  author       = {Sarit Agami and David M. Zucker and Donna Spiegelman},
  doi          = {10.1002/bimj.201800085},
  journal      = {Biometrical Journal},
  month        = {9},
  number       = {5},
  pages        = {1139-1163},
  shortjournal = {Bio. J.},
  title        = {Estimation in the cox survival regression model with covariate measurement error and a changepoint},
  volume       = {62},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020e). Contents: Biometrical journal 5’20. <em>BIMJ</em>,
<em>62</em>(5), 1137–1138. (<a
href="https://doi.org/10.1002/bimj.202070054">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_BIMJ},
  doi          = {10.1002/bimj.202070054},
  journal      = {Biometrical Journal},
  month        = {9},
  number       = {5},
  pages        = {1137-1138},
  shortjournal = {Bio. J.},
  title        = {Contents: Biometrical journal 5&#39;20},
  volume       = {62},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020d). Cover picture: Biometrical journal 4’20. <em>BIMJ</em>,
<em>62</em>(4), NA. (<a
href="https://doi.org/10.1002/bimj.202070041">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_BIMJ},
  doi          = {10.1002/bimj.202070041},
  journal      = {Biometrical Journal},
  month        = {7},
  number       = {4},
  pages        = {NA},
  shortjournal = {Bio. J.},
  title        = {Cover picture: Biometrical journal 4&#39;20},
  volume       = {62},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). List of reviewers for 2019. <em>BIMJ</em>, <em>62</em>(4),
1126–1128. (<a href="https://doi.org/10.1002/bimj.202070044">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_BIMJ},
  doi          = {10.1002/bimj.202070044},
  journal      = {Biometrical Journal},
  month        = {7},
  number       = {4},
  pages        = {1126-1128},
  shortjournal = {Bio. J.},
  title        = {List of reviewers for 2019},
  volume       = {62},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Dose-response analysis using r c. Ritz s. M. Jensen
d. Gerhard j. C. Streibig (2019). Boca raton, FL: CRC press, 214 pages.
ISBN: 978-1-138-03431-0. <em>BIMJ</em>, <em>62</em>(4), 1124–1125. (<a
href="https://doi.org/10.1002/bimj.202000099">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_BIMJ},
  author       = {Franziska Kappenberg and Jörg Rahnenführer},
  doi          = {10.1002/bimj.202000099},
  journal      = {Biometrical Journal},
  month        = {7},
  number       = {4},
  pages        = {1124-1125},
  shortjournal = {Bio. J.},
  title        = {Dose-response analysis using r c. ritz s. m. jensen d. gerhard j. c. streibig (2019). boca raton, FL: CRC press, 214 pages. ISBN: 978-1-138-03431-0.},
  volume       = {62},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Clinical prediction models: A practical approach to
development, validation, and updating ewout w. Steyerberg (2019). Second
edition, springer series statistics for biology and health. Cham:
Springer. 558 pages. ISBN: 978-3-030-16398-3.
Https://doi.org/10.1007/978-3-030-16399-0. <em>BIMJ</em>,
<em>62</em>(4), 1122–1123. (<a
href="https://doi.org/10.1002/bimj.202000088">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_BIMJ},
  author       = {Andreas Ziegler},
  doi          = {10.1002/bimj.202000088},
  journal      = {Biometrical Journal},
  month        = {7},
  number       = {4},
  pages        = {1122-1123},
  shortjournal = {Bio. J.},
  title        = {Clinical prediction models: a practical approach to development, validation, and updating ewout w. steyerberg (2019). second edition, springer series statistics for biology and health. cham: springer. 558 pages. ISBN: 978-3-030-16398-3. https://doi.org/10.1007/978-3-030-16399-0},
  volume       = {62},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020a). <em>BIMJ</em>, <em>62</em>(4), 1120–1121. (<a
href="https://doi.org/10.1002/bimj.201900390">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_BIMJ},
  author       = {Li-Pang Chen},
  doi          = {10.1002/bimj.201900390},
  journal      = {Biometrical Journal},
  month        = {7},
  number       = {4},
  pages        = {1120-1121},
  shortjournal = {Bio. J.},
  title        = {Model-based clustering and classification for data science: with applications in r charles BouveyronGilles CeleuxT. brendan MurphyAdrian e. raftery (2019). new york, NY: cambridge university press. 446 pages. CDN$91.95 (hardback). ISBN: 9781108494205.},
  volume       = {62},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A bayesian spatiotemporal statistical analysis of
out-of-hospital cardiac arrests. <em>BIMJ</em>, <em>62</em>(4),
1105–1119. (<a href="https://doi.org/10.1002/bimj.201900166">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a Bayesian spatiotemporal statistical model for predicting out-of-hospital cardiac arrests (OHCAs). Risk maps for Ticino, adjusted for demographic covariates, are built for explaining and forecasting the spatial distribution of OHCAs and their temporal dynamics. The occurrence intensity of the OHCA event in each area of interest, and the cardiac risk-based clustering of municipalities are efficiently estimated, through a statistical model that decomposes OHCA intensity into overall intensity, demographic fixed effects, spatially structured and unstructured random effects, time polynomial dependence, and spatiotemporal random effect. In the studied geography, time evolution and dependence on demographic features are robust over different categories of OHCAs, but with variability in their spatial and spatiotemporal structure. Two main OHCA incidence-based clusters of municipalities are identified.},
  archive      = {J_BIMJ},
  author       = {Stefano Peluso and Antonietta Mira and Håvard Rue and Nicholas John Tierney and Claudio Benvenuti and Roberto Cianella and Maria Luce Caputo and Angelo Auricchio},
  doi          = {10.1002/bimj.201900166},
  journal      = {Biometrical Journal},
  month        = {7},
  number       = {4},
  pages        = {1105-1119},
  shortjournal = {Bio. J.},
  title        = {A bayesian spatiotemporal statistical analysis of out-of-hospital cardiac arrests},
  volume       = {62},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A design criterion for symmetric model discrimination based
on flexible nominal sets. <em>BIMJ</em>, <em>62</em>(4), 1090–1104. (<a
href="https://doi.org/10.1002/bimj.201900074">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Experimental design applications for discriminating between models have been hampered by the assumption to know beforehand which model is the true one, which is counter to the very aim of the experiment. Previous approaches to alleviate this requirement were either symmetrizations of asymmetric techniques, or Bayesian, minimax, and sequential approaches. Here we present a genuinely symmetric criterion based on a linearized distance between mean-value surfaces and the newly introduced tool of flexible nominal sets. We demonstrate the computational efficiency of the approach using the proposed criterion and provide a Monte-Carlo evaluation of its discrimination performance on the basis of the likelihood ratio. An application for a pair of competing models in enzyme kinetics is given.},
  archive      = {J_BIMJ},
  author       = {Radoslav Harman and Werner G. Müller},
  doi          = {10.1002/bimj.201900074},
  journal      = {Biometrical Journal},
  month        = {7},
  number       = {4},
  pages        = {1090-1104},
  shortjournal = {Bio. J.},
  title        = {A design criterion for symmetric model discrimination based on flexible nominal sets},
  volume       = {62},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A simple method for correcting for the will rogers
phenomenon with biometrical applications. <em>BIMJ</em>, <em>62</em>(4),
1080–1089. (<a href="https://doi.org/10.1002/bimj.201900199">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In its basic form, the Will Rogers phenomenon takes place when an increase in the average value of each of two sets is achieved by moving an element from one set to another. This leads to the conclusion that there has been an improvement, when in fact essentially nothing has changed. Extended versions of this phenomenon can occur in epidemiological studies, rendering their results unreliable. After describing epidemiological and clinical studies that have been affected by the Will Rogers phenomenon, this paper presents a simple method to correct for it. The method involves introducing a transition matrix between the two sets and taking probability weighted expectations. Two real-world biometrical examples, based on migration economics and breast cancer epidemiology, are given and improvements against a naïve analysis are demonstrated. In the cancer epidemiology example, we take account of estimation uncertainty. We also discuss briefly some limitations associated with our method.},
  archive      = {J_BIMJ},
  author       = {Mark Stander and Julian Stander},
  doi          = {10.1002/bimj.201900199},
  journal      = {Biometrical Journal},
  month        = {7},
  number       = {4},
  pages        = {1080-1089},
  shortjournal = {Bio. J.},
  title        = {A simple method for correcting for the will rogers phenomenon with biometrical applications},
  volume       = {62},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). False discovery rate control for multiple testing based on
discrete p-values. <em>BIMJ</em>, <em>62</em>(4), 1060–1079. (<a
href="https://doi.org/10.1002/bimj.201900163">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For multiple testing based on discrete p -values, we propose a false discovery rate (FDR) procedure “BH+” with proven conservativeness. BH+ is at least as powerful as the BH (i.e., Benjamini-Hochberg) procedure when they are applied to superuniform p -values. Further, when applied to mid- p -values, BH+ can be more powerful than it is applied to conventional p -values. An easily verifiable necessary and sufficient condition for this is provided. BH+ is perhaps the first conservative FDR procedure applicable to mid- p -values and to p -values with general distributions. It is applied to multiple testing based on discrete p -values in a methylation study, an HIV study and a clinical safety study, where it makes considerably more discoveries than the BH procedure. In addition, we propose an adaptive version of the BH+ procedure, prove its conservativeness under certain conditions, and provide evidence on its excellent performance via simulation studies.},
  archive      = {J_BIMJ},
  author       = {Xiongzhi Chen},
  doi          = {10.1002/bimj.201900163},
  journal      = {Biometrical Journal},
  month        = {7},
  number       = {4},
  pages        = {1060-1079},
  shortjournal = {Bio. J.},
  title        = {False discovery rate control for multiple testing based on discrete p-values},
  volume       = {62},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Smoothed empirical likelihood inference for ROC curve in the
presence of missing biomarker values. <em>BIMJ</em>, <em>62</em>(4),
1038–1059. (<a href="https://doi.org/10.1002/bimj.201900121">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper considers statistical inference for the receiver operating characteristic (ROC) curve in the presence of missing biomarker values by utilizing estimating equations (EEs) together with smoothed empirical likelihood (SEL). Three approaches are developed to estimate ROC curve and construct its SEL-based confidence intervals based on the kernel-assisted EE imputation, multiple imputation, and hybrid imputation combining the inverse probability weighted imputation and multiple imputation. Under some regularity conditions, we show asymptotic properties of the proposed maximum SEL estimators for ROC curve. Simulation studies are conducted to investigate the performance of the proposed SEL approaches. An example is illustrated by the proposed methodologies. Empirical results show that the hybrid imputation method behaves better than the kernel-assisted and multiple imputation methods, and the proposed three SEL methods outperform existing nonparametric method.},
  archive      = {J_BIMJ},
  author       = {Weili Cheng and Niansheng Tang},
  doi          = {10.1002/bimj.201900121},
  journal      = {Biometrical Journal},
  month        = {7},
  number       = {4},
  pages        = {1038-1059},
  shortjournal = {Bio. J.},
  title        = {Smoothed empirical likelihood inference for ROC curve in the presence of missing biomarker values},
  volume       = {62},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Approaches for missing covariate data in logistic regression
with MNAR sensitivity analyses. <em>BIMJ</em>, <em>62</em>(4),
1025–1037. (<a href="https://doi.org/10.1002/bimj.201900117">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data with missing covariate values but fully observed binary outcomes are an important subset of the missing data challenge. Common approaches are complete case analysis (CCA) and multiple imputation (MI). While CCA relies on missing completely at random (MCAR), MI usually relies on a missing at random (MAR) assumption to produce unbiased results. For MI involving logistic regression models, it is also important to consider several missing not at random (MNAR) conditions under which CCA is asymptotically unbiased and, as we show, MI is also valid in some cases. We use a data application and simulation study to compare the performance of several machine learning and parametric MI methods under a fully conditional specification framework (MI-FCS). Our simulation includes five scenarios involving MCAR, MAR, and MNAR under predictable and nonpredictable conditions, where “predictable” indicates missingness is not associated with the outcome. We build on previous results in the literature to show MI and CCA can both produce unbiased results under more conditions than some analysts may realize. When both approaches were valid, we found that MI-FCS was at least as good as CCA in terms of estimated bias and coverage, and was superior when missingness involved a categorical covariate. We also demonstrate how MNAR sensitivity analysis can build confidence that unbiased results were obtained, including under MNAR-predictable, when CCA and MI are both valid. Since the missingness mechanism cannot be identified from observed data, investigators should compare results from MI and CCA when both are plausibly valid, followed by MNAR sensitivity analysis.},
  archive      = {J_BIMJ},
  author       = {Ralph C. Ward and Robert Neal Axon and Mulugeta Gebregziabher},
  doi          = {10.1002/bimj.201900117},
  journal      = {Biometrical Journal},
  month        = {7},
  number       = {4},
  pages        = {1025-1037},
  shortjournal = {Bio. J.},
  title        = {Approaches for missing covariate data in logistic regression with MNAR sensitivity analyses},
  volume       = {62},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). The effect of treatment delay on time-to-recovery in the
presence of unobserved heterogeneity. <em>BIMJ</em>, <em>62</em>(4),
1012–1024. (<a href="https://doi.org/10.1002/bimj.201900131">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study the effect of delaying treatment in the presence of (unobserved) heterogeneity. In a homogeneous population and assuming a proportional treatment effect, a treatment delay period will result in notably lower cumulative recovery percentages. We show in theoretical scenarios using frailty models that if the population is heterogeneous, the effect of a delay period is much smaller. This can be explained by the selection process that is induced by the frailty. Patient groups that start treatment later have already undergone more selection. The marginal hazard ratio for the treatment will act differently in such a more homogeneous patient group. We further discuss modeling approaches for estimating the effect of treatment delay in the presence of heterogeneity, and compare their performance in a simulation study. The conventional Cox model that fails to account for heterogeneity overestimates the effect of treatment delay. Including interaction terms between treatment and starting time of treatment or between treatment and follow up time gave no improvement. Estimating a frailty term can improve the estimation, but is sensitive to misspecification of the frailty distribution. Therefore, multiple frailty distributions should be used and the results should be compared using the Akaike Information Criterion. Non-parametric estimation of the cumulative recovery percentages can be considered if the dataset contains sufficient long term follow up for each of the delay strategies. The methods are demonstrated on a motivating application evaluating the effect of delaying the start of treatment with assisted reproductive techniques on time-to-pregnancy in couples with unexplained subfertility.},
  archive      = {J_BIMJ},
  author       = {Nan van Geloven and Theodor A. Balan and Hein Putter and Saskia le Cessie},
  doi          = {10.1002/bimj.201900131},
  journal      = {Biometrical Journal},
  month        = {7},
  number       = {4},
  pages        = {1012-1024},
  shortjournal = {Bio. J.},
  title        = {The effect of treatment delay on time-to-recovery in the presence of unobserved heterogeneity},
  volume       = {62},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Generalized parametric cure models for relative survival.
<em>BIMJ</em>, <em>62</em>(4), 989–1011. (<a
href="https://doi.org/10.1002/bimj.201900056">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cure models are used in time-to-event analysis when not all individuals are expected to experience the event of interest, or when the survival of the considered individuals reaches the same level as the general population. These scenarios correspond to a plateau in the survival and relative survival function, respectively. The main parameters of interest in cure models are the proportion of individuals who are cured, termed the cure proportion, and the survival function of the uncured individuals. Although numerous cure models have been proposed in the statistical literature, there is no consensus on how to formulate these. We introduce a general parametric formulation of mixture cure models and a new class of cure models, termed latent cure models, together with a general estimation framework and software, which enable fitting of a wide range of different models. Through simulations, we assess the statistical properties of the models with respect to the cure proportion and the survival of the uncured individuals. Finally, we illustrate the models using survival data on colon cancer, which typically display a plateau in the relative survival. As demonstrated in the simulations, mixture cure models which are not guaranteed to be constant after a finite time point, tend to produce accurate estimates of the cure proportion and the survival of the uncured. However, these models are very unstable in certain cases due to identifiability issues, whereas LC models generally provide stable results at the price of more biased estimates.},
  archive      = {J_BIMJ},
  author       = {Lasse Hjort Jakobsen and Martin Bøgsted and Mark Clements},
  doi          = {10.1002/bimj.201900056},
  journal      = {Biometrical Journal},
  month        = {7},
  number       = {4},
  pages        = {989-1011},
  shortjournal = {Bio. J.},
  title        = {Generalized parametric cure models for relative survival},
  volume       = {62},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A nonparametric method of estimation of the population size
in capture–recapture experiments. <em>BIMJ</em>, <em>62</em>(4),
970–988. (<a href="https://doi.org/10.1002/bimj.201900185">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A recent method for estimating a lower bound of the population size in capture–recapture samples is studied. Specifically, some asymptotic properties, such as strong consistency and asymptotic normality, are provided. The introduced estimator is based on the empirical probability generating function (pgf) of the observed data, and it is consistent for count distributions having a log-convex pgf ( -class). This is a large family that includes mixed and compound Poisson distributions, and their independent sums and finite mixtures as well. The finite-sample performance of the lower bound estimator is assessed via simulation showing a better behavior than some close competitors. Several examples of application are also analyzed and discussed.},
  archive      = {J_BIMJ},
  author       = {María Dolores Jiménez-Gamero and Pedro Puig},
  doi          = {10.1002/bimj.201900185},
  journal      = {Biometrical Journal},
  month        = {7},
  number       = {4},
  pages        = {970-988},
  shortjournal = {Bio. J.},
  title        = {A nonparametric method of estimation of the population size in capture–recapture experiments},
  volume       = {62},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Bayesian latent class models for capture–recapture in the
presence of missing data. <em>BIMJ</em>, <em>62</em>(4), 957–969. (<a
href="https://doi.org/10.1002/bimj.201900111">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a method for estimating the size of a population in a multiple record system in the presence of missing data. The method is based on a latent class model where the parameters and the latent structure are estimated using a Gibbs sampler. The proposed approach is illustrated through the analysis of a data set already known in the literature, which consists of five registrations of neural tube defects.},
  archive      = {J_BIMJ},
  author       = {Davide Di Cecco and Marco Di Zio and Brunero Liseo},
  doi          = {10.1002/bimj.201900111},
  journal      = {Biometrical Journal},
  month        = {7},
  number       = {4},
  pages        = {957-969},
  shortjournal = {Bio. J.},
  title        = {Bayesian latent class models for capture–recapture in the presence of missing data},
  volume       = {62},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Population size estimation with interval censored counts and
external information: Prevalence of multiple sclerosis in rome.
<em>BIMJ</em>, <em>62</em>(4), 945–956. (<a
href="https://doi.org/10.1002/bimj.201900268">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We discuss Bayesian log-linear models for incomplete contingency tables with both missing and interval censored cells, with the aim of obtaining reliable population size estimates. We also discuss use of external information on the censoring probability, which may substantially reduce uncertainty. We show in simulation that information on lower bounds and external information can each improve the mean squared error of population size estimates, even when the external information is not completely accurate. We conclude with an original example on estimation of prevalence of multiple sclerosis in the metropolitan area of Rome, where five out of six lists have interval censored counts. External information comes from mortality rates of multiple sclerosis patients.},
  archive      = {J_BIMJ},
  author       = {Alessio Farcomeni},
  doi          = {10.1002/bimj.201900268},
  journal      = {Biometrical Journal},
  month        = {7},
  number       = {4},
  pages        = {945-956},
  shortjournal = {Bio. J.},
  title        = {Population size estimation with interval censored counts and external information: Prevalence of multiple sclerosis in rome},
  volume       = {62},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Developing risk models for multicenter data using standard
logistic regression produced suboptimal predictions: A simulation study.
<em>BIMJ</em>, <em>62</em>(4), 932–944. (<a
href="https://doi.org/10.1002/bimj.201900075">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Although multicenter data are common, many prediction model studies ignore this during model development. The objective of this study is to evaluate the predictive performance of regression methods for developing clinical risk prediction models using multicenter data, and provide guidelines for practice. We compared the predictive performance of standard logistic regression, generalized estimating equations, random intercept logistic regression, and fixed effects logistic regression. First, we presented a case study on the diagnosis of ovarian cancer. Subsequently, a simulation study investigated the performance of the different models as a function of the amount of clustering, development sample size, distribution of center-specific intercepts, the presence of a center-predictor interaction, and the presence of a dependency between center effects and predictors. The results showed that when sample sizes were sufficiently large, conditional models yielded calibrated predictions, whereas marginal models yielded miscalibrated predictions. Small sample sizes led to overfitting and unreliable predictions. This miscalibration was worse with more heavily clustered data. Calibration of random intercept logistic regression was better than that of standard logistic regression even when center-specific intercepts were not normally distributed, a center-predictor interaction was present, center effects and predictors were dependent, or when the model was applied in a new center. Therefore, to make reliable predictions in a specific center, we recommend random intercept logistic regression.},
  archive      = {J_BIMJ},
  author       = {Nora Falconieri and Ben Van Calster and Dirk Timmerman and Laure Wynants},
  doi          = {10.1002/bimj.201900075},
  journal      = {Biometrical Journal},
  month        = {7},
  number       = {4},
  pages        = {932-944},
  shortjournal = {Bio. J.},
  title        = {Developing risk models for multicenter data using standard logistic regression produced suboptimal predictions: A simulation study},
  volume       = {62},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Modeling tails for collinear data with outliers in the
english longitudinal study of ageing: Quantile profile regression.
<em>BIMJ</em>, <em>62</em>(4), 916–931. (<a
href="https://doi.org/10.1002/bimj.201900146">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Research has shown that high blood glucose levels are important predictors of incident diabetes. However, they are also strongly associated with other cardiometabolic risk factors such as high blood pressure, adiposity, and cholesterol, which are also highly correlated with one another. The aim of this analysis was to ascertain how these highly correlated cardiometabolic risk factors might be associated with high levels of blood glucose in older adults aged 50 or older from wave 2 of the English Longitudinal Study of Ageing (ELSA). Due to the high collinearity of predictor variables and our interest in extreme values of blood glucose we proposed a new method, called quantile profile regression, to answer this question. Profile regression, a Bayesian nonparametric model for clustering responses and covariates simultaneously, is a powerful tool to model the relationship between a response variable and covariates, but the standard approach of using a mixture of Gaussian distributions for the response model will not identify the underlying clusters correctly, particularly with outliers in the data or heavy tail distribution of the response. Therefore, we propose quantile profile regression to model the response variable with an asymmetric Laplace distribution, allowing us to model more accurately clusters that are asymmetric and predict more accurately for extreme values of the response variable and/or outliers. Our new method performs more accurately in simulations when compared to Normal profile regression approach as well as robustly when outliers are present in the data. We conclude with an analysis of the ELSA.},
  archive      = {J_BIMJ},
  author       = {Xi Liu and Silvia Liverani and Kimberley J. Smith and Keming Yu},
  doi          = {10.1002/bimj.201900146},
  journal      = {Biometrical Journal},
  month        = {7},
  number       = {4},
  pages        = {916-931},
  shortjournal = {Bio. J.},
  title        = {Modeling tails for collinear data with outliers in the english longitudinal study of ageing: Quantile profile regression},
  volume       = {62},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Regressive models for risk prediction of repeated
multinomial outcomes: An illustration using health and retirement study
data. <em>BIMJ</em>, <em>62</em>(4), 898–915. (<a
href="https://doi.org/10.1002/bimj.201800101">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Life expectancy is increasing in many countries and this may lead to a higher frequency of adverse health outcomes. Therefore, there is a growing demand for predicting the risk of a sequence of events based on specified factors from repeated outcomes. We proposed regressive models and a framework to predict the joint probabilities of a sequence of events for multinomial outcomes from longitudinal studies. The Markov chain is used to link marginal and sequence of conditional probabilities to predict the joint probability. Marginal and sequence of conditional probabilities are estimated using marginal and regressive models. An application is shown using the Health and Retirement Study data. The bias of parameter estimates for all models from all bootstrap simulation is less than 1% in most of the cases. The estimated mean squared error is also very low. Results from the simulation study show negligible bias and the usefulness of the proposed model. The proposed model and framework would be useful to solve real-life problems from various fields and big data analysis.},
  archive      = {J_BIMJ},
  author       = {Rafiqul I. Chowdhury and Mohammed Ataharul Islam},
  doi          = {10.1002/bimj.201800101},
  journal      = {Biometrical Journal},
  month        = {7},
  number       = {4},
  pages        = {898-915},
  shortjournal = {Bio. J.},
  title        = {Regressive models for risk prediction of repeated multinomial outcomes: An illustration using health and retirement study data},
  volume       = {62},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020d). Contents: Biometrical journal 4’20. <em>BIMJ</em>,
<em>62</em>(4), 893–894. (<a
href="https://doi.org/10.1002/bimj.202070045">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_BIMJ},
  doi          = {10.1002/bimj.202070045},
  journal      = {Biometrical Journal},
  month        = {7},
  number       = {4},
  pages        = {893-894},
  shortjournal = {Bio. J.},
  title        = {Contents: Biometrical journal 4&#39;20},
  volume       = {62},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020c). Cover picture: Biometrical journal 3’20. <em>BIMJ</em>,
<em>62</em>(3), NA. (<a
href="https://doi.org/10.1002/bimj.202070031">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_BIMJ},
  doi          = {10.1002/bimj.202070031},
  journal      = {Biometrical Journal},
  month        = {5},
  number       = {3},
  pages        = {NA},
  shortjournal = {Bio. J.},
  title        = {Cover picture: Biometrical journal 3&#39;20},
  volume       = {62},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Automatic variable selection for exposure-driven propensity
score matching with unmeasured confounders. <em>BIMJ</em>,
<em>62</em>(3), 868–884. (<a
href="https://doi.org/10.1002/bimj.201800190">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multivariable model building for propensity score modeling approaches is challenging. A common propensity score approach is exposure-driven propensity score matching, where the best model selection strategy is still unclear. In particular, the situation may require variable selection, while it is still unclear if variables included in the propensity score should be associated with the exposure and the outcome, with either the exposure or the outcome, with at least the exposure or with at least the outcome. Unmeasured confounders, complex correlation structures, and non-normal covariate distributions further complicate matters. We consider the performance of different modeling strategies in a simulation design with a complex but realistic structure and effects on a binary outcome. We compare the strategies in terms of bias and variance in estimated marginal exposure effects. Considering the bias in estimated marginal exposure effects, the most reliable results for estimating the propensity score are obtained by selecting variables related to the exposure. On average this results in the least bias and does not greatly increase variances. Although our results cannot be generalized, this provides a counterexample to existing recommendations in the literature based on simple simulation settings. This highlights that recommendations obtained in simple simulation settings cannot always be generalized to more complex, but realistic settings and that more complex simulation studies are needed.},
  archive      = {J_BIMJ},
  author       = {Daniela Zöller and Leesa F. Wockner and Harald Binder},
  doi          = {10.1002/bimj.201800190},
  journal      = {Biometrical Journal},
  month        = {5},
  number       = {3},
  pages        = {868-884},
  shortjournal = {Bio. J.},
  title        = {Automatic variable selection for exposure-driven propensity score matching with unmeasured confounders},
  volume       = {62},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Nonparametric estimation of the cumulative incidences of
competing risks under double truncation. <em>BIMJ</em>, <em>62</em>(3),
852–867. (<a href="https://doi.org/10.1002/bimj.201800323">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Registry data typically report incident cases within a certain calendar time interval. Such interval sampling induces double truncation on the incidence times, which may result in an observational bias. In this paper, we introduce nonparametric estimation for the cumulative incidences of competing risks when the incidence time is doubly truncated. Two different estimators are proposed depending on whether the truncation limits are independent of the competing events or not. The asymptotic properties of the estimators are established, and their finite sample performance is investigated through simulations. For illustration purposes, the estimators are applied to childhood cancer registry data, where the target population is peculiarly defined conditional on future cancer development. Then, in our application, the cumulative incidences inform on the distribution by age of the different types of cancer.},
  archive      = {J_BIMJ},
  author       = {Jacobo de Uña-Álvarez},
  doi          = {10.1002/bimj.201800323},
  journal      = {Biometrical Journal},
  month        = {5},
  number       = {3},
  pages        = {852-867},
  shortjournal = {Bio. J.},
  title        = {Nonparametric estimation of the cumulative incidences of competing risks under double truncation},
  volume       = {62},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Modeling the hazard of transition into the absorbing state
in the illness-death model. <em>BIMJ</em>, <em>62</em>(3), 836–851. (<a
href="https://doi.org/10.1002/bimj.201800267">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The illness-death model is the simplest multistate model where the transition from the initial state 0 to the absorbing state 2 may involve an intermediate state 1 (e.g., disease relapse). The impact of the transition into state 1 on the subsequent transition hazard to state 2 enables insight to be gained into the disease evolution. The standard approach of analysis is modeling the transition hazards from 0 to 2 and from 1 to 2, including time to illness as a time-varying covariate and measuring time from origin even after transition into state 1. The hazard from 1 to 2 can be also modeled separately using only patients in state 1, measuring time from illness and including time to illness as a fixed covariate. A recently proposed approach is a model where time after the transition into state 1 is measured in both scales and time to illness is included as a time-varying covariate. Another possibility is a model where time after transition into state 1 is measured only from illness and time to illness is included as a fixed covariate. Through theoretical reasoning and simulation protocols, we discuss the use of these models and we develop a practical strategy aiming to (a) validate the properties of the illness-death process, (b) estimate the impact of time to illness on the hazard from state 1 to 2, and (c) quantify the impact that the transition into state 1 has on the hazard of the absorbing state. The strategy is also applied to a literature dataset on diabetes.},
  archive      = {J_BIMJ},
  author       = {Elena Tassistro and Davide Paolo Bernasconi and Paola Rebora and Maria Grazia Valsecchi and Laura Antolini},
  doi          = {10.1002/bimj.201800267},
  journal      = {Biometrical Journal},
  month        = {5},
  number       = {3},
  pages        = {836-851},
  shortjournal = {Bio. J.},
  title        = {Modeling the hazard of transition into the absorbing state in the illness-death model},
  volume       = {62},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Dynamic prediction: A challenge for biostatisticians, but
greatly needed by patients, physicians and the public. <em>BIMJ</em>,
<em>62</em>(3), 822–835. (<a
href="https://doi.org/10.1002/bimj.201800248">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Prognosis is usually expressed in terms of the probability that a patient will or will not have experienced an event of interest t years after diagnosis of a disease. This quantity, however, is of little informative value for a patient who is still event-free after a number of years. Such a patient would be much more interested in the conditional probability of being event-free in the upcoming t years, given that he/she did not experience the event in the s years after diagnosis, called “conditional survival.” It is the simplest form of a dynamic prediction and can be dealt with using straightforward extensions of standard time-to-event analyses in clinical cohort studies. For a healthy individual, a related problem with further complications is the so-called “age-conditional probability of developing cancer” in the next t years. Here, the competing risk of dying from other diseases has to be taken into account. For both situations, the hazard function provides the central dynamic concept, which can be further extended in a natural way to build dynamic prediction models that incorporate both baseline and time-dependent characteristics. Such models are able to exploit the most current information accumulating over time in order to accurately predict the further course or development of a disease. In this article, the biostatistical challenges as well as the relevance and importance of dynamic prediction are illustrated using studies of multiple myeloma, a hematologic malignancy with a formerly rather poor prognosis which has improved over the last few years.},
  archive      = {J_BIMJ},
  author       = {Martin Schumacher and Stefanie Hieke and Gabriele Ihorst and Monika Engelhardt},
  doi          = {10.1002/bimj.201800248},
  journal      = {Biometrical Journal},
  month        = {5},
  number       = {3},
  pages        = {822-835},
  shortjournal = {Bio. J.},
  title        = {Dynamic prediction: A challenge for biostatisticians, but greatly needed by patients, physicians and the public},
  volume       = {62},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Network meta-analysis of multicomponent interventions.
<em>BIMJ</em>, <em>62</em>(3), 808–821. (<a
href="https://doi.org/10.1002/bimj.201800167">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In network meta-analysis (NMA), treatments can be complex interventions, for example, some treatments may be combinations of others or of common components. In standard NMA, all existing (single or combined) treatments are different nodes in the network. However, sometimes an alternative model is of interest that utilizes the information that some treatments are combinations of common components, called component network meta-analysis (CNMA) model. The additive CNMA model assumes that the effect of a treatment combined of two components A and B is the sum of the effects of A and B, which is easily extended to treatments composed of more than two components. This implies that in comparisons equal components cancel out. Interaction CNMA models also allow interactions between the components. Bayesian analyses have been suggested. We report an implementation of CNMA models in the frequentist R package netmeta . All parameters are estimated using weighted least squares regression. We illustrate the application of CNMA models using an NMA of treatments for depression in primary care. Moreover, we show that these models can even be applied to disconnected networks, if the composite treatments in the subnetworks contain common components.},
  archive      = {J_BIMJ},
  author       = {Gerta Rücker and Maria Petropoulou and Guido Schwarzer},
  doi          = {10.1002/bimj.201800167},
  journal      = {Biometrical Journal},
  month        = {5},
  number       = {3},
  pages        = {808-821},
  shortjournal = {Bio. J.},
  title        = {Network meta-analysis of multicomponent interventions},
  volume       = {62},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). On the relation between the cause-specific hazard and the
subdistribution rate for competing risks data: The fine–gray model
revisited. <em>BIMJ</em>, <em>62</em>(3), 790–807. (<a
href="https://doi.org/10.1002/bimj.201800274">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Fine–Gray proportional subdistribution hazards model has been puzzling many people since its introduction. The main reason for the uneasy feeling is that the approach considers individuals still at risk for an event of cause 1 after they fell victim to the competing risk of cause 2. The subdistribution hazard and the extended risk sets, where subjects who failed of the competing risk remain in the risk set, are generally perceived as unnatural . One could say it is somewhat of a riddle why the Fine–Gray approach yields valid inference. To take away these uneasy feelings, we explore the link between the Fine–Gray and cause-specific approaches in more detail. We introduce the reduction factor as representing the proportion of subjects in the Fine–Gray risk set that has not yet experienced a competing event. In the presence of covariates, the dependence of the reduction factor on a covariate gives information on how the effect of the covariate on the cause-specific hazard and the subdistribution hazard relate. We discuss estimation and modeling of the reduction factor, and show how they can be used in various ways to estimate cumulative incidences, given the covariates. Methods are illustrated on data of the European Society for Blood and Marrow Transplantation.},
  archive      = {J_BIMJ},
  author       = {Hein Putter and Martin Schumacher and Hans C. van Houwelingen},
  doi          = {10.1002/bimj.201800274},
  journal      = {Biometrical Journal},
  month        = {5},
  number       = {3},
  pages        = {790-807},
  shortjournal = {Bio. J.},
  title        = {On the relation between the cause-specific hazard and the subdistribution rate for competing risks data: The Fine–Gray model revisited},
  volume       = {62},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Integrated evaluation of targeted and non-targeted therapies
in a network meta-analysis. <em>BIMJ</em>, <em>62</em>(3), 777–789. (<a
href="https://doi.org/10.1002/bimj.201800322">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Individualized therapies for patients with biomarkers are moving more and more into the focus of research interest when developing new treatments. Hereby, the term individualized (or targeted) therapy denotes a treatment specifically developed for biomarker-positive patients. A network meta-analysis model for a binary endpoint combining the evidence for a targeted therapy from individual patient data with the evidence for a non-targeted therapy from aggregate data is presented and investigated. The biomarker status of the patients is either available at patient-level in individual patient data or at study-level in aggregate data. Both types of biomarker information have to be included. The evidence synthesis model follows a Bayesian approach and applies a meta-regression to the studies with aggregate data. In a simulation study, we address three treatment arms, one of them investigating a targeted therapy. The bias and the root-mean-square error of the treatment effect estimate for the subgroup of biomarker-positive patients based on studies with aggregate data are investigated. Thereby, the meta-regression approach is compared to approaches applying alternative solutions. The regression approach has a surprisingly small bias even in the presence of few studies. By contrast, the root-mean-square error is relatively greater. An illustrative example is provided demonstrating implementation of the presented network meta-analysis model in a clinical setting.},
  archive      = {J_BIMJ},
  author       = {Tanja Proctor and Katrin Jensen and Meinhard Kieser},
  doi          = {10.1002/bimj.201800322},
  journal      = {Biometrical Journal},
  month        = {5},
  number       = {3},
  pages        = {777-789},
  shortjournal = {Bio. J.},
  title        = {Integrated evaluation of targeted and non-targeted therapies in a network meta-analysis},
  volume       = {62},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Estimating the decision curve and its precision from three
study designs. <em>BIMJ</em>, <em>62</em>(3), 764–776. (<a
href="https://doi.org/10.1002/bimj.201800240">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The decision curve plots the net benefit of a risk model for making decisions over a range of risk thresholds, corresponding to different ratios of misclassification costs. We discuss three methods to estimate the decision curve, together with corresponding methods of inference and methods to compare two risk models at a given risk threshold. One method uses risks ( R ) and a binary event indicator ( Y ) on the entire validation cohort. This method makes no assumptions on how well-calibrated the risk model is nor on the incidence of disease in the population and is comparatively robust to model miscalibration. If one assumes that the model is well-calibrated, one can compute a much more precise estimate of based on risks R alone. However, if the risk model is miscalibrated, serious bias can result. Case–control data can also be used to estimate if the incidence (or prevalence) of the event ( ) is known. This strategy has comparable efficiency to using the full data, and its efficiency is only modestly less than that for the full data if the incidence is estimated from the mean of Y . We estimate variances using influence functions and propose a bootstrap procedure to obtain simultaneous confidence bands around the decision curve for a range of thresholds. The influence function approach to estimate variances can also be applied to cohorts derived from complex survey samples instead of simple random samples.},
  archive      = {J_BIMJ},
  author       = {Ruth M. Pfeiffer and Mitchell H. Gail},
  doi          = {10.1002/bimj.201800240},
  journal      = {Biometrical Journal},
  month        = {5},
  number       = {3},
  pages        = {764-776},
  shortjournal = {Bio. J.},
  title        = {Estimating the decision curve and its precision from three study designs},
  volume       = {62},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). On the estimation of average treatment effects with
right-censored time to event outcome and competing risks. <em>BIMJ</em>,
<em>62</em>(3), 751–763. (<a
href="https://doi.org/10.1002/bimj.201800298">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We are interested in the estimation of average treatment effects based on right-censored data of an observational study. We focus on causal inference of differences between t -year absolute event risks in a situation with competing risks. We derive doubly robust estimation equations and implement estimators for the nuisance parameters based on working regression models for the outcome, censoring, and treatment distribution conditional on auxiliary baseline covariates. We use the functional delta method to show that these estimators are regular asymptotically linear estimators and estimate their variances based on estimates of their influence functions. In empirical studies, we assess the robustness of the estimators and the coverage of confidence intervals. The methods are further illustrated using data from a Danish registry study.},
  archive      = {J_BIMJ},
  author       = {Brice Maxime Hugues Ozenne and Thomas Harder Scheike and Laila Stærk and Thomas Alexander Gerds},
  doi          = {10.1002/bimj.201800298},
  journal      = {Biometrical Journal},
  month        = {5},
  number       = {3},
  pages        = {751-763},
  shortjournal = {Bio. J.},
  title        = {On the estimation of average treatment effects with right-censored time to event outcome and competing risks},
  volume       = {62},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). On the interpretation of the hazard ratio in cox regression.
<em>BIMJ</em>, <em>62</em>(3), 742–750. (<a
href="https://doi.org/10.1002/bimj.201800255">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We argue that the term “relative risk” should not be used as a synonym for “hazard ratio” and encourage to use the probabilistic index as an alternative effect measure for Cox regression. The probabilistic index is the probability that the event time of an exposed or treated subject exceeds the event time of an unexposed or untreated subject conditional on the other covariates. It arises as a well known and simple transformation of the hazard ratio and nicely reveals the interpretational limitations. We demonstrate how the probabilistic index can be obtained using the R-package Publish.},
  archive      = {J_BIMJ},
  author       = {Jan De Neve and Thomas A. Gerds},
  doi          = {10.1002/bimj.201800255},
  journal      = {Biometrical Journal},
  month        = {5},
  number       = {3},
  pages        = {742-750},
  shortjournal = {Bio. J.},
  title        = {On the interpretation of the hazard ratio in cox regression},
  volume       = {62},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Construction and assessment of prediction rules for binary
outcome in the presence of missing predictor data using multiple
imputation and cross-validation: Methodological approach and data-based
evaluation. <em>BIMJ</em>, <em>62</em>(3), 724–741. (<a
href="https://doi.org/10.1002/bimj.201800289">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We investigate calibration and assessment of predictive rules when missing values are present in the predictors. Our paper has two key objectives. The first is to investigate how the calibration of the prediction rule can be combined with use of multiple imputation to account for missing predictor observations. The second objective is to propose such methods that can be implemented with current multiple imputation software, while allowing for unbiased predictive assessment through validation on new observations for which outcome is not yet available. We commence with a review of the methodological foundations of multiple imputation as a model estimation approach as opposed to a purely algorithmic description. We specifically contrast application of multiple imputation for parameter (effect) estimation with predictive calibration. Based on this review, two approaches are formulated, of which the second utilizes application of the classical Rubin&#39;s rules for parameter estimation, while the first approach averages probabilities from models fitted on single imputations to directly approximate the predictive density for future observations. We present implementations using current software that allow for validation and estimation of performance measures by cross-validation, as well as imputation of missing data in predictors on the future data where outcome is missing by definition. To simplify, we restrict discussion to binary outcome and logistic regression throughout. Method performance is verified through application on two real data sets. Accuracy (Brier score) and variance of predicted probabilities are investigated. Results show substantial reductions in variation of calibrated probabilities when using the first approach.},
  archive      = {J_BIMJ},
  author       = {Bart J. A. Mertens and Erika Banzato and Liesbeth C. de Wreede},
  doi          = {10.1002/bimj.201800289},
  journal      = {Biometrical Journal},
  month        = {5},
  number       = {3},
  pages        = {724-741},
  shortjournal = {Bio. J.},
  title        = {Construction and assessment of prediction rules for binary outcome in the presence of missing predictor data using multiple imputation and cross-validation: Methodological approach and data-based evaluation},
  volume       = {62},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Meta-analysis of clinical trials with competing
time-to-event endpoints. <em>BIMJ</em>, <em>62</em>(3), 712–723. (<a
href="https://doi.org/10.1002/bimj.201900103">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recommendations for the analysis of competing risks in the context of randomized clinical trials are well established. Meta-analysis of individual patient data (IPD) is the gold standard for synthesizing evidence for clinical interpretation based on multiple studies. Surprisingly, no formal guidelines have been yet proposed to conduct an IPD meta-analysis with competing risk endpoints. To fill this gap, this work details (i) how to handle the heterogeneity between trials via a stratified regression model for competing risks and (ii) that the usual metrics of inconsistency to assess heterogeneity can readily be employed. Our proposal is illustrated by the re-analysis of a recently published meta-analysis in nasopharyngeal carcinoma, aiming at quantifying the benefit of the addition of chemotherapy to radiotherapy on each competing endpoint.},
  archive      = {J_BIMJ},
  author       = {Alessandra Meddis and Aurélien Latouche and Bingqing Zhou and Stefan Michiels and Jason Fine},
  doi          = {10.1002/bimj.201900103},
  journal      = {Biometrical Journal},
  month        = {5},
  number       = {3},
  pages        = {712-723},
  shortjournal = {Bio. J.},
  title        = {Meta-analysis of clinical trials with competing time-to-event endpoints},
  volume       = {62},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Estimating the distribution of heterogeneous treatment
effects from treatment responses and from a predictive biomarker in a
parallel-group RCT: A structural model approach. <em>BIMJ</em>,
<em>62</em>(3), 697–711. (<a
href="https://doi.org/10.1002/bimj.201800370">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {When the objective is to administer the best of two treatments to an individual, it is necessary to know his or her individual treatment effects (ITEs) and the correlation between the potential responses (PRs) and under treatments 1 and 0. Data that are generated in a parallel-group design RCT does not allow the ITE to be determined because only two samples from the marginal distributions of these PRs are observed and not the corresponding joint distribution. This is due to the “fundamental problem of causal inference.” Here, we present a counterfactual approach for estimating the joint distribution of two normally distributed responses to two treatments. This joint distribution of the PRs and can be estimated by assuming a bivariate normal distribution for the PRs and by using a normally distributed baseline biomarker functionally related to the sum . Such a functional relationship is plausible since a biomarker and the sum encode for the same information in an RCT, namely the variation between subjects. The estimation of the joint trivariate distribution is subjected to some constraints. These constraints can be framed in the context of linear regressions with regard to the proportions of variances in the responses explained and with regard to the residual variation. This presents new insights on the presence of treatment–biomarker interactions. We applied our approach to example data on exercise and heart rate and extended the approach to survival data.},
  archive      = {J_BIMJ},
  author       = {Ruediger P. Laubender and Ulrich Mansmann and Michael Lauseker},
  doi          = {10.1002/bimj.201800370},
  journal      = {Biometrical Journal},
  month        = {5},
  number       = {3},
  pages        = {697-711},
  shortjournal = {Bio. J.},
  title        = {Estimating the distribution of heterogeneous treatment effects from treatment responses and from a predictive biomarker in a parallel-group RCT: A structural model approach},
  volume       = {62},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). IV estimation without distributional assumptions.
<em>BIMJ</em>, <em>62</em>(3), 688–696. (<a
href="https://doi.org/10.1002/bimj.201800277">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {It is widely known that Instrumental Variable (IV) estimation allows the researcher to estimate causal effects between an exposure and an outcome even in face of serious uncontrolled confounding. The key requirement for IV estimation is the existence of a variable, the instrument, which only affects the outcome through its effects on the exposure and that the instrument–outcome relationship is unconfounded. Countless papers have employed such techniques and carefully addressed the validity of the IV assumption just mentioned. However, less appreciated is that fact that the IV estimation also depends on a number of distributional assumptions in particular linearities. In this paper, we propose a novel bounding procedure which can bound the true causal effect relying only on the key IV assumption and not on any distributional assumptions. For a purely binary case (instrument, exposure, and outcome all binary), such boundaries have been proposed by Balke and Pearl in 1997. We extend such boundaries to non-binary settings. In addition, our procedure offers a tuning parameter such that one can go from the traditional IV analysis, which provides a point estimate, to a completely unrestricted bound and anything in between. Subject matter knowledge can be used when setting the tuning parameter. To the best of our knowledge, no such methods exist elsewhere. The method is illustrated using a pivotal study which introduced IV estimation to epidemiologists. Here, we demonstrate that the conclusion of this paper indeed hinges on these additional distributional assumptions. R-code is provided in the Supporting Information.},
  archive      = {J_BIMJ},
  author       = {Theis Lange and Aksel K. G. Jensen},
  doi          = {10.1002/bimj.201800277},
  journal      = {Biometrical Journal},
  month        = {5},
  number       = {3},
  pages        = {688-696},
  shortjournal = {Bio. J.},
  title        = {IV estimation without distributional assumptions},
  volume       = {62},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Sampling uncertainty versus method uncertainty: A general
framework with applications to omics biomarker selection. <em>BIMJ</em>,
<em>62</em>(3), 670–687. (<a
href="https://doi.org/10.1002/bimj.201800309">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Uncertainty is a crucial issue in statistics which can be considered from different points of view. One type of uncertainty, typically referred to as sampling uncertainty, arises through the variability of results obtained when the same analysis strategy is applied to different samples. Another type of uncertainty arises through the variability of results obtained when using the same sample but different analysis strategies addressing the same research question. We denote this latter type of uncertainty as method uncertainty. It results from all the choices to be made for an analysis, for example, decisions related to data preparation, method choice, or model selection. In medical sciences, a large part of omics research is focused on the identification of molecular biomarkers, which can either be performed through ranking or by selection from among a large number of candidates. In this paper, we introduce a general resampling-based framework to quantify and compare sampling and method uncertainty. For illustration, we apply this framework to different scenarios related to the selection and ranking of omics biomarkers in the context of acute myeloid leukemia: variable selection in multivariable regression using different types of omics markers, the ranking of biomarkers according to their predictive performance, and the identification of differentially expressed genes from RNA-seq data. For all three scenarios, our findings suggest highly unstable results when the same analysis strategy is applied to two independent samples, indicating high sampling uncertainty and a comparatively smaller, but non-negligible method uncertainty, which strongly depends on the methods being compared.},
  archive      = {J_BIMJ},
  author       = {Simon Klau and Marie-Laure Martin-Magniette and Anne-Laure Boulesteix and Sabine Hoffmann},
  doi          = {10.1002/bimj.201800309},
  journal      = {Biometrical Journal},
  month        = {5},
  number       = {3},
  pages        = {670-687},
  shortjournal = {Bio. J.},
  title        = {Sampling uncertainty versus method uncertainty: A general framework with applications to omics biomarker selection},
  volume       = {62},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Quantitative assessment of adverse events in clinical
trials: Comparison of methods at an interim and the final analysis.
<em>BIMJ</em>, <em>62</em>(3), 658–669. (<a
href="https://doi.org/10.1002/bimj.201800234">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In clinical study reports (CSRs), adverse events (AEs) are commonly summarized using the incidence proportion (IP). IPs can be calculated for all types of AEs and are often interpreted as the probability that a treated patient experiences specific AEs. Exposure time can be taken into account with time-to-event methods. Using one minus Kaplan–Meier (1-KM) is known to overestimate the AE probability in the presence of competing events (CEs). The use of a nonparametric estimator of the cumulative incidence function (CIF) has therefore been advocated as more appropriate. In this paper, we compare different methods to estimate the probability of one selected AE. In particular, we investigate whether the proposed methods provide a reasonable estimate of the AE probability at an interim analysis (IA). The characteristics of the methods in the presence of a CE are illustrated using data from a breast cancer study and we quantify the potential bias in a simulation study. At the final analysis performed for the CSR, 1-KM systematically overestimates and in most cases IP slightly underestimates the given AE probability. CIF has the lowest bias in most simulation scenarios. All methods might lead to biased estimates at the IA except for AEs with early onset. The magnitude of the bias varies with the time-to-AE and/or CE occurrence, the selection of event-specific hazards and the amount of censoring. In general, reporting AE probabilities for prespecified fixed time points is recommended.},
  archive      = {J_BIMJ},
  author       = {Norbert Hollaender and Juan Gonzalez-Maffe and Valentine Jehl},
  doi          = {10.1002/bimj.201800234},
  journal      = {Biometrical Journal},
  month        = {5},
  number       = {3},
  pages        = {658-669},
  shortjournal = {Bio. J.},
  title        = {Quantitative assessment of adverse events in clinical trials: Comparison of methods at an interim and the final analysis},
  volume       = {62},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Validation of discrete time-to-event prediction models in
the presence of competing risks. <em>BIMJ</em>, <em>62</em>(3), 643–657.
(<a href="https://doi.org/10.1002/bimj.201800293">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Clinical prediction models play a key role in risk stratification, therapy assignment and many other fields of medical decision making. Before they can enter clinical practice, their usefulness has to be demonstrated using systematic validation. Methods to assess their predictive performance have been proposed for continuous, binary, and time-to-event outcomes, but the literature on validation methods for discrete time-to-event models with competing risks is sparse. The present paper tries to fill this gap and proposes new methodology to quantify discrimination, calibration, and prediction error (PE) for discrete time-to-event outcomes in the presence of competing risks. In our case study, the goal was to predict the risk of ventilator-associated pneumonia (VAP) attributed to Pseudomonas aeruginosa in intensive care units (ICUs). Competing events are extubation, death, and VAP due to other bacteria. The aim of this application is to validate complex prediction models developed in previous work on more recently available validation data.},
  archive      = {J_BIMJ},
  author       = {Rachel Heyard and Jean-François Timsit and Leonhard Held},
  doi          = {10.1002/bimj.201800293},
  journal      = {Biometrical Journal},
  month        = {5},
  number       = {3},
  pages        = {643-657},
  shortjournal = {Bio. J.},
  title        = {Validation of discrete time-to-event prediction models in the presence of competing risks},
  volume       = {62},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Adjustment for exploratory cut-off selection in randomized
clinical trials with survival endpoint. <em>BIMJ</em>, <em>62</em>(3),
627–642. (<a href="https://doi.org/10.1002/bimj.201800302">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Defining the target population based on predictive biomarkers plays an important role during clinical development. After establishing a relationship between a biomarker candidate and response to treatment in exploratory phases, a subsequent confirmatory trial ideally involves only subjects with high potential of benefiting from the new compound. In order to identify those subjects in case of a continuous biomarker, a cut-off is needed. Usually, a cut-off is chosen that resulted in a subgroup with a large observed treatment effect in an exploratory trial. However, such a data-driven selection may lead to overoptimistic expectations for the subsequent confirmatory trial. Treatment effect estimates, probability of success, and posterior probabilities are useful measures for deciding whether or not to conduct a confirmatory trial enrolling the biomarker-defined population. These measures need to be adjusted for selection bias. We extend previously introduced Approximate Bayesian Computation techniques for adjustment of subgroup selection bias to a time-to-event setting with cut-off selection. Challenges in this setting are that treatment effects become time-dependent and that subsets are defined by the biomarker distribution. Simulation studies show that the proposed method provides adjusted statistical measures which are superior to naïve Maximum Likelihood estimators as well as simple shrinkage estimators.},
  archive      = {J_BIMJ},
  author       = {Heiko Götte and Marietta Kirchner and Meinhard Kieser},
  doi          = {10.1002/bimj.201800302},
  journal      = {Biometrical Journal},
  month        = {5},
  number       = {3},
  pages        = {627-642},
  shortjournal = {Bio. J.},
  title        = {Adjustment for exploratory cut-off selection in randomized clinical trials with survival endpoint},
  volume       = {62},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Marginal variable screening for survival endpoints.
<em>BIMJ</em>, <em>62</em>(3), 610–626. (<a
href="https://doi.org/10.1002/bimj.201800269">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {When performing survival analysis in very high dimensions, it is often required to reduce the number of covariates using preliminary screening. During the last years, a large number of variable screening methods for the survival context have been developed. However, guidance is missing for choosing an appropriate method in practice. The aim of this work is to provide an overview of marginal variable screening methods for survival and develop recommendations for their use. For this purpose, a literature review is given, offering a comprehensive and structured introduction to the topic. In addition, a novel screening procedure based on distance correlation and martingale residuals is proposed, which is particularly useful in detecting nonmonotone associations. For evaluating the performance of the discussed approaches, a simulation study is conducted, comparing the true positive rates of competing variable screening methods in different settings. A real data example on mantle cell lymphoma is provided.},
  archive      = {J_BIMJ},
  author       = {Dominic Edelmann and Manuela Hummel and Thomas Hielscher and Maral Saadati and Axel Benner},
  doi          = {10.1002/bimj.201800269},
  journal      = {Biometrical Journal},
  month        = {5},
  number       = {3},
  pages        = {610-626},
  shortjournal = {Bio. J.},
  title        = {Marginal variable screening for survival endpoints},
  volume       = {62},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). On the use of comparison regions in visualizing stochastic
uncertainty in some two-parameter estimation problems. <em>BIMJ</em>,
<em>62</em>(3), 598–609. (<a
href="https://doi.org/10.1002/bimj.201800232">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {When considering simultaneous inference for two parameters, it is very common to visualize stochastic uncertainty by plotting two-dimensional confidence regions. This allows us to test post hoc null hypotheses about a single point in a simple manner. However, in some applications the interest is not in rejecting hypotheses on single points, but in demonstrating evidence for the two parameters to be in a convex subset of the parameter space. The specific convex subset to be considered may vary from one post hoc analysis to another. Then it is of interest to have a visualization allowing to perform corresponding analyses. We suggest comparison regions as a simple tool for this task.},
  archive      = {J_BIMJ},
  author       = {Maren Eckert and Werner Vach},
  doi          = {10.1002/bimj.201800232},
  journal      = {Biometrical Journal},
  month        = {5},
  number       = {3},
  pages        = {598-609},
  shortjournal = {Bio. J.},
  title        = {On the use of comparison regions in visualizing stochastic uncertainty in some two-parameter estimation problems},
  volume       = {62},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). The population-attributable fraction for time-dependent
exposures using dynamic prediction and landmarking. <em>BIMJ</em>,
<em>62</em>(3), 583–597. (<a
href="https://doi.org/10.1002/bimj.201800252">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The public health impact of a harmful exposure can be quantified by the population-attributable fraction (PAF). The PAF describes the attributable risk due to an exposure and is often interpreted as the proportion of preventable cases if the exposure was extinct. Difficulties in the definition and interpretation of the PAF arise when the exposure of interest depends on time. Then, the definition of exposed and unexposed individuals is not straightforward. We propose dynamic prediction and landmarking to define and estimate a PAF in this data situation. Two estimands are discussed which are based on two hypothetical interventions that could prevent the exposure in different ways. Considering the first estimand, at each landmark the estimation problem is reduced to a time-independent setting. Then, estimation is simply performed by using a generalized-linear model accounting for the current exposure state and further (time-varying) covariates. The second estimand is based on counterfactual outcomes, estimation can be performed using pseudo-values or inverse-probability weights. The approach is explored in a simulation study and applied on two data examples. First, we study a large French database of intensive care unit patients to estimate the population-benefit of a pathogen-specific intervention that could prevent ventilator-associated pneumonia caused by the pathogen Pseudomonas aeruginosa . Moreover, we quantify the population-attributable burden of locoregional and distant recurrence in breast cancer patients.},
  archive      = {J_BIMJ},
  author       = {Maja von Cube and Martin Schumacher and Hein Putter and Jéan-François Timsit and Cornelis van de Velde and Martin Wolkewitz},
  doi          = {10.1002/bimj.201800252},
  journal      = {Biometrical Journal},
  month        = {5},
  number       = {3},
  pages        = {583-597},
  shortjournal = {Bio. J.},
  title        = {The population-attributable fraction for time-dependent exposures using dynamic prediction and landmarking},
  volume       = {62},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Comparison of complex modeling strategies for prediction of
a binary outcome based on a few, highly correlated predictors.
<em>BIMJ</em>, <em>62</em>(3), 568–582. (<a
href="https://doi.org/10.1002/bimj.201800243">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Motivated by a clinical prediction problem, a simulation study was performed to compare different approaches for building risk prediction models. Robust prediction models for hospital survival in patients with acute heart failure were to be derived from three highly correlated blood parameters measured up to four times, with predictive ability having explicit priority over interpretability. Methods that relied only on the original predictors were compared with methods using an expanded predictor space including transformations and interactions. Predictors were simulated as transformations and combinations of multivariate normal variables which were fitted to the partly skewed and bimodally distributed original data in such a way that the simulated data mimicked the original covariate structure. Different penalized versions of logistic regression as well as random forests and generalized additive models were investigated using classical logistic regression as a benchmark. Their performance was assessed based on measures of predictive accuracy, model discrimination, and model calibration. Three different scenarios using different subsets of the original data with different numbers of observations and events per variable were investigated. In the investigated setting, where a risk prediction model should be based on a small set of highly correlated and interconnected predictors, Elastic Net and also Ridge logistic regression showed good performance compared to their competitors, while other methods did not lead to substantial improvements or even performed worse than standard logistic regression. Our work demonstrates how simulation studies that mimic relevant features of a specific data set can support the choice of a good modeling strategy.},
  archive      = {J_BIMJ},
  author       = {Marco Chiabudini and Martin Schumacher and Erika Graf},
  doi          = {10.1002/bimj.201800243},
  journal      = {Biometrical Journal},
  month        = {5},
  number       = {3},
  pages        = {568-582},
  shortjournal = {Bio. J.},
  title        = {Comparison of complex modeling strategies for prediction of a binary outcome based on a few, highly correlated predictors},
  volume       = {62},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A multistate model for early decision-making in oncology.
<em>BIMJ</em>, <em>62</em>(3), 550–567. (<a
href="https://doi.org/10.1002/bimj.201800250">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The development of oncology drugs progresses through multiple phases, where after each phase, a decision is made about whether to move a molecule forward. Early phase efficacy decisions are often made on the basis of single-arm studies based on a set of rules to define whether the tumor improves (“responds”), remains stable, or progresses (response evaluation criteria in solid tumors [RECIST]). These decision rules are implicitly assuming some form of surrogacy between tumor response and long-term endpoints like progression-free survival (PFS) or overall survival (OS). With the emergence of new therapies, for which the link between RECIST tumor response and long-term endpoints is either not accessible yet, or the link is weaker than with classical chemotherapies, tumor response-based rules may not be optimal. In this paper, we explore the use of a multistate model for decision-making based on single-arm early phase trials. The multistate model allows to account for more information than the simple RECIST response status, namely, the time to get to response, the duration of response, the PFS time, and time to death. We propose to base the decision on efficacy on the OS hazard ratio (HR) comparing historical control to data from the experimental treatment, with the latter predicted from a multistate model based on early phase data with limited survival follow-up. Using two case studies, we illustrate feasibility of the estimation of such an OS HR. We argue that, in the presence of limited follow-up and small sample size, and making realistic assumptions within the multistate model, the OS prediction is acceptable and may lead to better early decisions within the development of a drug.},
  archive      = {J_BIMJ},
  author       = {Ulrich Beyer and David Dejardin and Matthias Meller and Kaspar Rufibach and Hans Ulrich Burger},
  doi          = {10.1002/bimj.201800250},
  journal      = {Biometrical Journal},
  month        = {5},
  number       = {3},
  pages        = {550-567},
  shortjournal = {Bio. J.},
  title        = {A multistate model for early decision-making in oncology},
  volume       = {62},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Time-dependent mediators in survival analysis: Modeling
direct and indirect effects with the additive hazards model.
<em>BIMJ</em>, <em>62</em>(3), 532–549. (<a
href="https://doi.org/10.1002/bimj.201800263">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We discuss causal mediation analyses for survival data and propose a new approach based on the additive hazards model. The emphasis is on a dynamic point of view, that is, understanding how the direct and indirect effects develop over time. Hence, importantly, we allow for a time varying mediator. To define direct and indirect effects in such a longitudinal survival setting we take an interventional approach (Didelez, 2018) where treatment is separated into one aspect affecting the mediator and a different aspect affecting survival. In general, this leads to a version of the nonparametric g-formula (Robins, 1986). In the present paper, we demonstrate that combining the g-formula with the additive hazards model and a sequential linear model for the mediator process results in simple and interpretable expressions for direct and indirect effects in terms of relative survival as well as cumulative hazards. Our results generalize and formalize the method of dynamic path analysis (Fosen, Ferkingstad, Borgan, &amp; Aalen, 2006; Strohmaier et al., 2015). An application to data from a clinical trial on blood pressure medication is given.},
  archive      = {J_BIMJ},
  author       = {Odd O. Aalen and Mats J. Stensrud and Vanessa Didelez and Rhian Daniel and Kjetil Røysland and Susanne Strohmaier},
  doi          = {10.1002/bimj.201800263},
  journal      = {Biometrical Journal},
  month        = {5},
  number       = {3},
  pages        = {532-549},
  shortjournal = {Bio. J.},
  title        = {Time-dependent mediators in survival analysis: Modeling direct and indirect effects with the additive hazards model},
  volume       = {62},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Statistical models for complex data in clinical and
epidemiological research. <em>BIMJ</em>, <em>62</em>(3), 528–531. (<a
href="https://doi.org/10.1002/bimj.202000079">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_BIMJ},
  author       = {Jan Beyersmann and Willi Sauerbrei and Claudia Schmoor},
  doi          = {10.1002/bimj.202000079},
  journal      = {Biometrical Journal},
  month        = {5},
  number       = {3},
  pages        = {528-531},
  shortjournal = {Bio. J.},
  title        = {Statistical models for complex data in clinical and epidemiological research},
  volume       = {62},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020c). Contents: Biometrical journal 3’20. <em>BIMJ</em>,
<em>62</em>(3), 525–527. (<a
href="https://doi.org/10.1002/bimj.202070034">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_BIMJ},
  doi          = {10.1002/bimj.202070034},
  journal      = {Biometrical Journal},
  month        = {5},
  number       = {3},
  pages        = {525-527},
  shortjournal = {Bio. J.},
  title        = {Contents: Biometrical journal 3&#39;20},
  volume       = {62},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020b). Cover picture: Biometrical journal 2’20. <em>BIMJ</em>,
<em>62</em>(2), NA. (<a
href="https://doi.org/10.1002/bimj.202070021">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_BIMJ},
  doi          = {10.1002/bimj.202070021},
  journal      = {Biometrical Journal},
  month        = {3},
  number       = {2},
  pages        = {NA},
  shortjournal = {Bio. J.},
  title        = {Cover picture: Biometrical journal 2&#39;20},
  volume       = {62},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Nonlinear and time-dependent effects of sparsely measured
continuous time-varying covariates in time-to-event analysis.
<em>BIMJ</em>, <em>62</em>(2), 492–515. (<a
href="https://doi.org/10.1002/bimj.201900042">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many flexible extensions of the Cox proportional hazards model incorporate time-dependent (TD) and/or nonlinear (NL) effects of time-invariant covariates. In contrast, little attention has been given to the assessment of such effects for continuous time-varying covariates (TVCs). We propose a flexible regression B-spline–based model for TD and NL effects of a TVC. To account for sparse TVC measurements, we added to this model the effect of time elapsed since last observation (TEL), which acts as an effect modifier. TD, NL, and TEL effects are estimated with the iterative alternative conditional estimation algorithm. Furthermore, a simulation extrapolation (SIMEX)-like procedure was adapted to correct the estimated effects for random measurement errors in the observed TVC values. In simulations, TD and NL estimates were unbiased if the TVC was measured with a high frequency. With sparse measurements, the strength of the effects was underestimated but the TEL estimate helped reduce the bias, whereas SIMEX helped further to correct for bias toward the null due to “white noise” measurement errors. We reassessed the effects of systolic blood pressure (SBP) and total cholesterol, measured at two-year intervals, on cardiovascular risks in women participating in the Framingham Heart Study. Accounting for TD effects of SBP, cholesterol and age, the NL effect of cholesterol, and the TEL effect of SBP improved substantially the model&#39;s fit to data. Flexible estimates yielded clinically important insights regarding the role of these risk factors. These results illustrate the advantages of flexible modeling of TVC effects.},
  archive      = {J_BIMJ},
  author       = {Yishu Wang and Marie-Eve Beauchamp and Michal Abrahamowicz},
  doi          = {10.1002/bimj.201900042},
  journal      = {Biometrical Journal},
  month        = {3},
  number       = {2},
  pages        = {492-515},
  shortjournal = {Bio. J.},
  title        = {Nonlinear and time-dependent effects of sparsely measured continuous time-varying covariates in time-to-event analysis},
  volume       = {62},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Multilevel regression and poststratification as a modeling
approach for estimating population quantities in large population health
studies: A simulation study. <em>BIMJ</em>, <em>62</em>(2), 479–491. (<a
href="https://doi.org/10.1002/bimj.201900023">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {There are now a growing number of applications of multilevel regression and poststratification (MRP) in population health and epidemiological studies. MRP uses multilevel regression to model individual survey responses as a function of demographic and geographic covariates. Estimated mean outcome values for each demographic–geographic respondent subtype are then weighted by the proportions of each subtype in the population to produce an overall population-level estimate. We recently reported an extensive case study of a large nationwide survey and found that MRP performed favorably compared to conventional survey sampling weights for the estimation of population descriptive quantities in a highly selected sample. In this study, we aimed to evaluate, by way of a simulation experiment, both the accuracy and precision of MRP versus survey sampling weights in the context of large population health studies. While much of the research into MRP has been focused on U.S. political and social science, we considered an alternative population structure of smaller size and with notably fewer geographic subsets. We explored the impact on MRP performance of sample size, model misspecification, interactions, and the addition of a geographic-level covariate. MRP was found to achieve generally superior performance in both accuracy and precision at both the national and state levels. Results were generally robust to model misspecification, and MRP performance was further improved by the inclusion of a geographic-level covariate. These findings offer further evidence that MRP provides a promising analytic approach for addressing participation bias in the estimation of population descriptive quantities from large-scale health surveys and cohort studies.},
  archive      = {J_BIMJ},
  author       = {Marnie Downes and John B. Carlin},
  doi          = {10.1002/bimj.201900023},
  journal      = {Biometrical Journal},
  month        = {3},
  number       = {2},
  pages        = {479-491},
  shortjournal = {Bio. J.},
  title        = {Multilevel regression and poststratification as a modeling approach for estimating population quantities in large population health studies: A simulation study},
  volume       = {62},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Multiple imputation in the presence of an incomplete binary
variable created from an underlying continuous variable. <em>BIMJ</em>,
<em>62</em>(2), 467–478. (<a
href="https://doi.org/10.1002/bimj.201900011">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multiple imputation (MI) is used to handle missing at random (MAR) data. Despite warnings from statisticians, continuous variables are often recoded into binary variables. With MI it is important that the imputation and analysis models are compatible; variables should be imputed in the same form they appear in the analysis model. With an encoded binary variable more accurate imputations may be obtained by imputing the underlying continuous variable. We conducted a simulation study to explore how best to impute a binary variable that was created from an underlying continuous variable. We generated a completely observed continuous outcome associated with an incomplete binary covariate that is a categorized version of an underlying continuous covariate, and an auxiliary variable associated with the underlying continuous covariate. We simulated data with several sample sizes, and set 25% and 50% of data in the covariate to MAR dependent on the outcome and the auxiliary variable. We compared the performance of five different imputation methods: (a) Imputation of the binary variable using logistic regression; (b) imputation of the continuous variable using linear regression, then categorizing into the binary variable; (c, d) imputation of both the continuous and binary variables using fully conditional specification (FCS) and multivariate normal imputation; (e) substantive-model compatible (SMC) FCS. Bias and standard errors were large when the continuous variable only was imputed. The other methods performed adequately. Imputation of both the binary and continuous variables using FCS often encountered mathematical difficulties. We recommend the SMC-FCS method as it performed best in our simulation studies.},
  archive      = {J_BIMJ},
  author       = {Anneke C. Grobler and Katherine Lee},
  doi          = {10.1002/bimj.201900011},
  journal      = {Biometrical Journal},
  month        = {3},
  number       = {2},
  pages        = {467-478},
  shortjournal = {Bio. J.},
  title        = {Multiple imputation in the presence of an incomplete binary variable created from an underlying continuous variable},
  volume       = {62},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Multiple imputation methods for handling incomplete
longitudinal and clustered data where the target analysis is a linear
mixed effects model. <em>BIMJ</em>, <em>62</em>(2), 444–466. (<a
href="https://doi.org/10.1002/bimj.201900051">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multiple imputation (MI) is increasingly popular for handling multivariate missing data. Two general approaches are available in standard computer packages: MI based on the posterior distribution of incomplete variables under a multivariate (joint) model, and fully conditional specification (FCS), which imputes missing values using univariate conditional distributions for each incomplete variable given all the others, cycling iteratively through the univariate imputation models. In the context of longitudinal or clustered data, it is not clear whether these approaches result in consistent estimates of regression coefficient and variance component parameters when the analysis model of interest is a linear mixed effects model (LMM) that includes both random intercepts and slopes with either covariates or both covariates and outcome contain missing information. In the current paper, we compared the performance of seven different MI methods for handling missing values in longitudinal and clustered data in the context of fitting LMMs with both random intercepts and slopes. We study the theoretical compatibility between specific imputation models fitted under each of these approaches and the LMM, and also conduct simulation studies in both the longitudinal and clustered data settings. Simulations were motivated by analyses of the association between body mass index (BMI) and quality of life (QoL) in the Longitudinal Study of Australian Children (LSAC). Our findings showed that the relative performance of MI methods vary according to whether the incomplete covariate has fixed or random effects and whether there is missingnesss in the outcome variable. We showed that compatible imputation and analysis models resulted in consistent estimation of both regression parameters and variance components via simulation. We illustrate our findings with the analysis of LSAC data.},
  archive      = {J_BIMJ},
  author       = {Md Hamidul Huque and Margarita Moreno-Betancur and Matteo Quartagno and Julie A. Simpson and John B. Carlin and Katherine J. Lee},
  doi          = {10.1002/bimj.201900051},
  journal      = {Biometrical Journal},
  month        = {3},
  number       = {2},
  pages        = {444-466},
  shortjournal = {Bio. J.},
  title        = {Multiple imputation methods for handling incomplete longitudinal and clustered data where the target analysis is a linear mixed effects model},
  volume       = {62},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Estimating treatment effects with partially observed
covariates using outcome regression with missing indicators.
<em>BIMJ</em>, <em>62</em>(2), 428–443. (<a
href="https://doi.org/10.1002/bimj.201900041">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Missing data is a common issue in research using observational studies to investigate the effect of treatments on health outcomes. When missingness occurs only in the covariates, a simple approach is to use missing indicators to handle the partially observed covariates. The missing indicator approach has been criticized for giving biased results in outcome regression. However, recent papers have suggested that the missing indicator approach can provide unbiased results in propensity score analysis under certain assumptions. We consider assumptions under which the missing indicator approach can provide valid inferences, namely, (1) no unmeasured confounding within missingness patterns; either (2a) covariate values of patients with missing data were conditionally independent of treatment or (2b) these values were conditionally independent of outcome; and (3) the outcome model is correctly specified: specifically, the true outcome model does not include interactions between missing indicators and fully observed covariates. We prove that, under the assumptions above, the missing indicator approach with outcome regression can provide unbiased estimates of the average treatment effect. We use a simulation study to investigate the extent of bias in estimates of the treatment effect when the assumptions are violated and we illustrate our findings using data from electronic health records. In conclusion, the missing indicator approach can provide valid inferences for outcome regression, but the plausibility of its assumptions must first be considered carefully.},
  archive      = {J_BIMJ},
  author       = {Helen A. Blake and Clémence Leyrat and Kathryn E. Mansfield and Laurie A. Tomlinson and James Carpenter and Elizabeth J. Williamson},
  doi          = {10.1002/bimj.201900041},
  journal      = {Biometrical Journal},
  month        = {3},
  number       = {2},
  pages        = {428-443},
  shortjournal = {Bio. J.},
  title        = {Estimating treatment effects with partially observed covariates using outcome regression with missing indicators},
  volume       = {62},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Detecting possible persons of interest in a physical
activity program using step entries: Including a web-based application
for outlier detection and decision-making. <em>BIMJ</em>,
<em>62</em>(2), 414–427. (<a
href="https://doi.org/10.1002/bimj.201900008">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {According to recent statistics from the World Health Organization, 23% of people aged 18 years and over are not sufficiently physically active. Strangely, this is at a time when, due to the improvement in sensor technology, physical activity programs that track physical activity have become popular. However, some participants who enroll in these programs cheat by manipulating the data they enter. This can be discouraging for other participants, also invalidating the overall accuracy of program outcomes. Therefore, detecting these participants and discarding their manipulated entries is important in order to maintain the quality of the program. Currently, most of these physical activity programs use manual processes to detect and reject fraudulent step entries by reviewing the participant&#39;s demographic profiles along with their longitudinal step count performance data. In this study, a process, including two parallel models for detecting person of interest characteristics and abnormal step count entries, is developed. The first model uses the penalized logistic regression with Synthetic Minority Over-sampling Technique subsampling to address the imbalance in the proportion of genuine and persons of interest. Having a highly imbalanced distribution between genuine and person of interest profiles makes this task more challenging. The second model uses a variety of outlier detection methods to detect and reject abnormal step entries based on previously entered data. This process will be more efficient and productive compared to the current manual system and will support better decision-making in the future. The proposed system can be applied for other fraud detection applications after suitable adjustments.},
  archive      = {J_BIMJ},
  author       = {S. Sandun M. Silva and Denny Meyer and Madawa Jayawardana},
  doi          = {10.1002/bimj.201900008},
  journal      = {Biometrical Journal},
  month        = {3},
  number       = {2},
  pages        = {414-427},
  shortjournal = {Bio. J.},
  title        = {Detecting possible persons of interest in a physical activity program using step entries: Including a web-based application for outlier detection and decision-making},
  volume       = {62},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A tutorial on dynamic risk prediction of a binary outcome
based on a longitudinal biomarker. <em>BIMJ</em>, <em>62</em>(2),
398–413. (<a href="https://doi.org/10.1002/bimj.201900044">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dynamic risk predictions based on all available information are useful in timely identification of high-risk patients. However, in contrast with time to event outcomes, there is still a lack of studies that clearly demonstrate how to obtain and update predictions for a future binary outcome using a repeatedly measured biomarker. The aim of this study is to give an illustrative overview of four approaches to obtain such predictions: likelihood based two-stage method (2SMLE), likelihood based joint model (JMMLE), Bayesian two-stage method (2SB), and Bayesian joint model (JMB). We applied the approaches to provide weekly updated predictions of post–molar gestational trophoblastic neoplasia (GTN) based on age and repeated measurements of human chorionic gonadotropin (hCG). Discrimination and calibration measures were used to compare the accuracy of the weekly predictions. Internal validation of the models was conducted using bootstrapping. The four approaches resulted in the same predictive and discriminative performance in predicting GTN. A simulation study showed that the joint models outperform the two-stage methods when we increase the within- and the between-patients variability of the biomarker. The applicability of these models to produce dynamic predictions has been illustrated through a comprehensive explanation and accompanying syntax (R and SAS ® ).},
  archive      = {J_BIMJ},
  author       = {Rana Dandis and Steven Teerenstra and Leon Massuger and Fred Sweep and Yalck Eysbouts and Joanna IntHout},
  doi          = {10.1002/bimj.201900044},
  journal      = {Biometrical Journal},
  month        = {3},
  number       = {2},
  pages        = {398-413},
  shortjournal = {Bio. J.},
  title        = {A tutorial on dynamic risk prediction of a binary outcome based on a longitudinal biomarker},
  volume       = {62},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A utility approach to individualized optimal dose selection
using biomarkers. <em>BIMJ</em>, <em>62</em>(2), 386–397. (<a
href="https://doi.org/10.1002/bimj.201900030">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In many settings, including oncology, increasing the dose of treatment results in both increased efficacy and toxicity. With the increasing availability of validated biomarkers and prediction models, there is the potential for individualized dosing based on patient specific factors. We consider the setting where there is an existing dataset of patients treated with heterogenous doses and including binary efficacy and toxicity outcomes and patient factors such as clinical features and biomarkers. The goal is to analyze the data to estimate an optimal dose for each (future) patient based on their clinical features and biomarkers. We propose an optimal individualized dose finding rule by maximizing utility functions for individual patients while limiting the rate of toxicity. The utility is defined as a weighted combination of efficacy and toxicity probabilities. This approach maximizes overall efficacy at a prespecified constraint on overall toxicity. We model the binary efficacy and toxicity outcomes using logistic regression with dose, biomarkers and dose–biomarker interactions. To incorporate the large number of potential parameters, we use the LASSO method. We additionally constrain the dose effect to be non-negative for both efficacy and toxicity for all patients. Simulation studies show that the utility approach combined with any of the modeling methods can improve efficacy without increasing toxicity relative to fixed dosing. The proposed methods are illustrated using a dataset of patients with lung cancer treated with radiation therapy.},
  archive      = {J_BIMJ},
  author       = {Pin Li and Jeremy M.G. Taylor and Spring Kong and Shruti Jolly and Matthew J. Schipper},
  doi          = {10.1002/bimj.201900030},
  journal      = {Biometrical Journal},
  month        = {3},
  number       = {2},
  pages        = {386-397},
  shortjournal = {Bio. J.},
  title        = {A utility approach to individualized optimal dose selection using biomarkers},
  volume       = {62},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Extensions of the probabilistic ranking metrics of competing
treatments in network meta-analysis to reflect clinically important
relative differences on many outcomes. <em>BIMJ</em>, <em>62</em>(2),
375–385. (<a href="https://doi.org/10.1002/bimj.201900026">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {One of the key features of network meta-analysis is ranking of interventions according to outcomes of interest. Ranking metrics are prone to misinterpretation because of two limitations associated with the current ranking methods. First, differences in relative treatment effects might not be clinically important and this is not reflected in the ranking metrics. Second, there are no established methods to include several health outcomes in the ranking assessments. To address these two issues, we extended the P-score method to allow for multiple outcomes and modified it to measure the mean extent of certainty that a treatment is better than the competing treatments by a certain amount, for example, the minimum clinical important difference. We suggest to present the tradeoff between beneficial and harmful outcomes allowing stakeholders to consider how much adverse effect they are willing to tolerate for specific gains in efficacy. We used a published network of 212 trials comparing 15 antipsychotics and placebo using a random effects network meta-analysis model, focusing on three outcomes; reduction in symptoms of schizophrenia in a standardized scale, all-cause discontinuation, and weight gain.},
  archive      = {J_BIMJ},
  author       = {Dimitris Mavridis and Raphaël Porcher and Adriani Nikolakopoulou and Georgia Salanti and Philippe Ravaud},
  doi          = {10.1002/bimj.201900026},
  journal      = {Biometrical Journal},
  month        = {3},
  number       = {2},
  pages        = {375-385},
  shortjournal = {Bio. J.},
  title        = {Extensions of the probabilistic ranking metrics of competing treatments in network meta-analysis to reflect clinically important relative differences on many outcomes},
  volume       = {62},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Power gains by using external information in clinical trials
are typically not possible when requiring strict type i error control.
<em>BIMJ</em>, <em>62</em>(2), 361–374. (<a
href="https://doi.org/10.1002/bimj.201800395">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the era of precision medicine, novel designs are developed to deal with flexible clinical trials that incorporate many treatment strategies for multiple diseases in one trial setting. This situation often leads to small sample sizes in disease-treatment combinations and has fostered the discussion about the benefits of borrowing of external or historical information for decision-making in these trials. Several methods have been proposed that dynamically discount the amount of information borrowed from historical data based on the conformity between historical and current data. Specifically, Bayesian methods have been recommended and numerous investigations have been performed to characterize the properties of the various borrowing mechanisms with respect to the gain to be expected in the trials. However, there is common understanding that the risk of type I error inflation exists when information is borrowed and many simulation studies are carried out to quantify this effect. To add transparency to the debate, we show that if prior information is conditioned upon and a uniformly most powerful test exists, strict control of type I error implies that no power gain is possible under any mechanism of incorporation of prior information, including dynamic borrowing. The basis of the argument is to consider the test decision function as a function of the current data even when external information is included. We exemplify this finding in the case of a pediatric arm appended to an adult trial and dichotomous outcome for various methods of dynamic borrowing from adult information to the pediatric arm. In conclusion, if use of relevant external data is desired, the requirement of strict type I error control has to be replaced by more appropriate metrics.},
  archive      = {J_BIMJ},
  author       = {Annette Kopp-Schneider and Silvia Calderazzo and Manuel Wiesenfarth},
  doi          = {10.1002/bimj.201800395},
  journal      = {Biometrical Journal},
  month        = {3},
  number       = {2},
  pages        = {361-374},
  shortjournal = {Bio. J.},
  title        = {Power gains by using external information in clinical trials are typically not possible when requiring strict type i error control},
  volume       = {62},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Longitudinal analysis of pre- and post-treatment
measurements with equal baseline assumptions in randomized trials.
<em>BIMJ</em>, <em>62</em>(2), 350–360. (<a
href="https://doi.org/10.1002/bimj.201800389">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For continuous variables of randomized controlled trials, recently, longitudinal analysis of pre- and posttreatment measurements as bivariate responses is one of analytical methods to compare two treatment groups. Under random allocation, means and variances of pretreatment measurements are expected to be equal between groups, but covariances and posttreatment variances are not. Under random allocation with unequal covariances and posttreatment variances, we compared asymptotic variances of the treatment effect estimators in three longitudinal models. The data-generating model has equal baseline means and variances, and unequal covariances and posttreatment variances. The model with equal baseline means and unequal variance–covariance matrices has a redundant parameter. In large sample sizes, these two models keep a nominal type I error rate and have high efficiency. The model with equal baseline means and equal variance–covariance matrices wrongly assumes equal covariances and posttreatment variances. Only under equal sample sizes, this model keeps a nominal type I error rate. This model has the same high efficiency with the data-generating model under equal sample sizes. In conclusion, longitudinal analysis with equal baseline means performed well in large sample sizes. We also compared asymptotic properties of longitudinal models with those of the analysis of covariance (ANCOVA) and t -test.},
  archive      = {J_BIMJ},
  author       = {Ikuko Funatogawa and Takashi Funatogawa},
  doi          = {10.1002/bimj.201800389},
  journal      = {Biometrical Journal},
  month        = {3},
  number       = {2},
  pages        = {350-360},
  shortjournal = {Bio. J.},
  title        = {Longitudinal analysis of pre- and post-treatment measurements with equal baseline assumptions in randomized trials},
  volume       = {62},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). An optimal bayesian predictive probability design for phase
II clinical trials with simple and complicated endpoints. <em>BIMJ</em>,
<em>62</em>(2), 339–349. (<a
href="https://doi.org/10.1002/bimj.201900022">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most existing phase II clinical trial designs focus on conventional chemotherapy with binary tumor response as the endpoint. The advent of novel therapies, such as molecularly targeted agents and immunotherapy, has made the endpoint of phase II trials more complicated, often involving ordinal, nested, and coprimary endpoints. We propose a simple and flexible Bayesian optimal phase II predictive probability (OPP) design that handles binary and complex endpoints in a unified way. The Dirichlet-multinomial model is employed to accommodate different types of endpoints. At each interim, given the observed interim data, we calculate the Bayesian predictive probability of success, should the trial continue to the maximum planned sample size, and use it to make the go/no-go decision. The OPP design controls the type I error rate, maximizes power or minimizes the expected sample size, and is easy to implement, because the go/no-go decision boundaries can be enumerated and included in the protocol before the onset of the trial. Simulation studies show that the OPP design has satisfactory operating characteristics.},
  archive      = {J_BIMJ},
  author       = {Beibei Guo and Suyu Liu},
  doi          = {10.1002/bimj.201900022},
  journal      = {Biometrical Journal},
  month        = {3},
  number       = {2},
  pages        = {339-349},
  shortjournal = {Bio. J.},
  title        = {An optimal bayesian predictive probability design for phase II clinical trials with simple and complicated endpoints},
  volume       = {62},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A bayesian basket trial design that borrows information
across strata based on the similarity between the posterior
distributions of the response probability. <em>BIMJ</em>,
<em>62</em>(2), 330–338. (<a
href="https://doi.org/10.1002/bimj.201800404">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Basket trials simultaneously evaluate the effect of one or more drugs on a defined biomarker, genetic alteration, or molecular target in a variety of disease subtypes, often called strata. A conventional approach for analyzing such trials is an independent analysis of each of the strata. This analysis is inefficient as it lacks the power to detect the effect of drugs in each stratum. To address these issues, various designs for basket trials have been proposed, centering on designs using Bayesian hierarchical models. In this article, we propose a novel Bayesian basket trial design that incorporates predictive sample size determination, early termination for inefficacy and efficacy, and the borrowing of information across strata. The borrowing of information is based on the similarity between the posterior distributions of the response probability. In general, Bayesian hierarchical models have many distributional assumptions along with multiple parameters. By contrast, our method has prior distributions for response probability and two parameters for similarity of distributions. The proposed design is easier to implement and less computationally demanding than other Bayesian basket designs. Through a simulation with various scenarios, our proposed design is compared with other designs including one that does not borrow information and one that uses a Bayesian hierarchical model.},
  archive      = {J_BIMJ},
  author       = {Kei Fujikawa and Satoshi Teramukai and Isao Yokota and Takashi Daimon},
  doi          = {10.1002/bimj.201800404},
  journal      = {Biometrical Journal},
  month        = {3},
  number       = {2},
  pages        = {330-338},
  shortjournal = {Bio. J.},
  title        = {A bayesian basket trial design that borrows information across strata based on the similarity between the posterior distributions of the response probability},
  volume       = {62},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Adjusting simon’s optimal two-stage design for heterogeneous
populations based on stratification or using historical controls.
<em>BIMJ</em>, <em>62</em>(2), 311–329. (<a
href="https://doi.org/10.1002/bimj.201800390">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In many cancer studies, the population under consideration is highly heterogeneous in terms of clinical, demographical, and biological covariates. As the covariates substantially impact the individual prognosis, the response probabilities of patients entering the study may strongly vary. In this case, the operating characteristics of classical clinical trial designs heavily depend on the covariates of patients entering the study. Notably, both type I and type II errors can be much higher than specified. In this paper, two modifications of Simon&#39;s optimal two-stage design correcting for heterogeneous populations are derived. The first modification assumes that the patient population is divided into a finite number of subgroups, where each subgroup has a different response probability. The second approach uses a logistic regression model based on historical controls to estimate the response probabilities of patients entering the study. The performance of both approaches is demonstrated using simulation examples.},
  archive      = {J_BIMJ},
  author       = {Dominic Edelmann and Christina Habermehl and Richard F. Schlenk and Axel Benner},
  doi          = {10.1002/bimj.201800390},
  journal      = {Biometrical Journal},
  month        = {3},
  number       = {2},
  pages        = {311-329},
  shortjournal = {Bio. J.},
  title        = {Adjusting simon&#39;s optimal two-stage design for heterogeneous populations based on stratification or using historical controls},
  volume       = {62},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). SMARTp: A SMART design for nonsurgical treatments of chronic
periodontitis with spatially referenced and nonrandomly missing skewed
outcomes. <em>BIMJ</em>, <em>62</em>(2), 282–310. (<a
href="https://doi.org/10.1002/bimj.201900027">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes dynamic treatment regimes (DTRs) as effective individualized treatment strategies for managing chronic periodontitis. The proposed DTRs are studied via SMARTp —a two-stage sequential multiple assignment randomized trial (SMART) design. For this design, we propose a statistical analysis plan and a novel cluster-level sample size calculation method that factors in typical features of periodontal responses such as non-Gaussianity, spatial clustering, and nonrandom missingness. Here, each patient is viewed as a cluster, and a tooth within a patient&#39;s mouth is viewed as an individual unit inside the cluster, with the tooth-level covariance structure described by a conditionally autoregressive structure. To accommodate possible skewness and tail behavior, the tooth-level clinical attachment level (CAL) response is assumed to be skew- t , with the nonrandomly missing structure captured via a shared parameter model corresponding to the missingness indicator. The proposed method considers mean comparison for the regimes with or without sharing an initial treatment, where the expected values and corresponding variances or covariance for the sample means of a pair of DTRs are derived by the inverse probability weighting and method of moments. Simulation studies are conducted to investigate the finite-sample performance of the proposed sample size formulas under a variety of outcome-generating scenarios. An R package SMARTp implementing our sample size formula is available at the Comprehensive R Archive Network for free download.},
  archive      = {J_BIMJ},
  author       = {Jing Xu and Dipankar Bandyopadhyay and Sedigheh Mirzaei Salehabadi and Bryan Michalowicz and Bibhas Chakraborty},
  doi          = {10.1002/bimj.201900027},
  journal      = {Biometrical Journal},
  month        = {3},
  number       = {2},
  pages        = {282-310},
  shortjournal = {Bio. J.},
  title        = {SMARTp: A SMART design for nonsurgical treatments of chronic periodontitis with spatially referenced and nonrandomly missing skewed outcomes},
  volume       = {62},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Statistical strategies for the analysis of massive data
sets. <em>BIMJ</em>, <em>62</em>(2), 270–281. (<a
href="https://doi.org/10.1002/bimj.201900034">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The advent of the big data age has changed the landscape for statisticians. Public and private organizations alike these days are interested in capturing and analyzing complex customer data in order to improve their service and drive efficiency gains. However, the large volume of data involved often means that standard statistical methods fail and new ways of thinking are needed. Although great gains can be obtained through the use of more advanced computing environments or through developing sophisticated new statistical algorithms that handle data in a more efficient way, there are also many simpler things that can be done to handle large data sets in an efficient and intuitive manner. These include the use of distributed analysis methodologies, clever subsampling, data coarsening, and clever data reductions that exploit concepts such as sufficiency. These kinds of strategies represent exciting opportunities for statisticians to remain front and center in the data science world.},
  archive      = {J_BIMJ},
  author       = {Hon Hwang and Louise Ryan},
  doi          = {10.1002/bimj.201900034},
  journal      = {Biometrical Journal},
  month        = {3},
  number       = {2},
  pages        = {270-281},
  shortjournal = {Bio. J.},
  title        = {Statistical strategies for the analysis of massive data sets},
  volume       = {62},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A potpourri of biostatistical research: Special issue for
ISCB ASC 2018. <em>BIMJ</em>, <em>62</em>(2), 267–269. (<a
href="https://doi.org/10.1002/bimj.202000030">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_BIMJ},
  author       = {John Carlin and Jessica Kasza and Margarita Moreno-Betancur and Julie Simpson and Jonathan Bartlett and Chris Metcalfe and Katherine Lee},
  doi          = {10.1002/bimj.202000030},
  journal      = {Biometrical Journal},
  month        = {3},
  number       = {2},
  pages        = {267-269},
  shortjournal = {Bio. J.},
  title        = {A potpourri of biostatistical research: Special issue for ISCB ASC 2018},
  volume       = {62},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020b). Contents: Biometrical journal 2’20. <em>BIMJ</em>,
<em>62</em>(2), 265–266. (<a
href="https://doi.org/10.1002/bimj.202070024">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_BIMJ},
  doi          = {10.1002/bimj.202070024},
  journal      = {Biometrical Journal},
  month        = {3},
  number       = {2},
  pages        = {265-266},
  shortjournal = {Bio. J.},
  title        = {Contents: Biometrical journal 2&#39;20},
  volume       = {62},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020a). Cover picture: Biometrical journal 1’20. <em>BIMJ</em>,
<em>62</em>(1), NA. (<a
href="https://doi.org/10.1002/bimj.202070011">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_BIMJ},
  doi          = {10.1002/bimj.202070011},
  journal      = {Biometrical Journal},
  month        = {1},
  number       = {1},
  pages        = {NA},
  shortjournal = {Bio. J.},
  title        = {Cover picture: Biometrical journal 1&#39;20},
  volume       = {62},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Computational bayesian statistics: An introduction. M.
Antónia amaral turkman, carlos daniel paulino, and peter müller (2019).
Cambridge, UK: Cambridge university press. 243 pages, ISBN:
978-1-108-70374-1. <em>BIMJ</em>, <em>62</em>(1), 256–257. (<a
href="https://doi.org/10.1002/bimj.201900307">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_BIMJ},
  author       = {Owen Thomas and Leiv Rønneberg},
  doi          = {10.1002/bimj.201900307},
  journal      = {Biometrical Journal},
  month        = {1},
  number       = {1},
  pages        = {256-257},
  shortjournal = {Bio. J.},
  title        = {Computational bayesian statistics: an introduction. m. antónia amaral turkman, carlos daniel paulino, and peter müller (2019). cambridge, UK: cambridge university press. 243 pages, ISBN: 978-1-108-70374-1.},
  volume       = {62},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020b). <em>BIMJ</em>, <em>62</em>(1), 254–255. (<a
href="https://doi.org/10.1002/bimj.201900317">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_BIMJ},
  author       = {Li-Pang Chen},
  doi          = {10.1002/bimj.201900317},
  journal      = {Biometrical Journal},
  month        = {1},
  number       = {1},
  pages        = {254-255},
  shortjournal = {Bio. J.},
  title        = {The statistical analysis of multivariate failure time data: a marginal modeling approach. ross l. prentice and shanshan zhao (2019). new york, NY: chapman and Hall/CRC press. 240 pages. CDN $117.68 (hardback). ISBN 978-1-4822-5657-4.},
  volume       = {62},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Generalized linear models with examples in r. Peter k.
Dunnand gordon k. Smyth (2018). Berlin, germany: Springer
science+business media, pp. 562 pages, ISBN: 978-1-4419-0118-7.
<em>BIMJ</em>, <em>62</em>(1), 253. (<a
href="https://doi.org/10.1002/bimj.201900264">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_BIMJ},
  author       = {Dominic Edelmann},
  doi          = {10.1002/bimj.201900264},
  journal      = {Biometrical Journal},
  month        = {1},
  number       = {1},
  pages        = {253},
  shortjournal = {Bio. J.},
  title        = {Generalized linear models with examples in r. peter k. dunnand gordon k. smyth (2018). berlin, germany: springer Science+Business media, pp. 562 pages, ISBN: 978-1-4419-0118-7},
  volume       = {62},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Applied compositional data analysis: With worked examples in
r. Peter filzmoser, karel hron, and matthias templ (2018). Springer
series in statistics. Cham: Springer. 280 pages, ISBN:
978-3-319-96420-1. <em>BIMJ</em>, <em>62</em>(1), 250–252. (<a
href="https://doi.org/10.1002/bimj.201900263">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_BIMJ},
  author       = {Jan Graffelman and Josep Antoni Martín-Fernández},
  doi          = {10.1002/bimj.201900263},
  journal      = {Biometrical Journal},
  month        = {1},
  number       = {1},
  pages        = {250-252},
  shortjournal = {Bio. J.},
  title        = {Applied compositional data analysis: with worked examples in r. peter filzmoser, karel hron, and matthias templ (2018). springer series in statistics. cham: springer. 280 pages, ISBN: 978-3-319-96420-1},
  volume       = {62},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Berkson’s paradox and weighted distributions: An application
to alzheimer’s disease. <em>BIMJ</em>, <em>62</em>(1), 238–249. (<a
href="https://doi.org/10.1002/bimj.201900046">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {One reason for observing in practice a false positive or negative correlation between two random variables, which are either not correlated or correlated with a different direction, is the overrepresentation in the sample of individuals satisfying specific properties. In 1946, Berkson first illustrated the presence of a false correlation due to this last reason, which is known as Berkson&#39;s paradox and is one of the most famous paradox in probability and statistics. In this paper, the concept of weighted distributions is utilized to describe Berskon&#39;s paradox. Moreover, a proper procedure is suggested to make inference for the population given a biased sample which possesses all the characteristics of Berkson&#39;s paradox. A real data application for patients with dementia due to Alzheimer&#39;s disease demonstrates that the proposed method reveals characteristics of the population that are masked by the sampling procedure.},
  archive      = {J_BIMJ},
  author       = {Polychronis Economou and Apostolos Batsidis and George Tzavelas and Panagiotis Alexopoulos},
  doi          = {10.1002/bimj.201900046},
  journal      = {Biometrical Journal},
  month        = {1},
  number       = {1},
  pages        = {238-249},
  shortjournal = {Bio. J.},
  title        = {Berkson&#39;s paradox and weighted distributions: An application to alzheimer&#39;s disease},
  volume       = {62},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). SIMEX for correction of dietary exposure effects with
box-cox transformed data. <em>BIMJ</em>, <em>62</em>(1), 221–237. (<a
href="https://doi.org/10.1002/bimj.201900066">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Modelling dietary data, and especially 24-hr dietary recall (24HDR) data, is a challenge. Ignoring the inherent measurement error (ME) leads to biased effect estimates when the association between an exposure and an outcome is investigated. We propose an adapted simulation extrapolation (SIMEX) algorithm for modelling dietary exposures. For this purpose, we exploit the ME model of the NCI method where we assume the assumption of normally distributed errors of the reported intake on the Box-Cox transformed scale and of unbiased recalls on the original scale. According to the SIMEX algorithm, remeasurements of the observed data with additional ME are generated in order to estimate the association between the level of ME and the resulting effect estimate. Subsequently, this association is extrapolated to the case of zero ME to obtain the corrected estimate. We show that the proposed method fulfils the key property of the SIMEX approach, that is, that the MSE of the generated data will converge to zero if the ME variance converges to zero. Furthermore, the method is applied to real 24HDR data of the I.Family study to correct the effects of salt and alcohol intake on blood pressure. In a simulation study, the method is compared with the NCI method resulting in effect estimates with either smaller MSE or smaller bias in certain situations. In addition, we found our method to be more informative and easier to implement. Therefore, we conclude that the proposed method is useful to promote the dissemination of ME correction methods in nutritional epidemiology.},
  archive      = {J_BIMJ},
  author       = {Timm Intemann and Kirsten Mehlig and Stefaan De Henauw and Alfonso Siani and Tassos Constantinou and Luis A. Moreno and Dénes Molnár and Toomas Veidebaum and Iris Pigeot},
  doi          = {10.1002/bimj.201900066},
  journal      = {Biometrical Journal},
  month        = {1},
  number       = {1},
  pages        = {221-237},
  shortjournal = {Bio. J.},
  title        = {SIMEX for correction of dietary exposure effects with box-cox transformed data},
  volume       = {62},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Parametric modal regression with varying precision.
<em>BIMJ</em>, <em>62</em>(1), 202–220. (<a
href="https://doi.org/10.1002/bimj.201900132">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose a simple parametric modal linear regression model where the response variable is gamma distributed using a new parameterization of this distribution that is indexed by mode and precision parameters, that is, in this new regression model, the modal and precision responses are related to a linear predictor through a link function and the linear predictor involves covariates and unknown regression parameters. The main advantage of our new parameterization is the straightforward interpretation of the regression coefficients in terms of the mode of the positive response variable, as is usual in the context of generalized linear models, and direct inference in parametric mode regression based on the likelihood paradigm. Furthermore, we discuss residuals and influence diagnostic tools. A Monte Carlo experiment is conducted to evaluate the performances of these estimators in finite samples with a discussion of the results. Finally, we illustrate the usefulness of the new model by two applications, to biology and demography.},
  archive      = {J_BIMJ},
  author       = {Marcelo Bourguignon and Jeremias Leão and Diego I. Gallardo},
  doi          = {10.1002/bimj.201900132},
  journal      = {Biometrical Journal},
  month        = {1},
  number       = {1},
  pages        = {202-220},
  shortjournal = {Bio. J.},
  title        = {Parametric modal regression with varying precision},
  volume       = {62},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A two-phase bayesian methodology for the analysis of binary
phenotypes in genome-wide association studies. <em>BIMJ</em>,
<em>62</em>(1), 191–201. (<a
href="https://doi.org/10.1002/bimj.201900050">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent advances in sequencing and genotyping technologies are contributing to a data revolution in genome-wide association studies that is characterized by the challenging large p small n problem in statistics. That is, given these advances, many such studies now consider evaluating an extremely large number of genetic markers ( p ) genotyped on a small number of subjects ( n ). Given the dimension of the data, a joint analysis of the markers is often fraught with many challenges, while a marginal analysis is not sufficient. To overcome these obstacles, herein, we propose a Bayesian two-phase methodology that can be used to jointly relate genetic markers to binary traits while controlling for confounding. The first phase of our approach makes use of a marginal scan to identify a reduced set of candidate markers that are then evaluated jointly via a hierarchical model in the second phase. Final marker selection is accomplished through identifying a sparse estimator via a novel and computationally efficient maximum a posteriori estimation technique. We evaluate the performance of the proposed approach through extensive numerical studies, and consider a genome-wide application involving colorectal cancer.},
  archive      = {J_BIMJ},
  author       = {Chase Joyner and Christopher McMahan and James Baurley and Bens Pardamean},
  doi          = {10.1002/bimj.201900050},
  journal      = {Biometrical Journal},
  month        = {1},
  number       = {1},
  pages        = {191-201},
  shortjournal = {Bio. J.},
  title        = {A two-phase bayesian methodology for the analysis of binary phenotypes in genome-wide association studies},
  volume       = {62},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A comparison of the beta-geometric model with landmarking
for dynamic prediction of time to pregnancy. <em>BIMJ</em>,
<em>62</em>(1), 175–190. (<a
href="https://doi.org/10.1002/bimj.201900155">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We conducted a simulation study to compare two methods that have been recently used in clinical literature for the dynamic prediction of time to pregnancy. The first is landmarking, a semi-parametric method where predictions are updated as time progresses using the patient subset still at risk at that time point. The second is the beta-geometric model that updates predictions over time from a parametric model estimated on all data and is specific to applications with a discrete time to event outcome. The beta-geometric model introduces unobserved heterogeneity by modelling the chance of an event per discrete time unit according to a beta distribution. Due to selection of patients with lower chances as time progresses, the predicted probability of an event decreases over time. Both methods were recently used to develop models predicting the chance to conceive naturally. The advantages, disadvantages and accuracy of these two methods are unknown. We simulated time-to-pregnancy data according to different scenarios. We then compared the two methods by the following out-of-sample metrics: bias and root mean squared error in the average prediction, root mean squared error in individual predictions, Brier score and c statistic. We consider different scenarios including data-generating mechanisms for which the models are misspecified. We applied the two methods on a clinical dataset comprising 4999 couples. Finally, we discuss the pros and cons of the two methods based on our results and present recommendations for use of either of the methods in different settings and (effective) sample sizes.},
  archive      = {J_BIMJ},
  author       = {Rik van Eekelen and Hein Putter and David J. McLernon and Marinus J. Eijkemans and Nan van Geloven},
  doi          = {10.1002/bimj.201900155},
  journal      = {Biometrical Journal},
  month        = {1},
  number       = {1},
  pages        = {175-190},
  shortjournal = {Bio. J.},
  title        = {A comparison of the beta-geometric model with landmarking for dynamic prediction of time to pregnancy},
  volume       = {62},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). An approach to model clustered survival data with dependent
censoring. <em>BIMJ</em>, <em>62</em>(1), 157–174. (<a
href="https://doi.org/10.1002/bimj.201800391">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this study we introduce a likelihood-based method, via the Weibull and piecewise exponential distributions, capable of accommodating the dependence between failure and censoring times. The methodology is developed for the analysis of clustered survival data and it assumes that failure and censoring times are mutually independent conditional on a latent frailty. The dependent censoring mechanism is accounted through the frailty effect and this is accomplished by means of a key parameter accommodating the correlation between failure and censored observations. The full specification of the likelihood in our work simplifies the inference procedures with respect to Huang and Wolfe since it reduces the computation burden of working with the profile likelihood. In addition, the assumptions made for the baseline distributions lead to models with continuous survival functions. In order to carry out inferences, we devise a Monte Carlo EM algorithm. The performance of the proposed models is investigated through a simulation study. Finally, we explore a real application involving patients from the Dialysis Outcomes and Practice Patterns Study observed between 1996 and 2015.},
  archive      = {J_BIMJ},
  author       = {Silvana Schneider and Fábio Nogueira Demarqui and Enrico Antônio Colosimo and Vinícius Diniz Mayrink},
  doi          = {10.1002/bimj.201800391},
  journal      = {Biometrical Journal},
  month        = {1},
  number       = {1},
  pages        = {157-174},
  shortjournal = {Bio. J.},
  title        = {An approach to model clustered survival data with dependent censoring},
  volume       = {62},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Flexible parametric model for survival data subject to
dependent censoring. <em>BIMJ</em>, <em>62</em>(1), 136–156. (<a
href="https://doi.org/10.1002/bimj.201800375">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {When modeling survival data, it is common to assume that the (log-transformed) survival time ( T ) is conditionally independent of the (log-transformed) censoring time ( C ) given a set of covariates. There are numerous situations in which this assumption is not realistic, and a number of correction procedures have been developed for different models. However, in most cases, either some prior knowledge about the association between T and C is required, or some auxiliary information or data is/are supposed to be available. When this is not the case, the application of many existing methods turns out to be limited. The goal of this paper is to overcome this problem by developing a flexible parametric model, that is a type of transformed linear model. We show that the association between T and C is identifiable in this model. The performance of the proposed method is investigated both in an asymptotic way and through finite sample simulations. We also develop a formal goodness-of-fit test approach to assess the quality of the fitted model. Finally, the approach is applied to data coming from a study on liver transplants.},
  archive      = {J_BIMJ},
  author       = {Negera Wakgari Deresa and Ingrid Van Keilegom},
  doi          = {10.1002/bimj.201800375},
  journal      = {Biometrical Journal},
  month        = {1},
  number       = {1},
  pages        = {136-156},
  shortjournal = {Bio. J.},
  title        = {Flexible parametric model for survival data subject to dependent censoring},
  volume       = {62},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Beyond the proportional frailty model: Bayesian estimation
of individual heterogeneity on mortality parameters. <em>BIMJ</em>,
<em>62</em>(1), 124–135. (<a
href="https://doi.org/10.1002/bimj.201800280">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Today, we know that demographic rates can be greatly influenced by differences among individuals in their capacity to survive and reproduce. These intrinsic differences, commonly known as individual heterogeneity, can rarely be measured and are thus treated as latent variables when modeling mortality. Finite mixture models and mixed effects models have been proposed as alternative approaches for inference on individual heterogeneity in mortality. However, in general models assume that individual heterogeneity influences mortality proportionally, which limits the possibility to test hypotheses on the effect of individual heterogeneity on other aspects of mortality such as ageing rates. Here, we propose a Bayesian model that builds upon the mixture models previously developed, but that facilitates making inferences on the effect of individual heterogeneity on mortality parameters other than the baseline mortality. As an illustration, we apply this framework to the Gompertz–Makeham mortality model, commonly used in human and wildlife studies, by assuming that the Gompertz rate parameter is affected by individual heterogeneity. We provide results of a simulation study where we show that the model appropriately retrieves the parameters used for simulation, even for low variances in the heterogeneous parameter. We then apply the model to a dataset on captive chimpanzees and on a cohort life table of 1751 Swedish men, and show how model selection against a null model (i.e., without heterogeneity) can be carried out.},
  archive      = {J_BIMJ},
  author       = {Fernando Colchero and Burhan Y. Kiyakoglu},
  doi          = {10.1002/bimj.201800280},
  journal      = {Biometrical Journal},
  month        = {1},
  number       = {1},
  pages        = {124-135},
  shortjournal = {Bio. J.},
  title        = {Beyond the proportional frailty model: Bayesian estimation of individual heterogeneity on mortality parameters},
  volume       = {62},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Distribution-free simultaneous tests for location–scale and
lehmann alternative in two-sample problem. <em>BIMJ</em>,
<em>62</em>(1), 99–123. (<a
href="https://doi.org/10.1002/bimj.201900057">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The paper deals with the classical two-sample testing problem for the equality of two populations, one of the most fundamental problems in biomedical experiments and case–control studies. The most familiar alternatives are the difference in location parameters or the difference in scale parameters or in both the parameters of the population density. All the tests designed for classical location or scale or location–scale alternatives assume that there is no change in the shape of the distribution. Some authors also consider the Lehmann-type alternative that addresses the change in shape. Two-sample tests under Lehmann alternative assume that the location and scale parameters are invariant. In real life, when a shift in the distribution occurs, one or more of the location, scale, and shape parameters may change simultaneously. We refer to change of one or more of the three parameters as a versatile alternative. Noting the dearth of literature for the equality two populations against such versatile alternative, we introduce two distribution-free tests based on the Euclidean and Mahalanobis distance. We obtain the asymptotic distributions of the two test statistics and study asymptotic power. We also discuss approximating p -values of the proposed tests in real applications with small samples. We compare the power performance of the two tests with several popular existing distribution-free tests against various fixed alternatives using Monte Carlo. We provide two illustrations based on biomedical experiments. Unlike existing tests which are suitable only in certain situations, proposed tests offer very good power in almost all types of shifts.},
  archive      = {J_BIMJ},
  author       = {Wolfgang Kössler and Amitava Mukherjee},
  doi          = {10.1002/bimj.201900057},
  journal      = {Biometrical Journal},
  month        = {1},
  number       = {1},
  pages        = {99-123},
  shortjournal = {Bio. J.},
  title        = {Distribution-free simultaneous tests for location–scale and lehmann alternative in two-sample problem},
  volume       = {62},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Meta-analysis of the difference of medians. <em>BIMJ</em>,
<em>62</em>(1), 69–98. (<a
href="https://doi.org/10.1002/bimj.201900036">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider the problem of meta-analyzing two-group studies that report the median of the outcome. Often, these studies are excluded from meta-analysis because there are no well-established statistical methods to pool the difference of medians. To include these studies in meta-analysis, several authors have recently proposed methods to estimate the sample mean and standard deviation from the median, sample size, and several commonly reported measures of spread. Researchers frequently apply these methods to estimate the difference of means and its variance for each primary study and pool the difference of means using inverse variance weighting. In this work, we develop several methods to directly meta-analyze the difference of medians. We conduct a simulation study evaluating the performance of the proposed median-based methods and the competing transformation-based methods. The simulation results show that the median-based methods outperform the transformation-based methods when meta-analyzing studies that report the median of the outcome, especially when the outcome is skewed. Moreover, we illustrate the various methods on a real-life data set.},
  archive      = {J_BIMJ},
  author       = {Sean McGrath and Hojoon Sohn and Russell Steele and Andrea Benedetti},
  doi          = {10.1002/bimj.201900036},
  journal      = {Biometrical Journal},
  month        = {1},
  number       = {1},
  pages        = {69-98},
  shortjournal = {Bio. J.},
  title        = {Meta-analysis of the difference of medians},
  volume       = {62},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A multiple comparison procedure for dose-finding trials with
subpopulations. <em>BIMJ</em>, <em>62</em>(1), 53–68. (<a
href="https://doi.org/10.1002/bimj.201800111">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Identifying subgroups of patients with an enhanced response to a new treatment has become an area of increased interest in the last few years. When there is knowledge about possible subpopulations with an enhanced treatment effect before the start of a trial it might be beneficial to set up a testing strategy, which tests for a significant treatment effect not only in the full population, but also in these prespecified subpopulations. In this paper, we present a parametric multiple testing approach for tests in multiple populations for dose-finding trials. Our approach is based on the MCP-Mod methodology, which uses multiple comparison procedures (MCPs) to test for a dose–response signal, while considering multiple possible candidate dose–response shapes. Our proposed methods allow for heteroscedastic error variances between populations and control the family-wise error rate over tests in multiple populations and for multiple candidate models. We show in simulations that the proposed multipopulation testing approaches can increase the power to detect a significant dose–response signal over the standard single-population MCP-Mod, when the specified subpopulation has an enhanced treatment effect.},
  archive      = {J_BIMJ},
  author       = {Marius Thomas and Björn Bornkamp and Martin Posch and Franz König},
  doi          = {10.1002/bimj.201800111},
  journal      = {Biometrical Journal},
  month        = {1},
  number       = {1},
  pages        = {53-68},
  shortjournal = {Bio. J.},
  title        = {A multiple comparison procedure for dose-finding trials with subpopulations},
  volume       = {62},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Latent variable models for harmonization of test scores: A
case study on memory. <em>BIMJ</em>, <em>62</em>(1), 34–52. (<a
href="https://doi.org/10.1002/bimj.201800146">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Combining data from different studies has a long tradition within the scientific community. It requires that the same information is collected from each study to be able to pool individual data. When studies have implemented different methods or used different instruments (e.g., questionnaires) for measuring the same characteristics or constructs, the observed variables need to be harmonized in some way to obtain equivalent content information across studies. This paper formulates the main concepts for harmonizing test scores from different observational studies in terms of latent variable models. The concepts are formulated in terms of calibration, invariance, and exchangeability. Although similar ideas are present in measurement reliability and test equating, harmonization is different from measurement invariance and generalizes test equating. In addition, if a test score needs to be transformed to another test score, harmonization of variables is only possible under specific conditions. Observed test scores that connect all of the different studies, are necessary to be able to test the underlying assumptions of harmonization. The concepts of harmonization are illustrated on multiple memory test scores from three different Canadian studies.},
  archive      = {J_BIMJ},
  author       = {Edwin R. van den Heuvel and Lauren E. Griffith and Nazmul Sohel and Isabel Fortier and Graciela Muniz-Terrera and Parminder Raina},
  doi          = {10.1002/bimj.201800146},
  journal      = {Biometrical Journal},
  month        = {1},
  number       = {1},
  pages        = {34-52},
  shortjournal = {Bio. J.},
  title        = {Latent variable models for harmonization of test scores: A case study on memory},
  volume       = {62},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Joint model for recurrent event data with a cured fraction
and a terminal event. <em>BIMJ</em>, <em>62</em>(1), 24–33. (<a
href="https://doi.org/10.1002/bimj.201800321">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In a longitudinal study where the recurrence of an event and a terminal event such as death are observed, a certain portion of the subjects may experience no event during a long follow-up period; this often denoted as the cure group which is assumed to be the risk-free from both recurrent events and death. However, this assumption ignores the possibility of death, which subjects in the cure group may experience. In the present study, such misspecification is investigated with the addition of a death hazard model to the cure group. We propose a joint model using a frailty effect, which reflects the association between a recurrent event and death. For the estimation, an expectation-maximization (EM) algorithm was developed and PROC NLMIXED in SAS was incorporated under a piecewise constant baseline. Simulation studies were performed to check the performance of the suggested method. The proposed method was applied to leukemia patients experiencing both infection and death after bone marrow transplant.},
  archive      = {J_BIMJ},
  author       = {Yang-Jin Kim},
  doi          = {10.1002/bimj.201800321},
  journal      = {Biometrical Journal},
  month        = {1},
  number       = {1},
  pages        = {24-33},
  shortjournal = {Bio. J.},
  title        = {Joint model for recurrent event data with a cured fraction and a terminal event},
  volume       = {62},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Joint mean–covariance random effect model for longitudinal
data. <em>BIMJ</em>, <em>62</em>(1), 7–23. (<a
href="https://doi.org/10.1002/bimj.201800311">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we consider the inherent association between mean and covariance in the joint mean–covariance modeling and propose a joint mean–covariance random effect model based on the modified Cholesky decomposition for longitudinal data. Meanwhile, we apply M-H algorithm to simulate the posterior distributions of model parameters. Besides, a computationally efficient Monte Carlo expectation maximization (MCEM) algorithm is developed for carrying out maximum likelihood estimation. Simulation studies show that the model taking into account the inherent association between mean and covariance has smaller standard deviations of the estimators of parameters, which makes the statistical inferences much more reliable. In the real data analysis, the estimation of parameters in the mean and covariance structure is highly efficient.},
  archive      = {J_BIMJ},
  author       = {Yongxin Bai and Manling Qian and Maozai Tian},
  doi          = {10.1002/bimj.201800311},
  journal      = {Biometrical Journal},
  month        = {1},
  number       = {1},
  pages        = {7-23},
  shortjournal = {Bio. J.},
  title        = {Joint mean–covariance random effect model for longitudinal data},
  volume       = {62},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020a). Contents: Biometrical journal 1’20. <em>BIMJ</em>,
<em>62</em>(1), 5–6. (<a
href="https://doi.org/10.1002/bimj.202070014">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_BIMJ},
  doi          = {10.1002/bimj.202070014},
  journal      = {Biometrical Journal},
  month        = {1},
  number       = {1},
  pages        = {5-6},
  shortjournal = {Bio. J.},
  title        = {Contents: Biometrical journal 1&#39;20},
  volume       = {62},
  year         = {2020},
}
</textarea>
</details></li>
</ul>

</body>
</html>
