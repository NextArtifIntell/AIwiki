<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>IETCV_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="ietcv---70">IETCV - 70</h2>
<ul>
<li><details>
<summary>
(2020). Human-like evaluation method for object motion detection
algorithms. <em>IETCV</em>, <em>14</em>(8), 674–682. (<a
href="https://doi.org/10.1049/iet-cvi.2019.0997">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study proposes a new method to evaluate the performance of algorithms for moving object detection (MODA) in video sequences. The proposed method is based on human performance metric intervals, instead of ideal metric values (0 or 1) which are commonly used in the literature. These intervals are proposed to establish a more reliable evaluation and comparison, and to identify areas of improvement in the evaluation of MODA. The contribution of the study includes the determination of human segmentation performance metric intervals and their comparison with state-of-the-art MODA, and the evaluation of their segmentation results in a tracking task to establish the impact between performance and practical utility. Results show that human participants had difficulty with achieving a perfect segmentation score. Deep learning algorithms achieved performance above the human average, while other techniques achieved a performance between 88 and 92%. Furthermore, the authors demonstrate that algorithms not ranked at the top of the quantitative metrics worked satisfactorily in a tracking experiment; and therefore, should not be discarded for real applications.},
  archive      = {J_IETCV},
  author       = {Abimael Guzman-Pando and Mario Ignacio Chacon-Murguia and Lucia B. Chacon-Diaz},
  doi          = {10.1049/iet-cvi.2019.0997},
  journal      = {IET Computer Vision},
  month        = {12},
  number       = {8},
  pages        = {674-682},
  shortjournal = {IET Comput. Vis.},
  title        = {Human-like evaluation method for object motion detection algorithms},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Moving shadow detection via binocular vision and colour
clustering. <em>IETCV</em>, <em>14</em>(8), 665–673. (<a
href="https://doi.org/10.1049/iet-cvi.2019.0175">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A pedestrian segmentation algorithm in the presence of cast shadows is presented in this study. The novelty of this algorithm lies in the fusion of multi-view and multi-plane homographic projections of foregrounds and the use of the fused data to guide colour clustering. This brings about an advantage over the existing binocular algorithms in that it can remove cast shadows while keeping pedestrians’ body parts, which occlude shadows. Phantom detection, which is inherent with the binocular method, is also investigated. Experimental results with real-world videos have demonstrated the efficiency of this algorithm.},
  archive      = {J_IETCV},
  author       = {Lei Lu and Ming Xu and Jeremy S. Smith and Yuyao Yan},
  doi          = {10.1049/iet-cvi.2019.0175},
  journal      = {IET Computer Vision},
  month        = {12},
  number       = {8},
  pages        = {665-673},
  shortjournal = {IET Comput. Vis.},
  title        = {Moving shadow detection via binocular vision and colour clustering},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Diversified fisher kernel: Encoding discrimination in fisher
features to compete deep neural models for visual classification task.
<em>IETCV</em>, <em>14</em>(8), 658–664. (<a
href="https://doi.org/10.1049/iet-cvi.2019.0208">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fisher kernels derived from stochastic probabilistic models such as restricted and deep Boltzmann machines have shown competitive visual classification results in comparison to widely popular deep discriminative models. This genre of Fisher kernels bridges the gap between shallow and deep learning paradigm by inducing the characteristics of deep architecture into Fisher kernel, further deployed for classification in discriminative classifiers. Despite their success, the memory and computational costs of Fisher vectors do not make them amenable for large-scale visual retrieval and classification tasks. This study introduces a novel feature selection technique inspired from the functional characteristics of neural architectures for learning discriminative feature representations to boost the performance of Fisher kernels against deep discriminative models. The proposed technique condenses the large dimensional Fisher features for kernel learning and shows improvement in its classification performance and storage cost on leading benchmark data sets. A comparison of the proposed method with other state-of-the-art feature selection techniques is made to demonstrate its performance supremacy as well as time complexity required to learn in reduced Fisher space.},
  archive      = {J_IETCV},
  author       = {Sarah Ahmed and Tayyaba Azim},
  doi          = {10.1049/iet-cvi.2019.0208},
  journal      = {IET Computer Vision},
  month        = {12},
  number       = {8},
  pages        = {658-664},
  shortjournal = {IET Comput. Vis.},
  title        = {Diversified fisher kernel: Encoding discrimination in fisher features to compete deep neural models for visual classification task},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Creative and diverse artwork generation using adversarial
networks. <em>IETCV</em>, <em>14</em>(8), 650–657. (<a
href="https://doi.org/10.1049/iet-cvi.2020.0014">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing style transfer methods have achieved great success in artwork generation by transferring artistic styles onto everyday photographs while keeping their contents unchanged. Despite this success, these methods have one inherent limitation: they cannot produce newly created image contents, lacking creativity and flexibility. On the other hand, generative adversarial networks (GANs) can synthesise images with new content, whereas cannot specify the artistic style of these images. The authors consider combining style transfer with convolutional GANs to generate more creative and diverse artworks. Instead of simply concatenating these two networks: the first for synthesising new content and the second for transferring artistic styles, which is inefficient and inconvenient, they design an end-to-end network called ArtistGAN to perform these two operations at the same time and achieve visually better results. Moreover, to generate images of higher quality, they propose the bi-discriminator GAN containing a pixel discriminator and a feature discriminator that constrain the generated image from pixel level and feature level, respectively. They conduct extensive experiments and comparisons to evaluate their methods quantitatively and qualitatively. The experimental results verify the effectiveness of their methods.},
  archive      = {J_IETCV},
  author       = {Haibo Chen and Lei Zhao and Lihong Qiu and Zhizhong Wang and Huiming Zhang and Wei Xing and Dongming Lu},
  doi          = {10.1049/iet-cvi.2020.0014},
  journal      = {IET Computer Vision},
  month        = {12},
  number       = {8},
  pages        = {650-657},
  shortjournal = {IET Comput. Vis.},
  title        = {Creative and diverse artwork generation using adversarial networks},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Domain-invariant adversarial learning with conditional
distribution alignment for unsupervised domain adaptation.
<em>IETCV</em>, <em>14</em>(8), 642–649. (<a
href="https://doi.org/10.1049/iet-cvi.2019.0514">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unsupervised domain adaption aims to reduce the divergence between the source domain and the target domain. The final objective is to learn domain-invariant features from both domains that get the minimised expected error on the target domain. The divergence between domains which is also called domain shift is mainly between the distributions of domains&#39; samples. Additionally, the label shift is also a tricky challenge in domain adaptation. In this study, domain-invariant adversarial learning with conditional distribution alignment is proposed to alleviate the effect of domain shift with label shift. To obtain the domain-invariant features, the proposed method modifies adversarial auto-encoder architecture and performs semi-supervised learning to enlarge the inter-class discrepancy. The marginal distribution is aligned in the adversarial learning process of extracting domain-invariant features. Meanwhile, the label information is incorporated in this way to align the conditional distribution. The proposed work also theoretically analyses the generalisation bound of the proposed model. Finally, the proposed method is evaluated based on several domain adaptation tasks, including digit classification and object recognition, and achieves state-of-the-art performance.},
  archive      = {J_IETCV},
  author       = {Xingmei Wang and Boxuan Sun and Hongbin Dong},
  doi          = {10.1049/iet-cvi.2019.0514},
  journal      = {IET Computer Vision},
  month        = {12},
  number       = {8},
  pages        = {642-649},
  shortjournal = {IET Comput. Vis.},
  title        = {Domain-invariant adversarial learning with conditional distribution alignment for unsupervised domain adaptation},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Combination of temporal-channels correlation information and
bilinear feature for action recognition. <em>IETCV</em>, <em>14</em>(8),
634–641. (<a href="https://doi.org/10.1049/iet-cvi.2020.0023">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this study, the authors focus on improving the spatio–temporal representation ability of three-dimensional (3D) convolutional neural networks (CNNs) in the video domain. They observe two unfavourable issues: (i) the convolutional filters only dedicate to learning local representation along input channels. Also they treat channel-wise features equally, without emphasising the important features; (ii) traditional global average pooling layer only captures first-order statistics, ignoring finer detail features useful for classification. To mitigate these problems, they proposed two modules to boost 3D CNNs’ performance, which are temporal-channel correlation (TCC) and bilinear pooling module. The TCC module can capture the information of inter-channel correlations over the temporal domain. Moreover, the TCC module generates channel-wise dependencies, which can adaptively re-weight the channel-wise features. Therefore, the network can focus on learning important features. With regards to the bilinear pooling module, it can capture more complex second-order statistics in deep features and generate a second-order classification vector. We can get more accurate classification results by combining the first-order and second-order classification vector. Extensive experiments show that adding our proposed modules to I3D network could consistently improve the performance and outperform the state-of-the-art methods. The code and models are available at https://github.com/caijh33/I3D_TCC_Bilinear .},
  archive      = {J_IETCV},
  author       = {Jiahui Cai and Jianguo Hu and Shiren Li and Jialing Lin and Jun Wang},
  doi          = {10.1049/iet-cvi.2020.0023},
  journal      = {IET Computer Vision},
  month        = {12},
  number       = {8},
  pages        = {634-641},
  shortjournal = {IET Comput. Vis.},
  title        = {Combination of temporal-channels correlation information and bilinear feature for action recognition},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Referring expression comprehension model with matching
detection and linguistic feedback. <em>IETCV</em>, <em>14</em>(8),
625–633. (<a href="https://doi.org/10.1049/iet-cvi.2019.0483">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The task of referring expression comprehension (REC) is to localise an image region of a specific object described by a natural language expression, and all existing REC methods assume that the object described by the referring expression must be located in the given image. However, this assumption is not correct in some real applications. For example, a visually impaired user might tell his robot ‘please take the laptop on the table to me’. In fact, the laptop is not on the table anymore. To address this problem, the authors propose a novel REC model to deal with the situation where expression-image mismatching occurs and explain the mismatching by linguistic feedback. The authors&#39; REC model consists of four modules: the expression parsing module, the entity detection module, the relationship detection module, and the matching detection module. They built a data set called NP-RefCOCO+ from RefCOCO+ including both positive samples and negative samples. The positive samples are original expression-image pairs in RefCOCO+. The negative samples are the expression-image pairs in RefCOCO+, whose expressions are replaced. They evaluate the model on NP-RefCOCO+ and the experimental results show the advantages of their method for dealing with the problem of expression-image mismatching.},
  archive      = {J_IETCV},
  author       = {Jianming Wang and Enjie Cui and Kunliang Liu and Yukuan Sun and Jiayu Liang and Chunmiao Yuan and Xiaojie Duan and Guanghao Jin and Tae-Sun Chung},
  doi          = {10.1049/iet-cvi.2019.0483},
  journal      = {IET Computer Vision},
  month        = {12},
  number       = {8},
  pages        = {625-633},
  shortjournal = {IET Comput. Vis.},
  title        = {Referring expression comprehension model with matching detection and linguistic feedback},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Converting video classification problem to image
classification with global descriptors and pre-trained network.
<em>IETCV</em>, <em>14</em>(8), 614–624. (<a
href="https://doi.org/10.1049/iet-cvi.2019.0625">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Motion history image (MHI) is a spatio-temporal template that temporal motion information is collapsed into a single image where intensity is a function of recency of motion. Also, it consists of spatial information. Energy image (EI) based on the magnitude of optical flow is a temporal template that shows only temporal information of motion. Each video can be described in these templates. So, four new methods are introduced in this study. The first three methods are called basic methods. In method 1, each video splits into N groups of consecutive frames and MHI is calculated for each group. Transfer learning with fine-tuning technique has been used for classifying these templates. EIs are used for classifying in method 2 similar to method 1. Fusing two streams of these templates is introduced as method 3. Finally, spatial information is added in method 4. Among these methods, method 4 outperforms others and it is called the proposed method. It achieves the recognition accuracy of 92.30 and 94.50% for UCF Sport and UCF-11 action data sets, respectively. Also, the proposed method is compared with the state-of-the-art approaches and the results show that it has the best performance.},
  archive      = {J_IETCV},
  author       = {Saeedeh Zebhi and SMT Al-Modarresi and Vahid Abootalebi},
  doi          = {10.1049/iet-cvi.2019.0625},
  journal      = {IET Computer Vision},
  month        = {12},
  number       = {8},
  pages        = {614-624},
  shortjournal = {IET Comput. Vis.},
  title        = {Converting video classification problem to image classification with global descriptors and pre-trained network},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Robust locality preserving projections using angle-based
adaptive weight method. <em>IETCV</em>, <em>14</em>(8), 605–613. (<a
href="https://doi.org/10.1049/iet-cvi.2019.0403">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Locality preserving projections (LPP) method is a classical manifold learning method for dimensionality reduction. However, LPP is sensitive to outliers since squared L2-norm may exaggerate the distance of outliers. Besides, the normalisation constraint of LPP may impair its robustness during embedding. Motivated by this observation, the authors propose a novel robust LPP using angle-based adaptive weight (RLPP-AAW) method. RLPP-AAW not only considers the distance metric of training samples, but also take the reconstruction error into account, so as to reduce the influence of outliers and noise in the embedding process. In the RLPP-AAW, based on the angle between distance metric and reconstruction error, a novel way is used to combine them in the objective function. Besides, RLPP-AAW employs the L21-norm criterion, which retains rotational invariance and is more robust than squared L2-norm. An iterative algorithm is presented to solve the objective function of RLPP-AAW. Experimental results on the benchmark databases illustrate the effectiveness of the proposed algorithm.},
  archive      = {J_IETCV},
  author       = {Yunlong Gao and Shuxin Zhong and Kangli Hu and Jinyan Pan},
  doi          = {10.1049/iet-cvi.2019.0403},
  journal      = {IET Computer Vision},
  month        = {12},
  number       = {8},
  pages        = {605-613},
  shortjournal = {IET Comput. Vis.},
  title        = {Robust locality preserving projections using angle-based adaptive weight method},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Detecting dense text in natural images. <em>IETCV</em>,
<em>14</em>(8), 597–604. (<a
href="https://doi.org/10.1049/iet-cvi.2019.0916">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most existing text detection methods are mainly motivated by deep learning-based object detection approaches, which may result in serious overlapping between detected text lines, especially in dense text scenarios. It is because text boxes are not commonly overlapped, as different from general objects in natural scenes. Moreover, text detection requires higher localisation accuracy than object detection. To tackle these problems, the authors propose a novel dense text detection network (DTDN) to localise tighter text lines without overlapping. Their main novelties are: (i) propose an intersection-over-union overlap loss, which considers correlations between one anchor and GT boxes and measures how many text areas one anchor contains, (ii) propose a novel anchor sample selection strategy, named CMax-OMin, to select tighter positive samples for training. CMax-OMin strategy not only considers whether an anchor has the largest overlap with its corresponding GT box (CMax), but also ensures the overlapping between one anchor and other GT boxes as little as possible (OMin). Besides, they train a bounding-box regressor as post-processing to further improve text localisation performance. Experiments on scene text benchmark datasets and their proposed dense text dataset demonstrate that the proposed DTDN achieves competitive performance, especially for dense text scenarios.},
  archive      = {J_IETCV},
  author       = {Dianzhuan Jiang and Shengsheng Zhang and Yaping Huang and Qi Zou and Xingyuan Zhang and Mengyang Pu and Junbo Liu},
  doi          = {10.1049/iet-cvi.2019.0916},
  journal      = {IET Computer Vision},
  month        = {12},
  number       = {8},
  pages        = {597-604},
  shortjournal = {IET Comput. Vis.},
  title        = {Detecting dense text in natural images},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Multi-mode neural network for human action recognition.
<em>IETCV</em>, <em>14</em>(8), 587–596. (<a
href="https://doi.org/10.1049/iet-cvi.2019.0761">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Video data are of two different intrinsic modes, in-frame and temporal. It is beneficial to incorporate static in-frame features to acquire dynamic features for video applications. However, some existing methods such as recurrent neural networks do not have a good performance, and some other such as 3D convolutional neural networks (CNNs) are both memory consuming and time consuming. This study proposes an effective framework that takes the advantage of deep learning on the static image feature extraction to tackle the video data. After extracting in-frame feature vectors using a pretrained deep network, the authors integrate them and form a multi-mode feature matrix, which preserves the multi-mode structure and high-level representation. They propose two models for follow-up classification. The authors first introduce a temporal CNN, which directly feeds the multi-mode feature matrix into a CNN. However, they show that characteristics of the multi-mode features differ significantly in distinct modes. The authors therefore further propose the multi-mode neural network (MMNN), in which different modes deploy different types of layers. They evaluate their algorithm with the task of human action recognition. The experimental results show that the MMNN achieves a much better performance than the existing long short-term memory-based methods and consumes far fewer resources than the existing 3D end-to-end models.},
  archive      = {J_IETCV},
  author       = {Haohua Zhao and Weichen Xue and Xiaobo Li and Zhangxuan Gu and Li Niu and Liqing Zhang},
  doi          = {10.1049/iet-cvi.2019.0761},
  journal      = {IET Computer Vision},
  month        = {12},
  number       = {8},
  pages        = {587-596},
  shortjournal = {IET Comput. Vis.},
  title        = {Multi-mode neural network for human action recognition},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). GLStyleNet: Exquisite style transfer combining global and
local pyramid features. <em>IETCV</em>, <em>14</em>(8), 575–586. (<a
href="https://doi.org/10.1049/iet-cvi.2019.0844">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent studies using deep neural networks have shown remarkable success in style transfer, especially for artistic and photo-realistic images. However, these methods cannot solve more sophisticated problems. The approaches using global statistics fail to capture small, intricate textures and maintain correct texture scales of the artworks, and the others based on local patches are defective on global effect. To address these issues, this study presents a unified model [global and local style network (GLStyleNet)] to achieve exquisite style transfer with higher quality. Specifically, a simple yet effective perceptual loss is proposed to consider the information of global semantic-level structure, local patch-level style, and global channel-level effect at the same time. This could help transfer not just large-scale, obvious style cues but also subtle, exquisite ones, and dramatically improve the quality of style transfer. Besides, the authors introduce a novel deep pyramid feature fusion module to provide a more flexible style expression and a more efficient transfer process. This could help retain both high-frequency pixel information and low-frequency construct information. They demonstrate the effectiveness and superiority of their approach on numerous style transfer tasks, especially the Chinese ancient painting style transfer. Experimental results indicate that their unified approach improves image style transfer quality over previous state-of-the-art methods.},
  archive      = {J_IETCV},
  author       = {Zhizhong Wang and Lei Zhao and Sihuan Lin and Qihang Mo and Huiming Zhang and Wei Xing and Dongming Lu},
  doi          = {10.1049/iet-cvi.2019.0844},
  journal      = {IET Computer Vision},
  month        = {12},
  number       = {8},
  pages        = {575-586},
  shortjournal = {IET Comput. Vis.},
  title        = {GLStyleNet: Exquisite style transfer combining global and local pyramid features},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Partial disentanglement of hierarchical variational
auto-encoder for texture synthesis. <em>IETCV</em>, <em>14</em>(8),
564–574. (<a href="https://doi.org/10.1049/iet-cvi.2019.0416">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multiple research studies have recently demonstrated deep networks can generate realistic-looking textures and stylised images from a single texture example. However, they suffer from some drawbacks. Generative adversarial networks are in general difficult to train. Multiple feature variations, encoded in their latent representation, require a priori information to generate images with specific features. The auto-encoders are prone to generate a blurry output. One of the main reasons is the inability to parameterise complex distributions. The authors present a novel texture generative model architecture extending the variational auto-encoder approach. It gradually increases the accuracy of details in the reconstructed images. Thanks to the proposed architecture, the model is able to learn a higher level of details resulting from the partial disentanglement of latent variables. The generative model is also capable of synthesising complex real-world textures. The model consists of multiple separate latent layers responsible for learning the gradual levels of texture details. Separate training of latent representations increases the stability of the learning process and provides partial disentanglement of latent variables. The experiments with proposed architecture demonstrate the potential of variational auto-encoders in the domain of texture synthesis and also tend to yield sharper reconstruction as well as synthesised texture images.},
  archive      = {J_IETCV},
  author       = {Marek Jakab and Lukas Hudec and Wanda Benesova},
  doi          = {10.1049/iet-cvi.2019.0416},
  journal      = {IET Computer Vision},
  month        = {12},
  number       = {8},
  pages        = {564-574},
  shortjournal = {IET Comput. Vis.},
  title        = {Partial disentanglement of hierarchical variational auto-encoder for texture synthesis},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). ADFNet: Accumulated decoder features for real-time semantic
segmentation. <em>IETCV</em>, <em>14</em>(8), 555–563. (<a
href="https://doi.org/10.1049/iet-cvi.2019.0289">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Semantic segmentation is one of the important technologies in autonomous driving, and ensuring its real-time and high performance is of utmost importance for the safety of pedestrians and passengers. To improve its performance using deep neural networks that operate in real-time, the authors propose a simple and efficient method called ADFNet using accumulated decoder features, ADFNet operates by only using the decoder information without skip connections between the encoder and decoder. They demonstrate that the performance of ADFNet is superior to that of the state-of-the-art methods, including that of the baseline network on the cityscapes dataset. Further, they analyse the results obtained via ADFNet using class activation maps and RGB representations for image segmentation results.},
  archive      = {J_IETCV},
  author       = {Hyunguk Choi and Hoyeon Ahn and Joonmo Kim and Moongu Jeon},
  doi          = {10.1049/iet-cvi.2019.0289},
  journal      = {IET Computer Vision},
  month        = {12},
  number       = {8},
  pages        = {555-563},
  shortjournal = {IET Comput. Vis.},
  title        = {ADFNet: Accumulated decoder features for real-time semantic segmentation},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Subgraph and object context-masked network for scene graph
generation. <em>IETCV</em>, <em>14</em>(7), 546–553. (<a
href="https://doi.org/10.1049/iet-cvi.2019.0896">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Scene graph generation is to recognise objects and their semantic relationships in an image and can help computers understand visual scene. To improve relationship prediction, geometry information is essential and usually incorporated into relationship features. Existing methods use coordinates of objects to encode their spatial layout. However, in this way, they neglect the context of objects. In this study, to take full use of spatial knowledge efficiently, the authors propose a novel subgraph and object context-masked network (SOCNet) consisting of spatial mask relation inference (SMRI) and hierarchical message passing (HMP) modules to address the scene graph generation task. In particular, to take advantage of spatial knowledge, SMRI masks partial context of object features depending on their spatial layout of objects and corresponding subgraph to facilitate their relationship recognition. To refine the features of objects and subgraphs, they also propose HMP that passes highly correlated messages from both microcosmic and macroscopic aspects through a triple-path structure including subgraph–subgraph, object–object, and subgraph–object paths. Finally, statistical co-occurrence probability is used to regularise relationship prediction. SOCNet integrates HMP and SMRI into a unified network, and comprehensive experiments on visual relationship detection and visual genome datasets indicate that SOCNet outperforms several state-of-the-art methods on two common tasks.},
  archive      = {J_IETCV},
  author       = {Zhenxing Zheng and Zhendong Li and Gaoyun An and Songhe Feng},
  doi          = {10.1049/iet-cvi.2019.0896},
  journal      = {IET Computer Vision},
  month        = {10},
  number       = {7},
  pages        = {546-553},
  shortjournal = {IET Comput. Vis.},
  title        = {Subgraph and object context-masked network for scene graph generation},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Identification of crop diseases using improved convolutional
neural networks. <em>IETCV</em>, <em>14</em>(7), 538–545. (<a
href="https://doi.org/10.1049/iet-cvi.2019.0136">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Conventional AlexNet has the problems of slow training speed, single characteristic scale and low recognition accuracy. To solve these problems, a convolutional neural network identification model based on Inception module and dilated convolution is proposed in this study. The inception module combined with dilated convolution, could extract disease characteristics at different scales and increase the receptive field. By setting different parameters, six improved models were obtained. They were trained to identify 26 diseases of 14 different crops; then the authors selected optimal recognition model. On this basis, the segmented dataset and the grey-scaled dataset were trained as comparative experiments to explore the influence of background and colour features on the recognition results. After only two training epochs, the improved optimal model could achieve an accuracy of over 95%. Moreover, the final average identification accuracy reached 99.37%. Contrast experiments indicate that colour and background features may influence the recognition effect. The improved model can extract disease information from different scales in the feature map to identify diverse diseases of different crops. The proposed model has faster training speed and higher recognition accuracy than the traditional model, and thus it can provide a reference for crop disease identification in actual production.},
  archive      = {J_IETCV},
  author       = {Long Wang and Jun Sun and Xiaohong Wu and Jifeng Shen and Bing Lu and Wenjun Tan},
  doi          = {10.1049/iet-cvi.2019.0136},
  journal      = {IET Computer Vision},
  month        = {10},
  number       = {7},
  pages        = {538-545},
  shortjournal = {IET Comput. Vis.},
  title        = {Identification of crop diseases using improved convolutional neural networks},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Algorithm using supervised subspace learning and non-local
representation for pose variation recognition. <em>IETCV</em>,
<em>14</em>(7), 528–537. (<a
href="https://doi.org/10.1049/iet-cvi.2019.0017">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Pose variation has been one of the challenges of face recognition. To solve this challenge, the authors propose a classification algorithm using supervised subspace learning and non-local representation (SSLNR). In SSLNR, they first propose a supervised subspace learning algorithm (SSLA). SSLA includes three different terms. The first term is the difference term, which can reduce the intra-class differences. The second term is the block-diagonal regularisation term, which promotes the samples to be represented by intra-class samples. The last one is the noise robust term. Then, the original samples are mapped to the learned subspace by using SSLA. Thus, the intra-class differences of the samples mapped to the learned subspace are reduced. Finally, those mapped samples are classified by proposed non-local constraint-based extended sparse representation classifier. SSLNR is extensively evaluated using four databases, namely Georgia Tech, Label faces in the wild, FEI and CVL. Experimental results show that SSLNR achieves better performance than some state-of-the-art algorithms, such as DARG and RRNN.},
  archive      = {J_IETCV},
  author       = {Mengmeng Liao and Changzhi Wang and Xiaodong Gu},
  doi          = {10.1049/iet-cvi.2019.0017},
  journal      = {IET Computer Vision},
  month        = {10},
  number       = {7},
  pages        = {528-537},
  shortjournal = {IET Comput. Vis.},
  title        = {Algorithm using supervised subspace learning and non-local representation for pose variation recognition},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Deep emotion recognition based on audio–visual correlation.
<em>IETCV</em>, <em>14</em>(7), 517–527. (<a
href="https://doi.org/10.1049/iet-cvi.2020.0013">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Human emotion recognition is studied by means of unimodal channels over the last decade. However, efforts continue to answer tempting questions about how variant modalities can complement each other. This study proposes a multimodal approach using three-dimensional (3D) convolutional neural networks (CNNs) to model human emotion through a modality-referenced system while investigating the solution to such questions. The proposed modality-referenced system selects the input data based on one of the modalities regarded as reference or master. The other modality which is referred to as a slave simply adjusts or attunes itself with the master in the temporal domain. In this context, the authors developed three multimodal emotion recognition system, namely, video-referenced system, audio-referenced system, and the audio–visual-referenced system to explore the congruence impact of audio and video modalities on each other. Two pipelines of 3D CNN architectures are employed where k -means clustering is used in the master pipeline and the slave pipeline adapts itself in a temporal sense. The outputs of the two pipelines are fused to improve recognition performance. In addition, canonical correlation analysis and t -distributed stochastic neighbour embedding is used validating the experiments. Results show that temporal alignment of the data between two modalities improves the recognition performance significantly.},
  archive      = {J_IETCV},
  author       = {Noushin Hajarolasvadi and Hasan Demirel},
  doi          = {10.1049/iet-cvi.2020.0013},
  journal      = {IET Computer Vision},
  month        = {10},
  number       = {7},
  pages        = {517-527},
  shortjournal = {IET Comput. Vis.},
  title        = {Deep emotion recognition based on audio–visual correlation},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Stroke controllable style transfer based on dilated
convolutions. <em>IETCV</em>, <em>14</em>(7), 505–516. (<a
href="https://doi.org/10.1049/iet-cvi.2019.0912">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Transferring a photo to a stylised image with beautiful texture has become one of the most popular topics in computer vision and the application of image processing. Controlling the stroke size of the texture is one of the challenging problems in this task. Recent representative methods for such problem introduce a pyramid model to regulate receptive fields in the network. Meanwhile, dilated convolutions are proved to be a very efficient way to adjust receptive fields without losing resolution. By combining the advantages of both approaches and making special optimisation for VGG19 model for style transfer tasks, the authors propose to exploit dilated convolutions to extract texture information endowing the network with stroke controllable. Several sets of contrast experiments were conducted and results show that their algorithm can generate more attractive stylisation images and control stroke size flexibly. It demonstrates the superiority of applying dilated convolutions as a texture extraction method for maintaining more texture information and controlling stroke size.},
  archive      = {J_IETCV},
  author       = {Zhaopan Xu and Juan Zhang and Yu Zhang and Mingquan Zhou and Kang Li and Shengling Geng and Xiaojuan Zhang},
  doi          = {10.1049/iet-cvi.2019.0912},
  journal      = {IET Computer Vision},
  month        = {10},
  number       = {7},
  pages        = {505-516},
  shortjournal = {IET Comput. Vis.},
  title        = {Stroke controllable style transfer based on dilated convolutions},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Catadioptric hyperspectral imaging, an unmixing approach.
<em>IETCV</em>, <em>14</em>(7), 493–504. (<a
href="https://doi.org/10.1049/iet-cvi.2019.0784">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hyperspectral imaging systems provide dense spectral information on the scene under investigation by collecting data from a high number of contiguous bands of the electromagnetic spectrum. The low spatial resolutions of these sensors frequently give rise to the mixing problem in remote sensing applications. Several unmixing approaches are developed in order to handle the challenging mixing problem on perspective images. On the other hand, omnidirectional imaging systems provide a 360-degree field of view in a single image at the expense of lower spatial resolution. In this study, we propose a novel imaging system which integrates hyperspectral cameras with mirrors so on to yield catadioptric omnidirectional imaging systems to benefit from the advantages of both modes. Catadioptric images, incorporating a camera with a reflecting device, introduce radial warping depending on the structure of the mirror used in the system. This warping causes a non-uniformity in the spatial resolution which further complicates the unmixing problem. In this context, a novel spatial–contextual unmixing algorithm specifically for the large field of view of the hyperspectral imaging system is developed. The proposed algorithm is evaluated on various real-world and simulated cases. The experimental results show that the proposed approach outperforms compared methods.},
  archive      = {J_IETCV},
  author       = {Didem Ozisik Baskurt and Yalin Bastanlar and Yasemin Yardimci Cetin},
  doi          = {10.1049/iet-cvi.2019.0784},
  journal      = {IET Computer Vision},
  month        = {10},
  number       = {7},
  pages        = {493-504},
  shortjournal = {IET Comput. Vis.},
  title        = {Catadioptric hyperspectral imaging, an unmixing approach},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Learning across views for stereo image completion.
<em>IETCV</em>, <em>14</em>(7), 482–492. (<a
href="https://doi.org/10.1049/iet-cvi.2019.0775">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Stereo image completion (SIC) is to fill holes existing in a pair of stereo images. SIC is more complicated than single image repairing, which needs to complete the pair of images while keeping their stereoscopic consistency. In recent years, deep learning has been introduced into single image repairing but seldom used for SIC. The authors present a novel deep learning-based approach for SIC. In their method, an X-shaped fully convolutional network (called SICNet) is proposed and designed to complete stereo images, which is composed of two branches of convolutional neural network layers to encode the context of the left and right images separately, a fusion module for stereo-interactive completion, and two branches of decoders to produce completed left and right images, respectively. In consideration of both inter-view and intra-view cues, they introduce auxiliary networks and define comprehensive losses to train SICNet to perform single-view coherent and cross-view consistent completion simultaneously. Extensive experiments are conducted to show the state-of-the-art performances of the proposed approach and its key components.},
  archive      = {J_IETCV},
  author       = {Wei Ma and Mana Zheng and Wenguang Ma and Shibiao Xu and Xiaopeng Zhang},
  doi          = {10.1049/iet-cvi.2019.0775},
  journal      = {IET Computer Vision},
  month        = {10},
  number       = {7},
  pages        = {482-492},
  shortjournal = {IET Comput. Vis.},
  title        = {Learning across views for stereo image completion},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Generic wavelet-based image decomposition and reconstruction
framework for multi-modal data analysis in smart camera applications.
<em>IETCV</em>, <em>14</em>(7), 471–479. (<a
href="https://doi.org/10.1049/iet-cvi.2019.0780">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Effective acquisition, analysis and reconstruction of multi-modal data such as colour and multi-/hyper-spectral imagery is crucial in smart camera applications, where wavelet-based coding and compression of images are highly demanded. Many existing discrete wavelet filtering banks have fixed coefficients hence their performance is highly dependent on the signal/image being processed. To tackle this problem, a unified framework is proposed in this study, which can produce a series of discrete wavelet filtering banks, where many existing discrete wavelet filtering banks become special cases of the framework. For each generated filtering bank, it consists of two decomposition filters and two reconstruction filters through an optimisation process. The efficacy of the filtering banks produced by the framework has been validated in two case studies, including colour image decomposition and reconstruction, and hyperspectral image classification. Comprehensive experiments have demonstrated the superior performance of the proposed framework, which will benefit the efficacy of smart camera and camera network applications.},
  archive      = {J_IETCV},
  author       = {Yijun Yan and Yiguang Liu and Mingqiang Yang and Huimin Zhao and Yanmei Chai and Jinchang Ren},
  doi          = {10.1049/iet-cvi.2019.0780},
  journal      = {IET Computer Vision},
  month        = {10},
  number       = {7},
  pages        = {471-479},
  shortjournal = {IET Comput. Vis.},
  title        = {Generic wavelet-based image decomposition and reconstruction framework for multi-modal data analysis in smart camera applications},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Decentralised indoor smart camera mapping and hierarchical
navigation for autonomous ground vehicles. <em>IETCV</em>,
<em>14</em>(7), 462–470. (<a
href="https://doi.org/10.1049/iet-cvi.2019.0949">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this work, the authors propose a novel decentralised coordination scheme for autonomous ground vehicles to enable map building and path planning with a network of smart overhead cameras. Decentralised indoor smart camera mapping and hierarchical navigation supports the automatic generation of waypoint graphs for each camera in an environment and allows path planning through the environment across multiple camera fields of view, or subviews. The proposed solution utilises the growing neural gas algorithm to learn the topology of unoccupied working space in each subview for maintaining a dynamic waypoint graph on each camera. The authors’ pathing solution leverages a modified version of the A* algorithm to compute paths in a decentralised and hierarchical fashion. Waypoint generation was simulated and analysed on a generated environment to ensure it is both effective and efficient, while path planning was simulated on various randomised hierarchical graphs to effectively compare the proposed Decentralised-A* (D-A*) algorithm against standard greedy search. The proposed method efficiently handles the cases where other robot navigation methods are otherwise weak and ineffective, while still providing avenues for further optimisation of resource overhead for both the smart camera network as well as the robots themselves.},
  archive      = {J_IETCV},
  author       = {Taylor J.L. Whitaker and Samantha-Jo Cunningham and Christophe Bobda},
  doi          = {10.1049/iet-cvi.2019.0949},
  journal      = {IET Computer Vision},
  month        = {10},
  number       = {7},
  pages        = {462-470},
  shortjournal = {IET Comput. Vis.},
  title        = {Decentralised indoor smart camera mapping and hierarchical navigation for autonomous ground vehicles},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Drone swarm patrolling with uneven coverage requirements.
<em>IETCV</em>, <em>14</em>(7), 452–461. (<a
href="https://doi.org/10.1049/iet-cvi.2019.0963">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Swarms of drones are being more and more used in many practical scenarios, such as surveillance, environmental monitoring, search and rescue in hardly-accessible areas and so on. While a single drone can be guided by a human operator, the deployment of a swarm of multiple drones requires proper algorithms for automatic task-oriented control. In this study, the authors focus on visual coverage optimisation with drone-mounted camera sensors. In particular, they consider the specific case in which the coverage requirements are uneven, meaning that different parts of the environment have different coverage priorities. They model these coverage requirements with relevance maps and propose a deep reinforcement learning algorithm to guide the swarm. This study first defines a proper learning model for a single drone, and then extends it to the case of multiple drones both with greedy and cooperative strategies. Experimental results show the performance of the proposed method, also compared with a standard patrolling algorithm.},
  archive      = {J_IETCV},
  author       = {Claudio Piciarelli and Gian Luca Foresti},
  doi          = {10.1049/iet-cvi.2019.0963},
  journal      = {IET Computer Vision},
  month        = {10},
  number       = {7},
  pages        = {452-461},
  shortjournal = {IET Comput. Vis.},
  title        = {Drone swarm patrolling with uneven coverage requirements},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Dual attention module and multi-label based fully
convolutional network for crowd counting. <em>IETCV</em>,
<em>14</em>(7), 443–451. (<a
href="https://doi.org/10.1049/iet-cvi.2019.0674">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {High-density crowd counting in natural scenes is an extremely difficult and challenging research subject in computer vision. Although the algorithm based on the convolutional neural network has achieved significantly better results than the traditional algorithm, most of them tend to focus on the local features of images, and difficult to obtain the rich global contextual dependencies. To solve this problem, a dual attention module and a multi-label based fully convolutional network are proposed in this study. Moreover, the authors improve the algorithm by the following multiple perspectives. Firstly, introducing the dual attention module, the global-context and long-range dependency are adaptively integrated into both spatial and channel dimensions, which improve the network expression ability. Then, the prediction error is effectively reduced by designing a multi-label mechanism, so the crowd-counting task is transformed into foreground and background segmentation tasks to assist in the regression task of the density map. Furthermore, on the basis of the traditional Euclidean distance loss and cross-entropy loss, the structural similarity index is introduced to further improve the training effect of the model. The test results of the UCF_CC_50, ShanghaiTech, and UCF-QNRF datasets indicate that the proposed method is superior to the current mainstream algorithm.},
  archive      = {J_IETCV},
  author       = {Suyu Wang and Bin Yang and Bo Liu and Guanghui Zheng},
  doi          = {10.1049/iet-cvi.2019.0674},
  journal      = {IET Computer Vision},
  month        = {10},
  number       = {7},
  pages        = {443-451},
  shortjournal = {IET Comput. Vis.},
  title        = {Dual attention module and multi-label based fully convolutional network for crowd counting},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Harnessing feedback region proposals for multi-object
tracking. <em>IETCV</em>, <em>14</em>(7), 434–442. (<a
href="https://doi.org/10.1049/iet-cvi.2019.0943">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the tracking-by-detection approach of online multiple object tracking (MOT), a major challenge is how to associate object detections on the new video frame with previously tracked objects. Two important aspects that directly influence the performance of MOT are quality of detection and accuracy in data association. The authors propose an efficient and unified MOT framework for improved object detection, followed by enhanced object tracking. The object detection and tracking are considered as two independent functions in the tracking-by-detection paradigm. In this study, object detection accuracy has been increased by employing a faster region-based convolutional neural network (Faster R-CNN) modified with the feedback region proposals from the tracker. Target association is performed by the correlation filter-based Siamese CNN model, which finds the similarity score between the input image patches. The Siamese CNN is trained using a supervised hard sample mining strategy. An optical flow-based motion model is employed to predict the next probable location of the targets from the tracker and these region proposals are fed back to the classifier module of Faster R-CNN. The authors’ extensive analysis of publicly available MOT benchmark datasets and comparison with the state-of-the-art tracking methods demonstrate competitive tracking performance of the proposed MOT framework.},
  archive      = {J_IETCV},
  author       = {Aswathy Prasanna Kumar and Deepak Mishra},
  doi          = {10.1049/iet-cvi.2019.0943},
  journal      = {IET Computer Vision},
  month        = {10},
  number       = {7},
  pages        = {434-442},
  shortjournal = {IET Comput. Vis.},
  title        = {Harnessing feedback region proposals for multi-object tracking},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Modelling large scale camera networks for identification and
tracking: An abstract framework. <em>IETCV</em>, <em>14</em>(7),
426–433. (<a href="https://doi.org/10.1049/iet-cvi.2019.0959">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this study, the authors discuss a novel approach for multi-camera-based unobtrusive identification and tracking of occupants in wide-area, multi-building scenarios. Considering the scalability issues in adopting a centralised approach to monitor wide-area scenarios, they proposed a distributed approach to occupant identification and tracking. The key technical idea underlying their approach is to abstract a wide-area indoor surveillance environment using a distributed state transition system (DSTS) model, which in turn is composed of independent building-specific state transition systems, coordinating and collaborating with each other. This study presents the details of their DSTS model and examines the temporal ordering of recognition events within the DSTS for ensuring accurate state information and responses to spatio–temporal queries. They also provide an experimental evaluation of the performance of their model using precision-recall metrics. Their conclusion is that the DSTS model serves as an efficient mechanism for tracking occupants in wide-area, multi-building scenarios monitored by camera networks.},
  archive      = {J_IETCV},
  author       = {Lakshmi Mohan and Vivek Menon},
  doi          = {10.1049/iet-cvi.2019.0959},
  journal      = {IET Computer Vision},
  month        = {10},
  number       = {7},
  pages        = {426-433},
  shortjournal = {IET Comput. Vis.},
  title        = {Modelling large scale camera networks for identification and tracking: An abstract framework},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). YOLOpeds: Efficient real-time single-shot pedestrian
detection for smart camera applications. <em>IETCV</em>, <em>14</em>(7),
417–425. (<a href="https://doi.org/10.1049/iet-cvi.2019.0897">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep-learning-based pedestrian detectors can enhance the capabilities of smart camera systems in a wide spectrum of machine vision applications including video surveillance, autonomous driving, robots and drones, smart factory, and health monitoring. However, such complex paradigms do not scale easily and are not traditionally implemented in resource-constrained smart cameras for on-device processing which offers significant advantages in situations when real-time monitoring and privacy are vital. This work addresses the challenge of achieving a good trade-off between accuracy and speed for efficient deep-learning-based pedestrian detection in smart camera applications. The contributions of this work are the following: 1) a computationally efficient architecture based on separable convolutions that integrates dense connections across layers and multi-scale feature fusion to improve representational capacity while decreasing the number of parameters and operations, 2) a more elaborate loss function for improved localization, 3) and an anchor-less approach for detection. The proposed approach referred to as YOLOpeds is evaluated using the PETS2009 surveillance dataset on 320 × 320 images. A real-system implementation is presented using the Jetson TX2 embedded platform. YOLOpeds provides real-time sustained operation of over 30 frames per second with detection rates in the range of 86% outperforming existing deep learning models.},
  archive      = {J_IETCV},
  author       = {Christos Kyrkou},
  doi          = {10.1049/iet-cvi.2019.0897},
  journal      = {IET Computer Vision},
  month        = {10},
  number       = {7},
  pages        = {417-425},
  shortjournal = {IET Comput. Vis.},
  title        = {YOLOpeds: Efficient real-time single-shot pedestrian detection for smart camera applications},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Beyond top-n accuracy indicator: A comprehensive evaluation
indicator of CNN models in image classification. <em>IETCV</em>,
<em>14</em>(6), 407–414. (<a
href="https://doi.org/10.1049/iet-cvi.2018.5839">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nowadays, a large number of deep convolutional neural network (CNN) models are applied to image classification tasks. However, the authors find that the most widely used evaluation indicator, the Top- N Accuracy indicator, cannot discriminate these models effectively. In this study, they propose a new indicator called Maximum-Spanning-Confusion-Tree indicator to solve this problem. The Maximum-Spanning-Confusion-Tree indicator is computed based on the hierarchical structure of the Maximum Spanning Confusion Tree of the deep CNN model on the dataset and reflect the ability of deep CNN models to discriminate confused categories in the dataset. The hierarchical structure of the Maximum Spanning Confusion Tree can reveal the confused category set of one selected category in the dataset efficiently and flexibly. Experiments show that they can discriminate ten different deep CNN models more accurately with the Maximum Spanning Confusion Tree indicator than the Top- N Accuracy indicator and the Maximum Spanning Confusion Tree intuitively shows the distribution of confused category sets in the dataset so they can find out the weakness of deep CNN models effectively.},
  archive      = {J_IETCV},
  author       = {Yuntao Liu and Yong Dou and Peng Qiao},
  doi          = {10.1049/iet-cvi.2018.5839},
  journal      = {IET Computer Vision},
  month        = {9},
  number       = {6},
  pages        = {407-414},
  shortjournal = {IET Comput. Vis.},
  title        = {Beyond top-N accuracy indicator: A comprehensive evaluation indicator of CNN models in image classification},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Spatial–temporal representation for video re-identification
via key images. <em>IETCV</em>, <em>14</em>(6), 399–406. (<a
href="https://doi.org/10.1049/iet-cvi.2018.5562">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Video-based person re-identification aims to verify the pedestrian identity from image sequences. The sequences are captured by cameras located in different directions at different times. Existing studies have certain limitations in the case of occlusions and pose variations. To solve the aforementioned problems, this study proposes a new two-stage framework, from which the key-image-based fusion spatial–temporal feature (KISTF) of the pedestrian can be extracted from the video. The image-level features at all timestamps are aggregated into the sequence-level feature representation of the video by using an long short-term memory network. Additionally, the concept of key image is defined for the image sequence, and the frame-level feature of the pedestrian is extracted from these key images. The proposed spatial–temporal feature, KISTF, is obtained by fusing the sequence-level feature and the frame-level feature. It aims to solve the problem of pedestrian representation in small video data sets. Experiments are conducted on the iLIDS-VID and PRID2011 data sets. The results demonstrate that the proposed approach outperforms state-of-the-art video-based re-identification methods.},
  archive      = {J_IETCV},
  author       = {Wanru Song and Changhong Chen and Qingqing Zhao and Feng Liu},
  doi          = {10.1049/iet-cvi.2018.5562},
  journal      = {IET Computer Vision},
  month        = {9},
  number       = {6},
  pages        = {399-406},
  shortjournal = {IET Comput. Vis.},
  title        = {Spatial–temporal representation for video re-identification via key images},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Accurate and fast single shot multibox detector.
<em>IETCV</em>, <em>14</em>(6), 391–398. (<a
href="https://doi.org/10.1049/iet-cvi.2019.0711">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the development of deep learning, the performance of object detection has made great progress. However, there are still some challenging problems, such as the detection accuracy of small objects and the efficiency of the detector. This study proposes an accurate and fast single shot multibox detector, which includes context comprehensive enhancement (CCE) module and feature enhancement module (FEM). To integrate more efficient information when aggregating context information, the conv4_3 and fc_7 feature maps are merged to design the CCE module. To obtain more fine-grained feature information, this study presents a FEM and special feature enhancement module (FEM-s) module that can fuse different receptive field sizes to better adapt to the scale change of the object. Compared to existing methods based on deep learning, the proposed method helps to gradually produce more detailed feature maps with better performance. Under the premise of ensuring real-time speed, the authors network can achieve 81.2 mean average precision on the PASCAL VOC 2007 test with an input size of 320 × 320 on a single Nvidia 2080Ti GPU.},
  archive      = {J_IETCV},
  author       = {Lie Guo and Dongxing Wang and Linhui Li and Jindun Feng},
  doi          = {10.1049/iet-cvi.2019.0711},
  journal      = {IET Computer Vision},
  month        = {9},
  number       = {6},
  pages        = {391-398},
  shortjournal = {IET Comput. Vis.},
  title        = {Accurate and fast single shot multibox detector},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Motion boundary emphasised optical flow method for human
action recognition. <em>IETCV</em>, <em>14</em>(6), 378–390. (<a
href="https://doi.org/10.1049/iet-cvi.2018.5556">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study proposes a three-stream model using two different types of deep convolutional neural networks (CNNs): (i) a spatial stream with a CNN on images; (ii) a ResNet (residual network) on optical flows; and, (iii) a ResNet on the concatenation of motion features. This model is applied to four datasets: (i) UCF Sports; (ii) Youtube Sports; (iii) SBU action interaction; and (iv) a subset of the UCF-1M Sports. Using two optical flow estimation methods: (i) a motion boundary emphasised Epicflow (Edge Preserving Interpolation Correspondences for Optical Flow) method, (MBEpicflow); and (ii) the Flownet 2 method, a learning optical flow estimation method. It was found that (i) the proposed MBEpicflow outperforms the Flownet 2 method on the SBU dataset, while the Flownet 2 performs equally well or better than the MBEpicflow method on the other three datasets, and these results are the best when compared with those obtained using other approaches on all datasets evaluated. These results showed the importance of accurate optical flow plays in human action recognition, an aspect which has been seldom addressed. Moreover, it showed that if some measure of the global behaviours of motion is incorporated, the generalisation performance is often improved by 1–2%.},
  archive      = {J_IETCV},
  author       = {Cheng Peng and Haozhi Huang and Ah-Chung Tsoi and Sio-Long Lo and Yun Liu and Zi-yi Yang},
  doi          = {10.1049/iet-cvi.2018.5556},
  journal      = {IET Computer Vision},
  month        = {9},
  number       = {6},
  pages        = {378-390},
  shortjournal = {IET Comput. Vis.},
  title        = {Motion boundary emphasised optical flow method for human action recognition},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Offline mobile diagnosis system for citrus pests and
diseases using deep compression neural network. <em>IETCV</em>,
<em>14</em>(6), 370–377. (<a
href="https://doi.org/10.1049/iet-cvi.2018.5784">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study presents an offline mobile diagnosis system for citrus pests and diseases by compression convolutional neural network. Recently, with the growth of labelled data, the deep neural network incites the revolutionary change with a quantum leap in various fields. Benefiting from the backpropagation method, the proper network structure can automatically extract high-level representations and find corresponding labels. The authors made use of the advantages of the deep neural network to design an android application, which can be installed in any stand-alone devices to instantaneously identify the citrus pests and diseases. The proposed diagnosis system has three characteristics: low cost, low latency and high accuracy. These characteristics contribute to make the professional offline prediction for avoiding further economic loss caused by disease spreading. In order to validate the proposed system, the authors conducted thorough evaluations on two data sets, ‘citrus pests and diseases’, CIFAR, which show the superiority of the proposed approach in terms of the accuracy and the number of model parameters.},
  archive      = {J_IETCV},
  author       = {Jie You and Joonwhoan Lee},
  doi          = {10.1049/iet-cvi.2018.5784},
  journal      = {IET Computer Vision},
  month        = {9},
  number       = {6},
  pages        = {370-377},
  shortjournal = {IET Comput. Vis.},
  title        = {Offline mobile diagnosis system for citrus pests and diseases using deep compression neural network},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Interactive facial animation with deep neural networks.
<em>IETCV</em>, <em>14</em>(6), 359–369. (<a
href="https://doi.org/10.1049/iet-cvi.2019.0790">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Creating realistic animations of human faces is still a challenging task in computer graphics. While computer graphics (CG) models capture much variability in a small parameter vector, they usually do not meet the necessary visual quality. This is due to the fact, that geometry-based animation often does not allow fine-grained deformations and fails in difficult areas (mouth, eyes) to produce realistic renderings. Image-based animation techniques avoid these problems by using dynamic textures that capture details and small movements that are not explained by geometry. This comes at the cost of high-memory requirements and limited flexibility in terms of animation because dynamic texture sequences need to be concatenated seamlessly, which is not always possible and prone to visual artefacts. In this study, the authors present a new hybrid animation framework that exploits recent advances in deep learning to provide an interactive animation engine that can be used via a simple and intuitive visualisation for facial expression editing. The authors describe an automatic pipeline to generate training sequences that consist of dynamic textures plus sequences of consistent three-dimensional face models. Based on this data, they train a variational autoencoder to learn a low-dimensional latent space of facial expressions that is used for interactive facial animation.},
  archive      = {J_IETCV},
  author       = {Wolfgang Paier and Anna Hilsmann and Peter Eisert},
  doi          = {10.1049/iet-cvi.2019.0790},
  journal      = {IET Computer Vision},
  month        = {9},
  number       = {6},
  pages        = {359-369},
  shortjournal = {IET Comput. Vis.},
  title        = {Interactive facial animation with deep neural networks},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Going beyond free viewpoint: Creating animatable volumetric
video of human performances. <em>IETCV</em>, <em>14</em>(6), 350–358.
(<a href="https://doi.org/10.1049/iet-cvi.2019.0786">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {An end-to-end pipeline for the creation of high-quality animatable volumetric video of human performances is presented. Going beyond the application of free-viewpoint video, the authors allow re-animation and alteration of an actor&#39;s performance through the enrichment of the captured data with semantics and animation properties. Hybrid geometry- and video-based animation methods are applied that allow a direct animation of the high-quality data itself instead of creating a CG model that resembles the captured data. Semantic enrichment and animation are achieved by establishing temporal consistency followed by automatic rigging of each 3D frame using a parametric human body model. The hybrid approach combines the flexibility of classical CG animation with the realism of real captured data. For the face, coarse movements are modelled in the geometry only, while very fine and subtle details, often lacking in purely geometric methods, are captured in video textures, which can interactively be combined to form new facial expressions. On top of that, regions that are challenging to synthesise, such as the teeth or the eyes, are learned and filled in realistically in an autoencoder-based approach. This study covers the full pipeline from capturing, volumetric video production, and enrichment with semantics for the final hybrid animation.},
  archive      = {J_IETCV},
  author       = {Anna Hilsmann and Philipp Fechteler and Wieland Morgenstern and Wolfgang Paier and Ingo Feldmann and Oliver Schreer and Peter Eisert},
  doi          = {10.1049/iet-cvi.2019.0786},
  journal      = {IET Computer Vision},
  month        = {9},
  number       = {6},
  pages        = {350-358},
  shortjournal = {IET Comput. Vis.},
  title        = {Going beyond free viewpoint: Creating animatable volumetric video of human performances},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Deep quantised portrait matting. <em>IETCV</em>,
<em>14</em>(6), 339–349. (<a
href="https://doi.org/10.1049/iet-cvi.2019.0779">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Portrait matting is of vital importance for many applications such as portrait editing, background replacement, ecommerce demonstration, and augmented reality. The portrait matt can be accessed by predicting the α value of the original picture. Previous deep matting methods usually adopt a segmentation network to tackle portrait matting tasks. However, these traditional methods will introduce unpleasant blemishes in the matting results sometimes. The authors find that the key factor behind this phenomenon is how they model the matting problem. On the one hand, α value predicting can be modelled as a regression task. On the other hand, it can be viewed as a classification task of predicting background or foreground. To solve this problem, they explore different methods to model the nature of the α matting problem and propose a novel quantisation-based adaption. Their method comes up with an α quantisation loss to achieve multi-threshold filtering. Furthermore, they apply an α merging block to improve conventional regression methods. With their method, the gradient loss is reduced by 7.53% relatively, with mean square error and sum of absolute difference decreased by 14.7% relatively, leading to a more visually pleasant α matt in several segmentation backbones.},
  archive      = {J_IETCV},
  author       = {Zhan Zhang and Yuehai Wang and Jianyi Yang},
  doi          = {10.1049/iet-cvi.2019.0779},
  journal      = {IET Computer Vision},
  month        = {9},
  number       = {6},
  pages        = {339-349},
  shortjournal = {IET Comput. Vis.},
  title        = {Deep quantised portrait matting},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Motion-based frame interpolation for film and television
effects. <em>IETCV</em>, <em>14</em>(6), 323–338. (<a
href="https://doi.org/10.1049/iet-cvi.2019.0814">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Frame interpolation is the process of synthesising a new frame in-between existing frames in an image sequence. It has emerged as a key algorithmic module in motion picture effects. In the context of this special issue, this study provides a review of the technology used to create in-between frames and presents a Bayesian framework that generalises frame interpolation algorithms using the concept of motion interpolation. Unlike existing literature in this area, the authors also compare performance using the top industrial toolkits used in the post production industry. They find that all successful techniques employ motion-based interpolation, and the commercial version of the Bayesian approach performs best. Another goal of this study is to compare the performance gains with recent convolutional neural network (CNN) algorithms against the traditional explicit model-based approaches. They find that CNNs do not clearly outperform the explicit motion-based techniques, and require significant compute resources, but provide complementary improvements in certain types of sequences.},
  archive      = {J_IETCV},
  author       = {Anil Kokaram and Davinder Singh and Simon Robinson and Damien Kelly and Bill Collis and Kim Libreri},
  doi          = {10.1049/iet-cvi.2019.0814},
  journal      = {IET Computer Vision},
  month        = {9},
  number       = {6},
  pages        = {323-338},
  shortjournal = {IET Comput. Vis.},
  title        = {Motion-based frame interpolation for film and television effects},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Advances in colour transfer. <em>IETCV</em>, <em>14</em>(6),
304–322. (<a href="https://doi.org/10.1049/iet-cvi.2019.0920">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Colour grading is an essential step in movie post-production, which is done in the industry by experienced artists on expensive edit hardware and software suites. This paper presents a review of the advances made to automate this process. The review looks in particular at how the state-of-the-art in optimal transport and deep learning has advanced some of the fundamental problems of colour transfer, and how far are we still from being able to automatically grade images.},
  archive      = {J_IETCV},
  author       = {Francois Pitié},
  doi          = {10.1049/iet-cvi.2019.0920},
  journal      = {IET Computer Vision},
  month        = {9},
  number       = {6},
  pages        = {304-322},
  shortjournal = {IET Comput. Vis.},
  title        = {Advances in colour transfer},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Image stylisation: From predefined to personalised.
<em>IETCV</em>, <em>14</em>(6), 291–303. (<a
href="https://doi.org/10.1049/iet-cvi.2019.0787">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The authors present a framework for interactive design of new image stylisations using a wide range of predefined filter blocks. Both novel and off-the-shelf image filtering and rendering techniques are extended and combined to allow the user to unleash their creativity to intuitively invent, modify, and tune new styles from a given set of filters. In parallel to this manual design, they propose a novel procedural approach that automatically assembles sequences of filters, leading to unique and novel styles. An important aim of the authors’ framework is to allow for interactive exploration and design, as well as to enable videos and camera streams to be stylised on the fly. In order to achieve this real-time performance, they use the Best Linear Adaptive Enhancement (BLADE) framework – an interpretable shallow machine learning method that simulates complex filter blocks in real time. Their representative results include over a dozen styles designed using their interactive tool, a set of styles created procedurally, and new filters trained with their BLADE approach.},
  archive      = {J_IETCV},
  author       = {Ignacio Garcia-Dorado and Pascal Getreuer and Bartlomiej Wronski and Peyman Milanfar},
  doi          = {10.1049/iet-cvi.2019.0787},
  journal      = {IET Computer Vision},
  month        = {9},
  number       = {6},
  pages        = {291-303},
  shortjournal = {IET Comput. Vis.},
  title        = {Image stylisation: From predefined to personalised},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Accurate scale estimation for visual tracking with
significant deformation. <em>IETCV</em>, <em>14</em>(5), 278–287. (<a
href="https://doi.org/10.1049/iet-cvi.2019.0860">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Scale variation of a target frequently appears in tasks of visual tracking. Accurate scale estimation is challenging due to deformation, occlusion, rotation, change in the view angle and diversity of tracking object categories. Most tracking methods employ an exhaustive search of scales to estimate the target scales. However, only finite and discrete scales are usually searched due to the expensive computation requirement. Here, the authors propose a novel scale estimation method based on bounding box regression (BBR). They first formulate the scale tracking as a regression problem, and search for the entire continuous scale space without being limited by a manually specified number of scales. Then they extend the original single-channel BBR to multi-channel situations, to allow for better employment of multi-channel features. To further take advantage of the time prior information of training samples, they derive a time-related sample weighted multi-channel BBR. Besides, they propose a quantitative measurement, scale divergence degree, to reflect the diversity of sampling strategy. Experimental results on OTB-2015D dataset demonstrate that the proposed approach achieves outstanding scale estimation performance for visual tracking with significant deformation.},
  archive      = {J_IETCV},
  author       = {Lutao Chu and Huiyun Li and Zhiheng Yang},
  doi          = {10.1049/iet-cvi.2019.0860},
  journal      = {IET Computer Vision},
  month        = {8},
  number       = {5},
  pages        = {278-287},
  shortjournal = {IET Comput. Vis.},
  title        = {Accurate scale estimation for visual tracking with significant deformation},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Pose-invariant face recognition based on matching the
occlusion free regions aligned by 3D generic model. <em>IETCV</em>,
<em>14</em>(5), 268–277. (<a
href="https://doi.org/10.1049/iet-cvi.2019.0244">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Face recognition systems perform accurately in a controlled environment, but an unconstrained environment dramatically degrades their performance. In this study, a novel pose-invariant face recognition system is proposed based on the occlusion free regions. This method utilises a gallery set of frontal face images and can handle large pose variations. For a 2D probe face image with an arbitrary pose, the head pose is first obtained using a robust head pose estimation method. Then, this 2D face image is normalised by a novel 3D modelling method from a single input image. In consequence, pose invariant face recognition is converted to a frontal face recognition problem. The 3D structure is reconstructed using a new method based on the estimated head pose and only one facial feature point, which is significantly reduced in comparison with the number of landmarks used in previous methods. According to the estimated poses, occlusion free regions are extracted from normalised images as feature extraction. Finally, face matching and recognition is performed using these regions from normalised test images and the corresponding regions of gallery images. Experimental results on FERET and CAS-PEAL-R1 databases demonstrate that the proposed method outperforms other methods, and it is robust and efficient.},
  archive      = {J_IETCV},
  author       = {Arezoo Sadeghzadeh and Hossein Ebrahimnezhad},
  doi          = {10.1049/iet-cvi.2019.0244},
  journal      = {IET Computer Vision},
  month        = {8},
  number       = {5},
  pages        = {268-277},
  shortjournal = {IET Comput. Vis.},
  title        = {Pose-invariant face recognition based on matching the occlusion free regions aligned by 3D generic model},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Optimisation-based training of evolutionary convolution
neural network for visual classification applications. <em>IETCV</em>,
<em>14</em>(5), 259–267. (<a
href="https://doi.org/10.1049/iet-cvi.2019.0506">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Training of the convolution neural network (CNN) is a problem of global optimisation. This study proposed a hybrid modified particle swarm optimisation (MPSO) and conjugate gradient (CG) algorithm for efficient training of CNN. The training involves MPSO–CG to avoid trapping in local minima. Particularly, improvements in the MPSO by introducing a novel approach for control parameters, improved parameters updating criteria, a novel parameter in the velocity update equation, and fusion of the CG allows handling the issues in training CNN. In this study, the authors validate the proposed MPSO algorithm on three benchmark mathematical test functions and also compared with three different variants of the baseline particle swarm optimisation algorithm. Furthermore, the performance of the proposed MPSO–CG is also compared with other training algorithms focusing on the analysis of computational cost, convergence, and accuracy based on a standard problem specific to classification applications on CIFAR-10 dataset and face and skin detection dataset.},
  archive      = {J_IETCV},
  author       = {Shanshan Tu and Sadaqat ur Rehman and Muhammad Waqas and Obaid ur Rehman and Zhongliang Yang and Basharat Ahmad and Zahid Halim and Wei Zhao},
  doi          = {10.1049/iet-cvi.2019.0506},
  journal      = {IET Computer Vision},
  month        = {8},
  number       = {5},
  pages        = {259-267},
  shortjournal = {IET Comput. Vis.},
  title        = {Optimisation-based training of evolutionary convolution neural network for visual classification applications},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). STDC-flow: Large displacement flow field estimation using
similarity transformation-based dense correspondence. <em>IETCV</em>,
<em>14</em>(5), 248–258. (<a
href="https://doi.org/10.1049/iet-cvi.2019.0321">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In order to improve the accuracy and robustness of optical flow computation under large displacements and motion occlusions, the authors present in this study a large displacement flow field estimation approach using similarity transformation-based dense correspondence, named STDC-Flow approach. First, the authors compute an initial nearest-neighbour field by using the STDC-Flow of the consecutive two frames, and then extract the consistent regions as the robust nearest-neighbour field and label the inconsistent regions as the occlusion areas. Second, they improve a non-local total variation with the L 1 norm optical flow model by using the occlusion information to modify the weighted median filtering optimisation. Third, they fuse the robust nearest-neighbour field and the computed flow field of the improved variational optical flow model to construct the final flow field by using the quadratic pseudo-boolean optimisation fusion algorithm. Finally, the authors compare the proposed STDC-Flow method with several state-of-the-art approaches including the variational and deep learning-based optical flow models by using the MPI-Sintel and KITTI evaluation databases. The comparison results demonstrate that the proposed STDC-Flow method has a high accuracy for flow field computation, especially the capacity of dealing with large displacements and motion occlusions.},
  archive      = {J_IETCV},
  author       = {Congxuan Zhang and Zhen Chen and Fan Xiong and Wen Liu and Ming Li and Liyue Ge},
  doi          = {10.1049/iet-cvi.2019.0321},
  journal      = {IET Computer Vision},
  month        = {8},
  number       = {5},
  pages        = {248-258},
  shortjournal = {IET Comput. Vis.},
  title        = {STDC-flow: Large displacement flow field estimation using similarity transformation-based dense correspondence},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Polyp detection using CNNs in colonoscopy video.
<em>IETCV</em>, <em>14</em>(5), 241–247. (<a
href="https://doi.org/10.1049/iet-cvi.2019.0300">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Polyps are a group of cells growing on the inner surface of the colon. Over time, some polyps can lead to colon cancer, which is often fatal if found in its later stages. Colon cancer can be prevented if the polyps are identified and removed in their early stages. Colonoscopy is a very effective screening method to remove polyps and it largely prevents colon cancer. However, some polyps may not be detected during a colonoscopy due to human error. Over the past two decades, many studies have been conducted on computer-aided detection to reduce the miss rate of polyps. This study consists of two distinct parts, the detection of frames containing polyps and polyp segmentation. In the first section, a new convolutional neural network based on the VGG network is proposed. The proposed network has an accuracy of 86% on a newly collected dataset. In the polyp segmentation section, a fully convolutional network and an effective post-processing algorithm are presented. An evaluation of the proposed polyp segmentation system on the ETIS-LARIB database achieves an overall 82.00% F2 score, which outperforms the methods that participated in the sub-challenge of MICCAI.},
  archive      = {J_IETCV},
  author       = {Azadeh Haj-Manouchehri and Hossein Mahvash Mohammadi},
  doi          = {10.1049/iet-cvi.2019.0300},
  journal      = {IET Computer Vision},
  month        = {8},
  number       = {5},
  pages        = {241-247},
  shortjournal = {IET Comput. Vis.},
  title        = {Polyp detection using CNNs in colonoscopy video},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Orthogonal random projection for tensor completion.
<em>IETCV</em>, <em>14</em>(5), 233–240. (<a
href="https://doi.org/10.1049/iet-cvi.2018.5764">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The low-rank tensor completion problem, which aims to recover the missing data from partially observable data. However, most of the existing tensor completion algorithms based on Tucker decomposition cannot avoid using singular value decomposition (SVD) operation to calculate the Tucker factors, so they are not suitable for the completion of large-scale data. To solve this problem, they propose a new faster tensor completion algorithm, which uses the method of random projection to project the unfolding matrix of each mode of the tensor into the low-dimensional subspace, and then obtain the Tucker factors by the orthogonal decomposition. Their method can effectively avoid the high computational cost of SVD operation. The results of the synthetic data experiments and real data experiments verify the effectiveness and feasibility of their method.},
  archive      = {J_IETCV},
  author       = {Yali Feng and Guoxu Zhou},
  doi          = {10.1049/iet-cvi.2018.5764},
  journal      = {IET Computer Vision},
  month        = {8},
  number       = {5},
  pages        = {233-240},
  shortjournal = {IET Comput. Vis.},
  title        = {Orthogonal random projection for tensor completion},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). RTL3D: Real-time LIDAR-based 3D object detection with sparse
CNN. <em>IETCV</em>, <em>14</em>(5), 224–232. (<a
href="https://doi.org/10.1049/iet-cvi.2019.0508">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {LIDAR (light detection and ranging) based real-time 3D perception is crucial for applications such as autonomous driving. However, most of the convolutional neural network (CNN) based methods are time-consuming and computation-intensive. These drawbacks are mainly attributed to the highly variable density of LIDAR point cloud and the complexity of their pipelines. To find a balance between speed and accuracy for 3D object detection from LIDAR, authors propose RTL3D, a computationally efficient Real-time LIDAR-based 3D detector. In RTL3D, an effective voxel-wise feature representation is utilised to organise unstructured point cloud. By employing a sparse feature learning network (SFLN) on voxelised 3D data, RTL3D exploits the sparsity of point cloud and down-samples 3D data into 2D. Basing on the generated 2D feature map, an optimised dense detection network (DDN) is applied to regress the oriented bounding box without relying on any predefined anchor boxes. The authors also introduce an incremental data augmentation approach which greatly improves the performance of RTL3D. Empirical experiments on public KITTI benchmark demonstrate that RTL3D achieves a competitive performance with state-of-the-art works on 3D detection task. Owning to the simplicity of its single-stage and anchor-free design, RTL3D has a real-time inference speed of 40 FPS.},
  archive      = {J_IETCV},
  author       = {Lin Yan and Kai Liu and Evgeny Belyaev and Meiyu Duan},
  doi          = {10.1049/iet-cvi.2019.0508},
  journal      = {IET Computer Vision},
  month        = {8},
  number       = {5},
  pages        = {224-232},
  shortjournal = {IET Comput. Vis.},
  title        = {RTL3D: Real-time LIDAR-based 3D object detection with sparse CNN},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Architecture to improve the accuracy of automatic image
annotation systems. <em>IETCV</em>, <em>14</em>(5), 214–223. (<a
href="https://doi.org/10.1049/iet-cvi.2019.0500">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Automatic image annotation (AIA) is an image retrieval mechanism to extract relative semantic tags from visual content. So far, the improvement of accuracy in newly developed such methods have been about 1 or 2% in the F1-score and the architectures seem to have room for improvement. Therefore, the authors designed a more detailed architecture for AIA and suggested new algorithms for its main parts. The proposed architecture has three main parts: feature extraction, learning, and annotation. They designed a novel learning method using machine learning and probability bases. In the annotation part, they suggest a novel method that gains the maximum benefit from the learning part. The combination of the proposed architecture, algorithms, and novel ideas resulted in new accuracy milestones in F1-score on most commonly used datasets. In their architecture, N + measure which shows the number of tags with non-zero recalls showed that they could recall all tags for IAPRTC-12 and ESP-Games datasets.},
  archive      = {J_IETCV},
  author       = {Artin Ghostan Khatchatoorian and Mansour Jamzad},
  doi          = {10.1049/iet-cvi.2019.0500},
  journal      = {IET Computer Vision},
  month        = {8},
  number       = {5},
  pages        = {214-223},
  shortjournal = {IET Comput. Vis.},
  title        = {Architecture to improve the accuracy of automatic image annotation systems},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Adversarial examples detection through the sensitivity in
space mappings. <em>IETCV</em>, <em>14</em>(5), 201–213. (<a
href="https://doi.org/10.1049/iet-cvi.2019.0378">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Adversarial examples (AEs) against deep neural networks (DNNs) raise wide concerns about the robustness of DNNs. Existing detection mechanisms are often limited to a given attack algorithm. Therefore, it is highly desirable to develop a robust detection approach that remains effective for a large group of attack algorithms. In addition, most of the existing defences only perform well for small images (e.g. MNIST and Canadian institute for advanced research (CIFAR)) rather than large images (e.g. ImageNet). In this paper, the authors propose a robust and effective defence method for analysing the sensitivity of various AEs, especially in a much harder case (large images). Their method first creates a feature map from the input space to the new feature space, by utilising 19 different feature mapping methods. Then, a detector is learned with the machine-learning algorithm to recognise the unique distribution of AEs. Their extensive evaluations on their proposed detector show that their detector can achieve: (i) low false-positive rate (&lt;1%), (ii) high true-positive rate (higher than 98%), (iii) low overhead (&lt;0.1 s per input), and (iv) good robustness (work well across different learning models, attack algorithms, and parameters), which demonstrate the efficacy of the proposed detector in practise.},
  archive      = {J_IETCV},
  author       = {Xurong Li and Shouling Ji and Juntao Ji and Zhenyu Ren and Chunming Wu and Bo Li and Ting Wang},
  doi          = {10.1049/iet-cvi.2019.0378},
  journal      = {IET Computer Vision},
  month        = {8},
  number       = {5},
  pages        = {201-213},
  shortjournal = {IET Comput. Vis.},
  title        = {Adversarial examples detection through the sensitivity in space mappings},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Regularising neural networks for future trajectory
prediction via inverse reinforcement learning framework. <em>IETCV</em>,
<em>14</em>(5), 192–200. (<a
href="https://doi.org/10.1049/iet-cvi.2019.0546">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Predicting distant future trajectories of agents in a dynamic scene is challenging because the future trajectory of an agent is affected not only by their past trajectory but also the scene contexts. To tackle this problem, the authors propose a model based on recurrent neural networks, and a novel method for training this model. The proposed model is based on an encoder–decoder architecture where the encoder encodes inputs (past trajectory and scene context information), while the decoder produces a future trajectory from the context vector given by the encoder. To make the proposed model better utilise the scene context information, the authors let the encoder predict the positions in the past trajectory and a reward function evaluate the positions along with the scene context information generated by the positions. The reward function, which is simultaneously trained with the proposed model, plays the role of a regulariser for the model during the simultaneous training. The authors evaluate the proposed model on several public benchmark datasets. The experimental results show that the prediction performance of the proposed model is greatly improved by the proposed regularisation method, which outperforms the-state-of-the-art models in terms of accuracy.},
  archive      = {J_IETCV},
  author       = {Dooseop Choi and Kyoungwook Min and Jeongdan Choi},
  doi          = {10.1049/iet-cvi.2019.0546},
  journal      = {IET Computer Vision},
  month        = {8},
  number       = {5},
  pages        = {192-200},
  shortjournal = {IET Comput. Vis.},
  title        = {Regularising neural networks for future trajectory prediction via inverse reinforcement learning framework},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Efficient complex ISAR object recognition using adaptive
deep relation learning. <em>IETCV</em>, <em>14</em>(5), 185–191. (<a
href="https://doi.org/10.1049/iet-cvi.2019.0200">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Complex inverse synthetic aperture radar (ISAR) object recognition is a critical and challenging problem in computer vision tasks. An efficient complex object recognition method for ISAR images is proposed based on adaptive deep relation learning. (i) An adaptive multimodal mechanism is proposed to greatly improve the multimodal sampling and transformation capabilities of convolutional neural networks and increase the resolutions of the feature maps. In particular, an adaptive ranking and selection strategy for related regions are proposed. (ii) Deep graphical learning is proposed to jointly estimate large numbers of heterogeneous attributes and leverage the relations among features and attributes. Extensive experiments performed on real-world datasets show that the proposed method outperforms other state-of-the-art methods.},
  archive      = {J_IETCV},
  author       = {Chunsheng Liu and Zhongmei Wang},
  doi          = {10.1049/iet-cvi.2019.0200},
  journal      = {IET Computer Vision},
  month        = {8},
  number       = {5},
  pages        = {185-191},
  shortjournal = {IET Comput. Vis.},
  title        = {Efficient complex ISAR object recognition using adaptive deep relation learning},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Skeleton-based attention-aware spatial–temporal model for
action detection and recognition. <em>IETCV</em>, <em>14</em>(5),
177–184. (<a href="https://doi.org/10.1049/iet-cvi.2019.0751">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Action detection and recognition are popular subjects of research in the field of computer vision. The task of action detection can be regarded as the sum of action location and recognition. Action features described by using information concerning the human skeleton have the advantages of robustness against external factors and requiring a small amount of calculation. This study proposes a skeleton-based action analysis model based on a recurrent neural network framework. The model learns action features by modelling static and dynamic features of skeleton joints and the importance of different video frames by introducing an attention module. For action location, conditional random field loss function is introduced to establish the context dependency of output labels. In the aspect of action recognition, the hierarchical training mechanism with triple loss models action features at coarse-grained and fine-grained levels. The authors’ proposed method delivers state-of-the-art results on action location and recognition tasks.},
  archive      = {J_IETCV},
  author       = {Ran Cui and Aichun Zhu and Jingran Wu and Gang Hua},
  doi          = {10.1049/iet-cvi.2019.0751},
  journal      = {IET Computer Vision},
  month        = {8},
  number       = {5},
  pages        = {177-184},
  shortjournal = {IET Comput. Vis.},
  title        = {Skeleton-based attention-aware spatial–temporal model for action detection and recognition},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Directional dense-trajectory-based patterns for dynamic
texture recognition. <em>IETCV</em>, <em>14</em>(4), 162–176. (<a
href="https://doi.org/10.1049/iet-cvi.2019.0455">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Representation of dynamic textures (DTs), well-known as a sequence of moving textures, is a challenging problem in video analysis due to the disorientation of motion features. Analysing DTs to make them ‘understandable’ plays an important role in different applications of computer vision. In this study, an efficient approach for DT description is proposed by addressing the following novel concepts. First, the beneficial properties of dense trajectories are exploited for the first time to efficiently describe DTs instead of the whole video. Second, two substantial extensions of local vector pattern operator are introduced to form a completed model which is based on complemented components to enhance its performance in encoding directional features of motion points in a trajectory. Finally, the authors present a new framework, called directional dense trajectory patterns, which takes advantage of directional beams of dense trajectories along with spatio-temporal features of their motion points in order to construct dense-trajectory-based descriptors with more robustness. Evaluations of DT recognition on different benchmark datasets (i.e. UCLA, DynTex, and DynTex++) have verified the interest of the authors’ proposal.},
  archive      = {J_IETCV},
  author       = {Thanh Tuan Nguyen and Thanh Phuong Nguyen and Frédéric Bouchara},
  doi          = {10.1049/iet-cvi.2019.0455},
  journal      = {IET Computer Vision},
  month        = {6},
  number       = {4},
  pages        = {162-176},
  shortjournal = {IET Comput. Vis.},
  title        = {Directional dense-trajectory-based patterns for dynamic texture recognition},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). SGHs for 3D local surface description. <em>IETCV</em>,
<em>14</em>(4), 154–161. (<a
href="https://doi.org/10.1049/iet-cvi.2019.0601">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study proposes a distinctive and robust spatial and geometric histograms (SGHs) feature descriptor for three-dimensional (3D) local surface description. The authors also introduce a new local reference frame for the generation of their SGH descriptor. To fully describe a local surface, the SGH descriptor considers both spatial distribution and geometrical characteristics in its underlying support region. To encode neighbourhood information, the SGH descriptor is constructed using histogram statistics with spatial partition and interpolation strategies. The performance of the SGH descriptor was rigorously tested on six public datasets for applications of both 3D object recognition and registration. Compared to eight state-of-the-art descriptors, experimental results show that SGH achieves the best performance on noise-free data. It also produces the best results even under different nuisances. The promising descriptiveness and robustness of their SGH descriptor have been fully demonstrated.},
  archive      = {J_IETCV},
  author       = {Sheng Ao and Yulan Guo and Shangtai Gu and Jindong Tian and Dong Li},
  doi          = {10.1049/iet-cvi.2019.0601},
  journal      = {IET Computer Vision},
  month        = {6},
  number       = {4},
  pages        = {154-161},
  shortjournal = {IET Comput. Vis.},
  title        = {SGHs for 3D local surface description},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Local descriptor for retinal fundus image registration.
<em>IETCV</em>, <em>14</em>(4), 144–153. (<a
href="https://doi.org/10.1049/iet-cvi.2019.0623">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A feature-based retinal image registration (RIR) technique aligns multiple fundus images and composed of pre-processing, feature point extraction, feature descriptor, matching and geometrical transformation. Challenges in RIR include difference in scaling, intensity and rotation between images. The scale and intensity differences can be minimised with consistent imaging setup and image enhancement during the pre-processing, respectively. The rotation can be addressed with feature descriptor method that robust to varying rotation. Therefore, a feature descriptor method is proposed based on statistical properties (FiSP) to describe the circular region surrounding the feature point. From the experiments on public Fundus Image Registration dataset, FiSP established 99.227% average correct matches for rotations between 0° and 180°. Then, FiSP is paired with Harris corner, scale-invariant feature transform (SIFT), speeded-up robust feature (SURF), Ghassabi&#39;s and D-Saddle feature point extraction methods to assess its registration performance and compare with the existing feature-based RIR techniques, namely generalised dual-bootstrap iterative closet point (GDB-ICP), Harris-partial intensity invariant feature descriptor (PIIFD), Ghassabi&#39;s–SIFT, H-M 16, H-M 17 and D-Saddle–histogram of oriented gradients (HOG). The combination of SIFT–FiSP registered 64.179% of the image pairs and significantly outperformed other techniques with mean difference between 25.373 and 60.448% ( p = &lt;0.001*).},
  archive      = {J_IETCV},
  author       = {Roziana Ramli and Mohd Yamani Idna Idris and Khairunnisa Hasikin and Noor Khairiah A. Karim and Ainuddin Wahid Abdul Wahab and Ismail Ahmedy and Fatimah Ahmedy and Hamzah Arof},
  doi          = {10.1049/iet-cvi.2019.0623},
  journal      = {IET Computer Vision},
  month        = {6},
  number       = {4},
  pages        = {144-153},
  shortjournal = {IET Comput. Vis.},
  title        = {Local descriptor for retinal fundus image registration},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). RootsGLOH2: Embedding RootSIFT “square rooting” in sGLOH2.
<em>IETCV</em>, <em>14</em>(4), 138–143. (<a
href="https://doi.org/10.1049/iet-cvi.2019.0716">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study introduces an extension of the sGLOH2 local image descriptor inspired by RootSIFT ‘square rooting’ as a way to indirectly alter the matching distance used to compare the descriptor vectors. The extended descriptor, named RootsGLOH2, achieved the best results in terms of matching accuracy and robustness among the latest state-of-the-art non-deep descriptors in recent evaluation contests dealing with both planar and non-planar scenes. RootsGLOH2 also achieves a matching accuracy very close to that obtained by the best deep descriptors to date. Beside confirming that ‘square rooting’ has beneficial effects on sGLOH2 as it happens on SIFT, experimental evidence shows that classical norm-based distances, such as the Euclidean and Manhattan distances, only provide suboptimal solutions to the problem of local image descriptor matching. This suggests matching distance design as a topic to investigate further in the near future.},
  archive      = {J_IETCV},
  author       = {Fabio Bellavia and Carlo Colombo},
  doi          = {10.1049/iet-cvi.2019.0716},
  journal      = {IET Computer Vision},
  month        = {6},
  number       = {4},
  pages        = {138-143},
  shortjournal = {IET Comput. Vis.},
  title        = {RootsGLOH2: Embedding RootSIFT ‘square rooting’ in sGLOH2},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). SRP-AKAZE: An improved accelerated KAZE algorithm based on
sparse random projection. <em>IETCV</em>, <em>14</em>(4), 131–137. (<a
href="https://doi.org/10.1049/iet-cvi.2019.0622">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The AKAZE algorithm is a typical image registration algorithm that has the advantage of high computational efficiency based on non-linear diffusion. However, it is weaker than the scale-invariant feature transformation (SIFT) algorithm in terms of robustness and stability. We propose a new and improved version of the AKAZE algorithm by using the SIFT descriptor based on sparse random projection (SRP). The proposed method not only retains the advantage of high efficiency of the AKAZE algorithm in feature detection but also has the stability of the SIFT descriptor. Moreover, the computational complexity due to the high dimension of the SIFT descriptor, which limits the speed of feature matching, is drastically reduced by the SRP strategy. Experiments on several benchmark image datasets demonstrate that the proposed algorithm can significantly improve the stability of the AKAZE algorithm, and the results suggest the better matching performance and robustness of the feature descriptor.},
  archive      = {J_IETCV},
  author       = {Dan Li and Qiannan Xu and Wennian Yu and Bing Wang},
  doi          = {10.1049/iet-cvi.2019.0622},
  journal      = {IET Computer Vision},
  month        = {6},
  number       = {4},
  pages        = {131-137},
  shortjournal = {IET Comput. Vis.},
  title        = {SRP-AKAZE: An improved accelerated KAZE algorithm based on sparse random projection},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Locally lateral manifolds of normalised gabor features for
face recognition. <em>IETCV</em>, <em>14</em>(4), 122–130. (<a
href="https://doi.org/10.1049/iet-cvi.2019.0531">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to inherent characteristics of multiscale and orientation, normalised Gabor features have been successfully used in face recognition. Various previous works have showcased the strength and feasibility of this approach, especially on its robustness against local variations. However, the projected features are numerous and substantial in dimension, which is largely due to the convolution of multiscale and orientation of wavelets. Such features, when used in practical face recognition, would require relatively lengthy classification process, particularly when it involves computationally extensive local classifier or experts, such as ensembles of local cosine similarity (ELCS) classifier. The authors address this issue by simultaneously reducing the size of Gabor features laterally and locally using a manifold learning method called locally linear embedding (LLE). This method is thus denoted as locally lateral normalised local Gabor feature vector with LLE (LGFV/LN/LLE). Results on several publicly available face datasets reveal the superiority of the authors’ approach in terms of improvements in feature compression of LGFV features by up to a reduction of 95% of total dimensionality while increasing the average classification accuracy by 26%. Altogether, the authors show that their LGFV/LN/LLE augmented by ELCS classifiers delivers equivalent result when compared against the state-of-the-art.},
  archive      = {J_IETCV},
  author       = {Fadhlan Hafizhelmi Kamaru Zaman},
  doi          = {10.1049/iet-cvi.2019.0531},
  journal      = {IET Computer Vision},
  month        = {6},
  number       = {4},
  pages        = {122-130},
  shortjournal = {IET Comput. Vis.},
  title        = {Locally lateral manifolds of normalised gabor features for face recognition},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Fusion of visual salience maps for object acquisition.
<em>IETCV</em>, <em>14</em>(4), 113–121. (<a
href="https://doi.org/10.1049/iet-cvi.2019.0624">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The paradigm of visual attention has been widely investigated and applied to many computer vision applications. In this study, the authors propose a new saliency-based visual attention algorithm applied to object acquisition. The proposed algorithm automatically extracts points of visual attention (PVA) in the scene, based on different feature saliency maps. Each saliency map represents a specific feature domain, such as textural, contrast, and statistical-based features. A feature selection, based on probability of detection and false alarm rate and repeatability criteria, is proposed to choose the most efficient feature combination for saliency map. Motivated by the assumption that the extracted PVA represents the most visually salient regions in the image, they suggest using the visual attention approach for object acquisition. A comparison with other well-known algorithms for point of interest detection shows that the proposed algorithm performs better. The proposed algorithm was successfully tested on synthetic, charge-coupled device (CCD), and infrared (IR) images. Evaluation of the algorithm for object acquisition, based on ground truth, is carried out using synthetic images, which contain multiple examples of objects, with various sizes and brightness levels. A high probability of correct detection (greater than 90%) with a low false alarm rate (about 20 false alarms per image) was achieved.},
  archive      = {J_IETCV},
  author       = {Shlomo Greenberg and Moshe Bensimon and Yevgeny Podeneshko and Alon Gens},
  doi          = {10.1049/iet-cvi.2019.0624},
  journal      = {IET Computer Vision},
  month        = {6},
  number       = {4},
  pages        = {113-121},
  shortjournal = {IET Comput. Vis.},
  title        = {Fusion of visual salience maps for object acquisition},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Crowd counting by the dual-branch scale-aware network with
ranking loss constraints. <em>IETCV</em>, <em>14</em>(3), 101–109. (<a
href="https://doi.org/10.1049/iet-cvi.2019.0704">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image crowd counting is a challenging problem. This study proposes a new deep learning method that estimates crowd counting for the congested scene. The proposed network is composed of two major components: the first ten layers of VGG16 are used as the backbone network, and a dual-branch (named as Branch_S and Branch_D) network is proposed to be the second part of the network. Branch_S extracts low-level information (head blob) through a shallow fully convolutional network and Branch_D uses a deep fully convolutional network to extract high-level context features (faces and body). Features learnt from the two different branches can handle the problem of scale variation due to perspective effects and image size differences. Features of different scales extracted from the two branches are fused to generate predicted density map. On the basis of the fact that an original graph must contain more or equal number of persons than any of its sub-images, a ranking loss function utilising the constraint relationship inside an image is proposed. Moreover, the ranking loss is combined with Euclidean loss as the final loss function. Our approach is evaluated on three benchmark datasets, and better results are achieved compared with the state-of-the-art works.},
  archive      = {J_IETCV},
  author       = {Qin Wu and Fangfang Yan and Zhilei Chai and Guodong Guo},
  doi          = {10.1049/iet-cvi.2019.0704},
  journal      = {IET Computer Vision},
  month        = {4},
  number       = {3},
  pages        = {101-109},
  shortjournal = {IET Comput. Vis.},
  title        = {Crowd counting by the dual-branch scale-aware network with ranking loss constraints},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Semi-supervised uncorrelated dictionary learning for colour
face recognition. <em>IETCV</em>, <em>14</em>(3), 92–100. (<a
href="https://doi.org/10.1049/iet-cvi.2019.0125">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Colour images are increasingly used in the fields of computer vision, pattern recognition and machine learning, since they can provide more identifiable information than greyscale images. Thus, colour face recognition has attracted accumulating attention. Its key problem is how to remove the similarity between colour component images and take full advantage of colour difference information. Decision-level similarity reduction between colour component images directly affects the recognition effect, but it has been found in no work. In this study, the authors propose a novel colour face recognition approach named semi-supervised uncorrelated dictionary learning (SUDL), which realises decision-level similarity reduction and fusion of all colour components in face images. SUDL employs the labelled and unlabelled colour face image samples into structured dictionary learning to achieve three uncorrelated discriminating dictionaries corresponding to three colour components of face images, and then uses these dictionaries and the sparse coding technique to make a classification decision. Experimental results in multiple public colour face image databases demonstrate that the dictionary decorrelation, structured dictionary learning and unlabelled samples used in the proposed approach are effective and reasonable, and the proposed approach outperforms several representative colour face recognition methods in recognition rates, despite of its poor time performance.},
  archive      = {J_IETCV},
  author       = {Qian Liu and Bo Jiang and Jia-lei Zhang and Peng Gao and Zhi-jian Xia},
  doi          = {10.1049/iet-cvi.2019.0125},
  journal      = {IET Computer Vision},
  month        = {4},
  number       = {3},
  pages        = {92-100},
  shortjournal = {IET Comput. Vis.},
  title        = {Semi-supervised uncorrelated dictionary learning for colour face recognition},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). 3D driver pose estimation based on joint 2D–3D network.
<em>IETCV</em>, <em>14</em>(3), 84–91. (<a
href="https://doi.org/10.1049/iet-cvi.2019.0089">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Three-dimensional (3D) driver pose estimation is a promising and challenging problem for computer–human interaction. Recently convolutional neural networks have been introduced into 3D pose estimation, but these methods have the problem of slow running speed and are not suitable for driving scenario. In this study, the proposed method is based on two types of inputs, infrared image and point cloud obtained from time-of-flight camera. The authors propose a joint 2D–3D network incorporating image-based and point-based feature to promote the performance of 3D human pose estimation and run on a high speed. For point cloud with invalid points, the authors first do preprocess and then design a denoising module to handle this problem. Experiments on private driver data set and public Invariant-Top View data set show that the proposed method achieves efficient and competitive performance on 3D human pose estimation.},
  archive      = {J_IETCV},
  author       = {Zhijie Yao and Yazhou Liu and Zexuan Ji and Quansen Sun and Pongsak Lasang and Shengmei Shen},
  doi          = {10.1049/iet-cvi.2019.0089},
  journal      = {IET Computer Vision},
  month        = {4},
  number       = {3},
  pages        = {84-91},
  shortjournal = {IET Comput. Vis.},
  title        = {3D driver pose estimation based on joint 2D–3D network},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Self-adaptive weighted synthesised local directional pattern
integrating with sparse autoencoder for expression recognition based on
improved multiple kernel learning strategy. <em>IETCV</em>,
<em>14</em>(3), 73–83. (<a
href="https://doi.org/10.1049/iet-cvi.2018.5127">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study presents a novel method for solving facial expression recognition (FER) tasks which uses a self-adaptive weighted synthesised local directional pattern (SW-SLDP) descriptor integrating sparse autoencoder (SA) features based on improved multiple kernel learning (IMKL) strategy. The authors’ work includes three parts. Firstly, the authors propose a novel SW-SLDP feature descriptor which divides the facial images into patches and extracts sub-block features synthetically according to both distribution information and directional intensity contrast. Then self-adaptive weights are assigned to each sub-block feature according to the projection error between the expressional image and neutral image of each patch, which can highlight such areas containing more expressional texture information. Secondly, to extract a discriminative high-level feature, they introduce SA for feature representation, which extracts the hidden layer representation including more comprehensive information. Finally, to combine the above two kinds of features, an IMKL strategy is developed by effectively integrating both soft margin learning and intrinsic local constraints, which is robust to noisy condition and thus improve the classification performance. Extensive experimental results indicate their model can achieve competitive or even better performance with existing representative FER methods.},
  archive      = {J_IETCV},
  author       = {Lingshuang Du and Yongbo Wu and Haifeng Hu and Weixuan Wang},
  doi          = {10.1049/iet-cvi.2018.5127},
  journal      = {IET Computer Vision},
  month        = {4},
  number       = {3},
  pages        = {73-83},
  shortjournal = {IET Comput. Vis.},
  title        = {Self-adaptive weighted synthesised local directional pattern integrating with sparse autoencoder for expression recognition based on improved multiple kernel learning strategy},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). ADN for object detection. <em>IETCV</em>, <em>14</em>(2),
65–72. (<a href="https://doi.org/10.1049/iet-cvi.2018.5651">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Owing to large-scale diversity and location uncertainty in object detection, how to enrich semantic information has become an important issue that attracts a lot of concern. In this study, the authors propose a novel attentional detection network (ADN) to enrich semantic information of feature maps by adding an extra attention branch to the classic detection network. Compared to previous methods (e.g. feature pyramid network (FPN), single shot multibox detector (SSD)) that producing massive anchors in different layers of feature maps to detect objects with different scales and aspect ratios, which is very time-consuming, their network is lightweight and do not need to produce extra anchors. Furthermore, ADN can be applied to different object detectors with little computational cost. Extensive experiments indicate that ADN has good detection performance on different datasets without bells and whistles.},
  archive      = {J_IETCV},
  author       = {Jinding Wang and Haifeng Hu and Xinlong Lu},
  doi          = {10.1049/iet-cvi.2018.5651},
  journal      = {IET Computer Vision},
  month        = {3},
  number       = {2},
  pages        = {65-72},
  shortjournal = {IET Comput. Vis.},
  title        = {ADN for object detection},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Scale specified single shot multibox detector.
<em>IETCV</em>, <em>14</em>(2), 59–64. (<a
href="https://doi.org/10.1049/iet-cvi.2019.0461">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Detecting objects at vastly different scales is a fundamental challenge in computer vision. To solve this, some approaches (e.g. TridentNet) investigate the effect of receptive fields, whereas other approaches (e.g. SNIP, SNIPER) are based on the image pyramid strategy. In this study, a novel single-shot based detector, called scale specified single-shot multibox detector (4SD) is proposed. It aims to predict objects of a specific scale range separately by using feature maps of different sizes. First, a parallel multi-branch architecture with feature maps of different sizes is generated by scale specific inference module. Then, the authors propose a scale specific training scheme to specialise each branch by sampling object instances of proper scales for training. Results are shown on both PASCAL VOC and COCO detection. The proposed method can achieve a mean average precision of 83.1% on PASCAL VOC 2007, and 36.9% on MS-COCO at a speed of 28 frames per second, which is superior to most single-stage detectors.},
  archive      = {J_IETCV},
  author       = {Xiaoqiang Li and Chuanwei Liu and Songmin Dai and Huichen Lian and Guangtai Ding},
  doi          = {10.1049/iet-cvi.2019.0461},
  journal      = {IET Computer Vision},
  month        = {3},
  number       = {2},
  pages        = {59-64},
  shortjournal = {IET Comput. Vis.},
  title        = {Scale specified single shot multibox detector},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Optimisation for image salient object detection based on
semantic-aware clustering and CRF. <em>IETCV</em>, <em>14</em>(2),
49–58. (<a href="https://doi.org/10.1049/iet-cvi.2019.0063">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {State-of-the-art optimisation methods for salient object detection neglect that saliency maps of different images usually show different imperfections. Therefore, the saliency maps of some images cannot achieve effective optimisation. Based on the observation that the saliency maps of semantically similar images usually show similar imperfections, the authors propose an optimisation method for salient object detection based on semantic-aware clustering and conditional random field (CRF), named CCRF. They first cluster the training images into some clusters using the image semantic features extracted by using a deep convolutional neural network model for image classification. Then for each cluster, they use a CRF to optimise the saliency maps generated by existing salient object detection methods. A grid search method is used to compute the optimal weights of the kernels of the CRF. The saliency maps of the testing images are optimised by the corresponding CRFs with the optimal weights. The experimental results with 13 typical salient object detection methods on four datasets show that the proposed CCRF algorithm can effectively improve the results of a variety of image salient object detection methods and outperforms the compared optimisation methods.},
  archive      = {J_IETCV},
  author       = {Junhao Chen and Yuzhe Niu and Jianbin Wu and Junrong Chen},
  doi          = {10.1049/iet-cvi.2019.0063},
  journal      = {IET Computer Vision},
  month        = {3},
  number       = {2},
  pages        = {49-58},
  shortjournal = {IET Comput. Vis.},
  title        = {Optimisation for image salient object detection based on semantic-aware clustering and CRF},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Multiscale bilateral filtering to detect 3D interest points.
<em>IETCV</em>, <em>14</em>(1), 36–47. (<a
href="https://doi.org/10.1049/iet-cvi.2018.5405">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The detection of 3D interest points is a central problem in computer graphics, computer vision, and pattern recognition. It is also an important preprocessing step in the analysis of 3D model matching. Although studied for decades, detecting 3D interest points remains a challenge. In this study, a novel multiscale bilateral filtering method is presented to detect 3D interest points. This method first simplifies repeatedly the input 3D mesh to form k multiresolution meshes. For each mesh, on the basis of the computed saliency of the mesh vertex, the bilateral filtering is used to remove the noise of the mesh saliencies and the global contrast to normalise the saliencies, and then the interest points are extracted on the basis of the normalised saliency. The proposed method then gathers and clusters all interest points detected on the k multiresolution meshes, and the centres of these clusters are treated as the final interest points. In this method, both the spatial closeness and the geometric similarities of the mesh vertices are considered during the bilateral filtering process. The experimental results validate the effectiveness of the proposed method to detect 3D interest points. This method is also tested the potential to distinguish 3D models.},
  archive      = {J_IETCV},
  author       = {Han Guo and Dongmei Niu and Mingxuan Zhang and Xiuyang Zhao and Bo Yang and Caiming Zhang},
  doi          = {10.1049/iet-cvi.2018.5405},
  journal      = {IET Computer Vision},
  month        = {2},
  number       = {1},
  pages        = {36-47},
  shortjournal = {IET Comput. Vis.},
  title        = {Multiscale bilateral filtering to detect 3D interest points},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Incremental transfer learning for video annotation via
grouped heterogeneous sources. <em>IETCV</em>, <em>14</em>(1), 26–35.
(<a href="https://doi.org/10.1049/iet-cvi.2018.5730">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Here, the authors focus on incrementally acquiring heterogeneous knowledge from both internet and publicly available datasets to reduce the tedious and expensive labelling efforts required in video annotation. An incremental transfer learning framework is presented to integrate heterogeneous source knowledge and update the annotation model incrementally during the transfer learning process. Under this framework, web images and existing action videos form the source domain to provide labelled static and motion information of the target domain videos, respectively. Moreover, according to the semantic of the source domain data, all the source domain data are partitioned into several groups. Different from traditional methods, which compare the entire target domain videos with each source group from the source domain, the authors treat the group weights as sample-specific variables and optimise them along with new adding data. Two regularisers are used to prevent the incremental learning process from negative transfer. Experimental results on the two large-scale consumer video datasets (i.e. multimedia event detection (MED) and Columbia consumer video (CCV)) show the effectiveness of the proposed method.},
  archive      = {J_IETCV},
  author       = {Han Wang and Hao Song and Xinxiao Wu and Yunde Jia},
  doi          = {10.1049/iet-cvi.2018.5730},
  journal      = {IET Computer Vision},
  month        = {2},
  number       = {1},
  pages        = {26-35},
  shortjournal = {IET Comput. Vis.},
  title        = {Incremental transfer learning for video annotation via grouped heterogeneous sources},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Ensemble of fine-tuned convolutional neural networks for
urine sediment microscopic image classification. <em>IETCV</em>,
<em>14</em>(1), 18–25. (<a
href="https://doi.org/10.1049/iet-cvi.2018.5829">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this study, an ensemble of fine-tuned convolutional neural networks (CNNs) is proposed. As CNN training requires large annotated data, which are lacking in the field of urine sediment microscopic image processing, the authors first pre-trained the CNNs, including ResNet50 and GoogLeNet, and developed AlexNet on an ImageNet dataset. Thereafter, some of the weights of the pre-trained CNNs were transferred to the urine sediment microscopic image dataset. To guide fine-tuning of the learning rate and cascading features, the hierarchical nature of features in different convolutional layers was investigated by visualising the CNN. Then, they combined three CNNs as an ensemble of CNNs to decrease the differences and impurity interference among features of urine sediment microscopic image. These fusion features were employed to train the fully connected neural network for classification. In this study, they improved the accuracy of each CNN by an average of 2.2% through fine-tuning of the learning rate and cascading features. Moreover, the better experimental results were achieved compared with other state-of-the-art methods and indicated that a 97% classification accuracy can be attained.},
  archive      = {J_IETCV},
  author       = {Wenqian Liu and Weihong Li and Weiguo Gong},
  doi          = {10.1049/iet-cvi.2018.5829},
  journal      = {IET Computer Vision},
  month        = {2},
  number       = {1},
  pages        = {18-25},
  shortjournal = {IET Comput. Vis.},
  title        = {Ensemble of fine-tuned convolutional neural networks for urine sediment microscopic image classification},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Automatic drug pills detection based on enhanced feature
pyramid network and convolution neural networks. <em>IETCV</em>,
<em>14</em>(1), 9–17. (<a
href="https://doi.org/10.1049/iet-cvi.2019.0171">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Drug pill detection is one of the most important tasks in medication safety. The correct identification of drug based on the visual appearance is a key step towards the improvement of medication safety. Previous studies have aimed to recognise a drug based on the front or back view of the drug under a fixed viewing angle. In cases with multiple drugs and randomly placed drugs, the previous methods have difficulties in detecting and recognising different drugs in practical applications. A convolution neural network-based detector is proposed in this work to overcome the difficulties and to assist patients in drug identification. The proposed system includes a localisation stage and a classification stage. The enhanced feature pyramid network (EFPN), is proposed for drug localisation, and Inception-ResNet v2 is used in drug classification. The proposed Drug Pills Image Database contains a collection of 612 categories of drug datasets for deep learning research in the pharmaceutical field. The proposed EFPN achieves over 96% accuracy in the localisation experiment. In the complete system evaluation, the proposed system has obtained the Top-1, Top-3, and Top-5 accuracies of 82.1, 92.4, and 94.7%, respectively.},
  archive      = {J_IETCV},
  author       = {Yang-Yen Ou and An-Chao Tsai and Xuan-Ping Zhou and Jhing-Fa Wang},
  doi          = {10.1049/iet-cvi.2019.0171},
  journal      = {IET Computer Vision},
  month        = {2},
  number       = {1},
  pages        = {9-17},
  shortjournal = {IET Comput. Vis.},
  title        = {Automatic drug pills detection based on enhanced feature pyramid network and convolution neural networks},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Automated diabetic retinopathy grading and lesion detection
based on the modified r-FCN object-detection algorithm. <em>IETCV</em>,
<em>14</em>(1), 1–8. (<a
href="https://doi.org/10.1049/iet-cvi.2018.5508">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this work, we develop a computer-aided retinal image screening system that can perform automated diabetic retinopathy (DR) grading and DR lesion detection in retinal fundus images. We propose a modified object-detection method for this task via a region-based fully convolutional network (R-FCN). A feature pyramid network and a modified region proposal network are applied to enhance the detection of small objects. The DR-grading model based on the modified R-FCN is evaluated on the Messidor data set and images provided by the Shanghai Eye Hospital. High sensitivity of 99.39% and specificity of 99.93% are obtained on the hospital data. Moreover, high sensitivity of 92.59% and specificity of 96.20% are obtained on the Messidor data set. The modified R-FCN lesion-detection model is validated on the hospital data set and achieves a 92.15% mean average precision. The proposed R-FCN can efficiently accomplish DR grading and lesion detection with high accuracy.},
  archive      = {J_IETCV},
  author       = {Jialiang Wang and Jianxu Luo and Bin Liu and Rui Feng and Lina Lu and Haidong Zou},
  doi          = {10.1049/iet-cvi.2018.5508},
  journal      = {IET Computer Vision},
  month        = {2},
  number       = {1},
  pages        = {1-8},
  shortjournal = {IET Comput. Vis.},
  title        = {Automated diabetic retinopathy grading and lesion detection based on the modified R-FCN object-detection algorithm},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
</ul>

</body>
</html>
