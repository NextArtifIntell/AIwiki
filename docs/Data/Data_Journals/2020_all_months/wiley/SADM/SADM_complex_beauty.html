<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>SADM_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="sadm---38">SADM - 38</h2>
<ul>
<li><details>
<summary>
(2020). Two-stage hybrid learning techniques for bankruptcy
prediction*. <em>Statistical Analysis and Data Mining: The ASA Data
Science Journal</em>, <em>13</em>(6), 565–572. (<a
href="https://doi.org/10.1002/sam.11482">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Many machine learning-based techniques have been used for the prediction of bankruptcy. They can be divided into single, ensemble, and hybrid learning techniques. This paper focuses on a two-stage hybrid learning approach for bankruptcy prediction where, in the first stage, a clustering algorithm is used to perform the instance selection task in order to filter out a certain number of unrepresentative training data. The clustering results output from the first stage are used with a classification algorithm to construct the prediction model. The results of experiments based on five different country datasets show that the best support vector machine (SVM) classifier performance is obtained using instance selection by affinity propagation (AP) and k-means individually. Moreover, we also find that although the best AP/k-means and SVM combination is dataset dependent, the criteria for selecting representative training data are specific. This should become a guideline for developing bankruptcy prediction systems based on the hybrid learning approach.},
  archive  = {J},
  author   = {Chih-Fong Tsai},
  doi      = {10.1002/sam.11482},
  journal  = {Statistical Analysis and Data Mining: The ASA Data Science Journal},
  month    = {12},
  number   = {6},
  pages    = {565-572},
  title    = {Two-stage hybrid learning techniques for bankruptcy prediction*},
  volume   = {13},
  year     = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Clover plot: Versatile visualization in nonparametric
classification*. <em>Statistical Analysis and Data Mining: The ASA Data
Science Journal</em>, <em>13</em>(6), 548–564. (<a
href="https://doi.org/10.1002/sam.11481">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {A versatile visualization tool that aggregates several classification techniques is proposed and studied. In the clover plot we combine four complementary classifiers—the depth-depth plot, the bagdistance plot, an approach based on the illumination, and the classical diagnostic plot based on Mahalanobis distances. An interactive R implementation of this tool, including several model examples of its use, is freely available online.},
  archive  = {J},
  author   = {Jiří Dvořák and Šárka Hudecová and Stanislav Nagy},
  doi      = {10.1002/sam.11481},
  journal  = {Statistical Analysis and Data Mining: The ASA Data Science Journal},
  month    = {12},
  number   = {6},
  pages    = {548-564},
  title    = {Clover plot: Versatile visualization in nonparametric classification*},
  volume   = {13},
  year     = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). The next wave: We will all be data scientists.
<em>Statistical Analysis and Data Mining: The ASA Data Science
Journal</em>, <em>13</em>(6), 544–547. (<a
href="https://doi.org/10.1002/sam.11476">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {In the next wave of educating future data scientists, we need to think of all undergraduate students, regardless of background or major, as future data scientists. We should train them in supportive, interdisciplinary environments. Starting from their first day at college, they should be given the opportunity to apply powerful tools to large data sets, using real-world problems. Partnerships with research computing, academic departments, research centers, companies, government, and nonprofits will all be necessary to fully prepare these students for the breadth of the data science workforce.},
  archive  = {J},
  author   = {Margaret Betz and Ellen Gundlach and Elizabett Hillery and Jenna Rickus and Mark D. Ward},
  doi      = {10.1002/sam.11476},
  journal  = {Statistical Analysis and Data Mining: The ASA Data Science Journal},
  month    = {12},
  number   = {6},
  pages    = {544-547},
  title    = {The next wave: We will all be data scientists},
  volume   = {13},
  year     = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). The future of precision health is data-driven decision
support. <em>Statistical Analysis and Data Mining: The ASA Data Science
Journal</em>, <em>13</em>(6), 537–543. (<a
href="https://doi.org/10.1002/sam.11475">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {In the applied sciences, the ultimate goal is not just to acquire knowledge but to turn knowledge into action. The next wave for data disciplines may be experimental designs and analytical methods for closing the gap between the “real-world” situations faced by decision-makers and their idealized representations in optimization problems, and the health sciences are poised to be the discipline where these developments substantially improve lives. We discuss three recent trends in research—experimental designs and analytical methods for precision medicine and pragmatic trials; technological developments in sensors, wearables, and smartphones for measuring health data; and methods addressing algorithmic bias and model interpretability—and argue that these seemingly disparate trends point to a future where data-driven decision support tools are increasingly used to promote wellbeing.},
  archive  = {J},
  author   = {John Sperger and Nikki L. B. Freeman and Xiaotong Jiang and David Bang and Daniel de Marchi and Michael R. Kosorok},
  doi      = {10.1002/sam.11475},
  journal  = {Statistical Analysis and Data Mining: The ASA Data Science Journal},
  month    = {12},
  number   = {6},
  pages    = {537-543},
  title    = {The future of precision health is data-driven decision support},
  volume   = {13},
  year     = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Data-driven dimension reduction in functional principal
component analysis identifying the change-point in functional data.
<em>Statistical Analysis and Data Mining: The ASA Data Science
Journal</em>, <em>13</em>(6), 529–536. (<a
href="https://doi.org/10.1002/sam.11471">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Functional principal component analysis (FPCA) is the most commonly used technique to analyze infinite-dimensional functional data in finite lower-dimensional space for the ease of computational intensity. However, the power of a test detecting the existence of a change-point falls with the inclusion of more principal dimensions explaining a larger proportion of variability. We propose a new methodology for dynamically selecting the dimensions in FPCA that are used further for the testing of the existence of any change-point in the given data. This data-driven and efficient approach leads to a more powerful test than those available in the literature. We illustrate this method on the monthly global average anomaly of temperatures.},
  archive  = {J},
  author   = {Buddhananda Banerjee and Arnab K. Laha and Arjun Lakra},
  doi      = {10.1002/sam.11471},
  journal  = {Statistical Analysis and Data Mining: The ASA Data Science Journal},
  month    = {12},
  number   = {6},
  pages    = {529-536},
  title    = {Data-driven dimension reduction in functional principal component analysis identifying the change-point in functional data},
  volume   = {13},
  year     = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Multiclass machine learning classification of functional
brain images for parkinson’s disease stage prediction. <em>Statistical
Analysis and Data Mining: The ASA Data Science Journal</em>,
<em>13</em>(5), 508–523. (<a
href="https://doi.org/10.1002/sam.11480">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {We analyzed a data set containing functional brain images from 6 healthy controls and 196 individuals with Parkinson&#39;s disease (PD), who were divided into five stages according to illness severity. The goal was to predict patients&#39; PD illness stages by using their functional brain images. We employed the following prediction approaches: multivariate statistical methods (linear discriminant analysis, support vector machine, decision tree, and multilayer perceptron [MLP]), ensemble learning models (random forest [RF] and adaptive boosting), and deep convolutional neural network (CNN). For statistical and ensemble models, various feature extraction approaches (principal component analysis [PCA], multilinear PCA, intensity summary statistics [IStat], and Laws&#39; texture energy measure) were employed to extract features, the synthetic minority over-sampling technique was used to address imbalanced data, and the optimal combination of hyperparameters was found using a grid search. For CNN modeling, we applied an image augmentation technique to increase and balance data sizes over different disease stages. We adopted transfer learning to incorporate pretrained VGG16 weights and architecture into the model fitting, and we also tested a state-of-the-art machine learning model that could automatically generate an optimal neural architecture. We found that IStat consistently outperformed other feature extraction approaches. MLP and RF were the analytic approaches with the highest prediction accuracy rate for multivariate statistical and ensemble learning models, respectively. Overall, the deep CNN model with pretrained VGG16 weights and architecture outperformed other approaches; it captured critical features from imaging, effectively distinguished between normal controls and patients with PD, and achieved the highest classification accuracy.},
  archive  = {J},
  author   = {Guan-Hua Huang and Chih-Hsuan Lin and Yu-Ren Cai and Tai-Been Chen and Shih-Yen Hsu and Nan-Han Lu and Huei-Yung Chen and Yi-Chen Wu},
  doi      = {10.1002/sam.11480},
  journal  = {Statistical Analysis and Data Mining: The ASA Data Science Journal},
  month    = {10},
  number   = {5},
  pages    = {508-523},
  title    = {Multiclass machine learning classification of functional brain images for parkinson&#39;s disease stage prediction},
  volume   = {13},
  year     = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Multivariate hidden markov models for disease progression.
<em>Statistical Analysis and Data Mining: The ASA Data Science
Journal</em>, <em>13</em>(5), 499–507. (<a
href="https://doi.org/10.1002/sam.11479">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Disease progression models are a powerful tool for understanding the development of a disease, given some clinical measurements obtained from longitudinal events related to a sample of patients. These models are able to give some insights about the disease progression through the analysis of patients histories and can be also used to predict the future course of the disease for an individual. In particular, Hidden Markov Models are suitable for disease progression since they model the latent unobservable states of the disease. In this work, we propose a HMM where the outcome is multivariate and its components are not independent; to accomplish our aim, since we do not make any usual normality assumptions, we model the outcome using copulas. We first test the performance of our model in a simulation setting and show the validity of the method. Then, we study the course of Heart Failure, applying our model to an administrative dataset from Lombardia Region in Italy, showing how episodes of hospitalization can give information about the disease status of a patient.},
  archive  = {J},
  author   = {Andrea Martino and Giuseppina Guatteri and Anna Maria Paganoni},
  doi      = {10.1002/sam.11479},
  journal  = {Statistical Analysis and Data Mining: The ASA Data Science Journal},
  month    = {10},
  number   = {5},
  pages    = {499-507},
  title    = {Multivariate hidden markov models for disease progression},
  volume   = {13},
  year     = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Use of data mining in a two-step process of profiling
student preferences in relation to the enhancement of english as a
foreign language teaching. <em>Statistical Analysis and Data Mining: The
ASA Data Science Journal</em>, <em>13</em>(5), 482–498. (<a
href="https://doi.org/10.1002/sam.11478">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {The paper pursues a twofold goal. The first goal refers to the identification of university students&#39; needs regarding such modifications to English language courses that would improve English as a foreign language (EFL) teaching outcomes. The other goal refers to the methodical issue of achieving the first one. In this aspect, the use of selected data mining techniques in a hierarchical way in real data processing is proposed. These are: (a) Self-Organizing Map (SOM) dataset segmentation and then (b) market basket analysis applied to the individual SOM segments. The research data were collected from the students&#39; survey concerning their opinion of the EFL teaching process; 347 students of a faculty of a technical university in Poland completed the questionnaire. The use of SOM allowed the identification of homogeneous groups of students, while market basket analysis allowed indicating, within each group, the relationships between student opinions of effective methods of teaching English. In such a way, satisfactory student preference profiles as regards their approach to the improvement of English language competences were developed. On this basis, EFL teaching methods appropriate for the specific profile can be adapted.},
  archive  = {J},
  author   = {Marzena Nowakowska and Karolina Bęben and Michał Pajęcki},
  doi      = {10.1002/sam.11478},
  journal  = {Statistical Analysis and Data Mining: The ASA Data Science Journal},
  month    = {10},
  number   = {5},
  pages    = {482-498},
  title    = {Use of data mining in a two-step process of profiling student preferences in relation to the enhancement of english as a foreign language teaching},
  volume   = {13},
  year     = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Classification of high-dimensional electroencephalography
data with location selection using structured spike-and-slab prior.
<em>Statistical Analysis and Data Mining: The ASA Data Science
Journal</em>, <em>13</em>(5), 465–481. (<a
href="https://doi.org/10.1002/sam.11477">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {With the advent of modern technologies, it is increasingly common to deal with data of large dimensions in various scientific fields of study. In this paper, we develop a Bayesian approach for the classification of multi-subject high-dimensional electroencephalography (EEG) data. In this EEG data, we have a matrix of covariates corresponding to each subject from either the alcoholic or control group. The matrix covariates have a natural spatial correlation based on the locations of the brain, and temporal correlation as the measurements are taken over time. We employ a divide and conquer strategy by building multiple local Bayesian models at each time point separately. We incorporate the spatial structure through the structured spike-and-slab prior, which has inherent variable selection properties. The temporal structure is incorporated within the prior by learning from the local model from the previous time point. We pool the information from the local models and use a weighted average to design a prediction method. We perform simulation studies to show the efficiency of our approach and demonstrate the local Bayesian modeling with a case study on EEG data.},
  archive  = {J},
  author   = {Shariq Mohammed and Dipak K. Dey and Yuping Zhang},
  doi      = {10.1002/sam.11477},
  journal  = {Statistical Analysis and Data Mining: The ASA Data Science Journal},
  month    = {10},
  number   = {5},
  pages    = {465-481},
  title    = {Classification of high-dimensional electroencephalography data with location selection using structured spike-and-slab prior},
  volume   = {13},
  year     = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). An analytical toast to wine: Using stacked generalization to
predict wine preference. <em>Statistical Analysis and Data Mining: The
ASA Data Science Journal</em>, <em>13</em>(5), 451–464. (<a
href="https://doi.org/10.1002/sam.11474">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Due to the intricacies surrounding taste profiles, one&#39;s view of good wine is subjective. Therefore, it is advantageous to provide a more objective, data-driven way to assess wine preferences. Motivated by a previous study that modeled wine preferences using machine learning algorithms, this work presents an ensemble approach to predict a wine sample&#39;s quality level given its physiochemical properties. Results show the proposed framework out-performs many sophisticated models including the one recommended by the motivational study. Moreover, the proposed framework offers a simple variable importance strategy to gain insight as to the relevance of the predictor variables and is applied to both simulated and real data. Given the predictive power of using ensembles, especially when they can be interpretable, practitioners can use the following approach to provide an accurate and inferential perspective towards demystifying wine preferences.},
  archive  = {J},
  author   = {Taylor Larkin and Denise McManus},
  doi      = {10.1002/sam.11474},
  journal  = {Statistical Analysis and Data Mining: The ASA Data Science Journal},
  month    = {10},
  number   = {5},
  pages    = {451-464},
  title    = {An analytical toast to wine: Using stacked generalization to predict wine preference},
  volume   = {13},
  year     = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Weighted linear programming discriminant analysis for
high-dimensional binary classification. <em>Statistical Analysis and
Data Mining: The ASA Data Science Journal</em>, <em>13</em>(5), 437–450.
(<a href="https://doi.org/10.1002/sam.11473">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Linear discriminant analysis (LDA) is widely used for various binary classification problems. In contrast to the LDA that estimates the precision matrix Ω and the mean difference vector δ in the classification rule separately, the linear programming discriminant (LPD) rule estimates the product Ω δ directly through a constrained ℓ 1 minimization. The LPD rule has very good classification performance on many high-dimensional binary classification problems. However, to estimate β * = Ω δ , the LPD rule uses equal weights for all the elements of β * in the constrained ℓ 1 minimization. It may not deliver the optimal estimate of β * , and therefore the estimated discriminant direction can be suboptimal. In order to obtain better estimates of β * and the discriminant direction, we can heavily penalize β j in the constrained ℓ 1 minimization if we suspect the j th feature is useless for the classification while moderately penalize β j if we suspect the j th feature is useful. In this paper, based on the LPD rule and some popular feature screening methods, we propose a new weighted linear programming discriminant (WLPD) rule for the high-dimensional binary classification problem. The screening statistics used in the marginal two-sample t -test screening, Kolmogorov–Smirnov filter, and the maximum marginal likelihood screening will be used to construct appropriate weights for different elements of β * flexibly. Besides the linear programming algorithm, we develop a new alternating direction method of multipliers algorithm to solve the high-dimensional constrained ℓ 1 minimization problem efficiently. Our numerical studies show that our proposed WLPD rule can outperform LPD and serve as an effective binary classification tool.},
  archive  = {J},
  author   = {Yufei Wu and Guan Yu},
  doi      = {10.1002/sam.11473},
  journal  = {Statistical Analysis and Data Mining: The ASA Data Science Journal},
  month    = {10},
  number   = {5},
  pages    = {437-450},
  title    = {Weighted linear programming discriminant analysis for high-dimensional binary classification},
  volume   = {13},
  year     = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). SURE estimates for high dimensional classification.
<em>Statistical Analysis and Data Mining: The ASA Data Science
Journal</em>, <em>13</em>(5), 423–436. (<a
href="https://doi.org/10.1002/sam.11472">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {In the present paper, we consider the high dimensional classification problem, which has become much important in many modern statistical studies and applications. We develop new classifiers based on Fisher&#39;s linear classification rule and empirical Bayes. In particular, we propose to employ the Stein&#39;s unbiased risk estimate (SURE) to estimate the sparse or non-sparse mean difference, which could be plugged into the linear classification rules. Using simulation studies under a variety of settings, we demonstrate that our classifiers perform well especially when the features are non-sparse. We also illustrate the use of the new proposal to classification problems in some real data examples.},
  archive  = {J},
  author   = {Zhi Ji and Yang Wei and Zhouping Li},
  doi      = {10.1002/sam.11472},
  journal  = {Statistical Analysis and Data Mining: The ASA Data Science Journal},
  month    = {10},
  number   = {5},
  pages    = {423-436},
  title    = {SURE estimates for high dimensional classification},
  volume   = {13},
  year     = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). MR plot: A big data tool for distinguishing distributions.
<em>Statistical Analysis and Data Mining: The ASA Data Science
Journal</em>, <em>13</em>(4), 405–418. (<a
href="https://doi.org/10.1002/sam.11464">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Big data enables reliable estimation of continuous probability density, cumulative distribution, survival, hazard rate, and mean residual functions (MRFs). We illustrate that plot of the MRF provides the best resolution for distinguishing between distributions. At each point, the MRF gives the mean excess of the data beyond the threshold. Graph of the empirical MRF, called here the MR plot , provides an effective visualization tool. A variety of theoretical and data driven examples illustrate that MR plots of big data preserve the shape of the MRF and complex models require bigger data. The MRF is an optimal predictor of the excess of the random variable. With a suitable prior, the expected MRF gives the Bayes risk in the form of the entropy functional of the survival function, called here the survival entropy . We show that the survival entropy is dominated by the standard deviation ( SD ) and the equality between the two measures characterizes the exponential distribution. The empirical survival entropy provides a data concentration statistic which is strongly consistent, easy to compute, and less sensitive than the SD to heavy tailed data. An application uses the New York City Taxi database with millions of trip times to illustrate the MR plot as a powerful tool for distinguishing distributions.},
  archive  = {J},
  author   = {Omid M. Ardakani and Majid Asadi and Nader Ebrahimi and Ehsan S. Soofi},
  doi      = {10.1002/sam.11464},
  journal  = {Statistical Analysis and Data Mining: The ASA Data Science Journal},
  month    = {8},
  number   = {4},
  pages    = {405-418},
  title    = {MR plot: A big data tool for distinguishing distributions},
  volume   = {13},
  year     = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Weighted k-nearest neighbor based data complexity metrics
for imbalanced datasets. <em>Statistical Analysis and Data Mining: The
ASA Data Science Journal</em>, <em>13</em>(4), 394–404. (<a
href="https://doi.org/10.1002/sam.11463">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Empirical behavior of a classifier depends strongly on the characteristics of the underlying imbalanced dataset; therefore, an analysis of intrinsic data complexity would appear to be vital in order to choose classifiers suitable for particular problems. Data complexity metrics (CMs), a fairly recent proposal, identify dataset features which imply some difficulty for the classification task and identify relationships with classifier accuracy. In this paper, we introduce two CMs for imbalanced datasets, which help in explaining the factors responsible for the deterioration in classifier performance. These metrics are based on the weighted k -nearest neighbors approach. The experiments are performed in MATLAB software using 48 simulated datasets and 22 real-world datasets for different choices of neighborhood size k considered as 3, 5, 7, 9, 11. The results help to illustrate the usefulness of the proposed metrics.},
  archive  = {J},
  author   = {Deepika Singh and Anjana Gosain and Anju Saha},
  doi      = {10.1002/sam.11463},
  journal  = {Statistical Analysis and Data Mining: The ASA Data Science Journal},
  month    = {8},
  number   = {4},
  pages    = {394-404},
  title    = {Weighted k-nearest neighbor based data complexity metrics for imbalanced datasets},
  volume   = {13},
  year     = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Objective identification of local spatial structure for
material characterization. <em>Statistical Analysis and Data Mining: The
ASA Data Science Journal</em>, <em>13</em>(4), 377–393. (<a
href="https://doi.org/10.1002/sam.11462">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Objective tools for characterizing materials at the atomic level are often difficult to develop because of the size or structure of the data. Atom probe tomography (APT) is a measurement tool that maps the location and type of atoms in materials in three-dimensions (3D), producing data sets with potentially billions of observations. In this work, we present a set of spatial statistics methods developed to test the null hypotheses of no global spatial association; no local spatial association; and no local spatial cross-correlation and apply these for the first time to APT data. The empirical and modeled covariogram and Moran&#39;s I can be used to study the global structure of a spatially referenced atomic element. The local indicator of spatial association (LISA) identifies volumes where high levels of values (hot spots) or low levels of values (cold spots) of elemental clustering exist. The local indicator of spatial cross-correlation (LISC) reports where simultaneously high levels or low levels of two atomic elements occur. For each test statistic at each location, an associated p -value is produced that can be used to weigh the evidence in favor of spatial clustering. The size of APT data sets presents some challenges, so the effect of weight functions and neighborhood selection on the computation and significance of the test statistics are discussed, and the issue of multiple statistical testing is also considered. These methods are illustrated using an APT data set with atomic percentages reported in voxels binned to 1 nm 3 .},
  archive  = {J},
  author   = {Youjiao Yu and Brian P. Gorman and Amanda S. Hering},
  doi      = {10.1002/sam.11462},
  journal  = {Statistical Analysis and Data Mining: The ASA Data Science Journal},
  month    = {8},
  number   = {4},
  pages    = {377-393},
  title    = {Objective identification of local spatial structure for material characterization},
  volume   = {13},
  year     = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). GRATIS: GeneRAting TIme series with diverse and controllable
characteristics. <em>Statistical Analysis and Data Mining: The ASA Data
Science Journal</em>, <em>13</em>(4), 354–376. (<a
href="https://doi.org/10.1002/sam.11461">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {The explosion of time series data in recent years has brought a flourish of new time series analysis methods, for forecasting, clustering, classification and other tasks. The evaluation of these new methods requires either collecting or simulating a diverse set of time series benchmarking data to enable reliable comparisons against alternative approaches. We propose GeneRAting TIme Series with diverse and controllable characteristics, named GRATIS, with the use of mixture autoregressive (MAR) models. We simulate sets of time series using MAR models and investigate the diversity and coverage of the generated time series in a time series feature space. By tuning the parameters of the MAR models, GRATIS is also able to efficiently generate new time series with controllable features. In general, as a costless surrogate to the traditional data collection approach, GRATIS can be used as an evaluation tool for tasks such as time series forecasting and classification. We illustrate the usefulness of our time series generation process through a time series forecasting application.},
  archive  = {J},
  author   = {Yanfei Kang and Rob J. Hyndman and Feng Li},
  doi      = {10.1002/sam.11461},
  journal  = {Statistical Analysis and Data Mining: The ASA Data Science Journal},
  month    = {8},
  number   = {4},
  pages    = {354-376},
  title    = {GRATIS: GeneRAting TIme series with diverse and controllable characteristics},
  volume   = {13},
  year     = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Composite quantile-based classifiers. <em>Statistical
Analysis and Data Mining: The ASA Data Science Journal</em>,
<em>13</em>(4), 337–353. (<a
href="https://doi.org/10.1002/sam.11460">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Accurate classification of high-dimensional data is important in many scientific applications. We propose a family of high-dimensional classification methods based upon a comparison of the component-wise distances of the feature vector of a sample to the within-class population quantiles. These methods are motivated by the fact that quantile classifiers based on these component-wise distances are the most powerful univariate classifiers for an optimal choice of the quantile level. A simple aggregation approach for constructing a multivariate classifier based upon these component-wise distances to the within-class quantiles is proposed. It is shown that this classifier is consistent with the asymptotically optimal classifier as the sample size increases. Our proposed classifiers result in simple piecewise-linear decision rule boundaries that can be efficiently trained. Numerical results are shown to demonstrate competitive performance for the proposed classifiers on both simulated data and a benchmark email spam application.},
  archive  = {J},
  author   = {David A. Pritchard and Yufeng Liu},
  doi      = {10.1002/sam.11460},
  journal  = {Statistical Analysis and Data Mining: The ASA Data Science Journal},
  month    = {8},
  number   = {4},
  pages    = {337-353},
  title    = {Composite quantile-based classifiers},
  volume   = {13},
  year     = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Knot selection in sparse gaussian processes with a
variational objective function. <em>Statistical Analysis and Data
Mining: The ASA Data Science Journal</em>, <em>13</em>(4), 324–336. (<a
href="https://doi.org/10.1002/sam.11459">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Sparse, knot-based Gaussian processes have enjoyed considerable success as scalable approximations of full Gaussian processes. Certain sparse models can be derived through specific variational approximations to the true posterior, and knots can be selected to minimize the Kullback-Leibler divergence between the approximate and true posterior. While this has been a successful approach, simultaneous optimization of knots can be slow due to the number of parameters being optimized. Furthermore, there have been few proposed methods for selecting the number of knots, and no experimental results exist in the literature. We propose using a one-at-a-time knot selection algorithm based on Bayesian optimization to select the number and locations of knots. We showcase the competitive performance of this method relative to optimization of knots simultaneously on three benchmark datasets, but at a fraction of the computational cost.},
  archive  = {J},
  author   = {Nathaniel Garton and Jarad Niemi and Alicia Carriquiry},
  doi      = {10.1002/sam.11459},
  journal  = {Statistical Analysis and Data Mining: The ASA Data Science Journal},
  month    = {8},
  number   = {4},
  pages    = {324-336},
  title    = {Knot selection in sparse gaussian processes with a variational objective function},
  volume   = {13},
  year     = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Minimizing information loss in shared data: Hiding frequent
patterns with multiple sensitive support thresholds. <em>Statistical
Analysis and Data Mining: The ASA Data Science Journal</em>,
<em>13</em>(4), 309–323. (<a
href="https://doi.org/10.1002/sam.11458">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Privacy preserving data mining (PPDM) is the process of protecting sensitive knowledge from being discovered by data mining techniques in case of data sharing. Privacy preserving frequent itemset mining (PPFIM) is a subtask and NP-hard problem of PPDM. Its objective is to modify a given database in such a way that none of the sensitive itemsets of the database owner can be obtained by any frequent itemset mining technique from the modified database. The main challenge of PPFIM is to minimize the distortion given to the data and nonsensitive knowledge while sanitizing all given sensitive itemsets. Distortion-based sensitive itemset hiding algorithms decrease the support of each sensitive itemset under a predefined sensitive threshold through sanitization. Most of the distortion-based itemset hiding algorithms allow database owner to define a single sensitive threshold for each sensitive itemset. However, this is a limitation to the database owner since the importance of each sensitive itemset varies. In this paper we propose a distortion-based itemset hiding algorithm that allows database owner to assign multiple sensitive thresholds, namely itemset oriented pseudo graph based sanitization (IPGBS) algorithm. The purpose of IPGBS algorithm is to give minimum distortion to the nonsensitive knowledge and data while hiding all sensitive itemsets. For this reason, the IPGBS algorithm modifies least amount of transaction and transaction content. The performance evaluation of the IPGBS algorithm is conducted by using two different counterparts on four different databases. The results show that the IPGBS algorithm is more efficient in terms of nonsensitive frequent itemset loss on both dense and sparse databases. It has considerable good results in terms of number of transactions modified, number of items deleted, execution time and total memory allocation as well.},
  archive  = {J},
  author   = {Belgin Ergenç Bostanoǧlu and Ahmet Cumhur Öztürk},
  doi      = {10.1002/sam.11458},
  journal  = {Statistical Analysis and Data Mining: The ASA Data Science Journal},
  month    = {8},
  number   = {4},
  pages    = {309-323},
  title    = {Minimizing information loss in shared data: Hiding frequent patterns with multiple sensitive support thresholds},
  volume   = {13},
  year     = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Delaunay triangulation-based spatial colocation pattern
mining without distance thresholds. <em>Statistical Analysis and Data
Mining: The ASA Data Science Journal</em>, <em>13</em>(3), 282–304. (<a
href="https://doi.org/10.1002/sam.11457">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {A spatial colocation pattern is a group of spatial features whose instances frequently appear together in close proximity to each other. The proximity of instances is generally measured by the distance between them. If the distance is smaller than a distance threshold that is specified by users, they have a neighbor relationship. However, it is difficult for users to give a suitable distance threshold and mining results also vary widely with different distance thresholds. In addition, using distance thresholds are hard to accurately obtain neighborhoods of instances in heterogeneous distribution density data sets. In this study, we propose a new method for determining the neighbor relationship of instances in space without the distance threshold based on Delaunay triangulation (DT). We design three filtering strategies, such as a feature invalid edge, a global positive edge, and a local positive edge, to constrain the original DT to accurately extract the neighborhoods of instances in space. Then, a miner called DT-based colocation (DTC) pattern mining is developed. Different from the traditional algorithms which adopt the time-consuming generate-test candidate model, DTC directly collects the table instances of colocation patterns from the constrained DT by building neighboring polygons and filters prevalent patterns. We compare the results mined by DTC with by the traditional algorithms at macrolevel and microlevel on both real and synthetic data sets to prove that the DTC algorithm improves the effectiveness and fineness of mining results.},
  archive  = {J},
  author   = {Vanha Tran and Lizhen Wang},
  doi      = {10.1002/sam.11457},
  journal  = {Statistical Analysis and Data Mining: The ASA Data Science Journal},
  month    = {6},
  number   = {3},
  pages    = {282-304},
  title    = {Delaunay triangulation-based spatial colocation pattern mining without distance thresholds},
  volume   = {13},
  year     = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A machine learning method for selection of genetic variants
to increase prediction accuracy of type 2 diabetes mellitus using
sequencing data. <em>Statistical Analysis and Data Mining: The ASA Data
Science Journal</em>, <em>13</em>(3), 261–281. (<a
href="https://doi.org/10.1002/sam.11456">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Type 2 diabetes mellitus (T2DM) affects millions of people through its life-altering complications. Worldwide, 3.4 million people die of diabetes annually. Studying the effect of genetic polymorphism on T2DM has been plagued by the available sample size. A 2016 Nature Reviews article summarized that the accuracy of predicting future type 2 diabetes from genetic polymorphism is very low at the population level. Innumerable associations between genes, environmental factors, and type 2 diabetes remain to be discovered. This research presents a method to identify subtle effects of genetic variants using whole genome sequencing data and improve prediction accuracy of T2DM at the population level. To achieve this, a new feature selection procedure and a classifier are proposed. The method involves (a) first applying sparse principal component analysis to genotype data to obtain orthogonal features; (b) building a new classifier using single nucleotide polymorphism (SNP)-specific regularization parameters to reduce the false positive rate of feature selection; (c) verifying feature relevance through penalized logistic regression. After application to a dataset containing 625 597 SNPs and 23 environmental variables from each of 3326 humans, the method identified 271 genetic variants with subtle effects on T2DM prediction. These variants led to greatly improved prediction accuracy for new patients at the population level. The proposed method also has the advantage of computational efficiency, over 15 times faster than random forest and extreme gradient boosting (XGBoost) classifiers, and thus provides a promising tool for large-scale genome-wide association studies.},
  archive  = {J},
  author   = {Luann C. Jung and Haiyan Wang and Xukun Li and Cen Wu},
  doi      = {10.1002/sam.11456},
  journal  = {Statistical Analysis and Data Mining: The ASA Data Science Journal},
  month    = {6},
  number   = {3},
  pages    = {261-281},
  title    = {A machine learning method for selection of genetic variants to increase prediction accuracy of type 2 diabetes mellitus using sequencing data},
  volume   = {13},
  year     = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Lagged encoding for image-based time series classification
using convolutional neural networks. <em>Statistical Analysis and Data
Mining: The ASA Data Science Journal</em>, <em>13</em>(3), 245–260. (<a
href="https://doi.org/10.1002/sam.11455">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Time series classification is a thriving area of research in machine learning. Among many applications, it is frequently applied to human activity analysis. Time series describing a human in motion are ubiquitously collected via omnipresent mobile devices and can be subjected to further processing. In this paper, we propose a novel, deep learning approach to time series classification. It is based on a lagged time series representation stored as images and Convolutional Neural Network used to image classification. We present a comparative study on different variants of lagged time series representation and we evaluate their effectiveness in a series of empirical experiments. We show that the developed method provides satisfying classification accuracy. The proposed image-based time series encoding is less resource-consuming than encodings used in other image-based approaches to time series classification. It is worth to emphasize that the proposed time series encoding conceals original time series values. Images are saved without scales and the order of observations cannot be reconstructed. Thus, the method is particularly suitable for systems that need to store sensitive information.},
  archive  = {J},
  author   = {Agnieszka Jastrzebska},
  doi      = {10.1002/sam.11455},
  journal  = {Statistical Analysis and Data Mining: The ASA Data Science Journal},
  month    = {6},
  number   = {3},
  pages    = {245-260},
  title    = {Lagged encoding for image-based time series classification using convolutional neural networks},
  volume   = {13},
  year     = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Vertex nomination via seeded graph matching. <em>Statistical
Analysis and Data Mining: The ASA Data Science Journal</em>,
<em>13</em>(3), 229–244. (<a
href="https://doi.org/10.1002/sam.11454">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Consider two networks on overlapping, nonidentical vertex sets. Given vertices of interest (VOIs) in the first network, we seek to identify the corresponding vertices, if any exist, in the second network. While in moderately sized networks graph matching methods can be applied directly to recover the missing correspondences, herein we present a principled methodology appropriate for situations in which the networks are too large/noisy for brute-force graph matching. Our methodology identifies vertices in a local neighborhood of the VOIs in the first network that have verifiable corresponding vertices in the second network. Leveraging these known correspondences, referred to as seeds, we match the induced subgraphs in each network generated by the neighborhoods of these verified seeds, and rank the vertices of the second network in terms of the most likely matches to the original VOIs. We demonstrate the applicability of our methodology through simulations and real data examples.},
  archive  = {J},
  author   = {Heather G. Patsolic and Youngser Park and Vince Lyzinski and Carey E. Priebe},
  doi      = {10.1002/sam.11454},
  journal  = {Statistical Analysis and Data Mining: The ASA Data Science Journal},
  month    = {6},
  number   = {3},
  pages    = {229-244},
  title    = {Vertex nomination via seeded graph matching},
  volume   = {13},
  year     = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Bent line quantile regression via a smoothing technique.
<em>Statistical Analysis and Data Mining: The ASA Data Science
Journal</em>, <em>13</em>(3), 216–228. (<a
href="https://doi.org/10.1002/sam.11453">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {A bent line quantile regression model can describe the conditional quantile function of the response variable with two different straight lines, which intersect at an unknown change point. This paper proposes a new approach via a smoothing technique to simultaneously estimate the location of the change point and other regression coefficients for the bent line quantile regression model. Furthermore, the asymptotic properties of the proposed estimator are derived, and a formal test procedure for the existence of a change point is also provided. Simulation studies are carried out to demonstrate the finite sample performance of the proposed method. We also illustrate the proposed method by applying it to the gross domestic product (GDP) per capita and the life expectancy at birth data.},
  archive  = {J},
  author   = {Xiaoying Zhou and Feipeng Zhang},
  doi      = {10.1002/sam.11453},
  journal  = {Statistical Analysis and Data Mining: The ASA Data Science Journal},
  month    = {6},
  number   = {3},
  pages    = {216-228},
  title    = {Bent line quantile regression via a smoothing technique},
  volume   = {13},
  year     = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Prediction of kumaraswamy distribution in constant-stress
model based on type-i hybrid censored data. <em>Statistical Analysis and
Data Mining: The ASA Data Science Journal</em>, <em>13</em>(3), 205–215.
(<a href="https://doi.org/10.1002/sam.11452">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {In this work, a particular problem of Bayesian prediction concerning future observation from Kumaraswamy distribution under constant-stress partially accelerated life test is treated. Type-I hybrid censored data of the observed data are utilized. One- and two-sample Bayesian prediction intervals for an unobserved future sample from Kumaraswamy distribution are settled. Markov chain Monte Carlo (MCMC) procedure is used to get Bayesian predictive intervals. Lastly, simulation study and a numerical example are given to illustrate the consequences of the research.},
  archive  = {J},
  author   = {Mohamad A. Fawzy},
  doi      = {10.1002/sam.11452},
  journal  = {Statistical Analysis and Data Mining: The ASA Data Science Journal},
  month    = {6},
  number   = {3},
  pages    = {205-215},
  title    = {Prediction of kumaraswamy distribution in constant-stress model based on type-I hybrid censored data},
  volume   = {13},
  year     = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). An algorithm to compare two-dimensional footwear outsole
images using maximum cliques and speeded-up robust feature.
<em>Statistical Analysis and Data Mining: The ASA Data Science
Journal</em>, <em>13</em>(2), 188–199. (<a
href="https://doi.org/10.1002/sam.11449">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Footwear examiners are tasked with comparing an outsole impression ( Q ) left at a crime scene with an impression ( K ) from a database or from the suspect&#39;s shoe. We propose a method for comparing two shoe outsole impressions that relies on robust features (speeded-up robust feature; SURF) on each impression and aligns them using a maximum clique (MC). After alignment, an algorithm we denote MC-COMP is used to extract additional features that are then combined into a univariate similarity score using a random forest (RF). We use a database of shoe outsole impressions that includes images from two models of athletic shoes that were purchased new and then worn by study participants for about 6 months. The shoes share class characteristics such as outsole pattern and size, and thus the comparison is challenging. We find that the RF implemented on SURF outperforms other methods recently proposed in the literature in terms of classification precision. In more realistic scenarios where crime scene impressions may be degraded and smudged, the algorithm we propose—denoted MC-COMP-SURF—shows the best classification performance by detecting unique features better than other methods. The algorithm can be implemented with the R-package shoeprintr .},
  archive  = {J},
  author   = {Soyoung Park and Alicia Carriquiry},
  doi      = {10.1002/sam.11449},
  journal  = {Statistical Analysis and Data Mining: The ASA Data Science Journal},
  month    = {4},
  number   = {2},
  pages    = {188-199},
  title    = {An algorithm to compare two-dimensional footwear outsole images using maximum cliques and speeded-up robust feature},
  volume   = {13},
  year     = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A flexible procedure for mixture proportion estimation in
positive-unlabeled learning. <em>Statistical Analysis and Data Mining:
The ASA Data Science Journal</em>, <em>13</em>(2), 178–187. (<a
href="https://doi.org/10.1002/sam.11447">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Positive-unlabeled (PU) learning considers two samples, a positive set P with observations from only one class and an unlabeled set U with observations from two classes. The goal is to classify observations in U . Class mixture proportion estimation (MPE) in U is a key step in PU learning. Blanchard et al. showed that MPE in PU learning is a generalization of the problem of estimating the proportion of true null hypotheses in multiple testing problems. Motivated by this idea, we propose reducing the problem to one-dimension via construction of a probabilistic classifier trained on the P and U data sets followed by application of a one-dimensional mixture proportion method from the multiple testing literature to the observation class probabilities. The flexibility of this framework lies in the freedom to choose the classifier and the one-dimensional MPE method. We prove consistency of two mixture proportion estimators using bounds from empirical process theory, develop tuning parameter free implementations, and demonstrate that they have competitive performance on simulated waveform data and a protein signaling problem.},
  archive  = {J},
  author   = {Zhenfeng Lin and James P. Long},
  doi      = {10.1002/sam.11447},
  journal  = {Statistical Analysis and Data Mining: The ASA Data Science Journal},
  month    = {4},
  number   = {2},
  pages    = {178-187},
  title    = {A flexible procedure for mixture proportion estimation in positive-unlabeled learning},
  volume   = {13},
  year     = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Hybrid dynamic learning mechanism for multivariate time
series segmentation. <em>Statistical Analysis and Data Mining: The ASA
Data Science Journal</em>, <em>13</em>(2), 165–177. (<a
href="https://doi.org/10.1002/sam.11448">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {To improve the efficiency of segmentation methods for multivariate time series, a hybrid dynamic learning mechanism for such series&#39; segmentation is proposed. First, an incremental clustering algorithm is used to automatically cluster variables of multivariate time series. Second, common factors are extracted from every cluster by a dynamic factor model as an ensemble description of the system. Third, this common factor series is segmented by dynamic programming. The proposed method can potentially segment multivariate time series and not only performs segmentation better on multivariate time series with a large number of variables but also improves the running accuracy and efficiency of the algorithm, especially when analyzing complex datasets.},
  archive  = {J},
  author   = {Ling Wang and Kang Li and Qian Ma and YanRong Lu},
  doi      = {10.1002/sam.11448},
  journal  = {Statistical Analysis and Data Mining: The ASA Data Science Journal},
  month    = {4},
  number   = {2},
  pages    = {165-177},
  title    = {Hybrid dynamic learning mechanism for multivariate time series segmentation},
  volume   = {13},
  year     = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Dependability-based cluster weighting in clustering
ensemble. <em>Statistical Analysis and Data Mining: The ASA Data Science
Journal</em>, <em>13</em>(2), 151–164. (<a
href="https://doi.org/10.1002/sam.11451">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {After observing the ensemble success in supervised learning (such as classification), it was extended into unsupervised learning. Therefore, cluster ensemble, which merges multiple basic data partitions or clusters (called as ensemble pool) into an ordinarily better clustering solution usually named as consensus partition, emerged. Any cluster ensemble method tries to optimize a particular criterion during extracting the consensus partition out of the ensemble pool. But traditional cluster ensembles consider all the pool members with the equal importance in making the consensus partition; that is to say that each basic partition or cluster participates in the cluster ensemble algorithm equivalently. Indeed, they ignore to consider any ensemble member according to its importance. But it is obvious that some clusters with more quality deserve more emphasis and some clusters with less quality deserve less emphasis during generating consensus partition. This paper proposes (a) a metric to evaluate quality of any arbitrary cluster, (b) a mechanism to project the computed quality of a cluster into a meaningful weight value, and (c) an approach to apply the weight values of the basic clusters in the cluster ensemble process. Experimental results conducted on a number of real-world standard datasets indicate that the proposed method outperforms the state of the art methods.},
  archive  = {J},
  author   = {Fatemeh Najafi and Hamid Parvin and Kamal Mirzaie and Samad Nejatian and Vahideh Rezaie},
  doi      = {10.1002/sam.11451},
  journal  = {Statistical Analysis and Data Mining: The ASA Data Science Journal},
  month    = {4},
  number   = {2},
  pages    = {151-164},
  title    = {Dependability-based cluster weighting in clustering ensemble},
  volume   = {13},
  year     = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Tree aggregation for random forest class probability
estimation. <em>Statistical Analysis and Data Mining: The ASA Data
Science Journal</em>, <em>13</em>(2), 134–150. (<a
href="https://doi.org/10.1002/sam.11446">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {In random forest methodology, an overall prediction or estimate is made by aggregating predictions made by individual decision trees. Popular implementations of random forests rely on different methods for aggregating predictions. In this study, we provide an empirical analysis of the performance of aggregation approaches available for classification and regression problems. We show that while the choice of aggregation scheme usually has little impact in regression, it can have a profound effect on probability estimation in classification problems. Our study illustrates the causes of calibration issues that arise from two popular aggregation approaches and highlights the important role that terminal nodesize plays in the aggregation of tree predictions. We show that optimal choices for random forest tuning parameters depend heavily on the manner in which tree predictions are aggregated.},
  archive  = {J},
  author   = {Andrew J. Sage and Ulrike Genschel and Dan Nettleton},
  doi      = {10.1002/sam.11446},
  journal  = {Statistical Analysis and Data Mining: The ASA Data Science Journal},
  month    = {4},
  number   = {2},
  pages    = {134-150},
  title    = {Tree aggregation for random forest class probability estimation},
  volume   = {13},
  year     = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). In praise of partially interpretable predictors.
<em>Statistical Analysis and Data Mining: The ASA Data Science
Journal</em>, <em>13</em>(2), 113–133. (<a
href="https://doi.org/10.1002/sam.11450">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Often there is an uninterpretable model that is statistically as good as, if not better than, a successful interpretable model. Accordingly, if one restricts attention to interpretable models, then one may sacrifice predictive power or other desirable properties. A minimal condition for an interpretable, usually parametric, model to be better than another model is that the first should have smaller mean-squared error or integrated mean-squared error. We show through a series of examples that this is often not the case and give the asymptotic forms of a variety of interpretable, partially interpretable, and noninterpretable methods. We find techniques that combine aspects of both interpretability and noninterpretability in models seem to give the best results.},
  archive  = {J},
  author   = {Tri Le and Bertrand Clarke},
  doi      = {10.1002/sam.11450},
  journal  = {Statistical Analysis and Data Mining: The ASA Data Science Journal},
  month    = {4},
  number   = {2},
  pages    = {113-133},
  title    = {In praise of partially interpretable predictors},
  volume   = {13},
  year     = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A new method for performance analysis in nonlinear
dimensionality reduction. <em>Statistical Analysis and Data Mining: The
ASA Data Science Journal</em>, <em>13</em>(1), 98–108. (<a
href="https://doi.org/10.1002/sam.11445">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {In this paper, we develop a local rank correlation (LRC) measure which quantifies the performance of dimension reduction methods. The LRC is easily interpretable, and robust against the extreme skewness of nearest neighbor distributions in high dimensions. Some benchmark datasets are studied. We find that the LRC closely corresponds to our visual interpretation of the quality of the output. In addition, we demonstrate that the LRC is useful in estimating the intrinsic dimensionality of the original data, and in selecting a suitable value of tuning parameters used in some algorithms.},
  archive  = {J},
  author   = {Jiaxi Liang and Shojaeddin Chenouri and Christopher G. Small},
  doi      = {10.1002/sam.11445},
  journal  = {Statistical Analysis and Data Mining: The ASA Data Science Journal},
  month    = {2},
  number   = {1},
  pages    = {98-108},
  title    = {A new method for performance analysis in nonlinear dimensionality reduction},
  volume   = {13},
  year     = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Nonparametric clustering for image segmentation.
<em>Statistical Analysis and Data Mining: The ASA Data Science
Journal</em>, <em>13</em>(1), 83–97. (<a
href="https://doi.org/10.1002/sam.11444">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Image segmentation aims at identifying regions of interest within an image by grouping pixels according to their properties. This task resembles the statistical one of clustering, yet many standard clustering methods fail to meet the basic requirements of image segmentation since the identified segments are often biased toward predetermined shapes and their number is rarely determined automatically. Nonparametric clustering is, in principle, free from these limitations and particularly suitable for the task of image segmentation. We discuss the application of nonparametric clustering to image segmentation and provide an algorithm specific for this task. Pixel similarity is evaluated in terms of the density of the color representation. The adjacency structure of the pixels is exploited to introduce a simple, yet effective method to identify image segments as disconnected high-density regions. The proposed method answers to the need of both segmenting an image and detecting its boundaries and can be seen as a generalization to color images of the class of thresholding methods.},
  archive  = {J},
  author   = {Giovanna Menardi},
  doi      = {10.1002/sam.11444},
  journal  = {Statistical Analysis and Data Mining: The ASA Data Science Journal},
  month    = {2},
  number   = {1},
  pages    = {83-97},
  title    = {Nonparametric clustering for image segmentation},
  volume   = {13},
  year     = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Sufficient dimension folding in regression via distance
covariance for matrix-valued predictors. <em>Statistical Analysis and
Data Mining: The ASA Data Science Journal</em>, <em>13</em>(1), 71–82.
(<a href="https://doi.org/10.1002/sam.11442">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {In modern data, when predictors are matrix/array-valued, building a reasonable model is much more difficult due to the complicate structure. However, dimension folding that reduces the predictor dimensions while keeps its structure is critical in helping to build a useful model. In this paper, we develop a new sufficient dimension folding method using distance covariance for regression in such a case. The method works efficiently without strict assumptions on the predictors. It is model-free and nonparametric, but neither smoothing techniques nor selection of tuning parameters is needed. Moreover, it works for both univariate and multivariate response cases. In addition, we propose a new method of local search to estimate the structural dimensions. Simulations and real data analysis support the efficiency and effectiveness of the proposed method.},
  archive  = {J},
  author   = {Wenhui Sheng and Qingcong Yuan},
  doi      = {10.1002/sam.11442},
  journal  = {Statistical Analysis and Data Mining: The ASA Data Science Journal},
  month    = {2},
  number   = {1},
  pages    = {71-82},
  title    = {Sufficient dimension folding in regression via distance covariance for matrix-valued predictors},
  volume   = {13},
  year     = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Neural network for univariate and multivariate nonlinearity
tests. <em>Statistical Analysis and Data Mining: The ASA Data Science
Journal</em>, <em>13</em>(1), 50–70. (<a
href="https://doi.org/10.1002/sam.11441">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {This paper aims to introduce a multiple output artificial neural network as a tool for testing nonlinearity in multivariate time series. Unlike previous studies, we use weights from trained networks to determine the support space for defining random weights of nonlinear regressors and obtain greater power. Moreover, this paper uses two hidden layer neural networks for univariate and multivariate nonlinearity tests. Simulation results show that the proposed method is more powerful than the Terasvirta, Lin, and Granger test in most functional forms and more powerful than the Tsay test in some cases. Taking into account univariate and multivariate time series, the neural network approach is much more powerful than both these tests.},
  archive  = {J},
  author   = {Shapour Mohammadi},
  doi      = {10.1002/sam.11441},
  journal  = {Statistical Analysis and Data Mining: The ASA Data Science Journal},
  month    = {2},
  number   = {1},
  pages    = {50-70},
  title    = {Neural network for univariate and multivariate nonlinearity tests},
  volume   = {13},
  year     = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). The spatially conscious machine learning model.
<em>Statistical Analysis and Data Mining: The ASA Data Science
Journal</em>, <em>13</em>(1), 31–49. (<a
href="https://doi.org/10.1002/sam.11440">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Successfully predicting gentrification could have many social and commercial applications; however, real estate sales are difficult to predict because they belong to a chaotic system comprised of intrinsic and extrinsic characteristics, perceived value, and market speculation. Using New York City real estate as our subject, we combine modern techniques of data science and machine learning with traditional spatial analysis to create robust real estate prediction models for both classification and regression tasks. We compare several cutting edge machine learning algorithms across spatial, semispatial, and nonspatial feature engineering techniques, and we empirically show that spatially conscious machine learning models outperform nonspatial models when married with advanced prediction techniques such as Random Forests, generalized linear models, gradient boosting machines, and artificial neural networks.},
  archive  = {J},
  author   = {Timothy J. Kiely and Nathaniel D. Bastian},
  doi      = {10.1002/sam.11440},
  journal  = {Statistical Analysis and Data Mining: The ASA Data Science Journal},
  month    = {2},
  number   = {1},
  pages    = {31-49},
  title    = {The spatially conscious machine learning model},
  volume   = {13},
  year     = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Space-efficient estimation of empirical tail dependence
coefficients for bivariate data streams. <em>Statistical Analysis and
Data Mining: The ASA Data Science Journal</em>, <em>13</em>(1), 14–30.
(<a href="https://doi.org/10.1002/sam.11439">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {This article proposes a space-efficient approximation to empirical tail dependence coefficients of an indefinite bivariate stream of data. The approximation, which has stream-length invariant error bounds, utilizes recent work on the development of a summary for bivariate empirical copula functions. The work in this paper accurately approximates a bivariate empirical copula in the tails of each marginal distribution, therefore modeling the tail dependence between the two variables observed in the data stream. Copulas evaluated at these marginal tails can be used to estimate the tail dependence coefficients. Modifications to the space-efficient bivariate copula approximation, presented in this paper, allow the error of approximations to the tail dependence coefficients to remain stream-length invariant. Theoretical and numerical evidence of this, including a case-study using the Los Alamos National Laboratory netflow data-set, is provided within this article.},
  archive  = {J},
  author   = {Alastair Gregory and Kaushik Jana},
  doi      = {10.1002/sam.11439},
  journal  = {Statistical Analysis and Data Mining: The ASA Data Science Journal},
  month    = {2},
  number   = {1},
  pages    = {14-30},
  title    = {Space-efficient estimation of empirical tail dependence coefficients for bivariate data streams},
  volume   = {13},
  year     = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Two-sample test based on classification probability.
<em>Statistical Analysis and Data Mining: The ASA Data Science
Journal</em>, <em>13</em>(1), 5–13. (<a
href="https://doi.org/10.1002/sam.11438">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Robust classification algorithms have been developed in recent years with great success. We take advantage of this development and recast the classical two-sample test problem in the framework of classification. Based on the estimates of classification probabilities from a classifier trained from the samples, a test statistic is proposed. We explain why such a test can be a powerful test and compare its performance in terms of the power and efficiency with those of some other recently proposed tests with simulation and real-life data. The test proposed is nonparametric and can be applied to complex and high-dimensional data wherever there is a classifier that provides consistent estimate of the classification probability for such data.},
  archive  = {J},
  author   = {Haiyan Cai and Bryan Goggin and Qingtang Jiang},
  doi      = {10.1002/sam.11438},
  journal  = {Statistical Analysis and Data Mining: The ASA Data Science Journal},
  month    = {2},
  number   = {1},
  pages    = {5-13},
  title    = {Two-sample test based on classification probability},
  volume   = {13},
  year     = {2020},
}
</textarea>
</details></li>
</ul>

</body>
</html>
