<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>IETIP_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="ietip---501">IETIP - 501</h2>
<ul>
<li><details>
<summary>
(2020). Improved framework of many-objective evolutionary algorithm
to handle cloud detection problem in satellite imagery. <em>IETIP</em>,
<em>14</em>(17), 4795–4807. (<a
href="https://doi.org/10.1049/iet-ipr.2020.0535">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Automatic cloud detection algorithm based on supervised learning approach has emerged due to its effectiveness in extracting weather information in satellite images. However, algorithm requires field-expert intervention with huge database of training samples to evaluate its clustering performance. Moreover, lacking in availability of labelled data makes difficult to train the input samples. Therefore, this article puts forward unsupervised many-objective evolutionary clustering technique to discriminate cloudy regions on varying characteristic of underlying surfaces. The study begins with the modification in search capability of θ -NSGA-III optimisation algorithm by incorporating penalised vector angle concept in associate operator. The analysis of proposed approach has been carried out on benchmark many-objective DTLZ test problems, compared against original θ -NSGA-III and NSGA-III algorithms. The proposed modified θ -NSGA-III has been further utilised as clustering technique to solve unsupervised cloud detection problem. Optimal centroid vector for clustering using proposed approach is obtained through modified crossover operator, mutation operator and environmental selection method. Experimental results reveal that proposed approach outperforms comparative many-objective algorithms, MOEA/D and NSGA-III for Landsat 8, MODIS and NOAA satellite images with lower classification average error of 2.44 % in cloud detection for most of the evaluated test cases.},
  archive      = {J_IETIP},
  author       = {Rachana Gupta and Satyasai Jagannath Nanda},
  doi          = {10.1049/iet-ipr.2020.0535},
  journal      = {IET Image Processing},
  month        = {12},
  number       = {17},
  pages        = {4795-4807},
  shortjournal = {IET Image Process.},
  title        = {Improved framework of many-objective evolutionary algorithm to handle cloud detection problem in satellite imagery},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Multimodal action recognition using variational-based
beta-liouville hidden markov models. <em>IETIP</em>, <em>14</em>(17),
4785–4794. (<a href="https://doi.org/10.1049/iet-ipr.2020.0709">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The visible spectrum is the most widely used modality for video media. Nonetheless, it is highly dependent on the lighting conditions. Hence, infrared (IR) imaging lower light sensitivity characterisation presents the untapped potential for robust automatic recognition systems. This is applicable to many applications including IR action recognition (AR), which is a relatively young field in IR. As such, in this study, the authors tackle IR and multimodal AR with the proposed utilisation of variational learning of Beta-Liouville (BL) hidden Markov models (HMMs). Furthermore, to the best of the authors&#39; knowledge, this is the first evaluation of the BL HMM in visible AR and in multimodal fusion for AR. They present the results of the proposed model on the infrared action recognition and the IOSB datasets. Experimental results demonstrate promising outcomes. The importance of using IR and multispectral fusion in AR is also highlighted by the results.},
  archive      = {J_IETIP},
  author       = {Samr Ali and Nizar Bouguila},
  doi          = {10.1049/iet-ipr.2020.0709},
  journal      = {IET Image Processing},
  month        = {12},
  number       = {17},
  pages        = {4785-4794},
  shortjournal = {IET Image Process.},
  title        = {Multimodal action recognition using variational-based beta-liouville hidden markov models},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Blind deblurring and denoising via a learning deep CNN
denoiser prior and an adaptive l0-regularised gradient prior for passive
millimetre-wave images. <em>IETIP</em>, <em>14</em>(17), 4774–4784. (<a
href="https://doi.org/10.1049/iet-ipr.2020.1193">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Passive millimetre-wave (PMMW) imaging frequently suffers from blurring and low resolution due to the long wavelengths. In addition, the observed images are inevitably disturbed by noise. Traditional image deblurring methods are sensitive to image noise, even a small amount of which will greatly reduce the quality of the point spread function (PSF) estimation. In this paper, we propose a blind deblurring and denoising method via a learning deep denoising convolutional neural networks (DnCNN) denoiser prior and an adaptive -regularized gradient prior for passive millimetre-wave images. First, a blind deblurring restoration model based on the DnCNN denoising prior constraint is established. Second, an adaptive -regularized gradient prior is incorporated into the model to estimate the latent clear image, and the PSF is estimated in the gradient domain. In a multi-scale framework, alternate iterative denoising and deblurring are used to obtain the final PSF estimation and noise estimation. Ultimately, the final clear image is restored by non-blind deconvolution. The experimental results show that the algorithm used in this paper not only has good detail recovery ability but is also more stable to different noise levels. The proposed method is superior to state-of-the-art methods in terms of both subjective measure and visual quality.},
  archive      = {J_IETIP},
  author       = {Dianjun Sun and Yu Shi and Yayuan Feng},
  doi          = {10.1049/iet-ipr.2020.1193},
  journal      = {IET Image Processing},
  month        = {12},
  number       = {17},
  pages        = {4774-4784},
  shortjournal = {IET Image Process.},
  title        = {Blind deblurring and denoising via a learning deep CNN denoiser prior and an adaptive l0-regularised gradient prior for passive millimetre-wave images},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Blind multipurpose watermarking with insertion of a single
watermark: A generic construction based on verifiable threshold secret
sharing. <em>IETIP</em>, <em>14</em>(17), 4766–4773. (<a
href="https://doi.org/10.1049/iet-ipr.2019.1576">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Currently, the existing literature on multipurpose watermarking employs multiple watermarks for simultaneous achieving of multiple security objectives. This approach can be problematic since a watermark targeting tampers might fail to detect tampering of another watermark inserted for copyright violation. Moreover, most of the current schemes follow a non-blind approach where the original watermark is needed for authentication. A question that naturally arises is whether multipurpose watermarking can be realised with the insertion of a single watermark in a blind way or not. The goal of the authors study is to provide an affirmative answer to this important question. They show how a cryptographic primitive called verifiable threshold secret sharing can be used to come up with a generic construction for blind multipurpose watermarking which inserts a single watermark into the host image for simultaneous achieving of copyright protection, authentication, and tamper localisation. The generic property of the proposed scheme provides flexibility for choosing the embedding/extraction of the watermark based on the desired level of fidelity, robustness and capacity. Experimental results are provided to confirm the superiority of their proposed technique as compared to existing approaches.},
  archive      = {J_IETIP},
  author       = {Sorour Sheidani and Ziba Eslami},
  doi          = {10.1049/iet-ipr.2019.1576},
  journal      = {IET Image Processing},
  month        = {12},
  number       = {17},
  pages        = {4766-4773},
  shortjournal = {IET Image Process.},
  title        = {Blind multipurpose watermarking with insertion of a single watermark: A generic construction based on verifiable threshold secret sharing},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Infant brain segmentation based on a combination of VGG-16
and u-net deep neural networks. <em>IETIP</em>, <em>14</em>(17),
4756–4765. (<a href="https://doi.org/10.1049/iet-ipr.2020.0469">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Medical image segmentation plays a key role in identifying the disease type. In the last decade, various methods have been proposed for medical images segmentation. Despite many efforts made in medical imaging, segmentation of medical images still faces challenges, concerning the variety of shape, location, and texture quality. According to recent studies and magnetic resonance imaging, segmentation of brain images at around 6 months of age is a challenging issue in brain image segmentation due to low tissue contrast between white matter (WM) and grey matter (GM) regions. In this study, using the deep learning model, the convolutional network for the brain fragmentation is presented. First, the image quality is improved using the pre-processing method. The number of layers utilised in the proposed method is less than that of known models. In the pooling layer, instead of using the maximum function, the averaging function is employed. Sixty-four batches are also considered to improve the performance of the proposed method. The method is evaluated on the iSeg-2017 database. The DISC and ASC measures of the proposed method for the three classes of GM, WM, and cerebrovascular fluid are 0.902, 0.594, 0.930, 0.481, 0.971, and 0.231, respectively.},
  archive      = {J_IETIP},
  author       = {Sadegh Pasban and Sajad Mohamadzadeh and Javad Zeraatkar-Moghaddam and Amir Keivan Shafiei},
  doi          = {10.1049/iet-ipr.2020.0469},
  journal      = {IET Image Processing},
  month        = {12},
  number       = {17},
  pages        = {4756-4765},
  shortjournal = {IET Image Process.},
  title        = {Infant brain segmentation based on a combination of VGG-16 and U-net deep neural networks},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Forged text detection in video, scene, and document images.
<em>IETIP</em>, <em>14</em>(17), 4744–4755. (<a
href="https://doi.org/10.1049/iet-ipr.2020.0590">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Rapid advances in artificial intelligence have made it possible to produce forgeries good enough to fool an average user. As a result, there is growing interest in developing robust methods to counter such forgeries. This study presents a new Fourier spectrum-based method for detecting forged text in video images. The authors&#39; premise is that brightness distribution and the spectrum shape exhibit irregular patterns (inconsistencies) for forged text, while appearing more regular for original text. The method divides the spectrum of an input image into sectors and tracks to highlight these effects. Specifically, positive and negative coefficients for sectors and tracks are extracted to quantify the brightness distribution. Variations in the shape of the spectrum are analysed by determining the angular relationship between the principal axes and the sectors/tracks of the spectrum. Next, it combines these two features to detect forged text in the images of IMEI (International Mobile Equipment Identity) numbers and document. For evaluation, the following datasets are used: own video dataset and standard datasets, namely, IMEI number, ICPR 2018 Fraud Document Contest, and a natural scene text dataset. Experimental results show that the proposed method outperforms existing methods in terms of average classification rate and F -score.},
  archive      = {J_IETIP},
  author       = {Lokesh Nandanwar and Palaiahnakote Shivakumara and Prabir Mondal and Karpuravalli Srinivas Raghunandan and Umapada Pal and Tong Lu and Daniel Lopresti},
  doi          = {10.1049/iet-ipr.2020.0590},
  journal      = {IET Image Processing},
  month        = {12},
  number       = {17},
  pages        = {4744-4755},
  shortjournal = {IET Image Process.},
  title        = {Forged text detection in video, scene, and document images},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). End to end system for hazy image classification and
reconstruction based on mean channel prior using deep learning network.
<em>IETIP</em>, <em>14</em>(17), 4736–4743. (<a
href="https://doi.org/10.1049/iet-ipr.2020.0923">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Outdoor images are having several applications including autonomous vehicles, geo-mapping, and surveillance. It is a common phenomenon that the images captured outdoor are prone to noise, which arises due to natural and manmade extreme atmospheric conditions such as haze, fog, and smog. Importantly in autonomous vehicle navigation, it is very important to recover the ground truth image to get the better decision by the system. Estimation of the transmission map and air-light is very crucial in recovering the ground truth image. In this study, the authors proposed a new method to estimate the transmission map based on a mean channel prior (MCP), which represents the depth map to estimate the transmission map. The authors proposed a deep neural network to identify the hazy image for the further dehazing process. In this study, the authors presented, two novel contributions, first an MCP-based image dehazing and second, a deep neural network-based identification of hazy images as a pre-processing block in the proposed end to end system. The proposed deep learning network using the TensorFlow platform provided validation accuracy of 93.4% for hazy image classification. Finally, the proposed MCP-based dehazing network showed better performance in terms of peak-signal-to-noise ratio, structural similarity index, and computational time than that of existing methods.},
  archive      = {J_IETIP},
  author       = {Sivaji Satrasupalli and Ebenezer Daniel and Sitaramanjaneya Reddy Guntur and Shaik Shehanaz},
  doi          = {10.1049/iet-ipr.2020.0923},
  journal      = {IET Image Processing},
  month        = {12},
  number       = {17},
  pages        = {4736-4743},
  shortjournal = {IET Image Process.},
  title        = {End to end system for hazy image classification and reconstruction based on mean channel prior using deep learning network},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020b). Item-to-item recommender system with simultaneous use of
multiple images for image mosaicking creation in dynamic scenes.
<em>IETIP</em>, <em>14</em>(17), 4726–4735. (<a
href="https://doi.org/10.1049/iet-ipr.2020.0614">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image mosaicking is a series of successive algorithms that use a sequence of images or a video of a scene to create a single image with a wider field of view of the whole scene. In most cases, when dynamic objects exist in the input data, issues, such as ghosting, parallax effects or object duplication, are visible in the resulting mosaic. These technical errors appear when the objects in motion aren&#39;t properly treated. In order to create good result mosaics, a new method is presented in this paper. The proposed approach uses all the images at the same time to divide the images into different areas by using k-means clustering to create categories, then each category is recommended from the original images with a recommender system. In fact, by considering each image as a user, each pixel as an item, and each item belonging to a category, it is possible to use a recommender system by computing scores with the item profiles. The resulting mosaic will then be a new user to the system. Furthermore, by clustering the images, projection errors are avoided and a better quality mosaic can be created as is seen in the obtained results.},
  archive      = {J_IETIP},
  author       = {Saadeddine Laaroussi and Aziz Baataoui and Akram Halli and Khalid Satori},
  doi          = {10.1049/iet-ipr.2020.0614},
  journal      = {IET Image Processing},
  month        = {12},
  number       = {17},
  pages        = {4726-4735},
  shortjournal = {IET Image Process.},
  title        = {Item-to-item recommender system with simultaneous use of multiple images for image mosaicking creation in dynamic scenes},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Cervical cancer detection in cervical smear images using
deep pyramid inference with refinement and spatial-aware booster.
<em>IETIP</em>, <em>14</em>(17), 4717–4725. (<a
href="https://doi.org/10.1049/iet-ipr.2020.0688">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the development of artificial intelligence and image processing technology, more and more intelligent diagnosis technologies are used in cervical cancer screening. Among them, the detection of cervical lesions by thin liquid-based cytology is the most common method for cervical cancer screening. At present, most cervical cancer detection algorithms use the object detection technology of natural images, and often only minor modifications are made while ignoring the specificity of the complex application scenario of cervical lesions detection in cervical smear images. In this study, the authors combine the domain knowledge of cervical cancer detection and the characteristics of pathological cells to design a network and propose a booster for cervical cancer detection (CCDB). The booster mainly consists of two components: the refinement module and the spatial-aware module. The characteristics of cancer cells are fully considered in the booster, and the booster is light and transplantable. As far as the authors know, they are the first to design a CCDB according to the characteristics of cervical cancer cells. Compared with baseline (Retinanet), the sensitivity at four false positives per image and average precision of the proposed method are improved by 2.79 and 7.2%, respectively.},
  archive      = {J_IETIP},
  author       = {Dongyang Ma and Jinhua Liu and Jing Li and Yuanfeng Zhou},
  doi          = {10.1049/iet-ipr.2020.0688},
  journal      = {IET Image Processing},
  month        = {12},
  number       = {17},
  pages        = {4717-4725},
  shortjournal = {IET Image Process.},
  title        = {Cervical cancer detection in cervical smear images using deep pyramid inference with refinement and spatial-aware booster},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Edge-guided single facial depth map super-resolution using
CNN. <em>IETIP</em>, <em>14</em>(17), 4708–4716. (<a
href="https://doi.org/10.1049/iet-ipr.2019.1623">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, consumer depth cameras have been widely used in digital entertainment and human-machine interaction due to the advantages of real-time performance and low cost. Facial depth maps have shown great potential in 3D-face-related studies. However, disadvantages of low resolution and precision limit its further applications. In this work, the authors propose an edge-guided convolutional neural network for single facial depth map super-resolution. It consists of two parts: an edge prediction sub-network and a depth reconstruction sub-network. The edge prediction sub-network generates an edge guidance map to guide the depth reconstruction sub-network to recover sharp edges and fine structures. Effective data augmentation methods are proposed as well. The network is patch-based and able to cope with any size of the input depth maps. In addition, it is insensitive to the face pose since the synthetic training dataset they generated covers a wide range of face poses. The proposed method is validated with three datasets including a synthetic facial depth data set, a real Kinect V2 facial depth data set and Middlebury Stereo Data set. Experimental results show that it outperforms the state-of-the-art methods on all the three data sets.},
  archive      = {J_IETIP},
  author       = {Fan Zhang and Na Liu and Liang Chang and Fuqing Duan and Xiaoming Deng},
  doi          = {10.1049/iet-ipr.2019.1623},
  journal      = {IET Image Processing},
  month        = {12},
  number       = {17},
  pages        = {4708-4716},
  shortjournal = {IET Image Process.},
  title        = {Edge-guided single facial depth map super-resolution using CNN},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Multi-scale supervised network for crowd counting.
<em>IETIP</em>, <em>14</em>(17), 4701–4707. (<a
href="https://doi.org/10.1049/iet-ipr.2020.0897">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Crowd counting is getting more and more attention in our daily life, because it can effectively prevent some safety problems. However, due to scale variations and background noise in the image, such as buildings and trees, getting the accurate number from image is a hard work. In order to address these problems, this work introduces a new multi-scale supervised network. The proposed model uses part of vgg16 model as the backbone to extract feature. In the training process, a multi-scale dilated convolution module is added at the end of each stage of the backbone network to generate attention map with different resolutions to help the model focus on the head area in feature map. In addition, the dilated convolution adopts three dilation ratios to fit different sizes of head in the image. Finally, in order to get the high-quality density map with high-resolution, the authors employ the upsampling operation to restore the density map size to the quarter size of original image. A large number of experiments on these four datasets show that the proposed network has greatly improved the counting accuracy of many existing methods.},
  archive      = {J_IETIP},
  author       = {Yongjie Wang and Wei Zhang and Dongxiao Huang and Yanyan Liu and Jianghua Zhu},
  doi          = {10.1049/iet-ipr.2020.0897},
  journal      = {IET Image Processing},
  month        = {12},
  number       = {17},
  pages        = {4701-4707},
  shortjournal = {IET Image Process.},
  title        = {Multi-scale supervised network for crowd counting},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020b). Real-time keypoints detection for autonomous recovery of
the unmanned ground vehicle. <em>IETIP</em>, <em>14</em>(17), 4690–4700.
(<a href="https://doi.org/10.1049/iet-ipr.2020.0864">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The combination of a small unmanned ground vehicle (UGV) and a large unmanned carrier vehicle allows more flexibility in real applications such as rescue in dangerous scenarios. The autonomous recovery system, which is used to guide the small UGV back to the carrier vehicle, is an essential component to achieve a seamless combination of the two vehicles. This study proposes a novel autonomous recovery framework with a low-cost monocular vision system to provide accurate positioning and attitude estimation of the UGV during navigation. First, the authors introduce a light-weight convolutional neural network called UGV-KPNet to detect the keypoints of the small UGV form the images captured by a monocular camera. UGV-KPNet is computationally efficient with a small number of parameters and provides pixel-level accurate keypoints detection results in real-time. Then, six degrees of freedom (6-DoF) pose is estimated using the detected keypoints to obtain positioning and attitude information of the UGV. Besides, they are the first to create a large-scale real-world keypoints data set of the UGV. The experimental results demonstrate that the proposed system achieves state-of-the-art performance in terms of both accuracy and speed on UGV keypoint detection, and can further boost the 6-DoF pose estimation for the UGV.},
  archive      = {J_IETIP},
  author       = {Jie Li and Sheng Zhang and Kai Han and Xia Yuan and Chunxia Zhao and Yu Liu},
  doi          = {10.1049/iet-ipr.2020.0864},
  journal      = {IET Image Processing},
  month        = {12},
  number       = {17},
  pages        = {4690-4700},
  shortjournal = {IET Image Process.},
  title        = {Real-time keypoints detection for autonomous recovery of the unmanned ground vehicle},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Joint detection and tracking of non-ellipsoidal extended
targets based on cubature kalman-CBMeMBer sub-random matrices filter.
<em>IETIP</em>, <em>14</em>(17), 4676–4689. (<a
href="https://doi.org/10.1049/iet-ipr.2020.1181">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Joint detection and tracking of multiple extended targets (ETs) from image observations is a challenging radar technology; especially for extended stealth targets (ESTs). This work provides a new approach for the ESTs tracking under the non-linear Gaussian system based on track-before-detect (TBD) approach. The sequential Monte Carlo cardinality-balanced multi-target multi-Bernoulli (SMC-CBMeMBer) filter provides a good framework to cope with TBD approach. However, this filter suffers from the particles’ degradation problem seriously; especially for ETs tracking. Recently, the cubature Kalman (CK)-CBMeMBer filter which employs a third-degree spherical-radical cubature rule has been proposed to handle the non-linear models, the CK-CBMeMBer filter is more accurate and more principled in mathematical terms compared to SMC-CBMeMBer filter. To this point, the authors address a TBD of ESTs with extended CK-CBMeMBer filter based on random matrix model (RMM), which is an efficient way to track ellipsoidal ESTs. In RMM-ESTs scenarios, although the extension ellipsoid is efficient, it may not be accurate enough because of lacking useful information, such as size, shape, and orientation. Therefore, they introduce a filter composed of sub-ellipses; each one is represented by a RMM. The results confirm the effectiveness and robustness of the proposed filter.},
  archive      = {J_IETIP},
  author       = {Mohamed Barbary and Mohamed H. Abd ElAzeem},
  doi          = {10.1049/iet-ipr.2020.1181},
  journal      = {IET Image Processing},
  month        = {12},
  number       = {17},
  pages        = {4676-4689},
  shortjournal = {IET Image Process.},
  title        = {Joint detection and tracking of non-ellipsoidal extended targets based on cubature kalman-CBMeMBer sub-random matrices filter},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Novel framework for automatic localisation of gun carrying
by moving person using various indoor and outdoor mimic and real-time
views/scenes. <em>IETIP</em>, <em>14</em>(17), 4663–4675. (<a
href="https://doi.org/10.1049/iet-ipr.2020.0706">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hand held gun detection has an important application in both the field of video forensic and surveillance, because, gun is operative by hand only while committing any crime with it. The significant application encompasses the vulnerable places, such as around airport, marketplace, shopping malls, etc. In view of non-availability of relevant public data set, this study provides a newly created mimicked video data set for detection of gun carried by a person and entitled as Tripura University Video Data set for Crime-Scene-Analysis (TUVD-CSA). Effects of illumination, occlusion, rotation, pan, tilt, scaling of gun are effectively demonstrated in it. Moreover, the authors proposed an Iterative Model Generation Framework (IMGF) for gun detection, which is immune to scaling and rotation. Instead of locating the best matched object (gun) in the whole reference image to a query model via exhaustive search, IMGF searches only where the moving person carrying gun appears, which drastically reduces the computational overhead associated with a general template matching scheme. This has been employed by the background subtraction algorithm. Experimental results demonstrate that the proposed IMGF performs efficiently in gun detection with lesser number of true-negatives compared with the state-of-the-art methods.},
  archive      = {J_IETIP},
  author       = {Rajib Debnath and Mrinal Kanti Bhowmik},
  doi          = {10.1049/iet-ipr.2020.0706},
  journal      = {IET Image Processing},
  month        = {12},
  number       = {17},
  pages        = {4663-4675},
  shortjournal = {IET Image Process.},
  title        = {Novel framework for automatic localisation of gun carrying by moving person using various indoor and outdoor mimic and real-time views/Scenes},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). SIFT and HOG features for the retrieval of ancient kannada
epigraphs. <em>IETIP</em>, <em>14</em>(17), 4657–4662. (<a
href="https://doi.org/10.1049/iet-ipr.2020.0715">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this work, the authors presented the indexing method for efficient retrieval of Kannada characters. Here the characters were segmented by the connected component method. Then features were extracted by scale invariant feature transform (SIFT) and histogram of oriented gradients (HOG) methods. These features were condensed by principal component analysis. They presented the indexing approach using K -dimensional tree (K-D tree) to improve the identification process. For the experiment, they used their own database. The results of the experiment show that the indexing prior is faster than conventional identification approaches in terms of time to script. From experimentation, they observe that fusion features achieve the maximum accuracy of 90% with varying principal component analysis features.},
  archive      = {J_IETIP},
  author       = {Ravi Parashivamurthy and Chikkaguddaiah Naveena and Yeliyur Hanumathiah Sharath Kumar},
  doi          = {10.1049/iet-ipr.2020.0715},
  journal      = {IET Image Processing},
  month        = {12},
  number       = {17},
  pages        = {4657-4662},
  shortjournal = {IET Image Process.},
  title        = {SIFT and HOG features for the retrieval of ancient kannada epigraphs},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Intelligent monitoring method of water quality based on
image processing and RVFL-GMDH model. <em>IETIP</em>, <em>14</em>(17),
4646–4656. (<a href="https://doi.org/10.1049/iet-ipr.2020.0254">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The water quality, contaminant migration characteristics, and emissions quantity of pollutants in the basin would have a great impact on aquatic creatures, agricultural irrigation, human life, and so on. In the aquaculture industry, because water colour can reflect the species and number of phytoplankton in the water, the water quality type can be obtained by analysing the colour of the aquaculture water using image processing techniques. Therefore, this study proposes an intelligent monitoring approach for water quality. The critical features of water colour images are extracted, and then using the machine learning methods, an intelligent system for water quality monitoring is established based on the fused random vector functional link network (RVFL) and group method of data handling (GMDH) model. The proposed approach presents a superior performance relative to other state-of-the-art methods, and it achieves an average predicting accuracy of 96.19% on the feature dataset. Experimental findings demonstrate the validity of the proposed approach, and it is accomplished efficiently for the monitoring of water quality.},
  archive      = {J_IETIP},
  author       = {Junde Chen and Defu Zhang and Shuangyuan Yang and Yaser Ahangari Nanehkaran},
  doi          = {10.1049/iet-ipr.2020.0254},
  journal      = {IET Image Processing},
  month        = {12},
  number       = {17},
  pages        = {4646-4656},
  shortjournal = {IET Image Process.},
  title        = {Intelligent monitoring method of water quality based on image processing and RVFL-GMDH model},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Secure high capacity tetris-based scheme for data hiding.
<em>IETIP</em>, <em>14</em>(17), 4633–4645. (<a
href="https://doi.org/10.1049/iet-ipr.2019.1694">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Information hiding is a technique that conceals private information in a trustable carrier, making it imperceptible to unauthorised people. This technique has been used extensively for secure transmissions of multimedia, such as videos, animations, and images. This study proposes a novel Tetris-based data hiding scheme to flexibly hide more secret messages while ensuring message security. First, an L Q × L Q square lattice Q is selected to determine the maximum embedding capacity, and then it is filled without gaps through rotating and sliding tetrominoes while making the shape of each tetromino different. Secondly, according to the decided Q , the reference matrix and corresponding look-up table are constructed and then used for secret messages embedding and extraction. In the authors approach, each pixel pair of the original image can be processed to conceal 4- or 6-bit secret messages. The experimental results show that their proposed Tetris-based scheme has excellent performance, exceeding the performance of some state-of-the-art schemes in both embedding capacity and visual quality. The proposed scheme also provides secure covert communication.},
  archive      = {J_IETIP},
  author       = {Guo-Dong Su and Chin-Chen Chang and Chia-Chen Lin and Zhi-Qiang Yao},
  doi          = {10.1049/iet-ipr.2019.1694},
  journal      = {IET Image Processing},
  month        = {12},
  number       = {17},
  pages        = {4633-4645},
  shortjournal = {IET Image Process.},
  title        = {Secure high capacity tetris-based scheme for data hiding},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Vehicle detection based on improved multitask cascaded
convolutional neural network and mixed image enhancement.
<em>IETIP</em>, <em>14</em>(17), 4621–4632. (<a
href="https://doi.org/10.1049/iet-ipr.2020.1005">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In order to increase the detection effect of fuzzy vehicles and small size vehicles, an improved multitask cascaded convolutional neural network (IMC-CNN) based on mixed image enhancement is proposed. Firstly, contrast limited adaptive histogram equalisation and multi-scale Retinex are used to enhance images. Mixed image enhancement can effectively solve the problems of image blurring, low contrast and uneven illumination when the imaging environment is not ideal. IMC-CNN includes two stages: object location and object classification. The object location network based on multi-layer feature fusion can locate and extract the object from complex background, and output regions contain only a single vehicle object. The object classification network is a lightweight convolutional neural network with only two convolutional layers, which can effectively reduce information loss and improve the classification accuracy of small objects and fuzzy objects. In addition, online hard example mining algorithm and focal loss function are adopted in network training. These strategies can solve the problem of unbalance between positive and negative samples. To verify the validity of the proposed algorithm, the experiments are performed on SYIT-Fuzzy dataset and COCO-Vehicle dataset. Compared with Faster R-CNN, YOLO v4 and other recent models, the average classification accuracy of the proposed method is significantly increased.},
  archive      = {J_IETIP},
  author       = {Ke Xu and Hua Gong and Fang Liu},
  doi          = {10.1049/iet-ipr.2020.1005},
  journal      = {IET Image Processing},
  month        = {12},
  number       = {17},
  pages        = {4621-4632},
  shortjournal = {IET Image Process.},
  title        = {Vehicle detection based on improved multitask cascaded convolutional neural network and mixed image enhancement},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Multi-scale feature fusion network for person
re-identification. <em>IETIP</em>, <em>14</em>(17), 4614–4620. (<a
href="https://doi.org/10.1049/iet-ipr.2020.0008">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, it is becoming a challenging work for person re-identification due to the problems of occlusion, blurring and posture. The key of effective person re-identification is to capture sufficient detailed features of a person&#39;s appearance in images. Different from previous methods, our method mainly focuses on fusing different visual clues only depending on the features of different levels and scales without additional assistance. The major contributions of our paper are the mixed pooling strategy with different kernels and the mixed loss function. Firstly, we adopt ResNet50 as our backbone. We have slightly modified the backbone, which does not use the down-sampling operation at the beginning of stage 4. Inspired by pyramid pooling structure, we pass the outputs of Res4 and Res5 through the average pooling layer and max pooling layer with different kernels and strides separately. Secondly, we combine the averaged triplet losses and the averaged softmax losses as the final loss of the whole network. Extensive experiments on three datasets (CUHK3, Market1501, DukeMTMC-reID) show that compared with many state-of-the-art methods in recent years, our model achieve higher accuracy.},
  archive      = {J_IETIP},
  author       = {Yongjie Wang and Wei Zhang and Yanyan Liu},
  doi          = {10.1049/iet-ipr.2020.0008},
  journal      = {IET Image Processing},
  month        = {12},
  number       = {17},
  pages        = {4614-4620},
  shortjournal = {IET Image Process.},
  title        = {Multi-scale feature fusion network for person re-identification},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Two viewpoints based real-time recognition for hand
gestures. <em>IETIP</em>, <em>14</em>(17), 4606–4613. (<a
href="https://doi.org/10.1049/iet-ipr.2019.1458">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {It is extremely challenging to accomplish excellent accuracy for gesture recognition using an approach where complexity in computation time for recognition is less. This study compares accuracy in hand gesture recognition of a single viewpoint set-up with proposed two viewpoint set-up for different classification techniques. The efficacy of the presented approach is verified practically with various image processing, feature extraction and classification techniques. Two camera system make geometry learning and three-dimensional (3D) view feasible compared to a single camera system. Geometrical features from additional viewpoint contribute to 3D view estimation of the hand gesture. It also improves the classification accuracy. Experimental results demonstrate that the proposed method show escalation in recognition rate compared to the single-camera system, and also has great performance using simple classifiers like the nearest neighbour and decision tree. Classification within 1 s is considered as real-time in this study.},
  archive      = {J_IETIP},
  author       = {Amit Krishan Kumar and Abhishek Kaushal Kumar and Shuli Guo},
  doi          = {10.1049/iet-ipr.2019.1458},
  journal      = {IET Image Processing},
  month        = {12},
  number       = {17},
  pages        = {4606-4613},
  shortjournal = {IET Image Process.},
  title        = {Two viewpoints based real-time recognition for hand gestures},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Retinal vessel segmentation based on task-driven generative
adversarial network. <em>IETIP</em>, <em>14</em>(17), 4599–4605. (<a
href="https://doi.org/10.1049/iet-ipr.2020.1032">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Retinal vessel segmentation has important application value in clinical diagnosis. If experts manually segment the retinal vessels, the workload is heavy, and the result is strong subjectively. However, some existing automatic segmentation methods have the problems of incomplete vessel segmentation and low-segmentation accuracy. In order to solve the above problems, this study proposes a retinal vessel segmentation method based on task-driven generative adversarial network (GAN). In the generative model, a U-Net network is used to segment the retinal vessels. In the discriminative model, multi-scale discriminators with different receptive fields are used to guide the generative model to generate more details. On the other hand, in view of the uncontrollable characteristics of the data generated by the traditional GAN, a task-driven model based on perceptual loss is added to traditional GAN for feature matching, which makes the generated image more task-specific. Experimental results show that the accuracy, sensitivity, specificity and area under the receiver operating characteristic curve of the proposed method on data set digital retinal images for vessel extraction are 96.83, 80.66, 98.97 and 0.9830%, respectively.},
  archive      = {J_IETIP},
  author       = {Zhiyuan Chen and Wei Jin and Xingbin Zeng and Liang Xu},
  doi          = {10.1049/iet-ipr.2020.1032},
  journal      = {IET Image Processing},
  month        = {12},
  number       = {17},
  pages        = {4599-4605},
  shortjournal = {IET Image Process.},
  title        = {Retinal vessel segmentation based on task-driven generative adversarial network},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Segmentation of images through curve fitting analysis by
modified vandermonde matrix and modified gram-schmidt method.
<em>IETIP</em>, <em>14</em>(17), 4588–4598. (<a
href="https://doi.org/10.1049/iet-ipr.2020.0520">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {An algorithm is proposed for segmentation of digital images using the curve fitting method based on modified Vandermonde matrix and modified Gram-Schmidt method. Modified Vandermonde matrix is applied to linearly smoothed histogram data to find the coefficients of a polynomial of a given degree for fitting the data in a least square sense. Modified Gram-Schmidt method is applied to get polynomial coefficients to minimise the least square distance during the calculation. The threshold value is computed by locating the minimum number of pixels from the grey-level value. Threshold value is used with that of the input image to generate an output segmented image. The output segmented image is further improved by applying morphological operation which has made the segmented edge lines and regions free from any extraneous pixels. The final segmented image has been found to have sharp edges and filled in a region within the image contours. The experimental results show that the proposed method has produced superior performance compared to other state-of-art algorithms. Further, it is observed from the results that the proposed algorithm takes less program execution time than others in vogue algorithms.},
  archive      = {J_IETIP},
  author       = {Kuldip Acharya and Dibyendu Ghoshal and Bidyut K. Bhattacharyya},
  doi          = {10.1049/iet-ipr.2020.0520},
  journal      = {IET Image Processing},
  month        = {12},
  number       = {17},
  pages        = {4588-4598},
  shortjournal = {IET Image Process.},
  title        = {Segmentation of images through curve fitting analysis by modified vandermonde matrix and modified gram-schmidt method},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Flow-based frame interpolation networks combined with
occlusion-aware mask estimation. <em>IETIP</em>, <em>14</em>(17),
4579–4587. (<a href="https://doi.org/10.1049/iet-ipr.2020.0586">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Frame interpolation is one of the most challenging tasks in the video processing field. Recent advances have demonstrated that the deep learning-based frame interpolation methods are promising. However, the experiments show that most existing deep learning-based methods have the same problem as traditional methods. When these algorithms handle severe occlusions, they will produce distortions, especially around the motion boundaries. To better synthesise the image of the motion areas, the authors design a mask-guided frame synthesis model, which consists of multiple components, based on deep convolutional neural networks. The proposed model first estimates the asymmetric bi-directional optical flows from the intermediate frame to the input frames. Then it estimates the occlusion-aware masks, which can compensate for the optical flow inaccuracy based on optical flows and correlation information. Finally, the warped frames are adaptively fused under the guidance of the masks to generate a high-quality intermediate frame. Furthermore, to generate more realistic video frames, they train the network model with the pixel-based loss and the feature-based loss in a step-by-step way. In the experiment, they analyse the proposed model and compare it with the high-performance methods, both qualitative and quantitative results show that their method performs better.},
  archive      = {J_IETIP},
  author       = {Dacheng Zhang and Weimin Lei and Wei Zhang and Xinyi Chen},
  doi          = {10.1049/iet-ipr.2020.0586},
  journal      = {IET Image Processing},
  month        = {12},
  number       = {17},
  pages        = {4579-4587},
  shortjournal = {IET Image Process.},
  title        = {Flow-based frame interpolation networks combined with occlusion-aware mask estimation},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Microaneurysms segmentation and diabetic retinopathy
detection by learning discriminative representations. <em>IETIP</em>,
<em>14</em>(17), 4571–4578. (<a
href="https://doi.org/10.1049/iet-ipr.2019.0804">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning techniques are recently being used in fundus image analysis and diabetic retinopathy detection. Microaneurysms are important indicators of diabetic retinopathy progression. The authors introduce a two-stage deep learning approach for microaneurysms segmentation using multiple scales of the input with selective sampling and embedding triplet loss. The proposed approach facilitates a region proposal fully convolutional neural network trained on segmented patches and a patch-wise refinement network for improving the results suggested by the first stage hypothesis. To enhance the discriminative power of the second stage refinement network, the authors use triplet embedding loss with a selective sampling routine that dynamically assigns sampling probabilities to the oversampled class patches. This approach introduces a relative improvement over the vanilla fully convolutional neural network on the Indian Diabetic Retinopathy Image Data set segmentation data set. The proposed segmentation is incorporated in a classification model to solve two downstream tasks for diabetic retinopathy detection and referable diabetic retinopathy detection. The classification tasks are trained on the Kaggle diabetic retinopathy challenge data set and evaluated on the Messidor data. The authors show that adding the segmentation enhances the classification performance and achieves comparable performance to the state-of-the-art models.},
  archive      = {J_IETIP},
  author       = {Mhd Hasan Sarhan and Shadi Albarqouni and Mehmet Yigitsoy and Nassir Navab and Eslami Abouzar},
  doi          = {10.1049/iet-ipr.2019.0804},
  journal      = {IET Image Processing},
  month        = {12},
  number       = {17},
  pages        = {4571-4578},
  shortjournal = {IET Image Process.},
  title        = {Microaneurysms segmentation and diabetic retinopathy detection by learning discriminative representations},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Novel segmentation algorithm for jacquard patterns based on
multi-view image fusion. <em>IETIP</em>, <em>14</em>(17), 4563–4570. (<a
href="https://doi.org/10.1049/iet-ipr.2019.1264">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Pattern regeneration is one of the applications of reverse engineering technology in the textile field, which realises the process of textile-pattern regeneration-textile, and fundamentally provides an intelligent design means of textile. At present, the method of pattern regeneration in the jacquard fabric is to use the image segmentation algorithm to segment the image digitalised by unidirectional imaging, and then the segmented pattern could be identified to regeneration for the design of new fabrics. However, due to the concave and convex pattern textures on the surface of jacquard fabric, the traditional unidirectional imaging method cannot be used for the full characterisation of its structural information, resulting in unsatisfactory pattern segmentation effect. To solve this problem, a novel segmentation algorithm for jacquard patterns based on multi-view image fusion was proposed in this study. Based on multi-view image acquisition and fusion, the pattern image of jacquard fabric could be cluster-segmented by extracting the complete texture information of the fused image and the actual colour information of the calibrated image. Compared with the traditional unidirectional imaging method, the experimental results show that the enhanced texture information of the fused image is more workable for the pattern segmentation, it validates the effectiveness of the proposed method.},
  archive      = {J_IETIP},
  author       = {Wenzhen Wang and Na Deng and Binjie Xin and Yiliang Wang and Shuaigang Lu},
  doi          = {10.1049/iet-ipr.2019.1264},
  journal      = {IET Image Processing},
  month        = {12},
  number       = {17},
  pages        = {4563-4570},
  shortjournal = {IET Image Process.},
  title        = {Novel segmentation algorithm for jacquard patterns based on multi-view image fusion},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). MPA-net: Multi-path attention stereo matching network.
<em>IETIP</em>, <em>14</em>(17), 4554–4562. (<a
href="https://doi.org/10.1049/iet-ipr.2020.0660">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A novel learning-based end-to-end network for stereo matching, named Multi-path Attention Stereo Matching (MPA-Net), is introduced in this study. Different from existing methods, the multi-path attention aggregation module is designed firstly, named MPA, which is a unified structure using three different parallel layers with a respective attention mechanism to extract the multi-scale informational features. Secondly, the method of cost volume construction, which differs from the traditional stereo matching methods, is extended. And then, the absolute difference between two input features is calculated. Furthermore, a u-shaped structure with 3D attention gate is selected as the encoder-decoder module. Specifically, the module is used to fuse the encoding features to their corresponding decoding features under the supervision of the authors&#39; attention gate with skip-connection, and thus exploit more significant information for matching cost regularisation and disparity prediction. Finally, specific experiments are conducted to evaluate their network on SceneFlow, KITTI2012 and KITTI2015 data sets. The results show that their method achieves a better improvement in disparity maps prediction compared with some existing state-of-the-art methods on KITTI benchmark.},
  archive      = {J_IETIP},
  author       = {Haiwei Sang and Zuliu Yang and Xiaowei Yang and Yong Zhao},
  doi          = {10.1049/iet-ipr.2020.0660},
  journal      = {IET Image Processing},
  month        = {12},
  number       = {17},
  pages        = {4554-4562},
  shortjournal = {IET Image Process.},
  title        = {MPA-net: Multi-path attention stereo matching network},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Fast and robust superpixel generation method.
<em>IETIP</em>, <em>14</em>(17), 4543–4553. (<a
href="https://doi.org/10.1049/iet-ipr.2020.1179">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Superpixel segmentation approach, as a preprocessing step in computer vision tasks, groups pixels into perceptually coherence atomic regions to replace the pixel grid in images and reduce the primitives and redundancy of subsequent works. In this study, the authors proposed a fast and robust superpixel generation method based on non-iterative framework with the constraint of linear path. They collected neighbouring pixels as the initial superpixels one by one in the conventional order at first. To make the superpixels attach to the most object boundaries well and robust to noise, they defined a new distance measurement between pixels and superpixel seeds by considering the colour difference of pixels in the neighbourhood and along the linear path from the pixel to the seed. Meanwhile, they proposed a new way to set parameters adaptively based on the intrinsic quality of images. Then, they refined the initial superpixels by merging the smallest ones until the number of superpixel meets the expectation. The experimental results on clean and noisy images demonstrate that the proposed method is effective and presents a competitive performance in computational efficiency with the state-of-the-art real-time method.},
  archive      = {J_IETIP},
  author       = {Yongxia Zhang and Qiang Guo and Yongsheng Zhang and Caiming Zhang},
  doi          = {10.1049/iet-ipr.2020.1179},
  journal      = {IET Image Processing},
  month        = {12},
  number       = {17},
  pages        = {4543-4553},
  shortjournal = {IET Image Process.},
  title        = {Fast and robust superpixel generation method},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Composite attacks-based copy-move image forgery detection
using AKAZE and FAST with automatic contrast thresholding.
<em>IETIP</em>, <em>14</em>(17), 4528–4542. (<a
href="https://doi.org/10.1049/iet-ipr.2020.1118">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Copy-move image forgery is one of the most popular image tampering technique which can be performed for vicious purposes. In this forgery technique, selected region is copied and pasted at different locations on the same image to produce a manipulated image. Such forgery is denigratory as it can alter the image content by hiding or appending visual information. In this study, the authors propose a novel keypoint-based technique to detect forged images sustaining composite attacks consisting of various combinations of geometrical and post-processing operations. In the authors&#39; method, AKAZE and FAST techniques are used to extract keypoints from the image. Non-maximal value suppression with automatic contrast thresholding is performed during FAST keypoint extraction. SIFT and DAISY descriptors are computed corresponding to extracted keypoints. PCA is applied over SIFT and DAISY descriptors to discard lower components which are sensitive to distortions occurred in images. They apply a correlation-based nearest neighbour search technique to detect similarity among keypoint descriptors. HDBSCAN algorithm is applied to obtain matched keypoint clusters. Further, RANSAC algorithm is utilised for removal of keypoint outliers. In comparison to state-of-the-art techniques, their approach achieve high F-measure (%) and low FPR (%) for image-level as well as pixel-level copy-move forgery detection.},
  archive      = {J_IETIP},
  author       = {Anuja Dixit and Soumen Bag},
  doi          = {10.1049/iet-ipr.2020.1118},
  journal      = {IET Image Processing},
  month        = {12},
  number       = {17},
  pages        = {4528-4542},
  shortjournal = {IET Image Process.},
  title        = {Composite attacks-based copy-move image forgery detection using AKAZE and FAST with automatic contrast thresholding},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Generative adversarial image super-resolution network for
multiple degradations. <em>IETIP</em>, <em>14</em>(17), 4520–4527. (<a
href="https://doi.org/10.1049/iet-ipr.2020.1176">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The existing single image super-resolution methods based on deep learning cannot handle multiple degradations well, and the generated image tends to be blurred and over-smoothed due to poor generalisation ability. In this study, the authors propose a method based on a generative adversarial network (GAN) to deal with multiple degradations. In the generator network, blur kernel and noise level are used as input through dimensionality stretching strategy preprocessing to make full use of prior knowledge. In addition, three discriminators with different scales are used in the discriminator network to pay attention to the reconstruction of image details while focusing on the global consistency of the image. For the problems of vanishing gradient and mode collapse existing in GAN-based methods, a gradient penalty term is added in the loss function. Extensive experiments demonstrate that the proposed method not only can handle multiple degradations to obtain state-of-the-art performance, but also deliver visually credible results in real scenes.},
  archive      = {J_IETIP},
  author       = {Hong Lin and Jing Fan and Yangyi Zhang and Dewei Peng},
  doi          = {10.1049/iet-ipr.2020.1176},
  journal      = {IET Image Processing},
  month        = {12},
  number       = {17},
  pages        = {4520-4527},
  shortjournal = {IET Image Process.},
  title        = {Generative adversarial image super-resolution network for multiple degradations},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Simple accurate model-based phase diversity phase retrieval
algorithm for wavefront sensing in high-resolution optical imaging
systems. <em>IETIP</em>, <em>14</em>(17), 4513–4519. (<a
href="https://doi.org/10.1049/iet-ipr.2020.1075">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In optical imaging systems, the aberration is an important factor that impedes realising diffraction-limited imaging. Accurate wavefront sensing and control play important role in modern high-resolution optical imaging systems nowadays. In this study, a simple model-based phase retrieval algorithm is proposed for accurate efficient wavefront sensing with high dynamic range. In the authors’ algorithm, a wavefront is represented by the Zernike polynomials, and the Zernike coefficients are solved by the least-squares-based non-linear optimisation method, i.e. the Lederberg–Marquardt algorithm, with multiple phase-diversity images. The numerical results show that the proposed algorithm is capable of retrieving wavefront with a large dynamic range up to seven wavelength and robust to noise. In comparison, the proposed algorithm is more efficient than the existing model-based technique and more accurate than existing Fourier - transformation-based iterative techniques.},
  archive      = {J_IETIP},
  author       = {Shun Qin and Yongbing Zhang and Haoqian Wang and Wai Kin Chan},
  doi          = {10.1049/iet-ipr.2020.1075},
  journal      = {IET Image Processing},
  month        = {12},
  number       = {17},
  pages        = {4513-4519},
  shortjournal = {IET Image Process.},
  title        = {Simple accurate model-based phase diversity phase retrieval algorithm for wavefront sensing in high-resolution optical imaging systems},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Improved algorithm for multiple sclerosis diagnosis in MRI
using convolutional neural network. <em>IETIP</em>, <em>14</em>(17),
4507–4512. (<a href="https://doi.org/10.1049/iet-ipr.2019.0366">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Magnetic resonance imaging (MRI) is used to diagnose multiple sclerosis (MS) disease lesions in the brain. Diagnosis of MS disease from MRI images is an important and vital thing in today&#39;s world. This disease can cause many problems for people who have this disease and reduce the life expectancy in them. So, a strong approach is needed to overcome the challenges in this field. In this study, a method is presented based on convolutional neural networks to detect MS disease lesions from MRI. Four layers of convolution, two layers of pooling, three layers of ReLU are applied, and instead of a fully connected layer, a convolutional layer with a filter size of 1 × 1 has been used to reduce network parameters. Also, for network training, stochastic gradient descent with momentum has been used such that itgreatly improves the speed of learning. Convolutional neural network has a strong potential for MS disease diagnosis and provides good results without the need for lesions segmentation. Also, it has a low sensitivity to the challenges of blurring and different contrasts, and it shows a good performance. The proposed method in this study shows 99.66% accuracy, 99.98% sensitivity and 99.33% specificity.},
  archive      = {J_IETIP},
  author       = {Azam Soltani and Saeed Nasri},
  doi          = {10.1049/iet-ipr.2019.0366},
  journal      = {IET Image Processing},
  month        = {12},
  number       = {17},
  pages        = {4507-4512},
  shortjournal = {IET Image Process.},
  title        = {Improved algorithm for multiple sclerosis diagnosis in MRI using convolutional neural network},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Non-iterative blind deconvolution algorithm based on
power-law distribution. <em>IETIP</em>, <em>14</em>(17), 4499–4506. (<a
href="https://doi.org/10.1049/iet-ipr.2020.0647">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The spectral amplitude of most natural images is approximately isotropic and follows the power law. In this study, the authors propose a new non-iterative blind image deconvolution algorithm that builds an isosceles curve model to approximate the spectrum amplitude of the real image. In the authors’ proposed algorithm, the optical transfer function (OTF) is obtained by comparing the reconstructed and degraded spectra. Then they employ the integrated multidirectional comprehensive estimation to reduce the OTF estimation error. The restored image is then obtained by applying the estimated OTF and the Wiener filter. Experiments on image deconvolution tasks indicate that the proposed algorithm provides a significant performance gain by obtaining an accurate OTF, reducing ringing artefacts compared with existing algorithms, and realising real-time image restoration.},
  archive      = {J_IETIP},
  author       = {Weizhe Gao and Xuebin Xu and Yikang Yang and Zhiguang Zhang},
  doi          = {10.1049/iet-ipr.2020.0647},
  journal      = {IET Image Processing},
  month        = {12},
  number       = {17},
  pages        = {4499-4506},
  shortjournal = {IET Image Process.},
  title        = {Non-iterative blind deconvolution algorithm based on power-law distribution},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Improved probabilistic decision-based trimmed median filter
for detection and removal of high-density impulsive noise.
<em>IETIP</em>, <em>14</em>(17), 4486–4498. (<a
href="https://doi.org/10.1049/iet-ipr.2019.1240">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study focuses on the detection and expulsion of noisy pixels from an image contaminated by impulsive noise. A noise detection approach is developed to avoid the misinterpretation of noise-free pixel as noisy. In order to design the noise removal algorithm, a probabilistic decision-based improved trimmed median filter (PDITMF) algorithm is proposed which is intended to work out the conflict related to the even number of noise-free pixels in the trimmed median filter. It deploys two new estimation techniques for de-noising, namely, improved trimmed median filter (ITMF) and patch else ITMF (PEITMF) as per noise density. At last, the noise detection approach is applied in the proposed PDITMF to build up a new technique called a probabilistic decision-based adaptive improved trimmed median filter (PDAITMF) algorithm. The proposed algorithms, PDITMF and PDAITMF experiment with many standard sample images. Simulation results show the proposed algorithms are capable of detecting and de-noising the contaminated image very efficiently and have a better visual representation. Under the authors&#39; knowledge, the PDAITMF outperforms recently reported algorithms in context to peak signal-to-noise ratio as well as an image enhancement factor with the lower execution time at all noise densities.},
  archive      = {J_IETIP},
  author       = {Amit Prakash Sen and Nirmal Kumar Rout},
  doi          = {10.1049/iet-ipr.2019.1240},
  journal      = {IET Image Processing},
  month        = {12},
  number       = {17},
  pages        = {4486-4498},
  shortjournal = {IET Image Process.},
  title        = {Improved probabilistic decision-based trimmed median filter for detection and removal of high-density impulsive noise},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Performance enhancement of image segmentation analysis for
multi-grade tumour classification in MRI image. <em>IETIP</em>,
<em>14</em>(17), 4477–4485. (<a
href="https://doi.org/10.1049/iet-ipr.2019.1363">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Medical applications have a massive footprint in human&#39;s day-to-day life. Among that, MRI has a significant role, as it incorporates a significant impact on a brain tumour. Segmenting the tumour from MRI is substantial, but it is a time-consuming process. Both the normal and abnormal tissues found in the brain look similar, which increases the difficulty of the tumour detection process. The digital image needs to be processed to obtain an exact tumour detection result. The tumour detection process comprises five different stages, such as pre-processing, segmentation, feature extraction, feature selection, and classification. In this proposed work, hybrid wavelet Hadamard transform and grey-level co-occurrence matrix are included for feature extraction. Feature selection utilises sequential forward selection, which is an easy greedy search algorithm. This algorithm chooses only the predominant features for classification. The classification uses a hybrid support vector machine and adaptive emperor penguin optimisation. The experimental analysis shows the efficiency of the proposed work in terms of accuracy, specificity, and sensitivity values by computing the true positive, false positive, true negative, and false negative.},
  archive      = {J_IETIP},
  author       = {Rathinam Somas Kandan and Muthuvel Murugeswari},
  doi          = {10.1049/iet-ipr.2019.1363},
  journal      = {IET Image Processing},
  month        = {12},
  number       = {17},
  pages        = {4477-4485},
  shortjournal = {IET Image Process.},
  title        = {Performance enhancement of image segmentation analysis for multi-grade tumour classification in MRI image},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Balanced principal component for 3D shape recognition using
convolutional neural networks. <em>IETIP</em>, <em>14</em>(17),
4468–4476. (<a href="https://doi.org/10.1049/iet-ipr.2019.0844">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Currently, PCA (principal component analysis) is widely used in many neural networks and has become a crucial part of the convolutional neural network (CNN) feature extraction. However, whether PCA is suitable for this process remains to be elucidated. The authors proposed a new method called balanced principal component (BPC) that generates a balanced local feature and combines with CNN as a layer to cope with the fusion problem. Specifically, BPC layer includes regionalisation module and average compression PCA (AC-PCA) module. First, they used regionalisation module to generate some sub-region that focuses on the local feature in each view. Secondly, the AC-PCA module is a computational process that enlarges the feature matrix by PCA and eventually compacts the matrix to a one-dimensional (1D) vector by AC. Next, all 1D vectors are compacted by AC to obtain a multi-dimensional balance. Finally, they designed this layer with an end-to-end trainable structure to promote the feature extraction task of CNN. They addressed 3D shapes using a projection method that is pre-trained on ImageNet and migration learning on ModelNet dataset. By comparing with the state-of-the-art network, they achieved a significant gain in performance of retrieval and classification tasks.},
  archive      = {J_IETIP},
  author       = {Wenjie Luo and Han Zhang and Peng Ni and Xuedong Tian},
  doi          = {10.1049/iet-ipr.2019.0844},
  journal      = {IET Image Processing},
  month        = {12},
  number       = {17},
  pages        = {4468-4476},
  shortjournal = {IET Image Process.},
  title        = {Balanced principal component for 3D shape recognition using convolutional neural networks},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Super-pixel image segmentation algorithm based on adaptive
equalisation feature parameters. <em>IETIP</em>, <em>14</em>(17),
4461–4467. (<a href="https://doi.org/10.1049/iet-ipr.2020.0475">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image segmentation is a key step in the process of image data processing. The quality of image segmentation will directly affect the accuracy of image cognitive understanding. The purpose of image segmentation is to divide the image into regions with specific semantics. For the simple linear iterative clustering (SLIC) algorithm, the feature equalisation parameters need to be set manually during image segmentation, which results in the lack of segmentation effects and slow processing time. By introducing the theory of intermediary mathematics, an improved adaptive SLIC super-pixel algorithm is proposed, which can adaptive generate characteristic equalisation parameters according to the specific situation of the image, thereby simplifying the operation steps and improving the image segmentation effect. After experimental verification and analysis, compared with the original SLIC algorithm and several other super-pixel contrast algorithms, the algorithm in this study can effectively shorten the processing time and achieve a better segmentation effect.},
  archive      = {J_IETIP},
  author       = {Shifei Ding and Lijuan Wang and Lin Cong},
  doi          = {10.1049/iet-ipr.2020.0475},
  journal      = {IET Image Processing},
  month        = {12},
  number       = {17},
  pages        = {4461-4467},
  shortjournal = {IET Image Process.},
  title        = {Super-pixel image segmentation algorithm based on adaptive equalisation feature parameters},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Hybrid DSSCS and convolutional neural network for peripheral
blood cell recognition system. <em>IETIP</em>, <em>14</em>(17),
4450–4460. (<a href="https://doi.org/10.1049/iet-ipr.2020.0370">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this study, an efficient a deep learning architecture-based peripheral blood cell image recognition and classification is proposed using hybrid disruption-based salp-swarm and cat swarm (DSSCS)-based optimized convolutional neural networks (DSSCSCNNs) method. The DSSCSCNN method is employed to overcome the hyperparameter problem in CNN and it also helps this model to work on small peripheral blood cell data sets. In the DSSCSCNN method, the authors develop a binary coding technique that converts parameter tuning problems into an optimization problem. The original salp swarm algorithm is enhanced using a disruptive operator and salp swarm optimization algorithm to form the novel DSSCS algorithm which increases the diversity of the search space by providing higher classification accuracy. In this study, the CNNs use Vgg-16 architecture is used for training purposes. The global classification accuracy obtained when trained with the Vgg-16 model is 97%. This method establishes a fine-tuning process to develop a classifier trained using 15,976 images acquired from clinical practice. The proposed model gives improved performance in terms of accuracy, specificity, and sensitivity. In the WBC determination, the proposed approach has shown 100% achievement. It also provides the best overall classification accuracy of 99%.},
  archive      = {J_IETIP},
  author       = {Shivani Joshi and Rajiv Kumar and Avinash Dwivedi},
  doi          = {10.1049/iet-ipr.2020.0370},
  journal      = {IET Image Processing},
  month        = {12},
  number       = {17},
  pages        = {4450-4460},
  shortjournal = {IET Image Process.},
  title        = {Hybrid DSSCS and convolutional neural network for peripheral blood cell recognition system},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Bow image retrieval method based on SSD target detection.
<em>IETIP</em>, <em>14</em>(17), 4441–4449. (<a
href="https://doi.org/10.1049/iet-ipr.2020.0478">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The query image is usually a simple and single object in image retrieval, and the reference images in the database usually have many distractions. The precision of image retrieval can be greatly improved If the target regions in the database image are extracted during retrieval. So this paper proposes a Bow image retrieval method based on SSD target detection. First, the training gallery is manually annotated to record the location and size information. Second, the SSD target detection model is trained with the labeled training gallery to obtain the target object SSD model. Third, the SSD model is used to locate the similar target regions of the reference image and the query graph. Finally, the target region information is mapped into the convolutional features, and these feature vectors are used for image similarity matching. The performance of the proposed method is evaluated on Paris6k, Oxford5k, Paris106k and Oxford105k databases. The experimental results show that the accuracy of image retrieval will be greatly improved by adding optimization methods in the proposed image retrieval framework. The image retrieval accuracy of this method is higher than that of similar methods in recent years.},
  archive      = {J_IETIP},
  author       = {Kaiyang Liao and Bing Fan and Yuanlin Zheng and Guangfeng Lin and Congjun Cao},
  doi          = {10.1049/iet-ipr.2020.0478},
  journal      = {IET Image Processing},
  month        = {12},
  number       = {17},
  pages        = {4441-4449},
  shortjournal = {IET Image Process.},
  title        = {Bow image retrieval method based on SSD target detection},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Efficient computer-aided diagnosis technique for leukaemia
cancer detection. <em>IETIP</em>, <em>14</em>(17), 4435–4440. (<a
href="https://doi.org/10.1049/iet-ipr.2020.0978">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Computer-aided diagnosis (CAD) is a common tool for the detection of diseases, particularly different types of cancers, based on medical images. Digital image processing thus plays a significant role in the processing and analysis of medical images for diseases identification and detection purposes. In this study, an efficient CAD system for the acute lymphoblastic leukaemia (ALL) detection is proposed. The proposed approach entails two phases. In the first phase, the white blood cells (WBCs) are segmented from the microscopic blood image. The second phase involves extracting important features, such as shape and texture features from the segmented cells. Eventually, on the extracted features, Naïve Bayes and k-nearest neighbour classifier techniques are implemented to identify the segmented cells into normal and abnormal cells. The performance of the proposed approach has been assessed through comprehensive experiments carried out on the well-known ALL-IDB data set of microscopic blood images. The experimental results demonstrate the superior performance of the proposed approach over the state-of-the-art in terms of accuracy rate in which achieved 98.7%.},
  archive      = {J_IETIP},
  author       = {Alan Anwer Abdulla},
  doi          = {10.1049/iet-ipr.2020.0978},
  journal      = {IET Image Processing},
  month        = {12},
  number       = {17},
  pages        = {4435-4440},
  shortjournal = {IET Image Process.},
  title        = {Efficient computer-aided diagnosis technique for leukaemia cancer detection},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Computational complexity of fractal image compression
algorithm. <em>IETIP</em>, <em>14</em>(17), 4425–4434. (<a
href="https://doi.org/10.1049/iet-ipr.2019.0489">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study presents insights into the computational complexity of fractal image compression (FIC) algorithms. Unlike JPEG, a fractal encoder necessitates more CPU time in contrast to the decoder. The study examines various factors that impact the encoder and its computational cost. Many researchers have dedicated themselves to the field of fractal encoding to overcome the computational cost of the FIC algorithm. Here, this study offers a look over the approaches in the aspect of time complexity. The automated baseline fractal compression algorithm is studied to demonstrate the understanding of delay in the encoder. The study establishes how various approaches trade-off between the quality of decoder, compression ratio, and CPU time. The experiment section shows the bargain between fidelity criteria of the baseline algorithm.},
  archive      = {J_IETIP},
  author       = {Richa Gupta and Deepti Mehrotra and Rajesh Kumar Tyagi},
  doi          = {10.1049/iet-ipr.2019.0489},
  journal      = {IET Image Processing},
  month        = {12},
  number       = {17},
  pages        = {4425-4434},
  shortjournal = {IET Image Process.},
  title        = {Computational complexity of fractal image compression algorithm},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Two-scale fusion method of infrared and visible images via
parallel saliency features. <em>IETIP</em>, <em>14</em>(16), 4412–4423.
(<a href="https://doi.org/10.1049/iet-ipr.2020.1165">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {An efficient fusion method of infrared and visible images is proposed based on the parallel saliency features. The method integrates significant feature information from the source images of multiple imaging modalities into a single fused image in a multi-scale domain, while suppressing visual artefacts and retaining more detail and texture information. First, the input images are decomposed into two-scale image representations, namely the base and detail layers, using a Gaussian filter. Second, the parallel saliency features of the high contrast and detail textures are captured to acquire the saliency maps. The contrast saliency weight map of the base layers based on the weighted local intensity energy aims to highlight the salient targets in infrared images and preserve the high-intensity regions in visible images, while the detail saliency weight map of the detail layers using the structure tensor to extract the detail texture information. Finally, the final image is reconstructed by the fused base and detail layers. Sufficient experimental results convincingly demonstrate that the presented method can achieve a comparable or superior performance compared with several state-of-the-art fusion methods via subjective assessments and objective evaluations, and it is more suitable for the practical applications due to the high computing efficiency.},
  archive      = {J_IETIP},
  author       = {Chaowei Duan and Changda Xing and Shanshan Lu and Zhisheng Wang},
  doi          = {10.1049/iet-ipr.2020.1165},
  journal      = {IET Image Processing},
  month        = {12},
  number       = {16},
  pages        = {4412-4423},
  shortjournal = {IET Image Process.},
  title        = {Two-scale fusion method of infrared and visible images via parallel saliency features},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Efficient image structural similarity quality assessment
method using image regularised feature. <em>IETIP</em>, <em>14</em>(16),
4401–4411. (<a href="https://doi.org/10.1049/iet-ipr.2019.1570">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image regularised features play a critical role in image processing domain, by integrating regularised feature and structural similarity, a new full-reference image assessment method (IRF_SSIM) is proposed in this study. As well known, the gradient operator always be used to capture the edge information of the image, while the total variational regularised features can be adopted to calculate the detailed change information of image contrast and texture, as well as noise removal and edge retention. Therefore, the IRF_SSIM method extends the gradient features into the image regularised features to measure the structural changes in the image. In addition, image quality is also affected by variations of luminance and contrast. For a more comprehensive image quality assessment, the IRF_SSIM method considers the changes in structure, luminance and contrast simultaneously. In other words, the total image quality is estimated by structural similarity calculated by integrating the effects of image structure, luminance and contrast changes. Comparing with the representative methods, the experimental results illustrate that the IRF_SSIM method is highly consistent with the subjective assessment results.},
  archive      = {J_IETIP},
  author       = {Yajing Li and Baoxiang Huang and Huan Yang and Guojia Hou and Pengfei Zhang and Jinming Duan},
  doi          = {10.1049/iet-ipr.2019.1570},
  journal      = {IET Image Processing},
  month        = {12},
  number       = {16},
  pages        = {4401-4411},
  shortjournal = {IET Image Process.},
  title        = {Efficient image structural similarity quality assessment method using image regularised feature},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Hybridisation of single-image super-resolution with
edge-aware multi-focus image fusion for edge enrichment. <em>IETIP</em>,
<em>14</em>(16), 4392–4400. (<a
href="https://doi.org/10.1049/iet-ipr.2020.0527">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To break the curtailment of digital imaging and retrieve appropriate information with different focused images, a novel edge-aware multi-focus image fusion is proposed by integrating the single-image super-resolution (SISR) method along with the edge-preserving filters. Initially, the multi-focus images are converted to high resolution images by estimating missing high frequency details from its blurred versions. With acquired high resolution images, smoothing by median and the anisotropic diffusion filters are performed to extract focused regions. An initial weight map is constructed by using maximum selection of pixel intensities of the difference images obtained with filtering. The precision of estimated weight map is further improved by exhibiting morphological operations and guided filter. Finally, the images are fused based on the optimised decision map. Simulation results of proposed fusion work are evaluated with seven metrics and the values are compared with different state-of-the-art methods. Both the quantitative and qualitative analyses showed the excellence of proposed work over other fusion methods.},
  archive      = {J_IETIP},
  author       = {Sreeja Gopalakrishnan and Saraniya Ovireddy},
  doi          = {10.1049/iet-ipr.2020.0527},
  journal      = {IET Image Processing},
  month        = {12},
  number       = {16},
  pages        = {4392-4400},
  shortjournal = {IET Image Process.},
  title        = {Hybridisation of single-image super-resolution with edge-aware multi-focus image fusion for edge enrichment},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Residual-wider convolutional neural network for image
recognition. <em>IETIP</em>, <em>14</em>(16), 4385–4391. (<a
href="https://doi.org/10.1049/iet-ipr.2020.0726">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent works show that the performance of convolutional neural networks (CNNs) could be improved by making the network wider and introducing residual connections. For example, the Inception architecture is one of the classical models. However, the structure of the series of Inception is complex, and there are more convolution layers and existing redundant image feature information. In order to further improve the performance and observability of CNNs, a novel type of network structure based on two modules, wider module and residual module, is proposed to make full advantage of the information of the image and learn more abundant features in this study, which is called residual-wider network (R-WN). The structure of R-WN is easy to understand and adopts modular design method. Experiential results demonstrate that the proposed R-WN with optimal structure and much fewer convolution layers could achieve much better image recognition results on four classical open data sets (MNIST, CIFAR10, SVHN, and Oxford Flowers 17).},
  archive      = {J_IETIP},
  author       = {Guoqiang Li and Wenhua Chen and Chao Mu},
  doi          = {10.1049/iet-ipr.2020.0726},
  journal      = {IET Image Processing},
  month        = {12},
  number       = {16},
  pages        = {4385-4391},
  shortjournal = {IET Image Process.},
  title        = {Residual-wider convolutional neural network for image recognition},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Image compact-resolution and reconstruction using reversible
network. <em>IETIP</em>, <em>14</em>(16), 4376–4384. (<a
href="https://doi.org/10.1049/iet-ipr.2019.1652">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The dual problem of image super-resolution (SR), which is referred to as compact-resolution (CR), and the corresponding image reconstruction are studied. These two problems have been studied independently by the researchers. In this study, a novel model for image CR and the corresponding reconstruction using the reversible network has been proposed. The reversible network has two properties, the first property, lossless information forwarding, which makes the compact-resolved image retain more information from the original HR image. The second property, bidirectional mapping, by which the forward and reverse propagation of a reversible network can be utilised to implement image CR and reconstruction, respectively, i.e. using the reverse process of image CR to guide the reconstruction. In addition, the utilisation of a reversible network may reduce the size of the model. The superiority of the proposed model was demonstrated by comparing its performance with the state-of-the-art methods on four well-known benchmark datasets.},
  archive      = {J_IETIP},
  author       = {Jieming Yang and Hongwei Ge and Jinlong Yang and Yubing Tong},
  doi          = {10.1049/iet-ipr.2019.1652},
  journal      = {IET Image Processing},
  month        = {12},
  number       = {16},
  pages        = {4376-4384},
  shortjournal = {IET Image Process.},
  title        = {Image compact-resolution and reconstruction using reversible network},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Design of optimal blind watermarking technique based on
MOEA/d. <em>IETIP</em>, <em>14</em>(16), 4368–4375. (<a
href="https://doi.org/10.1049/iet-ipr.2019.1551">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In image watermarking, the quality of the watermarked image and the watermark&#39;s robustness are the main indicators for evaluating the performance of the watermarking scheme, but they contradict each other, making it difficult to design a rational watermarking method. In this study, the multi-objective evolutionary algorithm based on decomposition (MOEA/D) is employed to solve the multi-objective optimisation problem to ensure the balance between contradictory objectives. The watermark is embedded by selecting the regions with the lowest human visual characteristics in the host image and modifying the frequency coefficients of the discrete cosine transform. At this time, the determination of the positions of embedding coefficients and parameters related to the modifications results in the multi-objective optimisation problem on the robustness and imperceptibility, and the optimal solution is determined by finding the approximate Pareto set. Simulation results for various attacks on watermarked images demonstrate the effectiveness of the proposed watermarking technique.},
  archive      = {J_IETIP},
  author       = {Kwanghyok Mun and Cholmin Son},
  doi          = {10.1049/iet-ipr.2019.1551},
  journal      = {IET Image Processing},
  month        = {12},
  number       = {16},
  pages        = {4368-4375},
  shortjournal = {IET Image Process.},
  title        = {Design of optimal blind watermarking technique based on MOEA/D},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). MaskHunter: Real-time object detection of face masks during
the COVID-19 pandemic. <em>IETIP</em>, <em>14</em>(16), 4359–4367. (<a
href="https://doi.org/10.1049/iet-ipr.2020.1119">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to the COVID-19 pandemic at present, it is necessary to detect whether pedestrians in public places wear face masks or not for preventing the spread of novel coronavirus. The pedestrian flow in public places is large, and it puts forward higher requirements for the accuracy and speed of real-time mask detection. Improving the face mask detection effect especially in the night environment is a challenging problem. A novel object detector namely MaskHunter is proposed in this study for the real-time mask detection. Specifically, the authors propose novel effective structures of backbone, neck and prediction head based on YOLOv4 series, which achieves the state-of-the-art performance and a novel improved Mosaic data augmentation method. Moreover, they propose a novel mask-guided module to enhance the discrimination ability of face mask especially in the night environment. As a consequent, experiments show that MaskHunter achieves better detection performance for real-time mask detection compared with other obtained models in this scenario.},
  archive      = {J_IETIP},
  author       = {Zhihao Cao and Mingfeng Shao and Li Xu and Shaomin Mu and Hongchun Qu},
  doi          = {10.1049/iet-ipr.2020.1119},
  journal      = {IET Image Processing},
  month        = {12},
  number       = {16},
  pages        = {4359-4367},
  shortjournal = {IET Image Process.},
  title        = {MaskHunter: Real-time object detection of face masks during the COVID-19 pandemic},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). SE–RWNN: An synergistic evolution and randomly wired neural
network-based model for adaptive underwater image enhancement.
<em>IETIP</em>, <em>14</em>(16), 4349–4358. (<a
href="https://doi.org/10.1049/iet-ipr.2019.1677">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Under water images are likely to suffer from severe degradation such as colour distortion, low contrast, and fuzz content, caused by the absorption and scattering effects of the water. To improve the visual appearance of the image, the authors present an adaptive algorithm for effective underwater image enhancement using a randomly wired neural network (RWNN) and synergistic evolution (SE). In doing so, they sequentially conduct colours adjustment, contrast improvement and luminance enhancement while enhancing details by an edge-preserving technique. To set up the system, they develop a multi-strategy cooperating evolution algorithm to figure out the optimal parameter values. Extensive experimental results show that the proposed model improves both subjectively and quantitatively the quality of underwater images.},
  archive      = {J_IETIP},
  author       = {Yang Li and Rong Chen},
  doi          = {10.1049/iet-ipr.2019.1677},
  journal      = {IET Image Processing},
  month        = {12},
  number       = {16},
  pages        = {4349-4358},
  shortjournal = {IET Image Process.},
  title        = {SE–RWNN: An synergistic evolution and randomly wired neural network-based model for adaptive underwater image enhancement},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Novel superpixel-based algorithm for segmenting lung images
via convolutional neural network and random forest. <em>IETIP</em>,
<em>14</em>(16), 4340–4348. (<a
href="https://doi.org/10.1049/iet-ipr.2019.1171">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurately segmenting lungs from CT images is a fundamental step for quantitative analysis of lung diseases. However, it is still a challenging task because of some interferential factors, such as juxta-pleural nodules, pulmonary inflammation, as well as individual anatomical varieties. In this study, with the combination of a superpixel approach and a hybrid model composed of convolutional neural network and random forest (CNN-RF), the authors propose a novel algorithm to segment lungs from CT images in an automatic and accurate fashion. The authors&#39; lung segmentation covers three main stages: image preprocessing, lung segmenting and segmentation refining. A lung CT image denoised with a fractional-order grey similarity approach is first segmented to a set of superpixels, and the CNN-RF model is then employed to classify the superpixels and identify lungs from the CT image. The segmentation result is further refined by separating the left and right lungs, eliminating trachea, and correcting lung contours. Experiments show that their algorithm can generate more accurate lung segmentation results with 94.98% Jaccard&#39;s index and 97.99% Dice similarity coefficient, compared with ground truths, and it achieved better results compared with several feature-based machine learning techniques and current methods on lung segmentation.},
  archive      = {J_IETIP},
  author       = {Caixia Liu and Mingyong Pang and Ruibin Zhao},
  doi          = {10.1049/iet-ipr.2019.1171},
  journal      = {IET Image Processing},
  month        = {12},
  number       = {16},
  pages        = {4340-4348},
  shortjournal = {IET Image Process.},
  title        = {Novel superpixel-based algorithm for segmenting lung images via convolutional neural network and random forest},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Comprehensive evaluation of image enhancement for
unsupervised image description and matching. <em>IETIP</em>,
<em>14</em>(16), 4329–4339. (<a
href="https://doi.org/10.1049/iet-ipr.2020.1129">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The performance of an image enhancer is usually evaluated either perceptually or functionally. The perceptual evaluation is carried out from a human perspective, i.e. by considering features related to human perception of image content and details. The functional evaluation is made instead from a machine perspective, i.e. by judging the enhancer effects with a specific machine application. his work proposes a comprehensive, empirical evaluation accounting for both perceptual and functional aspects. Precisely, 13 enhancers lowering undesired illumination effects are considered within the keypoint based image description and matching task, which is relevant to many computer vision fields. Each enhancer is first evaluated perceptually, then it is employed as a pre-processing step of the popular algorithms SIFT and ORB and judged by measuring how its use influences the performance of these algorithms. his study, conducted on a freely available data set, shows that the enhancement generally improves the perceptual features of the input image as well as the SIFT and ORB performance. More importantly, it reveals the existence of a correlation among some of their perceptual and functional measures. In this way, this work contributes to promote a more aware use of enhancement techniques within the mage description and matching task.},
  archive      = {J_IETIP},
  author       = {Michela Lecca and Alessandro Torresani and Fabio Remondino},
  doi          = {10.1049/iet-ipr.2020.1129},
  journal      = {IET Image Processing},
  month        = {12},
  number       = {16},
  pages        = {4329-4339},
  shortjournal = {IET Image Process.},
  title        = {Comprehensive evaluation of image enhancement for unsupervised image description and matching},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Optimised hybrid classifiers for automatic HEp-2 cell
classification. <em>IETIP</em>, <em>14</em>(16), 4316–4328. (<a
href="https://doi.org/10.1049/iet-ipr.2020.0046">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper introduces a new HEp-2 cell classification model under two phases. In the initial phase, the input image is segmented using the morphological operations (opening and closing), and the segmented image is given for Convolutional Neural Network (CNN) classifier that gives the classified output. In the second phase, the given input is processed under (i) segmentation process (ii) Feature Extraction, and (iii) Classification. From the segmented images; the features like Gray level co-occurrence Matrix TGLCM) and Gray level Run Length Matrix (GLRM) are extracted. After extracting the features, they are subjected to a classification process, where Neural Network (NN) is used. Finally, the mean of both classified output (first phase and second phase) is considered to be the final classified output. As the main contribution, to enhance the classification accuracy, the hidden neurons of both classifiers (CNN and NN) are optimally chosen during the classification process. To make this possible, this paper aims to propose a new Randomized Update based Grey Wolf Optimization (RP-GWO) algorithm. Finally, the performance of the implemented approach is compared over other conventional approaches and its superiority is proven with respect to certain measures.},
  archive      = {J_IETIP},
  author       = {Manju Chariyamparambil Chandran and Marianthiran Victor Jose},
  doi          = {10.1049/iet-ipr.2020.0046},
  journal      = {IET Image Processing},
  month        = {12},
  number       = {16},
  pages        = {4316-4328},
  shortjournal = {IET Image Process.},
  title        = {Optimised hybrid classifiers for automatic HEp-2 cell classification},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Research on adaptive local feature enhancement in
convolutional neural networks. <em>IETIP</em>, <em>14</em>(16),
4306–4315. (<a href="https://doi.org/10.1049/iet-ipr.2020.0591">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Local feature extraction is one of the key characteristics of convolutional neural networks (CNNs). This study proposes an adaptive local feature enhancement (ALFE) model with a low-frequency general appearance-enhancement operator and a high-frequency local detail enhancement operator to improve local features of CNNs. Through supervised training, the model could adaptively adjust enhancement parameters and achieve a global-local enhancement of training images and CNNs. The performance of ALFE was first preliminarily evaluated with a self-built CNN on CIFAR-10 data set in different conditions of image augmentation and feature pooling. CNNs with ALFE could increase the top-1 accuracy compared against CNNs in the same conditions, and nearly reach the same level of performance for different pooling approaches. With only two extra adjustable parameters, this model could effectively avoid overfitting, without affecting the convergence speed of CNN. In addition, the extra burden of network complexity could be neglected. Further, experiments of three existing popular CNNs (AlexNet, VGGNet and ResNet) with ALFE were carried out on dogs versus cats, Tiny ImageNet, and SVHN data sets, respectively. The results show that ALFE is feasible for the existing popular CNN models, improving their top-1 accuracies without changing their convergence speeds.},
  archive      = {J_IETIP},
  author       = {Tongfeng Sun and Changlong Shao and Hongmei Liao and Shifei Ding and Xinzheng Xu},
  doi          = {10.1049/iet-ipr.2020.0591},
  journal      = {IET Image Processing},
  month        = {12},
  number       = {16},
  pages        = {4306-4315},
  shortjournal = {IET Image Process.},
  title        = {Research on adaptive local feature enhancement in convolutional neural networks},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). NSST domain CT–MR neurological image fusion using optimised
biologically inspired neural network. <em>IETIP</em>, <em>14</em>(16),
4291–4305. (<a href="https://doi.org/10.1049/iet-ipr.2020.0219">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Diagnostic medical imaging plays an imperative role in clinical assessment and treatment of medical abnormalities. The fusion of multimodal medical images merges complementary information present in the multi-source images and provides a better interpretation with improved diagnostic accuracy. This paper presents a CT–MR neurological image fusion method using an optimised biologically inspired neural network in nonsubsampled shearlet (NSST) domain. NSST decomposed coefficients are utilised to activate the optimised neural model using particle swarm optimisation method and to generate the firing maps. Low and high-frequency NSST subbands get fused using max-rule based on firing maps. In the optimisation process, a fitness function is evaluated based on spatial frequency and edge index of the resultant fused image. To analyse the fusion performance, extensive experiments are conducted on the different CT–MR neurological image dataset. Objective performance is evaluated based on different metrics to highlight the clarity, contrast, correlation, visual quality, complementary information, salient information, and edge information present in the fused images. Experimental results show that the proposed method is able to provide better-fused images and outperforms other existing methods in both visual and quantitative assessments.},
  archive      = {J_IETIP},
  author       = {Manisha Das and Deep Gupta and Petia Radeva and Ashwini M. Bakde},
  doi          = {10.1049/iet-ipr.2020.0219},
  journal      = {IET Image Processing},
  month        = {12},
  number       = {16},
  pages        = {4291-4305},
  shortjournal = {IET Image Process.},
  title        = {NSST domain CT–MR neurological image fusion using optimised biologically inspired neural network},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Automatic detection of multiple sclerosis lesions using mask
r-CNN on magnetic resonance scans. <em>IETIP</em>, <em>14</em>(16),
4277–4290. (<a href="https://doi.org/10.1049/iet-ipr.2020.1128">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multiple Sclerosis (MS) causes the central nervous system to malfunction due to inflammation surrounding nerve cells. Detection of MS at an early stage is very important to prevent progressive MS attacks. Clinical findings, cerebrospinal fluid examinations, the evoked potentials, magnetic resonance imaging (MRI) findings have an important role in the diagnosis and follow-up of MS. However, many of the findings on MRI may indicate brain disorders other than MS. In addition, the clinical practices accepted by physicians for MS detection are very limited. In this study, a Mask R-CNN based method in two dataset is proposed for the automatic detection of MS lesions on magnetic resonance scans. We also improved the ROI detection stage with RPN in the Mask R-CNN to easily adapt for different lesion sizes. MS lesions in different sizes in the dataset are successfully detected with 84.90% Dice similarity rate and 87.03% precision rates using the proposed method. In addition, volumetric overlap error and lesion-wise true positive rate are obtained as 12.97% and 73.75%, respectively. Moreover, performance tests of the use of different numbers of GPU hardware structures are also performed and the evaluation of its effects on processing speed is performed on experimental studies..},
  archive      = {J_IETIP},
  author       = {Mehmet Süleyman Yıldırım and Emre Dandıl},
  doi          = {10.1049/iet-ipr.2020.1128},
  journal      = {IET Image Processing},
  month        = {12},
  number       = {16},
  pages        = {4277-4290},
  shortjournal = {IET Image Process.},
  title        = {Automatic detection of multiple sclerosis lesions using mask R-CNN on magnetic resonance scans},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Diagnosis of brain tumours by MRI binarisation with variable
fuzzy level. <em>IETIP</em>, <em>14</em>(16), 4269–4276. (<a
href="https://doi.org/10.1049/iet-ipr.2019.1209">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Brain tumour detection is still a challenging problem both in medical and image processing. In this study, two scenarios are applied to diagnose human brain tumours, classical image processing (CIP) and a new algorithm called binary image with variable fuzzy level (BIVFL). Magnetic resonance imaging (MRI) process from the BraTs 2019 database. Edge detection and prognosis area cropping are parts of the high-computational CIP algorithm. For the CIP, the best filter is the combination of the horizontal and vertical modes of the Prewitt filter. In the BIVFL, by changing the fuzzy level for binarisation images two clusters are filled with tumour and no tumour areas, and the tumour area is extracted in various accuracies. Sensitivity, specificity, precision, and accuracy of the BIVFL are varied according to the fuzzy level, and the best value of sensitivity is 0.9, but the value of three other parameters is 1 for an interval of fuzzy levels. The BIVFL is a simple and fast tumour area extraction algorithm, and also it is easy to implement with digital signal processors. Histogram and power signal-to-noise ratio of the BIVFL is remarkable.},
  archive      = {J_IETIP},
  author       = {Armaghan Shirali and Nabiollah Shiri},
  doi          = {10.1049/iet-ipr.2019.1209},
  journal      = {IET Image Processing},
  month        = {12},
  number       = {16},
  pages        = {4269-4276},
  shortjournal = {IET Image Process.},
  title        = {Diagnosis of brain tumours by MRI binarisation with variable fuzzy level},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Low-complexity block tree image coder for visual sensor
networks. <em>IETIP</em>, <em>14</em>(16), 4258–4268. (<a
href="https://doi.org/10.1049/iet-ipr.2020.0124">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The wavelet block tree coding (WBTC) algorithm is an efficient wavelet-based image coder. In this coder, multiple spatial orientation trees are combined together to make a single block tree. It uses three ordered lists to keep track of significant/insignificant coefficients and sets while coding, which increases its memory requirement as well as computational complexity. Also, it uses memory inefficient conventional discrete wavelet transform (DWT) to compute the transformed coefficients. In this study, a Low-Complexity Block Tree Coding (LCBTC) algorithm that uses two state-tables and two very small lists, is proposed. Similar to WBTC, it also uses sorting and refinement passes in each bit-plane. However, it encodes the coefficients in block-tree manner using depth-first search approach to reduce the computational complexity. It uses DWT coefficients obtained from modified fractional wavelet filter (MFrWF) rather than conventional DWT, which further reduces the overall memory and complexity of the image coder. The simulation results show that the memory requirement and computational complexity of LCBTC is much less than WBTC and other state-of-the-art coding algorithms. These features make the image coder a better candidate for compression in memory constrained and real-time visual sensor networks.},
  archive      = {J_IETIP},
  author       = {Mohd Rafi Lone and Ekram Khan},
  doi          = {10.1049/iet-ipr.2020.0124},
  journal      = {IET Image Processing},
  month        = {12},
  number       = {16},
  pages        = {4258-4268},
  shortjournal = {IET Image Process.},
  title        = {Low-complexity block tree image coder for visual sensor networks},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Synthetic medical image generator for data augmentation and
anonymisation based on generative adversarial network for glioblastoma
tumors growth prediction. <em>IETIP</em>, <em>14</em>(16), 4248–4257.
(<a href="https://doi.org/10.1049/iet-ipr.2020.1141">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Prediction methods of glioblastoma tumours growth constitute a hard task due to the lack of medical data, which is mostly related to the patients&#39; privacy, the cost of collecting a large medical data set, and the availability of related notations by experts. In this study, the authors propose a synthetic medical image generator (SMIG) with the purpose of generating synthetic data based on the generative adversarial network in order to provide anonymised data. In addition, to predict the glioblastoma multiform tumour growth the authors developed a tumour growth predictor based on end to end convolution neural network architecture that allows training on a public data set from the cancer imaging archive (TCIA), combined with the generated synthetic data. The authors also highlighted the impact of implicating a synthetic data generated using SMIG as a data augmentation tool. Despite small data size provided by TCIA data set, the obtained results demonstrate valuable tumour growth prediction accuracy.},
  archive      = {J_IETIP},
  author       = {Adel Kamli and Rachida Saouli and Hadj Batatia and Mostefa Ben Naceur and Imane Youkana},
  doi          = {10.1049/iet-ipr.2020.1141},
  journal      = {IET Image Processing},
  month        = {12},
  number       = {16},
  pages        = {4248-4257},
  shortjournal = {IET Image Process.},
  title        = {Synthetic medical image generator for data augmentation and anonymisation based on generative adversarial network for glioblastoma tumors growth prediction},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Image dehazing method via a cycle generative adversarial
network. <em>IETIP</em>, <em>14</em>(16), 4240–4247. (<a
href="https://doi.org/10.1049/iet-ipr.2020.0928">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Aiming at the problems of colour distortion and residual dehazing in the existing image dehazing methods when processing outdoor images, a method of image dehazing based on the cycle generative adversarial network is proposed. Taking the cycle generative adversarial network as the overall framework of the model, firstly, the neural network is trained to obtain the mapping relationship between haze images and haze-free images. Secondly, to accelerate the convergence speed of network training, using residual structure to improve network stability and reduce parameters. Then, a new loss function is proposed, fusing the Wasserstein distance into adversarial loss and cycle consistency loss, which reduces the deviation between the generated dehaze image and the real haze-free image, and alleviating the problems of colour distortion and haze removal residue. Finally, use the optimised bounded ReLU (BReLU) activation function instead of the original activation function to improve the transmittance reflecting the haze depth information. The experimental results demonstrated that, compared with the comparison method, the proposed method improves the peak signal-to-noise ratio, structure similarity, information entropy, and average gradient, and has an achieved better performance in dehazing.},
  archive      = {J_IETIP},
  author       = {Chao-Yue Zhao and Rui-Sheng Jia and Qing-Ming Liu and Hong-Mei Sun and Hai-Bin Sun},
  doi          = {10.1049/iet-ipr.2020.0928},
  journal      = {IET Image Processing},
  month        = {12},
  number       = {16},
  pages        = {4240-4247},
  shortjournal = {IET Image Process.},
  title        = {Image dehazing method via a cycle generative adversarial network},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Reversible data hiding in encrypted images for coding
channel based on adaptive steganography. <em>IETIP</em>,
<em>14</em>(16), 4229–4239. (<a
href="https://doi.org/10.1049/iet-ipr.2020.1105">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this study, a novel reversible data hiding (RDH) in encrypted domain scheme for coding channel based on sliding-block segmentation and adaptive steganography is proposed. The proposed scheme enriches the residual information with as little additional encryption information as possible to improve the testing error rate of a steganalyser by sliding-block segmentation with bit stream encryption. The specific encryption process effectively weakens the correlation between the adjacent pixels and minimises the size of key stream bits. The encryption key can be further embedded in the channel code stream before transmitted in the channel. Experimental analysis shows that the image encrypted by the proposed RDH scheme can achieve a peak-signal-to-noise ratio of &gt;50 dB, as the payload is 0.5 bits per pixel (bpp). In terms of security performance, compared with the state-of-the-art methods, their method has a higher testing error rate when the steganalyser is utilised. Even if the payload is 0.5 bpp, the testing error rate is &gt;0.25.},
  archive      = {J_IETIP},
  author       = {Kunliang Yu and Liquan Chen and Yu Wang and Jinguang Han and Lejun Zhang},
  doi          = {10.1049/iet-ipr.2020.1105},
  journal      = {IET Image Processing},
  month        = {12},
  number       = {16},
  pages        = {4229-4239},
  shortjournal = {IET Image Process.},
  title        = {Reversible data hiding in encrypted images for coding channel based on adaptive steganography},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Ordered and fixed-length bit-string fingerprint
representation with minutia vicinity combined feature and spectral
clustering. <em>IETIP</em>, <em>14</em>(16), 4220–4228. (<a
href="https://doi.org/10.1049/iet-ipr.2020.1025">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The minutiae set defined by the ISO/IEC 19794-2 is one of the prevalent feature used in fingerprint recognition systems. Unfortunately, such characteristic of unordered and variable-sized minutiae information causes a restriction on the operation in some advanced template protection methods (e.g. fuzzy commitment), which usually require an ordered and fixed-length binary feature representation as the system input. In this study, in order to simultaneously extend the application of fingerprint recognition and provide satisfactory system performance, the authors propose a novel fixed-length bit-string conversion framework based on spectral clustering and the proposed newly designed discriminative fingerprint representation called minutia vicinity combined feature (MVCF). The proposed method consists of three stages: (i) the extraction of MVCF, (ii) bit conversion via the spectral clustering algorithm, and (iii) matching. Benefiting from feature invariance, fixed-length and bit-oriented coding, merits such as fast matching and decent accuracy are well guaranteed. The performance evaluation is conducted on six publicly available benchmark data sets: FVC2002 DB1, DB2, DB3 and FVC2004 DB1, DB2, DB3 confirms the superiority of the proposed method and suggests the promise of migrating to some other domains (e.g., template protection).},
  archive      = {J_IETIP},
  author       = {Yuxing Li and Heng Zhao and Zhicheng Cao and Eryun Liu and Liaojun Pang},
  doi          = {10.1049/iet-ipr.2020.1025},
  journal      = {IET Image Processing},
  month        = {12},
  number       = {16},
  pages        = {4220-4228},
  shortjournal = {IET Image Process.},
  title        = {Ordered and fixed-length bit-string fingerprint representation with minutia vicinity combined feature and spectral clustering},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Infrared and visible image fusion using multi-scale NSCT and
rolling-guidance filter. <em>IETIP</em>, <em>14</em>(16), 4210–4219. (<a
href="https://doi.org/10.1049/iet-ipr.2020.0781">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image fusion is essential to produce a complementary and comprehensive image, with the source images derived from different sensors, captured from different illumination conditions. In this study, a novel multi-scale image fusion based on the combination of non-subsampled contourlet transform (NSCT) and rolling-guidance filter (RGF) is used to enhance the edges and texture details better than the conventional methods. Initially, infrared (IR) and visible (VIS) source images are multi-scale decomposed to low-frequency and high-frequency sub-band coefficients by NSCT for the best representation of edges and curves. Further, the low-frequency coefficients are decomposed into the base and detail layers by a combination of RGF and GF (Gaussian filter) to retain the features in multiple scales and to reduce halos near the edges. Base layers are fused by saliencybased fusion rule and detail layers are fused by Max absolute rule. High-frequency coefficients are fused by consistency verification based fusion rule to preserve visual details and to suppress noise from source images. Finally, the image is reconstructed by inverse NSCT with good visual perception. Experimental results are evaluated by different evaluation metrics and the results suggest that the proposed method results with better improved source information, clarity and contrast.},
  archive      = {J_IETIP},
  author       = {Arivazhagan Selvaraj and Prema Ganesan},
  doi          = {10.1049/iet-ipr.2020.0781},
  journal      = {IET Image Processing},
  month        = {12},
  number       = {16},
  pages        = {4210-4219},
  shortjournal = {IET Image Process.},
  title        = {Infrared and visible image fusion using multi-scale NSCT and rolling-guidance filter},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Efficient image segmentation method based on an adaptive
selection of gabor filters. <em>IETIP</em>, <em>14</em>(16), 4198–4209.
(<a href="https://doi.org/10.1049/iet-ipr.2019.0723">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image segmentation is one of the most important aspects of image processing that has many applications in image analysis, computer vision, and machine vision. So far, researchers have proposed many image segmentation algorithms that provide a relatively appropriate response for a particular application and over a specific type of input images. At the same time, they have not yet suggested a simple, fast, and efficient segmentation method that can apply to a wide range of natural and texture images. In this study, the authors present a new adaptive method for selecting Gabor filters using the image frequency content. Adaptive spectral features of the image are then extracted using only these selected filters from a Gabor filter bank. A pre-setting parameter Gabor filter is used to extract the gradient of each feature in x and y directions. Combining these features gradient, the texture gradient image is calculated, and then by selected markers for different texture regions and applying the marker-based watershed transform on texture gradient image, the authors present a new image segmentation method that has a less computational cost and yields better results compared to using all other filter banks.},
  archive      = {J_IETIP},
  author       = {Alireza Sardar and Nasser Mehrshad and Seyyed Mohammad Razavi},
  doi          = {10.1049/iet-ipr.2019.0723},
  journal      = {IET Image Processing},
  month        = {12},
  number       = {16},
  pages        = {4198-4209},
  shortjournal = {IET Image Process.},
  title        = {Efficient image segmentation method based on an adaptive selection of gabor filters},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Depth estimation for underwater images from single view
image. <em>IETIP</em>, <em>14</em>(16), 4188–4197. (<a
href="https://doi.org/10.1049/iet-ipr.2019.1533">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Underwater images often undergo distortions from scattering, absorption, colour loss, diffraction, polarisation, and varying attenuation depending on the light frequency, due to the water medium. This study answers problems associated with the recovery of underwater images, by developing novel depth map estimation which may be used as an intermediary step for underwater image restoration. The depth map is an important factor for the recovery of the underwater image, as it has been shown that proper estimation of depth, results in better restoration of the underwater image. The proposed algorithm estimates a depth map from a single view image, using blurriness and lighting information, obtained using a simple background neutralisation method. Results from the algorithm on selected raw underwater image data set have been compared with other algorithms and it has been shown that the proposed method gives Pearson coefficients in the range of 0.75–0.95, which are considerably higher and better log root mean square value compared to other methods in the literature. This suggests a better estimation of depth by the proposed method.},
  archive      = {J_IETIP},
  author       = {Jarina Raihan A and Pg Emeroylariffion Abas and Liyanage C De Silva},
  doi          = {10.1049/iet-ipr.2019.1533},
  journal      = {IET Image Processing},
  month        = {12},
  number       = {16},
  pages        = {4188-4197},
  shortjournal = {IET Image Process.},
  title        = {Depth estimation for underwater images from single view image},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Segmentation using fuzzy cluster-based thresholding method
for apple fruit sorting. <em>IETIP</em>, <em>14</em>(16), 4178–4187. (<a
href="https://doi.org/10.1049/iet-ipr.2020.0705">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Apple fruit sorting has been an important postharvest process carried on for the sorting of diseased apple fruits. A fuzzy cluster-based thresholding (FCBT) method for segmenting the region of interest from an apple image has been proposed for sorting apples in this study. As the first step, the acquired RGB colour image of an apple fruit was converted into a greyscale image. Then, five different fuzzy cluster bins with overlapped pixel ranges were taken and greypixel values were binned into them. A cluster with the maximum number of pixels was selected for calculating the threshold value. The region of interest from the apple image was then segmented using the proposed FCBT value. Features extracted from the segmented images were given as input to a fully-connected deep neural network for a classification. The performance of the FCBT method was compared with similar greyscale thresholding methods like Otsu&#39;s and Kapur&#39;s methods. The visual segmentation accuracy and the execution speed showed that the FCBT outperformed the other methods in segmenting the diseased area. A fully-connected deep neural network model with the FCBT image extracted features as input values gave a 98.33% accuracy rate in sorting the apple images.},
  archive      = {J_IETIP},
  author       = {Manickam Henila and Palaniappan Chithra},
  doi          = {10.1049/iet-ipr.2020.0705},
  journal      = {IET Image Processing},
  month        = {12},
  number       = {16},
  pages        = {4178-4187},
  shortjournal = {IET Image Process.},
  title        = {Segmentation using fuzzy cluster-based thresholding method for apple fruit sorting},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020b). Inter-frame forgery detection and localisation in videos
using earth mover’s distance metric. <em>IETIP</em>, <em>14</em>(16),
4168–4177. (<a href="https://doi.org/10.1049/iet-ipr.2020.0287">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Video forensics is one of the hot topics in multimedia forensics. Nowadays, spreading fake videos across the internet is becoming a profession mainly in politics and entertainment. In this study, a novel two-stage inter-frame video forgery detection technique is proposed. The first stage analyses spatio-temporal feature flow consistency to detect suspicious tamper points. Identifying the type of forgery and validating the recovered video are done in the second stage. Earth mover&#39;s distance is used as a similarity metric in both stages. The authors concentrate on a robust inter-frame forgery detection approach which can be applied for any challenging video. Compression at a higher rate, noise addition, and filtering are the anti-forensic tricks used by forgers to fool forensic techniques. However, most of the literature in video forgery detection has handled these issues as post-processing attacks and reported lesser accuracies for it. Hence the authors propose a robust and efficient forgery detection technique capable of identifying all kinds of inter-frame forgeries in videos. Experimental evaluation of the public video data set shows that the proposed approach outperforms existing approaches with an improved rate of robustness.},
  archive      = {J_IETIP},
  author       = {Priyadharsini Selvaraj and Muneeswaran Karuppiah},
  doi          = {10.1049/iet-ipr.2020.0287},
  journal      = {IET Image Processing},
  month        = {12},
  number       = {16},
  pages        = {4168-4177},
  shortjournal = {IET Image Process.},
  title        = {Inter-frame forgery detection and localisation in videos using earth mover&#39;s distance metric},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). SAR image denoising based on multifractal feature analysis
and TV regularisation. <em>IETIP</em>, <em>14</em>(16), 4158–4167. (<a
href="https://doi.org/10.1049/iet-ipr.2020.0272">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A new denoising technique is proposed in this study for synthetic aperture radar (SAR) images corrupted by speckle noise. The authors method extract informative features from a noisy speckled image, and then a denoised version of this image is estimated from the informative gradients, which are restricted to the features of this image. The technique of extracting features is designed on the framework of multifractal formalism followed by a reconstruction technique for the informative gradients based on the total variational (TV) regularisation framework. Experimental results demonstrate that the proposed approach is able to retain the finer details of the original image while removing noise. The superiority of the proposed approach is manifested qualitatively and quantitatively on comparing with state-of-the-art denoising techniques.},
  archive      = {J_IETIP},
  author       = {Suman Kumar Maji and Ramesh Kumar Thakur and Hussein M. Yahia},
  doi          = {10.1049/iet-ipr.2020.0272},
  journal      = {IET Image Processing},
  month        = {12},
  number       = {16},
  pages        = {4158-4167},
  shortjournal = {IET Image Process.},
  title        = {SAR image denoising based on multifractal feature analysis and TV regularisation},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Colon cancer prediction using 2DReCA segmentation and hybrid
features on histopathology images. <em>IETIP</em>, <em>14</em>(16),
4144–4157. (<a href="https://doi.org/10.1049/iet-ipr.2019.1717">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Since histopathological images exist in various forms, performing segmentation on these images is tedious. While in cancer-free colon tissue, epithelial cells generally have an elliptical shape; their structure alters in a malignant tissue. This study proposes a technique consisting of colon biopsy image segmentation and a hybrid set of features for classification, and is evaluated on multiple databases with various levels of magnifications. This study presents a novel image segmentation method with multi-level thresholding based on Rényi&#39;s two-dimensional entropy with a cultural algorithm (2DR CA). Based on the entropy, elliptical epithelial cells, being the region of interest, are identified from the segmented background. After successful segmentation, shape descriptors are extracted with morphological operations. Two sets of texture features (grey-level co-occurrence matrix and block-wise elliptical local binary pattern) are calculated based on pre-processed grey-scale colon images. The proposed hybrid feature vector set, then concatenates the extracted features for training and testing with a random forest classifier. The proposed segmentation and classification model is evaluated by considering four data sets consisting of various colon images at different magnifications. In addition, it is evaluated by multiple performance measures and compared with existing techniques.},
  archive      = {J_IETIP},
  author       = {Tina Babu and Tripty Singh and Deepa Gupta},
  doi          = {10.1049/iet-ipr.2019.1717},
  journal      = {IET Image Processing},
  month        = {12},
  number       = {16},
  pages        = {4144-4157},
  shortjournal = {IET Image Process.},
  title        = {Colon cancer prediction using 2DReCA segmentation and hybrid features on histopathology images},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Image clustering algorithm using superpixel segmentation and
non-symmetric gaussian–cauchy mixture model. <em>IETIP</em>,
<em>14</em>(16), 4132–4143. (<a
href="https://doi.org/10.1049/iet-ipr.2020.0402">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this study, an unsupervised clustering algorithm is proposed to label superpixel density images. Firstly, the authors propose a novel superpixel segmentation algorithm driven by a modified fuzzy C-means objective function, Kullback–Leibler (KL) divergence, and an entropy term, which generate superpixels with good boundary adherence and intensity homogeneity. In this model, the logarithm of Gaussian distribution as a new distance metric is used to improve the accuracy of boundary pixel classification, the KL divergence is applied to regularise the fuzzy objective function. Based on this model, the generated superpixel intensity images with a highly distinctive background colour from the colour of the target are obtained. Grouping cues generated by superpixels can affect the performance of image clustering greatly. Next, according to the small amount of clustering data generated by the superpixel intensity images, they construct a non-symmetric mixture model based on a mixture of Gaussian distribution and Cauchy distribution for implementing image clustering. Thus, clustering of colour images is transformed into clustering of these newly generated data. The advantage of this model is its well adaption to different shapes of observed data. Experimental results on publicly available data sets are provided to demonstrate the effectiveness of the proposed algorithm.},
  archive      = {J_IETIP},
  author       = {Sifan Ji and Hongqing Zhu and Pengyu Wang and Xiaofeng Ling},
  doi          = {10.1049/iet-ipr.2020.0402},
  journal      = {IET Image Processing},
  month        = {12},
  number       = {16},
  pages        = {4132-4143},
  shortjournal = {IET Image Process.},
  title        = {Image clustering algorithm using superpixel segmentation and non-symmetric Gaussian–Cauchy mixture model},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Social bat optimisation dependent deep stacked auto-encoder
for skin cancer detection. <em>IETIP</em>, <em>14</em>(16), 4122–4131.
(<a href="https://doi.org/10.1049/iet-ipr.2020.0318">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nowadays, skin cancer is one of the most dangerous forms of cancer found in humans. There are various types of skin cancer, like basal, melanoma, carcinoma, and the squamous cell from which the melanoma is unpredictable. Thus, skin cancer detection in the early stage is very useful to treat it successfully. Hence, this study introduces a new algorithm called social bat optimisation algorithm for skin cancer detection. Initially, the pre-processing is done for the input image to eliminate the noise and artefacts present in the image. Then, the pre-processed image is fed to the feature extraction step where the features are extracted based on convolutional neural network features, and the local pixel pattern-based texture feature (local PPBTF). Here, the PPBTF is the combination of texture features and pixel pattern-based features in which the equation of PPBTF is modified based on the local binary pattern. Subsequently, the classification is done based on the extracted features using a deep stacked auto-encoder, which is trained by the proposed social bat optimisation. The performance of skin cancer detection based on the proposed model is evaluated based on accuracy, sensitivity, and specificity. The proposed model achieves the maximal accuracy of 93.38%, maximal sensitivity of 95%, and the maximal specificity of 96% for K -fold.},
  archive      = {J_IETIP},
  author       = {Ramachandro Majji and Ponnusamy Gnanaprakasam Om Prakash and Rajan Cristin and Govindaswamy Parthasarathy},
  doi          = {10.1049/iet-ipr.2020.0318},
  journal      = {IET Image Processing},
  month        = {12},
  number       = {16},
  pages        = {4122-4131},
  shortjournal = {IET Image Process.},
  title        = {Social bat optimisation dependent deep stacked auto-encoder for skin cancer detection},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Design and implementation of image kernels using reversible
logic gates. <em>IETIP</em>, <em>14</em>(16), 4110–4121. (<a
href="https://doi.org/10.1049/iet-ipr.2019.1681">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study presents the implementation of image kernels used for filtering and enhancing the images using reversible logic gates, a first in reversible logic literature. Image enhancement/filtering is achieved by performing convolution of an image with a filter kernel. This work proposes reversible logic based design and implementation of six filter kernels. The filter kernels implemented are Gaussian blur, Laplacian outline, Sobel, Emboss, Sharpen and Prewitt edge detection. The kernels are implemented individually using reversible logic gates and the designs are measured in terms of quantum cost, garbage outputs, ancilla inputs and gate count. The functional verification is carried out using 512 × 512 standard images on Kintex 7 FPGA platform. The filtered images from the proposed design have an average structural similarity index of 0.92.},
  archive      = {J_IETIP},
  author       = {Sithara Raveendran and Pranose Jose Edavoor and Nithin Kumar Yernad Balachandra and Vasantha Moodabettu Harishchandra},
  doi          = {10.1049/iet-ipr.2019.1681},
  journal      = {IET Image Processing},
  month        = {12},
  number       = {16},
  pages        = {4110-4121},
  shortjournal = {IET Image Process.},
  title        = {Design and implementation of image kernels using reversible logic gates},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Efficient and unified license plate recognition via
lightweight deep neural network. <em>IETIP</em>, <em>14</em>(16),
4102–4109. (<a href="https://doi.org/10.1049/iet-ipr.2020.1130">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this study, the authors are interested in building a unified deep learning framework to solve the recognition problem of both single-line and double-line car license plates. Most existing methods are designed for single-line license plate recognition. For double-line cases, detection and segmentation are usually adopted firstly to locate each line of characters. These methods are usually environmentally sensitive and will bring propagation of error between segmentation and recognition. To solve the problems, the authors propose a unified method that can recognise both single-line and double-line license plates in an end-to-end way without line segmentation and character segmentation. Specifically, an improved lightweight convolutional neural network is used to extract features efficiently. Then, the multi-task learning strategy is used to simultaneously perform license plate classification and character recognition. Finally, recognition task is treated as sequence labelling problems, which are solved by connectionist temporal classification directly. Experimental results indicate that the proposed method significantly outperforms the previous state-of-the-art methods on both public datasets and the synthetic dataset.},
  archive      = {J_IETIP},
  author       = {Shuxin Qin and Sijiang Liu},
  doi          = {10.1049/iet-ipr.2020.1130},
  journal      = {IET Image Processing},
  month        = {12},
  number       = {16},
  pages        = {4102-4109},
  shortjournal = {IET Image Process.},
  title        = {Efficient and unified license plate recognition via lightweight deep neural network},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Automatic layered RGB-d scene flow estimation with optical
flow field constraint. <em>IETIP</em>, <em>14</em>(16), 4092–4101. (<a
href="https://doi.org/10.1049/iet-ipr.2020.0230">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Scene flow estimation with RGB-D frames is receiving increasing attention in digital video processing and computer vision due to the widespread use of depth sensors. Existing methods based on object segmentation have shown their effectiveness for object occlusion and large displacement. However, improper segmentation often causes incomplete segmented areas or incorrect edges, which will result in inaccurate occlusion inference and scene flow estimation. To this end, an automatic layered RGB-D scene flow estimation method is proposed, which achieves more accurate layering of objects in depth image by exploring motion information. The authors exploit super-pixel segmentation for initial layering, which is beneficial for preserving edges and integrity of the objects. Furthermore, an optical flow which is highly correlated with the scene is also used for automatic layering. Finally, the utilisation of super-pixel segmentation and motion information would ensure the integrity of the object area and improve the accuracy of scene flow. They have validated their approach both qualitatively and quantitatively on several public datasets. Experimental results show that the proposed method is able to preserve the integrity of the object and achieve lower root mean square error and average angular error as compared with the current state-of-the-art algorithms.},
  archive      = {J_IETIP},
  author       = {Xiuxiu Li and Yanjuan Liu and Haiyan Jin and Jiangbin Zheng and Lei Cai},
  doi          = {10.1049/iet-ipr.2020.0230},
  journal      = {IET Image Processing},
  month        = {12},
  number       = {16},
  pages        = {4092-4101},
  shortjournal = {IET Image Process.},
  title        = {Automatic layered RGB-D scene flow estimation with optical flow field constraint},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Hybrid feature CNN model for point cloud classification and
segmentation. <em>IETIP</em>, <em>14</em>(16), 4086–4091. (<a
href="https://doi.org/10.1049/iet-ipr.2020.0658">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study proposes a hybrid feature convolutional neural network (HFCNN) model for the complete description of three-dimensional (3D) point cloud features. The HFCNN confers sensitivity to the local, global, and single-point properties simultaneously by a feature vector space expansion. Wherein, a pointwise convolutional network sub-model realises the extraction of the local features by using a pointwise convolutional operator to process point cloud data directly. To consider the global properties of the point cloud, a central-point radiation model is constructed as an input of the feature layer in a non-network form. Meanwhile, the single-point behaviour is characterised by the solo point coordinate information in the network. Within the constructed solo-local-global feature space, i.e. the fusion of single point feature, local feature and global feature, the HFCNN model can handle 3D point cloud data with unstructured and unordered properties. The HFCNN can be directly applied to the point cloud classification and segmentation without the modification of the CNN structure and training procedure. The experimental results have shown the effectiveness of the proposed model in prediction of class labels and point-by-point labels.},
  archive      = {J_IETIP},
  author       = {Xinliang Zhang and Chenlin Fu and Yunji Zhao and Xiaozhuo Xu},
  doi          = {10.1049/iet-ipr.2020.0658},
  journal      = {IET Image Processing},
  month        = {12},
  number       = {16},
  pages        = {4086-4091},
  shortjournal = {IET Image Process.},
  title        = {Hybrid feature CNN model for point cloud classification and segmentation},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Evaluation criterion of underwater object clustering
segmentation with pulse-coupled neural network. <em>IETIP</em>,
<em>14</em>(16), 4076–4085. (<a
href="https://doi.org/10.1049/iet-ipr.2019.1662">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The success of clustering algorithms in object segmentation depends on the quality of the evaluation criterion. However, sonar images are seriously affected by noise. Most of the existing evaluation criteria such as the Davies Bouldin (DB) criterion only considers their pixel value, and sonar image information extraction is not sufficient. As a result, they fail to achieve good underwater object segmentation results. To overcome this problem, this study proposes an improved DB criterion with pulse -coupled neural network (PCNN), which is called the DB-PCNN. In the calculation process of DB-PCNN, the role of internal activity items in PCNN is considered, which can make better use of pixel information in adjacent space on the sonar image. The experimental results show that DB-PCNN can further improve the accuracy of underwater object segmentation and has certain adaptability to different optimisation frameworks.},
  archive      = {J_IETIP},
  author       = {Xingmei Wang and Qiming Li and Yue Yu and Yichao Xu},
  doi          = {10.1049/iet-ipr.2019.1662},
  journal      = {IET Image Processing},
  month        = {12},
  number       = {16},
  pages        = {4076-4085},
  shortjournal = {IET Image Process.},
  title        = {Evaluation criterion of underwater object clustering segmentation with pulse-coupled neural network},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Automated detection of multiple structural changes of
diabetic macular oedema in SDOCT retinal images through transfer
learning in CNNs. <em>IETIP</em>, <em>14</em>(16), 4067–4075. (<a
href="https://doi.org/10.1049/iet-ipr.2020.0612">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Diabetic Maculopathy, the major cause of vision loss, occurs due to uncontrolled diabetes. This affects the retinal layers of the eye causing bleeding of vessels, which results in Diabetic Macular Edema (DME), macular detachment etc., Basically three structural changes are involved in DME viz., Cystoid Macular Edema (CME), Serous Macular Detachment (SMD) and Intra Retinal Fluid (IRF). These changes may also coexist with each other such as CME with SMD or CME with IRF etc. In this work, the retinal images acquired through Spectral Domain Optical Coherence Tomography (SDOCT) imaging modality, which would provide high level of precision and resolution of the retinal layers are utilized. An automated algorithm to detect and differentiate seven types of DME based on deep learning is implemented using transfer learning on three Convolutional Neural Networks (CNNs), ResNet 50, VGGNet and AlexNet. In detection of DME, ResNet 50 performs excellently well, when compared with Alexnet and VGGNet, because of its depth and skip connection features. The average values of the statistical parameters such as accuracy (0.993), F1 score (0.975), Mathews Correlation Coefficient(MCC)(0.972) of ResNet are high when compared to that of AlexNet and VGGNet},
  archive      = {J_IETIP},
  author       = {Natarajan Padmasini and Rengasamy Umamaheswari},
  doi          = {10.1049/iet-ipr.2020.0612},
  journal      = {IET Image Processing},
  month        = {12},
  number       = {16},
  pages        = {4067-4075},
  shortjournal = {IET Image Process.},
  title        = {Automated detection of multiple structural changes of diabetic macular oedema in SDOCT retinal images through transfer learning in CNNs},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Ensemble learning-based COVID-19 detection by feature
boosting in chest x-ray images. <em>IETIP</em>, <em>14</em>(16),
4059–4066. (<a href="https://doi.org/10.1049/iet-ipr.2020.1127">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The novel coronavirus has spread quite rapidly across the globe. The current testing rate is failing to match the exponential rate of rising cases. Moreover, the available testing methodologies are expensive and time-consuming. A sensitive automated diagnosis is one of the biggest need of the hour. In the proposed work, the authors analyse the chest X-ray images of normal, pneumonia and coronavirus disease-2019 (COVID-19) patients and process them to boost the COVID-specific features (opacities etc.), which enable to perform sensitive identification of COVID-19 patients. The sets of original and processed images are used with a stack of pre-trained deep models for ensemble learning. They used VGG-16 as base-learners, trained with a diverse set of inputs followed by a logistic regression model, the meta learner, to combine the base-learner predictions. The proposed fusion-based model is trained and tested for three types of classification, TYPE-I: binary (NORMAL/ABNORMAL), TYPE-II: binary (PNEUMONIA/COVID-19) and TYPE-III: multi-class (NORMAL/PNEUMONIA/COVID-19). The diagnosis results are quite promising, with high accuracy and sensitivity values for all the cases. The proposed algorithm can be used to assist the medical experts for quick identification and isolation of COVID-19 patients and thereby mitigating the effect of the virus.},
  archive      = {J_IETIP},
  author       = {Kamini Upadhyay and Monika Agrawal and Desh Deepak},
  doi          = {10.1049/iet-ipr.2020.1127},
  journal      = {IET Image Processing},
  month        = {12},
  number       = {16},
  pages        = {4059-4066},
  shortjournal = {IET Image Process.},
  title        = {Ensemble learning-based COVID-19 detection by feature boosting in chest X-ray images},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Common-specific feature learning for multi-source domain
adaptation. <em>IETIP</em>, <em>14</em>(16), 4049–4058. (<a
href="https://doi.org/10.1049/iet-ipr.2019.1712">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-source domain adaptation (MDA) aims to leverage knowledge from multiple source domains to improve the classification performance on target domains. Different degrees of distribution discrepancies between every two domains pose a huge challenge to MDA tasks. Most works focus on extracting features shared by all domains, which is critical but not enough to reduce distribution discrepancies. In this paper, we propose a method named as common-specific feature learning (CSFL). Constituting a framework of feature learning, CSFL explores a subspace where the combination of common and specific features makes learned representations comprehensive. Based on this framework, we conduct a metric learning method for learning a discriminative feature representation. Considering redundant information caused by source domains is likely to hurt the performance, we impose an effective low-rank constraint to remove the redundant information. Further, we adopt structure consistent constraint to preserve the local structure in each domain. CSFL has obtained about 1–5% improvement of mean accuracy, compared to the state-of-the-art shallow methods. Further, compared with 90.2% and 89.4% of the best baseline deep method, CSFL achieves mean accuracy of 90.8% and 89.7% on the Office-31 and ImageCLEF-DA datasets respectively. The encouraging results validate the effectiveness of our method.},
  archive      = {J_IETIP},
  author       = {Chang Niu and Junyuan Shang and Zhiheng Zhou and Junchu Huang and Tianlei Wang and Xiangwei Li},
  doi          = {10.1049/iet-ipr.2019.1712},
  journal      = {IET Image Processing},
  month        = {12},
  number       = {16},
  pages        = {4049-4058},
  shortjournal = {IET Image Process.},
  title        = {Common-specific feature learning for multi-source domain adaptation},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Fast visual saliency based on multi-scale difference of
gaussians fusion in frequency domain. <em>IETIP</em>, <em>14</em>(16),
4039–4048. (<a href="https://doi.org/10.1049/iet-ipr.2020.0773">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To reduce the computation required in determining the proper scale of salient object, a fast visual saliency based on multi-scale difference of Gaussians fusion in frequency domain (MDF) is proposed. First, based on the phenomenon that the foreground energy is highlighted and densely distributes on certain band of spectrum, the scale coefficients of foreground in an image can be literately approximated on the amplitude spectrum. Next, relying on the linear integration property of Fourier transform, the feature spectrum is obtained through the weighted infinite integral of difference of Gaussian feature maps with respect to the scale of object. Then, the saliency of each channel is obtained from feature spectrum by the inverse Fourier transform and scale filtering. Finally, through the channel integration, the MDF saliency map is obtained. Experiments on Li-Jian data set demonstrate that combined with most appropriate colour space and scale filter, MDF achieves obvious acceleration (5.4 times faster than frequency domain analysis and spatial information) while getting desired accuracy (area under the curve, 0.8814 at Li-Jian data set), which achieves the best accuracy efficiency trade-off.},
  archive      = {J_IETIP},
  author       = {Weipeng Li and Xiaogang Yang and Chuanxiang Li and Ruitao Lu and Xueli Xie},
  doi          = {10.1049/iet-ipr.2020.0773},
  journal      = {IET Image Processing},
  month        = {12},
  number       = {16},
  pages        = {4039-4048},
  shortjournal = {IET Image Process.},
  title        = {Fast visual saliency based on multi-scale difference of gaussians fusion in frequency domain},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Survey on mixed impulse and gaussian denoising filters.
<em>IETIP</em>, <em>14</em>(16), 4027–4038. (<a
href="https://doi.org/10.1049/iet-ipr.2018.6335">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study presents a comprehensive survey on mixed impulse and Gaussian denoising filters which are applied to an image in order to gauge the effects of this type of noise combination and to then determine optimal ways that can overcome such effects. The random noise model considered in this survey is the combined effect of impulse (salt and pepper) and Gaussian noise. After describing the noise models, the denoising filters which are applied to the images are classified and explained according to their design structure, the type of filters they use, the noise level they could overcome, and the limitations they face. This survey covers all related denoising methods and provides an assessment of the strengths and practical limitations of the different classes of denoising filters.},
  archive      = {J_IETIP},
  author       = {Mehdi Mafi and Walter Izquierdo and Mercedes Cabrerizo and Armando Barreto and Jean Andrian and Naphtali David Rishe and Malek Adjouadi},
  doi          = {10.1049/iet-ipr.2018.6335},
  journal      = {IET Image Processing},
  month        = {12},
  number       = {16},
  pages        = {4027-4038},
  shortjournal = {IET Image Process.},
  title        = {Survey on mixed impulse and gaussian denoising filters},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Machine learning and deep learning for clinical data and
PET/SPECT imaging in parkinson’s disease: A review. <em>IETIP</em>,
<em>14</em>(16), 4013–4026. (<a
href="https://doi.org/10.1049/iet-ipr.2020.1048">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Machine Learning (ML) is a subfield of Artificial Intelligence (AI) that is increasingly applied to several medical diagnosis tasks, including a wide range of diseases. Importantly, various ML models were developed to address the complexity of Parkinson&#39;s Disease (PD) diagnosis. PD is a neurodegenerative disease characterized by motor and non-motor disorders where its syndromes affect the daily lives of patients. Several Computer Aided Diagnosis and Detection (CADD) systems based on hand-crafted ML algorithms achieved promising results in distinguishing PD patients from Healthy Control (HC) subjects and other Parkinsonian syndrome categories using clinical data (e.g., speech and gait impairments) and medical imaging [e.g., Position Emission Tomography (PET) and Single Photon Emission Computed Tomography (SPECT)]. Despite the good performance of hand-crafted ML algorithms, there is still a problem linked to the features&#39; extraction and selection. In fact, Deep Learning DL has provided an ultimate solution for the features&#39; extraction and selection related issue. An important number of studies on the diagnosis of PD using DL algorithms were developed recently. This study provides an overview of the application of hand-crafted ML algorithms and DL techniques for PD diagnosis. It also introduces key concepts for understanding the application of ML methods to diagnose PD.},
  archive      = {J_IETIP},
  author       = {Hajer Khachnaoui and Rostom Mabrouk and Nawres Khlifa},
  doi          = {10.1049/iet-ipr.2020.1048},
  journal      = {IET Image Processing},
  month        = {12},
  number       = {16},
  pages        = {4013-4026},
  shortjournal = {IET Image Process.},
  title        = {Machine learning and deep learning for clinical data and PET/SPECT imaging in parkinson&#39;s disease: A review},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). DeepInterpolation: Fusion of multiple interpolations and CNN
to obtain super-resolution. <em>IETIP</em>, <em>14</em>(15), 4000–4011.
(<a href="https://doi.org/10.1049/iet-ipr.2019.1244">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The authors propose architectures that learn end-to-end mapping functions to improve the spatial resolution of the input natural images. The models are unique in forming non-linear combinations of three image interpolation techniques using the convolutional neural network. Another proposed architecture uses a skip connection with nearest-neighbour interpolation, achieving almost similar results. The architectures have been carefully designed to ensure that the reconstructed images lie precisely in the manifold of high-resolution images, thereby preserving the high-frequency components with fine details. They have compared with the state-of-the-art and recent deep learning-based natural image super-resolution techniques and found that their methods can preserve the sharp details in the image, while also obtaining comparable or better peak-signal-to-noise ratio values than them. Since their methods use image interpolations and a shallow convolutional neural network (CNN) with a fewer number of smaller filters, the computational cost is kept low. They have reported the results of the best two proposed architectures on five standard data sets for an upscale factor of 2. Their methods generalise well in most cases, which is evident from the better results obtained with increasingly complex data sets. For four times upscaling, they have designed similar architectures for comparing with other methods.},
  archive      = {J_IETIP},
  author       = {Ram Krishna Pandey and Ramakrishnan Angarai Ganesan},
  doi          = {10.1049/iet-ipr.2019.1244},
  journal      = {IET Image Processing},
  month        = {12},
  number       = {15},
  pages        = {4000-4011},
  shortjournal = {IET Image Process.},
  title        = {DeepInterpolation: Fusion of multiple interpolations and CNN to obtain super-resolution},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Iterative PET image reconstruction using cascaded data
consistency generative adversarial network. <em>IETIP</em>,
<em>14</em>(15), 3989–3999. (<a
href="https://doi.org/10.1049/iet-ipr.2020.1056">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study proposed a GAN-based reconstruction method-cascaded data consistency generative adversarial network (CDCGAN) to recover high-quality PET images from filtered back projection PET images with streaking artifacts and high noise. First, the authors embed defined data consistency layer (DC layer) in their generator network to constrain the reconstruction process and adjust accurately generated faked PET images. Second, to improve the accuracy of reconstruction on average, their generator network was built iteratively to achieve better performance with simple structures. They observed that the proposed CDCGAN allows the preservation of fine anomalous features while eliminating the streaking artifacts and noise. Experimental results show that the reconstructed PET images by their methods perform well comparably to other state-of-the-art methods but at a faster speed. A clinical experiment was also performed to show the validity of the CDCGAN for artifacts reduction.},
  archive      = {J_IETIP},
  author       = {Qianqian Du and Xueting Ren and Jiawen Wang and Yan Qiang and Xiaotang Yang and Ntikurako Guy-Fernand Kazihise},
  doi          = {10.1049/iet-ipr.2020.1056},
  journal      = {IET Image Processing},
  month        = {12},
  number       = {15},
  pages        = {3989-3999},
  shortjournal = {IET Image Process.},
  title        = {Iterative PET image reconstruction using cascaded data consistency generative adversarial network},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). CGGAN: A context-guided generative adversarial network for
single image dehazing. <em>IETIP</em>, <em>14</em>(15), 3982–3988. (<a
href="https://doi.org/10.1049/iet-ipr.2020.1153">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image haze removal is highly desired for the application of computer vision. This study proposes a novel context-guided generative adversarial network (CGGAN) for single image dehazing. Of which, a novel new encoder–decoder is employed as the generator. In addition, it consists of a feature-extraction net, a context-extraction net, and a fusion net in sequence. The feature-extraction net acts as an encoder, and is used for extracting haze features. The content-extraction net is a multi-scale parallel pyramid decoder and is used for extracting the deep features of the encoder and generating coarse dehazing image. The fusion net is a decoder and is used for obtaining the final haze-free image. In order to get better dehazing results, multi-scale information obtained during the decoding process of the context extraction decoder is used for guiding the fusion decoder. By introducing an extra coarse decoder to the original encoder–decoder, the CGGAN can make better use of the deep feature information extracted by the encoder. To ensure that the proposed CGGAN works effectively for different haze scenarios, different loss functions are employed for the two decoders. Experiments results show the advantage and the effectiveness of the proposed CGGAN, evidential improvements over existing state-of-the-art methods are obtained.},
  archive      = {J_IETIP},
  author       = {Zhaorun Zhou and Zhenghao Shi},
  doi          = {10.1049/iet-ipr.2020.1153},
  journal      = {IET Image Processing},
  month        = {12},
  number       = {15},
  pages        = {3982-3988},
  shortjournal = {IET Image Process.},
  title        = {CGGAN: A context-guided generative adversarial network for single image dehazing},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Tumour growth prediction of follow-up lung cancer via
conditional recurrent variational autoencoder. <em>IETIP</em>,
<em>14</em>(15), 3975–3981. (<a
href="https://doi.org/10.1049/iet-ipr.2020.0496">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The prediction of lung tumour growth is the key to early treatment of lung cancer. However, the lack of intuitive and clear judgments about the future development of the tumour often leads patients to miss the best treatment opportunities. Combining the characteristics of the variational autoencoder and recurrent neural networks, this study proposes a tumour growth prediction via a conditional recurrent variational autoencoder. The proposed model uses a variational autoencoder to reconstruct tumour images at different times. Meanwhile, the recurrent units are proposed to infer the relationship between tumour images according to the chronological order. The different tumour development varies in different patients, patients&#39; condition is adopted to achieve personalised prediction. To solve the problem of blurred results, the authors add the total variation regularisation term into the object function. The proposed method was tested on longitudinal studies, National Lung Screening Trial and cooperative hospital dataset, with three points on lung tumours. The precision, recall, and dice similarity coefficient reach 82.22, 79.89 and 82.49%, respectively. Both quantitative and qualitative experimental results show that the proposed method can produce realistic tumour images.},
  archive      = {J_IETIP},
  author       = {Ning Xiao and Yan Qiang and Zijuan Zhao and Juanjuan Zhao and Jianhong Lian},
  doi          = {10.1049/iet-ipr.2020.0496},
  journal      = {IET Image Processing},
  month        = {12},
  number       = {15},
  pages        = {3975-3981},
  shortjournal = {IET Image Process.},
  title        = {Tumour growth prediction of follow-up lung cancer via conditional recurrent variational autoencoder},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Category-wise feature extractor based on ADL method for
weak-supervised object localisation. <em>IETIP</em>, <em>14</em>(15),
3965–3974. (<a href="https://doi.org/10.1049/iet-ipr.2020.0640">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fully supervised object detection needs to use the data set with object category annotation and location annotation to train the model. In contrast, weak supervised object localisation only needs to use the data set with object category annotation to train the model, but it can complete the classification task and object localisation task at the same time. Inspired by the attention-based dropout layer (ADL) method, this study designs a category-wise feature extractor (CFE), which can explicitly obtain the localisation map used to indicate the object location, and it is directly related to the category output of the classification task. Although its amount of calculation is slightly larger than that of the ADL method, the performance is better than ADL in some tasks. In addition to the standard CFE method mentioned above, this study also designs a lightweight CFE-tiny method, which adopts split-attention mechanism, and the calculation amount of this method is much smaller than that of ADL method.},
  archive      = {J_IETIP},
  author       = {Yanzhu Hu and Dongdong Zhu and Xinbo Ai and Yabo Xu},
  doi          = {10.1049/iet-ipr.2020.0640},
  journal      = {IET Image Processing},
  month        = {12},
  number       = {15},
  pages        = {3965-3974},
  shortjournal = {IET Image Process.},
  title        = {Category-wise feature extractor based on ADL method for weak-supervised object localisation},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Deep feature learning for gender classification with
covered/camouflaged faces. <em>IETIP</em>, <em>14</em>(15), 3957–3964.
(<a href="https://doi.org/10.1049/iet-ipr.2020.0199">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The great attention to gender classification is increasing recently as genders carry rich information related to male and female social activities. Extracting discriminating visual representations for gender classification is challenging especially with covered or camouflaged faces. In this work, the authors propose a network that uses a combination of inceptions with variational feature learning (VFL) loss function. The proposed network recognises the gender of normal or covered/camouflaged faces through the middle face part. This network trained on the middle part of the faces that contain both eyes with a small margin from the top-left corner to the bottom-right corner of the area of the eyes. Experimental results showed that the proposed network achieved state-of-art performance on five public data sets: FEI, SCIEN, AR FACES, LFW, and ADIENCE. They also evaluated the authors’ network on another new collected data set for covered and camouflaged faces and obtained encouraging outcomes.},
  archive      = {J_IETIP},
  author       = {Mohammed Alghaili and Zhiyong Li and Hamdi A.R. Ali},
  doi          = {10.1049/iet-ipr.2020.0199},
  journal      = {IET Image Processing},
  month        = {12},
  number       = {15},
  pages        = {3957-3964},
  shortjournal = {IET Image Process.},
  title        = {Deep feature learning for gender classification with covered/camouflaged faces},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Vector ordering and regression learning-based ranking for
dynamic summarisation of user videos. <em>IETIP</em>, <em>14</em>(15),
3941–3956. (<a href="https://doi.org/10.1049/iet-ipr.2020.0234">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dynamic video summarisation (video skimming) is a process of generating a shorter video (video skim) as a summary of a given video, which helps in its easier and quicker comprehension. In this study, an efficient dynamic summarisation approach for user videos is proposed using vector ordering for ranking video units (frames/shots). User videos are casually shot unscripted videos, where skimming involves the selection of its interesting part(s) ignoring many uninteresting ones. The concept of R-ordering of vectors is employed to find a representative frame, which is used to perform relative ranking of the video frames. It is theoretically shown that significance is given to each element of a frame&#39;s feature vector while computing the importance scores that lead to the frame ranks used for skimming. Furthermore, the allocation of different weights to the features involved is also achieved using linear and Gaussian process regressions. Through extensive experiments considering several standard datasets with human-labelled ground truth, the proposed approach is demonstrated to be efficient and to perform better than the relevant state-of-the-art.},
  archive      = {J_IETIP},
  author       = {Vivekraj V K and Debashis Sen and Balasubramanian Raman},
  doi          = {10.1049/iet-ipr.2020.0234},
  journal      = {IET Image Processing},
  month        = {12},
  number       = {15},
  pages        = {3941-3956},
  shortjournal = {IET Image Process.},
  title        = {Vector ordering and regression learning-based ranking for dynamic summarisation of user videos},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Defect detection of printed circuit board based on
lightweight deep convolution network. <em>IETIP</em>, <em>14</em>(15),
3932–3940. (<a href="https://doi.org/10.1049/iet-ipr.2020.0841">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rapid development of the electronic industry, the defect detection of printed circuit board (PCB) components is becoming more and more important. The types of PCB components are diverse and accompanied by complex character information, which is difficult to identify. The traditional detection method is inefficient, and it is unable to effectively perform the diversified category detection of PCB components and character recognition in complex scenes. The deep convolutional neural network has obvious advantages in object detection and character recognition, which can be used to implement a PCB component defect detection system. In this study, the authors have established a lightweight PCB type detection model called LD-PCB, which can perform real-time detection while improving detection accuracy. In addition, in the character detection of PCB, they have established a fast and robust character recognition model, called CR-PCB. This model can effectively improve the accuracy of irregular character recognition. Finally, they established and published a dataset of PCB components, and combined with LD-PCB and CR-PCB to realise the PCB defect detection system. This system can realise the functions of defect detection, wrong insertion, missing insertion, and character recognition in industrial PCB production. The results show that the method proposed in this study can effectively detect defects on PCB components.},
  archive      = {J_IETIP},
  author       = {Jiaquan Shen and Ningzhong Liu and Han Sun},
  doi          = {10.1049/iet-ipr.2020.0841},
  journal      = {IET Image Processing},
  month        = {12},
  number       = {15},
  pages        = {3932-3940},
  shortjournal = {IET Image Process.},
  title        = {Defect detection of printed circuit board based on lightweight deep convolution network},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Printed persian OCR system using deep learning.
<em>IETIP</em>, <em>14</em>(15), 3920–3931. (<a
href="https://doi.org/10.1049/iet-ipr.2019.0728">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Optical character recognition, known as OCR, has been widely used due to high demand of different technologies. Currently, most existing OCR systems have been focused on Latin languages. In recent studies, OCR systems for non-Latin texts involving cursive style have also been introduced despite posing some challenges. In this study, the authors propose an OCR system based on long short-term memory neural networks for the Persian language. The authors also investigate the effects of variations of parameters, involved in this approach. The proposed OCR system solves false recognition of sub-word ‘LA’ and ‘LA’. Moreover, the authors present a preprocessing algorithm to remove ‘justification’ using image processing. A new comprehensive collated data set is introduced, comprising five million images with eight popular Persian fonts and in ten various font sizes. The proposed evaluations show that the accuracy of the proposed OCR is increased by 2%, compared to the existing Persian OCR system. The experimental results indicated that the proposed system has average accuracy of 99.69% at the letter level. The proposed system has an accuracy of 98.1% for ‘zero-width non-breaking space’ and 98.64% for ‘LA’ at the word level.},
  archive      = {J_IETIP},
  author       = {Marziye Rahmati and Mansoor Fateh and Mohsen Rezvani and Alireza Tajary and Vahid Abolghasemi},
  doi          = {10.1049/iet-ipr.2019.0728},
  journal      = {IET Image Processing},
  month        = {12},
  number       = {15},
  pages        = {3920-3931},
  shortjournal = {IET Image Process.},
  title        = {Printed persian OCR system using deep learning},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Hyperspectral remote sensing image classification using
combinatorial optimisation based un-supervised band selection and CNN.
<em>IETIP</em>, <em>14</em>(15), 3909–3919. (<a
href="https://doi.org/10.1049/iet-ipr.2020.0728">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hyperspectral image (HSI) consists of hundreds of contiguous spectral bands, which can be used in the classification of different objects on the earth. The inclusion of both spectral as well as spatial features stands essential in order that high classification accuracy is achieved. However, incorporation of the spectral and spatial information without preserving the intrinsic structure of the data leads on to downscaling the classification accuracy. To address the issue aforementioned, the proposed method which involves using unsupervised spectral band selection based on three major constrains: (i) low reconstruction error with neighbourhood bands, (ii) low noise, (iii) high information entropy, is put forward. In addition, the structure-preserving recursive filter is used to extract spatial features. Finally, the classification is performed using convolutional neural networks (CNNs) with different sets of convolutional, pooling, and fully connected layers. To test the performance of the proposed method, experiments have been carried out with three benchmark HSI datasets Indian pines, University of Pavia, and Salinas. These experiments reveal that the proposed method offers better classification accuracy over the purportedly state-of-the-art methods in terms of standard metrics like overall accuracy, average accuracy, and kappa coefficient ( K ). The proposed method has attained OAs of 99.9, 98.9, and 99.93% for the three datasets, respectively.},
  archive      = {J_IETIP},
  author       = {Radhesyam Vaddi and Prabukumar Manoharan},
  doi          = {10.1049/iet-ipr.2020.0728},
  journal      = {IET Image Processing},
  month        = {12},
  number       = {15},
  pages        = {3909-3919},
  shortjournal = {IET Image Process.},
  title        = {Hyperspectral remote sensing image classification using combinatorial optimisation based un-supervised band selection and CNN},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). High-precision refocusing method with one interpolation for
camera array images. <em>IETIP</em>, <em>14</em>(15), 3899–3908. (<a
href="https://doi.org/10.1049/iet-ipr.2019.0081">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Camera array image refocusing can change the in-focus region so that objects lying on a specified plane are in focus, whereas objects lying off this plane are blurred. Existing refocusing methods for camera array or light field images usually contain two interpolations. Since interpolation brings distortion, especially on the sharp edge in the images, existing methods are not sufficiently precise. In order to improve the quality of the refocusing result, the authors propose a high-precision method to refocus camera array images. They first back-project the pixel coordinates to a corresponding location on the focal plane in the world coordinate. Then they reproject the world coordinates to the pixel coordinates by using the parameters of each camera in the array. After that, they align the images with the focal plane by employing interpolation according to the acquired pixel coordinates. Finally, they get the synthetic image refocused on the focal plane by averaging the resulted images. In the proposed method, only one interpolation is used. So that it alleviates the quality degradation of the refocused image compared to the existing methods. Experiments on real-world scenes (captured by their self-developed light field devices) demonstrate that their method can yield better results than the existing methods.},
  archive      = {J_IETIP},
  author       = {Jungang Yang and Chao Xiao and Yingqian Wang and Chengjin An and Wei An},
  doi          = {10.1049/iet-ipr.2019.0081},
  journal      = {IET Image Processing},
  month        = {12},
  number       = {15},
  pages        = {3899-3908},
  shortjournal = {IET Image Process.},
  title        = {High-precision refocusing method with one interpolation for camera array images},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). HU-PageScan: A fully convolutional neural network for
document page crop. <em>IETIP</em>, <em>14</em>(15), 3890–3898. (<a
href="https://doi.org/10.1049/iet-ipr.2020.0532">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {November The offer of online, automated, and impersonal services demand users to upload scanned copies of their documents to the organisations. As a consequence of this decentralisation, the documents present more challenges to the already complex process of image processing and information extraction. To address this problem, the authors presented an optimised fully convolutional neural network model for document segmentation that works on mobile devices to detect the region of the document in the captured image. They performed experiments in three representative datasets comparing the proposed method with the Geodesic object Proposals, U-net, Mask R-CNN, and OctHU-PageScan algorithms. They also compared the proposed model with all competitors of the ICDAR2015 Competition on smartphone document capture. Furthermore, they performed a qualitative and comparative analysis with the CamScanner software, a popular app for Android and iOS smartphones used for more than 100 million users in over 200 countries. The proposed approach achieved a significant performance compared with the current state-of-the-art methods, providing a powerful approach for document segmentation in photos and scanned images.},
  archive      = {J_IETIP},
  author       = {Ricardo Batista das Neves and Estanislau Lima and Byron L.D. Bezerra and Cleber Zanchettin and Alejandro H. Toselli},
  doi          = {10.1049/iet-ipr.2020.0532},
  journal      = {IET Image Processing},
  month        = {12},
  number       = {15},
  pages        = {3890-3898},
  shortjournal = {IET Image Process.},
  title        = {HU-PageScan: A fully convolutional neural network for document page crop},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Image enhancement using convolutional neural network to
identify similar patterns. <em>IETIP</em>, <em>14</em>(15), 3880–3889.
(<a href="https://doi.org/10.1049/iet-ipr.2020.0560">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {An image may be disturbed by impulse noise during transmission or acquisition. To effectively restore the disturbed image is important for the applications of image processing. This study aims at enhancing the disturbed images by using the convolutional neural network (CNN) to identify similar patterns for the restoration of noisy pixels. In the training phase, each noisy pixel is analysed and compared with the noise-free image to find the closest neighbouring pixels. The pixels in a local window form a micro-pattern. All the captured micro-patterns, whose centre pixel is noisy, become a dataset for the training of a position CNN. The closest neighbouring pixel of a noisy image to the centre one of the noise-free image at the same position of each micro-pattern is selected to be the target. In the enhancement phase, a noisy micro-pattern, where the centre pixel is noisy, is input into the trained position CNN. The top N pixels are recognised and averaged to replace the grey level of the centre pixel. An enhanced pixel is obtained. The experimental results show that the position CNN can well recognise the similar neighbouring pixels and effectively enhance the noisy pixels in an image disturbed by salt-and-pepper noise.},
  archive      = {J_IETIP},
  author       = {Ching-Ta Lu and Ruei-Han Chen and Ling-Ling Wang and Jia-An Lin},
  doi          = {10.1049/iet-ipr.2020.0560},
  journal      = {IET Image Processing},
  month        = {12},
  number       = {15},
  pages        = {3880-3889},
  shortjournal = {IET Image Process.},
  title        = {Image enhancement using convolutional neural network to identify similar patterns},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). PReLU and edge-aware filter-based image denoiser using
convolutional neural network. <em>IETIP</em>, <em>14</em>(15),
3869–3879. (<a href="https://doi.org/10.1049/iet-ipr.2020.0717">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Convolutional neural networks (CNNs) based on the discriminative learning model have been widely used for image denoising. In this study, a feed-forward denoising CNN (DnCNN) with a parametric rectified linear unit (PReLU) is used to improve the denoising performance. PReLU enhances the model fitting of the DnCNN network without affecting computational cost. This network learns the leaky parameter of negative inputs in an activation function and therefore finds a proper slope in a negative direction. The proposed denoising network is based on residual learning, which comprises repeated convolutional and PReLU units along with batch normalisation. Residual learning with batch normalisation accelerates the network training, which can be used for blind Gaussian denoising. In this network, feature maps are processed by principal component analysis and transferred to subsequent convolution layers. An adaptive bilateral filter further processes the output image of the proposed CNN for image smoothening and sharpening. The mean and variance of the Gaussian kernel of adaptive filter vary from pixel to pixel. The performance of this network is analysed on BSD-68 and Set-12 datasets, and it exhibits an improvement in peak signal-to-noise ratio and structural similarity index metric and visual representation over other state-of-the-art methods.},
  archive      = {J_IETIP},
  author       = {Rini Smita Thakur and Ram Narayan Yadav and Lalita Gupta},
  doi          = {10.1049/iet-ipr.2020.0717},
  journal      = {IET Image Processing},
  month        = {12},
  number       = {15},
  pages        = {3869-3879},
  shortjournal = {IET Image Process.},
  title        = {PReLU and edge-aware filter-based image denoiser using convolutional neural network},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Multimodal biometric recognition systems using deep learning
based on the finger vein and finger knuckle print fusion.
<em>IETIP</em>, <em>14</em>(15), 3859–3868. (<a
href="https://doi.org/10.1049/iet-ipr.2020.0491">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recognition systems using multimodal biometrics attracts attention because they improve recognition efficiency and high-security level compared to the unimodal biometrics system. In this study, the authors present a secure multimodal biometrics recognition system based on the deep learning method that uses convolutional neural networks (CNNs). The authors propose two multimodal architectures using the finger knuckle print (FKP) and the finger vein (FV) biometrics with different levels of fusion: the features level fusion and scores level fusion. The features extraction for FKP and FV are performed using transfer learning CNN architectures: AlexNet, VGG16, and ResNet50. The key step aims to select separate features descriptors from each unimodal biometrics modality. After that, the authors combine them using the proposed fusion approaches were support vector machine or Softmax applies as classifiers to increase the proposed system security. The efficiency of the proposed algorithms is tested using publicly available biometrics databases. The experimental results show that the proposed fusion architectures achieve an accuracy of 99.89% and an equal error rate of 0.05%. The obtained results indicate that the biometrics recognition system using deep learning is secure, robust, and reliable.},
  archive      = {J_IETIP},
  author       = {Sara Daas and Amira Yahi and Toufik Bakir and Mouna Sedhane and Mohamed Boughazi and El-Bay Bourennane},
  doi          = {10.1049/iet-ipr.2020.0491},
  journal      = {IET Image Processing},
  month        = {12},
  number       = {15},
  pages        = {3859-3868},
  shortjournal = {IET Image Process.},
  title        = {Multimodal biometric recognition systems using deep learning based on the finger vein and finger knuckle print fusion},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Ensemble-based glioma grade classification using gabor
filter bank and rotation forest. <em>IETIP</em>, <em>14</em>(15),
3851–3858. (<a href="https://doi.org/10.1049/iet-ipr.2020.0908">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This work aims at developing an automated ensemble-based glioma grade classification framework that classifies glioma into low-grade glioma (LGG) and high-grade glioma (HGG). Discriminant features are extracted using the Gabor filter bank and concatenated in a vectorised form. The feature set is then divided into k subsets of features. An ensemble of base classifiers known as rotation forest is employed for classification purpose. Independent components analysis (ICA) is applied on every feature subset and independent features are extracted. Each classifier in the ensemble is trained with these independent features from all the subset of features. These k feature subsets are responsible for different rotations during the training phase. This results in classifier diversity in the ensemble. Extensive experiments are conducted on benchmark BraTS 2017 data set and comparative analysis reveals that the proposed framework outperforms the competitive techniques in terms of various performance metrics. Data-augmentation technique, synthetic minority over-sampling technique is applied to oversample minority class samples alleviate class biasness problem. The proposed classification framework achieves an accuracy of 98.63%, dice similarity coefficient of 0.98 and sensitivity of 0.96. The authors conduct different comparative experiments with state-of-the-art ensemble-based, deep learning-based and traditional machine learning-based classification approaches to validate the performance of the proposed framework.},
  archive      = {J_IETIP},
  author       = {Rahul Singh and Aditya Goel and D.K. Raghuvanshi},
  doi          = {10.1049/iet-ipr.2020.0908},
  journal      = {IET Image Processing},
  month        = {12},
  number       = {15},
  pages        = {3851-3858},
  shortjournal = {IET Image Process.},
  title        = {Ensemble-based glioma grade classification using gabor filter bank and rotation forest},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). CMCS-net: Image compressed sensing with convolutional
measurement via DCNN. <em>IETIP</em>, <em>14</em>(15), 3839–3850. (<a
href="https://doi.org/10.1049/iet-ipr.2020.0834">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, deep learning methods have made a remarkable improvement in compressed sensing image recovery stage. In the compressed measurement stage, the existing methods measured by block by block owing to a huge measurement dictionary for the whole images and the high computational complexity. In this work, a novel deep convolutional neural network (DCNN) named Convolutional Measurement Compressed Sensing network (CMCS-net) is proposed for image compressed sensing considering both convolutional measurement (CM) and sparse prior. Different from existing works, the convolution operation is adopted both in the measurement phase and reconstruction phase, which retains the structure information of images much better. Simultaneously, the size of the measurement matrix is no longer limited by data dimensions. Particularly, by unfolding the CM process to analyse a Toeplitz-type matrix, the theoretical support of the convolutional compressed measurement is proposed. In addition, in the recovery phase, the authors consider the sparse prior in nature images by embedding the truncated hierarchical projection algorithm into their architecture to solve the problem of multilayered convolutional sparse coding. Furthermore, extensive experiments demonstrate that their proposed CMCS-net can marvellously reconstruct the images and fully remove the block artefact.},
  archive      = {J_IETIP},
  author       = {Yahong Xie and Hailin Wang and Jianjun Wang},
  doi          = {10.1049/iet-ipr.2020.0834},
  journal      = {IET Image Processing},
  month        = {12},
  number       = {15},
  pages        = {3839-3850},
  shortjournal = {IET Image Process.},
  title        = {CMCS-net: Image compressed sensing with convolutional measurement via DCNN},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Fast and robust image watermarking method in the spatial
domain. <em>IETIP</em>, <em>14</em>(15), 3829–3838. (<a
href="https://doi.org/10.1049/iet-ipr.2019.1740">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To solve the copyright protection problem of a colour image, a new blind colour image watermarking method combining a discrete cosine transform (DCT) in the spatial domain is presented in this study. The advantages of the spatial-domain watermarking algorithm and frequency-domain one are made full use in this scheme. Based on the different quantisation steps in red, green, and blue three-layer images, the processes of watermark embedding and blind extraction are completed in the spatial domain without a real DCT domain. The scheme is realised by using the unique features of the direct current (DC) coefficient and the relativity of DC coefficients between adjacent pixel blocks. This scheme can effectively solve the problems of the large-capacity colour image watermarking algorithm, such as long-running time and weak robustness. Comparing with other advanced watermarking algorithms, the presented scheme has better invisibility, stronger robustness, and higher real-time performance.},
  archive      = {J_IETIP},
  author       = {Zihan Yuan and Qingtang Su and Decheng Liu and Xueting Zhang and Tao Yao},
  doi          = {10.1049/iet-ipr.2019.1740},
  journal      = {IET Image Processing},
  month        = {12},
  number       = {15},
  pages        = {3829-3838},
  shortjournal = {IET Image Process.},
  title        = {Fast and robust image watermarking method in the spatial domain},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Two-stage visible watermark removal architecture based on
deep learning. <em>IETIP</em>, <em>14</em>(15), 3819–3828. (<a
href="https://doi.org/10.1049/iet-ipr.2020.0444">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rapid development of the Internet, watermarks are widely used in images to protect copyright. This implies that the robustness of watermark is very important. In recent years, there have been some studies to evaluate watermark performance by removing the watermark. Among them, some methods need to mark the watermark position in advance, and some require multiple images with the same watermark. Moreover, when the colour of thewatermark is similar to that of the background, the existing methods can hardly remove the watermark from the watermarked image. In the proposed work, the authors presented a watermark removal structure consisting of watermark extraction and image inpainting to address the aforementioned issues. In particular, the extraction network is used to extract the watermark in the watermarked image, and the inpainting network is used to inpainting image for a better watermark removal image, respectively. Finally, the authors train and test the developed network architecture by constructing two data sets, i.e. white watermarked image data set (WW-data set) and colour watermarked image data set (CW-data set). The proposed method not only has better performance on the WW-data set than the current latest methods (on the CW-data set, other methods have almost failed) but also effectively removes the watermarks.},
  archive      = {J_IETIP},
  author       = {Pei Jiang and Shiwen He and Hufei Yu and Yaoxue Zhang},
  doi          = {10.1049/iet-ipr.2020.0444},
  journal      = {IET Image Processing},
  month        = {12},
  number       = {15},
  pages        = {3819-3828},
  shortjournal = {IET Image Process.},
  title        = {Two-stage visible watermark removal architecture based on deep learning},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Automatic production of synthetic labelled OCT images using
an active shape model. <em>IETIP</em>, <em>14</em>(15), 3812–3818. (<a
href="https://doi.org/10.1049/iet-ipr.2020.0075">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Limited labelled data is a challenge in the field of medical imaging and the need for a large number of them is paramount for the training of machine learning algorithms, as well as measuring the performance of image processing algorithms. The purpose of this study is to construct synthetic and labelled optical coherence tomography (OCT) data to solve the problems of having access to accurately labelled data and evaluating the processing algorithms. In this study, a modified active shape model is used which considers the anatomical features of available images such as the number and thickness of the layers as well as their associated brightness, the location of retinal blood vessels and shadow information with respect to speckle noise. The algorithm is also able to provide different data sets with the varying noise level. The validity of the proposed method for the synthesis of retinal images is measured by two methods (qualitative assessment and quantitative analysis).},
  archive      = {J_IETIP},
  author       = {Hajar Danesh and Keivan Maghooli and Alireza Dehghani and Rahele Kafieh},
  doi          = {10.1049/iet-ipr.2020.0075},
  journal      = {IET Image Processing},
  month        = {12},
  number       = {15},
  pages        = {3812-3818},
  shortjournal = {IET Image Process.},
  title        = {Automatic production of synthetic labelled OCT images using an active shape model},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Wise optimisation: Deep image embedding by informative pair
weighting and ranked list learning. <em>IETIP</em>, <em>14</em>(15),
3802–3811. (<a href="https://doi.org/10.1049/iet-ipr.2020.0454">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep image embedding learns how to map images onto feature vectors. Image retrieval performance is often used to evaluate embedding quality. In this study, the authors proposed a wise deep image embedding optimisation (WDIEO) algorithm based on informative pair weighting and ranked list learning (IPWRLL) for network optimisation of fine-grained image retrieval. First, a hard sample mining method Top- k is proposed to select positive and negative samples. Then, for the selected query sample, a ranking list is obtained by comparing the similarity between samples in the data set and the query sample, and the sample is labelled according to the similarity. Finally, for positive samples, two optimisation rules with different functions are used, while ensuring two key issues of instance weighting and intra-class data distribution. For negative samples, different from the widely adopted methods based on the weight of sample information, the authors’ algorithm&#39;s weights are set according to the ranking list, which keeps the inter-class data distribution and the optimisation direction consistent with the loss reduction direction. The WDIEO-IPWRLL model is an end-to-end optimisation that can share parameters in the testing process. Experiments show that their proposed model achieves the state-of-the-art performance on the benchmark data set.},
  archive      = {J_IETIP},
  author       = {Lili Fan and Hongwei Zhao and Haoyu Zhao},
  doi          = {10.1049/iet-ipr.2020.0454},
  journal      = {IET Image Processing},
  month        = {12},
  number       = {15},
  pages        = {3802-3811},
  shortjournal = {IET Image Process.},
  title        = {Wise optimisation: Deep image embedding by informative pair weighting and ranked list learning},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Deep convolutional neural network for mixed random impulse
and gaussian noise reduction in digital images. <em>IETIP</em>,
<em>14</em>(15), 3791–3801. (<a
href="https://doi.org/10.1049/iet-ipr.2019.0931">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study utilises a deep convolutional neural network (CNN) implementing regularisation and batch normalisation for the removal of mixed, random, impulse, and Gaussian noise of various levels from digital images. This deep CNN achieves minimal loss of detail and yet yields an optimal estimation of structural metrics when dealing with both known and unknown noise mixtures. Moreover, a comprehensive comparison of denoising filters through the use of different structural metrics is provided to highlight the merits of the proposed approach. Optimal denoising results were obtained by using a 20-layer network with 40 × 40 patches trained on 400 180 × 180 images from the Berkeley segmentation data set (BSD) and tested on the BSD100 data set and an additional 12 images of general interest to the research community. The comparative results provide credence to the merits of the proposed filter and the comprehensive assessment of results highlights the novelty and performance of this CNN-based approach.},
  archive      = {J_IETIP},
  author       = {Mehdi Mafi and Walter Izquierdo and Harold Martin and Mercedes Cabrerizo and Malek Adjouadi},
  doi          = {10.1049/iet-ipr.2019.0931},
  journal      = {IET Image Processing},
  month        = {12},
  number       = {15},
  pages        = {3791-3801},
  shortjournal = {IET Image Process.},
  title        = {Deep convolutional neural network for mixed random impulse and gaussian noise reduction in digital images},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Target object detection using chicken social-based deep
belief network with hyperspectral imagery. <em>IETIP</em>,
<em>14</em>(15), 3781–3790. (<a
href="https://doi.org/10.1049/iet-ipr.2020.0344">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Target object detection is an important research direction in the area of hyperspectral imaging (HSI) as it aims to detect the anomalies or objects in HSI. Some of the existing target object detection methods are merely suitable for HSI with low resolution as they failed to apply directly in the high-resolution HSI. Therefore, an effective target detection method named chicken social-based deep belief network (CS-based DBN) is proposed to achieve an automatic target object detection framework in the high-resolution HSI. The proposed CS-based DBN is developed by integrating the chicken swarm optimisation with the social ski-driver algorithm. The optimal solution for detecting the target object is revealed through the fitness function, which accepts the minimal error value as the best solution. Moreover, the weights of the DBN classifier are optimally trained based on the proposed algorithm to render an accurate and optimal solution in detecting the target objects. The proposed CS-based DBN obtained better performance through the facility of stochastic exploration in search space. Moreover, the results achieved using the proposed model in terms of accuracy, specificity, and sensitivity are 0.8950, 0.8940, and 0.9, respectively.},
  archive      = {J_IETIP},
  author       = {Sherin Shibi and Gayathri Rajagopal},
  doi          = {10.1049/iet-ipr.2020.0344},
  journal      = {IET Image Processing},
  month        = {12},
  number       = {15},
  pages        = {3781-3790},
  shortjournal = {IET Image Process.},
  title        = {Target object detection using chicken social-based deep belief network with hyperspectral imagery},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). EKFPnP: Extended kalman filter for camera pose estimation in
a sequence of images. <em>IETIP</em>, <em>14</em>(15), 3774–3780. (<a
href="https://doi.org/10.1049/iet-ipr.2020.0606">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In real-world applications the perspective-n-point (PnP) problem should generally be applied to a sequence of images which a set of drift-prone features are tracked over time. In this study, the authors consider both the temporal dependency of camera poses and the uncertainty of features for the vision-only sequential camera pose estimation. Using the extended Kalman filter (EKF), a priori estimate of the camera pose is calculated from the camera motion model and then it is corrected by minimising the reprojection error of the reference points. Applying probabilistic approach also provides the covariance of the pose parameters which helps to measure the reliability of the estimated parameters. Experimental results, using both synthetic and real data, demonstrate that the proposed method improves the robustness of the camera pose estimation, in the presence of tracking error and feature matching outliers, compared to the state of the art.},
  archive      = {J_IETIP},
  author       = {Mohammad Amin Mehralian and Mohsen Soryani},
  doi          = {10.1049/iet-ipr.2020.0606},
  journal      = {IET Image Processing},
  month        = {12},
  number       = {15},
  pages        = {3774-3780},
  shortjournal = {IET Image Process.},
  title        = {EKFPnP: Extended kalman filter for camera pose estimation in a sequence of images},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Rotational invariant biologically inspired object
recognition. <em>IETIP</em>, <em>14</em>(15), 3762–3773. (<a
href="https://doi.org/10.1049/iet-ipr.2019.1621">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The biologically inspired hierarchical model and X (HMAX) has been one of the superior techniques for object recognition purposes. HMAX is a robust method in the presence of some image variations including illumination, different scales, and location changes. However, the performance of HMAX extremely deteriorates if the orientation of the applied images in training phase is different than the orientation of testing images. In this study, the authors propose rotational invariant HMAX (RIMAX) to overcome the existing issues in object recognition imposed by rotations in the images. To this end, they embed two new layers into the structure of the standard HMAX. In the first added layer, non-accidental properties (e.g. corners and edges) are extracted as features that lead to obtaining a repeatable object recognition process. The second added layer provides robustness of RIMAX to image rotations by normalising the dominant orientation of the extracted features. Furthermore, they considerably reduce the imposed computational load by modifying template matching strategy as well as removing multiple scales of the Gabor filter. Simulation results employ Caltech101, TUD, Caltech5, and GRAZ-02 databases that validate RIMAX outperforms the standard HMAX and the other mathematical approaches in terms of robustness, accuracy, repeatability, and speed.},
  archive      = {J_IETIP},
  author       = {Hiwa Sufi karimi and Karim Mohammadi},
  doi          = {10.1049/iet-ipr.2019.1621},
  journal      = {IET Image Processing},
  month        = {12},
  number       = {15},
  pages        = {3762-3773},
  shortjournal = {IET Image Process.},
  title        = {Rotational invariant biologically inspired object recognition},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Multi-camera 3D ball tracking framework for sports video.
<em>IETIP</em>, <em>14</em>(15), 3751–3761. (<a
href="https://doi.org/10.1049/iet-ipr.2020.0757">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate ball tracking in sports is vital for automatic sports analysis yet it is challenging mainly due to the small size and occlusions. This study proposes a novel multi-camera 3D ball tracking (MBT) framework for sports video. The proposed framework consists of four parts: 2D ball detection, 2D ball tracking, 3D position fusion, and 3D ball tracking. In 2D aspect, the multi-scale features are introduced to enhance the 2D ball detection, and the 2D ball tracking is also improved by exploring cross-view information to handle the occlusion and timely updating tracking model with detection results to alleviate the problem of tracking drift. For 3D ball, a novel 3D position fusion method is proposed to optimise the ball position and the 3D ball tracking approach with improved Kalman filter is finally applied to ensure a smooth 3D ball trajectory. Moreover, compared to the existing products in commercial, the proposed framework does not require any special equipment and is thus low cost. Extensive experiments for 2D and 3D ball on a public dataset demonstrate that the proposed framework is robust to ball tracking in sports video, even in the presence of environmental interferences, substantial occlusions, and even calibration errors.},
  archive      = {J_IETIP},
  author       = {Wanneng Wu and Min Xu and Qiaokang Liang and Li Mei and Yu Peng},
  doi          = {10.1049/iet-ipr.2020.0757},
  journal      = {IET Image Processing},
  month        = {12},
  number       = {15},
  pages        = {3751-3761},
  shortjournal = {IET Image Process.},
  title        = {Multi-camera 3D ball tracking framework for sports video},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Genetic-based feature fusion in face recognition using
arithmetic coded local binary patterns. <em>IETIP</em>, <em>14</em>(15),
3742–3750. (<a href="https://doi.org/10.1049/iet-ipr.2020.0394">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Local binary patterns (LBPs) are one of the attempts for gathering local features with face recognition algorithms. Although the application of LBP&#39;s in many recognition contents is too apparent, these methods have limited accuracy because of their threshold value. One problem is earning one value for two different regions with a diverse pixel neighbourhood, which causes mistakes in feature vector and decreases the discriminative power. In this study, the authors proposed a modified LBP that covers the LBP&#39;s disadvantages. The proposed approach is arithmetic coded LBP (ACLBP) that uses arithmetic coding process during LBP calculation instead of applying original thresholds. The proposed policy addresses the problem of returning one similar LBP value for two different patches. Moreover, the proposed method modifies LBP by using a different threshold for calculating the pixels differences. Using this algorithm, the authors conducted a genetic-based feature fusion method by combining LBP and histogram of oriented gradients and ACLBP. The proposed approach could work better on LFW dataset, and the ORL dataset and Yale face dataset that shows the improving role of ACLBP in comparison with the earlier version of LBP.},
  archive      = {J_IETIP},
  author       = {Saeed Najafi Khanbebin and Vahid Mehrdad},
  doi          = {10.1049/iet-ipr.2020.0394},
  journal      = {IET Image Processing},
  month        = {12},
  number       = {15},
  pages        = {3742-3750},
  shortjournal = {IET Image Process.},
  title        = {Genetic-based feature fusion in face recognition using arithmetic coded local binary patterns},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Water level monitoring using classification techniques on
landsat-8 data at sangam region, prayagraj, india. <em>IETIP</em>,
<em>14</em>(15), 3733–3741. (<a
href="https://doi.org/10.1049/iet-ipr.2020.1078">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the present study, Landsat-8 data have been used for water identification at the Sangam region, the confluence of the Ganges and Yamuna rivers. Since the water of both the rivers varies in characteristics, it is necessary to explore the most appropriate band of Landsat-8 imagery for the identification of water bodies. Accordingly, a two folded study has been performed, viz., the selection of a suitable band to identify water bodies and to monitor the water level at the Sangam region from 2013 to 2018. It is observed in the study that band-7 is the most suitable band for water identification, which is used for the classification and monitoring of water. The classification results are highly significant as the overall accuracy ranges between 95.49 and 100%. The monitoring of water level is done based on classification results for mapping the area of water in real units which can be useful for the prediction of flood situations. The achievement of the present study is the mapping of water area in real units with the help of image pixels which is one of the prominent applications of image processing in earth sciences.},
  archive      = {J_IETIP},
  author       = {Vikash K. Mishra and Triloki Pant},
  doi          = {10.1049/iet-ipr.2020.1078},
  journal      = {IET Image Processing},
  month        = {12},
  number       = {15},
  pages        = {3733-3741},
  shortjournal = {IET Image Process.},
  title        = {Water level monitoring using classification techniques on landsat-8 data at sangam region, prayagraj, india},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Local contrast measure with iterative error for infrared
small target detection. <em>IETIP</em>, <em>14</em>(15), 3725–3732. (<a
href="https://doi.org/10.1049/iet-ipr.2020.1157">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Local contrast measure (LCM) has been proved to be an effective method for infrared small target detection. However, the detection performance of LCM decreases dramatically when the background contains strong edges and pixel-sized noises with high brightness (PNHB). Based on the analysis of the inherent causes of the poor performance of LCM in extremely complex backgrounds, this study presents an effective LCM with an iterative error. The contribution is as follows: first, the two-dimensional least mean square (TDLMS) filter with an adaptive parameter is applied to suppress the background clutters roughly in each multiscale window. Then, the partial maximum pixel mean is applied to the LCM to optimise the sub-block statistical parameters, which achieves excellent strong edges suppression performance. Finally, the iteration error generated by TDLMS and the sub-block weight matrix is updated alternately to further optimise the statistical parameters of the contrast measure to make it more effective in suppressing PNHB. Experimental results demonstrate that the proposed approach is not only superior to the contrast methods in terms of high detection efficiency and low false alarm rate but also has satisfactory adaptability under extremely complex backgrounds.},
  archive      = {J_IETIP},
  author       = {Zujing Yan and Yunhong Xin and Yixuan Zhang},
  doi          = {10.1049/iet-ipr.2020.1157},
  journal      = {IET Image Processing},
  month        = {12},
  number       = {15},
  pages        = {3725-3732},
  shortjournal = {IET Image Process.},
  title        = {Local contrast measure with iterative error for infrared small target detection},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Retinal image blood vessel extraction and quantification
with euclidean distance transform approach. <em>IETIP</em>,
<em>14</em>(15), 3718–3724. (<a
href="https://doi.org/10.1049/iet-ipr.2020.0336">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image processing applications remarkably contributes to modern ophthalmology. This technology is designed to analyse the characteristics of the human eye microvasculature images. The retinal microvasculature is an excellent non-invasive screening window for the assessment of systemic diseases such as diabetes, hypertension, and stroke. Retinal microvasculature character such as widening vessel diameter is recognised as an analysable feature for stroke or transient ischemic attack for predicting the progression of this pathology. Thus, in this study, a computer-assisted method has been developed for this task applying the Euclidean distance transform (EDT) technique. This newly developed algorithm computes the Euclidean distance of the remaining white pixels on the area of interest. Central Light Reflex Image Set (CLRIS) and Vascular Disease Image Set (VDIS) of Retinal Vessel Image set for Estimation of Width database were used for the performance evaluation of the proposed algorithm that showed 98.1 and 97.7% accurate result for both CLRIS and VDIS, respectively. The significantly high accuracy in this newly developed vessel diameter quantification algorithm indicates excellent potential for further development, evaluation, validation, and integration into ophthalmic diagnostic instruments.},
  archive      = {J_IETIP},
  author       = {Kuryati Kipli and Mohammed Enamul Hoque and Lik Thai Lim and Tengku Mohd Afendi Zulcaffle and Siti Kudnie Sahari and Muhammad Hamdi Mahmood},
  doi          = {10.1049/iet-ipr.2020.0336},
  journal      = {IET Image Processing},
  month        = {12},
  number       = {15},
  pages        = {3718-3724},
  shortjournal = {IET Image Process.},
  title        = {Retinal image blood vessel extraction and quantification with euclidean distance transform approach},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Segmentation of brain magnetic resonance images using a
novel fuzzy clustering based method. <em>IETIP</em>, <em>14</em>(15),
3705–3717. (<a href="https://doi.org/10.1049/iet-ipr.2020.0383">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Segmentation of tissues in brain magnetic resonance (MR) images has a crucial role in computer-aided diagnosis (CAD) of various brain diseases. However, due to the complex anatomical structure and the presence of intensity non-uniformity (INU) artefact, the segmentation of brain MR images is considered as a complicated task. In this study, the authors propose a novel locally influenced fuzzy C-means (LIFCM) clustering for segmentation of tissues in MR brain images. The proposed method incorporates local information in the clustering process to achieve accurate labelling of pixels. A novel local influence factor is proposed, which estimates the influence of a neighbouring pixel on the centre pixel. Furthermore, they have introduced the kernel-induced distance in LIFCM, which deals with complex brain MR data and produces effective segmentation. To evaluate the performance of the proposed method, they have used one simulated and one real MRI data set. Extensive experimental findings suggest that the authors&#39; method not only produces effective segmentation but also retains crucial image details. The statistical significance test has been further conducted to support their experimental observations.},
  archive      = {J_IETIP},
  author       = {Prasun Chandra Tripathi and Soumen Bag},
  doi          = {10.1049/iet-ipr.2020.0383},
  journal      = {IET Image Processing},
  month        = {12},
  number       = {15},
  pages        = {3705-3717},
  shortjournal = {IET Image Process.},
  title        = {Segmentation of brain magnetic resonance images using a novel fuzzy clustering based method},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Retracted: Combined mixed gaussian model with pattern
recognition in the automatic diagnosis of alzheimer’s disease.
<em>IETIP</em>, <em>14</em>(15), 3698–3704. (<a
href="https://doi.org/10.1049/iet-ipr.2019.1629">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The diagnosis and prevention of Alzheimer&#39;s disease plays an important role in improving patient cognition. It can be seen from the current situation that the diagnosis of Alzheimer&#39;s disease is still poor because it is affected by many factors. Based on this, combined with the symptoms of Alzheimer&#39;s disease, this study used computer-aided to diagnose the symptoms of patients. First of all, this study analyses classical machine learning and chooses the appropriate model for diagnosis. Next, this study constructs a diagnostic system based on a mixed Gaussian model and uses a mixed Gaussian model to predict the probability of different distribution methods. Finally, this study designs experiments to analyse the performance of diagnostic models. Studies have shown that the mixed Gaussian model has a good effect on the automatic diagnosis of Alzheimer&#39;s disease, and can provide a theoretical reference for subsequent related research.},
  archive      = {J_IETIP},
  author       = {Mingyue Qian and Zhaoting Zhang and Jiechun Chen},
  doi          = {10.1049/iet-ipr.2019.1629},
  journal      = {IET Image Processing},
  month        = {12},
  number       = {15},
  pages        = {3698-3704},
  shortjournal = {IET Image Process.},
  title        = {Retracted: Combined mixed gaussian model with pattern recognition in the automatic diagnosis of alzheimer&#39;s disease},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Retracted: Jointly network image processing: Multi-task
image semantic segmentation of indoor scene based on CNN.
<em>IETIP</em>, <em>14</em>(15), 3689–3697. (<a
href="https://doi.org/10.1049/iet-ipr.2020.0088">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image semantic segmentation has always been a research hotspot in the field of robots. Its purpose is to assign different semantic category labels to objects by segmenting different objects. However, in practical applications, in addition to knowing the semantic category information of objects, robots also need to know the position information of objects to complete more complex visual tasks. Aiming at a complex indoor environment, this study designs an image semantic segmentation network framework of joint target detection. Using the parallel operation of adding semantic segmentation branches to the target detection network, it innovatively implements multi-vision task combining object classification, detection and semantic segmentation. By designing a new loss function, adjusting the training using the idea of transfer learning, and finally verifying it on the self-built indoor scene data set, the experiment proves that the method in this study is feasible and effective, and has good robustness.},
  archive      = {J_IETIP},
  author       = {Li Huang and Meiling He and Chong Tan and Du Jiang and Gongfa Li and Hui Yu},
  doi          = {10.1049/iet-ipr.2020.0088},
  journal      = {IET Image Processing},
  month        = {12},
  number       = {15},
  pages        = {3689-3697},
  shortjournal = {IET Image Process.},
  title        = {Retracted: jointly network image processing: multi-task image semantic segmentation of indoor scene based on CNN},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Retracted: Research on measurement and energy efficiency
improvement of flat panel display based on industrial control.
<em>IETIP</em>, <em>14</em>(15), 3682–3688. (<a
href="https://doi.org/10.1049/iet-ipr.2020.0081">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In view of the fact that on flat panel display industrial production line, the power consumption is very important and determined by their different driving modes and light-emitting principles. Based on the industrial control systems, this paper analyses the power consumption of flat panel display, studies the main factors affecting its power consumption and analyses the main methods to improve light efficiency and reduce power consumption. This paper also discusses the focus issues in the current standard of flat panel display power measurement at home and abroad. According to the characteristics of different flat panel displays, this paper discusses the dual indexes of power consumption and efficiency in power measurement, and how to consider the influence of screen brightness and angle of view in the measurement.},
  archive      = {J_IETIP},
  author       = {Yuan Cui and Yang Yu and Zhuyang Chen and Bo Xue},
  doi          = {10.1049/iet-ipr.2020.0081},
  journal      = {IET Image Processing},
  month        = {12},
  number       = {15},
  pages        = {3682-3688},
  shortjournal = {IET Image Process.},
  title        = {Retracted: Research on measurement and energy efficiency improvement of flat panel display based on industrial control},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Retracted: Convolutional neural network based on
differential privacy in exponential attenuation mode for image
classification. <em>IETIP</em>, <em>14</em>(15), 3676–3681. (<a
href="https://doi.org/10.1049/iet-ipr.2020.0078">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Privacy information leaks have become a major problem hindering the development of the convolutional neural network and deep learning. Differential privacy protection has been applied to deep learning by more and more scholars to protect image training sets. The differentially private SGD (DP-SGD) algorithm is adding Gaussian noise of a fixed level will cause the accuracy of the model to increase slowly with the increase of training times. To solve this problem, this study presents a convolutional neural network based on differential privacy in exponential attenuation mode. Firstly, the attenuation coefficient of Gaussian noise is linked with the training times and the accuracy of the model. Secondly, the DP-SGD algorithm in exponential attenuation mode is proposed. The calculation method of the differential privacy protection budget in exponential attenuation mode is given. Finally, experiments with the MNIST dataset and X-ray images verify the feasibility of using the original data of the Gaussian noise convolutional neural network based on differential privacy protection in exponential attenuation mode.},
  archive      = {J_IETIP},
  author       = {Jingjing Yang and Jinzhao Wu and Xiaojing Wang},
  doi          = {10.1049/iet-ipr.2020.0078},
  journal      = {IET Image Processing},
  month        = {12},
  number       = {15},
  pages        = {3676-3681},
  shortjournal = {IET Image Process.},
  title        = {Retracted: Convolutional neural network based on differential privacy in exponential attenuation mode for image classification},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Retracted: Innovative approach for multimodal fusion
recognition based feature extraction using band-limited phase-only
correlation and discrete orthonormal stockwell transform.
<em>IETIP</em>, <em>14</em>(15), 3669–3675. (<a
href="https://doi.org/10.1049/iet-ipr.2020.0145">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Biometrics is the bureau of science that helps to measure the individual&#39;s features by utilising their behavioural and physiological characteristics. Since years ago biometric technology are contemplated to be a unique tool for other security purposes. Inauspicious biometrics, Ear recognition, and Finger Knuckle Print have been enchanted as a booming analysis with interests among several researchers in modern periods. It has an ample range of private and other law enforcement applications. A combination of multiple human attributes is authenticated and considered to be a competent strategy in case of a multimodal Personal authentication system. In this paper extracting the local and global features via the structure of the time-frequency domain has been studied. This proposed scheme exploits the analysis of dual biometric modalities i.e. Ear and Finger Knuckle Print which are carried out at the stage of feature-level fusion. The feature Extraction two biometric patterns are obtained by generating the Local and Global feature information that helps in refining the alignment of dual biometric images in matching i.e. Discrete Orth normal Stockwell Transform-Ear recognition and Band Limited phase-only correlation with Finger Knuckle Print. Experiment results conducted with these FKP and EAR are demonstrated in improving recognition of efficient accuracy.},
  archive      = {J_IETIP},
  author       = {A. Kavipriya and A. Muthukumar},
  doi          = {10.1049/iet-ipr.2020.0145},
  journal      = {IET Image Processing},
  month        = {12},
  number       = {15},
  pages        = {3669-3675},
  shortjournal = {IET Image Process.},
  title        = {Retracted: Innovative approach for multimodal fusion recognition based feature extraction using band-limited phase-only correlation and discrete orthonormal stockwell transform},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Retracted: Gesture recognition algorithm based on
multi-scale feature fusion in RGB-d images. <em>IETIP</em>,
<em>14</em>(15), 3662–3668. (<a
href="https://doi.org/10.1049/iet-ipr.2020.0148">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rapid development of sensor technology and artificial intelligence, the video gesture recognition technology under the background of big data makes human–computer interaction more natural and flexible, bringing the richer interactive experience to teaching, on-board control, electronic games etc. To perform robust recognition under the conditions of illumination change, background clutter, rapid movement, and partial occlusion, an algorithm based on multi-level feature fusion of two-stream convolutional neural network is proposed, which includes three main steps. Firstly, the Kinect sensor obtains red–green–blue-depth (RGB-D) images to establish a gesture database. At the same time, data enhancement is performed on the training set and test set. Then, a model of multi-level feature fusion of a two-stream convolutional neural network is established and trained. Experiments show that the proposed network model can robustly track and recognise gestures under complex backgrounds (such as similar complexion, illumination changes, and occlusion), and compared with the single-channel model, the average detection accuracy is improved by 1.08%, and mean average precision is improved by 3.56%.},
  archive      = {J_IETIP},
  author       = {Ying Sun and Yaoqing Weng and Bowen Luo and Gongfa Li and Bo Tao and Du Jiang and Disi Chen},
  doi          = {10.1049/iet-ipr.2020.0148},
  journal      = {IET Image Processing},
  month        = {12},
  number       = {15},
  pages        = {3662-3668},
  shortjournal = {IET Image Process.},
  title        = {Retracted: Gesture recognition algorithm based on multi-scale feature fusion in RGB-D images},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Retracted: KSSD: Single-stage multi-object detection
algorithm with higher accuracy. <em>IETIP</em>, <em>14</em>(15),
3651–3661. (<a href="https://doi.org/10.1049/iet-ipr.2020.0077">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Considering that the single shot multibox detector (SSD) algorithm will be missed or even false when is used to detect the small- and medium-sized objects, in this study, Kullback–Leibler single shot multibox detection (KSSD) object detection algorithm is proposed to improve the accuracy of small- and medium-sized objects detection. Firstly, the details in the detection process are visualised with gradient-weighted class activation mapping technology, and the details of each detection layer are shown in the form of class activation maps. Then it is noted that the phenomenon of the false or missed detection of the objects to be detected on small- and medium-sized objects in the SSD algorithm is related to the regression loss function. Accordingly, Kullback–Leibler border regression loss strategy is adopted and non-maximum suppression algorithm is used to output the final prediction boxes. Experimental results show that compared with the existed detection algorithms, the improved algorithm in this study has higher accuracy and stability, and can significantly improve the detection effect on small- and medium-sized objects.},
  archive      = {J_IETIP},
  author       = {Qingshan Hou and Jinsheng Xing},
  doi          = {10.1049/iet-ipr.2020.0077},
  journal      = {IET Image Processing},
  month        = {12},
  number       = {15},
  pages        = {3651-3661},
  shortjournal = {IET Image Process.},
  title        = {Retracted: KSSD: single-stage multi-object detection algorithm with higher accuracy},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Retracted: Improved artificial bee colony algorithm with
opposition-based learning. <em>IETIP</em>, <em>14</em>(15), 3639–3650.
(<a href="https://doi.org/10.1049/iet-ipr.2020.0111">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The artificial bee colony (ABC) algorithm is a biological-inspired optimisation algorithm proposed by Karaboga. Since its solution search equation is good at exploration but poor at exploitation, the ABC algorithm converges slowly and is easy to fall into local optimum. Inspired by opposition-based learning (OBL), the authors propose an improved ABC algorithm called opposition-based learning ABC (OLABC). In OLABC, firstly, the population would be initialised using OBL. Secondly, to ensure the diversity of the population during the iterative process, the solution search equation is employed to bee phase would be improved. Generate the opposite solution when the fitness value of the newly generated solution is smaller than the current solution, and then apply the greedy selection strategy to update the solution. Thirdly, the adaptive weight strategy is used to dynamically adjust the weight, balancing the global exploration and local exploitation capabilities of the algorithm. Experiments on a set of benchmark functions show that OLABC has better convergence speed and optimisation precision than the compared algorithms.},
  archive      = {J_IETIP},
  author       = {Yongcun Cao and Saisai Ji and Yong Lu},
  doi          = {10.1049/iet-ipr.2020.0111},
  journal      = {IET Image Processing},
  month        = {12},
  number       = {15},
  pages        = {3639-3650},
  shortjournal = {IET Image Process.},
  title        = {Retracted: Improved artificial bee colony algorithm with opposition-based learning},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Retracted: Research on image sharpening algorithm in weak
illumination environment. <em>IETIP</em>, <em>14</em>(15), 3635–3638.
(<a href="https://doi.org/10.1049/iet-ipr.2019.1588">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Owing to low light intensity, the image detector is underexposure, the colour and contrast of scene images will be changed, so it is of great significance to study the image sharpening in a weak illumination environment. In this study, the causes of image blurring are firstly analysed by histogram equalisation (HE) and retinex theory. A method combined HE and multi-scale retinex theory with colour recovery is proposed. The experimental simulation results show that the image processing effect of this algorithm is the most obvious, the local details and dark areas are clearly visible, the colour is rich, and the visual effect is the best, which has a certain practical application value.},
  archive      = {J_IETIP},
  author       = {Jinxing Niu and Yajie Jiang and Yayun Fu},
  doi          = {10.1049/iet-ipr.2019.1588},
  journal      = {IET Image Processing},
  month        = {12},
  number       = {15},
  pages        = {3635-3638},
  shortjournal = {IET Image Process.},
  title        = {Retracted: Research on image sharpening algorithm in weak illumination environment},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Salient object detection via reliability-based depth
compactness and depth contrast. <em>IETIP</em>, <em>14</em>(14),
3623–3631. (<a href="https://doi.org/10.1049/iet-ipr.2019.1495">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {It can be intuitively inferred that a high-quality depth map can be used to quickly detect the salient region in stereo vision, implying that depth information plays an essential role in stereoscopic visual attention. However, existing methods generally use the depth map as an auxiliary cue to improve the saliency detection performance. In this study, the authors present an algorithm to directly detect the salient object from a high-quality depth image. The proposed algorithm utilises a depth reliability indicator to assess the confidence of a depth image. Depth compactness, a novel feature that incorporates the depth reliability of the super-pixels, is computed as a primary salient feature. Moreover, in order to enhance another salient feature (i.e. depth contrast), they develop a coarse background filtering method to suppress background interference. Experimental results demonstrate that the proposed method performs favourably against the popular depth-aware saliency detection approaches at a lower computational cost.},
  archive      = {J_IETIP},
  author       = {Yang Zhou and Xiaoqi Liu and Yun Zhang and Haibing Yin and Yu Lu},
  doi          = {10.1049/iet-ipr.2019.1495},
  journal      = {IET Image Processing},
  month        = {12},
  number       = {14},
  pages        = {3623-3631},
  shortjournal = {IET Image Process.},
  title        = {Salient object detection via reliability-based depth compactness and depth contrast},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). UCT-GAN: Underwater image colour transfer generative
adversarial network. <em>IETIP</em>, <em>14</em>(14), 3613–3622. (<a
href="https://doi.org/10.1049/iet-ipr.2020.0003">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Underwater image enhancement algorithms improve image quality and indirectly enhance underwater visibility. Although many underwater image enhancement neural networks have been proposed, they require large amounts of data. To reduce the amount of data required while providing better image enhancement, this study proposes an underwater image colour transfer generative adversarial network (UCT-GAN). The authors first design a non-linear mapping function to generate colour cast images according to original images. Then, the authors utilise these image pairs (i.e. colour cast images and corresponding original images) to guide the UCT-GAN in learning the inverse function of the designed non-linear mapping function. Finally, colour cast images are restored via the inverse function. A data augmentation method based on Poisson fusion and block combination is also proposed to overcome the problem of requiring a large amount of training data. Moreover, the authors extend UCT-GAN into a multi-class colour transfer network to achieve an array of underwater image enhancements. Experimental results indicate that the proposed UCT-GAN can more effectively resolve underwater image colour cast compared to existing algorithms.},
  archive      = {J_IETIP},
  author       = {Junjie Deng and Gege Luo and Caidan Zhao},
  doi          = {10.1049/iet-ipr.2020.0003},
  journal      = {IET Image Processing},
  month        = {12},
  number       = {14},
  pages        = {3613-3622},
  shortjournal = {IET Image Process.},
  title        = {UCT-GAN: Underwater image colour transfer generative adversarial network},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Tensor total variation regularised low-rank approximation
framework for video deraining. <em>IETIP</em>, <em>14</em>(14),
3602–3612. (<a href="https://doi.org/10.1049/iet-ipr.2019.1409">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Outdoor monitoring systems are known to exhibit better performance under normal weather conditions, while it lacks effectiveness under inclement conditions. Often video footage captured by the camera under rainy conditions comprises several visual distortions. It eventually leads to flaws when handled with succeeding computer vision algorithms, namely the object identification and tracking. Additionally, eliminating such unpleasant rainy effects is essential prior to the processing of video footage by suitable algorithms. The present work attempts to formulate a new low-rank tensor recovery based deraining algorithm that enables to remove the rain streaks from video footage. The proposed method detects the rain streaks by adopting optical flow estimation along with the brightness features inherent with the rain streaks. A unified framework comprised of tensor singular value decomposition (t-SVD) based weighted nuclear norm minimisation and tensor total variation (TTV) regularisation effectively removes rain streaks and recovers the original rain-free data from the available rainy data. The use of t-SVD enforces the concept of low rankness and also exploits the temporal redundancy among the video frames. Furthermore, TTV regularisation facilitates to promote the temporal continuity for discriminating most of the natural image contents from sparse rain streaks by preserving piece-wise smoothness of video frames. Comprehensive experimental findings based on real and synthetic data with dynamic background show that the rain streaks are more efficaciously eliminated by adopting the proposed method without much loss in the information.},
  archive      = {J_IETIP},
  author       = {Baiju P.S. and Deepak Jayan P. and Sudhish N. George},
  doi          = {10.1049/iet-ipr.2019.1409},
  journal      = {IET Image Processing},
  month        = {12},
  number       = {14},
  pages        = {3602-3612},
  shortjournal = {IET Image Process.},
  title        = {Tensor total variation regularised low-rank approximation framework for video deraining},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Correlation scan matching algorithm based on
multi-resolution auxiliary historical point cloud and lidar simultaneous
localisation and mapping positioning application. <em>IETIP</em>,
<em>14</em>(14), 3596–3601. (<a
href="https://doi.org/10.1049/iet-ipr.2019.1657">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The matching algorithm is an important part of simultaneous location and mapping. Aiming at the problem of large computation and poor real-time performance of two-dimensional lidar traditional correlation scan matching (CSM) algorithm, a multi-resolution auxiliary historical point cloud matching algorithm is proposed, which combines high and low resolution and adopts a single-frame to multi-frame step-by-step matching scheme. The algorithm was carried out on the sweeping robot. Compared with the traditional CSM algorithm and iterative closest points algorithm, the single position accuracy of the method in this study is improved. In the indoor space of ∼10 m × 10 m, the cumulative error is reduced by 16.24 and 33.96%, respectively. Consequently, our algorithm can still manage to process in real-time.},
  archive      = {J_IETIP},
  author       = {Haiqiao Liu and Shibin Luo and Jiazhen Lu},
  doi          = {10.1049/iet-ipr.2019.1657},
  journal      = {IET Image Processing},
  month        = {12},
  number       = {14},
  pages        = {3596-3601},
  shortjournal = {IET Image Process.},
  title        = {Correlation scan matching algorithm based on multi-resolution auxiliary historical point cloud and lidar simultaneous localisation and mapping positioning application},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020a). Highly accurate 3D reconstruction based on a precise and
robust binocular camera calibration method. <em>IETIP</em>,
<em>14</em>(14), 3588–3595. (<a
href="https://doi.org/10.1049/iet-ipr.2019.1525">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The precision of the camera calibration is one of the key factors that affect attitude measurement accuracy in many computer vision tasks. This study proposes a new calibration approach for binocular cameras. Firstly, based on singular value decomposition, the best transformation matrix to the essential matrix is approximated as the initial guess, which is solved in using the Frobenius norm. Secondly, the initial guess is refined through maximum likelihood estimation. A new calculating expression is derived for computing the relative position matrix of the binocular cameras. The Levenberg–Marquardt algorithm is then implemented to refine the initial guess. Large sets of synthesised and real point correspondences were tested to demonstrate the validity of the proposed method. Extensive experiments demonstrated that the proposed method outperforms the state-of-the-art methods. The error rate of the proposed method was 0.5% for the length test and about 1% for the angle test at a range of 1 m. This method can advance three-dimensional (3D) computer vision one additional step from laboratory environments to real-world use.},
  archive      = {J_IETIP},
  author       = {Guoliang Hu and Zuofeng Zhou and Jianzhong Cao and Huimin Huang},
  doi          = {10.1049/iet-ipr.2019.1525},
  journal      = {IET Image Processing},
  month        = {12},
  number       = {14},
  pages        = {3588-3595},
  shortjournal = {IET Image Process.},
  title        = {Highly accurate 3D reconstruction based on a precise and robust binocular camera calibration method},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Multi-view hand gesture recognition via pareto optimal
front. <em>IETIP</em>, <em>14</em>(14), 3579–3587. (<a
href="https://doi.org/10.1049/iet-ipr.2019.0924">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Computational cost-ineffectiveness and ambiguity due to self-occlusion are bottlenecks of vision-based hand gesture recognition. In this study, the authors address these issues by proposing a novel multi-view hand gesture recognition method based on Pareto optimal front. They first present an oriented gradient local binary pattern operator to generate a groupwise gesture feature data set for multi-view hand gesture images. Then they take hand gesture recognition over multi-view hand images as a multi-query image retrieval problem and Pareto optimal front is constructed based on the dissimilarities between the testing images and sample images. The gesture corresponding to the point with the shortest distance to the origin on the Pareto optimal front is the final recognised result. Extensive experiments verify the accuracy and efficiency of their Pareto optimal font-based method.},
  archive      = {J_IETIP},
  author       = {Jin Sun and Zhe Zhang and Liutao Yang and Jiping Zheng},
  doi          = {10.1049/iet-ipr.2019.0924},
  journal      = {IET Image Processing},
  month        = {12},
  number       = {14},
  pages        = {3579-3587},
  shortjournal = {IET Image Process.},
  title        = {Multi-view hand gesture recognition via pareto optimal front},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Image compression using adaptive multiresolution image
decomposition algorithm. <em>IETIP</em>, <em>14</em>(14), 3572–3578. (<a
href="https://doi.org/10.1049/iet-ipr.2019.1699">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the growth of modern digital technologies, demand for transmission multimedia and digital images, which require more storage space and transmission bandwidth, has been increased rapidly. Hence, developing new image compression techniques for reducing data size without degrading the quality of the image, has gained a lot of interest recently. In this study, an adaptive multiresolution image decomposition (AMID) algorithm is proposed and its application for image compression is explored. The developed algorithm is capable of decomposing an image along the vertical, horizontal, and diagonal directions using the pyramidal multiresolution scheme. Compared to the wavelet transform, the AMID can be used for decimation with the guarantees of perfect signal reconstruction. Furthermore, the application of the AMID for image compression is explored and its performance is compared with the state-of-the-art image compression techniques. The performance of compression method is evaluated using peak signal-to-noise ratio and compression ratio. Experimental results have shown promising performance compared with the results of using other image compression approaches.},
  archive      = {J_IETIP},
  author       = {Osama A.S. Alkishriwo},
  doi          = {10.1049/iet-ipr.2019.1699},
  journal      = {IET Image Processing},
  month        = {12},
  number       = {14},
  pages        = {3572-3578},
  shortjournal = {IET Image Process.},
  title        = {Image compression using adaptive multiresolution image decomposition algorithm},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Infrared and visible image fusion using a shallow CNN and
structural similarity constraint. <em>IETIP</em>, <em>14</em>(14),
3562–3571. (<a href="https://doi.org/10.1049/iet-ipr.2020.0360">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, image fusion methods based on deep networks have been proposed to combine infrared and visible images for achieving better fusion image. However, issues such as limited training data, scarce reference images and misalignment of multi-source images, still limit the fusion performance. To address these problems, we propose an end-to-end shallow convolutional neural network with structural constraints, which has only one convolutional layer to fuse infrared and visible images. Different from other methods, our proposed model requires less training data and reference images and is more robust to the misalignment of a couple of images. More specifically, the infrared image and the visible image are first provided as inputs to a convolutional layer to extract the information that should be fused; then, all feature maps are concatenated together and fed into a convolutional layer with one channel to obtain the fused image; finally, a structural similarity loss between the fused image and the input infrared and visible images is computed to update the network parameters and eliminate the effects of pixel misalignment. Extensive experiments show the effectiveness of our proposed method on fusion of infrared and visible images with the performance that outperforms the state-of-the-art methods.},
  archive      = {J_IETIP},
  author       = {Lei Li and Zhaoqiang Xia and Huijian Han and Guiqing He and Fabio Roli and Xiaoyi Feng},
  doi          = {10.1049/iet-ipr.2020.0360},
  journal      = {IET Image Processing},
  month        = {12},
  number       = {14},
  pages        = {3562-3571},
  shortjournal = {IET Image Process.},
  title        = {Infrared and visible image fusion using a shallow CNN and structural similarity constraint},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Adaptive trainable non-linear reaction diffusion for rician
noise removal. <em>IETIP</em>, <em>14</em>(14), 3547–3561. (<a
href="https://doi.org/10.1049/iet-ipr.2019.1097">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Rician noise reduction is an essential issue in magnetic resonance imaging (MRI). Recently, learning-based methods have achieved great success in dealing with image restoration problems, which provide fast inference and good performance. One limitation of these methods, however, is that the training procedure is usually noise-level dependent, i.e. the trained models are bound to a specific noise level and lack the ability to automatically adapt to different noise levels. In this study, the authors propose a variational model for Rician noise removal by integrating a noise adaption function into the field of experts image prior, which can adapt to different noise levels. Instead of directly solving the energy minimisation problem, the authors unroll the gradient descent step of the energy functional for several iterations, the time-dependent parameters of which can be learned through a supervised training process. The authors call this methodology as the noise adaptive trainable non-linear reaction–diffusion model. The proposed methodology is robustness against noise level changing and noise distributions. Experimental results over -, - and PD-weighted MRI data set demonstrate that the proposed model can achieve superior performance compared with other methods in terms of both the peak signal-to-noise ratio and the structural similarity index.},
  archive      = {J_IETIP},
  author       = {Huan Yang and Hongwei Li and Yuping Duan},
  doi          = {10.1049/iet-ipr.2019.1097},
  journal      = {IET Image Processing},
  month        = {12},
  number       = {14},
  pages        = {3547-3561},
  shortjournal = {IET Image Process.},
  title        = {Adaptive trainable non-linear reaction diffusion for rician noise removal},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Deep facial emotion recognition in video using eigenframes.
<em>IETIP</em>, <em>14</em>(14), 3536–3546. (<a
href="https://doi.org/10.1049/iet-ipr.2019.1566">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, video-based facial emotion recognition (FER) has been an attractive topic in the computer vision society. However, processing several hundreds of frames for a single video of a particular emotion is not efficient. In this study, the authors propose a novel approach to obtain a representative set of frames for a video in the eigenspace domain. Principal component analysis (PCA) is applied to a single emotional video extracting the most significant eigenframes representing the temporal motion variance embedded in the video. Given that faces are segmented and normalised, the variance captured by PCA is attributed to the facial expression dynamics. The variation in the temporal domain is mapped to the eigenspace reducing the redundancy. The proposed approach is used to extract the input eigenframes. Later, VGG-16, ResNet50, and 2D and 3D CNN architectures called eigenFaceNet are trained on the RML, eNTERFACE′05, and AFEW 6.0 databases. The experimental results are superior to the state-of-the-art by 8 and 4% for RML, eNTERFACE′05 databases, respectively. The performance achievement is also coupled with a reduction in the computational time.},
  archive      = {J_IETIP},
  author       = {Noushin Hajarolasvadi and Hasan Demirel},
  doi          = {10.1049/iet-ipr.2019.1566},
  journal      = {IET Image Processing},
  month        = {12},
  number       = {14},
  pages        = {3536-3546},
  shortjournal = {IET Image Process.},
  title        = {Deep facial emotion recognition in video using eigenframes},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Progressive learning with style transfer for distant domain
adaptation. <em>IETIP</em>, <em>14</em>(14), 3527–3535. (<a
href="https://doi.org/10.1049/iet-ipr.2020.0166">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article studies a novel transfer learning problem termed distant domain transfer learning. Different from traditional transfer learning which assumes there is a close relation between source and target data, in this study, the objective is to execute an unseen and unrelated task based on a labelled data set training previously without any samples from intermediate domains. To this end, the authors propose deep unsupervised progressive learning (DUPL) framework and its upgraded version, end-to-end DUPL (eDUPL). eDUPL consists of two components, i.e. (i) translating the style of labelled images from irrelevant source domain to the target domain and (ii) learning a domain adaptation model with progressive learning for testing on the target domain. In comparison, eDUPL can integrate the two components of the framework seamlessly. In general, the proposed method is easy to be implemented and can be viewed as a strong convolutional baseline for distant domain adaptation task. Comprehensive experiments based on VeRi Vehicle, CUB-200-2011 Birds and Oxford5k Buildings data sets are conducted and the results indicate that the proposed method robustly achieves state-of-the-art performances compared with existing approaches, which demonstrates the effectiveness and superiority of the proposed algorithm.},
  archive      = {J_IETIP},
  author       = {Suncheng Xiang and Yuzhuo Fu and Ting Liu},
  doi          = {10.1049/iet-ipr.2020.0166},
  journal      = {IET Image Processing},
  month        = {12},
  number       = {14},
  pages        = {3527-3535},
  shortjournal = {IET Image Process.},
  title        = {Progressive learning with style transfer for distant domain adaptation},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Automated unsupervised learning-based clustering approach
for effective anomaly detection in brain magnetic resonance imaging
(MRI). <em>IETIP</em>, <em>14</em>(14), 3516–3526. (<a
href="https://doi.org/10.1049/iet-ipr.2020.0597">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This research study is intended to deliver effective magnetic resonance (MR) brain image segmentation, which is an ambiguous process in the domain of medical image analysis. In general, MR brain image comprises various tissue structures; and an accurate representation of the above-mentioned regions is essential to have a perfect identification of different grades of tumours, and obtaining effective demarcation of different areas in which the oedema portion is widespread. The accurate representation and identification of the abnormal regions in the MR images can be a vital tool for the radiologists and oncologists to proceed further with the treatment processes. This study aims in developing a novel automated approach that combines self-organising map and interval type-2 fuzzy logic clustering, providing ample knowledge to the clinicians in identifying the aberrant regions present in the patient brain. A non-invasive analysis blended with quicker segmentation results are proffered by the proposed methodology and its functioning abilities have been assessed using comparison metrics such as mean-squared error (MSE), peak signal-to-noise ratio (PSNR), processing time duration, and few other standard metrics. The proposed methodology has offered commendable MSE and PSNR values, which are 0.234778 and 54.847 dB, and it can be undeniably utilised for analysing the patient diseases.},
  archive      = {J_IETIP},
  author       = {Vishnuvarthanan Govindaraj and Arunprasath Thiyagarajan and Pallikonda Rajasekaran and Yudong Zhang and Rajesh Krishnasamy},
  doi          = {10.1049/iet-ipr.2020.0597},
  journal      = {IET Image Processing},
  month        = {12},
  number       = {14},
  pages        = {3516-3526},
  shortjournal = {IET Image Process.},
  title        = {Automated unsupervised learning-based clustering approach for effective anomaly detection in brain magnetic resonance imaging (MRI)},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Compressive sensing MR imaging based on adaptive tight frame
and reference image. <em>IETIP</em>, <em>14</em>(14), 3508–3515. (<a
href="https://doi.org/10.1049/iet-ipr.2019.0834">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Compressive sensing magnetic resonance (MR) imaging is aimed at achieving high-quality MR image reconstruction by undersampling K-space data. It is crucial to explore prior information since compressive sensing MR imaging relies heavily on some prior assumptions, such as signal&#39;s sparse property. In this study, in order to explore the prior information fully, an improved MR image reconstruction model based on compressive sensing theory is proposed, named reference image MR imaging with adaptive tight frame. In the proposed model, an adaptive tight frame is involved to explore the sparse prior information adapt to MR images and the similarity prior information to the target image. Meanwhile, improved adaptive weighting parameters are used to trade off the sparsity between the regions with much similarity and that of little similarity. In addition, the smoothing-based fast iterative shrinkage-threshold algorithm is utilised to tackle the optimisation problem so as to speed up imaging. The experimental results demonstrate that the proposed MR image reconstruction method outperforms some state-of-the-art methods in terms of quantitative results.},
  archive      = {J_IETIP},
  author       = {Chunhong Cao and Wei Duan and Kai Hu and Fen Xiao},
  doi          = {10.1049/iet-ipr.2019.0834},
  journal      = {IET Image Processing},
  month        = {12},
  number       = {14},
  pages        = {3508-3515},
  shortjournal = {IET Image Process.},
  title        = {Compressive sensing MR imaging based on adaptive tight frame and reference image},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Grain segmentation of sandstone images based on
convolutional neural networks and weighted fuzzy clustering.
<em>IETIP</em>, <em>14</em>(14), 3499–3507. (<a
href="https://doi.org/10.1049/iet-ipr.2019.1761">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Grain segmentation of sandstone images is to partition the images into non-overlapping regions, each of which is an independent mineral grain. However, a sandstone image usually contains hundreds of mineral grains and complicated micro-structures, which makes current segmentation methods inefficient. In this study, the authors propose a three-stage framework for the automatic segmentation of sandstone images. In the first stage, the input sandstone images are pre-segmented into over-segmented mineral superpixels. In the second stage, the instance-independent features are extracted by a specially designed convolutional neural network, and the instance-aware features are extracted by computing histogram statistics and Gabor responses of mineral superpixels. In the third stage, a novel weighted fuzzy clustering algorithm is proposed to cluster the mineral superpixels into different classes, afterwards the adjacent mineral superpixels are merged to yield the complete minerals according to their classes. The experimental results conducted on the sandstone image datasets demonstrate the effectiveness of the proposed method, which evidently outperform the state-of-the-art segmentation methods.},
  archive      = {J_IETIP},
  author       = {Feng Jiang and Na Li and Lili Zhou},
  doi          = {10.1049/iet-ipr.2019.1761},
  journal      = {IET Image Processing},
  month        = {12},
  number       = {14},
  pages        = {3499-3507},
  shortjournal = {IET Image Process.},
  title        = {Grain segmentation of sandstone images based on convolutional neural networks and weighted fuzzy clustering},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Modification and hardware implementation of cortex-like
object recognition model. <em>IETIP</em>, <em>14</em>(14), 3490–3498.
(<a href="https://doi.org/10.1049/iet-ipr.2019.0264">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Object recognition in the visual cortex of mammals and humans has inspired many computational object recognition models. Hierarchical model and X (HMAX) is a well-known biologically motivated object recognition model with scale and position tolerance and high accuracy. Due to the computational intensive nature, hardware implementation with massive parallel processing is suggested for real-time applications. However, it is important to explore algorithmic trade-offs when mapping an algorithm to are configurable hardware. A direct conversion of the software implementation of an algorithm generally results inefficient hardware resource usage. In this study, the authors propose a novel modification into the HMAX model which makes it suitable for hardware implementation. More precisely, to reduce the number of memory blocks and multipliers of the S2 layer of HMAX produces, they replace the first norm by the second norm, which critically affects the silicon area in an application-specific integrated circuit implementation or the required resources in field-programmable gate array (FPGA). To evaluate the proposed model, they implement a pipelined version of the revised model on a mid-range commercial Xilinx FPGA, i.e. XC6VLX240T platform from a Virtex 6 family of Xilinx using ISE. Compared to the recent hardware implementation of HMAX, the proposed model offers 83% resource degradation in DSP48 slices and 3% in memory blocks.},
  archive      = {J_IETIP},
  author       = {Alireza Mohammadi Anbaran and Pooya Torkzadeh and Reza Ebrahimpour and Nasour Bagheri},
  doi          = {10.1049/iet-ipr.2019.0264},
  journal      = {IET Image Processing},
  month        = {12},
  number       = {14},
  pages        = {3490-3498},
  shortjournal = {IET Image Process.},
  title        = {Modification and hardware implementation of cortex-like object recognition model},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). FI-GAN: Fewer iterations GAN for rapid synthesis
controllable realistic images. <em>IETIP</em>, <em>14</em>(14),
3481–3489. (<a href="https://doi.org/10.1049/iet-ipr.2020.0639">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Conditional generative adversarial networks (GANs) have led to considerable improvements in the task of conditional image generation. However, there are still some problems, including the low definition of generated images, mode collapse, gradient disappearance and slow convergence, which need to be solved. In this study, the authors proposed a novel conditional GAN model, fewer iterations GAN (FI-GAN), that requires fewer iterations to generate controllable and realistic images. First, to improve the performance of feature extraction of the discriminator and data distribution fitting of the generator, two residual blocks are added into the discriminator and generator. Then, two new loss functions, including quality and category loss functions for both the discriminator and generator, are designed to enhance the details and distinguishability of the generated images. Finally, to avoid mode collapse and improve the convergence, the Wasserstein distance is used as the quality loss function. With the CIFAR-10 and large scene understanding datasets, FI-GAN can generate delicate and diverse colour samples with fewer iterations. It achieved better performance evaluated by three objective criteria, the inception score, Fréchet inception distance and kernel maximum mean discrepancy, compared with four methods of conditional GAN.},
  archive      = {J_IETIP},
  author       = {He Yu and Nannan Yu},
  doi          = {10.1049/iet-ipr.2020.0639},
  journal      = {IET Image Processing},
  month        = {12},
  number       = {14},
  pages        = {3481-3489},
  shortjournal = {IET Image Process.},
  title        = {FI-GAN: Fewer iterations GAN for rapid synthesis controllable realistic images},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Image decomposition and denoising using fractional-order
partial differential equations. <em>IETIP</em>, <em>14</em>(14),
3471–3480. (<a href="https://doi.org/10.1049/iet-ipr.2018.5499">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this study, the authors propose a fractional derivative-based image decomposition and denoising model which decomposes the image into the cartoon component (the component formed by homogeneous regions and with sharp boundaries) and the texture (or noise) component. The cartoon component is modelled by a function of the fractional-order total bounded variation, while the texture component is modelled by an oscillatory function, bounded in the negative Sobolev space norm. The authors give the corresponding minimisation functional, after some transformations, and then the resulting fractional-order partial differential equation can be solved using the Fourier transform. By symmetry and asymmetry of the fractional-order derivative, some generalisations and variants of the proposed model are also introduced. Finally, the authors implement the algorithm by the fractional-order finite difference in the frequency-domain. The experimental results demonstrate that the proposed models make objective and visual improvements compared with other standard approaches in the task of decomposition and denoising.},
  archive      = {J_IETIP},
  author       = {Jian Bai and Xiang-Chu Feng},
  doi          = {10.1049/iet-ipr.2018.5499},
  journal      = {IET Image Processing},
  month        = {12},
  number       = {14},
  pages        = {3471-3480},
  shortjournal = {IET Image Process.},
  title        = {Image decomposition and denoising using fractional-order partial differential equations},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Efficient image classification technique for weather
degraded fruit images. <em>IETIP</em>, <em>14</em>(14), 3463–3470. (<a
href="https://doi.org/10.1049/iet-ipr.2018.5310">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fruit image classification is an ill-posed problem. Many machine learning techniques have been developed until now to improve the classification problem of fruit images. However, the performance of these techniques depends upon the quality of acquired fruit images. Thus, the performance of competitive fruit classification techniques reduces for images captured under poor environmental conditions, such as haze, fog, smog etc. To overcome this issue, type-II fuzzy-based fruit image improvement approach is employed to improve the visibility of weather degraded fruit images. After that, fruit images will be classified using an integrated classification model. The integrated model combines two well-known models (i.e. CNN and RNN). CNN is utilised to evaluate the discriminative features of fruit images. RNN is utilised to asses sequential labels. Extensive analysis shows that the proposed integrated classification model outperforms competitive fruit image classification techniques in terms of accuracy and coefficient of correlation.},
  archive      = {J_IETIP},
  author       = {Harmandeep Singh Gill and Baljit Singh Khehra},
  doi          = {10.1049/iet-ipr.2018.5310},
  journal      = {IET Image Processing},
  month        = {12},
  number       = {14},
  pages        = {3463-3470},
  shortjournal = {IET Image Process.},
  title        = {Efficient image classification technique for weather degraded fruit images},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Volumetric lung nodule segmentation in thoracic CT scan
using freehand sketch. <em>IETIP</em>, <em>14</em>(14), 3456–3462. (<a
href="https://doi.org/10.1049/iet-ipr.2020.0671">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The authors presented a novel computerised scheme to segment pulmonary nodules using freehand sketch. Here, freehand sketching is considered to identify the location of a nodule and it serves as a natural way and also it provides inferring adaptive information i.e. size, density, texture and mass centre etc. The proposed scheme includes two phases. In first phase using the freehand sketch analysis the multi seed points to select RoI. In the second phase nodule volumetric extraction is done using geometric modelling and implicit surface reconstruction for volumetric analysis. Spherical bins are used for ray triangle intersections and then local implicit surface fitting and blending method for surface reconstruction and depiction. The performance of the proposed scheme is assessed by accuracy and consistency using 112 CT examinations from LIDC. The IoU and ASD were used to assess the discrepancy between proposed method and inter observer agreement in the proposed approach. In estimating the reproducibility, the discrepancy in proposed scheme and the manual contouring by the expert is observed to be on an average of 0.13 ± 0.07 mm and 3.04 ± 1.7 mm respectively. The experiment shows that, the proposed scheme performs reasonably well and demonstrate merit of freehand sketch.},
  archive      = {J_IETIP},
  author       = {S. Pramod Kumar and Mrityunjaya V. Latte and Sangeeta K. Siri},
  doi          = {10.1049/iet-ipr.2020.0671},
  journal      = {IET Image Processing},
  month        = {12},
  number       = {14},
  pages        = {3456-3462},
  shortjournal = {IET Image Process.},
  title        = {Volumetric lung nodule segmentation in thoracic CT scan using freehand sketch},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Recognition of cursive video text using a deep learning
framework. <em>IETIP</em>, <em>14</em>(14), 3444–3455. (<a
href="https://doi.org/10.1049/iet-ipr.2019.1070">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study focuses on cursive text recognition appearing in videos, using a complete framework of deep neural networks. While mature video optical character recognition systems (V-OCRs) are available for text in non-cursive scripts, recognition of cursive scripts is marked by many challenges. These include complex and overlapping ligatures, context-dependent shape variations and presence of a large number of dots and diacritics. The authors present an analytical technique for recognition of cursive caption text that relies on a combination of convolutional and recurrent neural networks trained in an end-to-end framework. Text lines extracted from video frames are preprocessed to segment the background and are fed to a convolutional neural network for feature extraction. The extracted feature sequences are fed to different variants of bi-directional recurrent neural networks along with the ground truth transcription to learn sequence-to-sequence mapping. Finally, a connectionist temporal classification layer is employed to produce the final transcription. Experiments on a data set of more than 40,000 text lines from 11,192 video frames of various News channel videos reported an overall character recognition rate of 97.63%. The proposed work employs Urdu text as a case study but the findings can be generalised to other cursive scripts as well.},
  archive      = {J_IETIP},
  author       = {Ali Mirza and Imran Siddiqi},
  doi          = {10.1049/iet-ipr.2019.1070},
  journal      = {IET Image Processing},
  month        = {12},
  number       = {14},
  pages        = {3444-3455},
  shortjournal = {IET Image Process.},
  title        = {Recognition of cursive video text using a deep learning framework},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Learning an adaptive model for extreme low-light raw image
processing. <em>IETIP</em>, <em>14</em>(14), 3433–3443. (<a
href="https://doi.org/10.1049/iet-ipr.2020.0100">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Low-light images suffer from severe noise and low illumination. In this work, the authors propose an adaptive low-light raw image enhancement network to avoid parameter-handcrafting in current deep learning models and to improve image quality. The proposed method can be divided into two sub-models: brightness prediction and exposure shifting (ES). The former is designed to control the brightness of the resulting image by estimating a guideline exposure time . The latter learns to approximate an exposure-shifting operator ES, converting a low-light image with real exposure time to a noise-free image with guideline exposure time . Additionally, structural similarity loss and image enhancement vector are introduced to promote image quality, and a new campus image data set (CID) for training the proposed model is proposed to overcome the limitations of the existing data sets. In quantitative tests, it is shown that the proposed method has the lowest noise level estimation score compared with the state-of-the-art low-light algorithms, suggesting a superior denoising performance. Furthermore, those tests illustrate that the proposed method is able to adaptively control the global image brightness according to the content of the image scene. Lastly, the potential application in video processing is briefly discussed.},
  archive      = {J_IETIP},
  author       = {Qingxu Fu and Xiaoguang Di and Yu Zhang},
  doi          = {10.1049/iet-ipr.2020.0100},
  journal      = {IET Image Processing},
  month        = {12},
  number       = {14},
  pages        = {3433-3443},
  shortjournal = {IET Image Process.},
  title        = {Learning an adaptive model for extreme low-light raw image processing},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Blind atmospheric turbulence deconvolution. <em>IETIP</em>,
<em>14</em>(14), 3422–3432. (<a
href="https://doi.org/10.1049/iet-ipr.2019.1442">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A new blind image deconvolution technique is developed for atmospheric turbulence deblurring to overcome limitations of ‘generic’ blind deconvolution algorithms that do not take into account the complicated physics of the turbulence. The originality of the proposed approach relies on an actual physical model, known as the Fried kernel, that quantifies the impact of the atmospheric turbulence on the optical resolution of images. While the original expression of the Fried kernel can seem cumbersome at first sight, the authors show that it can be reparameterised in a much simpler form. This simple expression allows to efficiently embed this kernel in the proposed blind atmospheric turbulence deconvolution (BATUD) algorithm. BATUD is an iterative algorithm that alternately performs deconvolution and estimates the Fried kernel by jointly relying on a Gaussian mixture model prior to natural image patches and controlling for the square Euclidean norm of the Fried kernel. Numerical experiments show that the proposed blind deconvolution algorithm behaves well in different simulated turbulence scenarios, as well as on real images. Not only BATUD outperforms state-of-the-art approaches used in atmospheric turbulence deconvolution in terms of image quality metrics but is also faster.},
  archive      = {J_IETIP},
  author       = {Charles-Alban Deledalle and Jérôme Gilles},
  doi          = {10.1049/iet-ipr.2019.1442},
  journal      = {IET Image Processing},
  month        = {12},
  number       = {14},
  pages        = {3422-3432},
  shortjournal = {IET Image Process.},
  title        = {Blind atmospheric turbulence deconvolution},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Outlier percentage estimation for shape- and
parameter-independent outlier detection. <em>IETIP</em>,
<em>14</em>(14), 3414–3421. (<a
href="https://doi.org/10.1049/iet-ipr.2020.0334">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate and robust three-dimensional reconstruction of objects allows for applications in many aspects of modern life. Yet, it typically suffers from outliers and noise which often need to be post-processed. Although many algorithms are able to effectively remove the outliers, most require a certain amount of manual tuning of the parameter(s) or to have a parameter(s) set based on the rule of thumb. New machine learning and artificial intelligence-based methods have also been introduced but may require vast parallel computing resources as well as training data. In the present study, a novel combinatory-distance-based method capable of high accuracy outlier detection named as the sorted distance divergence point (SDDP) is introduced. Results show that SDDP is able to achieve an average accuracy of 98% in outlier detection. Moreover, the introduced distance function and outlier percentage allow clear labelling of inliers and outliers cloud points. Therefore, SDDP presents an attractive enhancement to existing methods; namely, the manual parameter(s) tuning may not be necessary. The adaptability and utility of SDDP is further demonstrated by incorporating SDDP with current methods, to produce a high accuracy outlier detector. When tested with 17 objects with 20–50% outliers, attain F 1 and F 2 scores averaging 0.960 and 0.968, respectively.},
  archive      = {J_IETIP},
  author       = {Michael Joon Seng Goh and Yeong Shiong Chiew and Ji Jinn Foo},
  doi          = {10.1049/iet-ipr.2020.0334},
  journal      = {IET Image Processing},
  month        = {12},
  number       = {14},
  pages        = {3414-3421},
  shortjournal = {IET Image Process.},
  title        = {Outlier percentage estimation for shape- and parameter-independent outlier detection},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Simple algorithm for l1-norm regularisation-based compressed
sensing and image restoration. <em>IETIP</em>, <em>14</em>(14),
3405–3413. (<a href="https://doi.org/10.1049/iet-ipr.2020.0194">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {L 1-norm regularisation plays an important role in compressed sensing reconstruction and image restoration. However, the discontinuity of L 1-norm function makes solving the involved optimisation problem very challenging with traditional optimisation methods. In this article, a simple but efficient algorithm is proposed for the L 1-norm regularised compressed sensing and image restoration problem. In the proposed algorithm, the L 1-norm regularised optimisation problem is converted to a non-linear optimisation problem with L 1-norm approximation by a smoothening function, which then can be solved by existing powerful non-linear optimisation methods. The simulation results show that the proposed algorithm is more efficient and results in a higher accurate solution. Compared to existing methods, the proposed algorithm is very easy to implement and promising for applications in medical and biological imaging.},
  archive      = {J_IETIP},
  author       = {Shun Qin},
  doi          = {10.1049/iet-ipr.2020.0194},
  journal      = {IET Image Processing},
  month        = {12},
  number       = {14},
  pages        = {3405-3413},
  shortjournal = {IET Image Process.},
  title        = {Simple algorithm for l1-norm regularisation-based compressed sensing and image restoration},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Dense optical flow based background subtraction technique
for object segmentation in moving camera environment. <em>IETIP</em>,
<em>14</em>(14), 3393–3404. (<a
href="https://doi.org/10.1049/iet-ipr.2019.0960">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Segmentation of moving object in video with moving background is a challenging problem and it becomes more difficult with varying illumination. The authors propose a dense optical flow-based background subtraction technique for object segmentation. The proposed technique is fast and reliable for segmentation of moving objects in realistic unconstrained videos. In the proposed work, they stabilise the camera motion by computing homography matrix, then they perform statistical background modelling using single Gaussian background modelling approach. Moving pixels are identified using dense optical flow in the background modelled scenario. The dense optical flow provides motion information of each pixel between consecutive frames, therefore for moving pixel identification they compute motion flow vector of each pixel between consecutive frames. To distinguish between foreground and background pixels, they labelled each pixel and thresholding the magnitude of motion flow vector identifies the moving pixels. The effectiveness of the proposed algorithm has been evaluated both qualitatively and quantitatively. The proposed algorithm has been evaluated on several realistic videos of different complex conditions. To assess the performance of the proposed work, the authors compared their algorithm with other state-of-art methods and found that the proposed method outperforms the other methods.},
  archive      = {J_IETIP},
  author       = {Arati Kushwaha and Ashish Khare and Om Prakash and Manish Khare},
  doi          = {10.1049/iet-ipr.2019.0960},
  journal      = {IET Image Processing},
  month        = {12},
  number       = {14},
  pages        = {3393-3404},
  shortjournal = {IET Image Process.},
  title        = {Dense optical flow based background subtraction technique for object segmentation in moving camera environment},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Corner detection using the point-to-centroid distance
technique. <em>IETIP</em>, <em>14</em>(14), 3385–3392. (<a
href="https://doi.org/10.1049/iet-ipr.2020.0164">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Corners, highly important local features of images and corner finding, play a crucial role in computer vision and image processing, such as object tracking and vehicle detection. Proposing effective and efficient corner detectors is the aim of corner detection. In this study, the authors first present a new measure of corner sharpness termed as the point-to-centroid distance (PCD) and then examine its behaviours, which display beneficial characteristics that help distinguish corners from non-corners. Based on PCD behaviours, the authors propose a novel corner detector. Extensive experimental results demonstrate that the PCD technique is effective and simultaneously efficient for corner detection compared with six other contour-based corner detectors in terms of two commonly used evaluation metrics – average repeatability and localisation error.},
  archive      = {J_IETIP},
  author       = {Shizheng Zhang and Luwen Huangfu and Zhifeng Zhang and Sheng Huang and Pu Li and Heng Wang},
  doi          = {10.1049/iet-ipr.2020.0164},
  journal      = {IET Image Processing},
  month        = {12},
  number       = {14},
  pages        = {3385-3392},
  shortjournal = {IET Image Process.},
  title        = {Corner detection using the point-to-centroid distance technique},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Robust defect detection in 2D images printed on 3D
micro-textured surfaces by multiple paired pixel consistency in
orientation codes. <em>IETIP</em>, <em>14</em>(14), 3373–3384. (<a
href="https://doi.org/10.1049/iet-ipr.2019.0724">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Defect detection is now an active research area for production quality assurance. Traditional visual inspection systems are developed by human beings, which is a time-consuming, labour-intensive, and highly error-prone process, and are therefore unreliable. To overcome these problems, the authors proposed a new method for detecting defects when printing on a 3D micro-textured surface. They utilise an orientation code as the basis to resist the fluctuations in illumination. Based on the consistency of the pixel pairs, they developed a model called multiple paired pixel consistency to represent the statistical relationship between each pixel pair in defect-free images. Finally, based on this model, they designed a defect detection method. Even with different defect sizes, illumination conditions, noise intensities, and other characteristics, the performance of the proposed algorithm is extremely stable and highly accurate, and the recall, precision, and F-measure in most of the results can reach 0.85,0.93, and 0.9, respectively. In addition, the defect detection rate can reach almost 100%. This demonstrates that the authors&#39; approach can achieve state-of-the-art accuracy in real industrial applications.},
  archive      = {J_IETIP},
  author       = {Sheng Xiang and Dong Liang and Shun&#39;ichi Kaneko and Hirokazu Asano},
  doi          = {10.1049/iet-ipr.2019.0724},
  journal      = {IET Image Processing},
  month        = {12},
  number       = {14},
  pages        = {3373-3384},
  shortjournal = {IET Image Process.},
  title        = {Robust defect detection in 2D images printed on 3D micro-textured surfaces by multiple paired pixel consistency in orientation codes},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Proximal approach to denoising hyperspectral images under
mixed-noise model. <em>IETIP</em>, <em>14</em>(14), 3366–3372. (<a
href="https://doi.org/10.1049/iet-ipr.2019.1763">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The authors present a proximal approach to hyperspectral image denoising adapted to the mixed noise behaviour of hyperspectral data; named hyperspectral image proximal denoiser ( HSIProxDenoiser ). A combination of Gaussian-impulse noise has been handled under maximum a posteriori framework using two data fidelity terms. They have incorporated prior information about the data in the form of two regularisation terms, namely Tikhonov–Miller (TM) and total variation (TV). Since TV possesses feature selection capability by setting some of the coefficients to zero, it works well when there are a small number of significant features. On the other hand, TM works well if there are a large number of similar features. Hence, including both regularisation terms can help achieve the desired denoising performance. The resultant optimisation problem is solved using a variant of primal-dual hybrid gradient by splitting the former into different functions and calculating their proximal operators individually. Experimental results over both synthetic as well as real hyperspectral image data validate the potential of the proposed technique both visually and in terms of quantitative metrics.},
  archive      = {J_IETIP},
  author       = {Hazique Aetesam and Kumari Poonam and Suman Kumar Maji},
  doi          = {10.1049/iet-ipr.2019.1763},
  journal      = {IET Image Processing},
  month        = {12},
  number       = {14},
  pages        = {3366-3372},
  shortjournal = {IET Image Process.},
  title        = {Proximal approach to denoising hyperspectral images under mixed-noise model},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Automatic segmentation and classification of lung tumour
using advance sequential minimal optimisation techniques.
<em>IETIP</em>, <em>14</em>(14), 3355–3365. (<a
href="https://doi.org/10.1049/iet-ipr.2020.0407">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A chronic disorder caused by abnormal growth of the lung cells in the pulmonary tumour. This study suggests a modern automated approach to improve efficiency and decrease the difficulty of lung tumour diagnosis. The proposed algorithm for lung tumour sensing consists of four phases: pre-processing, segmentation, extraction, and characteristics classification. The first stage is the image acquisition here input lung image is read and then resized. The second stage is the image pre-processing here Perona–Malik diffusion with unsharp masking filter is proposed for enhancement purposes. The third stage is the segmentation here the improved histogram–based fast 2D Otsu&#39;s thresholding is proposed for lung tumour segmentation purposes. Finally, linear discriminant analysis classifier, support vector machine (SVM) classifier, SVM–sequence minimal optimisation classifier, Naive Bayes classifier, SVM–advance sequence minimal optimisation (SVM–ASMO) classification [proposed] included in the various classifier groups adopted in this report. Overall performance accuracy of 0.962 is obtained using the proposed SVM–ASMO method that helps to diagnose the cancer cells using the feature extraction process, which is done automatically. The specificity, precision, recall, and F 1 score of the proposed method is found to be a value of 0.984, 0.974, 0.98, and 0.984,respectively.},
  archive      = {J_IETIP},
  author       = {K. Vijila Rani and S. Joseph Jawhar},
  doi          = {10.1049/iet-ipr.2020.0407},
  journal      = {IET Image Processing},
  month        = {12},
  number       = {14},
  pages        = {3355-3365},
  shortjournal = {IET Image Process.},
  title        = {Automatic segmentation and classification of lung tumour using advance sequential minimal optimisation techniques},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Guided adaptive interpolation filter. <em>IETIP</em>,
<em>14</em>(14), 3341–3354. (<a
href="https://doi.org/10.1049/iet-ipr.2019.1577">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Edge-aware smoothing has proved to be a fundamental technique for various image processing and computer vision tasks. In this study, the authors introduce a local, non-iterative, and effective edge-preserving filter namely guided adaptive interpolation filter (GAIF). GAIF can be used as a post-processing step after any smoothing filter to improve its edge preservation performance without reformulation. GAIF has an computation complexity, where N is the total number of pixels in the image. To further increase the efficiency of GAIF at edge-preservation, two techniques are introduced and demonstrated. GAIF efficiency is demonstrated and compared to state-of-the-art techniques on a number of tasks including image smoothing, flash/no-flash image denoising/fusion, single image dehazing, and image details enhancement.},
  archive      = {J_IETIP},
  author       = {Waseem Waheed and Mukhalad Al-nasrawi and Guang Deng},
  doi          = {10.1049/iet-ipr.2019.1577},
  journal      = {IET Image Processing},
  month        = {12},
  number       = {14},
  pages        = {3341-3354},
  shortjournal = {IET Image Process.},
  title        = {Guided adaptive interpolation filter},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Empirical greedy machine-based automatic liver segmentation
in CT images. <em>IETIP</em>, <em>14</em>(14), 3333–3340. (<a
href="https://doi.org/10.1049/iet-ipr.2019.0690">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Segmentation of the liver from 3D computed tomography volumes plays a significant role in trajectory development for computer-assisted interventional surgery for the liver disease. Despite a lot of studies, liver segmentation remains a challenging task due to the lack of clear edges on most liver boundaries coupled with high variability of both anatomical and intensity patterns. In addition, there is a problem with the segmentation of the left portal vein, in which the size of this vein prominently estimates the liver tumour area. The empirical greedy machine is proposed to make the precise, automated segmentation of the liver as well as the left portal vein. In which the empirical robust nature trains the features of the liver proficiently thereby segmenting the liver from other organs without the omission of adjacent organs and liver lobe region. Hence this proposed method can achieve one of the highest accuracies compared to other segmentation methods and the performance is calculated using several parameters such as volumetric overlap error, relative absolute volume difference (RVD), average symmetric absolute surface distance (ASD), root mean square surface distance, maximum symmetric ASD.},
  archive      = {J_IETIP},
  author       = {Gajendra Kumar Mourya and Dinesh Bhatia and Akash Handique},
  doi          = {10.1049/iet-ipr.2019.0690},
  journal      = {IET Image Processing},
  month        = {12},
  number       = {14},
  pages        = {3333-3340},
  shortjournal = {IET Image Process.},
  title        = {Empirical greedy machine-based automatic liver segmentation in CT images},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Medical image segmentation using deep learning with feature
enhancement. <em>IETIP</em>, <em>14</em>(14), 3324–3332. (<a
href="https://doi.org/10.1049/iet-ipr.2019.0772">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Pre-segmentation is known as a crucial step in medical image analysis. Many approaches have been proposed to make improvement to both the quality and efficiency of segmentation. However, existing methods are lacking in robustness to the variation in the edges and textures of the target. In order to address these drawbacks, a novel attention Gabor network (AGnet) based on deep learning for medical image segmentation that is capable of automatically paying more attention to the edge and consistently for improvement to the segmentation performance is proposed. The proposed model consists of two components. The first one is to determine the approximate location of the organs of interest in the image using convolution filters, and the other one is to highlight salient edge features intended for a specific segmentation task using Gabor filters. In order to facilitate collaboration in between the two parts, a region attention mechanism based on Gabor maps is suggested. The mechanism improved performance by learning to focus on the salient regions of the image that are useful for the authors&#39; tasks. As indicated by the experimental results, the AGnet is capable of enhancing the prediction performance while maintaining the computational efficiency, which makes it comparable with other state-of-the-art approaches.},
  archive      = {J_IETIP},
  author       = {Shaoqiong Huang and Mengxing Huang and Yu Zhang and Jing Chen and Uzair Bhatti},
  doi          = {10.1049/iet-ipr.2019.0772},
  journal      = {IET Image Processing},
  month        = {12},
  number       = {14},
  pages        = {3324-3332},
  shortjournal = {IET Image Process.},
  title        = {Medical image segmentation using deep learning with feature enhancement},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Automatic 3D point cloud registration algorithm based on
triangle similarity ratio consistency. <em>IETIP</em>, <em>14</em>(14),
3314–3323. (<a href="https://doi.org/10.1049/iet-ipr.2019.1087">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Three-dimensional (3D) point cloud registration is a fundamental key issue in 3D reconstruction, 3D object recognition and augmented reality. In this study, the authors propose a novel local feature descriptor called local angle statistics histogram (LASH) for efficient 3D point cloud registration. LASH forms a description of local shape geometries by encoding their properties on angles between the normal vector of the point and the vector formed by the point and other points in its local neighbourhood. In addition, they propose a 3D point cloud registration algorithm based on LASH. The registration algorithm firstly detects triangle matching points with consistent similarity ratios, and then aggregates each pair of triangular matching points into a set of matching points. They can use these matching sets to calculate multiple transformations between two point clouds. Finally, they use the error function to identify the best transformation and to achieve coarse alignment of the two point clouds. Experiments and comparisons with other global algorithms demonstrate that the proposed approach can be applied to register point clouds with considerable or limited overlaps and is robust to noise.},
  archive      = {J_IETIP},
  author       = {Xuyan Zou and Hanwu He and Yueming Wu and Youbin Chen and Mingxi Xu},
  doi          = {10.1049/iet-ipr.2019.1087},
  journal      = {IET Image Processing},
  month        = {12},
  number       = {14},
  pages        = {3314-3323},
  shortjournal = {IET Image Process.},
  title        = {Automatic 3D point cloud registration algorithm based on triangle similarity ratio consistency},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Classifying functional nuclear images with convolutional
neural networks: A survey. <em>IETIP</em>, <em>14</em>(14), 3300–3313.
(<a href="https://doi.org/10.1049/iet-ipr.2019.1690">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Functional imaging has successfully been applied to capture functional changes in the pathological tissues of a body in recent years. Nuclear medicine functional imaging has been used to acquire information about areas of concerns (e.g. lesions and organs) in a non-invasive manner, enabling semi-automated or automated decision-making for disease diagnosis, treatment, evaluation, and prediction. Focusing on functional nuclear medicine images, in this study, the authors review existing work on the classification of single-photon emission computed tomography, positron emission tomography, and their hybrid modalities with computed tomography and magnetic resonance imaging images by using convolutional neural network (CNN) techniques. Specifically, they first present an overview of nuclear imaging and the CNN technique, such as nuclear imaging modalities, nuclear image data format, CNN architecture, and the main CNN classification models. According to the diseases of concern, they then classify the existing CNN-based work on the classification of functional nuclear images into three different categories. For the typical work in each of these categories, they present details about their research objectives, adopted CNN models, and achieved main results. Finally, they discuss research challenges and directions for developing technological solutions to classify nuclear medicine images based on the CNN technique.},
  archive      = {J_IETIP},
  author       = {Qiang Lin and Zhengxing Man and Yongchun Cao and Tao Deng and Chengcheng Han and Chuangui Cao and Linjun Zhang and Sitao Zeng and Ruiting Gao and Weilan Wang and Jinshui Ji and Xiaodi Huang},
  doi          = {10.1049/iet-ipr.2019.1690},
  journal      = {IET Image Processing},
  month        = {12},
  number       = {14},
  pages        = {3300-3313},
  shortjournal = {IET Image Process.},
  title        = {Classifying functional nuclear images with convolutional neural networks: A survey},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Deep visual unsupervised domain adaptation for
classification tasks: A survey. <em>IETIP</em>, <em>14</em>(14),
3283–3299. (<a href="https://doi.org/10.1049/iet-ipr.2020.0087">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Learning methods are challenged when there is not enough labelled data. It gets worse when the existing learning data have different distributions in different domains. To deal with such situations, deep unsupervised domain adaptation techniques have newly been widely used. This study surveys such domain adaptation methods that have been used for classification tasks in computer vision. The survey includes the very recent papers on this topic that have not been included in the previous surveys and introduces a taxonomy by grouping methods published on unsupervised domain adaptation into five groups of discrepancy-, adversarial-, reconstruction-, representation-, and attention-based methods.},
  archive      = {J_IETIP},
  author       = {Yeganeh Madadi and Vahid Seydi and Kamal Nasrollahi and Reshad Hosseini and Thomas B. Moeslund},
  doi          = {10.1049/iet-ipr.2020.0087},
  journal      = {IET Image Processing},
  month        = {12},
  number       = {14},
  pages        = {3283-3299},
  shortjournal = {IET Image Process.},
  title        = {Deep visual unsupervised domain adaptation for classification tasks: A survey},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Robust segmentation of the colour image by fusing the SDD
clustering results from different colour spaces. <em>IETIP</em>,
<em>14</em>(13), 3273–3281. (<a
href="https://doi.org/10.1049/iet-ipr.2019.1481">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Segmentation of the colour image is challenging because the colour information is lost after being projected into three channels of the colour space. Many state-of-the-art colour image segmentation methods are based on monochrome segmentation in one channel of the colour space. However, the optimal performance of a segmentation method usually could not be achieved in a single colour space due to the complexity and diversity of the colour images. In this study, the authors propose to segment the colour image by fusing the slope difference distribution (SDD) clustering results in different colour spaces. For simplicity, the segmentation approach is designed as two-label segmentation and it could be easily generalised to be multiple-label segmentation. The proposed approach is compared with the state-of-the-art colour image segmentation methods both quantitatively and qualitatively. Experimental results verified the effectiveness of the proposed approach.},
  archive      = {J_IETIP},
  author       = {Zhenzhou Wang},
  doi          = {10.1049/iet-ipr.2019.1481},
  journal      = {IET Image Processing},
  month        = {11},
  number       = {13},
  pages        = {3273-3281},
  shortjournal = {IET Image Process.},
  title        = {Robust segmentation of the colour image by fusing the SDD clustering results from different colour spaces},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Image dehazing with uneven illumination prior by dense
residual channel attention network. <em>IETIP</em>, <em>14</em>(13),
3260–3272. (<a href="https://doi.org/10.1049/iet-ipr.2019.0873">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing dehazing methods based on convolutional neural networks estimate the transmission map by treating channel-wise features equally, which lacks flexibility in handling different types of haze information, leading to the poor representational ability of the network. Besides, the scene lights are predicted by an even illumination prior which does not work for a real situation. To solve these problems, the authors propose a dense residual channel attention network (DRCAN) for estimating the transmission map and use an image segmentation strategy to predict scene lights. Specifically, DRCAN is built based on the proposed dense residual block (DRB) and dense residual channel attention block (DRCAB). DRB extracts the hierarchical features with increasing receptive fields. DRCAB makes the network focus on the features containing heavy haze information. After the transmission map is estimated, fuzzy partition entropy combined with graph cuts is used to segment the transmission map into scene regions covered with varying scene lights. This strategy not only considers the fuzzy intensities of the low-contrast transmission map but also takes spatial correlation into account. Finally, a clear image is obtained by the transmission map and varying scene lights. Extensive experiments demonstrate that our method is comparable to most of existing methods.},
  archive      = {J_IETIP},
  author       = {Shibai Yin and Jin Xin and Yibin Wang and Anup Basu},
  doi          = {10.1049/iet-ipr.2019.0873},
  journal      = {IET Image Processing},
  month        = {11},
  number       = {13},
  pages        = {3260-3272},
  shortjournal = {IET Image Process.},
  title        = {Image dehazing with uneven illumination prior by dense residual channel attention network},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Novel breast cancer classification framework based on deep
learning. <em>IETIP</em>, <em>14</em>(13), 3254–3259. (<a
href="https://doi.org/10.1049/iet-ipr.2020.0122">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {AbstractBreast cancer is a major cause of transience amongst women. In this paper, two novel techniques, ResNet50 and VGG-16, are utilised and re-trained to recognise two classes rather than 1000 classes with high accuracy and low computational requirements. In addition, transfer learning and data augmentation are performed to solve the problem of lack of tagged data. To get a better accuracy, the support vector machine (SVM) classifier is utilised instead of the last fully connected layer. Our models performance are verified utilising k -fold cross-validation. Our proposed techniques are trained and evaluated on three mammographic datasets: mammographic image analysis society, digital database for screening mammography (DDSM) and the curated breast imaging subset of DDSM. This paper explains end-to-end fully convolutional neural networks without any prepossessing or post-processing. The proposed technique of employing ResNet50 hybridised with SVM achieves the best performance, specifically with the DDSM dataset, producing 97.98% accuracy, 98.46% area under the curve, 97.63% sensitivity, 96.51% precision, 95.97% F1 score and computational time 1.8934 s.},
  archive      = {J_IETIP},
  author       = {Wessam M. Salama and Azza M. Elbagoury and Moustafa H. Aly},
  doi          = {10.1049/iet-ipr.2020.0122},
  journal      = {IET Image Processing},
  month        = {11},
  number       = {13},
  pages        = {3254-3259},
  shortjournal = {IET Image Process.},
  title        = {Novel breast cancer classification framework based on deep learning},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020a). F2PNet: Font-to-painting translation by adversarial
learning. <em>IETIP</em>, <em>14</em>(13), 3243–3253. (<a
href="https://doi.org/10.1049/iet-ipr.2019.0476">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For Chinese font images, when all their strokes are replaced by pattern elements such as flowers and birds, they become flower–bird character paintings, which are traditional Chinese art treasures. The generation of flower–bird painting requires professional painters’ great efforts. How to automatically generate these paintings from font images? There is a huge gap between the font domain and the painting domain. Although many image-to-image translation frameworks have been proposed, they are unable to handle this situation effectively. In this study, a novel method called font-to-painting network (F2PNet) is proposed for font-to-painting translation. Specifically, an encoder equipped with dilated convolutions extracts features of the font image, and then the features are fed into the domain translation module for mapping the font feature space to the painting feature space. The acquired features are further adjusted by the refinement module and utilised by the decoder to obtain the target painting. The authors apply adversarial loss and cycle-consistency loss to F2PNet and further propose a loss term, which is called recognisability loss and makes the generated painting have font-level recognisability. It is proved by experiments that F2PNet is effective and can be used as an unsupervised image-to-image translation framework to solve more image translation tasks.},
  archive      = {J_IETIP},
  author       = {Guanzhao Li and Jianwei Zhang and Danni Chen},
  doi          = {10.1049/iet-ipr.2019.0476},
  journal      = {IET Image Processing},
  month        = {11},
  number       = {13},
  pages        = {3243-3253},
  shortjournal = {IET Image Process.},
  title        = {F2PNet: Font-to-painting translation by adversarial learning},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Active contours driven by modified LoG energy term and
optimised penalty term for image segmentation. <em>IETIP</em>,
<em>14</em>(13), 3232–3242. (<a
href="https://doi.org/10.1049/iet-ipr.2020.0214">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {An active contour model to segment the images is proposed by combining local binary fitting (LBF) energy function and modified Laplacian of Gaussian (MLoG) energy function. A MLoG energy function based on a new boundary indicator function or edge stop function (ESF) is introduced to smoothen the homogeneous regions and enhance the edge information of objects. Also, MLoG energy term with LBF energy term is incorporated to drive the initial contour towards the object boundary. Finally, the penalty term is replaced with a new optimized potential function, which can improve the corresponding speed function. By adding the optimized area energy term, contour position is accelerated towards the object boundary. Further, the addition of MLoG based on new ESF, makes the proposed model insensitive to the initial contour. Experiments are performed on various real images, MS-COCO 2014 train data set images and Segmentation Evaluation Database images shared in Weizmann Institute of Science website. The proposed model provides better segmentation results compared to the other state of the art models in terms of segmentation accuracy, F -score and CPU execution time. Further, experimental results also prove the robustness of the proposed model in terms of contour initialization, intensity inhomogeneity and noise.},
  archive      = {J_IETIP},
  author       = {Soumen Biswas and Ranjay Hazra},
  doi          = {10.1049/iet-ipr.2020.0214},
  journal      = {IET Image Processing},
  month        = {11},
  number       = {13},
  pages        = {3232-3242},
  shortjournal = {IET Image Process.},
  title        = {Active contours driven by modified LoG energy term and optimised penalty term for image segmentation},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Human activity recognition using improved dynamic image.
<em>IETIP</em>, <em>14</em>(13), 3223–3231. (<a
href="https://doi.org/10.1049/iet-ipr.2019.1739">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In action recognition, the dynamic image (DI) approach is recently proposed to code a video signal to a still image. Since DI descriptor is strongly dependent on first frames, it cannot extract dynamics that do not occur in the first frames or even long dynamics. On the other hand, most of the video frames are not informative for the task of action recognition. Therefore, the authors&#39; intuition is that the process of representing a video using all frames is inefficient. Thus, in this study, they proposed to remove the existing redundancy inside the frames and extract some processed informative images based on the information theory which are called key frames. The proposed method is capable enough to extract sufficient frames regardless of the duration and the position of frames in the entire video. Motivated by this method and DI, they proposed a novel key frames dynamic image (KFDI) approach. Experimental results on popular UCF11, Olympic Sports, and J-HMDB datasets show the superiority of the proposed KFDI approach compared to the DI in capturing long dynamics of videos for action recognition. Their experiments show KFDI improves the accuracy 2–6%compared to DI.},
  archive      = {J_IETIP},
  author       = {Mohammadreza Riahi and Mohammad Eslami and Seyed Hamid Safavi and Farah Torkamani Azar},
  doi          = {10.1049/iet-ipr.2019.1739},
  journal      = {IET Image Processing},
  month        = {11},
  number       = {13},
  pages        = {3223-3231},
  shortjournal = {IET Image Process.},
  title        = {Human activity recognition using improved dynamic image},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Multiscale matters for part segmentation of instruments in
robotic surgery. <em>IETIP</em>, <em>14</em>(13), 3215–3222. (<a
href="https://doi.org/10.1049/iet-ipr.2020.0320">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A challenging aspect of instrument segmentation in robotic surgery is to distinguish different parts of the same instrument. Parts with similar textures are common in a practical instrument and are difficult to distinguish. In this work, the authors introduce an end-to-end recurrent model that comprises a multiscale semantic segmentation network and a refinement model. Specifically, the semantic segmentation network uniformly transforms the input images in multiple scales into a semantic mask, and the refinement model is a single-scale net recurrently optimising the above semantic mask. Through extensive experiments, the authors validate that the models with multiscale inputs perform better than those to fuse encoded feature maps and ones with spatial attention. Furthermore, the authors verify the effectiveness of the proposed model with state-of-the-art performances on several robotic instrument datasets derived from MICCAI Endoscopic Vision Challenges.},
  archive      = {J_IETIP},
  author       = {Wenhao He and Haitao Song and Yue Guo and Guibin Bian and Yuejie Sun and Xiaowei Zhou and Xiaonan Wang},
  doi          = {10.1049/iet-ipr.2020.0320},
  journal      = {IET Image Processing},
  month        = {11},
  number       = {13},
  pages        = {3215-3222},
  shortjournal = {IET Image Process.},
  title        = {Multiscale matters for part segmentation of instruments in robotic surgery},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Colour image enhancement with brightness preservation and
edge sharpening using a heat conduction matrix. <em>IETIP</em>,
<em>14</em>(13), 3202–3214. (<a
href="https://doi.org/10.1049/iet-ipr.2020.0393">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this study, an enhancement process obtained by applying the heat conduction equation of solid and stagnant fluids on colour images is proposed. After the colour channel stretching, the RGB colour image was converted to the HSI model. The heat conduction equation was applied for each pixel on the I channel of the HSI colour model. The elements of the feature matrix called heat conduction matrix (HCM) can have negative, positive or zero values. A pixel with a small negative HCM value indicates that I needs level enhancement for a good image, whereas a small positive HCM value means that the I level value will be reduced and aligned with its neighbours. High positive or negative values are defined as the edges of the objects and the I level values of such pixels are not changed to protect the edges. In addition, whether HCM is negative or positive, the balanced increment and decrement path at a level I ensures that the mean brightness value performs natural protection. Finally, an enhanced image is obtained by transitioning from the HSI to the RGB colour model. Experimental results show that this method can enhance colour image details better than other methods.},
  archive      = {J_IETIP},
  author       = {Ferzan Katırcıoğlu},
  doi          = {10.1049/iet-ipr.2020.0393},
  journal      = {IET Image Processing},
  month        = {11},
  number       = {13},
  pages        = {3202-3214},
  shortjournal = {IET Image Process.},
  title        = {Colour image enhancement with brightness preservation and edge sharpening using a heat conduction matrix},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Multi-modal image fusion based on saliency guided in NSCT
domain. <em>IETIP</em>, <em>14</em>(13), 3188–3201. (<a
href="https://doi.org/10.1049/iet-ipr.2019.1319">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image fusion aims at aggregating the redundant and complementary information in multiple original images, the most challenging aspect is to design robust features and discriminant model, which enhances saliency information in the fused image. To address this issue, the authors develop a novel image fusion algorithm for preserving the invariant knowledge of the multimodal image. Specifically, they formulate a novel unified architecture based on non-subsampled contourlet transform (NSCT). Their method introduces Quadtree decomposition and Bezier interpolation to extract crucial infrared features. Furthermore, they propose a saliency advertising phase congruency-based rule and local Laplacian energy-based rule for low- and high-pass sub-bands fusion, respectively. In this approach, the fusion image could not only combine the local and global features of the source image to avoid smoothing the edge of the target, but also retain the minor scales details and resists the interference noise of the multi-modal image. Both objective assessments and subjective visions of experimental results indicate that the proposed algorithm performs competitively in both objective evaluation criteria and visual quality.},
  archive      = {J_IETIP},
  author       = {Shiying Wang and Yan Shen},
  doi          = {10.1049/iet-ipr.2019.1319},
  journal      = {IET Image Processing},
  month        = {11},
  number       = {13},
  pages        = {3188-3201},
  shortjournal = {IET Image Process.},
  title        = {Multi-modal image fusion based on saliency guided in NSCT domain},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Target distance measurement method using monocular vision.
<em>IETIP</em>, <em>14</em>(13), 3181–3187. (<a
href="https://doi.org/10.1049/iet-ipr.2019.1293">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most existing machine vision-based location methods mainly focus on the spatial positioning schemes using one or two cameras along with non-vision sensors. To achieve an accurate location, both schemes require processing a large amount of data. In this study, the authors propose a novel method, which requires much less amount of data to be processed for measuring target distance using monocular vision. Based on the geometric model of camera imaging, the parameters of the camera (such as camera&#39;s focal length and equivalent focal length.), as well as the principle of analogue signal being transformed into a digital signal, the authors derive the relationship among the target distance, field of view, equivalent focal length and camera resolution. Experimental results show that the proposed method can effectively and accurately achieve the target distance measurement.},
  archive      = {J_IETIP},
  author       = {Mao Jiafa and Huang Wei and Sheng Weiguo},
  doi          = {10.1049/iet-ipr.2019.1293},
  journal      = {IET Image Processing},
  month        = {11},
  number       = {13},
  pages        = {3181-3187},
  shortjournal = {IET Image Process.},
  title        = {Target distance measurement method using monocular vision},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020a). Dynamic mosaicking: Combining a* algorithm with fractional
brownian motion for an optimal seamline detection. <em>IETIP</em>,
<em>14</em>(13), 3169–3180. (<a
href="https://doi.org/10.1049/iet-ipr.2019.1619">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image mosaicking is a combination of algorithms that use two or several images to create a single image. The resulting mosaic is a representation of a scene of the used images with a larger field of vision. However, since dynamic objects can exist in the overlap regions of these images, ghosting and parallax effects appear, therefore poor results are obtained. To overcome these unwanted effects and to achieve better results, a new method is presented in this paper. This approach uses a new way to detect dynamic objects in the common areas by using a fractional Brownian motion with a predetermined similarity function instead of a noise function, the Zero Normalized Cross Correlation. Thus, it will ensure that a map is created with each pixel having a unique value based on their surroundings even in homogeneous areas. Furthermore, this new approach combines the previously computed map with the machine learning algorithm A* for a fast and efficient way to find an optimal seamline. Consequently, the obtained experimental results were compared with different methods and better results were obtained as can be seen by a better quality seamline measure, a result mosaic without any artifacts and a faster computation time.},
  archive      = {J_IETIP},
  author       = {Saadeddine Laaroussi and Aziz Baataoui and Akram Halli and Khalid Satori},
  doi          = {10.1049/iet-ipr.2019.1619},
  journal      = {IET Image Processing},
  month        = {11},
  number       = {13},
  pages        = {3169-3180},
  shortjournal = {IET Image Process.},
  title        = {Dynamic mosaicking: Combining a* algorithm with fractional brownian motion for an optimal seamline detection},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Multi-scale patches based image denoising using weighted
nuclear norm minimisation. <em>IETIP</em>, <em>14</em>(13), 3161–3168.
(<a href="https://doi.org/10.1049/iet-ipr.2019.1654">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As a prior knowledge, non-local self-similarity (NSS) has been widely utilised in ill-posed problems. Actually, similar textures appear not only in a single scale, but also in different scales. Unlike most existing patch-based methods that only explore NSS in the same scale, a multi-scale patches based image denoising algorithm is proposed in this study. The authors have designed a multi-scale strategy to expand the search space of block-matching, which will increase the probability of finding more similar patches. After that, the weighted nuclear norm minimisation (WNNM) algorithm is employed to reveal latent clean patches. With the join of the multi-scale framework, the performance of WNNM can be improved. The proposed algorithm can be used to solve NSS-based image restoration tasks. In this study, mainly image denoising is studied, and its effectiveness is derived through experiments on widely used test images.},
  archive      = {J_IETIP},
  author       = {Yuli Fu and Junwei Xu and Youjun Xiang and Zhen Chen and Tao Zhu and Lei Cai and Weihong He},
  doi          = {10.1049/iet-ipr.2019.1654},
  journal      = {IET Image Processing},
  month        = {11},
  number       = {13},
  pages        = {3161-3168},
  shortjournal = {IET Image Process.},
  title        = {Multi-scale patches based image denoising using weighted nuclear norm minimisation},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Recognition of distorted QR codes with one missing position
detection pattern. <em>IETIP</em>, <em>14</em>(13), 3154–3160. (<a
href="https://doi.org/10.1049/iet-ipr.2019.1095">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Quick response (QR) codes are widely used in many fields. Various QR code recognition approaches have been proposed to improve the accuracy of decoding QR code. However, the recognition of distorted QR codes with one missing position detection pattern (PDP) remains a problem. In this study, based on the vector relationship and the structural features, the authors introduce a new method for decoding distorted QR code with one missing PDP. Three methods, Zxing, Halcon, and the newly proposed method, are used to test the decoding capability. For QR codes with one missing PDP, the experimental results show that the proposed method could meet the recognition angle range as much as 110°, while Zxing fails to recognise, and the angle of decoding for Halcon is 90°. Especially, the proposed method is available at an extremely harsh luminance and contrast environment, e.g. both phases as a 60% discount, when the decoding angle of Halcon is only 35°, while the proposed one better than 2.7 times of it. Besides, the proposed method is more robust to decode the QR codes with a missing PDP under different backgrounds and noisy images.},
  archive      = {J_IETIP},
  author       = {Jianfen Huang and Liyan Li and Xiao Wang and Baoli Lu and Yuliang Liu},
  doi          = {10.1049/iet-ipr.2019.1095},
  journal      = {IET Image Processing},
  month        = {11},
  number       = {13},
  pages        = {3154-3160},
  shortjournal = {IET Image Process.},
  title        = {Recognition of distorted QR codes with one missing position detection pattern},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Advanced framework for highly secure and cloud-based storage
of colour images. <em>IETIP</em>, <em>14</em>(13), 3143–3153. (<a
href="https://doi.org/10.1049/iet-ipr.2018.5654">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The evergrowing virtualised information technology infrastructure is powered by cloud-centric technology around the world. Cloud-based multimedia storage has become an essential aspect for users and business behemoths. However, as per a survey of Norton, around 3800 breaches have been publicly disclosed with 4.1 billion numbers of records exposed in 2019, which is a 54% rise when compared to 2018. So the data security is a widely quoted barrier for cloud storage. Ciphering the confidential images before transmission and subsequent storage in cloud database needs critical attention for techno-specific applications. In image encryption, chaos-based keys can do better confusion, but the diffusion process using XOR is vulnerable to chosen plain text attack. The proposed colour image encryption scheme innately uses Deoxyribo Nucleic Acid coding that blends well with chaotic cryptosystem for an efficient statistical shift. The ciphered images are stored in authenticated and authorised cloud storage facilities. The experimentation is carried out with the help of Amazon Web Services storage instances. The proposed image encryption scheme offers a strong resistance towards the brute force, occlusion, statistical and differential attacks and yields near-zero correlation and good entropy.},
  archive      = {J_IETIP},
  author       = {Nithya Chidambaram and Pethuru Raj and Karruppuswamy Thenmozhi and Rengarajan Amirtharajan},
  doi          = {10.1049/iet-ipr.2018.5654},
  journal      = {IET Image Processing},
  month        = {11},
  number       = {13},
  pages        = {3143-3153},
  shortjournal = {IET Image Process.},
  title        = {Advanced framework for highly secure and cloud-based storage of colour images},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Video summarisation with visual and semantic cues.
<em>IETIP</em>, <em>14</em>(13), 3134–3142. (<a
href="https://doi.org/10.1049/iet-ipr.2019.1355">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Video summarisation greatly improves the efficiency of people browsing videos and saves storage space. A good video summary should satisfy human visual interestingness and preserve the theme of the original video at the semantic level. Unlike many existing methods that consider only visual features to generate video summaries, this study proposes a method that combines visual and semantic cues to extract important information for dynamic video summarisation. The authors propose visual-verbal saliency consistency to add semantic information and propose a novel attention motion, along with other visual features to fully represent visual interestingness. Based on the importance score of each frame calculated by combining these features, they select an optimal subset of segments to generate an important and interesting summary. They evaluate their method using the SumMe and TVSum datasets and experimental results show that their method generates high-quality video summaries.},
  archive      = {J_IETIP},
  author       = {Binwei Xu and Haoran Liang and Ronghua Liang},
  doi          = {10.1049/iet-ipr.2019.1355},
  journal      = {IET Image Processing},
  month        = {11},
  number       = {13},
  pages        = {3134-3142},
  shortjournal = {IET Image Process.},
  title        = {Video summarisation with visual and semantic cues},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Multi-exposure image fusion via a pyramidal integration of
the phase congruency of input images with the intensity-based maps.
<em>IETIP</em>, <em>14</em>(13), 3127–3133. (<a
href="https://doi.org/10.1049/iet-ipr.2019.1147">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The most important part of the common algorithms for multi-exposure image fusion (MEF) is the selection of features and metrics that are appropriate for weight map extraction. This study presents a structure-based multi-exposure image fusion by employing the phase congruency (PC) of the input image. The main idea behind PC-based analysis is that the locations of image key attributes are at points where frequency components are maximally in phase. PC detects the details of an image invariant to its contrast and also emphasises on the texture- or structure-based features. In this work, alongside intensity-based maps, the extracted PC-based map is utilised for MEF in a pyramidal manner. Several experiments conducted on the benchmark dataset including a variety of natural multi-exposed image sequences to evaluate the proposed algorithm. Quantitative evaluations in terms of MEF structural similarity index and visual quality assessments show that the proposed method achieves better performance and produces comparable fused images in comparison to other approaches.},
  archive      = {J_IETIP},
  author       = {Alireza Asadi and Mehdi Ezoji},
  doi          = {10.1049/iet-ipr.2019.1147},
  journal      = {IET Image Processing},
  month        = {11},
  number       = {13},
  pages        = {3127-3133},
  shortjournal = {IET Image Process.},
  title        = {Multi-exposure image fusion via a pyramidal integration of the phase congruency of input images with the intensity-based maps},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Toward a general model for reflection recovery and single
image enhancement. <em>IETIP</em>, <em>14</em>(13), 3117–3126. (<a
href="https://doi.org/10.1049/iet-ipr.2019.1175">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Images often suffer from low visual quality due to poor imaging conditions such as low light or hazy weather. The haze imaging model is widely used in contrast enhancement in daylight condition with haze, while the retinex model is universal for low-light conditions. Although their forms and applications are different, they can be unified into a more general form through the proposed observation. Based on this model, the authors can estimate the reflection of the scene more accurately in more complex imaging conditions. In this study, the authors propose a simple but effective method for estimating the reflection and enhancing the image contrast based on a general imaging model. To preserve the image details and control contrast, the authors introduce dark boundary and bright boundary to handle the high-light and low-light conditions, and a guided structure-preserving optimization algorithm is proposed to estimate them. After obtaining the dark and bright boundaries, the reflection is calculated and the image is enhanced accordingly. Different from previous approaches, which were designed for specific applications, the proposed method can be used for more diverse imaging conditions. Experiments show that the proposed method can be applied to many poor imaging conditions and maintain good performance.},
  archive      = {J_IETIP},
  author       = {Meng Chang and Qi Li and Zhuang He and Huajun Feng and Zhihai Xu},
  doi          = {10.1049/iet-ipr.2019.1175},
  journal      = {IET Image Processing},
  month        = {11},
  number       = {13},
  pages        = {3117-3126},
  shortjournal = {IET Image Process.},
  title        = {Toward a general model for reflection recovery and single image enhancement},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Siamese convolutional neural network-based approach towards
universal image forensics. <em>IETIP</em>, <em>14</em>(13), 3105–3116.
(<a href="https://doi.org/10.1049/iet-ipr.2019.1114">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study proposes a novel deep learning-based method which can detect different types of image editing operations carried out on images. Unlike most of the existing methods, which can only detect the editing operations considered in the training stage, the proposed method can generalise to manipulations not seen in the training stage. The method is based on the classification of image pairs as either similarly or differently processed using a deep siamese neural network. Once the network learns features that can discriminate different editing operations, it can check whether an image is processed with an editing operation, not present in the training stage, using the one-shot classification strategy. An image forgery detection and localisation technique is also proposed using the trained siamese network. The experimental results show the efficacy of the proposed method in detecting different editing operations and also show the ability in detecting and localising image forgeries.},
  archive      = {J_IETIP},
  author       = {Aniruddha Mazumdar and Prabin Kumar Bora},
  doi          = {10.1049/iet-ipr.2019.1114},
  journal      = {IET Image Processing},
  month        = {11},
  number       = {13},
  pages        = {3105-3116},
  shortjournal = {IET Image Process.},
  title        = {Siamese convolutional neural network-based approach towards universal image forensics},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Denoising real bursts with squeeze-and-excitation residual
network. <em>IETIP</em>, <em>14</em>(13), 3095–3104. (<a
href="https://doi.org/10.1049/iet-ipr.2020.0041">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The goal of image denoising is to recover a clean image from noisy input(s). For single image denoising, utilising similarities (or priors) within and across an image dataset helps recover clean images. As the noise level increases, using multiple frames become feasible, which is defined as burst denoising. In this study, the authors propose a deep residual model with squeeze-and-excitation (SE) modules for the burst denoising. Unlike previous methods, the authors&#39; model does not need an explicit aligning procedure, which is light-weighted and fast. The network contains a noise estimation convolutional neural network, which makes it capable of blind denoising. Besides, by inverting the image processing pipeline and simulating real noise in bursts, their model can suppress real noise blindly. Since denoising performance is closely related to the noise level, frame displacement, and the number of frames (burst length), intensive experiments including ablation study are performed. Quantitative results show that the proposed method performs significantly better than previous state-of-the-art methods V-BM4D and KPN in removing Gaussian noise. Qualitative results show that the proposed method is also effective in removing real noise using bursts and the SE module is key to reduce blur in results.},
  archive      = {J_IETIP},
  author       = {Hanlin Tan and Huaxin Xiao and Shiming Lai and Yu Liu and Maojun Zhang},
  doi          = {10.1049/iet-ipr.2020.0041},
  journal      = {IET Image Processing},
  month        = {11},
  number       = {13},
  pages        = {3095-3104},
  shortjournal = {IET Image Process.},
  title        = {Denoising real bursts with squeeze-and-excitation residual network},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Analysis of the correlation between infrared thermal
sequence images of nostril area and respiratory rate. <em>IETIP</em>,
<em>14</em>(13), 3089–3094. (<a
href="https://doi.org/10.1049/iet-ipr.2019.0416">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this study, a thermal imaging instrument was used to obtain facial thermal image information which was then used to calculate the number of breaths taken. However, small movements were inevitable and the first issue addressed was the means by which image calibration and region selection was to be made. To this end, thermal image sequence data calibration was done using technology that resolved small natural deviations in the nostril area. After these problems had been solved, a Hampel filter was used to process the nostril area signals. The independent component method was used to filter the effects of non-respiratory signals, and the least squares method was employed for smoothing. Savitzky-Golay filtering was used to adjust the signal baseline and the processed nostril region thermal image signals were compared with standard abdominal breathing band signals. Results showed that the difference in the number of breaths per minute was less than 1.5 times. The usual normal respiratory frequency range lies within a range of 0.1–0.5Hz. In the calculation of ‘coherence obtained from MVAR model’, the spectral coherence analysis results showed the methods proposed in this study can substantially enhance the relevance between 0.15 and 0.2Hz.},
  archive      = {J_IETIP},
  author       = {Bo-Lin Jian and Min-Wei Huang and Shin-Hsiung Lee and Her-Terng Yau},
  doi          = {10.1049/iet-ipr.2019.0416},
  journal      = {IET Image Processing},
  month        = {11},
  number       = {13},
  pages        = {3089-3094},
  shortjournal = {IET Image Process.},
  title        = {Analysis of the correlation between infrared thermal sequence images of nostril area and respiratory rate},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Eigenstructure involving the histogram for image
thresholding. <em>IETIP</em>, <em>14</em>(13), 3084–3088. (<a
href="https://doi.org/10.1049/iet-ipr.2019.1428">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The idea of the proposed image thresholding scheme is simply to consider the histogram as a 2D plot rather than a 1D function. The data can now be represented as a two-row matrix. The first row is simply the grey levels of the image and the second row is the corresponding histogram values. Multiplying this matrix by its transpose will result in a power-type matrix of size 2 × 2. The best threshold is the one producing a power matrix closer to that of the original image. Many combinations of the eigenvalues are suggested. To increase the correlation with the first row of the matrix, the histogram is replaced by the cumulative histogram. It is noticed that the trace of the matrix produces the best results. Comparative results show the effectiveness of the proposed schemes.},
  archive      = {J_IETIP},
  author       = {Salah Ameer},
  doi          = {10.1049/iet-ipr.2019.1428},
  journal      = {IET Image Processing},
  month        = {11},
  number       = {13},
  pages        = {3084-3088},
  shortjournal = {IET Image Process.},
  title        = {Eigenstructure involving the histogram for image thresholding},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Efficient segmentation of lumbar intervertebral disc from MR
images. <em>IETIP</em>, <em>14</em>(13), 3076–3083. (<a
href="https://doi.org/10.1049/iet-ipr.2019.0971">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Segmentation of spine Magnetic Resonance Images (MRIs) has become an indispensable process in the diagnosis of lumbar disc degeneration, causing low back pain. Over the last decade of years, computer-directed diagnosis of disease, as well as computer-guided spine surgery, is based on the two-dimensional (2D) analysis of mid-sagittal slice of MRI. This work proposes an automatic strategy to extract the 3D segmentation of the normal disc as well as degenerated lumbar intervertebral discs (IVDs) from T2-weighted Turbo Spin Echo MRI of the spine using Connected Component (CC) analysis algorithm and statistical shape analysis. The challenges faced by the IVD segmentation includes (i) partial volume effects (ii) intensity inhomogeneity (iii) grey level overlap of different soft tissues. The proposed method first pre-processes the dataset and enables it for the application of the CC algorithm. The CC (subsets of pixels of the disc) of the spine MRI is extracted and apply statistical shape analysis for the refinement of the segmentation results to detect IVDs. Experimental results of the proposed method show a robust segmentation, accomplishing the dice similarity index of 92.4% and thus achieving a low error rate. Other performance measures such as Precision, Accuracy, JaccardIdx, JaccardDist, Global Consistency Error, Variation of Information, etc were also evaluated. The algorithm is evaluated quantitatively using adequate experiments on a dataset of 15 MRI scans, of different scenarios such as healthy and degenerate disc and this proposed method is verified as a promising accurate method for the automatic segmentation of IVD.},
  archive      = {J_IETIP},
  author       = {Leena Silvoster M and Retnaswami Mathusoothana S. Kumar},
  doi          = {10.1049/iet-ipr.2019.0971},
  journal      = {IET Image Processing},
  month        = {11},
  number       = {13},
  pages        = {3076-3083},
  shortjournal = {IET Image Process.},
  title        = {Efficient segmentation of lumbar intervertebral disc from MR images},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Mutual information guided 3D ResNet for self-supervised
video representation learning. <em>IETIP</em>, <em>14</em>(13),
3066–3075. (<a href="https://doi.org/10.1049/iet-ipr.2020.0019">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this work, the authors propose a novel self-supervised learning method based on mutual information to learn representations from the videos without manual annotation. Different video clips sampled from the same video usually have coherence in the temporal domain. To guide the network to learn such temporal coherence, they maximise the mutual information between global features extracted from different clips sampled from the same video (Global-MI). However, maximising the Global-MI leads the network to seek shared content from different video clips and may make the network degenerate to focus on the background of the video. Considering the structure of the video, they further maximise the average mutual information between the global feature and local patches of multiple regions of the video clip (multi-region Local-MI). Their approach, which is called Max-GL, learns the temporal coherence by jointly maximising the Global-MI and multi-region Local-MI. Experiments are conducted to validate the effectiveness of the proposed Max-GL. Experimental results show that the Max-GL can serve as an effective pre-training method for the task of action recognition in videos. Additional experiments for the task of action similarity labelling and dynamic scene recognition also validate the generalisation of the learned representations of the Max-GL.},
  archive      = {J_IETIP},
  author       = {Fei Xue and Hongbing Ji and Wenbo Zhang},
  doi          = {10.1049/iet-ipr.2020.0019},
  journal      = {IET Image Processing},
  month        = {11},
  number       = {13},
  pages        = {3066-3075},
  shortjournal = {IET Image Process.},
  title        = {Mutual information guided 3D ResNet for self-supervised video representation learning},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Occlusion-handling tracker based on discriminative
correlation filters. <em>IETIP</em>, <em>14</em>(13), 3054–3065. (<a
href="https://doi.org/10.1049/iet-ipr.2019.0651">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Visual object tracking (VOT) based on discriminative correlation filters (DCF) has received great attention due to its higher computational efficiency and better robustness. However, DCF-based methods suffer from the problem of model contamination. The tracker will drift into the background due to the uncertainties brought by shifting among peaks, which will further lead to the issues of model degradation. To deal with occlusions, a novel Occlusion-Handling Tracker Based on Discriminative Correlation Filters (OHDCF) framework is proposed for online visual object tracking, where an occlusion-handling strategy is integrated into the spatial–temporal regularized correlation filters (STRCF). The occlusion-handling tracker follows a hybrid approach to handle partial occlusion and complete occlusion. Specifically, we first present a function to determine whether occlusion occurs. Then, the proposed filter uses block-based and feature-matching methods to determine whether an object is partially occluded or completely occluded. Following this, we use different methods to track the target. Extensive experiments have performed on OTB-100, Temple-Color-128, VOT-2016 and VOT-2018 datasets, the results show that OHDCF achieves promising performance compared to other state-of-the-art trackers. On VOT-2018, OHDCF significantly outperforms STRCF from the challenge with a relative gain of 4.8 in EAO and a gain of 4.6 in Accuracy.},
  archive      = {J_IETIP},
  author       = {Yue Xie and Hanling Zhang and Lijun Li},
  doi          = {10.1049/iet-ipr.2019.0651},
  journal      = {IET Image Processing},
  month        = {11},
  number       = {13},
  pages        = {3054-3065},
  shortjournal = {IET Image Process.},
  title        = {Occlusion-handling tracker based on discriminative correlation filters},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Class-aware single image to 3D object translational
autoencoder. <em>IETIP</em>, <em>14</em>(13), 3046–3053. (<a
href="https://doi.org/10.1049/iet-ipr.2019.1152">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The performances of generative adversarial network (GAN) and autoencoder (AE) models on images have been gathering a great deal of interest in terms of transferring them to three-dimensional (3D) domain. In this study, single image to object reconstruction problem was focused by presenting a novel 2D-to-3D AE model inspired by the recent improvements. To benefit from middle-level features, a model with skip connections was constructed by transferring 2D features to 3D domain. Moreover, the authors considered class-awareness for obtaining a category-agnostic model using limited class-annotations. Apart from recent 3D reconstruction models, they adapted the intersection-over-union score based objective, which is used in the object segmentation model, for improving reconstruction performance. With all these contributions, they call their model as skipped volumetric class-aware AE (SkipVCAE). According to experimental studies, proposed model obtained higher scores than the state-of-the-art model given. The results have proven its performance as a category-specific and category-agnostic model together owing to its class-aware nature. In the further analysis, it was seen that presented model yielded satisfactory results on a single image to object modelling compared to its multi-view version thanks to class-awareness.},
  archive      = {J_IETIP},
  author       = {Ceren Guzel Turhan and Hasan Sakir Bilge},
  doi          = {10.1049/iet-ipr.2019.1152},
  journal      = {IET Image Processing},
  month        = {11},
  number       = {13},
  pages        = {3046-3053},
  shortjournal = {IET Image Process.},
  title        = {Class-aware single image to 3D object translational autoencoder},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Minimum class variance broad learning system for
hyperspectral image classification. <em>IETIP</em>, <em>14</em>(13),
3039–3045. (<a href="https://doi.org/10.1049/iet-ipr.2019.1200">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A new machine learning method named as a broad learning system (BLS) has been proposed recently. The advantage of simple, fast, and good generalisation ability make it attracting extensive attention. In this study, by introducing BLS to solving hyperspectral image (HSI) classification, a minimum class variance BLS (MCVBLS) was proposed. Firstly, in order to get spectral–spatial representation of original HSI, spectral–spatial feature learning has been performed to take full advantage of abundant spectral and spatial information of HSI. Then, the authors use MCVBLS to classify the extracted spectral–spatial features. MCVBLS, in contrast to BLS, fully considers the global data structure and discriminant information of the data. MCVBLS enhances the classification performance model by minimising the intra-class distribution structure while maximising the inter-class discriminant information, the measure of placing restrictions on output weights will take more discriminative information and global discriminative structure information into consideration. Conducting an experiment on three benchmark hyperspectral datasets, they demonstrate that the proposed MCVBLS methods are effective for HSI classification, better than other state-of-the-art methods.},
  archive      = {J_IETIP},
  author       = {Peng Chen},
  doi          = {10.1049/iet-ipr.2019.1200},
  journal      = {IET Image Processing},
  month        = {11},
  number       = {13},
  pages        = {3039-3045},
  shortjournal = {IET Image Process.},
  title        = {Minimum class variance broad learning system for hyperspectral image classification},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020b). Adaptive iterative global image denoising method based on
SVD. <em>IETIP</em>, <em>14</em>(13), 3028–3038. (<a
href="https://doi.org/10.1049/iet-ipr.2020.0082">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Based on the image self-similarity and singular value decomposition (SVD) techniques, the authors propose an iterative adaptive global denoising method. For the structural differences between image patches, they adaptively determine the size of the search window. In each window, a similar image patch matrix is constructed based on the multi-scale similarity measure. In order to ensure the speed of the method, the adaptive step size and the number of image patches are introduced, and all image patches are denoised in different iterations. This not only ensures the speed of the method, suppresses residual noise, but also reduces the artefacts caused by the fixed step size and the number of image patches. Therefore, the problem of image denoising is converted to the estimation of low-rank matrix. New singular values are estimated according to the noise level, and similar image patch matrices without noise are estimated using them and corresponding singular vectors. Experimental results show that compared with the state-of-the-art denoising algorithms, this method has a higher PSNR and FSIM, and has a good visual effect. The new method can be applied to image and video restoration, target recognition and image classification.},
  archive      = {J_IETIP},
  author       = {Yepeng Liu and Xuemei Li and Qiang Guo and Caiming Zhang},
  doi          = {10.1049/iet-ipr.2020.0082},
  journal      = {IET Image Processing},
  month        = {11},
  number       = {13},
  pages        = {3028-3038},
  shortjournal = {IET Image Process.},
  title        = {Adaptive iterative global image denoising method based on SVD},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Feedback evaluations to promote image captioning.
<em>IETIP</em>, <em>14</em>(13), 3021–3027. (<a
href="https://doi.org/10.1049/iet-ipr.2019.1317">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image captioning can be treated as a policy gradient problem. A retrieval model to obtain the discriminability score to distinguish between two images, given the caption for one of them, has been proposed previously; the discriminability score and one of the image captioning evaluation metrics were optimised using policy gradient. Based on this, two methods to evaluate the caption and caption-generating process, referred to as feedback evaluations, are proposed in this study. The results of the evaluations were used to improve the model. First, an auxiliary retrieval loss (ARL) is introduced to evaluate the generated caption to improve the discriminability of the model. ARL has been utilised as a feedback evaluation method because it calculates similarity between the generated caption and convolutional neural network features. With ARL, a higher similarity and better discriminability were achieved. Second, an evaluation reward is introduced to evaluate the captioning process. With ER, the overall evaluation metrics can be improved. A policy gradient was used, and a captioning model could be trained by jointly adjusting the captioning process and captioning itself. The attention long short-term memory network was trained with ARL and ER successively and it demonstrated state-of-the-art performance on the COCO database.},
  archive      = {J_IETIP},
  author       = {Jun He and Yijia Zhao and Bo Sun and Lejun Yu},
  doi          = {10.1049/iet-ipr.2019.1317},
  journal      = {IET Image Processing},
  month        = {11},
  number       = {13},
  pages        = {3021-3027},
  shortjournal = {IET Image Process.},
  title        = {Feedback evaluations to promote image captioning},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Blood flow imaging of high frame rate two-dimensional vector
in cardiovascular ultrasound detection. <em>IETIP</em>, <em>14</em>(13),
3014–3020. (<a href="https://doi.org/10.1049/iet-ipr.2019.1052">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traditional cardiovascular ultrasound detection using focused ultrasound can cause a decrease in frame rate, which affects the results of the diagnosis. In order to improve the effect of cardiovascular ultrasound detection, this study used ultrasound vector blood flow imaging technology to improve the image frame rate and introduce planar high frame rate imaging technology. Simultaneously, this work studies the parameters of the image through parameter analysis and combines the advantages of various methods to improve the analysis. In addition, this work proposes a high frame rate blood flow imaging method, and designs simulation experiments to analyse the effectiveness of the method. The results show that the proposed algorithm has a certain effect in two-dimensional vector blood flow imaging, which can be applied to clinical practice, and can provide theoretical reference for subsequent related research.},
  archive      = {J_IETIP},
  author       = {Cungang Wu and Chao Huang},
  doi          = {10.1049/iet-ipr.2019.1052},
  journal      = {IET Image Processing},
  month        = {11},
  number       = {13},
  pages        = {3014-3020},
  shortjournal = {IET Image Process.},
  title        = {Blood flow imaging of high frame rate two-dimensional vector in cardiovascular ultrasound detection},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Image super-resolution based on conditional generative
adversarial network. <em>IETIP</em>, <em>14</em>(13), 3006–3013. (<a
href="https://doi.org/10.1049/iet-ipr.2018.5767">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Generative adversarial network (GAN) is one of the most prevalent generative models that can synthesise realistic high-frequency details. However, a mismatch between the input and the output may arise when GAN is directly applied to image super-resolution. To alleviate this issue, the authors adopted a conditional GAN (cGAN) in this study. The cGAN discriminator attempted to guess whether the unknown high-resolution (HR) image was produced by the generator with the aid of the original low-resolution (LR) image. They propose a novel discriminator that only penalises at the scale of the patch and, thus, has relatively few parameters to train. The generator of cGAN is an encoder–decoder with skip connections to shuttle the shared low-level information directly across the network. To better maintain the low-frequency information and recover the high-frequency information, they designed a generator loss function combining adversarial loss term and L1 loss term. The former term is beneficial to the synthesis of fine-grained textures, while the latter is responsible for learning the overall structure of the LR input. The experiments revealed that the proposed method could generate HR images with richer details and less over-smoothness.},
  archive      = {J_IETIP},
  author       = {Hongxia Gao and Zhanhong Chen and Binyang Huang and Jiahe Chen and Zhifu Li},
  doi          = {10.1049/iet-ipr.2018.5767},
  journal      = {IET Image Processing},
  month        = {11},
  number       = {13},
  pages        = {3006-3013},
  shortjournal = {IET Image Process.},
  title        = {Image super-resolution based on conditional generative adversarial network},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Approach for shadow detection and removal using machine
learning techniques. <em>IETIP</em>, <em>14</em>(13), 2998–3005. (<a
href="https://doi.org/10.1049/iet-ipr.2020.0001">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this work, the authors have proposed a method for shadow detection and removal from videos by utilising methods of machine learning. From literature, various algorithms on shadow detection and removal have been accounted with advantages and disadvantages. Here some algorithms have a need for manual alignment and predefined explicit parameters, but fail to give precise outcome in different lighting and ecological surroundings. In this work, the authors propose a three-phase framework. In first stage, key frames are chosen by utilising features based K-means clustering which selects the key frames using features like colour, shape and surface. In second stage, they utilised two-stage segmentation techniques to segment the shadows by marking the region of interest. In the final step, they use threshold based segmentation to remove the shadow in videos. The performance of the proposed method is compared by performance evaluation of all state-of-the-art methods. The proposed strategies are established to achieve superior results in comparison to other state-of-the-art methods.},
  archive      = {J_IETIP},
  author       = {Mohankumar Shilpa and Madigondanahalli Thimmaiah Gopalakrishna and Chikkaguddaiah Naveena},
  doi          = {10.1049/iet-ipr.2020.0001},
  journal      = {IET Image Processing},
  month        = {11},
  number       = {13},
  pages        = {2998-3005},
  shortjournal = {IET Image Process.},
  title        = {Approach for shadow detection and removal using machine learning techniques},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Ring oscillator as confusion – diffusion agent: A complete
TRNG drove image security. <em>IETIP</em>, <em>14</em>(13), 2987–2997.
(<a href="https://doi.org/10.1049/iet-ipr.2019.0168">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The utility of true random number generators (TRNGs) is not only restricted to session key generation, nonce generation, OTP generation etc. in cryptography. In the proposed work, two ring oscillator (RO) based TRNG structures adopting identical and non-identical ring of inverters have alone been employed for confusion (scrambling) and diffusion (intensity variation) processes for encrypting the greyscale and RGB images. Cyclone IVE EP4CE115F29C7 FPGA was utilised to generate a couple of random synthetic images using the two RO architectures which took a maximum of 520 combinational units and 543 logic registers. The suggested scheme of image encryption was tested on 100 test greyscale images of size 256 × 256. This non-chaos influenced image ciphering has resulted in an approximate average entropy of 7.99 and near-zero correlation figures for the greyscale &amp; RGB cipher images. The attack resistance capability was checked by performing various occlusion and noise attacks on encrypted images.},
  archive      = {J_IETIP},
  author       = {Rethinam Sivaraman and Sundararaman Rajagopalan and John Bosco Balaguru Rayappan and Rengarajan Amirtharajan},
  doi          = {10.1049/iet-ipr.2019.0168},
  journal      = {IET Image Processing},
  month        = {11},
  number       = {13},
  pages        = {2987-2997},
  shortjournal = {IET Image Process.},
  title        = {Ring oscillator as confusion – diffusion agent: A complete TRNG drove image security},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Boundary detection using unbiased sparseness-constrained
colour-opponent response and superpixel contrast. <em>IETIP</em>,
<em>14</em>(13), 2976–2986. (<a
href="https://doi.org/10.1049/iet-ipr.2019.0949">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Boundaries play a crucial role in various image-based tasks, but many existing non-learning-based boundary detection methods underperform in recognising authentic boundaries from a complex background. In this study, the authors address this problem using the sparseness-constrained colour-opponent response and the superpixel contrast. First, building on the biologically inspired colour-opponency mechanism, the authors elaborate a method to compute the unbiased sparseness-constrained colour-opponent response. In this procedure, locations showing colour variations are enhanced, while the textural locations are preliminarily suppressed by the cue of local sparseness measure. Second, with the help of superpixel segmentation, the authors present an effective approach to obtain the superpixel contrast map. This approach helps to exploit the object shape information in suppressing textures. Consequently, the authors propose a non-learning-based method to detect boundaries in images, combining the unbiased sparseness-constrained colour-opponent response and the overall superpixel contrast map. Experiment results on widely adopted datasets manifest that the authors method outperforms most of the competing methods. In particular, compared with the state-of-the-art surround-modulation method, the proposed method obtains a comparable performance while consuming much less runtime.},
  archive      = {J_IETIP},
  author       = {Gang Wang and Yong-guang Chen and Min Gao and Suo-chang Yang and Fu-qiang Feng and Bernard De Baets},
  doi          = {10.1049/iet-ipr.2019.0949},
  journal      = {IET Image Processing},
  month        = {11},
  number       = {13},
  pages        = {2976-2986},
  shortjournal = {IET Image Process.},
  title        = {Boundary detection using unbiased sparseness-constrained colour-opponent response and superpixel contrast},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Structure preservation in content-aware image retargeting
using multi-operator. <em>IETIP</em>, <em>14</em>(13), 2965–2975. (<a
href="https://doi.org/10.1049/iet-ipr.2019.1032">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The evolution of image retargeting technique demands the exploitation of multi-operators since they are capable of preserving the structure and salient objects of the image. However, these multi-operators are mostly based on seam carving with scaling or cropping operators which lead to significant distortions in the retargeted image. This study proposes a new multi-operator scheme which has improved seam carving, through the proposed seam diversion based image retargeting algorithm, integrated with the cropping and warping operator. A total of six different multi-operator schemes have been proposed out of which the MO6 technique gave remarkable results in terms of image quality, least distortion, and lowest run-time. To simplify image retargeting operations, an optimised image distance function was used. The optimised image distance function was formulated which combines bidirectional image Euclidean distance, dominant colour descriptor, and an energy-based coefficient to bypass seams from the point where seams start clashing and hence the defined threshold violates. By integrating cropping and warping into the proposed algorithm, it preserves the salient features of the retargeted images. Typical results have been presented which demonstrates the effectiveness of the proposed methods. User-based subjective analysis has also been carried out which shows that image retargeted using the MO6 technique has high user preference.},
  archive      = {J_IETIP},
  author       = {Ankit Garg and Ashish Negi},
  doi          = {10.1049/iet-ipr.2019.1032},
  journal      = {IET Image Processing},
  month        = {11},
  number       = {13},
  pages        = {2965-2975},
  shortjournal = {IET Image Process.},
  title        = {Structure preservation in content-aware image retargeting using multi-operator},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Accelerating convolutional neural network training using
ProMoD backpropagation algorithm. <em>IETIP</em>, <em>14</em>(13),
2957–2964. (<a href="https://doi.org/10.1049/iet-ipr.2019.0761">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Convolutional neural networks (CNNs) play an important role in image recognition applications. Fast training of image recognition systems is a crucial point, because the system should be trained for each new image class. These networks are trained using lengthy calculations. Focus of engineering is on obtaining a fast, but stable optimisation method. Momentum technique which is used in backpropagation algorithms is like a proportional–integral (PI) controller that is widely employed in automatic control systems. It takes the integral of past errors and helps reaching the training targets. Proportional + momentum + derivative (ProMoD) method adds gradient of update matrices to the training process and builds an optimiser such as the widely used PI–derivative controller. The method accelerates the movement toward the target accuracy levels. This is achieved by doing bigger corrections in the beginning using the differences in the calculated update matrices. In this research, ProMoD method is tested on image recognition applications and CNNs. Modified national institute of standards and technology database (MNIST) and Fashion-MNIST datasets are used for evaluating the performance. Experimental results showed that ProMoD might perform much faster in training of CNNs and consume proportionally less power with respect to the momentum and stochastic gradient descent (SGD) techniques.},
  archive      = {J_IETIP},
  author       = {Ahmet Gürhanlı},
  doi          = {10.1049/iet-ipr.2019.0761},
  journal      = {IET Image Processing},
  month        = {11},
  number       = {13},
  pages        = {2957-2964},
  shortjournal = {IET Image Process.},
  title        = {Accelerating convolutional neural network training using ProMoD backpropagation algorithm},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A point-based redesign algorithm for designing geometrically
complex surfaces. A case study: Miralles’s croissant paradox.
<em>IETIP</em>, <em>14</em>(12), 2948–2956. (<a
href="https://doi.org/10.1049/iet-ipr.2020.0223">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study explores the use of point clouds for both representation and genetic morphogenesis of complex geometry. The accurate representation of existing objects of complex curved geometry, which are subsequently geometrically modified by evolutionary morphogenetic processes, is analysed. To this end, as a method of representation and generation of complex geometries, a point-based genetic algorithm and the use of large unstructured point clouds are proposed. A study of convergence and diversity of the implemented algorithm is detailed, as well as a comparison with the Coyote optimisation algorithm in terms of representation error demonstrating its efficiency. Some commonly used three-dimensional formats in architecture, such as NURBS and polygon meshes, are analysed, and compared against point clouds. This study also includes an evaluation regarding whether the use of point clouds is a more suitable format for realistic representation, rationalisation and genetic morphogenesis.},
  archive      = {J_IETIP},
  author       = {Adrian Carballal and Rafael Iván Pazos-Pérez and Nereida Rodriguez-Fernandez and Iria Santos and María D. García-Vidaurrázaga and Juan R. Rabuñal},
  doi          = {10.1049/iet-ipr.2020.0223},
  journal      = {IET Image Processing},
  month        = {10},
  number       = {12},
  pages        = {2948-2956},
  shortjournal = {IET Image Process.},
  title        = {A point-based redesign algorithm for designing geometrically complex surfaces. a case study: Miralles&#39;s croissant paradox},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Distributed ImageJ(fiji): A framework for parallel image
processing. <em>IETIP</em>, <em>14</em>(12), 2937–2947. (<a
href="https://doi.org/10.1049/iet-ipr.2019.0150">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {ImageJ is an open-source application widely used for image processing. It has developer API that can be used to implement new plugins for specific image processing tasks. However, ImageJ wasn&#39;t designed to work on distributed systems. Currently, it is still being used on single machines to process large medical images, which takes several hours to complete. In this article, we present the approaches to make several essential and widely used ImageJ plugins to work in a cluster. As the cluster nodes parallelly run the existing plugins for image processing, they write the results on a shared drive. But one of the main challenges is, merging those results with high accuracy. Several ImageJ plugins were developed to distribute tasks and generate combined results efficiently. The existing 3D-Object-Counter plugin was used for testing the designed system. The experimental results on the test images of 3D objects stored in Tagged Image File Format(TIFF) show faster processing time with high accuracy and similarity compared to the single machine-based results. The image processing time depends on the number of nodes in the cluster. So, we present a mathematical model that determines the cluster size automatically for optimizing the overall image processing time.},
  archive      = {J_IETIP},
  author       = {Md Amjad Hossain and Preoyati Khan and Cheng Chang Lu and Robert J. Clements},
  doi          = {10.1049/iet-ipr.2019.0150},
  journal      = {IET Image Processing},
  month        = {10},
  number       = {12},
  pages        = {2937-2947},
  shortjournal = {IET Image Process.},
  title        = {Distributed ImageJ(Fiji): A framework for parallel image processing},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Super-resolution image reconstruction using molecular
docking. <em>IETIP</em>, <em>14</em>(12), 2922–2936. (<a
href="https://doi.org/10.1049/iet-ipr.2019.0491">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Molecular-docking is an essential tool in the drug designing process, where a small molecule ligand (drug) binds with disease-causing protein molecule to prevent its further activity. Docking process helps in predicting the most appropriate configuration and the optimal interaction energy between the interacting molecules (ligand and protein) to form a stable complex. Based on this idea, a new learning-based single image super-resolution reconstruction (LSI-SRR) method is proposed here. Estimation of a high resolution (HR) patch is achieved by optimising the interaction energy between the input low resolution patch and its corresponding candidate patches appropriately chosen from the training image dataset via Genetic algorithm. Structured-spatiogram based measure; a new and competent similarity criterion is proposed to select potentially efficient training images which encompass better statistical and structural co-relation with the input image. The proposed method is tested on synthetic and real-time images at different magnification factors. Performance analysis of the proposed work is compared with some of the representative state-of-the-art LSI-SRR methods. Experimental results demonstrate that the proposed method produces HR images with enhanced image details, minimal artefacts and most importantly enables an efficient trade-off between the image qualities to speed than the competing methods.},
  archive      = {J_IETIP},
  author       = {Rajashree Nayak and Dipti Patra and Bunil Ku Balabantaray},
  doi          = {10.1049/iet-ipr.2019.0491},
  journal      = {IET Image Processing},
  month        = {10},
  number       = {12},
  pages        = {2922-2936},
  shortjournal = {IET Image Process.},
  title        = {Super-resolution image reconstruction using molecular docking},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Row-level algorithm to improve real-time performance of
glass tube defect detection in the production phase. <em>IETIP</em>,
<em>14</em>(12), 2911–2921. (<a
href="https://doi.org/10.1049/iet-ipr.2019.1506">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the case of the glass tube for pharmaceutical applications, high-quality defect detection is made via inspection systems based on image processing. Such processing must be fast enough to guarantee real-time inspection and to meet the increasing rate and quality required by the market. Defect detection is complex due to specific problems of the production process: vibration, rotation and irregularity of the tube. All these aspects prevent the efficient use of known techniques. The authors present an algorithm that decreases the processing time of the defect detection phase. The algorithm is based on a moving average filter working at row level, that allows to minimize the effects of rotation, vibration, and irregularity of the tube. Luminosity variations due to the tube curvature are cut by the filter and a threshold algorithm can be applied. They made the evaluation considering different solutions taken from literature. The algorithm outperforms, in processing time, all these solutions with increased accuracy. Experimental measures show that the algorithm achieves a throughput gain of 2.6 times with respect to Canny. They develop also a methodology to get the best values for the algorithm parameters directly at the factory, during the change of production batches.},
  archive      = {J_IETIP},
  author       = {Gabriele A. De Vitis and Pierfrancesco Foglia and Cosimo A. Prete},
  doi          = {10.1049/iet-ipr.2019.1506},
  journal      = {IET Image Processing},
  month        = {10},
  number       = {12},
  pages        = {2911-2921},
  shortjournal = {IET Image Process.},
  title        = {Row-level algorithm to improve real-time performance of glass tube defect detection in the production phase},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Method of multi-region tumour segmentation in brain MRI
images using grid-based segmentation and weighted bee swarm
optimisation. <em>IETIP</em>, <em>14</em>(12), 2901–2910. (<a
href="https://doi.org/10.1049/iet-ipr.2019.1234">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-region segmentation plays a major role in numerous medical diagnostics especially brain tumour identification and classification in Magnetic Resonance Imaging (MRI). Brain tumour segmentation is used in medical field for early diagnostics and detection of tumour. The main goal of this work is to improve the performance of detection by using grid based techniques with Weighted Bee Swarm Intelligence and K-means clustering. This technique is more effective due to hybrid combination of segmentation and optimisation as it seems to possess specific tasks of image information and detection to obtain a detailed and accurate image analysis. Grid based segmentation balance overall computation time and reduces complexity. Weighted Bee Swarm Optimisation is used to optimise segmentation parameters to get maximum performance. The various informative regions such as cerebrospinal fluid, grey matter, white matter are segmented by using proposed algorithm which will be most useful to study and characterise the tumour. The experimental outcomes show that the proposed strategy enhances performance measures in terms of sensitivity and specificity analysis. The performance of this technique is also improved by a factor of 1.5%.},
  archive      = {J_IETIP},
  author       = {Abhisha Mano and Swaminathan Anand},
  doi          = {10.1049/iet-ipr.2019.1234},
  journal      = {IET Image Processing},
  month        = {10},
  number       = {12},
  pages        = {2901-2910},
  shortjournal = {IET Image Process.},
  title        = {Method of multi-region tumour segmentation in brain MRI images using grid-based segmentation and weighted bee swarm optimisation},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Novel approach for automatic mid-diastole frame detection in
2D echocardiography sequences for performing planimetry of the mitral
valve orifice. <em>IETIP</em>, <em>14</em>(12), 2890–2900. (<a
href="https://doi.org/10.1049/iet-ipr.2019.1757">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The mitral valve orifice area is a reliable measure for evaluating mitral valve stenosis (MS) severity, which is obtained by the planimetry of the mid-diastole frame in the echocardiography sequences. Since the manual method for determining this frame is time-consuming and user-dependent, a novel automatic method has been proposed in this study. First, the region of interest (ROI) containing the mitral valve orifice region is detected using circular Hough transform and k-means algorithms. Then, the dimension reduction method is applied to the ROI of each frame to map it into a point in a 2D space. The performance of the local linear embedding (LLE), isometric mapping, kernel principal component analysis (PCA), and linear PCA algorithms has been evaluated in this study. Finally, a distance curve is obtained by calculating the Euclidean distance between consecutive points in 2D space, and the mid-diastole frame is determined by interpreting this curve. The proposed algorithm was validated using 2D echocardiography of the 20 MS patients. Finally, the LLE method showed the best result, and the average frame difference for 20 cases using the proposed method compared with the gold standard (the echo-cardiologist opinion) was 1.40.},
  archive      = {J_IETIP},
  author       = {Mahtab Faraji and Hamid Behnam and Mohammad Norizadeh Cherloo and Maryam Shojaeifard},
  doi          = {10.1049/iet-ipr.2019.1757},
  journal      = {IET Image Processing},
  month        = {10},
  number       = {12},
  pages        = {2890-2900},
  shortjournal = {IET Image Process.},
  title        = {Novel approach for automatic mid-diastole frame detection in 2D echocardiography sequences for performing planimetry of the mitral valve orifice},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Target segmentation of industrial smoke image based on LBP
silhouettes coefficient variant (LBPSCV) algorithm. <em>IETIP</em>,
<em>14</em>(12), 2879–2889. (<a
href="https://doi.org/10.1049/iet-ipr.2019.1315">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The use of computer vision technology to analyse the characteristics of smoke such as Ringelmann blackness coefficient and colour information can directly and efficiently reflect the situation of smoke emissions in industrial production, which has great significance in improving air quality. As many factors stand in the way, including the amount and speed of industrial smoke emissions, natural wind speed, illumination etc., an accurate and complete detection of the targeted smoke in images becomes a difficult issue in this field. In this study, a local binary pattern Silhouettes coefficient variant (LBPSCV) is proposed to segment industrial smoke images. The variant of Silhouettes coefficient was used as the weight when calculating the local binary pattern (LBP) feature vector in the LBPSCV. The algorithm overcame the shortcoming that the texture information described by LBP lacks local contrast information, making the extracted texture features more easily to be distinguished between smoke and non-smoke images. Smoke emission monitoring videos with different characteristics have been used in experiments, such as smoke emission videos with low light, multiple chimney exhaust, multi-colour smoke etc. The results show that the proposed method has higher detection accuracy and a lower false-positive rate.},
  archive      = {J_IETIP},
  author       = {Qingrong Li and Hui Liu and Junpeng Zhang and Pengfei Zeng},
  doi          = {10.1049/iet-ipr.2019.1315},
  journal      = {IET Image Processing},
  month        = {10},
  number       = {12},
  pages        = {2879-2889},
  shortjournal = {IET Image Process.},
  title        = {Target segmentation of industrial smoke image based on LBP silhouettes coefficient variant (LBPSCV) algorithm},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Adaptive switching interpolation filter for restoring
impulse corrupted digital images. <em>IETIP</em>, <em>14</em>(12),
2869–2878. (<a href="https://doi.org/10.1049/iet-ipr.2019.1445">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The study introduces an adaptive switching interpolation filter (ASIF) for restoring images contaminated with salt &amp; pepper impulse noise. The new method restores noisy pixels by applying a new interpolation scheme that combines Shepard&#39;s inverse distance weighting and Simpson&#39;s natural neighbour interpolation techniques. The new interpolation scheme is designed based on the percentage of the overlapping area and the distance between the sites created by noisy pixels with the sites of uncorrupted pixels in the Voronoi tessellation. Since natural neighbour-based techniques do not support extrapolation, the proposed algorithm performs Shepard&#39;s inverse distance weighting-based restoration when the noisy pixels fall outside the convex hull formed by uncorrupted pixels. As the proposed method combines the advantages of Simpson&#39;s natural neighbour and Shepard&#39;s inverse distance weighting interpolation methods, it better preserves the natural intensity variations in images and provides smoother approximation than other methods used in the comparative study. Visual and quantitative experimental analysis performed on various images with peak signal to noise ratio, mean structural similarity index measure, image enhancement factor, and feature-similarity index measure demarcates the improved capability of ASIF over other comparative filters.},
  archive      = {J_IETIP},
  author       = {Justin Varghese and Saudia Subash and Kuttaiyur Palaniswamy Sridhar and Narayanasamy Venkattaramanujam Balaji and Gopalakrishnan Ashok Kumar},
  doi          = {10.1049/iet-ipr.2019.1445},
  journal      = {IET Image Processing},
  month        = {10},
  number       = {12},
  pages        = {2869-2878},
  shortjournal = {IET Image Process.},
  title        = {Adaptive switching interpolation filter for restoring impulse corrupted digital images},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Deep learning algorithm for breast masses classification in
mammograms. <em>IETIP</em>, <em>14</em>(12), 2860–2868. (<a
href="https://doi.org/10.1049/iet-ipr.2020.0070">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A mammogram is an image of a breast used to detect and diagnose breast cancer. This paper emphases a Computer-Aided Detection system based on convolutional neural network (CNN) that uses the concept of deep learning to classify the mammogram images into benign, malignant and normal. The proposed CNN model consists of eight convolutional, four max-pooling and two fully connected layers and achieved better results compared to the pre-trained nets, AlexNet and VGG16. The proposed model demonstrates the feasibility of using CNNs on medical image processing techniques for the classification of breast masses. The results are also compared with the state-of-the-art machine learning algorithm like kNN classifier. Experimentation is done with three datasets. Among them, two are publicly available, Mammographic Image Analysis Society (MIAS), digital database for screening mammography (DDSM) and an internally collected dataset. The proposed model achieved accuracies of 92.54, 96.47 and 95 and the Area under the ROC curve (AUC) score of 0.85, 0.96 and 0.94 for MIAS, DDSM and the internally collected dataset respectively. Furthermore, the images of the three datasets are merged to build one large set and used to fine tune the proposed CNN model and produced accuracy of 98.32 and AUC of 0.98.},
  archive      = {J_IETIP},
  author       = {Vaira Suganthi Gnanasekaran and Sutha Joypaul and Parvathy Meenakshi Sundaram and Durga Devi Chairman},
  doi          = {10.1049/iet-ipr.2020.0070},
  journal      = {IET Image Processing},
  month        = {10},
  number       = {12},
  pages        = {2860-2868},
  shortjournal = {IET Image Process.},
  title        = {Deep learning algorithm for breast masses classification in mammograms},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). SiameseCCR: A novel method for one-shot and few-shot chinese
CAPTCHA recognition using deep siamese network. <em>IETIP</em>,
<em>14</em>(12), 2855–2859. (<a
href="https://doi.org/10.1049/iet-ipr.2019.0618">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The research of CAPTCHA recognition is helpful to discover the security vulnerabilities in time and improve its safety. In comparison with digits and English letters, Chinese characters have many more categories which lead to the requirement of a large amount of training data. Therefore, this study proposes a novel method for one-shot and few-shot Chinese CAPTCHA recognition, using the deep Siamese network, based on the idea of template matching. In this method, the residual convolutional neural network branches are used for feature extraction of CAPTCHAs, a fully-connected layer is used for calculating the similarity of features, and a hard negative mining algorithm is designed to promote convergence. Experiments are done on a self-built small-scale Chinese CAPTCHA dataset. The results show that this proposed method can achieve higher accuracy on the known characters than traditional methods. For the brand-new characters, only one template is required to recognise them and the accuracy is close to known characters. To summarise, it is able to build a Chinese CAPTCHA recognition model with high accuracy and extensibility by using a small-scale dataset.},
  archive      = {J_IETIP},
  author       = {Zhe Chen and Weifeng Ma and Nanfan Xu and Caoting Ji and Yulai Zhang},
  doi          = {10.1049/iet-ipr.2019.0618},
  journal      = {IET Image Processing},
  month        = {10},
  number       = {12},
  pages        = {2855-2859},
  shortjournal = {IET Image Process.},
  title        = {SiameseCCR: A novel method for one-shot and few-shot chinese CAPTCHA recognition using deep siamese network},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Subpixel image registration regularised by and norms.
<em>IETIP</em>, <em>14</em>(12), 2845–2854. (<a
href="https://doi.org/10.1049/iet-ipr.2019.1384">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this study, the authors propose a subpixel image registration framework that detects and matches feature points. Rigid and nonrigid registration models are employed to solve the problem of subpixel image registration problem. A rigid registration model based on the norm is proposed to regularise the rotation coefficients using the indicator function to estimate the rigid transformation parameters. The latter estimation simplified is made easy by the reduction in the rigid transformation from two dimensions to one dimension. Furthermore, a non-rigid registration model based on the and norms is proposed to estimate the elastic coefficients of the compact support radial basis functions. Due to the linear representation of the transformation function, the rigid and nonrigid subpixel image registration models can be solved efficiently using the fast iterative shrinkage-thresholding algorithm. Experiments on a demosaicing data set, the ocean of remote sensing data set, a brain data set and the fundus image registration data set show that the proposed rigid and non-rigid registration models can accurately perform subpixel image registration.},
  archive      = {J_IETIP},
  author       = {Qirui Huang and Xuan Yang},
  doi          = {10.1049/iet-ipr.2019.1384},
  journal      = {IET Image Processing},
  month        = {10},
  number       = {12},
  pages        = {2845-2854},
  shortjournal = {IET Image Process.},
  title        = {Subpixel image registration regularised by and norms},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Fast multi-spectral image super-resolution via sparse
representation. <em>IETIP</em>, <em>14</em>(12), 2833–2844. (<a
href="https://doi.org/10.1049/iet-ipr.2019.0714">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sparse reconstruction is used to solve the inverse problem of single image super-resolution (SR) as a patch-based sparsity promoting regularisation problem. A coupled trained overcomplete dictionary from high-resolution (HR) and low-resolution (LR) image patches containing significant features is proposed using sparse representation to produce HR patches from their LR counterparts. In this study, the authors develop a multi-core single image SR technique for LR multi-spectral images based on patch-wise sparse representation coupled with morphological component analysis driven feature extraction. Simulations are carried out to evaluate the proposed method using real remote sensing images of a few Indian satellites, RESOURCESAT-2 and CARTOSAT-2, as well as other satellites, such as QuickBird etc. Results are also compared with other existing SR methods to establish the superiority of the proposed method in terms of both objective metrics and visual analysis.},
  archive      = {J_IETIP},
  author       = {Helal Uddin Mullah and Bhabesh Deka and A.V.V. Prasad},
  doi          = {10.1049/iet-ipr.2019.0714},
  journal      = {IET Image Processing},
  month        = {10},
  number       = {12},
  pages        = {2833-2844},
  shortjournal = {IET Image Process.},
  title        = {Fast multi-spectral image super-resolution via sparse representation},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Quantitative analysis of cell morphology based on the
contourlet transform. <em>IETIP</em>, <em>14</em>(12), 2826–2832. (<a
href="https://doi.org/10.1049/iet-ipr.2019.0909">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cellular morphology analysis has been widely used to detect abnormalities in biological processes. Clinicians have observed that lymphocytes become highly deformable under special conditions, particularly when graft rejection occurs. The characterisation of lymphocyte boundary deformation provides important quantitative parameters to assist clinical rejection diagnosis. To evaluate the dynamic features of the boundaries of target lymphocyte when a graft rejection occurs, a contourlet transform-based method is proposed to extract the characteristics of cell boundary variation. First, the lymphocyte is segmented and tracked to obtain their edge-to-centroid distance signals. Subsequently, a contourlet transform is performed on these signals, during which the edge-to-centroid distance signals of the lymphocyte is decomposed at multiple scales using the Laplacian pyramid; a multi-directional decomposition is then performed using a direction filter to merge the singularities distributed along the same direction and obtain the contourlet transform coefficients. Finally, statistical parameters of the cell dynamic boundaries are calculated, then fed into the support vector machine for classification of the cell deformation. Our findings demonstrate that contourlet transform has better performance in representing image features such as cell boundaries than wavelet transform for its prosperities of multi-scale and multi-directional decomposition for cell images.},
  archive      = {J_IETIP},
  author       = {Yali Huang and Xuefang Hu and Lei Hao and Yuehua Gao and Zhiwen Liu and Peiguang Wang},
  doi          = {10.1049/iet-ipr.2019.0909},
  journal      = {IET Image Processing},
  month        = {10},
  number       = {12},
  pages        = {2826-2832},
  shortjournal = {IET Image Process.},
  title        = {Quantitative analysis of cell morphology based on the contourlet transform},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Underwater sonar image classification using generative
adversarial network and convolutional neural network. <em>IETIP</em>,
<em>14</em>(12), 2819–2825. (<a
href="https://doi.org/10.1049/iet-ipr.2019.1735">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study presents a generative adversarial network (GAN) called conditional Wasserstein GAN-gradient penalty (CWGAN-GP)&amp;DenseNet and ResNet, and a convolutional neural network (CNN) called improved CNN to complete underwater sonar image classification. Specifically, to solve the problem of insufficient underwater sonar image data, the CWGAN-GP&amp;DR is developed to expand underwater sonar image data set. Besides, to improve the analysis and utilisation of the feature map and reduce the misclassification rate of categories with similar probabilities, improved CNN is proposed to complete the final underwater sonar image classification. Finally, compared with other methods, the CWGAN-GP&amp;DR generate better underwater sonar images and effectively expand the underwater sonar image data set. Moreover, compared with the original data set and other expanded data set, the highest accuracy rate of 85.00% can be obtained on the CWGAN-GP&amp;DR expanded data set by CNN. Furthermore, CNN, CNN-bais and improved CNN are used to perform classification experiments on each data set, and the accuracy of the improved CNN is the highest on all data sets and reached the highest accuracy of 87.71% on CWGAN-GP&amp;DR expanded data set. The experimental results demonstrate that the proposed method can effectively improve the performance of underwater sonar image classification.},
  archive      = {J_IETIP},
  author       = {Yichao Xu and Xingmei Wang and Kunhua Wang and Jiahao Shi and Wei Sun},
  doi          = {10.1049/iet-ipr.2019.1735},
  journal      = {IET Image Processing},
  month        = {10},
  number       = {12},
  pages        = {2819-2825},
  shortjournal = {IET Image Process.},
  title        = {Underwater sonar image classification using generative adversarial network and convolutional neural network},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Classification of magnetic resonance images for brain tumour
detection. <em>IETIP</em>, <em>14</em>(12), 2808–2818. (<a
href="https://doi.org/10.1049/iet-ipr.2019.1631">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image segmentation of magnetic resonance image (MRI) is a crucial process for visualisation and examination of abnormal tissues, especially during clinical analysis. Complexity and variations of the tumour structure magnify the challenges in the automated detection of a brain tumour in MRIs. This study presents an automatic lesion recognition method in the MRI followed by classification. In the proposed multistage image segmentation method, the intent region initialisation is performed using low-level information by the keypoint descriptors. A set of the linear filter is used to transform low-level information into higher-level image features. The set of features and filter training data are accomplished to track the tumour region. The authors adopt a possibilistic model for region growing, and disparity map for the refinement process to grave consist boundary. Further, the features are extracted using the Fisher vector and autoencoder. A set of handcrafted features is also extracted using a segmentation-based localised region to train and test the support vector machine and multilayer perceptron classifiers. The experiments that are performed using five MRI datasets confirm the superiority of proposal as that of the state-of-the-art methods. It reports 94.5 and 91.76%, average accuracy of segmentation and classification, respectively.},
  archive      = {J_IETIP},
  author       = {Yashwant Kurmi and Vijayshri Chaurasia},
  doi          = {10.1049/iet-ipr.2019.1631},
  journal      = {IET Image Processing},
  month        = {10},
  number       = {12},
  pages        = {2808-2818},
  shortjournal = {IET Image Process.},
  title        = {Classification of magnetic resonance images for brain tumour detection},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Image copy-move forgery detection using sparse recovery and
keypoint matching. <em>IETIP</em>, <em>14</em>(12), 2799–2807. (<a
href="https://doi.org/10.1049/iet-ipr.2018.6246">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Copy-move forgery (CMF) detection is one of the most practical problems in image forensics. The authenticity of the image becomes more crucial when the images are used in the criminal investigations, intelligence services, and medical documentations. In this study, the authors suggest a CMF detection algorithm. At first they they suggest to use a sparse recovery algorithm to identify the suspicious segments. To incorporate the colour information of the image segments, they propose to compare the histograms of the identified segments to detect the similar ones. The keypoints of those parts are obtained and the matched ones are located. In the last step, they suggest a morphology scheme to extract the forged region. They have evaluated the proposed method in the detection of various forged images. The simulation results reveal the forgery detection capabilities of the suggested algorithm compared to the other state-of-the-art schemes. The proposed method has superiority over its counterparts in detecting the scaled forgeries. Moreover, the sparse recovery step enables the proposed algorithm to remove the genuine repeated patterns of the image, while the other CMF detection techniques wrongly consider those parts as a forgery. Furthermore, the proposed scheme is on-average faster than the other schemes.},
  archive      = {J_IETIP},
  author       = {Somayeh Fatan Hajialilu and Masoumeh Azghani and Neda Kazemi},
  doi          = {10.1049/iet-ipr.2018.6246},
  journal      = {IET Image Processing},
  month        = {10},
  number       = {12},
  pages        = {2799-2807},
  shortjournal = {IET Image Process.},
  title        = {Image copy-move forgery detection using sparse recovery and keypoint matching},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Improved MR image denoising via low- rank approximation and
laplacian-of-gaussian edge detector. <em>IETIP</em>, <em>14</em>(12),
2791–2798. (<a href="https://doi.org/10.1049/iet-ipr.2019.1648">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The low rank approximation for MR image denoising has been recently attracting considerable attentions due to its favorable denoising performance. In spite of the great success of existing low rank approximation methods, these tend to lose the subtle edge texture when removing noise. It could degrade the image visual quality and affect the final clinical diagnosis. In this paper, a novel MR image denoising approach is proposed based on low rank approximation model and the Laplacian-of-Gaussian edge detector. In the proposed approach, a similarity evaluation scheme for noisy patch is employed to avoid the effect of the noise in the patch matching, and the details of the edge texture are preserved by the Laplacian-of-Gaussian edge detector. Experimental results show that the proposed approach is efficient and superior to some of the existing approaches in both objective criterion and visual fidelity. The proposed method can retrieve a clear MR image from the noisy one, with the detail of the edge texture, which could be very important in the clinical diagnosis.},
  archive      = {J_IETIP},
  author       = {Xiaoqun Qiu and Zhen Chen and Saifullah Adnan and Hongwei He},
  doi          = {10.1049/iet-ipr.2019.1648},
  journal      = {IET Image Processing},
  month        = {10},
  number       = {12},
  pages        = {2791-2798},
  shortjournal = {IET Image Process.},
  title        = {Improved MR image denoising via low- rank approximation and laplacian-of-gaussian edge detector},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Multi-metric domain adaptation for unsupervised transfer
learning. <em>IETIP</em>, <em>14</em>(12), 2780–2790. (<a
href="https://doi.org/10.1049/iet-ipr.2019.1434">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unsupervised domain adaptation aims to learn a classifier for the unlabelled target domain by leveraging knowledge from a labelled source domain. This study presents a novel domain adaptation framework from global and local transfer perspectives, referred to as multi-metric domain adaptation (MMDA) for unsupervised transfer learning. At the global level, MMDA minimises the marginal and within-class distances and maximises the between-class distance between domains while maintaining the features of the source domain to improve the cross-domain adaptability. At the local level, MMDA exploits both in- and cross-domain manifold structures embedded in data samples to increase the discriminative ability. The authors learn a coupled transformation that projects the source and target domain data onto respective subspace where the statistical and geometrical divergences are reduced simultaneously. They formulate global and local adaptation methods in an optimisation problem and derive an analytic solution to the objective function. Extensive experiments demonstrate that MMDA shows improvements in classification accuracy compared with several existing state-of-the-art methods.},
  archive      = {J_IETIP},
  author       = {Hongwei Yang and Hui He and Tao Li and Yawen Bai and Weizhe Zhang},
  doi          = {10.1049/iet-ipr.2019.1434},
  journal      = {IET Image Processing},
  month        = {10},
  number       = {12},
  pages        = {2780-2790},
  shortjournal = {IET Image Process.},
  title        = {Multi-metric domain adaptation for unsupervised transfer learning},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Adaptive colour-guided non-local means algorithm for
compound noise reduction of depth maps. <em>IETIP</em>, <em>14</em>(12),
2768–2779. (<a href="https://doi.org/10.1049/iet-ipr.2019.0074">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Depth maps are used to describe object positioning information in three-dimensional (3D) space, and they are crucial for RGB-D data representation, which is useful for numerous interactive visual applications. In practice, depth maps are often contaminated by compound noise, including intrinsic noise and missing regions owing to active illumination shadows. As existing noise models cannot describe the above-mentioned compound noise effectively, the subsequent filter design is a challenging task. In this study, an adaptive colour-guided non-local mean (NLM) filter is proposed to address such compound noise. First, the authors classify the depth map into hole and non-hole pixels. Then, the proposed filter is designed on the basis of the NLM framework, where the colour image is used as a guide prior for hole-artifact removal. Finally, the authors use a shock filter to effectively address the non-regularisation of the restored depth map edges and remove the remaining noise. Experiments show that the proposed filter qualitatively and quantitatively outperforms existing colour-guided and unguided filters. Moreover, the authors verify the superiority of the proposed filter through virtual view synthesis and 3D scene reconstruction applications.},
  archive      = {J_IETIP},
  author       = {Mostafa M. Ibrahim and Qiong Liu and You Yang},
  doi          = {10.1049/iet-ipr.2019.0074},
  journal      = {IET Image Processing},
  month        = {10},
  number       = {12},
  pages        = {2768-2779},
  shortjournal = {IET Image Process.},
  title        = {Adaptive colour-guided non-local means algorithm for compound noise reduction of depth maps},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Simultaneous filter tuning and calibration of the camera and
inertial measurement unit camera for a vision inertial navigation
system. <em>IETIP</em>, <em>14</em>(12), 2756–2767. (<a
href="https://doi.org/10.1049/iet-ipr.2019.0007">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this study, a novel approach based on multi-objective genetic algorithm (MO-GA) is used for simultaneous tuning the unscented Kalman Filter (UKF) parameters and camera and inertial measurement unit camera calibration in a vision inertial navigation system (VINS). This system consists of visual odometry and inertial navigation system (INS) which integrates with a UKF. In order to obtain simultaneous tuning and calibration of the parameters and variables, the MO-GA minimises the root mean square error of the position and velocity of the vehicle on a selected trajectory of the benchmark data set. Then, the tuned parameters and calibrated variables are placed in the VINS and an adjusted VINS (AVINS) is obtained. For investigating the AVINS, the mentioned system is compared with INS only, VINS based on calibration data of the benchmark data set, and GPS/INS as Real Data on the identical trajectory. Furthermore, in order to evaluate the results of the proposed approach, the AVINS is examined in the second trajectory. The results indicate the proper performance of the presented approach in the simultaneous tuning the filter parameters and calibrating the variables of sensors that are used in the uncalibrated VINS.},
  archive      = {J_IETIP},
  author       = {Mohammadvali Arbabmir and Masoud Ebrahimi},
  doi          = {10.1049/iet-ipr.2019.0007},
  journal      = {IET Image Processing},
  month        = {10},
  number       = {12},
  pages        = {2756-2767},
  shortjournal = {IET Image Process.},
  title        = {Simultaneous filter tuning and calibration of the camera and inertial measurement unit camera for a vision inertial navigation system},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). 3D reconstruction of spine image from 2D MRI slices along
one axis. <em>IETIP</em>, <em>14</em>(12), 2746–2755. (<a
href="https://doi.org/10.1049/iet-ipr.2019.0800">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Magnetic resonance imaging (MRI) is a very effective method for identifying any abnormality in the structure and physiology of the spine. However, MRI is time consuming as well as costly. In this work, the authors propose an algorithm which can reduce the time of MRI and thus the cost, with minimal compromise on accuracy. They reconstruct a three-dimensional (3D) image of the spine from a sequence of 2D MRI slices along any one axis with reasonable slice gap. In order to preserve the image at the edges properly, they regenerate the 3D image by using a combination of bicubic and bilinear interpolation along the orthogonal axis. From the reconstructed 3D, they use a simple geometric method to slice out any possible location along any axis and get the information in that region. They have tested their algorithm on real data, and found that their algorithm reduces the time by 80%, with high internal data preservation accuracy of about 96%.},
  archive      = {J_IETIP},
  author       = {Somoballi Ghoshal and Sourav Banu and Amlan Chakrabarti and Susmita Sur-Kolay and Alok Pandit},
  doi          = {10.1049/iet-ipr.2019.0800},
  journal      = {IET Image Processing},
  month        = {10},
  number       = {12},
  pages        = {2746-2755},
  shortjournal = {IET Image Process.},
  title        = {3D reconstruction of spine image from 2D MRI slices along one axis},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Bayesian multilevel random-effects model for estimating
noise in image sensors. <em>IETIP</em>, <em>14</em>(12), 2737–2745. (<a
href="https://doi.org/10.1049/iet-ipr.2018.5926">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sensor noise sources cause differences in the signal recorded across pixels in a single image and across multiple images. This study presents a Bayesian approach to decomposing and characterising the sensor noise sources involved in imaging with digital cameras. A Bayesian probabilistic model based on the (theoretical) model for noise sources in image sensing is fitted to a set of a time-series of images with different reflectance and wavelengths under controlled lighting conditions. The image sensing model is a complex model, with several interacting components dependent on reflectance and wavelength. The properties of the Bayesian approach of defining conditional dependencies among parameters in a fully probabilistic model, propagating all sources of uncertainty in inference, makes the Bayesian modelling framework more attractive and powerful than classical methods for approaching the image sensing model. A feasible correspondence of noise parameters to their expected theoretical behaviours and well-calibrated posterior predictive distributions with a small root mean square error for model predictions have been achieved in this study, thus showing that the proposed model accurately approximates the image sensing model. The Bayesian approach could be extended to formulate further components aimed at identifying even more specific parameters of the imaging process.},
  archive      = {J_IETIP},
  author       = {Gabriel Riutort-Mayol and Virgilio Gómez-Rubio and Ángel Marqués-Mateu and José Luis Lerma and Antonio López-Quílez},
  doi          = {10.1049/iet-ipr.2018.5926},
  journal      = {IET Image Processing},
  month        = {10},
  number       = {12},
  pages        = {2737-2745},
  shortjournal = {IET Image Process.},
  title        = {Bayesian multilevel random-effects model for estimating noise in image sensors},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Automatic labelling of brain tissues in MR images through
spatial indexes based hybrid atlas forest. <em>IETIP</em>,
<em>14</em>(12), 2728–2736. (<a
href="https://doi.org/10.1049/iet-ipr.2018.6073">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The multi-atlas-based methods are widely applied in the automatic labelling in magnetic resonance (MR) images. However, most multi-atlas-based methods require that all atlases be registered to the target image accurately to have a correct label propagation. In this study, the authors introduce the term spatial indexes and construct a hybrid atlas forest model to gather the labelling information from all atlases without propagating labels from every single atlas. Furthermore, a new automatic labelling method using the hybrid atlas forest model based on spatial indexes is proposed. In the proposed framework, an atlas is chosen arbitrarily as a reference image and the spatial indexes are constructed on this image space. Then, the samples are selected from all atlases in the dataset based on the spatial indexes to construct a samples pool. Finally, the hybrid atlas forest model will be trained on the samples pool and used to predict the labelling of the target. Experiments are conducted on two public datasets to evaluate the effectiveness of the proposed method. The experimental results show that the proposed method reduces the requirement of strong dependence on precise registration and improve the accuracy of labelling.},
  archive      = {J_IETIP},
  author       = {Hong Liu and Lijun Xu and Enmin Song and Renchao Jin and Chih-Cheng Hung},
  doi          = {10.1049/iet-ipr.2018.6073},
  journal      = {IET Image Processing},
  month        = {10},
  number       = {12},
  pages        = {2728-2736},
  shortjournal = {IET Image Process.},
  title        = {Automatic labelling of brain tissues in MR images through spatial indexes based hybrid atlas forest},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). MapReduce framework based big data clustering using
fractional integrated sparse fuzzy c means algorithm. <em>IETIP</em>,
<em>14</em>(12), 2719–2727. (<a
href="https://doi.org/10.1049/iet-ipr.2019.0899">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Big data analytics gain significant interest over the traditional data-processing methodologies that engage in extracting the hidden patterns and correlations from the massive data, termed as big data. With the aim of relieving the computational complexity the clustering method plays a significant role. With the knowledge of the clustering algorithms, the big data arriving from the distributed sources is processed using the MapReduce framework (MRF). The MRF possesses two functions, namely, map function and reduce function, such that the map function is based on the proposed Fractional Sparse Fuzzy C-Means (FrSparse FCM) algorithm and reduce function is based on particle swarm optimisation-based whale optimisation algorithm (P-Whale). Initially, the optimal centroids are computed using the proposed algorithm in the mapper phase that is optimally tuned in the reducer phase, and it is clear that the proposed FrSparse FCM-based MRF ensures the parallel processing of the big data. Experimentation is performed using the Skin data set and the localisation data set taken from the UCI machine learning repository, and the analysis is progressed using the metrics, such as accuracy and DB Index. The analysis proves that the proposed method acquired a maximum accuracy of 90.6012% and a minimum DB Index of 5.33.},
  archive      = {J_IETIP},
  author       = {Omkaresh Kulkarni and Sudarson Jena and V. Ravi Sankar},
  doi          = {10.1049/iet-ipr.2019.0899},
  journal      = {IET Image Processing},
  month        = {10},
  number       = {12},
  pages        = {2719-2727},
  shortjournal = {IET Image Process.},
  title        = {MapReduce framework based big data clustering using fractional integrated sparse fuzzy c means algorithm},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Video compression based on sphere-rotated frame prediction.
<em>IETIP</em>, <em>14</em>(12), 2711–2718. (<a
href="https://doi.org/10.1049/iet-ipr.2019.1663">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {360 video is very popular due to its 360 views of a scene. Although 360 videos are also compressed by a hybrid coding framework like 2D video, its high resolution and serious shape deformation affect coding efficiency. In equirectangular projection (ERP) format of 360 videos, if an object moves from equator regions to pole regions or vice versa, large deformation will be introduced and motion estimation cannot find the best-matched part. To solve the above problem, the authors propose to generate a better reference frame for the current to be encoded frame. First, they project the frame prior to the current one from ERP to the sphere and rotate it at an appropriate angle depending on motion vectors. Subsequently, they insert this generated frame to the rear of the reference queue and let the encoder work as usual. The advantage is that the inserted frame has a more similar shape deformation as the current frame, which greatly helps motion estimation and makes full use of 360 video characters. Their method is simple and friendly compatible with the existing compression standard. Experiments prove that their method achieves 1.57% Bjøntegaard Delta (BD)-gain compared with standard high efficiency video coding.},
  archive      = {J_IETIP},
  author       = {Ning Yu and Chunyu Lin and Yao Zhao and Meiqin Liu and Xue Zhang},
  doi          = {10.1049/iet-ipr.2019.1663},
  journal      = {IET Image Processing},
  month        = {10},
  number       = {12},
  pages        = {2711-2718},
  shortjournal = {IET Image Process.},
  title        = {Video compression based on sphere-rotated frame prediction},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Method for automatic railway track surface defect
classification and evaluation using a laser-based 3D model.
<em>IETIP</em>, <em>14</em>(12), 2701–2710. (<a
href="https://doi.org/10.1049/iet-ipr.2019.1616">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Inspection of physical surface defects is a significant concern in many industrial areas. In railway systems, this process mainly includes the detection and classification of defects in rails and wheels, for which laser-based optical inspection technologies have gradually been applied in the form of 2D profile measurement, benefiting from its high precision and robustness to surface conditions. However, defect classification and evaluation after the initial detection works still rely heavily on human inspectors to make maintenance suggestions. The linear nature of rails makes it possible to increase the dimension of rail measurement data from 2D to 3D by aligning 2D profiles along the rail, from which more comprehensive diagnosis information becomes available. In combination with appropriate artificial intelligence algorithms, this approach can potentially replace human-dominated defect classification and evaluation work. This study presents a 3D model-based railway track surface defect classification and evaluation method. A set of geometrical features are extracted from the 3D model of track surface defects to describe a distinguishable pattern for each category of defect. Multi-class classifiers are then tested and have shown promising results on a group of artificial track surface defects, giving a systemic solution for 3D model-based automatic track surface defect inspection.},
  archive      = {J_IETIP},
  author       = {Jiaqi Ye and Edward Stewart and Dingcheng Zhang and Qianyu Chen and Clive Roberts},
  doi          = {10.1049/iet-ipr.2019.1616},
  journal      = {IET Image Processing},
  month        = {10},
  number       = {12},
  pages        = {2701-2710},
  shortjournal = {IET Image Process.},
  title        = {Method for automatic railway track surface defect classification and evaluation using a laser-based 3D model},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Coarse-to-fine 3D road model registration for traffic video
augmentation. <em>IETIP</em>, <em>14</em>(12), 2690–2700. (<a
href="https://doi.org/10.1049/iet-ipr.2019.1036">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study addresses the problem of non-perspective pose estimation from line correspondences in the traffic scenarios. A coarse-to-fine 3D road registration method is proposed for this problem in two stages. Firstly, the iterative closest point algorithm is exploited to estimate the pose coarsely. An objective function is then established to incorporate the feature correspondences for refining the coarse pose. Besides, the framework including road registration is employed for traffic video augmentation. The framework begins with the inputs of traffic videos, road information from Geographic Information Systems and 3D models of traffic elements (e.g. vehicles, pedestrians). Subsequently, 3D road model generation and point-to-line correspondence establishment are achieved in the preprossessing stage. After road and viewpoint registration, the 3D graphic engine is employed to simulate the traffic scene with the road, viewpoints and traffic elements. The augmented videos are generated by fusing the original frames and newly projected traffic elements. The authors demonstrate the superiority of the proposed registration method by the comparison to state-of-the-arts in both quantitative and qualitative experiments. In addition, the frames of the augmented videos validate the proposed method in the application.},
  archive      = {J_IETIP},
  author       = {Zhichao Cui and Yaochen Li and Chi Zhang and Yuehu Liu and Fuji Ren},
  doi          = {10.1049/iet-ipr.2019.1036},
  journal      = {IET Image Processing},
  month        = {10},
  number       = {12},
  pages        = {2690-2700},
  shortjournal = {IET Image Process.},
  title        = {Coarse-to-fine 3D road model registration for traffic video augmentation},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). DenseUNet: Densely connected UNet for electron microscopy
image segmentation. <em>IETIP</em>, <em>14</em>(12), 2682–2689. (<a
href="https://doi.org/10.1049/iet-ipr.2019.1527">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Electron microscopy (EM) image segmentation plays an important role in computer-aided diagnosis of specific pathogens or disease. However, EM image segmentation is a laborious task and needs to impose experts knowledge, which can take up valuable time from research. Convolutional neural network (CNN)-based methods have been proposed for EM image segmentation and achieved considerable progress. Among those CNN-based methods, UNet is regarded as the state-of-the-art method. However, the UNet usually has millions of parameters to increase training difficulty and is limited by the issue of vanishing gradients. To address those problems, the authors present a novel highly parameter efficient method called DenseUNet, which is inspired by the approach that takes particular advantage of recent advances in both UNet and DenseNet. In addition, they successfully apply the weighted loss, which enables us to boost the performance of segmentation. They conduct several comparative experiments on the ISBI 2012 EM dataset. The experimental results show that their method can achieve state-of-the-art results on EM image segmentation without any further post-processing module or pre-training. Moreover, due to smart design of the model, their approach has much less parameters than currently published encoder–decoder architecture variants for this dataset.},
  archive      = {J_IETIP},
  author       = {Yue Cao and Shigang Liu and Yali Peng and Jun Li},
  doi          = {10.1049/iet-ipr.2019.1527},
  journal      = {IET Image Processing},
  month        = {10},
  number       = {12},
  pages        = {2682-2689},
  shortjournal = {IET Image Process.},
  title        = {DenseUNet: Densely connected UNet for electron microscopy image segmentation},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Individual retrieval based on oral cavity point cloud data
and correntropy-based registration algorithm. <em>IETIP</em>,
<em>14</em>(12), 2675–2681. (<a
href="https://doi.org/10.1049/iet-ipr.2019.1420">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this study, the authors present a novel individual retrieval method based on oral cavity point cloud data and correntropy-based registration algorithm. Since the three-dimensional oral cavity data contains a large amount of noise and outliers, it may lead to a decrease in registration accuracy, which affects the accuracy of retrieval rate. Therefore, the authors introduce the correntropy into the rigid registration algorithm to solve this problem. Then, they filter the matched point cloud data and then use the mean squared error to judge the individual differences of the model data. Finally, the accurate retrieval of the oral cavity data is realised. Experimental results demonstrate the proposed retrieval three-dimensional model algorithm can be successfully searched under different model data, which can help forensics use the characteristics of biological individuals to accurately search and identify, and improve recognition efficiency.},
  archive      = {J_IETIP},
  author       = {Wenting Cui and Jianyi Liu and Shaoyi Du and Yuying Liu and Teng Wan and Mengqi Han and Qingnan Mou and Jing Yang and Yu-cheng Guo},
  doi          = {10.1049/iet-ipr.2019.1420},
  journal      = {IET Image Processing},
  month        = {10},
  number       = {12},
  pages        = {2675-2681},
  shortjournal = {IET Image Process.},
  title        = {Individual retrieval based on oral cavity point cloud data and correntropy-based registration algorithm},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). New multi-view human motion capture framework.
<em>IETIP</em>, <em>14</em>(12), 2668–2674. (<a
href="https://doi.org/10.1049/iet-ipr.2019.1606">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Estimating human pose and shape without markers is a challenging problem. This study proposes a multiple-view markerless human motion capture framework. Firstly, a multi-view camera system is built for capturing real-time images of moving humans on multiple views. Secondly, by employing the OpenPose method, the authors calculate robust 3D key points from 2D key points of the human body, which are estimated from the multi-view images. And dense 3D point cloud is reconstructed from images. Thirdly, they propose a novel SMPL-based method to represent human motion by fitting the SMPL model to 3D key points and 3D point clouds. In order to achieve a more accurate human pose, a penalty term is utilised to solve the problem of error accumulation in the process of human motion capture. In addition, they present a dense mesh template-based SMPL that can be deformed to point cloud to recover a real human body shape. Finally, they map multi-view colour images onto the human mesh model to acquire rendered mesh. The experimental results show that the proposed method improves the accuracy of human pose and realises the 3D human body model more realistic.},
  archive      = {J_IETIP},
  author       = {Yuan Wang and Feiyi Xu and Chi-Man Pun and Wenqi Xiao and Jianhui Nie and Jian Xiong and Hao Gao and Feng Xu},
  doi          = {10.1049/iet-ipr.2019.1606},
  journal      = {IET Image Processing},
  month        = {10},
  number       = {12},
  pages        = {2668-2674},
  shortjournal = {IET Image Process.},
  title        = {New multi-view human motion capture framework},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Energy minimisation-based multi-class multi-instance
geometric primitives extraction from 3D point clouds. <em>IETIP</em>,
<em>14</em>(12), 2660–2667. (<a
href="https://doi.org/10.1049/iet-ipr.2019.1625">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Geometric primitives contained in three-dimensional (3D) point clouds can provide the meaningful and concise abstraction of 3D data, which plays a vital role in improving 3D vision-based intelligent applications. However, how to efficiently and robustly extract multiple geometric primitives from point clouds is still a challenge, especially when multiple instances of multiple classes of geometric primitives are present. In this study, a novel energy minimisation-based algorithm for multi-class multi-instance geometric primitives extraction from the 3D point cloud is proposed. First, an improved sampling strategy is proposed to generate model hypotheses. Then, an improved strategy to establish the neighbourhood is proposed to help construct and optimise an energy function for points labelling. After that, hypotheses and parameters of models are refined. Iterate this process until the energy does not decrease. Finally, models of multi-class multi-instance geometric primitives are simultaneously and robustly extracted from the 3D point cloud. In comparison with the state-of-the-art methods, it can automatically determine the classes and numbers of geometric primitives in the 3D point cloud. Experimental results with synthetic and real data validate the proposed algorithm.},
  archive      = {J_IETIP},
  author       = {Liang Wang and Biying Yan and Fuqing Duan and Ke Lu},
  doi          = {10.1049/iet-ipr.2019.1625},
  journal      = {IET Image Processing},
  month        = {10},
  number       = {12},
  pages        = {2660-2667},
  shortjournal = {IET Image Process.},
  title        = {Energy minimisation-based multi-class multi-instance geometric primitives extraction from 3D point clouds},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020a). Geometry and context guided refinement for stereo matching.
<em>IETIP</em>, <em>14</em>(12), 2652–2659. (<a
href="https://doi.org/10.1049/iet-ipr.2019.1636">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The disparity refinement phase of existing end-to-end stereo matching networks refines the disparity by learning the mapping from the concatenated coarse disparity and corresponding features to fine disparity. It depends on the scenarios&#39; characteristics, such as the distribution of disparity and semantic categories contained in the domain, which makes the network fail to work on unseen domain. In this paper, we propose a geometry and context guided refinement network (GCGR-Net) containing a Fine Matching module and an Upsampling module. GCGR-Net learns to utilize pixels&#39; relationship to get high resolution dense disparity, which is independent of the data&#39;s content. The Fine Matching module performs a minimum range search based on the relationship between the possible matching pixel pairs, i.e. the called geometry information, to recover the internal structure of the object. The Upsampling module obtains context information, the relationship between central pixel and the pixels in its neighborhood, to upsample the lower resolution disparity. The final disparity map is obtained step by step through an iterative refinement model. Experiment results show that our method not only has good performance in the training scenarios, but also outperforms previous methods on the unseen domain without fine-tuning.},
  archive      = {J_IETIP},
  author       = {Hong Zhang and Haojie Li and Zhihui Wang and Yuxin Yue and Shenglun Chen},
  doi          = {10.1049/iet-ipr.2019.1636},
  journal      = {IET Image Processing},
  month        = {10},
  number       = {12},
  pages        = {2652-2659},
  shortjournal = {IET Image Process.},
  title        = {Geometry and context guided refinement for stereo matching},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Absolute phase unwrapping with SVM for fringe-projection
profilometry. <em>IETIP</em>, <em>14</em>(12), 2645–2651. (<a
href="https://doi.org/10.1049/iet-ipr.2019.1611">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Phase unwrapping is a fundamental task in phase-based profilometry. Existing spatial and temporal approaches are facing challenges such as error propagation and low efficiency. In this study, the authors propose a learning-based method that uses a support vector machine (SVM) to perform phase unwrapping, where the problem is solved as a classification task. To be specific, seven elements, extracted from the captured patterns and the wrapped phase, form the input feature vector and the fringe order is the output class. Besides, a radial basis function kernel SVM is adopted as the model. The proposed method is conducted independently for every pixel, and does not suffer from error propagation in the spatial unwrapping. Moreover, it needs fewer patterns than temporal unwrapping since only one phase map is required. Simulation and experimental results demonstrate that the proposed scheme produces precise depth maps, which are comparable with the complex quality-guided methods but at a much faster speed.},
  archive      = {J_IETIP},
  author       = {Sen Xiang and Huiping Deng and Jin Wu and Changjian Zhu},
  doi          = {10.1049/iet-ipr.2019.1611},
  journal      = {IET Image Processing},
  month        = {10},
  number       = {12},
  pages        = {2645-2651},
  shortjournal = {IET Image Process.},
  title        = {Absolute phase unwrapping with SVM for fringe-projection profilometry},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Depth map artefacts reduction: A review. <em>IETIP</em>,
<em>14</em>(12), 2630–2644. (<a
href="https://doi.org/10.1049/iet-ipr.2019.1622">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Depth maps are crucial for many visual applications, where they represent the positioning information of the objects in a three-dimensional scene. Usually, depth maps can be acquired via various devices, including Time of Flight, Kinect or light field camera, in practical applications. However, a brutal truth is that both intrinsic and extrinsic artefacts can be found in these depth maps which limits the prosperity of three-dimensional visual applications. In this study, the authors survey the depth map artefacts reduction methods proposed in the literature, from mono- to multi-view, via spatial to temporal dimension, in local to global manner, with signal processing to learning-based methods. They also compare the state-of-the-arts via different metrics to show their potentials in future visual applications.},
  archive      = {J_IETIP},
  author       = {Mostafa Mahmoud Ibrahim and Qiong Liu and Rizwan Khan and Jingyu Yang and Ehsan Adeli and You Yang},
  doi          = {10.1049/iet-ipr.2019.1622},
  journal      = {IET Image Processing},
  month        = {10},
  number       = {12},
  pages        = {2630-2644},
  shortjournal = {IET Image Process.},
  title        = {Depth map artefacts reduction: A review},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Unsupervised multiscale retinal blood vessel segmentation
using fundus images. <em>IETIP</em>, <em>14</em>(11), 2616–2625. (<a
href="https://doi.org/10.1049/iet-ipr.2019.0969">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Blood vessel segmentation is a vital step in automated diagnosis of retinal diseases. Some retinal diseases progress with structural changes in the vessels whereas in others, vessels may remain unaffected. Segmentation of vessels is inevitable in both the cases. The extracted vessel map can be studied for these structural changes or can be removed to highlight other abnormalities of the retina. This study presents a rule-based retinal blood vessel segmentation algorithm. It implements two multi-scale approaches, local directional-wavelet transform and global curvelet transform, together in a novel manner for vessel enhancement and thereby segmentation. The authors have proposed a generic field-of-view mask for extraction of region-of-interest. Further, a morphological thickness-correction step, to recover vessel-boundary pixels, is also proposed. The significant contribution of this work is, segmentation of fine vessels while preserving the thickness of major vessels. Moreover, the algorithm is robust, as it performs consistently well, on four public databases, DRIVE, STARE, CHASE_DB-1 and HRF. Performance of the proposed algorithm is evaluated in terms of eight measures : accuracy, sensitivity, specificity, precision, F-1 score, G-mean, MCC and AUC, where it has outperformed many other existing methods. Zero data dependency gives the suggested algorithm, an edge over other state-of-the-art supervised methods.},
  archive      = {J_IETIP},
  author       = {Kamini Upadhyay and Monika Agrawal and Praveen Vashist},
  doi          = {10.1049/iet-ipr.2019.0969},
  journal      = {IET Image Processing},
  month        = {9},
  number       = {11},
  pages        = {2616-2625},
  shortjournal = {IET Image Process.},
  title        = {Unsupervised multiscale retinal blood vessel segmentation using fundus images},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Support vector machine classification combined with
multimodal magnetic resonance imaging in detection of patients with
schizophrenia. <em>IETIP</em>, <em>14</em>(11), 2610–2615. (<a
href="https://doi.org/10.1049/iet-ipr.2019.1108">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The brain avatar of schizophrenic patients is different from the normal human brain avatar, and it is difficult to overcome the complex environmental effects of the brain through traditional magnetic resonance imaging (MRI). In order to improve the accuracy of MRI in detecting brain information in patients with schizophrenia, this study is based on the support vector machine classification algorithm and combined with multimodal MRI detection method to construct a detection model suitable for patients with schizophrenia. In addition, this study combines the existing test cases to divide the brain into regions and design a comparative experiment to study the accuracy of the model proposed in this study. Finally, the study draws the results by sub-regional comparison. Studies have shown that the algorithm model of this study has certain effects on brain detection in patients with schizophrenia, and can be applied to practice, and can provide theoretical reference for subsequent related research.},
  archive      = {J_IETIP},
  author       = {Yunsong Zheng and Hangbin Tong and Teng Zhao and Xiaoxia Guo and Hui Xu and Ruwu Yang},
  doi          = {10.1049/iet-ipr.2019.1108},
  journal      = {IET Image Processing},
  month        = {9},
  number       = {11},
  pages        = {2610-2615},
  shortjournal = {IET Image Process.},
  title        = {Support vector machine classification combined with multimodal magnetic resonance imaging in detection of patients with schizophrenia},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Higher order PDE based model for segmenting noisy image.
<em>IETIP</em>, <em>14</em>(11), 2597–2609. (<a
href="https://doi.org/10.1049/iet-ipr.2019.0885">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this study, a fourth-order non-linear partial differential equation (PDE) model together with multi-well potential has been proposed for greyscale image segmentation. The multi-well potential is constructed from the histogram of the given image to make the segmentation process fully automatic and unsupervised. Further, the model is refined for effective segmentation of noisy greyscale image. The fourth-order anisotropic term with the multi-well potential is shown to properly segment noisy images. Fourier spectral method in space with semi-implicit convexity splitting in time is used to derive an unconditionally stable scheme. Numerical studies on some standard test images and comparison of results with those in literature clearly depict the superiority of the anisotropic variant of the non-linear PDE model.},
  archive      = {J_IETIP},
  author       = {B.V. Rathish Kumar and Abdul Halim and Rowthu Vijayakrishna},
  doi          = {10.1049/iet-ipr.2019.0885},
  journal      = {IET Image Processing},
  month        = {9},
  number       = {11},
  pages        = {2597-2609},
  shortjournal = {IET Image Process.},
  title        = {Higher order PDE based model for segmenting noisy image},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Non-uniform image blind deblurring by two-stage fully
convolution network. <em>IETIP</em>, <em>14</em>(11), 2588–2596. (<a
href="https://doi.org/10.1049/iet-ipr.2018.5716">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep neural networks have recently demonstrated high performance for deblurring. However, few methods are designed for both non-uniform image blur estimation and removal with highly efficient. In this study, the authors proposed a fully convolutional network that outputs estimated blur and restored image in one feed-forward pass for the non-uniformly blurred image of any input-size. The proposed network contains two subnets. The parameter estimation subnet P-net predicts pixel-wise parameters of multiple blur types with high accuracy. The output of P-net is used as a condition, which guides the blur removal subnet G-net to restore a high quality latent sharp image. P-net and G-net are ultimately integrated into a single framework called PG-net, which guarantees the consistency of parameter estimation and blur removal, thereby improves algorithm efficiency. Experiment results show that the authors blur parameter estimation method as well as their deblurring method outperforms the comparison methods both quantitatively and qualitatively.},
  archive      = {J_IETIP},
  author       = {Chudan Wu and Yan Wo and Guoqing Han and Zhangyong Wu and Jiyun Liang},
  doi          = {10.1049/iet-ipr.2018.5716},
  journal      = {IET Image Processing},
  month        = {9},
  number       = {11},
  pages        = {2588-2596},
  shortjournal = {IET Image Process.},
  title        = {Non-uniform image blind deblurring by two-stage fully convolution network},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Hybrid deep emperor penguin classifier algorithm-based image
quality assessment for visualisation application in HDR environments.
<em>IETIP</em>, <em>14</em>(11), 2579–2587. (<a
href="https://doi.org/10.1049/iet-ipr.2019.1371">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {One of the main open challenges in visualisation applications such as cathode ray tube (CRT) monitor, liquid-crystal display (LCD), and organic light-emitting diode (OLED) display is the robustness for high dynamic range (HDR) environs. This is due to the imperfections in the sensor and the incapability to track interest points successfully because of the brightness constancy in visualisation applications. To address this problem, different tone mapping operators are required for visualising HDR images on standard displays. However, these standard displays have different dynamic ranges. Thus, there is a need for a new model to find the best quality tone mapped image for specific kinds of visualisation applications. The authors propose a hybrid deep emperor penguin classifier to accurately classify the tone mapped images for different visualisation applications. Here, a selective deep neural network is trained to predict the quality of a tone-mapped image. Based on this quality, a decision is made as to the suitability of the image for CRT monitor, LCD display or OLED display. Also, they evaluate the proposed model on the TMIQD database and the simulation results prove that the proposed model outperforms the state-of-the-art image quality assessment methods.},
  archive      = {J_IETIP},
  author       = {Sunil L. Tade and Vibha Vyas},
  doi          = {10.1049/iet-ipr.2019.1371},
  journal      = {IET Image Processing},
  month        = {9},
  number       = {11},
  pages        = {2579-2587},
  shortjournal = {IET Image Process.},
  title        = {Hybrid deep emperor penguin classifier algorithm-based image quality assessment for visualisation application in HDR environments},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). SAR multi-target interactive motion recognition based on
convolutional neural networks. <em>IETIP</em>, <em>14</em>(11),
2567–2578. (<a href="https://doi.org/10.1049/iet-ipr.2019.0861">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Synthetic aperture radar (SAR) multi-target interactive motion recognition classifies the type of interactive motion and generates descriptions of the interactive motions at the semantic level by considering the relevance of multi-target motions. A method for SAR multi-target interactive motion recognition is proposed, which includes moving target detection, target type recognition, interactive motion feature extraction, and multi-target interactive motion type recognition. Wavelet thresholding denoising combined with a convolutional neural network (CNN) is proposed for target type recognition. The method performs wavelet thresholding denoising on SAR target images and then uses an eight-layer CNN named EilNet to achieve target recognition. After target type recognition, a multi-target interactive motion type recognition method is proposed. A motion feature matrix is constructed for recognition and a four-layer CNN named FolNet is designed to perform interactive motion type recognition. A motion simulation dataset based on the MSTAR dataset is built, which includes four kinds of interactive motions by two moving targets. The experimental results show that the recognition performance of the authors’ Wavelet + EilNet method for target type recognition and FolNet for multi-target interactive motion type recognition are both better than other methods. Thus, the proposed method is an effective method for SAR multi-target interactive motion recognition.},
  archive      = {J_IETIP},
  author       = {Ruo-Hong Huan and Luo-Qi Ge and Peng Yang and Chao-Jie Xie and Kai-Kai Chi and Ke-Ji Mao and Yun Pan},
  doi          = {10.1049/iet-ipr.2019.0861},
  journal      = {IET Image Processing},
  month        = {9},
  number       = {11},
  pages        = {2567-2578},
  shortjournal = {IET Image Process.},
  title        = {SAR multi-target interactive motion recognition based on convolutional neural networks},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Self-guided filter for image denoising. <em>IETIP</em>,
<em>14</em>(11), 2561–2566. (<a
href="https://doi.org/10.1049/iet-ipr.2019.1471">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The guided filter has been acknowledged as an exceptional edge-preserving filter whose output is a locally linear transform of the guidance image. However, the traditional guided filter heavily relies on the guidance image and fails to achieve the desired result when performing image denoising without a clear guidance image. In this study, to address this limitation, the authors propose a simple yet effective guided filter variant for the single image noise removing. They further show that the proposed denoising strategy can be easily realised by using the iterative framework. Moreover, the weak textured patches based image noise estimation is utilised to generate a clear intermediate image which makes the proposed method highly adaptable to the local noise level. Experimental results demonstrate that their proposed algorithm can compete with the state-of-the-art local denoising methods in edge-preserving.},
  archive      = {J_IETIP},
  author       = {Shujin Zhu and Zekuan Yu},
  doi          = {10.1049/iet-ipr.2019.1471},
  journal      = {IET Image Processing},
  month        = {9},
  number       = {11},
  pages        = {2561-2566},
  shortjournal = {IET Image Process.},
  title        = {Self-guided filter for image denoising},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Robust landmark-free head pose estimation by learning to
crop and background augmentation. <em>IETIP</em>, <em>14</em>(11),
2553–2560. (<a href="https://doi.org/10.1049/iet-ipr.2019.1369">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {It is well known that the performance of head pose estimation is greatly affected by the bounding box margin of the face and its background. Traditionally, researchers will manually choose a suitable bounding box margin to strike a balance between ensuring sufficient information and minimising background noise. However, head pose estimation is still worse when the background is complex in reality or when the box margin changes slightly. To make estimation results more robust, the authors propose two methods to improve it: (i) a convolutional cropping module that can learn to crop the input image to an attentional area for head pose regression. (ii) Background augmentation that can make the network more robust to the background noise. Rather than using the face landmarking to calculate head pose angles, they use another convolutional neural network to regress the head pose angles, which is independent of the landmark detection results. They evaluate the method on BIWI and AFLW2000 dataset and experimental results show that their approach outperforms many other methods. Besides, they evaluate the method on Pointing′04 dataset using head pose accuracy. Furthermore, the approach is more robust and has a lower variance in realistic scenarios.},
  archive      = {J_IETIP},
  author       = {Aoru Xue and Kai Sheng and Songming Dai and Xiaoqiang Li},
  doi          = {10.1049/iet-ipr.2019.1369},
  journal      = {IET Image Processing},
  month        = {9},
  number       = {11},
  pages        = {2553-2560},
  shortjournal = {IET Image Process.},
  title        = {Robust landmark-free head pose estimation by learning to crop and background augmentation},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). DeepJoint segmentation for the classification of
severity-levels of glioma tumour using multimodal MRI images.
<em>IETIP</em>, <em>14</em>(11), 2541–2552. (<a
href="https://doi.org/10.1049/iet-ipr.2018.6682">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Brain tumour segmentation is the process of separating the tumour from normal brain tissues. A glioma is a kind of tumour, which fires up in the glial cells of the spine or the brain. This study introduces a technique for classifying the severity levels of glioma tumour using a novel segmentation algorithm, named DeepJoint segmentation and the multi-classifier. Initially, the brain images are subjected to pre-processing and the region of interest is extracted. Then, the segmentation of the pre-processed image is done using the proposed DeepJoint segmentation, which is developed through the iterative procedure of joining the grid segments. After the segmentation, feature extraction is carried out from core and oedema tumours using information-theoretic measures. Finally, the classification is done by the deep convolutional neural network (DCNN), which is trained by an optimisation algorithm, named fractional Jaya whale optimiser (FJWO). FJWO is developed by integrating the whale optimisation algorithm in fractional Jaya optimiser. The performance of the proposed FJWO–DCNN with the DeepJoint segmentation method is analysed using accuracy, true positive rate, specificity, and sensitivity. The results depicted that the proposed method produces a maximum accuracy of 96%, which indicates its superiority.},
  archive      = {J_IETIP},
  author       = {Michael Mahesh K and Arokia Renjit J},
  doi          = {10.1049/iet-ipr.2018.6682},
  journal      = {IET Image Processing},
  month        = {9},
  number       = {11},
  pages        = {2541-2552},
  shortjournal = {IET Image Process.},
  title        = {DeepJoint segmentation for the classification of severity-levels of glioma tumour using multimodal MRI images},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Comparative analysis of texture feature extraction
techniques for rice grain classification. <em>IETIP</em>,
<em>14</em>(11), 2532–2540. (<a
href="https://doi.org/10.1049/iet-ipr.2019.1055">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Classifications of eight different varieties of rice grain are discussed in this study based on various texture models. Four local texture feature extraction techniques are proposed and three sets of texture features (SET-A, SET-B and SET-C) are formed, for the classification task. Performances of the proposed feature sets are compared with the existing techniques based on, run length matrix, co-occurrence matrix, size zone matrix, neighbourhood grey tone difference matrix and wavelet decomposition, towards classification of rice grain using a back propagation neural network (BPNN). The proposed techniques are also tested against publicly available data from Brodatz&#39;s texture data set and their results are compared with other techniques. The classification accuracy by the BPNN classifier is also compared with other statistical classifiers namely, K-nearest neighbour, linear discriminant classifier and Naive Bayes classifier. It is found that, the proposed feature sets yield better classification results on both rice data and Brodatz&#39;s data. Results show that, feature SET-B, is able to classify rice grain with an average classification accuracy of 99.63% with a minimum of six features.},
  archive      = {J_IETIP},
  author       = {Kshetrimayum Robert Singh and Saurabh Chaudhury},
  doi          = {10.1049/iet-ipr.2019.1055},
  journal      = {IET Image Processing},
  month        = {9},
  number       = {11},
  pages        = {2532-2540},
  shortjournal = {IET Image Process.},
  title        = {Comparative analysis of texture feature extraction techniques for rice grain classification},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Structure–texture image decomposition using a new non-local
TV-hilbert model. <em>IETIP</em>, <em>14</em>(11), 2525–2531. (<a
href="https://doi.org/10.1049/iet-ipr.2019.0392">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Combining the advantages of the non-local total variation (TV) and the Gabor function, a new Gabor function based non-local TV-Hilbert model is presented to separate the structure and texture components of the image. Computationally, by introducing the dual form of the non-local TV, the authors reformulate the non-local TV-Hilbert minimisation problem into a convex–concave saddle-point problem. In the aspect of solving algorithm, by transforming the Chambolle–Pock&#39;s first-order primal–dual algorithm into a different equivalent form. The authors propose a proximal-based primal–dual algorithm to solve the convex–concave saddle-point problem. At last, experimental results demonstrate that the proposed new model outperforms several existing state-of-the-art variational models.},
  archive      = {J_IETIP},
  author       = {Yehu Lv},
  doi          = {10.1049/iet-ipr.2019.0392},
  journal      = {IET Image Processing},
  month        = {9},
  number       = {11},
  pages        = {2525-2531},
  shortjournal = {IET Image Process.},
  title        = {Structure–texture image decomposition using a new non-local TV-hilbert model},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A bi-directional fractional-order derivative mask for image
processing applications. <em>IETIP</em>, <em>14</em>(11), 2512–2524. (<a
href="https://doi.org/10.1049/iet-ipr.2019.0467">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fractional computation has been recently designed as a major mathematical tool in image and signal processing fields. This study presents a novel operator established for two-dimensional fractional differentiation. It is developed based on the one-dimensional Charef fractional differentiation extension. A new multi-directional mask is proposed and a new adaptive fractional-order computation is introduced. The proposed method uses the gradient computation properties. It has been applied in edge detection and de-noising problems using real and synthetic images. Obtained results have been compared to those given by integer and fractional useful operators. Results demonstrate that the fractional edge images obtained using the proposed operator has more complete and clear contour information and more abundant texture detail information. The performances have been improved by the proposed method.},
  archive      = {J_IETIP},
  author       = {Meriem Hacini and Fella Hachouf and Abdelfatah Charef},
  doi          = {10.1049/iet-ipr.2019.0467},
  journal      = {IET Image Processing},
  month        = {9},
  number       = {11},
  pages        = {2512-2524},
  shortjournal = {IET Image Process.},
  title        = {A bi-directional fractional-order derivative mask for image processing applications},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Seed picking crossover optimisation algorithm for semantic
segmentation from images. <em>IETIP</em>, <em>14</em>(11), 2503–2511.
(<a href="https://doi.org/10.1049/iet-ipr.2019.1189">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Semantic image segmentation treats the issues involved in the object recognition and image segmentation as a combined task. The chief notion of semantic segmentation is to partition the image into visually uniform regions and to discriminate the class of the partitioned regions. Pixel classification is done over the segmented regions by assigning semantic labels. In general, inference frameworks are fed with the combination of low-level features and high-level contextual cues to segment an image. Since these combinations are rarely object consistent, result with minimum classification accuracy because of choosing non-influencing features and cues to track specific objects. To overcome this problem, a nature-inspired meta-heuristic optimization algorithm called Seed Picking Crossover Optimization (SPCO) is proposed to optimize i.e. train the CRF (Conditional Random Field) for choosing relevant feature to segment the object with high accuracy. To meritoriously recognize the objects, a semi-segmentation process is initially performed using Simple Linear Iterative Clustering (SLIC) algorithm. For pixel transformation and pixel association, Dirichlet process mixture model and CRF are employed. Optimized CRFs are used where the parametric optimization is done using the proposed SPCO algorithm. The proposed work results with 84% on classification accuracy and the performance evaluations are done using MSRC-21 dataset.},
  archive      = {J_IETIP},
  author       = {Manonmani Arunkumar and Vijayakumari Pushparaj},
  doi          = {10.1049/iet-ipr.2019.1189},
  journal      = {IET Image Processing},
  month        = {9},
  number       = {11},
  pages        = {2503-2511},
  shortjournal = {IET Image Process.},
  title        = {Seed picking crossover optimisation algorithm for semantic segmentation from images},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Extreme learning machine with feature mapping of kernel
function. <em>IETIP</em>, <em>14</em>(11), 2495–2502. (<a
href="https://doi.org/10.1049/iet-ipr.2019.1016">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Kernel-based extreme learning machine (KELM) solves the problem of random initialisation of extreme learning machine (ELM), and it has a faster learning speed and higher learning accuracy. However, when it comes to a scenario in which the dimensionality of kernel function mapping space is less than the number of samples, the kernel function theoretically cannot be introduced into ELM. To solve this problem, ELM with feature mapping (FM) of kernel function (FM-KELM) is proposed in this study, in which the random FM between the input layer and hidden layer of ELM is replaced with the FM of the kernel function. Moreover, the authors prove that when the regularised parameter C is close to zero, the solution of introduced kernel function is approximately equal to the correct solution. The proposed algorithm is more robust than KELM for the parameter C. Several experimental results show that the proposed algorithm in this study achieves higher classification accuracy without excessive parameter tuning, and the duration of the training and testing process is significantly reduced.},
  archive      = {J_IETIP},
  author       = {Zhaoxi Wang and Shengyong Chen and Rongwei Guo and Bin Li and Yangbo Feng},
  doi          = {10.1049/iet-ipr.2019.1016},
  journal      = {IET Image Processing},
  month        = {9},
  number       = {11},
  pages        = {2495-2502},
  shortjournal = {IET Image Process.},
  title        = {Extreme learning machine with feature mapping of kernel function},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Vanishing point detection using the teaching learning-based
optimisation algorithm. <em>IETIP</em>, <em>14</em>(11), 2487–2494. (<a
href="https://doi.org/10.1049/iet-ipr.2019.0516">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Within the computer vision field, estimating image vanishing points has many applications regarding robotic navigation, camera calibration, image understanding, visual measurement, 3D reconstruction, among others. Different methods for detecting vanishing points relies on accumulator space techniques, while others employ a heuristic approach such as RANSAC. Nevertheless, these types of methods suffer from low accuracy or high computational cost. To explore a different technique, this paper focuses on improving the efficiency of the metaheuristic search for vanishing points by using a recently proposed population-based method: The Teaching Learning Based Optimisation algorithm (TLBO). The TLBO algorithm is a metaheuristic technique inspired by the teaching–learning process. In our method, the TLBO algorithm is used after a line segment detection, to cluster line segments according to their more optimal vanishing point. Thus, our algorithm detects both orthogonal and nonorthogonal vanishing points in real images. To corroborate the performance of our proposed algorithm, different comparison and tests with other approaches were carried out. The results validate the accuracy and efficiency of our proposed method. Our approach had an average computational time of1.42 seconds and obtained a cumulative focal length error of 1 pixel, and cumulative angular error of 0.1°.},
  archive      = {J_IETIP},
  author       = {Alan López-Martinez and Francisco J. Cuevas},
  doi          = {10.1049/iet-ipr.2019.0516},
  journal      = {IET Image Processing},
  month        = {9},
  number       = {11},
  pages        = {2487-2494},
  shortjournal = {IET Image Process.},
  title        = {Vanishing point detection using the teaching learning-based optimisation algorithm},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Dynamic gesture recognition based on feature fusion network
and variant ConvLSTM. <em>IETIP</em>, <em>14</em>(11), 2480–2486. (<a
href="https://doi.org/10.1049/iet-ipr.2019.1248">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Gesture is a natural form of human communication, and it is of great significance in human–computer interaction. In the dynamic gesture recognition method based on deep learning, the key is to obtain comprehensive gesture feature information. Aiming at the problem of inadequate extraction of spatiotemporal features or loss of feature information in current dynamic gesture recognition, a new gesture recognition architecture is proposed, which combines feature fusion network with variant convolutional long short-term memory (ConvLSTM). The architecture extracts spatiotemporal feature information from local, global and deep aspects, and combines feature fusion to alleviate the loss of feature information. Firstly, local spatiotemporal feature information is extracted from video sequence by 3D residual network based on channel feature fusion. Then the authors use the variant ConvLSTM to learn the global spatiotemporal information of dynamic gesture, and introduce the attention mechanism to change the gate structure of ConvLSTM. Finally, a multi-feature fusion depthwise separable network is used to learn higher-level features including depth feature information. The proposed approach obtains very competitive performance on the Jester dataset with the classification accuracies of 95.59%, achieving state-of-the-art performance with 99.65% accuracy on the SKIG (Sheffifield Kinect Gesture) dataset.},
  archive      = {J_IETIP},
  author       = {Yuqing Peng and Huifang Tao and Wei Li and Hongtao Yuan and Tiejun Li},
  doi          = {10.1049/iet-ipr.2019.1248},
  journal      = {IET Image Processing},
  month        = {9},
  number       = {11},
  pages        = {2480-2486},
  shortjournal = {IET Image Process.},
  title        = {Dynamic gesture recognition based on feature fusion network and variant ConvLSTM},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Automatic food recognition system for middle-eastern
cuisines. <em>IETIP</em>, <em>14</em>(11), 2469–2479. (<a
href="https://doi.org/10.1049/iet-ipr.2019.1051">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The concerns for a healthier diet are increasing day by day, especially in diabetics wherein the aim of healthier diet can only be achieved by keeping a track of daily food intake and glucose-level. As a consequence, there is an ever-increasing need for automatic tools able to help diabetics to manage their diet and also help physicians to better analyse the effects of various types of food on the glucose-level of diabetics. In this paper, we propose an intelligent food recognition and tracking system for diabetics, which is potentially an essential part of a mobile application that we propose to couple food intake with the blood glucose-level using glucose measuring sensors. For food recognition, we rely on several feature extraction and classification techniques individually and jointly using an early and three different late fusion techniques, namely (i) Particle Swarm Optimisation (PSO), (ii) Genetic Algorithms (GA) based fusion and (iii) simple averaging. Moreover, we also evaluate the performance of several handcrafted and deep features and compare the results against state-of-the-art. In addition, we collect a large-scale dataset containing images from several types of local Middle-Eastern food, which is intended to become a powerful support tool for future research in the domain.},
  archive      = {J_IETIP},
  author       = {Marwa Qaraqe and Muhammad Usman and Kashif Ahmad and Amir Sohail and Ali Boyaci},
  doi          = {10.1049/iet-ipr.2019.1051},
  journal      = {IET Image Processing},
  month        = {9},
  number       = {11},
  pages        = {2469-2479},
  shortjournal = {IET Image Process.},
  title        = {Automatic food recognition system for middle-eastern cuisines},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Perceptual accessible image encryption scheme conjugating
multiple chaotic maps. <em>IETIP</em>, <em>14</em>(11), 2457–2468. (<a
href="https://doi.org/10.1049/iet-ipr.2019.0527">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the recent decade, chaos-based image encryption algorithms gained attention due to their pros and cons. The authors suggest one such algorithm of image encryption for different colour space using feed forward initial condition to pursue random permutation by combining 1D logistic map with a series of tent maps. The algorithm adds one more novel step to protect and encrypt binary image by concealing it within a cover grey image using bit-plane decomposition methods. Sensitivity towards initial condition is shown using block division of image by applying a stream of random initial conditions to each block for encryption along with XOR operation which leads the key space high . For better performance analysis, the proposed scheme has been tested with many colour space images including a binary image. The proposed technique is more efficient with respect to time complexity and resistance to vulnerability aspects.},
  archive      = {J_IETIP},
  author       = {Ram Chandra Barik and Suvamoy Changder},
  doi          = {10.1049/iet-ipr.2019.0527},
  journal      = {IET Image Processing},
  month        = {9},
  number       = {11},
  pages        = {2457-2468},
  shortjournal = {IET Image Process.},
  title        = {Perceptual accessible image encryption scheme conjugating multiple chaotic maps},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Detection, quantification and classification of ripened
tomatoes: A comparative analysis of image processing and machine
learning. <em>IETIP</em>, <em>14</em>(11), 2442–2456. (<a
href="https://doi.org/10.1049/iet-ipr.2019.0738">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this study, specifically for the detection of ripe/unripe tomatoes with/without defects in the crop field, two distinct methods are described and compared from captured images by a camera mounted on a mobile robot. One is a machine learning approach, known as ‘Cascaded Object Detector’ (COD) and the other is a composition of traditional customised methods, individually known as ‘Colour Transformation’: ‘Colour Segmentation’ and ‘Circular Hough Transformation’. The (Viola-Jones) COD generates ‘histogram of oriented gradient’ (HOG) features to detect tomatoes. For ripeness checking, the RGB mean is calculated with a set of rules. However, for traditional methods, colour thresholding is applied to detect tomatoes either from natural or solid background and RGB colour is adjusted to identify ripened tomatoes. This algorithm is shown to be optimally feasible for any micro-controller based miniature electronic devices in terms of its run time complexity of O ( n 3 ) for a traditional method in best and average cases. Comparisons show that the accuracy of the machine learning method is 95%, better than that of the Colour Segmentation Method using MATLAB.},
  archive      = {J_IETIP},
  author       = {Kazy Noor e Alam Siddiquee and Md. Shabiul Islam and Mohammad Yasin Ud Dowla and Karim Mohammed Rezaul and Vic Grout},
  doi          = {10.1049/iet-ipr.2019.0738},
  journal      = {IET Image Processing},
  month        = {9},
  number       = {11},
  pages        = {2442-2456},
  shortjournal = {IET Image Process.},
  title        = {Detection, quantification and classification of ripened tomatoes: A comparative analysis of image processing and machine learning},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Acceleration of multi-task cascaded convolutional networks.
<em>IETIP</em>, <em>14</em>(11), 2435–2441. (<a
href="https://doi.org/10.1049/iet-ipr.2019.0141">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-task cascaded convolutional neural network (MTCNN) is a human face detection architecture which uses a cascaded structure with three stages (P-Net, R-Net and O-Net). The authors intend to reduce the computation time of the whole process of the MTCNN. They find that the non-maximum suppression (NMS) processes after the P-Net occupy over half of the computation time. Therefore, the authors propose a self-fine-tuning method which makes the control of computation time for the NMS process easier. Self-fine-tuning is a training trick which uses hard samples generated by P-Net to retrain P-Net. After self-fine-tuning, the distribution of human face probabilities generated by P-Net is changed, and the tail of distribution becomes thinner. The control of the number of NMS input boxes can be made easier when the distribution has a thinner tail, and choosing a suitable threshold to filter the face boxes will generate less boxes. So the computation time can be reduced. In order to keep the performance of MTCNN, the authors still propose a landmark data set augmentation, which can enhance the performance of the self-fine-tuned MTCNN. From the experiments, it is found that the proposed scheme can significantly reduce the computation time of MTCNN.},
  archive      = {J_IETIP},
  author       = {Long-Hua Ma and Hang-Yu Fan and Zhe-Ming Lu and Dong Tian},
  doi          = {10.1049/iet-ipr.2019.0141},
  journal      = {IET Image Processing},
  month        = {9},
  number       = {11},
  pages        = {2435-2441},
  shortjournal = {IET Image Process.},
  title        = {Acceleration of multi-task cascaded convolutional networks},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). ResDNN: Deep residual learning for natural image denoising.
<em>IETIP</em>, <em>14</em>(11), 2425–2434. (<a
href="https://doi.org/10.1049/iet-ipr.2019.0623">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image denoising is a thoroughly studied research problem in the areas of image processing and computer vision. In this work, a deep convolution neural network with added benefits of residual learning for image denoising is proposed. The network is composed of convolution layers and ResNet blocks along with rectified linear unit activation functions. The network is capable of learning end-to-end mappings from noise distorted images to restored cleaner versions. The deeper networks tend to be challenging to train and often are posed with the problem of vanishing gradients. The residual learning and orthogonal kernel initialisation keep the gradients in check. The skip connections in the ResNet blocks pass on the learned abstractions further down the network in the forward pass, thus achieving better results. With a single model, one can tackle different levels of Gaussian noise efficiently. The experiments conducted on the benchmark datasets prove that the proposed model obtains a significant improvement in structural similarity index than the previously existing state-of-the-art techniques.},
  archive      = {J_IETIP},
  author       = {Gurprem Singh and Ajay Mittal and Naveen Aggarwal},
  doi          = {10.1049/iet-ipr.2019.0623},
  journal      = {IET Image Processing},
  month        = {9},
  number       = {11},
  pages        = {2425-2434},
  shortjournal = {IET Image Process.},
  title        = {ResDNN: Deep residual learning for natural image denoising},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). E2-capsule neural networks for facial expression recognition
using AU-aware attention. <em>IETIP</em>, <em>14</em>(11), 2417–2424.
(<a href="https://doi.org/10.1049/iet-ipr.2020.0063">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Capsule neural network is a new and popular technique in deep learning. However, the traditional capsule neural network does not extract features sufficiently before the dynamic routing between capsules. In this study, one double enhanced capsule neural network (E2-Capsnet) that uses AU-aware attention for facial expression recognition (FER) is proposed. The E2-Capsnet takes advantage of dynamic routing between capsules and has two enhancement modules which are beneficial to FER. The first enhancement module is the convolutional neural network with AU-aware attention, which can focus on the active areas of the expression. The second enhancement module is the capsule neural network with multiple convolutional layers, which enhances the ability of the feature representation. Finally, the squashing function is used to classify the facial expression. The authors demonstrate the effectiveness of E2-Capsnet on the two public benchmark datasets, RAF-DB and EmotioNet. The experimental results show that their E2-Capsnet is superior to the state-of-the-art methods. The code is available at https://github.com/ShanCao18/E2-Capsnet .},
  archive      = {J_IETIP},
  author       = {Shan Cao and Yuqian Yao and Gaoyun An},
  doi          = {10.1049/iet-ipr.2020.0063},
  journal      = {IET Image Processing},
  month        = {9},
  number       = {11},
  pages        = {2417-2424},
  shortjournal = {IET Image Process.},
  title        = {E2-capsule neural networks for facial expression recognition using AU-aware attention},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). New flexible directional filter bank by tuning hermite
transform parameters for content based medical image retrieval.
<em>IETIP</em>, <em>14</em>(11), 2403–2416. (<a
href="https://doi.org/10.1049/iet-ipr.2019.0252">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study presents a flexible directional filter bank (DFB) by tuning Hermite transform parameters for content-based medical image retrieval (CBMIR). Two imperative Hermite transform parameters such as the scale of Gaussian kernel and the order of Hermite polynomial are tuned to adapt the flexible Hermite orthogonal basis to improve the retrieval performance of the CBMIR system. First, the tuned 1D Hermite filter transformed into 2D diamond shape filter by McClellan transformation and made it adequate for the directional decomposition of images using DFB structure. The dominant features of these directionally decomposed images are extracted by the rotation-invariant local neighbourhood frequency pattern. The mean retrieval precision is used as a high-level criterion to measure retrieval performance. The proposed method is assessed on three medical image databases: two for computed tomography and one for magnetic resonance imaging and achieved 99.41, 89.07, and 88.71% mean precision, respectively, when ten images are returned by the system.},
  archive      = {J_IETIP},
  author       = {Amita Shinde and Amol Rahulkar and Chetankumar Patil},
  doi          = {10.1049/iet-ipr.2019.0252},
  journal      = {IET Image Processing},
  month        = {9},
  number       = {11},
  pages        = {2403-2416},
  shortjournal = {IET Image Process.},
  title        = {New flexible directional filter bank by tuning hermite transform parameters for content based medical image retrieval},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Multi-head mutual-attention CycleGAN for unpaired
image-to-image translation. <em>IETIP</em>, <em>14</em>(11), 2395–2402.
(<a href="https://doi.org/10.1049/iet-ipr.2019.1153">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The image-to-image translation, i.e. from source image domain to target image domain, has made significant progress in recent years. The most popular method for unpaired image-to-image translation is CycleGAN. However, it always cannot accurately and rapidly learn the key features in target domains. So, the CycleGAN model learns slowly and the translation quality needs to be improved. In this study, a multi-head mutual-attention CycleGAN (MMA-CycleGAN) model is proposed for unpaired image-to-image translation. In MMA-CycleGAN, the cycle-consistency loss and adversarial loss in CycleGAN are still used, but a mutual-attention (MA) mechanism is introduced, which allows attention-driven, long-range dependency modelling between the two image domains. Moreover, to efficiently deal with the large image size, the MA is further improved to the multi-head mutual-attention (MMA) mechanism. On the other hand, domain labels are adopted to simplify the MMA-CycleGAN architecture, so only one generator is required to perform bidirectional translation tasks. Experiments on multiple datasets demonstrate MMA-CycleGAN is able to learn rapidly and obtain photo-realistic images in a shorter time than CycleGAN.},
  archive      = {J_IETIP},
  author       = {Wei Ji and Jing Guo and Yun Li},
  doi          = {10.1049/iet-ipr.2019.1153},
  journal      = {IET Image Processing},
  month        = {9},
  number       = {11},
  pages        = {2395-2402},
  shortjournal = {IET Image Process.},
  title        = {Multi-head mutual-attention CycleGAN for unpaired image-to-image translation},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Approach to model human appearance based on sparse
representation for human tracking in surveillance. <em>IETIP</em>,
<em>14</em>(11), 2383–2394. (<a
href="https://doi.org/10.1049/iet-ipr.2018.5961">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In human tracking, sparse representation successfully localises the human in a video with minimal reconstruction error using target templates. However, the state-of-the-art approaches use colour and local appearance of a human to discriminate the human from the background regions, and hence fail when the human is occluded and appears in the varying illumination environment. In this study, a robust tracking algorithm is proposed that utilises gradient orientation and fine and coarse sparse representation of the target template. Sparse representation-based human appearance model utilises weighted gradient orientation that is insensitive to illumination variation. Coarse and fine representation of sparse code facilitates tracking under varying scales. Subspace learning from image gradient orientation is enforced with occlusion detection during the dictionary updation stage to capture the visual characteristics of the local human appearance that supports tracking under partial occlusion with lesser tracking error. The proposed human tracking algorithm is evaluated on various datasets and shows efficient human tracking performance when compared to the other state-of-the-art approaches. Furthermore, the proposed human tracking algorithm is suitable for surveillance applications.},
  archive      = {J_IETIP},
  author       = {Sangeetha Damotharasamy},
  doi          = {10.1049/iet-ipr.2018.5961},
  journal      = {IET Image Processing},
  month        = {9},
  number       = {11},
  pages        = {2383-2394},
  shortjournal = {IET Image Process.},
  title        = {Approach to model human appearance based on sparse representation for human tracking in surveillance},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Feature channel enhancement for crowd counting.
<em>IETIP</em>, <em>14</em>(11), 2376–2382. (<a
href="https://doi.org/10.1049/iet-ipr.2019.1308">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Crowd counting, i.e. count the number of people in a crowded visual space, is emerging as an essential research problem with public security. A key in the design of the crowd counting system is to create a stable and accurate robust model, which requires to process on the feature channels of the counting network. In this study, the authors present a featured channel enhancement (FCE) block for crowd counting. First, they use a feature extraction unit to obtain the information of each channel and encodes the information of each channel. Then use a non-linear variation unit to deal with the encoded channel information, finally, normalise the data and affixed to each channel separately. With the use of the FCE, the positive characteristic channel can be enhanced and weak or negative channel information can be suppressed. The authors successfully incorporate the FCE with two compact networks on the standard benchmarks and prove that the proposed FCE achieves promising results.},
  archive      = {J_IETIP},
  author       = {Xingjiao Wu and Shuchen Kong and Yingbin Zheng and Hao Ye and Jing Yang and Liang He},
  doi          = {10.1049/iet-ipr.2019.1308},
  journal      = {IET Image Processing},
  month        = {9},
  number       = {11},
  pages        = {2376-2382},
  shortjournal = {IET Image Process.},
  title        = {Feature channel enhancement for crowd counting},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020b). Sparse representation based computed tomography images
reconstruction by coupled dictionary learning algorithm. <em>IETIP</em>,
<em>14</em>(11), 2365–2375. (<a
href="https://doi.org/10.1049/iet-ipr.2019.1312">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {It is very interesting to reconstruct high-resolution computed tomography (CT) medical images that are very useful for clinicians to analyse the diseases. This study proposes an improved super-resolution method for CT medical images in the sparse representation domain with dictionary learning. The sparse coupled K-singular value decomposition (KSVD) algorithm is employed for dictionary learning purposes. Images are divided into two sets of low resolution (LR) and high resolution (HR), to improve the quality of low-resolution images, the authors prepare dictionaries over LR and HR image patches using the KSVD algorithm. The main idea behind the proposed method is that sparse coupled dictionaries learn about each patch and establish the relationship between sparse coefficients of LR and HR image patches to recover the HR image patch for LR image. The proposed method is compared to conventional algorithms in terms of mean peak signal-to-noise ratio and structural similarity index measurements by using three different data set images, including CT chest, CT dental and CT brain images. The authors also analysed the proposed improved method for different dictionary sizes and patch size to obtain a similar high-resolution image. These parameters play an essential role in the reconstruction of the HR images.},
  archive      = {J_IETIP},
  author       = {Farah Deeba and She Kun and Fayaz Ali Dharejo and Yuanchun Zhou},
  doi          = {10.1049/iet-ipr.2019.1312},
  journal      = {IET Image Processing},
  month        = {9},
  number       = {11},
  pages        = {2365-2375},
  shortjournal = {IET Image Process.},
  title        = {Sparse representation based computed tomography images reconstruction by coupled dictionary learning algorithm},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Physics-based dynamic texture analysis and synthesis model
using GPU. <em>IETIP</em>, <em>14</em>(11), 2356–2364. (<a
href="https://doi.org/10.1049/iet-ipr.2019.0984">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The texture is a repetition of a particular structure. Textures classified into static texture and dynamic texture. There are two approaches for synthesising dynamic textures: image-based approach and physics-based approach. In the proposed work, the synthesis of different dynamic textures is shown. The physics-based approach is used to synthesise dynamic texture videos. Different mathematical models are proposed which are suitable to give appropriate motion to the dynamic textures. The raw frames are created, and these frames are further used to synthesise the dynamic textures using physics laws and mathematical formulae. The flexibility of each model is demonstrated. The proposed model has less specificity and less computational complexity. The proposed algorithms are implemented on the graphics processing unit to reduce the overall execution time and time complexity. High-quality videos are produced, and the evaluation of every frame assures the quality.},
  archive      = {J_IETIP},
  author       = {Premanand Pralhad Ghadekar},
  doi          = {10.1049/iet-ipr.2019.0984},
  journal      = {IET Image Processing},
  month        = {9},
  number       = {11},
  pages        = {2356-2364},
  shortjournal = {IET Image Process.},
  title        = {Physics-based dynamic texture analysis and synthesis model using GPU},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Conditional semi-fuzzy c-means clustering for imbalanced
dataset. <em>IETIP</em>, <em>14</em>(11), 2343–2355. (<a
href="https://doi.org/10.1049/iet-ipr.2019.0253">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fuzzy c-means algorithms have been widely utilised in several areas such as image segmentation, pattern recognition and data mining. However, the related studies showed the limitations in facing imbalanced datasets. The maximum fuzzy boundary tends to be located on the largest cluster which is not desirable. The overall fuzzy partition results in false grouping of edge objects and weakens the compactness of cluster. It is important the clusters are delineated by the maximum fuzzy boundary. In this study, a semi-fuzzy c-means algorithm is proposed by combining hard partition and soft partition. This study aims to provide an effective partition for the edge objects, such that the compactness of cluster can be improved. The proposed algorithm integrates the semi-fuzzy c-means method with the size-insensitive integrity-based fuzzy c-means algorithm. In particular, the latter algorithm has the ability to deal with imbalanced data. With the experiment validation, the proposed algorithm is robust and outperforms the two component algorithms by using synthetic and widely known benchmark datasets.},
  archive      = {J_IETIP},
  author       = {Yunlong Gao and Chengyu Yang and Kuo-Yi Lin and Jinyan Pan and Li Li},
  doi          = {10.1049/iet-ipr.2019.0253},
  journal      = {IET Image Processing},
  month        = {9},
  number       = {11},
  pages        = {2343-2355},
  shortjournal = {IET Image Process.},
  title        = {Conditional semi-fuzzy c-means clustering for imbalanced dataset},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Robust palmprint identification using efficient enhancement
and two-stage matching technique. <em>IETIP</em>, <em>14</em>(11),
2333–2342. (<a href="https://doi.org/10.1049/iet-ipr.2018.5736">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Palmprint-based human authentication has shown great potential for civil, forensic, and corporate security applications in recent years. Palmprint recognition systems suffer because of large palmprint sizes and the presence of a large number of creases and erroneous minutiae that make the enhancement and matching phases a challenge. In this study, a novel approach is presented based on efficient enhancement and a two-stage matching technique that demonstrates highly accurate identification results. The enhancement approach extracts minutia features from high-quality regions based on local ridge characteristics. The selected minutiae are then matched using a two-stage local and global minutiae neighbour-based matching technique. To demonstrate the performance of the proposed technique, comparisons with open-source algorithms are made based on equal error rate and detection error trade-off graph. The results confirm the efficacy of proposed palmprint enhancement and identification technique.},
  archive      = {J_IETIP},
  author       = {Mubeen Ghafoor and Syed Ali Tariq and Imtiaz A. Taj and Noman M. Jafri and Tehseen Zia},
  doi          = {10.1049/iet-ipr.2018.5736},
  journal      = {IET Image Processing},
  month        = {9},
  number       = {11},
  pages        = {2333-2342},
  shortjournal = {IET Image Process.},
  title        = {Robust palmprint identification using efficient enhancement and two-stage matching technique},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Background subtraction using infinite asymmetric gaussian
mixture models with simultaneous feature selection. <em>IETIP</em>,
<em>14</em>(11), 2321–2332. (<a
href="https://doi.org/10.1049/iet-ipr.2019.1029">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mixture models are broadly applied in image processing domains. Related existing challenges include failure to approximate exact data shapes, estimate correct number of components, and ignore irrelevant features. In this study, the authors develop a statistical self-refinement framework for the background subtraction task by using Dirichlet Process-based asymmetric Gaussian mixture model. The parameters of this model are learned using variational inference methods. They also incorporate feature selection simultaneously within the framework to avoid noisy influence from uninformative features. To validate the proposed framework, they report their results on background subtraction tasks on 8 different datasets for infrared and visible videos.},
  archive      = {J_IETIP},
  author       = {Ziyang Song and Samr Ali and Nizar Bouguila},
  doi          = {10.1049/iet-ipr.2019.1029},
  journal      = {IET Image Processing},
  month        = {9},
  number       = {11},
  pages        = {2321-2332},
  shortjournal = {IET Image Process.},
  title        = {Background subtraction using infinite asymmetric gaussian mixture models with simultaneous feature selection},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Novel image encryption by combining dynamic DNA sequence
encryption and the improved 2D logistic sine map. <em>IETIP</em>,
<em>14</em>(11), 2310–2320. (<a
href="https://doi.org/10.1049/iet-ipr.2019.1340">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The one-dimensional (2D) chaotic encryption algorithm has good encryption performance. For its properties, such as the excellent complexity, pseudo-randomness, and sensitivity to the initial value of the chaotic sequence. However, compared with other methods, its biggest drawback is that the key space is too small. To address these problems, in this study, the authors introduce an improved 2D logistic sine chaotic map (2D-LSMM). A novel image encryption scheme based on dynamic DNA sequences encryption and improved 2D-LSMM is presented. The logistic map is used to control the input of the sine map. And the encoding and operation rules of DNA sequences are determined by 2D-LSMM chaotic sequences. By implementing dynamic DNA sequence encryption, the encryption process becomes more complicated and harder to be attacked. Simulation experimental results and security analysis show that the authors’ encryption scheme not only achieves proper encryption but can also resist different attacks.},
  archive      = {J_IETIP},
  author       = {Jieyu Zheng and LingFeng Liu},
  doi          = {10.1049/iet-ipr.2019.1340},
  journal      = {IET Image Processing},
  month        = {9},
  number       = {11},
  pages        = {2310-2320},
  shortjournal = {IET Image Process.},
  title        = {Novel image encryption by combining dynamic DNA sequence encryption and the improved 2D logistic sine map},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Symbol positions-based slepian–wolf coding with application
to distributed video coding. <em>IETIP</em>, <em>14</em>(11), 2301–2309.
(<a href="https://doi.org/10.1049/iet-ipr.2018.5942">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this study, the authors will show that coding the positions of the symbols, instead of their values, can be a good way to implement efficient Slepian–Wolf (SW) coding and can reduce the complexity of both the encoder and the decoder. The authors will also propose a practical distributed video coding (DVC) system that exploits this idea. This system will use binary maps to indicate the positions of the most probable symbols, instead of separating them into bitplanes. Simulations show that this position-based SW coding allows a simple and more efficient DVC system with improved rate-distortion performance, compared to the bitplane-based DVC system that uses the same side information. The memory requirements at the encoder are reduced by about 50% and the number of channel coding operations is also reduced. This DVC system also allows to use an easy way to control quantisation, reduces the decoding latency, and allows fast parallel decoding.},
  archive      = {J_IETIP},
  author       = {Said Benierbah and Mohammed Khamadja},
  doi          = {10.1049/iet-ipr.2018.5942},
  journal      = {IET Image Processing},
  month        = {9},
  number       = {11},
  pages        = {2301-2309},
  shortjournal = {IET Image Process.},
  title        = {Symbol positions-based Slepian–Wolf coding with application to distributed video coding},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Urdu handwritten text recognition: A survey. <em>IETIP</em>,
<em>14</em>(11), 2291–2300. (<a
href="https://doi.org/10.1049/iet-ipr.2019.0401">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Work on the problem of handwritten text recognition in Urdu script has been an active research area. A significant progress is made in this interesting and challenging field in the last few years. In this study, the authors presented a comprehensive survey for a number of offline and online handwritten text recognition systems for Urdu script written in Nastaliq font style from 2004 to 2019. Following features make their contribution worthwhile and unique among the reviews of a similar kind: (i) their review classifies the existing studies based on types of recognition systems used for Urdu handwritten text, (ii) it covers a very different outlook of the recognition process of the Urdu handwritten text at different granularity levels (e.g. character, word, ligature, or sentence level), (iii) this review article also presents each of surveyed articles in following dimensions: the task performed, its granularity level, dataset used, results obtained, and future dimensions, and (iv) lastly it gives the summary of the surveyed articles according to the granularity levels, publishing years, related tasks or subtasks, and types of classifiers used. In the end, major challenges and tasks related to Urdu handwritten text recognition approaches are also discussed in detail.},
  archive      = {J_IETIP},
  author       = {Mujtaba Husnain and Malik Muhammad Saad Missen and Shahzad Mumtaz and Mickaël Coustaty and Muzzamil Luqman and Jean-Marc Ogier},
  doi          = {10.1049/iet-ipr.2019.0401},
  journal      = {IET Image Processing},
  month        = {9},
  number       = {11},
  pages        = {2291-2300},
  shortjournal = {IET Image Process.},
  title        = {Urdu handwritten text recognition: A survey},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Survey of single image super-resolution reconstruction.
<em>IETIP</em>, <em>14</em>(11), 2273–2290. (<a
href="https://doi.org/10.1049/iet-ipr.2019.1438">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image super-resolution reconstruction refers to a technique of recovering a high-resolution (HR) image (or multiple images) from a low-resolution (LR) degraded image (or multiple images). Due to the breakthrough progress in deep learning in other computer vision tasks, people try to introduce deep neural network and solve the problem of image super-resolution reconstruction by constructing a deep-level network for end-to-end training. The currently used deep learning models can divide the SISR model into four types: interpolation-based preprocessing-based model, original image processing based model, hierarchical feature-based model, and high-frequency detail-based model, or shared the network model. The current challenges for super-resolution reconstruction are mainly reflected in the actual application process, such as encountering an unknown scaling factor, losing paired LR–HR images, and so on.},
  archive      = {J_IETIP},
  author       = {Kai Li and Shenghao Yang and Runting Dong and Xiaoying Wang and Jianqiang Huang},
  doi          = {10.1049/iet-ipr.2019.1438},
  journal      = {IET Image Processing},
  month        = {9},
  number       = {11},
  pages        = {2273-2290},
  shortjournal = {IET Image Process.},
  title        = {Survey of single image super-resolution reconstruction},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Automatic recognition and location system for electric
vehicle charging port in complex environment. <em>IETIP</em>,
<em>14</em>(10), 2263–2272. (<a
href="https://doi.org/10.1049/iet-ipr.2019.1138">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study proposes an automatic recognition and location system of electric vehicle charging port with application to automatic charging. The system obtains the charging port posture through image processing, and performs the insertion motion in combination with the robot arm to complete the charging gun insertion of the automatic charging link. The framework of the system is mainly divided into three parts, recognition, location and insertion. In the charging port recognition, the convolutional neural network-based method is used, and the recognition success rate is up to 98.9% under the light intensity of 4000 lux; in the location of the charging port, the method of solving the pose based on the circle feature is adopted. The average value of the position error is within 1.4 mm, and the average value of the attitude angle error is within 1.6°, which meets the accuracy requirement of the insertion experiment; in the charging gun insertion, the motion of the robot is planned by interpolation algorithm. The lower limit of the successful insertion is about 135 lux and the upper limit is about 9350 lux.},
  archive      = {J_IETIP},
  author       = {Mingqiang Pan and Cheng Sun and Jizhu Liu and Yangjun Wang},
  doi          = {10.1049/iet-ipr.2019.1138},
  journal      = {IET Image Processing},
  month        = {8},
  number       = {10},
  pages        = {2263-2272},
  shortjournal = {IET Image Process.},
  title        = {Automatic recognition and location system for electric vehicle charging port in complex environment},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Evaluating salient object detection in natural images with
multiple objects having multi-level saliency. <em>IETIP</em>,
<em>14</em>(10), 2249–2262. (<a
href="https://doi.org/10.1049/iet-ipr.2019.0787">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Salient object detection is evaluated using binary ground truth (GT) with the labels being salient object class and background. In this study, the authors corroborate based on three subjective experiments on a novel image dataset that objects in natural images are inherently perceived to have varying levels of importance. The authors&#39; dataset, named SalMoN (saliency in multi-object natural images), has 588 images containing multiple objects. The subjective experiments performed record spontaneous attention and perception through eye fixation duration, point clicking and rectangle drawing. As object saliency in a multi-object image is inherently multi-level, they propose that salient object detection must be evaluated for the capability to detect all multi-level salient objects apart from the salient object class detection capability. For this purpose, they generate multi-level maps as GT corresponding to all the dataset images using the results of the subjective experiments, with the labels being multi-level salient objects and background. They then propose the use of mean absolute error, Kendall&#39;s rank correlation and average area under precision–recall curve to evaluate existing salient object detection methods on their multi-level saliency GT dataset. Approaches that represent saliency detection on images as local-global hierarchical processing of a graph perform well in their dataset.},
  archive      = {J_IETIP},
  author       = {Gökhan Yildirim and Debashis Sen and Mohan Kankanhalli and Sabine Süsstrunk},
  doi          = {10.1049/iet-ipr.2019.0787},
  journal      = {IET Image Processing},
  month        = {8},
  number       = {10},
  pages        = {2249-2262},
  shortjournal = {IET Image Process.},
  title        = {Evaluating salient object detection in natural images with multiple objects having multi-level saliency},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Accurate leukocoria predictor based on deep VGG-net CNN
technique. <em>IETIP</em>, <em>14</em>(10), 2241–2248. (<a
href="https://doi.org/10.1049/iet-ipr.2018.6656">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The most important part of digital image analysis is object classification. Nowadays, deep learning makes an enormous achievement in computer vision problems. So there has been a lot of interests in applying features learned by convolutional neural networks (CNNs) on general image recognition to more tasks such as object detection, segmentation and face recognition. Leukocoria detection is one of the serious challenges in infant retinal treatment. Leukocoria is represented as an abnormal white reflection appearing in the eyes of an infant suffering from retinoblastoma. This research proposes a deep Visual Geometry Group-net CNN classifier for automatic detection of leukocoria. The proposed classifier comprises pre-processing, feature extraction and classification. The deep CNN classifier contains convolution layer, pooling layer and fully connected layer with weights are developed on each image. Experimental results based on several eye images consist of ordinary and leukocoric from flicker, and it demonstrates that the proposed classifier provides better results with the accuracy of 98.5% and the error rate is below 2% which exceeds the current results.},
  archive      = {J_IETIP},
  author       = {Boyina Subrahmanyeswara Rao},
  doi          = {10.1049/iet-ipr.2018.6656},
  journal      = {IET Image Processing},
  month        = {8},
  number       = {10},
  pages        = {2241-2248},
  shortjournal = {IET Image Process.},
  title        = {Accurate leukocoria predictor based on deep VGG-net CNN technique},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). MRGAN: A generative adversarial networks model for global
mosaic removal. <em>IETIP</em>, <em>14</em>(10), 2235–2240. (<a
href="https://doi.org/10.1049/iet-ipr.2019.1111">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this study, the authors introduce a novel deep generative adversarial networks (GANs) model for global mosaic removal. The methods used in the proposed study consist of GANs model and a novel algorithm for maintaining and repairing (MR) images. The conventional mosaic removal algorithms all employ the correlation between the inserted pixel and its neighbouring pixels, which have a limited effect on the local mosaic removal but do not work well for the global mosaic removal. To respond to this difficulty, the authors introduce an MRGAN model with two novel parsing networks. Unlike previous GANs, the MR algorithm is used to calculate the pixel loss and content loss. The experimental comparison results show that the proposed MRGAN model has achieved leading results for the global mosaic removal task.},
  archive      = {J_IETIP},
  author       = {Zhiyi Cao and Shaozhang Niu and Jiwei Zhang and Xinyi Wang},
  doi          = {10.1049/iet-ipr.2019.1111},
  journal      = {IET Image Processing},
  month        = {8},
  number       = {10},
  pages        = {2235-2240},
  shortjournal = {IET Image Process.},
  title        = {MRGAN: A generative adversarial networks model for global mosaic removal},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Target tracker with masked discriminative correlation
filter. <em>IETIP</em>, <em>14</em>(10), 2227–2234. (<a
href="https://doi.org/10.1049/iet-ipr.2019.0881">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The discriminative correlation filter (DCF) method is widely used in target tracking due to its real-time performance. However, the computational efficiency of DCF results in boundary effect, which reduces the tracking accuracy in fast motion scene. Besides, background noise is always required to be carefully handled for they will cause trouble in scenes such as background clutter, occlusion, deformation etc. To address the two issues, this study proposes masked discriminative correlation filter, which uses mask to process DCF filter as well as target samples so as to suppress boundary effect and background noise. Experimental results on benchmark datasets show that the proposed tracker performs better than a series of benchmark trackers, and is superior to them in almost various scenes.},
  archive      = {J_IETIP},
  author       = {Hang Liu and Bodong Li},
  doi          = {10.1049/iet-ipr.2019.0881},
  journal      = {IET Image Processing},
  month        = {8},
  number       = {10},
  pages        = {2227-2234},
  shortjournal = {IET Image Process.},
  title        = {Target tracker with masked discriminative correlation filter},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Adaptive road detection method combining lane line and
obstacle boundary. <em>IETIP</em>, <em>14</em>(10), 2216–2226. (<a
href="https://doi.org/10.1049/iet-ipr.2018.6433">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In order to realise autonomous navigation of unmanned platforms in urban or off-road environments, it is crucial to study accurate, versatile and real-time road detection methods. This study proposes an adaptive road detection method that combines lane lines and obstacle boundaries, which can be applied to a variety of driving environments. Combining multi-channel threshold processing, it is robust to lane feature detection under various complex situations. Obstacle information extracted from the grid image constructed by 3D LIDAR point cloud is used for lane feature selection to avoid interference from pedestrians and vehicles. The proposed method makes use of adaptive sliding window for feature selection, and piecewise least squares method for road line fitting. Experimental results on dataset and in real-world environments show that the proposed method can overcome illumination changes, shadow occlusion, pedestrian, vehicle interference and so on in a variety of scenes. The proposed method has good enough efficiency, robustness and real-time performance.},
  archive      = {J_IETIP},
  author       = {Jing Li and Xinxin Shi and Junzheng Wang and Min Yan},
  doi          = {10.1049/iet-ipr.2018.6433},
  journal      = {IET Image Processing},
  month        = {8},
  number       = {10},
  pages        = {2216-2226},
  shortjournal = {IET Image Process.},
  title        = {Adaptive road detection method combining lane line and obstacle boundary},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Advanced lung cancer classification approach adopting
modified graph clustering and whale optimisation-based feature selection
technique accompanied by a hybrid ensemble classifier. <em>IETIP</em>,
<em>14</em>(10), 2204–2215. (<a
href="https://doi.org/10.1049/iet-ipr.2019.0178">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nowadays, lung cancer is the leading cause of cancer death in both men and women. The early detection of potentially cancerous cells is the best way to improve the patient&#39;s chances of survival. In the medical field, computed tomography (CT) is the best imaging technique and it is helpful for doctors to accurately find the cancerous cells. The authors propose an automatic approach to analyse and segment the lungs and classify each lung into normal or cancer. Initially, the CT lung image is pre-processed to remove noise. Then, they combine the histogram analysis with thresholding and morphological operations to segment and extract the lung regions. In feature extraction stage, the radiomic features of each lung image are extracted separately. Then to improve the classification accuracy, some of the optimum features are selected using modified graph clustering-based whale optimisation algorithm. Finally, the selected features are classified using ensemble classifiers such as support vector machine, K-nearest neighbour, and random forest. Experimental result demonstrates that the proposed method achieves better performance in terms of sensitivity, specificity, precision, recall, F -measure, and accuracy when compared with other state-of-art approaches.},
  archive      = {J_IETIP},
  author       = {Michael Mary Adline Priya and S. Joseph Jawhar},
  doi          = {10.1049/iet-ipr.2019.0178},
  journal      = {IET Image Processing},
  month        = {8},
  number       = {10},
  pages        = {2204-2215},
  shortjournal = {IET Image Process.},
  title        = {Advanced lung cancer classification approach adopting modified graph clustering and whale optimisation-based feature selection technique accompanied by a hybrid ensemble classifier},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Phase congruency and ODBTC based image retrieval.
<em>IETIP</em>, <em>14</em>(10), 2195–2203. (<a
href="https://doi.org/10.1049/iet-ipr.2019.1023">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this study, the authors investigate content based image retrieval (CBIR) using ordered-dither block truncation coding (ODBTC) and phase congruency feature (PCF). Relevant feature extraction plays a vital role for retrieving the image in CBIR. The unique reason to choose PCF with ODBTC is that it detects the edges and corners during variation of image while preserving image brightness and contrast. Combining the PCF and ODBTC features improves CBIR system usage in various visual data processing domains. Thus, yields a better CBIR system which assists in the reduction of storage space, decreases retrieval time and increases accuracy of the system. The precision and recall are used as performance metrics to evaluate the proposed method based on retrieval of relevant images. Extensive experimental results with Corel 1 K (1000 images), Corel 10 K (10000 images) and CALTECH 256 (30144 images) proves that the proposed method is more desirable than antecedent proposed CBIR system in terms of accuracy, precision and recall.},
  archive      = {J_IETIP},
  author       = {Shraddha Gupta and Sudhakar Modem and Vandana Vikas Thakre},
  doi          = {10.1049/iet-ipr.2019.1023},
  journal      = {IET Image Processing},
  month        = {8},
  number       = {10},
  pages        = {2195-2203},
  shortjournal = {IET Image Process.},
  title        = {Phase congruency and ODBTC based image retrieval},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Multi-frame image super resolution using spatially weighted
total variation regularisations. <em>IETIP</em>, <em>14</em>(10),
2187–2194. (<a href="https://doi.org/10.1049/iet-ipr.2019.0901">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image super resolution refers to a class of signal processing algorithms to post-process a captured image to obtain its high resolution version. Multi-frame super resolution synthesises high resolution image from multiple low resolution observations. Performance of super resolution algorithms are adversely affected by the noise present in the input images. To develop a noise robust multi-frame image super resolution, an objective function is formulated which contains a weighted data fidelity term and a regularisation term consisting of a bilateral total variation (BTV) term and structure tensor total variation (STV) term. Both BTV and STV are weighted appropriately in a per pixel basis in such a way that the BTV contributes more in smooth regions and STV contributes more on the edges. These terms ensure the continuity of edges and the smoothness of flat regions. An adaptive weighting scheme with the data fidelity term helps to select the reliable pixel alone in the reconstruction process. The proposed method is experimentally evaluated for its performance in real data and different types of noises.},
  archive      = {J_IETIP},
  author       = {Abdu Rahiman V and Sudhish N. George},
  doi          = {10.1049/iet-ipr.2019.0901},
  journal      = {IET Image Processing},
  month        = {8},
  number       = {10},
  pages        = {2187-2194},
  shortjournal = {IET Image Process.},
  title        = {Multi-frame image super resolution using spatially weighted total variation regularisations},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Hyperspectral image classification using three-dimensional
geometric moments. <em>IETIP</em>, <em>14</em>(10), 2175–2186. (<a
href="https://doi.org/10.1049/iet-ipr.2019.0603">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Exploitation of both spectral and spatial information in hyperspectral imagery is important for effective classification. Considering the cubical arrangement of data, the three-dimensional (3D) techniques could be effectively used to model hypespectral features. In this study, the 3D geometric moments are used to extract the rotation, scale, and translation invariant features. Unlike 2D moments, the 3D moments characterise the joint spectral–spatial properties. A classification method is proposed that uses the features derived from 3D geometric moments without vectorising or changing the original structure of the raw hyperspectral image. Unlike many other methods, the new method does not need a separate step for spectral feature extraction or dimensionality reduction. The moments are computed on the raw image that generate a comprehensive and smaller feature set. The experimental results from five benchmark airborne hyperspectral images demonstrate that the 3D moment based method yields good classification results better or comparable to several state-of-the-art methods.},
  archive      = {J_IETIP},
  author       = {Brajesh Kumar},
  doi          = {10.1049/iet-ipr.2019.0603},
  journal      = {IET Image Processing},
  month        = {8},
  number       = {10},
  pages        = {2175-2186},
  shortjournal = {IET Image Process.},
  title        = {Hyperspectral image classification using three-dimensional geometric moments},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Feature encoding with hybrid heterogeneous structure model
for image classification. <em>IETIP</em>, <em>14</em>(10), 2166–2174.
(<a href="https://doi.org/10.1049/iet-ipr.2019.0719">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the standard bag-of-visual-words model, the relationship between visual words and geometric structure information embedding in Voronoi cells is important for expressing the topology of the feature space. However, this information is usually ignored by recent works. To overcome it, the authors proposed a hybrid heterogeneous structure model (HHSM), where local hyperspheres and local structure subspaces are applied to simulate the intrinsic structure of the feature space. Firstly, the local hypersphere is formed by choosing some links between parts of visual words, with the use of a proposed decision strategy derived from k -dense neighbour algorithm. In order to capture the geometric structure information around the visual word, they then construct the local structure subspace with the transformed PCA principal vectors of the visual features within a Voronoi cell. Finally, this study introduces a novel feature encoding method based on the HHSM. Experiments are conducted on 15-Scenes, Pascal VOC2007, Caltech101, Caltech256 and MIT Indoor 67 datasets, which include 4485, 9963, 9146, 30607 and 15620 images, respectively. The results demonstrate the effectiveness of the proposed method in improving the accuracy of the classification. In addition, the proposed method achieves comparable performance when combined with CNN local features.},
  archive      = {J_IETIP},
  author       = {Zhihang Ji and Yan Yang and Fan Wang and Lijuan Xu and Xiaopeng Hu},
  doi          = {10.1049/iet-ipr.2019.0719},
  journal      = {IET Image Processing},
  month        = {8},
  number       = {10},
  pages        = {2166-2174},
  shortjournal = {IET Image Process.},
  title        = {Feature encoding with hybrid heterogeneous structure model for image classification},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). 1D representation of laplacian eigenmaps and dual k-nearest
neighbours for unified video coding. <em>IETIP</em>, <em>14</em>(10),
2156–2165. (<a href="https://doi.org/10.1049/iet-ipr.2019.1119">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study proposes a framework of video coding based on Laplacian eigenmaps (LEM) and its related embedding and reconstruction algorithm (ERA). Firstly, a one-dimensional (1D) representation of LEM is adopted to achieve an extremely low bit per pixel (BPP). Secondly, dual k -nearest neighbours, which keeps neighbour relationships both in high-dimensional data space and low-dimensional representation space and overcomes the disadvantage of classical non-linear dimensionality reduction methods which cannot preserve the neighbour properties in both of the spaces, based ERA of LEM is employed to gain extraordinarily high peak-signal-to-noise ratio (PSNR). Thirdly, a unified framework of video coding is fit for intra-frame, inter-frame and multi-view video coding. Finally, it is evaluated by simulation experiments that, in the situation of low bitrate transmission, the proposed method can attain better performance of BPP and PSNR than that of the state-of-the-art methods, such as highly efficient video coding.},
  archive      = {J_IETIP},
  author       = {Honggui Li},
  doi          = {10.1049/iet-ipr.2019.1119},
  journal      = {IET Image Processing},
  month        = {8},
  number       = {10},
  pages        = {2156-2165},
  shortjournal = {IET Image Process.},
  title        = {1D representation of laplacian eigenmaps and dual k-nearest neighbours for unified video coding},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Sonar image mosaic based on a new feature matching method.
<em>IETIP</em>, <em>14</em>(10), 2149–2155. (<a
href="https://doi.org/10.1049/iet-ipr.2019.0695">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sonar images are valuable in exploring underwater environmental information. As these images are generally limited by the viewing angle, sonar image mosaicking becomes an important research topic. By combining several frames consecutively acquired while the underwater vehicle is manoeuvring, an image with a wider view can be obtained. This study presents a fast sonar image mosaicking approach consisting of denoising, feature extraction, initial matching, splicing, and optimisation. Based on the Euclidean distance between initially matched points and dip angle of connection line, poorly matched feature point pairs are removed to avoid false matching. This way, the success rate of image mosaicking and quality of the resulting mosaicking is effectively improved.},
  archive      = {J_IETIP},
  author       = {Zhijie Tang and Gaoqian Ma and Jiaqi Lu and Zhen Wang and Bin Fu and Yijie Wang},
  doi          = {10.1049/iet-ipr.2019.0695},
  journal      = {IET Image Processing},
  month        = {8},
  number       = {10},
  pages        = {2149-2155},
  shortjournal = {IET Image Process.},
  title        = {Sonar image mosaic based on a new feature matching method},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Visual saliency based global–local feature representation
for skin cancer classification. <em>IETIP</em>, <em>14</em>(10),
2140–2148. (<a href="https://doi.org/10.1049/iet-ipr.2019.1018">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rapid increase in the cases of deadly skin cancer, the classification on different types of skin cancer has been emerging as one of the most significant issues in the field of medical image. Several approaches have been proposed to help in diagnosing the categories of the skin lesions by means of traditional features or leveraging the widely used deep learning models. However, there are lack of the integrated frameworks to combine the hand-crafted traditional features and the deep Conv-features. Furthermore, the effective way to extract global and local features is also conducive to distinguish the specific lesions from normal skin. Hence, in this study, the authors present an integrated model to acquire more representative global–local features including the traditional local binary pattern features and deep Conv-features. In addition, several fusion strategies have conducted on the Global-DNN and Local-DNN for better performance. In order to extract more explicit features from the specific lesion areas, a target segmentation method based on visual saliency detection is employed to eliminate the background interference. Experimental results on ISIC-2017 skin cancer dataset demonstrate that the proposed Global-DNN and Global-Local models can obtain more effective feature representation which achieve outperformed results for skin cancer classification.},
  archive      = {J_IETIP},
  author       = {Feng Xiao and Qiuxia Wu},
  doi          = {10.1049/iet-ipr.2019.1018},
  journal      = {IET Image Processing},
  month        = {8},
  number       = {10},
  pages        = {2140-2148},
  shortjournal = {IET Image Process.},
  title        = {Visual saliency based global–local feature representation for skin cancer classification},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Bi-dictionary learning model for medical image
reconstruction from undersampled data. <em>IETIP</em>, <em>14</em>(10),
2130–2139. (<a href="https://doi.org/10.1049/iet-ipr.2019.0886">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, dictionary learning has shown to be an efficient tool in recovering images from their degraded, damaged or incomplete version. Especially, for medical images that contain significant details and characteristics. In this work, the authors are interested in this unsupervised learning technique for discovering and visualising the underlying structure of a medical image. Therefore, an adaptive bi-dictionary learning model for recovering magnetic resonance (MR) image from undersampled measurements is introduced. The proposed model learns two dictionaries, one over the underlying image and the other over its sparse gradient. Hence, the algorithm minimises a linear combination of three terms corresponding to the least-squares data fitting, dictionary learning over the pixel domain, and gradient-based dictionary. Numerically, experimental results on several MR images demonstrate that the proposed bi-dictionary framework can improve reconstruction accuracy over other methods.},
  archive      = {J_IETIP},
  author       = {Souad Mohaoui and Abdelilah Hakim and Said Raghay},
  doi          = {10.1049/iet-ipr.2019.0886},
  journal      = {IET Image Processing},
  month        = {8},
  number       = {10},
  pages        = {2130-2139},
  shortjournal = {IET Image Process.},
  title        = {Bi-dictionary learning model for medical image reconstruction from undersampled data},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Use of gradient and normal vectors for face recognition.
<em>IETIP</em>, <em>14</em>(10), 2121–2129. (<a
href="https://doi.org/10.1049/iet-ipr.2019.1128">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The main objective of this study is to compare face recognition accuracies in the case when the grey levels in each pixel of the face images are replaced by the gradient and the surface normal vectors. Extensive information is provided to explain the differences between the gradient and the proposed features. Some well-known face recognition methods, such as common vector approach (CVA), discriminative CVA, and support vector machines are applied to the well-known databases of AR and Yale for comparison other than introducing a new method what the authors called as Sum of Pixel Slope Similarities Approach. The authors’ experimental results are compared with the state-of-the-art methods to the best of their knowledge. In conclusion, their results imply that using the surface normal vectors rather than the gradient vectors in each pixel with no additional work on their elements gives better recognition rates.},
  archive      = {J_IETIP},
  author       = {Mehmet Koc and Semih Ergin and Mehmet Bilginer Gülmezoğlu and Rifat Edizkan and Atalay Barkana},
  doi          = {10.1049/iet-ipr.2019.1128},
  journal      = {IET Image Processing},
  month        = {8},
  number       = {10},
  pages        = {2121-2129},
  shortjournal = {IET Image Process.},
  title        = {Use of gradient and normal vectors for face recognition},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Rapid region analysis for classification. <em>IETIP</em>,
<em>14</em>(10), 2110–2120. (<a
href="https://doi.org/10.1049/iet-ipr.2019.0273">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The authors describe and evaluate a method that detects ridges (symmetric axes) in an Euclidean distance map. The method detects ridge-pixels with a local-maxima search using only relational operations and has therefore minimal complexity. The resulting ridges exhibit a height profile that is suitable for region abstraction by means of simple parameterisation. The method is firstly evaluated on artificial stimuli with systematic shape variations using spatial jitter and contour fragmentation. Then it is mentioned, how the method can be used for developing region abstractions. Such abstractions have been already exploited in two tasks, hand-written digit identification (MNIST database) and image classification (satellite images and images of urban/natural landscapes); the classification results of those systems are competitive.},
  archive      = {J_IETIP},
  author       = {Christoph Rasche},
  doi          = {10.1049/iet-ipr.2019.0273},
  journal      = {IET Image Processing},
  month        = {8},
  number       = {10},
  pages        = {2110-2120},
  shortjournal = {IET Image Process.},
  title        = {Rapid region analysis for classification},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Optimisation of both classifier and fusion based feature set
for static american sign language recognition. <em>IETIP</em>,
<em>14</em>(10), 2101–2109. (<a
href="https://doi.org/10.1049/iet-ipr.2019.0195">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sign language recognition becomes a popular research field in human–computer interaction. Attention on hand signal analysis helps to make easy communication among computer and human for information sharing. Major focus of the gesture recognition system is to identify and recognise various gestures, by a computer. This study introduces optimisation of both classifier and feature set for static American sign language recognition. Initially, the hand part is segmented from other parts of the image through effective edge and skin colour detection. Thereafter, robust features are obtained using discrete cosine transform, Zernike moment, scale-invariant feature transform, speeded-up robust features, histogram of oriented gradients and binary object features from the segmented hand image. From these extracted features, an optimal feature set is selected by social ski driver optimisation algorithm. Deep Elman recurrent neural network classifier is then introduced for recognition purpose. Optimisation is performed on feature sets, derived by fusion of features obtained from the above methods, based on precision, accuracy, F -measure and recall. Finally, optimised feature set and best classifier are used to recognise the hand gesture for classification purpose. The performance of this proposed method is evaluated and compared with existing literature.},
  archive      = {J_IETIP},
  author       = {Arun C. and R. Gopikakumari},
  doi          = {10.1049/iet-ipr.2019.0195},
  journal      = {IET Image Processing},
  month        = {8},
  number       = {10},
  pages        = {2101-2109},
  shortjournal = {IET Image Process.},
  title        = {Optimisation of both classifier and fusion based feature set for static american sign language recognition},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Image copy-move forgery detection algorithm based on ORB and
novel similarity metric. <em>IETIP</em>, <em>14</em>(10), 2092–2100. (<a
href="https://doi.org/10.1049/iet-ipr.2019.1145">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image forgery poses a serious threat in electric power, medicine and other fields. Relevant departments need to pay a great price to identify the authenticity of the image. For traditional copy-move forgery image detection, the existing methods have at least two problems: low robustness and poor matching caused by a low number of feature points. Here, a novel similarity metric combining cosine and Jaccard is proposed to improve feature matching, which combines with oriented features from accelerated segment test and rotated binary robust independent elementary features (ORB) feature extraction to realise effective and fast image forgery detection. First, the image is divided into overlapping blocks, and ORB is used to extract the feature points of each image block to obtain the text information. Second, the novel similarity metric is used to calculate similarity and match the text. Finally, two image blocks with the highest similarity are located. The experimental results show that, on the one hand, ORB can greatly lessen detection time. On the other hand, the novel similarity metric can improve the poor matching caused by the small number of feature points. Combining the two methods can exhibit high robustness to translation, rotation, noise, illumination and JPEG compression.},
  archive      = {J_IETIP},
  author       = {Xiuxia Tian and Guoshuai Zhou and Man Xu},
  doi          = {10.1049/iet-ipr.2019.1145},
  journal      = {IET Image Processing},
  month        = {8},
  number       = {10},
  pages        = {2092-2100},
  shortjournal = {IET Image Process.},
  title        = {Image copy-move forgery detection algorithm based on ORB and novel similarity metric},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Image multi-encryption architecture based on hybrid
keystream sequence interspersed with haar discrete wavelet transform.
<em>IETIP</em>, <em>14</em>(10), 2081–2091. (<a
href="https://doi.org/10.1049/iet-ipr.2019.0991">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A novel image multi-encryption architecture based on hybrid keystream sequence generated by a single hyperchaotic system and Haar discrete wavelet transform (HDWT) is proposed. The architecture consists of a pre-cipher stage, first encryption operation, Haar discrete wavelet decomposition stage and a second encryption operation. In the pre-cipher stage, the algorithm applies two-level pixel position permutation of the image. The first encryption operation is accomplished by diffusing the pixel&#39;s numbers with keystream sequence generated by a sequence generator SG-1. The resulting cipher image is decomposed using two-dimensional HDWT decomposition technique. The decomposed image is further encrypted through operation between keystream sequence generated by sequence generator SG-2 and bytes of the decomposed image which are selected with the aid of a novel pseudo – 4 bit Boolean truth table-based byte selection mechanism. SG-1 and SG-2 are special cases of the hyperchaotic system and are evolved online by separately nullifying selected parameters of the hyperchaotic system. The novelty of the proposed architecture lies in the possibility of increasing the number of sequence generators, which can result in exponentially huge key space and fast encryption speed. A comprehensive complexity analysis of performance, security and robustness to attacks, confirmed the feasibility of the architecture.},
  archive      = {J_IETIP},
  author       = {Edwin A. Umoh and Ogechukwu N. Iloanusi and Uche A. Nnolim},
  doi          = {10.1049/iet-ipr.2019.0991},
  journal      = {IET Image Processing},
  month        = {8},
  number       = {10},
  pages        = {2081-2091},
  shortjournal = {IET Image Process.},
  title        = {Image multi-encryption architecture based on hybrid keystream sequence interspersed with haar discrete wavelet transform},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Block cosparsity overcomplete learning transform image
segmentation algorithm based on burr model. <em>IETIP</em>,
<em>14</em>(10), 2074–2080. (<a
href="https://doi.org/10.1049/iet-ipr.2019.1212">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To improve the performance of the high-voltage copper contact burr image segmentation, a block cosparsity overcomplete learning transform image segmentation algorithm based on burr model is proposed in this study. In this study, k -means clustering method is used to initialise the clustering results; the authors found the algorithm is very effective for burr image processing in production process and the sparse overcomplete transform matrix is initialised by discrete cosine transform. The algorithm is expressed by a set of transforms. When the set of transforms is fixed, the penalty is corresponding to the condition number. A new burr model is proposed in this study. The parameters of the burr are the factors on infection of the sparse-level constant and the regularisation coefficient of the block cosparsity overcomplete learning transform algorithm. The algorithm divides all pixels into several groups. To evaluate the performance of the model, a large number of experiments have been carried out, and three image segmentation evaluation criterions have been used to evaluate the effectiveness of the algorithm. Experimental results show that this method is excellent in retaining weak edge information and avoiding the influence of three-dimensional structure compared with other algorithms.},
  archive      = {J_IETIP},
  author       = {Lili Han and Shujuan Li and Pengxin Ren and Dingdan Xue},
  doi          = {10.1049/iet-ipr.2019.1212},
  journal      = {IET Image Processing},
  month        = {8},
  number       = {10},
  pages        = {2074-2080},
  shortjournal = {IET Image Process.},
  title        = {Block cosparsity overcomplete learning transform image segmentation algorithm based on burr model},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Comparative analysis of reversible data hiding schemes.
<em>IETIP</em>, <em>14</em>(10), 2064–2073. (<a
href="https://doi.org/10.1049/iet-ipr.2019.1066">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The data hiding method embeds the data into covers such as video, audio, and images, which are used for integrity authentication, media protection, communication covert, copyright protection etc. In this work, several reversible data hiding (RDH) algorithms are analysed. Here, RDH algorithms are classified into six categories. They are histogram shifting centred RDH, code division multiplexing-based RDH, compression-based RDH, contrast enhancement with RDH, and expansion-based RDH, and RDH for encrypted images. In RDH,no information is lost when the message is recovered as the message is embedded into the host image. The embedded message is extracted using an extraction procedure. This study critically reviews the technological advancement for the past decade on the aspects of classification, pre-processing, feature extraction, and analysis technique. The current trend in RDH is to insert confidential data into a video by using RDH in the encoded area that has a wide selection of applications on the confidential communication inside the area of cybersecurity.},
  archive      = {J_IETIP},
  author       = {Asha Jose and Kamalraj Subramaniam},
  doi          = {10.1049/iet-ipr.2019.1066},
  journal      = {IET Image Processing},
  month        = {8},
  number       = {10},
  pages        = {2064-2073},
  shortjournal = {IET Image Process.},
  title        = {Comparative analysis of reversible data hiding schemes},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Optimised robust watermarking technique using CKGSA in
DCT-SVD domain. <em>IETIP</em>, <em>14</em>(10), 2052–2063. (<a
href="https://doi.org/10.1049/iet-ipr.2019.1059">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Digital watermarking embeds a watermark to minimise the problem of illegal copying and disseminating multimedia contents. However, the existing techniques do not maintain the imperceptibility and robustness simultaneously. To achieve the same, this study proposes an optimised robust watermarking technique using chaotic kbest gravitational search algorithm. The chaotic kbest gravitational search algorithm is used to obtain the optimal values of embedding factors. The efficacy of the proposed technique has been experimented on a standard images and compared with the six recent state-of-the-art techniques in terms of imperceptibility and robustness. The experimental results validate that the proposed technique outperforms the other considered techniques.},
  archive      = {J_IETIP},
  author       = {Roop Singh and Alaknanda Ashok and Mukesh Saraswat},
  doi          = {10.1049/iet-ipr.2019.1059},
  journal      = {IET Image Processing},
  month        = {8},
  number       = {10},
  pages        = {2052-2063},
  shortjournal = {IET Image Process.},
  title        = {Optimised robust watermarking technique using CKGSA in DCT-SVD domain},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Performance evaluation of single and cross-dimensional
feature detection and description. <em>IETIP</em>, <em>14</em>(10),
2035–2051. (<a href="https://doi.org/10.1049/iet-ipr.2019.1523">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Three-dimensional (3D) local feature detection and description techniques are widely used for object registration and recognition applications. Although several evaluations of 3D local feature detection and description methods have already been published, these are constrained in a single dimensional scheme, i.e. either 3D or 2D methods that are applied onto multiple projections of the 3D data. However, cross-dimensional (mixed 2D and 3D) feature detection and description are yet to be investigated. Here, the authors evaluated the performance of both single and cross-dimensional feature detection and description methods on several 3D data sets and demonstrated the superiority of cross-dimensional over single-dimensional schemes.},
  archive      = {J_IETIP},
  author       = {Odysseas Kechagias-Stamatis and Nabil Aouf and Mark A. Richardson},
  doi          = {10.1049/iet-ipr.2019.1523},
  journal      = {IET Image Processing},
  month        = {8},
  number       = {10},
  pages        = {2035-2051},
  shortjournal = {IET Image Process.},
  title        = {Performance evaluation of single and cross-dimensional feature detection and description},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Automated fish cage net inspection using image processing
techniques. <em>IETIP</em>, <em>14</em>(10), 2028–2034. (<a
href="https://doi.org/10.1049/iet-ipr.2019.1667">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fish-cage dysfunction in aquaculture installations can trigger significant negative consequences affecting the operational costs. Low oxygen levels, due to excessive fooling&#39;s, leads to decrease growth performance, and feed efficiency. Therefore, frequent periodic inspection of fish-cage nets is required, but this task can become quite expensive with the traditional means of employing professional divers that perform visual inspections at regular time intervals. The modern trend in aquaculture is to take advantage of IT technologies with the use of a small-sized, low-cost autonomous underwater vehicle, permanently residing within a fish cage and performing regular video inspection of the infrastructure for the entire net surface. In this study, we explore specialised image processing schemes to detect net holes of multiple area size and shape. These techniques are designed with the vision to provide robust solutions that take advantage of either global or local image structures to provide the efficient inspection of multiple net holes.},
  archive      = {J_IETIP},
  author       = {Stavros Paspalakis and Konstantia Moirogiorgou and Nikos Papandroulakis and George Giakos and Michalis Zervakis},
  doi          = {10.1049/iet-ipr.2019.1667},
  journal      = {IET Image Processing},
  month        = {8},
  number       = {10},
  pages        = {2028-2034},
  shortjournal = {IET Image Process.},
  title        = {Automated fish cage net inspection using image processing techniques},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Where to look: A collection of methods forMAV heading
correction in underground tunnels. <em>IETIP</em>, <em>14</em>(10),
2020–2027. (<a href="https://doi.org/10.1049/iet-ipr.2019.1423">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Degraded Subterranean environments are an attractive case for miniature aerial vehicles, since there is a constant need to increase the safety operations in underground mines. The starting point for integrating aerial vehicles in the mining process is the capability to reliably navigate along tunnels. Inspired by recent advancements, this paper presents a collection of different, experimentally verified, methods tackling the problem of MAVs heading regulation while navigating in dark and textureless tunnel areas. More specifically, four different methods are presented in this work with the common goal to identify open space in the tunnel and align the MAV heading using either visual sensor in methods a) single image depth estimation, b) darkness contour detection, c) Convolutional Neural Network (CNN) regression and 2D Lidar sensor in method d) range geometry. For the works a)-c) the dark scene in the middle of the tunnel is considered as open space and is processed and converted to yaw rate command, while d) examines the geometry of the range measurements to calculate the yaw rate command. Experimental results from real underground tunnel demonstrate the performance of the methods in the field, while setting the ground for further developments in the aerial robotics community.},
  archive      = {J_IETIP},
  author       = {Christoforos Kanellakis and Sina Sharif Mansouri and Miguel Castaño and Petros Karvelis and Dariusz Kominiak and G. Nikolakopoulos},
  doi          = {10.1049/iet-ipr.2019.1423},
  journal      = {IET Image Processing},
  month        = {8},
  number       = {10},
  pages        = {2020-2027},
  shortjournal = {IET Image Process.},
  title        = {Where to look: A collection of methods forMAV heading correction in underground tunnels},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Combination of multi-scale and residual learning in deep CNN
for image denoising. <em>IETIP</em>, <em>14</em>(10), 2013–2019. (<a
href="https://doi.org/10.1049/iet-ipr.2019.1386">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To better restore a clean image from a noise observation under high noise levels, the authors propose an image denoising network based on the combination of multi-scale and residual learning. Instead of using filters with different large sizes in traditional multi-scale schemes, they arrange multi-layer convolutions with the filters of the same size to speed up the model. Some dilated convolutions of different rates are combined with the common convolutions to enrich the extracted features in multi-layer convolutions. Furthermore, they cascade the multi-layer convolutions with residual blocks to improve the performance of image denoising. Their extensive evaluations on several challenging datasets demonstrate that the proposed model outperforms the state-of-art methods under all different noise levels in terms of peak signal-to-noise ratio, and the visual effects achieved by the proposed model are also better than the competing methods.},
  archive      = {J_IETIP},
  author       = {Haiying Xia and Fuyu Zhu and Haisheng Li and Shuxiang Song and Xiangwei Mou},
  doi          = {10.1049/iet-ipr.2019.1386},
  journal      = {IET Image Processing},
  month        = {8},
  number       = {10},
  pages        = {2013-2019},
  shortjournal = {IET Image Process.},
  title        = {Combination of multi-scale and residual learning in deep CNN for image denoising},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Evaluation of nano-filler dispersion quality in polymeric
films with binary feature characteristics and fractal analysis.
<em>IETIP</em>, <em>14</em>(10), 2006–2012. (<a
href="https://doi.org/10.1049/iet-ipr.2019.1512">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study investigates the use of binary features and fractal dimension analysis to evaluate the dispersion quality of nanofillers in thin polymeric films by using light microscopy images. For this purpose, polymeric films were cast with the inclusion of various montmorillonite (MMT) nanofiller amounts. Then the light microscopy images were captured from the polymeric films then preprocessed for the evaluation. Thresholding process was applied to the obtained images for each nanofiller percentage level. The obtained binary level images were used in the feature extraction process with binary statistics and fractal dimension. Thermogravimetric analysis (TGA) was used to evaluate the flame resist behaviour of polymeric films based on the dispersion quality of nanofillers. The samples with various nanofiller contents were tested using the image processing method and the results were all compared with the TGA results. The results obtained by the feature extraction process and TGA, about the dispersion quality of nanofillers, were all in good agreement.},
  archive      = {J_IETIP},
  author       = {Kazim Yildiz and Zehra Yildiz},
  doi          = {10.1049/iet-ipr.2019.1512},
  journal      = {IET Image Processing},
  month        = {8},
  number       = {10},
  pages        = {2006-2012},
  shortjournal = {IET Image Process.},
  title        = {Evaluation of nano-filler dispersion quality in polymeric films with binary feature characteristics and fractal analysis},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Smart feature extraction and classification of hyperspectral
images based on convolutional neural networks. <em>IETIP</em>,
<em>14</em>(10), 1999–2005. (<a
href="https://doi.org/10.1049/iet-ipr.2019.1282">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hyperspectral satellite imagery (HSI) is an advanced technology for object detection because it provides a large amount of information. Thus, the classification of HSIs is very complicated, so the methods of reducing spectral or spatial information generally degrade the quality of classification. In order to solve this problem and guarantee faster and more efficient processing, we propose a smart feature extraction (SFE) and classification by convolutional neural network (2D-CNN) method made up of two parts. The first consists in reducing spectral information by a probabilistic method based on the Softmax function. The second is classification by processing batches of data in the proposed CNN network. The method was tested on two public hyperspectral images (Indian Pines and SalinasA) to prove its effectiveness in increasing classification accuracy and reducing computing time.},
  archive      = {J_IETIP},
  author       = {Maissa Hamouda and Karim Saheb Ettabaa and Med Salim Bouhlel},
  doi          = {10.1049/iet-ipr.2019.1282},
  journal      = {IET Image Processing},
  month        = {8},
  number       = {10},
  pages        = {1999-2005},
  shortjournal = {IET Image Process.},
  title        = {Smart feature extraction and classification of hyperspectral images based on convolutional neural networks},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Deep learning-based automated detection of human knee
joint’s synovial fluid from magnetic resonance images with transfer
learning. <em>IETIP</em>, <em>14</em>(10), 1990–1998. (<a
href="https://doi.org/10.1049/iet-ipr.2019.1646">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As an analytic tool in medicine, particularly in radiology, deep learning is gaining much attention and opening a new way for disease diagnosis. Nonetheless, it is rather challenging to acquire large-scale detailed labelled datasets in the field of medical imaging. In fact, transfer learning provides a possible way to resolve this issue to a certain extent such that the parameter learning of a neural network starts with its pre-trained weights learned from a large-scale dataset of certain similar task, and fine-tunes on a small comprehensively annotated dataset for the particular target task. The main aim of this study is to apply the deep learning model to detect the synovial fluid of human knee joint from magnetic resonance images. A specialized convolutional neural network architecture is proposed for automated detection of human knee joint&#39;s synovial fluid. Two independent datasets are used in the training, development, and evaluation of the proposed model. It is demonstrated by the experimental results that the proposed model obtains high sensitivity, specificity, precision, and accuracy to the detection of human knee joint&#39;s synovial fluid. As a result, this proposed approach provides a novel and feasible way for automating and expediting the synovial fluid analysis.},
  archive      = {J_IETIP},
  author       = {Imran Iqbal and Ghazala Shahzad and Nida Rafiq and Ghulam Mustafa and Jinwen Ma},
  doi          = {10.1049/iet-ipr.2019.1646},
  journal      = {IET Image Processing},
  month        = {8},
  number       = {10},
  pages        = {1990-1998},
  shortjournal = {IET Image Process.},
  title        = {Deep learning-based automated detection of human knee joint&#39;s synovial fluid from magnetic resonance images with transfer learning},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Unified deep learning approach for prediction of parkinson’s
disease. <em>IETIP</em>, <em>14</em>(10), 1980–1989. (<a
href="https://doi.org/10.1049/iet-ipr.2019.1526">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The study presents a novel approach, based on deep learning, for diagnosis of Parkinson&#39;s disease through medical imaging. The approach includes analysis and use of the knowledge extracted by deep convolutional and recurrent neural networks when trained with medical images, such as magnetic resonance images and dopamine transporters scans. Internal representations of the trained DNNs constitute the extracted knowledge which is used in a transfer learning and domain adaptation manner, so as to create a unified framework for prediction of Parkinson&#39;s across different medical environments. A large experimental study is presented illustrating the ability of the proposed approach to effectively predict Parkinson&#39;s, using different medical image sets from real environments.},
  archive      = {J_IETIP},
  author       = {James Wingate and Ilianna Kollia and Luc Bidaut and Stefanos Kollias},
  doi          = {10.1049/iet-ipr.2019.1526},
  journal      = {IET Image Processing},
  month        = {8},
  number       = {10},
  pages        = {1980-1989},
  shortjournal = {IET Image Process.},
  title        = {Unified deep learning approach for prediction of parkinson&#39;s disease},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Automatic grading of brain tumours using LSTM neural
networks on magnetic resonance spectroscopy signals. <em>IETIP</em>,
<em>14</em>(10), 1967–1979. (<a
href="https://doi.org/10.1049/iet-ipr.2019.1416">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Brain tumours have increased rapidly in recent years as in other tumour types. Therefore, early and accurate diagnosis of brain tumour is vital for treatment. Magnetic resonance imaging (MRI) and histopathological assessments are the most common methods used in the detection of brain tumours. The research studies on non-invasive imaging methods such as MRI and magnetic resonance spectroscopy (MRS) have become widespread in recent years for brain tumour detection. In this study, a computer-assisted method is proposed for automatic grading of brain tumours on MRS signals. The classification of brain tumours with different grades is performed using long short term memory (LSTM) neural networks. In addition, additional features from MRS signals based on spectral entropy and instantaneous frequency are extracted. As a result of the experimental studies on the international MRS database (INTERPRET), it is seen that grading is achieved using the proposed method with average accuracy of 98.20%, sensitivity of 100%, and specificity of 97.53% performance results in three test studies carried out for the classification of brain tumour. Furthermore, in the grading of brain tumours using the proposed method, the average area under of the receiver operating characteristic curve is measured with high performance of 0.9936.},
  archive      = {J_IETIP},
  author       = {Emre Dandil and Ali Biçer},
  doi          = {10.1049/iet-ipr.2019.1416},
  journal      = {IET Image Processing},
  month        = {8},
  number       = {10},
  pages        = {1967-1979},
  shortjournal = {IET Image Process.},
  title        = {Automatic grading of brain tumours using LSTM neural networks on magnetic resonance spectroscopy signals},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Development of an intelligent CAD system for mass detection
in mammographic images. <em>IETIP</em>, <em>14</em>(10), 1960–1966. (<a
href="https://doi.org/10.1049/iet-ipr.2019.1295">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mammography is a very useful tool to diagnose breast cancer in early stages when it is easier to treat. There are two types of evidence that radiologists look for in a mammogram, calcifications and the existence of masses. In this study, an intelligent computer-aided diagnosis system is proposed for the detection of masses in mammographic images regardless of their nature. The proposed method uses a combination of extended maxima transformations, having different threshold values, in order to find suitable internal and external markers for a marker-based watershed segmentation. After segmentation, a two-stage classifier is used to distinguish the masses better from the healthy breast tissue. A feature vector based mainly on contrast and texture features is calculated and two alternative approaches, a Bayesian classifier and a support vector machine (SVM) with Gaussian kernel function, are implemented for further reduction of the false positive areas. The system was evaluated using the data from two online databases. Specifically, 73 mammographic images from the new curated breast imaging subset of digital database for screening mammography (CBIS-DDSM) database and all the mammographic images that contain masses from the mini-mammographic image analysis society (MIAS) database were used. The overall sensitivity, in both datasets, was near 80% when the Bayesian classifier was used and above 85% when the SVM was applied.},
  archive      = {J_IETIP},
  author       = {Theofilos Andreadis and Christodoulos Emmanouilidis and Stefanos Goumas and Dimitrios Koulouriotis},
  doi          = {10.1049/iet-ipr.2019.1295},
  journal      = {IET Image Processing},
  month        = {8},
  number       = {10},
  pages        = {1960-1966},
  shortjournal = {IET Image Process.},
  title        = {Development of an intelligent CAD system for mass detection in mammographic images},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Hybrid deep learning and machine learning approach for
passive image forensic. <em>IETIP</em>, <em>14</em>(10), 1952–1959. (<a
href="https://doi.org/10.1049/iet-ipr.2019.1291">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image forgery detection using traditional algorithms takes much time to find forgeries. The new emerging methods for the detection of image forgery use a deep neural network algorithm. A hybrid deep learning (DL) and machine learning-based approach is used in this study for passive image forgery detection. A DL algorithm classifies images into the forged and not forged categories, whereas colour illumination localises forgery. The simulated results are compared to other algorithms on public datasets. The simulated results achieved 99% accuracy for CASIA1.0, 98% accuracy for CASIA2.0, 98% accuracy for BSDS300, 97% accuracy for DVMM, and 99% accuracy for CMFD image manipulation dataset.},
  archive      = {J_IETIP},
  author       = {Abhishek Thakur and Neeru Jindal},
  doi          = {10.1049/iet-ipr.2019.1291},
  journal      = {IET Image Processing},
  month        = {8},
  number       = {10},
  pages        = {1952-1959},
  shortjournal = {IET Image Process.},
  title        = {Hybrid deep learning and machine learning approach for passive image forensic},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020b). Corrigendum: Speckle suppression in medical ultrasound
images through schurdecomposition. <em>IETIP</em>, <em>14</em>(9), 1948.
(<a href="https://doi.org/10.1049/iet-ipr.2020.0352">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_IETIP},
  doi          = {10.1049/iet-ipr.2020.0352},
  journal      = {IET Image Processing},
  month        = {7},
  number       = {9},
  pages        = {1948},
  shortjournal = {IET Image Process.},
  title        = {Corrigendum: Speckle suppression in medical ultrasound images through schurdecomposition},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Reweighted infrared patch image model for small target
detection based on non-convex ℒp-norm minimisation and TV
regularisation. <em>IETIP</em>, <em>14</em>(9), 1937–1947. (<a
href="https://doi.org/10.1049/iet-ipr.2019.1660">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Infrared small target detection in a complex background has always been a challenging task in an infrared detection system. The existing methods based on the infrared patch image (IPI) model have achieved a good result but are sensitive to the complex background. So, to effectively detect the small target in complex background, model based on the reweighted IPI model along with total variance (TV) is proposed in this study. In this study firstly, the problem of using nuclear norm minimisation (NNM) in the existing IPI-based methods is discussed, and a solution is proposed by replacing the existing NNM with the ℒp- norm minimisation of singular values in the existing IPI methods. Secondly, a TV regularisation term is added to the background patch image to suppress the noise and preserve the strong edges in the background. The proposed method is solved by the alternating direction method of the multiplier. The robustness of the proposed method is validated by experimenting with the large dataset of real infrared images as well as the synthetic images. The proposed method not only has good background suppression ability, but also enhances and detect the target well in comparisons with the other baseline methods.},
  archive      = {J_IETIP},
  author       = {Sur Singh Rawat and Sashi Kant Verma and Yatindra Kumar},
  doi          = {10.1049/iet-ipr.2019.1660},
  journal      = {IET Image Processing},
  month        = {7},
  number       = {9},
  pages        = {1937-1947},
  shortjournal = {IET Image Process.},
  title        = {Reweighted infrared patch image model for small target detection based on non-convex ℒp-norm minimisation and TV regularisation},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Novel fuzzy clustering-based bias field correction technique
for brain magnetic resonance images. <em>IETIP</em>, <em>14</em>(9),
1929–1936. (<a href="https://doi.org/10.1049/iet-ipr.2019.0942">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Bias field correction is an essential pre-processing requirement for brain tissue segmentation task. Authentic brain tissue regions are highly useful for classification and detection of abnormalities. A poor resolution magnetic resonance (MR) image is produced with irregularities in structure, abnormalities in the intensity distribution and noise during the acquisition procedure. The existing bias field correction methods do not consider the spatial information. Further, the problem of equidistant pixels while clustering is not addressed. These problems lead to poor segmentation accuracy. To solve these problems, the authors suggest a novel biased fuzzy clustering technique for the problem on hand. The basic idea is to incorporate the spatial information by altering the membership matrix of standard fuzzy C-means clustering to lower the effect of noise and intensity inhomogeneity. It also helps in improving the segmentation accuracies of the tissue regions by assigning the equidistant pixels to a single cluster. The suggested technique is validated with different modalities of brain MR images. Various evaluation indices are computed followed by the statistical analysis to justify the superiority of the suggested technique in comparison to the state-of-the-art methods.},
  archive      = {J_IETIP},
  author       = {Pranaba K. Mishro and Sanjay Agrawal and Rutuparna Panda and Ajith Abraham},
  doi          = {10.1049/iet-ipr.2019.0942},
  journal      = {IET Image Processing},
  month        = {7},
  number       = {9},
  pages        = {1929-1936},
  shortjournal = {IET Image Process.},
  title        = {Novel fuzzy clustering-based bias field correction technique for brain magnetic resonance images},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Fully-automatic raw g-band chromosome image segmentation.
<em>IETIP</em>, <em>14</em>(9), 1920–1928. (<a
href="https://doi.org/10.1049/iet-ipr.2019.1104">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Analysis of the chromosome images plays an important role in discovering one&#39;s genetic information and possible genetic disorders. Segmentation has a very substantial place in the chromosome analysis and without an automatic solution, it is a time-consuming and error-prone procedure. Many researchers tried to automate the segmentation process. However, background noise, objects other than chromosomes in the image, touching and overlapped chromosomes are still current issues. To address these issues, the authors proposed fully-automatic raw G-band chromosome image segmentation, which aims to segment every single chromosome with a minimal error. The proposed algorithm contains the following steps: clearing the background noise, eliminating the objects other than chromosomes, distinguishing single chromosomes and chromosome clusters, separating touching and overlapping chromosomes. The proposed algorithm is tested on 508 raw images and achieved an accuracy of 94.7% for touching chromosome separation, 96.3% for overlapped chromosome separation, and 98.94% for segmentation of all chromosomes. The whole segmentation process takes 2–7 s for one image, depending on the number of touching and overlapping chromosomes. The segmentation results showed that compared to the previously proposed methods, their algorithm achieved better accuracy.},
  archive      = {J_IETIP},
  author       = {Emrecan Altinsoy and Jie Yang and Can Yilmaz},
  doi          = {10.1049/iet-ipr.2019.1104},
  journal      = {IET Image Processing},
  month        = {7},
  number       = {9},
  pages        = {1920-1928},
  shortjournal = {IET Image Process.},
  title        = {Fully-automatic raw G-band chromosome image segmentation},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Automatic tracing and extraction of text-line and word
segments directly in JPEG compressed document images. <em>IETIP</em>,
<em>14</em>(9), 1909–1919. (<a
href="https://doi.org/10.1049/iet-ipr.2019.1437">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {JPEG is one of the popular and efficient compression algorithms supported in the consumer electronics world. Excessive usage of mobile phones and e-governance applications have all resulted in a huge collection of JPEG compressed document images. The major challenge with these images is that its processing becomes expensive as it requires repeated decompression and recompression operations. Recently, it has been proved that developing algorithms to operate directly on the compressed data is one of the solutions in overcoming the above issue. This research study investigates a novel algorithm for segmentation of text-lines and words directly from JPEG compressed handwritten document images. Segmenting a handwritten document is challenging due to the presence of uneven spacing, variable font sizes, overlapping and touching components, and it becomes much more challenging if it is to be done directly in the compressed image. The proposed technique virtually fixes a vertical stripe at the beginning of the document to detect starting points of text-lines. Then a moving window-based space penetration algorithm is used for tracing the exact line boundary between two text-lines, resolving the issues of space and font variations, touching and overlapping components. Subsequently, a word boundary tracing algorithm is used to segment words.},
  archive      = {J_IETIP},
  author       = {Bulla Rajesh and Mohammed Javed and P. Nagabhushan},
  doi          = {10.1049/iet-ipr.2019.1437},
  journal      = {IET Image Processing},
  month        = {7},
  number       = {9},
  pages        = {1909-1919},
  shortjournal = {IET Image Process.},
  title        = {Automatic tracing and extraction of text-line and word segments directly in JPEG compressed document images},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Multi-objectives optimisation of features selection for the
classification of thyroid nodules in ultrasound images. <em>IETIP</em>,
<em>14</em>(9), 1901–1908. (<a
href="https://doi.org/10.1049/iet-ipr.2019.1540">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ultrasound (US) imaging is the leading diagnostic method for assessing the early-stage thyroid nodule. However, the visual evaluation of nodules can be influenced by the subjectivity of radiologists&#39; interpretations. Computer-aided Diagnostic (CAD) systems can be useful in classifying these nodules according to their benign or malignant nature. The extraction of the characteristics, which relate in the author&#39;s case to the US of thyroid nodules, is essential in the differentiation of these nodules. The complex nature of images, however, generates a significant number of features, many of which are either redundant or irrelevant. This study presents a new CAD system that has been developed to categorise thyroid nodules. In this survey, 447 US images of thyroid nodules were retained. These images were used to extract features using statistical features extraction methods. A feature selection method based on the multi objective particle swarm optimisation algorithm was used to choose the most relevant and non-redundant ones. Then, support vector machine (SVM) and random forests (RFs) were applied to classify these nodules. 10-fold cross-validation was used to assess the classification performance metrics. Their proposed CAD has reached a maximum accuracy of 94.28% for SVM; and 96.13% for RF using the contour-based ROI.},
  archive      = {J_IETIP},
  author       = {Noura Aboudi and Ramzi Guetari and Nawres Khlifa},
  doi          = {10.1049/iet-ipr.2019.1540},
  journal      = {IET Image Processing},
  month        = {7},
  number       = {9},
  pages        = {1901-1908},
  shortjournal = {IET Image Process.},
  title        = {Multi-objectives optimisation of features selection for the classification of thyroid nodules in ultrasound images},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Fast prediction mode selection and CU partition for HEVC
intra coding. <em>IETIP</em>, <em>14</em>(9), 1892–1900. (<a
href="https://doi.org/10.1049/iet-ipr.2019.0259">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Compared with the predecessor H.264/advanced video coding, high-efficiency video coding (HEVC) is a new video coding standard with nearly double coding efficiency under the same coding quality. However, the computing complexity of HEVC increases sharply. To solve this problem, a fast algorithm for intra prediction mode selection based on mode grouping is proposed by reducing the number of modes entering rough mode decision. Moreover, the dual support vector machine is proposed to efficiently select the coding unit (CU) size, which is based on texture features of CU and sub-CUs including content complexity and direction complexity. By using the new CU size selection algorithm, the encoder can confirm the split for complex CU and terminate the split for simple CU in advance, so as to reduce the computing complexity of CU size selection. The experimental results show that by employing the two fast algorithms in intra coding, it can save 42.80% encoding time, with 0.98% increment in bit rate and 0.018 dB loss of peak-signal-to-noise ratio of luminance, compared with the reference software x265-1.7.},
  archive      = {J_IETIP},
  author       = {Bin Fu and Qiangqing Zhang and Jianling Hu},
  doi          = {10.1049/iet-ipr.2019.0259},
  journal      = {IET Image Processing},
  month        = {7},
  number       = {9},
  pages        = {1892-1900},
  shortjournal = {IET Image Process.},
  title        = {Fast prediction mode selection and CU partition for HEVC intra coding},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Detecting abnormal events in traffic video surveillance
using superorientation optical flow feature. <em>IETIP</em>,
<em>14</em>(9), 1881–1891. (<a
href="https://doi.org/10.1049/iet-ipr.2019.0549">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Detection of abnormal events in the traffic scene is very challenging and is a significant problem in video surveillance. The authors proposed a novel scheme called super orientation optical flow (SOOF)-based clustering for identifying the abnormal activities. The key idea behind the proposed SOOF features is to efficiently reproduce the motion information of a moving vehicle with respect to superorientation motion descriptor within the sequence of the frame. Here, the authors adopt the mean absolute temporal difference to identify the anomalies by motion block (MB) selection and localisation. SOOF features obtained from MB are used as motion descriptor for both normal and abnormal events. Simple and efficient K-means clustering is used to study the normal motion flow during the training. The abnormal events are identified using the nearest-neighbour searching technique in the testing phase. The experimental outcome shows that the proposed work is effectively detecting anomalies and found to give results better than the state-of-the-art techniques.},
  archive      = {J_IETIP},
  author       = {Joshan Athanesious and Vasuhi Srinivasan and Vaidehi Vijayakumar and Shiny Christobel and Sibi Chakkaravarthy Sethuraman},
  doi          = {10.1049/iet-ipr.2019.0549},
  journal      = {IET Image Processing},
  month        = {7},
  number       = {9},
  pages        = {1881-1891},
  shortjournal = {IET Image Process.},
  title        = {Detecting abnormal events in traffic video surveillance using superorientation optical flow feature},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Medical image fusion using the PCNN based on IQPSO in NSST
domain. <em>IETIP</em>, <em>14</em>(9), 1870–1880. (<a
href="https://doi.org/10.1049/iet-ipr.2020.0040">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this study, an improved quantum-behaved particle swarm optimisation based pulse-coupled neural network (IQPSO-PCNN) is proposed in the non-subsampled shearlet transform (NSST) domain for medical image fusion. First, NSST tool is used to decompose the source image into low-frequency and high-frequency subbands. Then, for low-frequency subbands, the fusion rules of two different functions are presented, which simultaneously addresses two key issues of energy preservation and detail extraction. For high-frequency subbands, unlike conventional PCNN-based methods, parameters are manually set based on experience, and the decomposed high-frequency subbands share a set of parameters. The IQPSO-PCNN model can obtain the optimal parameters for each high-frequency subband adaptively according to its own information. Finally, the fused low-frequency subband and high-frequency subbands are inversely transformed by NSST to acquire the final fused image. The proposed algorithm uses &gt;90 pairs of images with four different modalities. In addition, fusion experiments are performed on different sequences of the three modes. The experimental results demonstrate that the proposed method is superior to existing state-of-art methods in subjective visual performance and objective evaluation.},
  archive      = {J_IETIP},
  author       = {Di Gai and Xuanjing Shen and Haipeng Chen and Zeyu Xie and Pengxiang Su},
  doi          = {10.1049/iet-ipr.2020.0040},
  journal      = {IET Image Processing},
  month        = {7},
  number       = {9},
  pages        = {1870-1880},
  shortjournal = {IET Image Process.},
  title        = {Medical image fusion using the PCNN based on IQPSO in NSST domain},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Optimised multi-wavelet domain for decomposed
electrooculogram-based eye movement classification. <em>IETIP</em>,
<em>14</em>(9), 1862–1869. (<a
href="https://doi.org/10.1049/iet-ipr.2019.0277">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The human eye movement tracking is possible with the assistance of the electrooculography (EOG) signals. The human eye tracking system allows researchers to analyse the participant&#39;s eye movements during certain activities. This study offers the EOG signals to control the human–computer interface systems with the help of Empirical Mean Curve Decomposition (EMCD) decomposition model. At first, the input EOG signal is provided as input to the EMCD decomposition model, later the resultant signal is given to principal component analysis for dimensional reduction, and then the dimensional reduced signal is offered to multi-wavelet decomposition model. The resultant dimensionally reduced multi-wavelet decomposed signal is passed to the proposed Feature Mapping (FM) model, using the k -means clustering model. Then, the Grey Wolf Optimization (GWO) algorithm is utilised to tune the margin. Next to mapping, the obtained features are provided to the nearest neighbour classifier, to obtain the eye movement. Next to the implementation, the proposed method is compared with the existing methods, and it is witnessed that the proposed methodology gives the superior performance in correspondence with accuracy, sensitivity, specificity, precision, false positive rate, false negative rate, negative predictive value, false discovery rate, F1 score and Mathews correlation coefficient.},
  archive      = {J_IETIP},
  author       = {Harikrishna Mulam and Malini Mudigonda},
  doi          = {10.1049/iet-ipr.2019.0277},
  journal      = {IET Image Processing},
  month        = {7},
  number       = {9},
  pages        = {1862-1869},
  shortjournal = {IET Image Process.},
  title        = {Optimised multi-wavelet domain for decomposed electrooculogram-based eye movement classification},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Combining highlight removal and low-light image enhancement
technique for HDR-like image generation. <em>IETIP</em>, <em>14</em>(9),
1851–1861. (<a href="https://doi.org/10.1049/iet-ipr.2019.1099">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Low dynamic range (LDR) image may contain low-light and highlight areas due to the limitations of the dynamic range of conventional image sensors. Low-light and highlight phenomena limit colour richness and visibility of objects in an image. Therefore, it can cause a reduction in the quality of images and a loss in accuracy in the application of image recognition. To overcome this, high dynamic range (HDR)-like images have been developed with rich colours such as those seen by the human eye. In this study, the authors propose a method to obtain an HDR-like image from a single LDR image by removing the specular component from highlight pixels as well as strengthening the actual colour. Next, they select low-light image enhancement via illumination map estimation as a low-light enhancement technique by showing the comparison with gamma-based expansion operator. They evaluate their HDR-like output images with non-reference and full-reference metrics. They show the comparison of their proposed method with six other methods. Besides, visually, their proposed method delivers more pleasing output than the output of other competitive methods.},
  archive      = {J_IETIP},
  author       = {Rappy Saha and Partha Pratim Banik and Shantanu Sen Gupta and Ki-Doo Kim},
  doi          = {10.1049/iet-ipr.2019.1099},
  journal      = {IET Image Processing},
  month        = {7},
  number       = {9},
  pages        = {1851-1861},
  shortjournal = {IET Image Process.},
  title        = {Combining highlight removal and low-light image enhancement technique for HDR-like image generation},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Optimal weighted bilateral filter with dual-range kernel for
gaussian noise removal. <em>IETIP</em>, <em>14</em>(9), 1840–1850. (<a
href="https://doi.org/10.1049/iet-ipr.2018.6272">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The bilateral filter is a classical technique for edge-preserving smoothing. It has been widely used as an effective image denoising approach to remove Gaussian noise. The performance of bilateral filtering highly depends on the accuracy of its range distance estimation, which is used for pixel-neighbourhood similarity measurement. However, in the conventional bilateral filtering approach, estimating the range distance directly from noisy observation results in the degradation of denoising performance. To address this issue, the authors propose a novel bilateral filtering scheme with a dual-range kernel, which provides a more robust range of distance estimation at various noise levels compared with existing methods. To further improve the denoising performance, they employ a linear model to retrieve the remaining image details from the method noise and add them back to the denoised image by employing an optimal approach based on Stein&#39;s unbiased risk estimate. Experiments on standard test images demonstrate that the proposed method outperforms conventional bilateral filter and its major state-of-the-art variants.},
  archive      = {J_IETIP},
  author       = {Geming Wu and Shuqian Luo and Zhi Yang},
  doi          = {10.1049/iet-ipr.2018.6272},
  journal      = {IET Image Processing},
  month        = {7},
  number       = {9},
  pages        = {1840-1850},
  shortjournal = {IET Image Process.},
  title        = {Optimal weighted bilateral filter with dual-range kernel for gaussian noise removal},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Specific category region proposal network for text detection
in natural scene. <em>IETIP</em>, <em>14</em>(9), 1832–1839. (<a
href="https://doi.org/10.1049/iet-ipr.2019.0652">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Natural scene text usually carries considerable abstract semantic information, which is closely related to the surrounding environment. Thus, natural scene text detection plays a vital role in image content retrieval and understanding. In this study, the authors propose a novel specific category region proposal network (SCRPN) based on maximally stable extremal regions (MSER) and fully convolutional network (FCN) for natural scene text detection. First, FCN for pixel-level recognition is utilised to obtain the text saliency map and MSER is used to obtain oversegmented regions. Then, the multiple features of oversegmented regions and text saliency map are used for region aggregation. Next, single-linkage clustering method is adopted to cluster the segmentation regions to obtain a hierarchical structure of text region proposals. Finally, for the top-ranking region proposals, SCRPN built an end-to-end pipeline for scene text detection directly. Experiments on street view text and international conference on document analysis and recognition (ICDAR) 2013 have demonstrated the effectiveness of SCRPN for generating the text proposals. SCRPN could work with various two-stage text detection networks; thus, faster region convolutional neural network was used as the text detection framework to evaluate the performance of SCRPN in the ICDAR 2015 and MSRA-TD500 benchmarks. The experimental results confirmed that SCRPN makes text detection more robust in complex scenarios.},
  archive      = {J_IETIP},
  author       = {Yuanhong Zhong and Xinyu Cheng and Zhaokun Zhou and Shun Zhang and Jing Zhang and Guan Huang},
  doi          = {10.1049/iet-ipr.2019.0652},
  journal      = {IET Image Processing},
  month        = {7},
  number       = {9},
  pages        = {1832-1839},
  shortjournal = {IET Image Process.},
  title        = {Specific category region proposal network for text detection in natural scene},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Swift distance transformed belief propagation using a novel
dynamic label pruning method. <em>IETIP</em>, <em>14</em>(9), 1822–1831.
(<a href="https://doi.org/10.1049/iet-ipr.2019.1035">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Loopy belief propagation (LBP) suffers from high computational time, specifically when each node in the Markov random field (MRF) model has lots of labels. In this study, a swift distance transformed belief propagation (SDT-BP) method is proposed. SDT-BP employs an efficient dynamic label pruning approach together with distance transformation to boost the running time of the LBP. The proposed dynamic label pruning approach is independent of any specific message scheduling. The resultant solution&#39;s energy is less than Priority-BP. Furthermore, SDT-BP guarantees convergence in fewer numbers of iterations. The direct combination of distance transformed belief propagation (DT-BP) with the dynamic label pruning in Priority-BP has O ( KTN log N ) computational complexity. However, the proposed method results in O ( KTN ) complexity. Where N is the number of nodes, K is the number of labels for each node, and T is the number of iterations. The authors conduct several experiments on image inpainting case studies, to evaluate this method. According to this analysis, DT-BP faces nearly 90% speedup by preserving the energy of the solution at almost the same level. Furthermore, this method can be utilised in any MRF model where its distance function is transformable, i.e. in various image processing and computer vision problems.},
  archive      = {J_IETIP},
  author       = {Hamed Ayoobi and Mehdi Rezaeian},
  doi          = {10.1049/iet-ipr.2019.1035},
  journal      = {IET Image Processing},
  month        = {7},
  number       = {9},
  pages        = {1822-1831},
  shortjournal = {IET Image Process.},
  title        = {Swift distance transformed belief propagation using a novel dynamic label pruning method},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Surface segmentation and environment change analysis using
band ratio phenology index method – supervised aspect. <em>IETIP</em>,
<em>14</em>(9), 1813–1821. (<a
href="https://doi.org/10.1049/iet-ipr.2018.6526">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Remote sensing is an escalating field that helps to monitor the earth in different perspectives like vegetation assessment, coastal studies, global warming analysis etc. Presently many satellites are orbiting the earth for taking multispectral imagery, which is working behind the principle remote sensing applications. Though there are mechanisms for image classification still innovative method is required to detect and monitor the physical characteristics of the environment. Weather forecasting, ecology assessment and irrigation management are relying upon the seasonal changes. This research study concentrates on seasonal change analysis by supervised image classification called Band Ratio Phenology Index (BRPI) method. This BRPI has helped to learn seasonal impact on the environment for the last six years. Confusion Matrix, Overall Accuracy, and Kappa Coefficient are the quality measures used to legitimise the classification exactness.},
  archive      = {J_IETIP},
  author       = {K.R. Sivabalan and E. Ramaraj},
  doi          = {10.1049/iet-ipr.2018.6526},
  journal      = {IET Image Processing},
  month        = {7},
  number       = {9},
  pages        = {1813-1821},
  shortjournal = {IET Image Process.},
  title        = {Surface segmentation and environment change analysis using band ratio phenology index method – supervised aspect},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Segmentation method of multiple sclerosis lesions based on
3D-CNN networks. <em>IETIP</em>, <em>14</em>(9), 1806–1812. (<a
href="https://doi.org/10.1049/iet-ipr.2019.0880">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Histopathology image segmentation is an important area in the field of computer aided diagnosis using image processing. The segmentation of Multiple sclerosis (MS) lesions from MR images can establish the basis for subsequent lesion reconstruction, volume estimation, and course evaluation. This study proposes a method for automatically segmenting MS lesions based on 3D convolutional neural network (CNN). The method is divided into two stages, each of which includes two convolution layers and two pooling layers. The alternative lesion voxels are selected in the first stage, while in the second stage, the final lesion voxels are segmented from the lesion voxels which are obtained in the first stage by restricting the conditions. The method has been tested on the MICCAI 2008 and 2016 datasets and compared to the other baseline methods. The experiment results show that the method has better performance than the other baseline methods on different evaluation indicators, including dice similarity coefficient, absolute difference in lesion volume, true positive rate, false positive rate, and predictive positivity value.},
  archive      = {J_IETIP},
  author       = {Yan Xiang and Han Liu and Shuo Wang and Lei Ma and Xin Xiong and Chunrong Xu and Dangguo Shao},
  doi          = {10.1049/iet-ipr.2019.0880},
  journal      = {IET Image Processing},
  month        = {7},
  number       = {9},
  pages        = {1806-1812},
  shortjournal = {IET Image Process.},
  title        = {Segmentation method of multiple sclerosis lesions based on 3D-CNN networks},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). H-WordNet: A holistic convolutional neural network approach
for handwritten word recognition. <em>IETIP</em>, <em>14</em>(9),
1794–1805. (<a href="https://doi.org/10.1049/iet-ipr.2019.1398">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Segmentation of handwritten words into isolated characters and their recognition are challenging due to the presence of high variability and cursiveness in Indian scripts. The complex shapes and availability of numerous atomic character classes, compound characters, modifiers, ascendants, and descendants make the recognition task even more difficult. A holistic approach effectively tackles such issues by avoiding the character-level segmentation and the earlier holistic methods have been mostly developed using multi-stage machine learning architecture. In this study, a deep convolutional neural network-based holistic method termed ‘H-WordNet’ is proposed for handwritten word recognition. The H-WordNet model includes merely four convolutional layers and one fully connected layer to effectively classify the word images&#39;, which lead to a significant reduction in parameters. The efficacy of different pooling operations with the proposed model is investigated. The main purpose of this study is to avoid the need for handcrafted feature extraction and obtain a more stable and generalised system for word recognition. The proposed model is evaluated using a standard handwritten Bangla word database (CMATERdb2.1.2), which contains 18000 Bangla word images of 120 different categories and it obtained a higher recognition accuracy of 96.17% when compared to recent state-of-the-art methods.},
  archive      = {J_IETIP},
  author       = {Dibyasundar Das and Deepak Ranjan Nayak and Ratnakar Dash and Banshidhar Majhi and Yu-Dong Zhang},
  doi          = {10.1049/iet-ipr.2019.1398},
  journal      = {IET Image Processing},
  month        = {7},
  number       = {9},
  pages        = {1794-1805},
  shortjournal = {IET Image Process.},
  title        = {H-WordNet: A holistic convolutional neural network approach for handwritten word recognition},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Neural network-based image quality comparator without
collecting the human score for training. <em>IETIP</em>, <em>14</em>(9),
1787–1793. (<a href="https://doi.org/10.1049/iet-ipr.2019.0809">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Emulating human behaviours in automated image quality assessment (IQA) enables a comparator framework to remove the differences in human bias naturally. Based on the observation of the practical applications of IQA, this study focuses on similar-content image quality comparison based on a new image quality comparator (IQC). Outstanding proven IQAs can be utilised in this comparator to achieve a new non-linear combination strategy to boost the IQAs&#39; performance in image quality comparison. For both input images to be compared, proven IQAs are utilised to obtain nine features from each image, yielding 18 total features. Then, a four-layer comparison network conducts a classification task to indicate which input image has better quality. In the training phase, the commonly used human scores as training labels are replaced with pairwise comparison results that are automatically generated from assigned distortion level differences. By not utilising human score in training phase, this IQC shows two advantages: (i) it removes huge labor and time cost to collect the human scores and (ii) it solves the problem of over-fitting benefiting from simplicity of creating a large image training dataset. Furthermore, the experimental tests and cross-dataset validation comparison tests demonstrate its impressive performance.},
  archive      = {J_IETIP},
  author       = {Long Bao and Karen Panetta and Sos Agaian},
  doi          = {10.1049/iet-ipr.2019.0809},
  journal      = {IET Image Processing},
  month        = {7},
  number       = {9},
  pages        = {1787-1793},
  shortjournal = {IET Image Process.},
  title        = {Neural network-based image quality comparator without collecting the human score for training},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Denoising framework based on external prior guided
rotational clustering. <em>IETIP</em>, <em>14</em>(9), 1777–1786. (<a
href="https://doi.org/10.1049/iet-ipr.2019.0918">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The image denoising model based on non-local self-similarity prior (NSS) has received extensive attention in recent years because of the repeated structure of natural image patches. Similar patches collected by exploiting NSS prior are sparse, which can be used to estimate potential lowrank subspace. Meanwhile, the modelling of natural images, such as Gaussian mixture models (GMMs), has been successful in all aspects of computer vision by reducing the patterns of image patches. However, the version of its geometric transformation (e.g. rotational transformation) cannot be matched directly by using distance. How to further reduce the patterns of the patches by geometric prior and accelerate rotational matching through parallel calculation is an issue that needs to be solved. In this study, an external guided rotational matching denoising framework is proposed. The proposed framework combines non-local, sparse and low-rank image priors and we design a parallel computing scheme. They demonstrate the performance improvement of the proposed algorithm on images with strong rotational properties and the comparison with traditional state-of-the-art denoising methods. The scalability and effectiveness of the new framework are verified by simulation experiments in public and real datasets.},
  archive      = {J_IETIP},
  author       = {Hang Yan and Zhan Yan and Jian Chen and Duo Xuan},
  doi          = {10.1049/iet-ipr.2019.0918},
  journal      = {IET Image Processing},
  month        = {7},
  number       = {9},
  pages        = {1777-1786},
  shortjournal = {IET Image Process.},
  title        = {Denoising framework based on external prior guided rotational clustering},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Adaptive bag-of-visual word modelling using
stacked-autoencoder and particle swarm optimisation for the unsupervised
categorisation of images. <em>IETIP</em>, <em>14</em>(9), 1769–1776. (<a
href="https://doi.org/10.1049/iet-ipr.2019.1160">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The bag-of-visual words (BOVWs) have been recognised as an effective mean of representing images for image classification. However, its reliance on a visual codebook developed using handcrafted image feature extraction algorithms and vector quantisation via k -means clustering often results in significant computational overhead, and poor classification accuracies. Therefore, this study presents an adaptive BOVW modelling, in which image feature extraction is achieved using deep feature learning and the amount of computation required for the development of visual codebook is minimised using a batch implementation of particle swarm optimisation. The proposed method is tested using Caltech-101 image dataset, and the results confirm the suitability of the proposed method in improving the categorisation performance while reducing the computational load.},
  archive      = {J_IETIP},
  author       = {Abass Olaode and Golshah Naghdy},
  doi          = {10.1049/iet-ipr.2019.1160},
  journal      = {IET Image Processing},
  month        = {7},
  number       = {9},
  pages        = {1769-1776},
  shortjournal = {IET Image Process.},
  title        = {Adaptive bag-of-visual word modelling using stacked-autoencoder and particle swarm optimisation for the unsupervised categorisation of images},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Collaborative similarity metric learning for face
recognition in the wild. <em>IETIP</em>, <em>14</em>(9), 1759–1768. (<a
href="https://doi.org/10.1049/iet-ipr.2019.0510">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Utilising different representations of face images is known to be helpful in face recognition. Inthis study,theauthorspropose two fusion techniques that make use of multiple face image features bycollaboratively training a similarity metric learner, based on Siamese neuralnetworks. This training procedure takes two (or possibly more) features of twoface images and outputs a similarity score that depicts whether the faces belongto the same person or not.Theauthorsinvestigate two approaches of collaborative similarity metric learning (CoSiM),both of which are based on training Siamese neural networks jointly, as a meansof early fusion. The experiments are employed on hand-crafted features such asscale-invariant feature transform (SIFT) and variants of the local binarypattern (LBP), on the YouTube Faces and the Labeled Faces in the Wild data sets.The authors provide theoretical and empirical comparisons of the proposed modelsagainst the related methods in the literature. It is shown that the proposedtechnique improves on the verification accuracy, compared to singlefeature-based baselines. By only utilising simple features like SIFT and LBP,the proposed techniques are shown to yield comparable results to the state ofthe art techniques, which depend on deep convolutional architectures or higherlevel features.},
  archive      = {J_IETIP},
  author       = {Batuhan Gundogdu and Michael J. Bianco},
  doi          = {10.1049/iet-ipr.2019.0510},
  journal      = {IET Image Processing},
  month        = {7},
  number       = {9},
  pages        = {1759-1768},
  shortjournal = {IET Image Process.},
  title        = {Collaborative similarity metric learning for face recognition in the wild},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). General generative model-based image compression method
using an optimisation encoder. <em>IETIP</em>, <em>14</em>(9),
1750–1758. (<a href="https://doi.org/10.1049/iet-ipr.2019.0715">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image compression is an intensively studied subject in computer vision. The deep generative model, especially generative adversarial networks (GANs), is a popular new direction for this subject. In this study, the authors propose a new compression method based on a generative model and focus on its application by GANs. The decoder in the proposed method is modified from the GAN generator model, which can produce visually real-like synthetic images. It is one of the two models in GANs, which is trained through a two-players&#39; contest game. The encoder is an optimisation algorithm called backpropagation-to-the-input, which derives from an image inpainting algorithm based on generative models. In the proposed method, the authors turn the encoding process into an optimisation task to search for optimal encoded representations. Compared with traditional methods, the proposed method can compress images from certain domains into extremely small and shape-fixed encoded space but still retain better visual representations. It is easy and convenient to apply without any retraining or additional modification to the generative models.},
  archive      = {J_IETIP},
  author       = {Mengtian Wu and Zaixing He and Xinyue Zhao and Shuyou Zhang},
  doi          = {10.1049/iet-ipr.2019.0715},
  journal      = {IET Image Processing},
  month        = {7},
  number       = {9},
  pages        = {1750-1758},
  shortjournal = {IET Image Process.},
  title        = {General generative model-based image compression method using an optimisation encoder},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Robust graph regularised sparse matrix regression for
two-dimensional supervised feature selection. <em>IETIP</em>,
<em>14</em>(9), 1740–1749. (<a
href="https://doi.org/10.1049/iet-ipr.2019.1404">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Bilinear matrix regression based on matrix data can directly select the features from matrix data by deploying several couples of left and right regression matrices. However, the existing matrix regression methods do not consider the local geometric structure of the samples, which results in poor classification performance. This study proposes a robust graph regularised sparse matrix regression method for two-dimensional supervised feature selection, where the intra-class compactness graph based on the manifold learning is used as the regularisation item, and the -norm as loss functions to establish the authors’ matrix regression model. An alternating optimisation algorithm is also devised to solve it and give its closed-form solutions in each iteration. The proposed method not only can learn the left and right regression matrices, but also can preserve the intrinsic geometry structure by using the label information. Extensive experiments on several data sets demonstrate the superiority of the proposed method.},
  archive      = {J_IETIP},
  author       = {Xiuhong Chen and Yun Lu},
  doi          = {10.1049/iet-ipr.2019.1404},
  journal      = {IET Image Processing},
  month        = {7},
  number       = {9},
  pages        = {1740-1749},
  shortjournal = {IET Image Process.},
  title        = {Robust graph regularised sparse matrix regression for two-dimensional supervised feature selection},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Fast fractal image compression algorithm using specific
update search. <em>IETIP</em>, <em>14</em>(9), 1733–1739. (<a
href="https://doi.org/10.1049/iet-ipr.2019.0522">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The fractal image compression (FIC) algorithm is difficult to be widely used in real-time applications due to the huge consumption of its encoding time. Inspired by the fact that in the decoding process, for any original image, a fixed point is generated by the iterations of the fractal codes, the authors propose a specific update search FIC (SUSFIC) algorithm, which uses a scale number to control the update times of the fractal codes and to find acceptable matching domain blocks rather than the best ones in the selected domain blocks pool. To further reduce the computation time, in their proposed algorithm, the image blocks created by the equidistant sampling in the range of the original blocks are used to replace themselves when calculating the correlation coefficients as the distances between the adjacent domain blocks. The experimental results presented show that their proposed SUSFIC algorithm has a significant improvement in encoding time under the premise of setting an appropriate search update threshold and maintaining image quality when compared with the state-of-the-art FIC algorithms. Therefore, it is a better FIC algorithm.},
  archive      = {J_IETIP},
  author       = {Yunping Zheng and Xiangpeng Li and Mudar Sarem},
  doi          = {10.1049/iet-ipr.2019.0522},
  journal      = {IET Image Processing},
  month        = {7},
  number       = {9},
  pages        = {1733-1739},
  shortjournal = {IET Image Process.},
  title        = {Fast fractal image compression algorithm using specific update search},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Segmentation techniques for early cancer detection in red
blood cells with deep learning-based classifier—a comparative approach.
<em>IETIP</em>, <em>14</em>(9), 1726–1732. (<a
href="https://doi.org/10.1049/iet-ipr.2019.1067">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Red Blood Corpuscles called Erythrocytes are the most important element in blood composition which is mainly responsible in all living cells. To detect the cancer cell various methods are employed. In this paper, proper identification of cancer cells from unaffected RBCs are detected. The proposed novel method called Online Region Based Segmentation (ORBS) method is done that is used to find the regions of corpuscles. By using properties, metric is formulated for determination of shape which is abnormal in blood cells. Overall accuracy of 96.9% is obtained using proposed ORBS method and deep learning classification (DLC) method has accuracy of 97.1% that helps to diagnose cancer cells using feature extraction process done automatically. Sensitivity, specificity and precision values of the proposed segmentation method is found to be 96.7%, 95.6% and 98.4% respectively. The computation time was found as 22 seconds. Closeness of Proposed method in relative to True Positive values at the ROC curve indicates the performance as higher. Comparative analysis is made with ResNet-50 based on the different testing and training data at rate of 90%−10%, 80%−20% and 70%−30% respectively, which proves the robustness of proposed research work. Experimental results prove proposed system effectiveness compared with other detection methods.},
  archive      = {J_IETIP},
  author       = {Jeya Sudharsan Shemona and Agees Kumar Chellappan},
  doi          = {10.1049/iet-ipr.2019.1067},
  journal      = {IET Image Processing},
  month        = {7},
  number       = {9},
  pages        = {1726-1732},
  shortjournal = {IET Image Process.},
  title        = {Segmentation techniques for early cancer detection in red blood cells with deep learning-based classifier—a comparative approach},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Novel distortion free and histogram based data hiding
scheme. <em>IETIP</em>, <em>14</em>(9), 1716–1725. (<a
href="https://doi.org/10.1049/iet-ipr.2018.6428">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the most data hiding methods, the cover media is entirely distorted and it cannot be recovered when the secretive data is extracted. However, in some applications, such as medical imaging, military imaging and law enforcement, even a slight modification in the cover image is impermissible. Therefore, reversible data hiding schemes have proposed as a solution of this issue, recently. Histogram based embedding schemes are among wide investigated reversible data hiding schemes owing to the high quality of the marked images. In histogram based embedding techniques, construction of histogram greatly affects the embedding capacity and the distortion performance of schemes. In this study, the authors propose a novel reversible data hiding scheme for grey scale images. In this method, a new histogram is generated according to the differences between pixels and their neighbouring pixels values. The proposed scheme can achieve higher embedding capacity than other existing reversible data hiding algorithms in the literature by considering the same value of peak signal-to-noise ratio. Experimental and analytical results show that this scheme is successfully employed for data hiding in a wide range of images.},
  archive      = {J_IETIP},
  author       = {Keyvan Aghababaiyan and Amir S. Mortazavi},
  doi          = {10.1049/iet-ipr.2018.6428},
  journal      = {IET Image Processing},
  month        = {7},
  number       = {9},
  pages        = {1716-1725},
  shortjournal = {IET Image Process.},
  title        = {Novel distortion free and histogram based data hiding scheme},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Fingerprint liveness detection based on guided filtering and
hybrid image analysis. <em>IETIP</em>, <em>14</em>(9), 1710–1715. (<a
href="https://doi.org/10.1049/iet-ipr.2018.5915">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fingerprints are widely used for biometric recognition. However, many spoofing attacks based on an artificially made fingerprint occur. In this study, the authors propose an approach to detect fingerprint liveness which uses the guided filtering and hybrid image analysis. This study deals with the problem of ignoring the contribution that is brought by the sharp features when analysing the denoised image. The method described utilises both the enhanced sharp features and denoised features from the hybrid images to get better results. The input fingerprint is pre-processed by region of interest extraction and then is filtered by a guidance image for obtaining the denoised image. Then, histogram equalisation is introduced to eliminate the impact of illumination condition. The authors extract the co-occurrence of adjacent local binary pattern features from both the cropped images and the denoised images. Whilst concatenating both the features together to form a long feature, t-Distributed Stochastic Neighbour Embedding is applied to reduce the data dimension. The authors consider the fingerprint liveness detection as a two-class classification problem and use support vector machine with radial basis function kernel to solve this problem. The authors evaluate the experiments on three benchmark data sets. Experimental results demonstrate that the accuracy of the proposed method can outperform most of the state-of-art methods.},
  archive      = {J_IETIP},
  author       = {Guanghua Tan and Qiong Zhang and Haiyang Hu and Xianyi Zhu and Xiangqiong Wu},
  doi          = {10.1049/iet-ipr.2018.5915},
  journal      = {IET Image Processing},
  month        = {7},
  number       = {9},
  pages        = {1710-1715},
  shortjournal = {IET Image Process.},
  title        = {Fingerprint liveness detection based on guided filtering and hybrid image analysis},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Collaborative model tracking with robust occlusion handling.
<em>IETIP</em>, <em>14</em>(9), 1701–1709. (<a
href="https://doi.org/10.1049/iet-ipr.2019.0827">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Currently, the discriminative correlation filter-based trackers have achieved higher tracking accuracy. However, visual tracking still faces challenges in terms of heavy occlusion, scale variation and so on. In this study, the authors intend to solve heavy occlusion by introducing collaborative model into classifier-box. Firstly, they introduce complex colour features into correlation filter tracker to improve the effect of the tracker. Secondly, they introduce a multi-scale method into their tracker to ease the scale problem. Thirdly, in order to solve the heavy occlusion in the tracking process, they adopt the locally weighted distance and classifier-box. Their algorithm achieves distance precision rates of 81.7 and 77.4% on OTB2013 dataset and OTB2015 dataset, respectively. Their contribution focuses on solving heavy occlusion by using colour features, locally weighted distance and classifier-box. The experimental results on OTB2013 and OTB2015 datasets demonstrate their algorithm to perform better than state-of-the-art methods.},
  archive      = {J_IETIP},
  author       = {Jun Kong and Yitao Ding and Min Jiang and Sha Li},
  doi          = {10.1049/iet-ipr.2019.0827},
  journal      = {IET Image Processing},
  month        = {7},
  number       = {9},
  pages        = {1701-1709},
  shortjournal = {IET Image Process.},
  title        = {Collaborative model tracking with robust occlusion handling},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). DRGAN: A deep residual generative adversarial network for
PET image reconstruction. <em>IETIP</em>, <em>14</em>(9), 1690–1700. (<a
href="https://doi.org/10.1049/iet-ipr.2019.1107">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Positron emission tomography (PET) image reconstruction from low-count projection data and physical effects is challenging because the inverse problem is ill-posed and the resultant image is usually noisy. Recently, generative adversarial networks (GANs) have also shown their superior performance in many computer vision tasks and attracted growing interests in medical imaging. In this work, the authors proposed a novel model [deep residual generative adversarial network (DRGAN)] based on GANs for the reduction of streaking artefacts and the improvement of PET image quality. An innovative feature of the proposed method is that the authors trained a generator to produce ‘residual PET map’ (RPM) for image representation, rather than generate PET images directly. DRGAN used two discriminators (critics) to enforce anatomically realistic PET images and RPM. To better boost the contextual information, the authors designed residual dense connections followed with pixel shuffle operations (RDPS blocks) that encourage feature reuse and prevent losing resolution. Both simulation data and real clinical PET data are used to evaluate the proposed method. Compared with other state-of-the-art methods, the quantification results show that DRGAN can achieve better performance in bias–variance trade-off and provide comparable image quality. Their results were rigorously evaluated by one radiologist at the Shanxi Cancer Hospital.},
  archive      = {J_IETIP},
  author       = {Qianqian Du and Yan Qiang and Wenkai Yang and Yanfei Wang and Yong Ma and Muhammad Bilal Zia},
  doi          = {10.1049/iet-ipr.2019.1107},
  journal      = {IET Image Processing},
  month        = {7},
  number       = {9},
  pages        = {1690-1700},
  shortjournal = {IET Image Process.},
  title        = {DRGAN: A deep residual generative adversarial network for PET image reconstruction},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). From pyramids to state-of-the-art: A study and comprehensive
comparison of visible–infrared image fusion techniques. <em>IETIP</em>,
<em>14</em>(9), 1671–1689. (<a
href="https://doi.org/10.1049/iet-ipr.2019.0322">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image fusion has emerged as a major area of research in the past few decades due to its extended applications. While progressing in the field of image fusion, a large number of techniques-based image transforms and spatial filters have been devised for both general and specific sets of images. The primary criterion of image fusion technique is to deliver high-quality visual perception besides giving a considerable objective evaluation rate. In this study, an information fusion rate-based study is done on recent, most researched, and high-performing state-of-the-art techniques using visible and infrared image datasets. These techniques have been chosen carefully, owing to their superiority in performance on both objective and subjective scales of evaluation and have been discussed in terms of their respective advantages and disadvantages. It is clearly evident that some rather primitive techniques can perform well as well as techniques based on a hybrid of various domains can significantly boost the information fusion rate.},
  archive      = {J_IETIP},
  author       = {Apoorav M. Sharma and Ayush Dogra and Bhawna Goyal and Renu Vig and Sunil Agrawal},
  doi          = {10.1049/iet-ipr.2019.0322},
  journal      = {IET Image Processing},
  month        = {7},
  number       = {9},
  pages        = {1671-1689},
  shortjournal = {IET Image Process.},
  title        = {From pyramids to state-of-the-art: A study and comprehensive comparison of visible–infrared image fusion techniques},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). SODNet: Small object detection using deconvolutional neural
network. <em>IETIP</em>, <em>14</em>(8), 1662–1669. (<a
href="https://doi.org/10.1049/iet-ipr.2019.0833">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Convolution neural network (CNN) is an efficient technique to detect objects in various kinds of images, especially for microaneurysm (MA) of diabetic retinopathy in retinal fundus image. This study proposes a deconvolutional neural network to accurately discriminate MA from non-MA. The deconvolution, instead of pooling operation, is embedded into the CNN to recover the erased details of feature maps of convolutional layers. Three types of images are collected for training and predicting. Furthermore, the extracted features are fed into the fully-connected layers to classify using a softmax layer. Experimental results demonstrate that the proposed method can achieve significant sensitivity and accuracy on multiple public datasets, in comparison to the state-of-the-art. For Retinopathy Online Challenge dataset, the sensitivity and accuracy are improved up to 0.798 and 0.986, respectively.},
  archive      = {J_IETIP},
  author       = {Xinpeng Zhang and Jigang Wu and Zhihao Peng and Min Meng},
  doi          = {10.1049/iet-ipr.2019.0833},
  journal      = {IET Image Processing},
  month        = {6},
  number       = {8},
  pages        = {1662-1669},
  shortjournal = {IET Image Process.},
  title        = {SODNet: Small object detection using deconvolutional neural network},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). FuSENet: Fused squeeze-and-excitation network for
spectral-spatial hyperspectral image classification. <em>IETIP</em>,
<em>14</em>(8), 1653–1661. (<a
href="https://doi.org/10.1049/iet-ipr.2019.1462">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning-based approaches have become very prominent in recent years due to its outstanding performance as compared to the hand-extracted feature-based methods. Convolutional neural network (CNN) is a type of deep learning architecture to deal with the image/video data. Residual network and squeeze and excitation network (SENet) are among recent developments in CNN for image classification. However, the performance of SENet depends on the squeeze operation done by global pooling, which sometimes may lead to poor performance. In this study, the authors propose a bilinear fusion mechanism over different types of squeeze operation such as global pooling and max pooling. The excitation operation is performed using the fused output of squeeze operation. They used to model the proposed fused SENet with the residual unit and name it as FuSENet . Here the classification experiments are performed over benchmark hyperspectral image datasets. The experimental results confirm the superiority of the proposed FuSENet method with respect to the state-of-the-art methods. The source code of the complete system is made publicly available at https://github.com/swalpa/FuSENet .},
  archive      = {J_IETIP},
  author       = {Swalpa Kumar Roy and Shiv Ram Dubey and Subhrasankar Chatterjee and Bidyut Baran Chaudhuri},
  doi          = {10.1049/iet-ipr.2019.1462},
  journal      = {IET Image Processing},
  month        = {6},
  number       = {8},
  pages        = {1653-1661},
  shortjournal = {IET Image Process.},
  title        = {FuSENet: Fused squeeze-and-excitation network for spectral-spatial hyperspectral image classification},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Pain intensity recognition via multi-scale deep network.
<em>IETIP</em>, <em>14</em>(8), 1645–1652. (<a
href="https://doi.org/10.1049/iet-ipr.2019.1448">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Similar to the basic facial expression recognition, one challenge for pain intensity recognition is some individual characteristics, e.g. face shapes, may cause great diversities in the same emotion. So it is usually very difficult to distinguish two adjacent intensity levels of pain expression as each intensity has a large variation. In this study, a coarse-to-fine combination method is proposed for pain intensity recognition. The results of multi-scale outputs from multiple base deep network are combined in a probabilistic way for improving the discrimination between visually similar adjacent levels. A two-layer tree classifier is proposed in a multi-task framework for pain intensity recognition as well as face shape recognition, replacing the planar Softmax classifier in each base deep network. In the first layer of tree classifier, multi-scale classifiers are constructed for recognizing facial pain intensities and the conventional classifiers are constructed for face shape recognition in the second layer. Finally, the tree classifier including multi-scale classifiers and conventional classifiers is jointly optimised during the training phase and only high level classifiers are used for recognising pain intensities in the test phase. The extensive experiments on UNBC shoulder pain dataset show the proposed method gets promising results in pain intensity recognition.},
  archive      = {J_IETIP},
  author       = {Xianlin Peng and Dong Huang and Haixi Zhang},
  doi          = {10.1049/iet-ipr.2019.1448},
  journal      = {IET Image Processing},
  month        = {6},
  number       = {8},
  pages        = {1645-1652},
  shortjournal = {IET Image Process.},
  title        = {Pain intensity recognition via multi-scale deep network},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Visual object tracking via iterative ant particle filtering.
<em>IETIP</em>, <em>14</em>(8), 1636–1644. (<a
href="https://doi.org/10.1049/iet-ipr.2019.0967">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Visual object tracking remains a challenging task in computer vision although important progress has been made in the past decades. Particle filter (PF) is now a standard framework for solving non-linear/non-Gaussian problems, especially in visual object tracking. This study proposes an ant colony optimisation (ACO)-based iterative PF for object tracking. In the proposed method, the basic idea of ACO is used to simulate the behaviour of a particle moving toward the posterior distribution. Such idea is incorporated into the particle filtering framework in order to overcome the well-known particle impoverishment problem. An iterative unscented Kalman filter is used to design a proposal distribution for particle generation in order to generate better predicted sample states. For the likelihood model, the authors adopt the locality sensitive histogram to model the appearance of the target object, which can better handle the illumination variation during tracking. The experimental results demonstrate that the proposed tracker shows better performance than the other tracking methods.},
  archive      = {J_IETIP},
  author       = {Fasheng Wang and Yanbo Wang and Jianjun He and Fuming Sun and Xucheng Li and Junxing Zhang},
  doi          = {10.1049/iet-ipr.2019.0967},
  journal      = {IET Image Processing},
  month        = {6},
  number       = {8},
  pages        = {1636-1644},
  shortjournal = {IET Image Process.},
  title        = {Visual object tracking via iterative ant particle filtering},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Automatic aircraft extraction using video matting and frame
registration. <em>IETIP</em>, <em>14</em>(8), 1628–1635. (<a
href="https://doi.org/10.1049/iet-ipr.2019.0067">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study presents a technique to automatically extract the aircraft from video sequences. The technique incorporates object tracking algorithm to separate the region of interest (ROI), i.e. movement of aircraft, from video sequence, to reduce the computational complexity. Trimaps are generated automatically for given ROI using non-rigid image registration to obtain the prior information of the object and alpha mattes are estimated for each frame using KNN matting algorithm. However, the alpha matte of static region is estimated only once, that is combined with the alpha matte of moving region to generate final alpha matte. Simulations performed on different videos show that the proposed technique not only reduces the computational complexity, but also gives competitive results by generating high quality alpha maps.},
  archive      = {J_IETIP},
  author       = {Benish Amin and Muhammad Mohsin Riaz and Abdul Ghafoor},
  doi          = {10.1049/iet-ipr.2019.0067},
  journal      = {IET Image Processing},
  month        = {6},
  number       = {8},
  pages        = {1628-1635},
  shortjournal = {IET Image Process.},
  title        = {Automatic aircraft extraction using video matting and frame registration},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020b). Object counting method based on dual attention network.
<em>IETIP</em>, <em>14</em>(8), 1621–1627. (<a
href="https://doi.org/10.1049/iet-ipr.2019.0465">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The challenging problem that the authors solved in this study is to precisely estimate the number of objects in an image. Combining the spatial attention mechanism and pyramid structure, a novel atrous pyramid attention module is introduced to extract precise dense multi-scale features for object counting. Also, a global attention feature module is designed to enhance the ability of the network to learn feature representation based on channel attention mechanism. Combining the proposed atrous pyramid attention module and global attention feature module, a novel object counting method based on a dual attention network is established in this study. The experiments on public vehicle counting dataset including TRANCOS and crowd counting dataset including Mall and Shanghitech_A datasets demonstrate the proposed method achieves competitive performance, and the ablation study verifies the structure rationality of the designed modules.},
  archive      = {J_IETIP},
  author       = {Shihui Zhang and He Li and Weihang Kong},
  doi          = {10.1049/iet-ipr.2019.0465},
  journal      = {IET Image Processing},
  month        = {6},
  number       = {8},
  pages        = {1621-1627},
  shortjournal = {IET Image Process.},
  title        = {Object counting method based on dual attention network},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). NSST and vector-valued c–v model based image segmentation
algorithm. <em>IETIP</em>, <em>14</em>(8), 1614–1620. (<a
href="https://doi.org/10.1049/iet-ipr.2018.5027">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image segmentation is a process of partitioning an image into non-overlapping regions. Existing unsupervised image segmentation methods include level set, automatic thresholding and region-based CV mode and so on. However, image segmentation as a key technology in the field of image processing has not been solved indeed, especially for images with complex texture. For this reason, the authors proposed a novel image segmentation algorithm based on NSST and the vector-valued Chan–Vese (C–V) model. First, they obtained a multi-scale representation by exploiting the non-subsampled shearlet transform (NSST) to extract multi-dimensional data in the image. Afterwards, they gave the vector-valued C–V model, and applied it to all subbands of NSST, which are treated as a vector-valued image. By comparing with other class methods, the experimental results show that the proposed method has better visual effects and lower error rates. But at the same time, it is a little time consuming. The proposed method is reasonable and effective, by taking full advantages of each subband&#39;s directional information during its diffusion process, compared with traditional C–V model.},
  archive      = {J_IETIP},
  author       = {Xianghai Wang and Xiaoyang Zhao and Yihuan Zhu and Xin Su},
  doi          = {10.1049/iet-ipr.2018.5027},
  journal      = {IET Image Processing},
  month        = {6},
  number       = {8},
  pages        = {1614-1620},
  shortjournal = {IET Image Process.},
  title        = {NSST and vector-valued C–V model based image segmentation algorithm},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Improved compound image segmentation using automatic pixel
block classification with SVM. <em>IETIP</em>, <em>14</em>(8),
1605–1613. (<a href="https://doi.org/10.1049/iet-ipr.2018.6523">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Computer screen images such as wallpaper, web pages and powerpoint images are compound images. As these images contain a mixture of textual, graphical, pictorial and smooth regions, compression of computer screen images necessitates accurate classification and segmentation of these regions. In this study, an improved compound image segmentation using automatic block classification with support vector machine (SVM) is presented. First, the input compound image is divided into several non-overlapping blocks, and then the statistical features embedded in each block are mined after applying discrete wavelet transform. Then, the SVM model is trained effectively by using the informative samples of fuzzy c-means clustering, which takes block-level edge features and block neighbourhood information as input. Finally, the compound image is classified into two classes such as text/graphics and picture/background with the trained SVM model. Experimental results show that the proposed method performs automatic block classification with high accuracy. As the proposed classifier uses both structural and contextual information as features, block classification accuracy has been improved to a great extent. Hence, the proposed method has made ∼6.2% improvement in block classification accuracy while comparing with existing approaches.},
  archive      = {J_IETIP},
  author       = {Ebenezer Juliet Selwyn and Selvi Shunmuga Velayutham and Jemi Florinabel Deva George},
  doi          = {10.1049/iet-ipr.2018.6523},
  journal      = {IET Image Processing},
  month        = {6},
  number       = {8},
  pages        = {1605-1613},
  shortjournal = {IET Image Process.},
  title        = {Improved compound image segmentation using automatic pixel block classification with SVM},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Quantitative assessment of capabilities of colour models for
pen ink discrimination in handwritten documents. <em>IETIP</em>,
<em>14</em>(8), 1594–1604. (<a
href="https://doi.org/10.1049/iet-ipr.2018.6616">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The addition of new words in handwritten documents such as bank cheques, bills, and notes is considered as common crime. Such immoral activities on handwritten documents have a bad effect on the victim in terms of mental and financial loss. For facilitating an impartial judicial process, it is important to differentiate between the used pen inks. Earlier, image processing and pattern recognition-based techniques for pen ink differentiation gained attention among researchers due to their non-destructive nature. Existing techniques use various colour models to represent the colour of pen inks. Thus, it is crucial to assess the capabilities of various colour models for differentiating pen inks in handwritten documents. In this paper, we propose a histogram distance based quantitative assessment technique for suitable colour model identification for differentiating pen inks. Seven blue and seven black pen ink samples are acquired on 112 cheque leaves. The k -means binarisation is used to identify pen ink pixels. colour model has been identified as the best colour model for this task. The statistical features of the ink colours in the identified colour model representation are extracted and a multi-layer perceptron (MLP) classifier validates the capability of the identified colour model for pen ink discrimination.},
  archive      = {J_IETIP},
  author       = {Prabhat Dansena and Rajarshi Pal and Soumen Bag},
  doi          = {10.1049/iet-ipr.2018.6616},
  journal      = {IET Image Processing},
  month        = {6},
  number       = {8},
  pages        = {1594-1604},
  shortjournal = {IET Image Process.},
  title        = {Quantitative assessment of capabilities of colour models for pen ink discrimination in handwritten documents},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Single image super-resolution based on sparse representation
using dictionaries trained with input image patches. <em>IETIP</em>,
<em>14</em>(8), 1587–1593. (<a
href="https://doi.org/10.1049/iet-ipr.2019.0129">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this study, an efficient self-learning method for image super-resolution (SR) is presented. In the proposed algorithm, the input image is divided into equal size patches. Using these patches, a dictionary is learned based on K-SVD, referred to as high resolution (HR) dictionary. Then, by down-sampling, the columns of the dictionary, called atoms, a low resolution (LR) version of the dictionary is obtained. An initial estimate of the SR image is constructed using the bicubic interpolation on the input image. Then in an iterative algorithm, the difference between the down-sampled version of the estimated SR image and the input image is obtained. This difference image, which includes reconstructed details is enlarged using sparse representation and LR/HR dictionaries. The enlarged detail is added to the latest reconstructed SR image. This process gradually improves the quality of the initial SR image. After several iterations, the reconstructed image is an SR version of the input image. Experimental results confirm that the proposed method performance is promising.},
  archive      = {J_IETIP},
  author       = {Rasoul Asgarian Dehkordi and Hossein Khosravi and Alireza Ahmadyfard},
  doi          = {10.1049/iet-ipr.2019.0129},
  journal      = {IET Image Processing},
  month        = {6},
  number       = {8},
  pages        = {1587-1593},
  shortjournal = {IET Image Process.},
  title        = {Single image super-resolution based on sparse representation using dictionaries trained with input image patches},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Pavement crack detection network based on pyramid structure
and attention mechanism. <em>IETIP</em>, <em>14</em>(8), 1580–1586. (<a
href="https://doi.org/10.1049/iet-ipr.2019.0973">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Automatic detection of pavement crack is an important task for conducting road maintenance. However, as an important part of the intelligent transportation system, automatic pavement crack detection is challenging due to the poor continuity of cracks, the different width of cracks, and the low contrast between cracks and the surrounding pavement. This study proposes a novel pavement crack detection method based on an end-to-end trainable deep convolution neural network. The authors build the network using the encoder–decoder architecture and adopt a pyramid module to exploit global context information for the complex topology structures of cracks. Moreover, they introduce a spatial-channel combinational attention module into the encoder–decoder network for refining crack features. Further, the dilated convolution is used to reduce the loss of crack details due to the pooling operation in the encoder network. In addition, they introduce a lovász hinge loss function, which is suitable for small objects. They train the authors&#39; network on the CRACK500 dataset and evaluate it on three pavement crack datasets. Among the methods they compare, their method can achieve the best experimental results.},
  archive      = {J_IETIP},
  author       = {Xuezhi Xiang and Yuqi Zhang and Abdulmotaleb El Saddik},
  doi          = {10.1049/iet-ipr.2019.0973},
  journal      = {IET Image Processing},
  month        = {6},
  number       = {8},
  pages        = {1580-1586},
  shortjournal = {IET Image Process.},
  title        = {Pavement crack detection network based on pyramid structure and attention mechanism},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Classification of dry age-related macular degeneration and
diabetic macular oedema from optical coherence tomography images using
dictionary learning. <em>IETIP</em>, <em>14</em>(8), 1571–1579. (<a
href="https://doi.org/10.1049/iet-ipr.2018.6186">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Age-related Macular Degeneration (AMD) and Diabetic Macular Edema (DME) are the major causes of vision loss in developed countries. Alteration of retinal layer structure and appearance of exudates are the most significant signs of these diseases. In this paper, with the aim of automatic classification of DME, AMD, and normal subjects using Optical Coherence Tomography (OCT) images, a dictionary-learning based classification is proposed. The two important issues intended in this approach are avoiding retinal layer segmentation and attempting to mimic the authors&#39; understanding based on normal and abnormal region identifications, considering that the signs of diseases appear in a small fraction of B-Scans. The histogram of oriented gradients feature descriptor was utilized to characterize the distribution of local intensity gradients and edge directions. To capture the structure of extracted features, different dictionary learning-based classifiers are employed. The dataset consists of 45 subjects: 15 patients with AMD, 15 patients with DME, and 15 normal subjects. The proposed classifier leads to an accuracy of 95.13, 100.00, and 100.00% for DME, AMD, and normal OCT images, respectively, only by considering 4% of all B-Scans of a volume, which outperforms the state-of-the-art methods.},
  archive      = {J_IETIP},
  author       = {Elahe Mousavi and Rahele Kafieh and Hossein Rabbani},
  doi          = {10.1049/iet-ipr.2018.6186},
  journal      = {IET Image Processing},
  month        = {6},
  number       = {8},
  pages        = {1571-1579},
  shortjournal = {IET Image Process.},
  title        = {Classification of dry age-related macular degeneration and diabetic macular oedema from optical coherence tomography images using dictionary learning},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Measuring photography aesthetics with deep CNNs.
<em>IETIP</em>, <em>14</em>(8), 1561–1570. (<a
href="https://doi.org/10.1049/iet-ipr.2019.1300">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In spite of the recent advancements of deep learning based techniques, automatic photo aesthetic assessment still remains a challenging computer vision task. Existing approaches used to focus on providing a single aesthetic score or category (“good” or “bad”) of photograph, rather than quantifying “goodness” or “badness”. The existing algorithms often ignore the importance of different attributes contributing to the artistic quality of the photograph. To obtain the human-interpretability of aesthetic score of photo, we advocate learning the aesthetic attributes alongwith the prediction of the general aesthetic score. We propose a multi-task deep CNN, that collectively learns aesthetic attributes alongwith a general aesthetic score for the photograph. To understand the mathematical representation of the attributes in the proposed model, a visualization technique is proposed using back propagation of gradients. These visualization of attributes correspond to the location of objects in the images in order to find out which part of an image “triggers” the classification outcome, thus providing the insights about the model&#39;s understanding of these attributes. This paper proposes an aesthetic feature vector based on the relative foreground position of the object in the image. The proposed aesthetic features outperform the state-of-art methods especially for Rule of Thirds attribute.},
  archive      = {J_IETIP},
  author       = {Gajjala Viswanatha Reddy and Snehasis Mukherjee and Mainak Thakur},
  doi          = {10.1049/iet-ipr.2019.1300},
  journal      = {IET Image Processing},
  month        = {6},
  number       = {8},
  pages        = {1561-1570},
  shortjournal = {IET Image Process.},
  title        = {Measuring photography aesthetics with deep CNNs},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Hierarchical morphological graph signal multi-layer
decomposition for editing applications. <em>IETIP</em>, <em>14</em>(8),
1549–1560. (<a href="https://doi.org/10.1049/iet-ipr.2019.0576">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The authors address the problem of editing signals such as 2D colour images or 3D coloured meshes that are represented under the general framework of graph signals. As state-of-the-art editing approaches decompose an image into several layers in order to manipulate them, they propose a hierarchical multi-layer decomposition of graph signals that relies on morphological filtering. Since morphological filtering operators require a complete lattice, a dedicated approach for the morphological processing of vectorial data on graphs is used. By iterating the application of morphological filterings of decreasing sizes, the graph signal is decomposed into several detail layers, each capturing a given detail level. Editing applications such as abstraction, sharpness enhancement and tone mapping are shown to illustrate the benefits of the proposed approach.},
  archive      = {J_IETIP},
  author       = {Olivier Lézoray},
  doi          = {10.1049/iet-ipr.2019.0576},
  journal      = {IET Image Processing},
  month        = {6},
  number       = {8},
  pages        = {1549-1560},
  shortjournal = {IET Image Process.},
  title        = {Hierarchical morphological graph signal multi-layer decomposition for editing applications},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Image interpolation with adaptive k-nearest neighbours
search and random non-linear regression. <em>IETIP</em>, <em>14</em>(8),
1539–1548. (<a href="https://doi.org/10.1049/iet-ipr.2019.1591">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Learning-based image interpolation methods have been proved to be effective in image interpolation. In this study, the authors propose an accurate image interpolation with adaptive k -nearest neighbour searching and non-linear regression. The proposed method aims to find k -nearest neighbours of the input image patch and use them to learn the non-linear mapping between low-resolution and high-resolution image patches. To be specific, they first divide the training image patches into many subspaces, then they utilise an adaptive robust and precise k nearest neighbour searching scheme with proposed normalised Gaussian similarity to find the k nearest neighbours in the matched subspace. The selected k image patch pairs are then used to learn the non-linear regression model through an extreme learning machine. Furthermore, the proposed interpolation method is a cascade framework that consists of two stages. Stage 2 takes the results of Stage 1 as input to further improve the performance. Extensive experimental results on commonly used test images and image datasets indicate that their proposed algorithm obtains competitive performance against the state-of-the-art methods both in terms of objective evaluation values and the subjective effect of reconstructed images.},
  archive      = {J_IETIP},
  author       = {Jieying Zheng and Wanru Song and Yahong Wu and Feng Liu},
  doi          = {10.1049/iet-ipr.2019.1591},
  journal      = {IET Image Processing},
  month        = {6},
  number       = {8},
  pages        = {1539-1548},
  shortjournal = {IET Image Process.},
  title        = {Image interpolation with adaptive k-nearest neighbours search and random non-linear regression},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Adaptive gaussian notch filter for removing periodic noise
from digital images. <em>IETIP</em>, <em>14</em>(8), 1529–1538. (<a
href="https://doi.org/10.1049/iet-ipr.2018.5707">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Periodic noise corrupts digital images during acquisition and transmission stages by adding repetitive patterns. This study introduces a new adaptive Gaussian notch filter (AGNF) in Fourier transform domain for effectively restoring images contaminated with periodic, quasi-periodic and Moiré pattern noises. Since periodic noises are sinusoidal functions added to the uncorrupted images, Fourier transform of images make these noisy functions to concentrate as easily distinguishable conjugate peaks in frequency domain. The proposed AGNF effectively identifies the noisy peak positions and adaptively quantifies the associated noisy areas by analysing the ratio of averages of frequencies from adaptively varying neighbourhoods. These identified noisy peaks are then diffused by Gaussian notch filter of adaptively varying sizes. The proposed filter ensures maximum diffusion of identified noisy peak areas by maintaining the minimum frequency values from the outputs of overlapping notch filters. Visual and quantitative experimental analysis of the proposed algorithm with mean absolute error, peak signal-to-noise ratio, mean structural similarity index measure and computation time reveals that AGNF is better in restoring images contaminated with periodic noises when compared to other methods.},
  archive      = {J_IETIP},
  author       = {Justin Varghese and Saudia Subhash and Kamalraj Subramaniam and Kuttaiyur Palaniswamy Sridhar},
  doi          = {10.1049/iet-ipr.2018.5707},
  journal      = {IET Image Processing},
  month        = {6},
  number       = {8},
  pages        = {1529-1538},
  shortjournal = {IET Image Process.},
  title        = {Adaptive gaussian notch filter for removing periodic noise from digital images},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Segmentation of the pulmonary nodule and the attached
vessels in the CT scan of the chest using morphological features and
topological skeleton of the nodule. <em>IETIP</em>, <em>14</em>(8),
1520–1528. (<a href="https://doi.org/10.1049/iet-ipr.2019.1054">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nowadays, the proficiency of Computer-Aided Diagnosis systems for early diagnosis of malignant nodules in baseline Computed Tomography (CT) scan of chest crucially depends on the authenticity of the segmented nodule. In this study, the authors introduce a new morphological feature called solidity radius (SR). They then employ this feature in the new segmentation framework for the automatic segmentation of nodule and the attached vessels around the seed point on the nodule, delineated by an expert. In the framework, they extract the SR and the curvature features and employ them to determine the candidate pixels of the nodule. They then use the convex-hull image of the candidate pixels to surround the nodule area. Afterward, using the region growing on the Hessian-based vesselness enhancement map, the attached vessels are labelled. Finally, they apply the traditional solidity feature of the segmented nodule and the pattern of the related skeleton to prune the false positive pieces. They validate the introduced approach on two datasets, including 56 and 481 CTs (containing 1205 nodules). They show the proficiency of their SR-based approach compared to the state-of-the-art methods with average Dice Similarity Coefficients of 77.98 and 77.47% for the two datasets, respectively.},
  archive      = {J_IETIP},
  author       = {Mahsa Bank Tavakoli and Mahdi Orooji and Mehdi Teimouri and Ramita Shahabifar},
  doi          = {10.1049/iet-ipr.2019.1054},
  journal      = {IET Image Processing},
  month        = {6},
  number       = {8},
  pages        = {1520-1528},
  shortjournal = {IET Image Process.},
  title        = {Segmentation of the pulmonary nodule and the attached vessels in the CT scan of the chest using morphological features and topological skeleton of the nodule},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Optical flow refinement using iterative propagation under
colour, proximity and flow reliability constraints. <em>IETIP</em>,
<em>14</em>(8), 1509–1519. (<a
href="https://doi.org/10.1049/iet-ipr.2019.0370">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study proposes a strategy to refine optical flow based on the estimated reliability maps. These maps are firstly estimated a posteriori after the motion estimation by the well-known Kanade–Lucas–Tomasi (KLT). With two new defined criteria based, respectively, on the optical flow local variance and the temporal evolution of the KLT residuals, a global refinement of the motion map is then carried out through two stages under the control of the reliability measures and the colour local homogeneousness. According to the experiments performed on the Middlebury dataset, the authors&#39; reliability measures prove to be a good indicator for the quality of the estimation. Indeed, the correction process increases the global reliability measures and reduces the global errors in a significant way. The experiments show that the quality is higher than classical estimation methods and ranked at 88/168 on Middlebury website.},
  archive      = {J_IETIP},
  author       = {Tan Khoa Mai and Michèle Gouiffès and Samia Bouchafa},
  doi          = {10.1049/iet-ipr.2019.0370},
  journal      = {IET Image Processing},
  month        = {6},
  number       = {8},
  pages        = {1509-1519},
  shortjournal = {IET Image Process.},
  title        = {Optical flow refinement using iterative propagation under colour, proximity and flow reliability constraints},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Object detection based on RGC mask r-CNN. <em>IETIP</em>,
<em>14</em>(8), 1502–1508. (<a
href="https://doi.org/10.1049/iet-ipr.2019.0057">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Object detection is a crucial topic in computer vision. Mask Region-Convolution Neural Network (R-CNN) based methods, wherein a large intersection over union (IoU) threshold is chosen for high quality samples, have often been employed for object detection. However, the detection performance of such methods deteriorates when samples are reduced. To address this, the authors propose an improved Mask R-CNN-based method: the ResNet Group Cascade (RGC) Mask R-CNN. First, they compared ResNet with different layers, finding that ResNeXt-101-64 × 4d is superior to other backbone networks. Secondly, during the training of the test model, the performance of Mask R-CNN suffered from a small batch processing scale, resulting in inaccurately calculated mean and variance; thus, group normalisation was added to the backbone, feature pyramid network neck and bounding box head of the network. Finally, the higher the intersection of Mask R-CNN than the threshold, the easier it is to obtain high-quality samples. However, blindly selecting a high threshold leads to sample reduction and overfitting. Thus, a proposed cascade network configuration with three IoU thresholds was utilised in the process of model training. The model was trained and tested on the COCO and PASCAL VOC07 datasets. Their proposed algorithm demonstrated superior performance compared to that of the Mask R-CNN.},
  archive      = {J_IETIP},
  author       = {Minghu Wu and Hanhui Yue and Juan Wang and Yongxi Huang and Min Liu and Yuhan Jiang and Cong Ke and Cheng Zeng},
  doi          = {10.1049/iet-ipr.2019.0057},
  journal      = {IET Image Processing},
  month        = {6},
  number       = {8},
  pages        = {1502-1508},
  shortjournal = {IET Image Process.},
  title        = {Object detection based on RGC mask R-CNN},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Deep detector classifier (DeepDC) for moving objects
segmentation and classification in video surveillance. <em>IETIP</em>,
<em>14</em>(8), 1490–1501. (<a
href="https://doi.org/10.1049/iet-ipr.2019.0769">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this study, the authors present a new approach to segment and classify moving objects in video sequences by combining an unsupervised anomaly discovery framework called DeepSphere and generative adversarial networks. The proposed deep detector classifier employs and validates DeepSphere, which aims mainly to identify the anomalous cases in the spatial and temporal context in order to perform foreground objects segmentation. For post-processing, some morphological operations are considered to better segment and extract the desired objects. Finally, they take advantage of the power of generative models, which recognise the problem of semi-supervised learning as a specific missing data imputation task in order to classify the segmented objects. They evaluate the method with multiple datasets and the results confirm the effectiveness of the proposed approach, which achieves superior performance over the state-of-the-art methods having the capabilities of segmenting and classifying moving objects from videos surveillance.},
  archive      = {J_IETIP},
  author       = {Sirine Ammar and Thierry Bouwmans and Nizar Zaghden and Mahmoud Neji},
  doi          = {10.1049/iet-ipr.2019.0769},
  journal      = {IET Image Processing},
  month        = {6},
  number       = {8},
  pages        = {1490-1501},
  shortjournal = {IET Image Process.},
  title        = {Deep detector classifier (DeepDC) for moving objects segmentation and classification in video surveillance},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Pulmonary nodule risk classification in adenocarcinoma from
CT images using deep CNN with scale transfer module. <em>IETIP</em>,
<em>14</em>(8), 1481–1489. (<a
href="https://doi.org/10.1049/iet-ipr.2019.0248">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Pulmonary nodules risk classification in adenocarcinoma is essential for early detection of lung cancer and clinical treatment decision. Improving the level of early diagnosis and the identification of small lung adenocarcinoma has been always an important topic for imaging studies. In this study, the authors propose a deep convolutional neural network (CNN) with scale-transfer module (STM) and incorporate multi-feature fusion operation, named STM-Net. This network can amplify small targets and adapt to different resolution images. The evaluation data were obtained from the computed tomography (CT) database provided by Zhongshan Hospital Fudan University (ZSDB). All data have a pathological label and their lung adenocarcinomas risk are classified into four categories: atypical adenomatous hyperplasia, adenocarcinoma in situ, minimally invasive adenocarcinoma, and invasive adenocarcinoma. The authors’ deep learning network STM-Net was trained and tested for the risk stage prediction. The accuracy and the average area under the receiver operating characteristic curve achieved by their method are 95.455% and 0.987 for the ZSDB dataset. The experimental results show that STM-Net largely boosts classification accuracy on the pulmonary nodules classification compared with state-of-the-art approaches. The proposed method will be an effective auxiliary to help physicians diagnosis pulmonary nodules risk classification in adenocarcinoma in early-stage.},
  archive      = {J_IETIP},
  author       = {Jie Zheng and Dawei Yang and Yu Zhu and Wanghuan Gu and Bingbing Zheng and Chunxue Bai and Lin Zhao and Hongcheng Shi and Jie Hu and Shaohua Lu and Weibin Shi and Ningfang Wang},
  doi          = {10.1049/iet-ipr.2019.0248},
  journal      = {IET Image Processing},
  month        = {6},
  number       = {8},
  pages        = {1481-1489},
  shortjournal = {IET Image Process.},
  title        = {Pulmonary nodule risk classification in adenocarcinoma from CT images using deep CNN with scale transfer module},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Fast algorithm for large-scale subspace clustering by LRR.
<em>IETIP</em>, <em>14</em>(8), 1475–1480. (<a
href="https://doi.org/10.1049/iet-ipr.2018.6596">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Low-rank representation (LRR) and its variants have been proved to be powerful tools for handling subspace clustering problems. Most of these methods involve a sub-problem of computing the singular value decomposition of an matrix, which leads to a computation complexity of . Obviously, when n is large, it will be time consuming. To address this problem, the authors introduce a fast solution, which reformulates the large-scale problem to an equal form with smaller size. Thus, the proposed method remarkably reduces the computation complexity by solving a small-scale problem. Theoretical analysis proves the efficiency of the proposed model. Furthermore, we extend LRR to a general model by using Schatten p -norm instead of nuclear norm and present a fast algorithm to solve large-scale problem. Experiments on MNIST and Caltech101 databse illustrate the equivalence of the proposed algorithm and the original LRR solver. Experimental results show that the proposed algorithm is remarkably faster than traditional LRR algorithm, especially in the case of large sample number.},
  archive      = {J_IETIP},
  author       = {Deyan Xie and Feiping Nie and Quanxue Gao and Song Xiao},
  doi          = {10.1049/iet-ipr.2018.6596},
  journal      = {IET Image Processing},
  month        = {6},
  number       = {8},
  pages        = {1475-1480},
  shortjournal = {IET Image Process.},
  title        = {Fast algorithm for large-scale subspace clustering by LRR},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Single image rain removal with reusing original input
squeeze-and-excitation network. <em>IETIP</em>, <em>14</em>(8),
1467–1474. (<a href="https://doi.org/10.1049/iet-ipr.2019.0716">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this study, the authors propose a novel network architecture to address the problem of removing rain streaks from single images. To strengthen the representational power of the network, they adopt the squeeze-and-excitation block in the network. Furthermore, they propose a new network connection called reusing original input (ROI). The ROI connection reuses the original input of the network and can provide more texture details of the background. These details can be useful for the restoration of the image after removing the rain streaks. Batch normalisation is applied to further improve the rain removal performance of the network. Despite the fact that the network is trained on synthetic data, experimental results show that the proposed network has a comparable performance on both synthetic images and real-world images to the state-of-the-art methods.},
  archive      = {J_IETIP},
  author       = {Meihua Wang and Lunbao Chen and Yun Liang and Yuexing Hao and Haijun He and Chao Li},
  doi          = {10.1049/iet-ipr.2019.0716},
  journal      = {IET Image Processing},
  month        = {6},
  number       = {8},
  pages        = {1467-1474},
  shortjournal = {IET Image Process.},
  title        = {Single image rain removal with reusing original input squeeze-and-excitation network},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Joint low-rank project embedding and optimal mean principal
component analysis. <em>IETIP</em>, <em>14</em>(8), 1457–1466. (<a
href="https://doi.org/10.1049/iet-ipr.2019.1027">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Principal component analysis (PCA) is the most widely used unsupervised dimensionality reduction approach. A number of variants of PCA have been proposed to improve the robustness of the algorithm. However, the existing methods either cannot select the useful features consistently or is still sensitive to outliers. In order to reveal the intrinsic manifold structure and preserve the global structure of data, it is needed to learn more efficient optimal projection matrix for sample sets with outliers. To this end, the authors propose a novel PCA, named low-rank project embedding and optimal mean principal component analysis (abbreviated as LRPE-OMPCA), which can learn the optimal mean and the optimal projection matrix and preserve the global geometric information and discriminative structure captured by the self-representation coefficient weight matrix into the low-dimensional embedding subspace. Thus, not only can the proposed method further reduce the influence of outliers but also can discard the useless features, which effectively improve the robustness of the method. An effective iterative algorithm to solve the LRPE-OMPCA is designed. Experimental results on several image databases illustrate the robustness and effectiveness of the proposed method.},
  archive      = {J_IETIP},
  author       = {Xiuhong Chen and Huiqiang Sun},
  doi          = {10.1049/iet-ipr.2019.1027},
  journal      = {IET Image Processing},
  month        = {6},
  number       = {8},
  pages        = {1457-1466},
  shortjournal = {IET Image Process.},
  title        = {Joint low-rank project embedding and optimal mean principal component analysis},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Survey on visual sentiment analysis. <em>IETIP</em>,
<em>14</em>(8), 1440–1456. (<a
href="https://doi.org/10.1049/iet-ipr.2019.1270">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Visual Sentiment Analysis aims to understand how images affect people, in terms of evoked emotions. Although this field is rather new, a broad range of techniques have been developed for various data sources and problems, resulting in a large body of research. This paper reviews pertinent publications and tries to present an exhaustive overview of the field. After a description of the task and the related applications, the subject is tackled under different main headings. The paper also describes principles of design of general Visual Sentiment Analysis systems from three main points of view: emotional models, dataset definition, feature design. A formalization of the problem is discussed, considering different levels of granularity, as well as the components that can affect the sentiment toward an image in different ways. To this aim, this paper considers a structured formalization of the problem which is usually used for the analysis of text, and discusses it&#39;s suitability in the context of Visual Sentiment Analysis. The paper also includes a description of new challenges, the evaluation from the viewpoint of progress toward more sophisticated systems and related practical applications, as well as a summary of the insights resulting from this study.},
  archive      = {J_IETIP},
  author       = {Alessandro Ortis and Giovanni Maria Farinella and Sebastiano Battiato},
  doi          = {10.1049/iet-ipr.2019.1270},
  journal      = {IET Image Processing},
  month        = {6},
  number       = {8},
  pages        = {1440-1456},
  shortjournal = {IET Image Process.},
  title        = {Survey on visual sentiment analysis},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Review of image processing approaches for detecting plant
diseases. <em>IETIP</em>, <em>14</em>(8), 1427–1439. (<a
href="https://doi.org/10.1049/iet-ipr.2018.6210">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {There is intense pressure on agricultural productivity due to the ever-growing population. Several diseases affect crop yield and thus, effective control of these can significantly improve the production of food for all. In this regard, detection of diseases at an early stage and quantification of the severity, in general, has acquired urgent attention of the researchers. In this study, a summary of prevalent techniques and methodologies used for the detection, quantification and classification of diseases is presented to understand the scope of improvement. The study pays attention to critical gaps that exist in available approaches and enhance them for the early prediction of diseases. Diseases affect almost all parts of plants, e.g. root, stem, flower, leaf; a manifestation in different ways for different parts of the plant of the same disease presents a challenge for researchers. This study extends the review work published by JGA Barbedo in 2013, as there have been significant advances and numerous new techniques introduced since then. A novel approach of classifying and categorisation of the existing techniques based on pathogen types is a significant contribution by the authors in this study.},
  archive      = {J_IETIP},
  author       = {Aditya Sinha and Rajveer Singh Shekhawat},
  doi          = {10.1049/iet-ipr.2018.6210},
  journal      = {IET Image Processing},
  month        = {6},
  number       = {8},
  pages        = {1427-1439},
  shortjournal = {IET Image Process.},
  title        = {Review of image processing approaches for detecting plant diseases},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Property-based shadow detection and removal method for
licence plate image. <em>IETIP</em>, <em>14</em>(7), 1415–1425. (<a
href="https://doi.org/10.1049/iet-ipr.2018.5660">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Shadow detection and removal is a classic research in the field of image processing. This work aims to address the problem of shadow detection and removal for licence plate. The shadow detection and removal method based on inherent properties of licence plate is presented in this study. First, an S–V-channel-property-based shadow detection algorithm is investigated to extract the background-shaded area of licence plate. Afterwards, a shadow edge location algorithm is proposed to detect the complete shadow edge. Then, the shadow is removed on H, S, and V channels with different strategies, respectively. Also, a filter-based median filtering algorithm is employed to remove the pseudo-edge. Finally, the proposed method is verified through comparing with other generic shadow detection and removal methods quantitatively and qualitatively.},
  archive      = {J_IETIP},
  author       = {Fei Gao and Yunjing Xu and Yisu Ge and Shufang Lu and Yuanming Zhang},
  doi          = {10.1049/iet-ipr.2018.5660},
  journal      = {IET Image Processing},
  month        = {5},
  number       = {7},
  pages        = {1415-1425},
  shortjournal = {IET Image Process.},
  title        = {Property-based shadow detection and removal method for licence plate image},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020b). Non-linear calibration optimisation based on the
levenberg–marquardt algorithm. <em>IETIP</em>, <em>14</em>(7),
1402–1414. (<a href="https://doi.org/10.1049/iet-ipr.2019.1489">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {An outstanding calibration algorithm is the most important factor that affects the precision of attitude measurement. This study proposes a non-linear optimisation algorithm to refine the solutions of the initial guess obtained using the Zhang&#39;s technique, the Bouget&#39;s technique, or the Hartley&#39;s algorithm. Large sets of point correspondences were adopted to test the validity of the proposed method. Extensive practical experiments demonstrated that the proposed method can significantly improve the accuracy of calibration and ultimately obtains higher measurement precision. The error of the reprojection in the proposed method was &lt;0.13 px. At a range of 1 m, the error rate was 0.5% for the length test and about 3% for the angle test. This study proposes a new method to calibrate the relationship between laser radar and the camera. Binocular vision was used to reconstruct the point cloud of the non-cooperative target. At the same time, data was also obtained using laser radar. Finally, the two groups of systems were fused. Accurate and dense three-dimensional information of the target was obtained. It could not only obtain the dense pose information of the target surface but also the texture and colour feature information of the target surface.},
  archive      = {J_IETIP},
  author       = {Guoliang Hu and Zuofeng Zhou and Jianzhong Cao and Huimin Huang},
  doi          = {10.1049/iet-ipr.2019.1489},
  journal      = {IET Image Processing},
  month        = {5},
  number       = {7},
  pages        = {1402-1414},
  shortjournal = {IET Image Process.},
  title        = {Non-linear calibration optimisation based on the Levenberg–Marquardt algorithm},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Efficient image noise estimation based on skewness
invariance and adaptive noise injection. <em>IETIP</em>, <em>14</em>(7),
1393–1401. (<a href="https://doi.org/10.1049/iet-ipr.2019.1548">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The precise estimation of the noise level is a crucial issue in image processing. In this study, the authors propose a new method for noise standard deviation (STD) estimation from natural images based on skewness-scale invariance in the transform domain and an adaptive noise injection strategy. The method is divided into two steps. The first step assumes that the natural clean image has the property of constancy of skewness in the transform domain. Then, a preliminary noise estimation method based on skewness invariance is designed by solving a constrained non-linear optimisation problem. The second step involves noise rectification via noise injection. According to the phenomenon that compared with the high-noise circumstance, the error of preliminary estimation is more serious under a low amount of noise, the noise STD is re-estimated by injecting another noise for which the STD is known. In addition, the threshold model with respect to image complexity is established to identify whether a second estimation is needed. The experimental results demonstrate the efficacy of the proposed method and performance is superior to other state-of-the-art methods.},
  archive      = {J_IETIP},
  author       = {Ben Ma and Jincao Yao and Yanfen Le and Chuan Qin and Heng Yao},
  doi          = {10.1049/iet-ipr.2019.1548},
  journal      = {IET Image Processing},
  month        = {5},
  number       = {7},
  pages        = {1393-1401},
  shortjournal = {IET Image Process.},
  title        = {Efficient image noise estimation based on skewness invariance and adaptive noise injection},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Improved water quality mapping based on cross-fusion of
sentinel-2 and landsat-8 imageries. <em>IETIP</em>, <em>14</em>(7),
1382–1392. (<a href="https://doi.org/10.1049/iet-ipr.2019.1503">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study proposed methods based on Sentinel-2 and Landsat-8 cross-fusion for improving water quality mapping (WQM). Therefore, four traditional fusion methods including intensity–hue–saturation, Gram–Schmidt transform, wavelet transform and Brovey transform and different scenarios of cross-fusion have been implemented. The proposed cross-fusion methods highly improved the correlation coefficient (CR) between the images and the water quality parameter (WQP). Considering the higher CR values, the created WQP maps showed very good accuracy, in which the root-mean-square error values were 0.03, 0.59, 0.96, 0.26 and 279.76 for potential hydrogen (PH), dissolved oxygen (DO), chemical oxygen demand (COD), biological oxygen demand (BOD) and electrical conductivity (EC) maps, respectively. Also, the effect of considering 1 px value or the mean of a 3×3 window of the input images for calculating the regression models on the accuracy of the final maps was tested. Only the best outputs for mapping PH and DO parameters were based on applying the mean of a 3×3 window. The results also showed that increasing the window size could increase the computational complexity and decrease the WQM accuracy. Comparing the output maps with the traditional maps confirmed the higher accuracy of the proposed methods.},
  archive      = {J_IETIP},
  author       = {Kazem Rangzan and Mostafa Kabolizadeh and Danya Karimi},
  doi          = {10.1049/iet-ipr.2019.1503},
  journal      = {IET Image Processing},
  month        = {5},
  number       = {7},
  pages        = {1382-1392},
  shortjournal = {IET Image Process.},
  title        = {Improved water quality mapping based on cross-fusion of sentinel-2 and landsat-8 imageries},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Novel deep learning model for facial expression recognition
based on maximum boosted CNN and LSTM. <em>IETIP</em>, <em>14</em>(7),
1373–1381. (<a href="https://doi.org/10.1049/iet-ipr.2019.1188">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning has received increasing attention in all fields and has made considerable progress in facial expression recognition (FER). Mainly, the conventional FER methods are trained for constrained datasets, which may not operate well for real-time images. Such real-time image sequences limit the accuracy and efficacy of the traditional system. In this work, the authors present a novel deep learning framework which combines convolutional neural network (CNN) with long short-term memory (LSTM) cell for real-time FER. The novel framework has three main aspects: (i) two different pre-processing techniques are employed to handle illumination variances and to preserve subtle edge information of each image; (ii) correspondingly, the pre-processed images are inputted to the two individual CNN architecture which extracts the spatial features very effectively; (iii) spatial feature maps from two individual CNN layers are fused and integrated with an LSTM layer which extracts temporal relations between the successive frames. They experimented the authors’ proposed method on three publically available FER databases and also with self-created database. With pre-processing, their proposed model achieves comparable and better results to the state-of-the-art.},
  archive      = {J_IETIP},
  author       = {Saranya Rajan and Poongodi Chenniappan and Somasundaram Devaraj and Nirmala Madian},
  doi          = {10.1049/iet-ipr.2019.1188},
  journal      = {IET Image Processing},
  month        = {5},
  number       = {7},
  pages        = {1373-1381},
  shortjournal = {IET Image Process.},
  title        = {Novel deep learning model for facial expression recognition based on maximum boosted CNN and LSTM},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Tablet identification using support vector machine based
text recognition and error correction by enhanced n-grams algorithm.
<em>IETIP</em>, <em>14</em>(7), 1366–1372. (<a
href="https://doi.org/10.1049/iet-ipr.2019.0993">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unidentified and/or misidentified tablets always present challenges to both patients and health care professionals alike. Consumption of these misidentified tablets often results in adverse drug reaction and sometimes may even cause ill health leading to death. Thus, identification of unknown tablets is an important task in the medical industry. This study proposes an algorithm that uses the text imprinted on the tablet images to identify unknown images. Text imprinted on pills often contains important information that can be used to identify tablets. In this study, multiple feature sets were extracted from tablet images. The proposed work first identifies the text region, from which the text is recognised using support vector machine classifier. A post processing algorithm based on the confusion model combined with n -grams is used to correct the substitution, insertion and deletion errors in the text recognised. Finally, the corrected text is matched with a template tablet database to identify similar tablets. Experimental results showed that the proposed system is efficient with respect to accuracy and can be safely used by both common public and health care professionals to identify tablets.},
  archive      = {J_IETIP},
  author       = {A.B. Dhivya and M. Sundaresan},
  doi          = {10.1049/iet-ipr.2019.0993},
  journal      = {IET Image Processing},
  month        = {5},
  number       = {7},
  pages        = {1366-1372},
  shortjournal = {IET Image Process.},
  title        = {Tablet identification using support vector machine based text recognition and error correction by enhanced n-grams algorithm},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Chua’s diode and strange attractor: A three-layer
hardware–software co-design for medical image confidentiality.
<em>IETIP</em>, <em>14</em>(7), 1354–1365. (<a
href="https://doi.org/10.1049/iet-ipr.2019.0562">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Medical image security relies upon the arrival of improved techniques for enhancing the confusion as well as diffusion processes in the pixel level manipulations. Non-linear circuits constructed with hardware components can be employed for hardware–software co-design in medical image encryption. The proposed approach has adopted Chua diode circuit for the generation of 256 × 256 random synthetic image with the entropy of 7.9972. This synthetic image has been utilised to scramble the DICOM image for the second level of diffusion as the cellular automata provides the first level of scrambling and diffusion. The average correlation coefficients of encrypted pixels are 0.00204, −0.00298 and −0.00054 in three directions. The encrypted pixels pass the NIST SP 800-22 test suite and offer resistance to statistical, differential and chosen plain text attacks.},
  archive      = {J_IETIP},
  author       = {Sundararaman Rajagopalan and Siva Poori and Mukund Narasimhan and Sivaraman Rethinam and Chandrasekar Vallipalayam Kuppusamy and Ramalingam Balasubramanian and Vijaya Moorthi Paramasivam Annamalai and Amirtharajan Rengarajan},
  doi          = {10.1049/iet-ipr.2019.0562},
  journal      = {IET Image Processing},
  month        = {5},
  number       = {7},
  pages        = {1354-1365},
  shortjournal = {IET Image Process.},
  title        = {Chua&#39;s diode and strange attractor: A three-layer hardware–software co-design for medical image confidentiality},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Automatic segmentation for pulmonary nodules in CT images
based on multifractal analysis. <em>IETIP</em>, <em>14</em>(7),
1347–1353. (<a href="https://doi.org/10.1049/iet-ipr.2019.0884">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To characterise pulmonary nodules, the volume analysis of solitary pulmonary nodules (SPNs) is important for diagnosis. To accurately estimate the volume of pulmonary nodules and reduce physician workloads, this study presents an intelligent method to automatically detect and segment pulmonary nodules from computed tomography (CT) scans. To detect nodule candidates, a modified ‘Iso’ capacity measure based on multifractals analysis is proposed to reflect the local singularity properties and capture the textures of pulmonary nodules. Subsequently, the proposed two-stage false nodules pruning procedure is applied to categorise a true SPN from nodule candidates. Preliminary results demonstrate that the proposed method can automatically and successfully detect and segment 118 SPNs in 118 CT images from a public lung database. The average and standard deviation of segmentation overlap measures are 0.71 and 0.08, respectively. The authors’ method is competitive compared to the methods reported in the literature. Besides, the proposed method can avoid the problem of selecting seeds when applying region-growing techniques to segment pulmonary nodules.},
  archive      = {J_IETIP},
  author       = {Cheng-Hsiung Lee and Jung-Sing Jwo},
  doi          = {10.1049/iet-ipr.2019.0884},
  journal      = {IET Image Processing},
  month        = {5},
  number       = {7},
  pages        = {1347-1353},
  shortjournal = {IET Image Process.},
  title        = {Automatic segmentation for pulmonary nodules in CT images based on multifractal analysis},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Multi-focus image fusion with siamese self-attention
network. <em>IETIP</em>, <em>14</em>(7), 1339–1346. (<a
href="https://doi.org/10.1049/iet-ipr.2019.0883">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, convolutional neural networks (CNNs) have achieved impressive progress in multi-focus image fusion (MFF). However, it always fails to capture sufficient discrimination features due to the local receptive field limitations of the convolutional operator, restricting most current CNN-based methods’ performance. To address this issue, by leveraging self-attention (SA) mechanism, the authors propose Siamese SA network (SSAN) for MFF. Specifically, two kinds of SA modules, position SA (PSA) and channel SA (CSA) are utilised to model the long-range dependencies across focused and defocused regions in the multi-focus image, alleviating the local receptive field limitations of convolution operators in CNN. To search a better feature representation of the input image for MFF, the captured features obtained by PSA and CSA are further merged through a learnable 1 × 1 convolution operator. The whole pipeline is in a Siamese network fashion to reduce the complexity. After training, the authors SSAN can accomplish well the fusion task with no post-processing. Experiments demonstrate that their approach outperforms other current state-of-the-art methods, not only in subjective visual perception but also in the quantitative assessment.},
  archive      = {J_IETIP},
  author       = {Xiaopeng Guo and Lingyu Meng and Liye Mei and Yueyun Weng and Hengqing Tong},
  doi          = {10.1049/iet-ipr.2019.0883},
  journal      = {IET Image Processing},
  month        = {5},
  number       = {7},
  pages        = {1339-1346},
  shortjournal = {IET Image Process.},
  title        = {Multi-focus image fusion with siamese self-attention network},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Construction of high dynamic range image based on gradient
information transformation. <em>IETIP</em>, <em>14</em>(7), 1327–1338.
(<a href="https://doi.org/10.1049/iet-ipr.2019.0118">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study proposes a fusion method for high dynamic range images based on gradient information transformation. In the proposed work, the authors first measure the three exposure weights of the source images, namely, local contrast, luminance and spatial structure. Then, the exposure weights are merged through a multi-scale Laplacian pyramid scheme. For the weight maps measurement, the dense scale-invariant feature transform method is used to calculate the local contrast around each pixel location, rather than a single pixel. The image luminance levels are computed in the gradient domain to get more visual information and the authors leverage the dictionary learning to effectively extract the luminance of images. Additionally, to better preserve the spatial structure of the source images, the just-noticeable-distortion technique is employed. By comparing the experimental results both subjectively and objectively, it is evident that the proposed method represents an improvement over some exciting methods.},
  archive      = {J_IETIP},
  author       = {Yanyu Liu and Dongming Zhou and Rencan Nie and Ruichao Hou and Zhaisheng Ding},
  doi          = {10.1049/iet-ipr.2019.0118},
  journal      = {IET Image Processing},
  month        = {5},
  number       = {7},
  pages        = {1327-1338},
  shortjournal = {IET Image Process.},
  title        = {Construction of high dynamic range image based on gradient information transformation},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). No-reference video quality assessment method based on
spatio-temporal features using the ELM algorithm. <em>IETIP</em>,
<em>14</em>(7), 1316–1326. (<a
href="https://doi.org/10.1049/iet-ipr.2019.0941">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This work presents an application of the extreme learning machine (ELM) algorithm based on a single-hidden layer feedforward neural network for no-reference video quality assessment. The present research introduces an augmented version of ELM through simple stop criteria, which proved the effectiveness of the video quality assessment method. The authors present empirical studies using LIVE video data base show that the proposed method delivers accuracy (Pearson&#39;s correlation coefficient) and monotonicity (Spearman&#39;s correlation coefficient) with subjective scores against no-reference, Joint Photographic Experts Group No-Reference, metric and full-reference metrics, for instance, peak signal-to-noise ratio, structural similarity (SSIM) and multi-scale-SSIM indexes, and the proposed method is suitable for quality monitoring of video transmission and reception system.},
  archive      = {J_IETIP},
  author       = {Wyllian Bezerra da Silva and Alexandre Mikowski and Rafael Machado Casali},
  doi          = {10.1049/iet-ipr.2019.0941},
  journal      = {IET Image Processing},
  month        = {5},
  number       = {7},
  pages        = {1316-1326},
  shortjournal = {IET Image Process.},
  title        = {No-reference video quality assessment method based on spatio-temporal features using the ELM algorithm},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Texture and colour region separation based image retrieval
using probability annular histogram and weighted similarity matching
scheme. <em>IETIP</em>, <em>14</em>(7), 1303–1315. (<a
href="https://doi.org/10.1049/iet-ipr.2018.6619">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Content-based image retrieval (CBIR) uses primitive image features for retrieval of similarimages from a dataset. Generally, researchers extract these visual features fromthe whole image. Therefore, the extracted features contain overlappedinformation of texture, colour, and shape features, and it is a criticalchallenge in the field of CBIR. This problem can be overcome by extracting thecolour features from the colour as well as shape and texture features from theintensity dominant part only. In this study, the authors have proposed aniterative algorithm to separate colour and texture dominant part of the imageinto two different images. Here, a combination of edge maps and gradients hasbeen used to achieve separate colour and texture images. Further,scale-invariant feature transform and 2D dual-tree complex wavelet transform hasbeen realised to extract unique shape and texture features from the textureimage. Simultaneously, a probability-based semantic centred annular histogramhas been suggested to extract unique colour features from the colour image.Finally, a novel weighted distance-based feature comparison scheme has beenproposed for similarity matching and retrieval. All the image retrievalexperiments have been carried out on seven standard datasets and demonstratedsignificant improvements over other state-of-arts CBIR systems},
  archive      = {J_IETIP},
  author       = {Jitesh Pradhan and Sumit Kumar and Arup Kumar Pal and Haider Banka},
  doi          = {10.1049/iet-ipr.2018.6619},
  journal      = {IET Image Processing},
  month        = {5},
  number       = {7},
  pages        = {1303-1315},
  shortjournal = {IET Image Process.},
  title        = {Texture and colour region separation based image retrieval using probability annular histogram and weighted similarity matching scheme},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Adaptive frequency median filter for the salt and pepper
denoising problem. <em>IETIP</em>, <em>14</em>(7), 1291–1302. (<a
href="https://doi.org/10.1049/iet-ipr.2019.0398">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, the authors propose an adaptive frequency median filter (AFMF) to remove the salt and pepper noise. AFMF uses the same adaptive condition of adaptive median filter (AMF). However, AFMF employs frequency median to restore grey values of the corrupted pixels instead of the median of AMF. The frequency median can exclude noisy pixels from evaluating a grey value of the centre pixel of the considered window, and it focuses on the uniqueness of grey values. Hence, the frequency median produces a grey value closer to the original grey value than the one by the median of AMF. Therefore, AFMF outperforms AMF. In experiments, the authors tested the proposed method on a variety of natural images of the MATLAB library, as well as the TESTIMAGES data set. Additionally, they also compared the denoising results of AFMF to the ones of other state-of-the-art denoising methods. The results showed that AFMF denoises more effectively than other methods.},
  archive      = {J_IETIP},
  author       = {Uğur Erkan and Serdar Enginoğlu and Dang N.H. Thanh and Le Minh Hieu},
  doi          = {10.1049/iet-ipr.2019.0398},
  journal      = {IET Image Processing},
  month        = {5},
  number       = {7},
  pages        = {1291-1302},
  shortjournal = {IET Image Process.},
  title        = {Adaptive frequency median filter for the salt and pepper denoising problem},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Super-resolution mapping of hyperspectral satellite images
using hybrid genetic algorithm. <em>IETIP</em>, <em>14</em>(7),
1281–1290. (<a href="https://doi.org/10.1049/iet-ipr.2018.5108">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To assess the rate of sedimentation and the consequent reduction in the storage capacity, periodical capacity surveys of multi-purpose reservoirs is essential. Hydrographic surveys and acoustic surveys are time-consuming and expensive. The limited availability and high cost of the high-resolution images require a different methodology to accurately estimate the water-spread area of the reservoir. In this study, 30 m resolution hyperspectral image (hyperion) and multi-spectral image (The Earth Observing One (EO-1) advanced land imager) are used to estimate the water-spread area of the Peechi Reservoir, South India. A hybrid genetic algorithm (GA)-based super-resolution mapping approach is developed and demonstrated, which incorporates the multi-objective GA and Hopfield neural network (HNN). The hybrid GA-based super-resolution mapping approach gives a global optimum solution in half of the original computation time. Furthermore, mapping approach gives an error of 6.38% for the multi-spectral image and a lesser error of 3.86% for the hyperspectral image, while the HNN-based super-resolution mapping approach gives an error of 8.23% for the multi-spectral image and 5.71% for the hyperspectral image. Thus, in this work, an efficient technique based on hybrid GA is presented, which is a useful tool for accurate mapping of water bodies at the sub-pixel scale using hyperspectral imagery.},
  archive      = {J_IETIP},
  author       = {Heltin Genitha Cyril Amala Dhason and Indhumathi Muthaia and Shanmuga Priyaa Sakthivel and Sanjeevi Shanmugam},
  doi          = {10.1049/iet-ipr.2018.5108},
  journal      = {IET Image Processing},
  month        = {5},
  number       = {7},
  pages        = {1281-1290},
  shortjournal = {IET Image Process.},
  title        = {Super-resolution mapping of hyperspectral satellite images using hybrid genetic algorithm},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Automatic cloud segmentation from INSAT-3D satellite image
via IKM and IFCM clustering. <em>IETIP</em>, <em>14</em>(7), 1273–1280.
(<a href="https://doi.org/10.1049/iet-ipr.2018.5271">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cloud extraction and classification from satellite imagery is important for many applications in remote sensing. Satellite images are segmented based on distance, intensity and texture of the images. The popular segmentation algorithms, k -means (KM) and fuzzy c -means (FCM) clustering algorithms, face some problems such as unknown number of groups, unknown initialization and dead centers. In this paper, an unsupervised pixel classification by the KM and FCM algorithms is improved and the selection of centroids is made automatic. The proposed improved k -means (IKM) and improved fuzzy c -means (IFCM) clustering algorithms segment the INSAT-3D satellite&#39;s thermal infrared image into low-level, middle-level, high-level clouds and non-cloudy region. As human beings can easily find the clouds in the satellite images, visible image is used to differentiate the clouds from the background. A threshold is found from the histogram of the visible image to separate the cloudy and non-cloudy pixels. The other three thresholds to divide the clouds into three types are found from the thermal infrared image&#39;s histogram. The segmentation results of IKM and IFCM algorithms are compared with the existing segmentation algorithms. The comparison shows that IFCM algorithm matches well with original image followed by IKM algorithm as compared with existing algorithms.},
  archive      = {J_IETIP},
  author       = {Pugazhenthi A and Lakshmi Sutha Kumar},
  doi          = {10.1049/iet-ipr.2018.5271},
  journal      = {IET Image Processing},
  month        = {5},
  number       = {7},
  pages        = {1273-1280},
  shortjournal = {IET Image Process.},
  title        = {Automatic cloud segmentation from INSAT-3D satellite image via IKM and IFCM clustering},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Fabric defect detection using saliency of multi-scale local
steering kernel. <em>IETIP</em>, <em>14</em>(7), 1265–1272. (<a
href="https://doi.org/10.1049/iet-ipr.2018.5857">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fabric defect detection (FDD) plays an important role in the quality control in textile industry. In this study, the authors propose an efficient FDD method by using the saliency analysis of multi-scale local steering kernel (LSK). In the proposed method, a given RGB fabric image is first converted into the Commission International Eclairage (CIE) L*a*b colour space and then the LSK in each colour channel is computed by the singular value decomposition and the centre surrounding definition. Next, the matrix cosine similarity is employed to measure the similarity between different LSK features for generating the desired defective maps. Finally, a multi-scale averaging fusion scheme is applied to integrate the obtained defective maps at different scales for the final defective map. The experimental results indicate that the proposed method achieves the state-of-the-art performance on FDD compared to the other competitors.},
  archive      = {J_IETIP},
  author       = {Kaibing Zhang and Yadi Yan and Pengfei Li and Junfeng Jing and Zhen Wang and Zenggang Xiong},
  doi          = {10.1049/iet-ipr.2018.5857},
  journal      = {IET Image Processing},
  month        = {5},
  number       = {7},
  pages        = {1265-1272},
  shortjournal = {IET Image Process.},
  title        = {Fabric defect detection using saliency of multi-scale local steering kernel},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Multi-dimensional data modelling of video image action
recognition and motion capture in deep learning framework.
<em>IETIP</em>, <em>14</em>(7), 1257–1264. (<a
href="https://doi.org/10.1049/iet-ipr.2019.0588">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In order to improve the accuracy of small-range human motion recognition in video and the computational efficiency of large-scale data sets, a multi-dimensional data model of motion recognition and motion capture in video image based on deep-learning framework was proposed. First, the moving foreground of the target is extracted by the Gauss mixture model, and the human body is recognised by the gradient histogram. At the second level, the dense trajectory feature and the deep learning feature are fused, according to the integration of global encoding algorithm and convolutional neural network. In the deep learning feature, the fusion of the deep video feature and the video RGB tricolour feature is taken as the feature of deep learning. Finally, the classification is based on the deep learning network model. The simulation experiments based on large-scale real data sets and small-scale gesture data sets show that the algorithm has high recognition accuracy for large-scale data sets and small-scale gesture actions. In addition, Imperial Computer Vision &amp; Learning Lab human behaviour data set is used to classify the experimental data. The average classification accuracy is 85.79%. The algorithm can run at a speed of about 20 frames per second.},
  archive      = {J_IETIP},
  author       = {Peijun Gao and Dan Zhao and Xuanang Chen},
  doi          = {10.1049/iet-ipr.2019.0588},
  journal      = {IET Image Processing},
  month        = {5},
  number       = {7},
  pages        = {1257-1264},
  shortjournal = {IET Image Process.},
  title        = {Multi-dimensional data modelling of video image action recognition and motion capture in deep learning framework},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Computational approach to body mass index estimation from
dressed people in 3D space. <em>IETIP</em>, <em>14</em>(7), 1248–1256.
(<a href="https://doi.org/10.1049/iet-ipr.2019.1170">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Body mass index (BMI) defines as a person&#39;s weight divided by the square of height (BMI ), which is an important indicator of the health condition. The authors study BMI estimation from the three-dimensional (3D) visual data by measuring the correlation between the estimated body volume and BMIs, and then develop an efficient BMI computation method. Their approach consists of body weight and height estimation from normally dressed people in 3D space. To address the influence of loose clothes on body volume estimation, two clothes models are developed to make the volume estimation more accurate. A new RGB-D video dataset is collected for this study, and the reconstructed 3D data are provided by the KinectFusion on depth data. Experimental results show the effectiveness of the approach to work on normal conditions of dressed people. The mean absolute error of the estimated BMI can achieve 2.54 in their experiments.},
  archive      = {J_IETIP},
  author       = {Min Jiang and Yuanyuan Shang and Guodong Guo},
  doi          = {10.1049/iet-ipr.2019.1170},
  journal      = {IET Image Processing},
  month        = {5},
  number       = {7},
  pages        = {1248-1256},
  shortjournal = {IET Image Process.},
  title        = {Computational approach to body mass index estimation from dressed people in 3D space},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Deep3DSCan: Deep residual network and morphological
descriptor based framework forlung cancer classification and 3D
segmentation. <em>IETIP</em>, <em>14</em>(7), 1240–1247. (<a
href="https://doi.org/10.1049/iet-ipr.2019.1164">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the increasing incidence rate of lung cancer patients, early diagnosis could help in reducing the mortality rate. However, accurate recognition of cancerous lesions is immensely challenging owing to factors such as low contrast variation, heterogeneity and visual similarity between benign and malignant nodules. Deep learning techniques have been very effective in performing natural image segmentation with robustness to previously unseen situations, reasonable scale invariance and the ability to detect even minute differences. However, they usually fail to learn domain-specific features due to the limited amount of available data and domain agnostic nature of these techniques. This work presents an ensemble framework Deep3DSCan for lung cancer segmentation and classification. The deep 3D segmentation network generates the 3D volume of interest from computed tomography scans of patients. The deep features and handcrafted descriptors are extracted using a fine-tuned residual network and morphological techniques, respectively. Finally, the fused features are used for cancer classification. The experiments were conducted on the publicly available LUNA16 dataset. For the segmentation, the authors achieved an accuracy of 0.927, significant improvement over the template matching technique, which had achieved an accuracy of 0.927. For the detection, previous state-of-the-art is 0.866, while ours is 0.883.},
  archive      = {J_IETIP},
  author       = {Gaurang Bansal and Vinay Chamola and Pratik Narang and Subham Kumar and Sundaresan Raman},
  doi          = {10.1049/iet-ipr.2019.1164},
  journal      = {IET Image Processing},
  month        = {5},
  number       = {7},
  pages        = {1240-1247},
  shortjournal = {IET Image Process.},
  title        = {Deep3DSCan: Deep residual network and morphological descriptor based framework forlung cancer classification and 3D segmentation},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020b). Support vector machine combined with magnetic resonance
imaging for accurate diagnosis of paediatric pancreatic cancer.
<em>IETIP</em>, <em>14</em>(7), 1233–1239. (<a
href="https://doi.org/10.1049/iet-ipr.2019.1041">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {When performing paediatric pancreatic cancer (PC) diagnosis, magnetic resonance imaging (MRI) will be interfered by many factors, resulting in poor diagnosis. In order to improve the diagnostic effect of MRI on PC, this study is based on machine learning and combines SVM (support vector machine) and MRI detection with the support of image processing technology and improves the diagnostic accuracy of MRI for PC by the computer-assisted method. At the same time, in the research, this study combines the characteristics of MRI detection to construct an MRI detection model based on SVM. In addition, this study obtains test samples through data collection, analyses the performance of the algorithm model through actual cases, and combines image processing to improve the detection effect. Studies have shown that the algorithm model proposed in this study can effectively improve the accuracy of MRI diagnosis of PC and provide a theoretical reference for subsequent related research.},
  archive      = {J_IETIP},
  author       = {Yuling Zhang and Shuchang Wang and Shuqiang Qu and Hongli Zhang},
  doi          = {10.1049/iet-ipr.2019.1041},
  journal      = {IET Image Processing},
  month        = {5},
  number       = {7},
  pages        = {1233-1239},
  shortjournal = {IET Image Process.},
  title        = {Support vector machine combined with magnetic resonance imaging for accurate diagnosis of paediatric pancreatic cancer},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Optimisation analysis of pulmonary nodule diagnostic test
based on deep belief nets. <em>IETIP</em>, <em>14</em>(7), 1227–1232.
(<a href="https://doi.org/10.1049/iet-ipr.2019.1022">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {At present, the rate of missed diagnosis of lung cancer is high. The reason is that the pulmonary nodule phenomenon cannot be effectively monitored due to various interference factors in the actual detection process. In order to improve the detection accuracy, this study combined with the actual situation to analyse the diversity of nodular shape and constructed a deep belief network-based diagnosis model for pulmonary nodules. At the same time, in order to improve the detection effect, this study sets the model to have multi-layer non-linear structure and analyses the previous clinical data to improve the model learning rate and training effect. In addition, in order to verify the performance of the model, the diagnostic effect of the model is studied by comparative experiments. The research shows that the model proposed in this study is higher than the traditional algorithm in detection accuracy, which can provide theoretical reference for subsequent related research.},
  archive      = {J_IETIP},
  author       = {Wei Yang and Wenhua Xia and Yuanliang Xie and Shilong Mao and Rong Li},
  doi          = {10.1049/iet-ipr.2019.1022},
  journal      = {IET Image Processing},
  month        = {5},
  number       = {7},
  pages        = {1227-1232},
  shortjournal = {IET Image Process.},
  title        = {Optimisation analysis of pulmonary nodule diagnostic test based on deep belief nets},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Dynamic image reconstruction and synthesis framework using
deep learning algorithm. <em>IETIP</em>, <em>14</em>(7), 1219–1226. (<a
href="https://doi.org/10.1049/iet-ipr.2019.0900">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This research study shows an effective deformable complex 3D image reconstruction and image synthesis technique by consolidating needed high-level features from a deep convolutional neural network (CNN) system. By recognising the inherent deep features in image patches lead to information discovery in medicinal imaging. Utilising the ADNI and LONI imaging datasets, the performance of the proposed deep learning algorithm image reconstruction and synthesis performance was verified. For validation, various performance indices obtained with the proposed deep learning algorithm were compared with two conventional algorithms namely support vector machine and CNN. Likewise, to reveal the adaptability of the proposed image reconstruction and synthesis system, synthesis and reconstruction experiments were directed on the 7 T cerebrum magnetic resonance image. As presented in the study outcomes, the proposed method can accomplish predominant performance compared with other cutting-edge techniques with either low- or high-level features in terms of the synthesis and reconstruction rate. The proposed algorithm has a training time of 5 s with a structural similarity index of 0.97. In all investigations, the outcome shows that the proposed image reconstruction framework reliably exhibited progressively precise outcomes when contrasted with best in class. Hence, it can be used for possible precise image reconstruction and synthesis related applications.},
  archive      = {J_IETIP},
  author       = {Pandia Rajan Jeyaraj and Edward Rajan Samuel Nadar},
  doi          = {10.1049/iet-ipr.2019.0900},
  journal      = {IET Image Processing},
  month        = {5},
  number       = {7},
  pages        = {1219-1226},
  shortjournal = {IET Image Process.},
  title        = {Dynamic image reconstruction and synthesis framework using deep learning algorithm},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Securing DICOM images by a new encryption algorithm using
arnold transform and vigenère cipher. <em>IETIP</em>, <em>14</em>(6),
1209–1216. (<a href="https://doi.org/10.1049/iet-ipr.2019.0042">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study presents a novel encryption method for the security of DICOM (Digital Imaging and Communications in Medicine) images used in medical applications. The proposed algorithm splits the original image into blocks of size 16 × 16 pixels, then encrypts it through three steps. Firstly, the keys k 1 , k 2 , k 3 and k 4 are transformed from four vectors of 16 pixels to a matrix of 16 × 16 pixels. Then, the proposed algorithm encrypts the image block-by-block using the Vigenère cipher algorithm. For each block, the proposed algorithm modifies the key using Arnold transform. The proposed encryption algorithm is scalable with colour images and JPEG (Joint Photographic Experts Group) compression. The cryptanalysis of the proposed algorithm demonstrates that it passed the cryptography attacks tests with success. Its running time shows that it is faster than a typical and recent image encryption algorithm.},
  archive      = {J_IETIP},
  author       = {Mohamed Boussif and Noureddine Aloui and Adnene Cherif},
  doi          = {10.1049/iet-ipr.2019.0042},
  journal      = {IET Image Processing},
  month        = {5},
  number       = {6},
  pages        = {1209-1216},
  shortjournal = {IET Image Process.},
  title        = {Securing DICOM images by a new encryption algorithm using arnold transform and vigenère cipher},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Generalised deep learning framework for HEp-2 cell
recognition using local binary pattern maps. <em>IETIP</em>,
<em>14</em>(6), 1201–1208. (<a
href="https://doi.org/10.1049/iet-ipr.2019.0705">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The authors propose a novel HEp-2 cell image classifier to improve the automation process of patients&#39; serum evaluation. The authors&#39; solution builds on the recent progress in deep learning based image classification. They propose an ensemble approach using multiple state-of-the-art architectures. They incorporate additional texture information extracted by an improved version of local binary patterns maps, LBP-maps, which enables to create a very effective cell image classifier. This innovative combination is trained on three publicly available datasets and its general applicability is demonstrated through the evaluation on three independent test sets. The presented results show that their approach leads to a general improvement of performance on average on the three public datasets.},
  archive      = {J_IETIP},
  author       = {Buda Bajić and Tomáš Majtner and Joakim Lindblad and Nataša Sladoje},
  doi          = {10.1049/iet-ipr.2019.0705},
  journal      = {IET Image Processing},
  month        = {5},
  number       = {6},
  pages        = {1201-1208},
  shortjournal = {IET Image Process.},
  title        = {Generalised deep learning framework for HEp-2 cell recognition using local binary pattern maps},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Compressive sensed video recovery via iterative thresholding
with random transforms. <em>IETIP</em>, <em>14</em>(6), 1187–1199. (<a
href="https://doi.org/10.1049/iet-ipr.2019.0661">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The authors consider the problem of compressive sensed video recovery via iterative thresholding algorithm. Traditionally, it is assumed that some fixed sparsifying transform is applied at each iteration of the algorithm. In order to improve the recovery performance, at each iteration the thresholding could be applied for different transforms in order to obtain several estimates for each pixel. Then the resulting pixel value is computed based on obtained estimates using simple averaging. However, calculation of the estimates leads to significant increase in reconstruction complexity. Therefore, the authors propose a heuristic approach, where at each iteration only one transform is randomly selected from some set of transforms. First, they present simple examples, when block-based 2D discrete cosine transform is used as the sparsifying transform, and show that the random selection of the block size at each iteration significantly outperforms the case when fixed block size is used. Second, building on these simple examples, they apply the proposed approach when video block-matching and 3D filtering (VBM3D) is used for the thresholding and show that the random transform selection within VBM3D allows to improve the recovery performance as compared with the recovery based on VBM3D with fixed transform.},
  archive      = {J_IETIP},
  author       = {Evgeny Belyaev and Marian Codreanu and Markku Juntti and Karen Egiazarian},
  doi          = {10.1049/iet-ipr.2019.0661},
  journal      = {IET Image Processing},
  month        = {5},
  number       = {6},
  pages        = {1187-1199},
  shortjournal = {IET Image Process.},
  title        = {Compressive sensed video recovery via iterative thresholding with random transforms},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Robust random walk for leaf segmentation. <em>IETIP</em>,
<em>14</em>(6), 1180–1186. (<a
href="https://doi.org/10.1049/iet-ipr.2018.6255">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this study, the authors focus on the task of leaf segmentation under different imaging conditions (e.g. backgrounds and shadows). A new method - robust random walk (RW) is proposed to propagate the prior of user&#39;s specified pixels. Specifically, they first employ RWs to take the relationship of pairwise pixels into consideration. A superpixel-consistent constraint is added to make the edges of segmentation smooth. Owing to the effect of illumination, some parts of a leaf surface are brighter than others and it may further harm the subsequent label propagation. To address this problem, they learn a common subspace by taking into account the illumination of local and non-local pixels. By doing so, it has good adaptability to process noise interfering and non-uniform illumination. In addition, since RW only considers the pairwise relationship of pixels, it will be sensitive to the specified and connected pixels. Thus, they further employ a log-likelihood ratio to predict the probability of a pixel belonging to the background and use it to guide the label propagation. Based on the proposed method, they can obtain a smoothed and robust leaf segmentation. Experimental results on unconstrained leaf images demonstrate the efficiency of their algorithm.},
  archive      = {J_IETIP},
  author       = {Jing Hu and Zhibo Chen and Rongguo Zhang and Meng Yang and Shuai Zhang},
  doi          = {10.1049/iet-ipr.2018.6255},
  journal      = {IET Image Processing},
  month        = {5},
  number       = {6},
  pages        = {1180-1186},
  shortjournal = {IET Image Process.},
  title        = {Robust random walk for leaf segmentation},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Empirical wavelet transform-based fog removal via dark
channel prior. <em>IETIP</em>, <em>14</em>(6), 1170–1179. (<a
href="https://doi.org/10.1049/iet-ipr.2019.0496">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Haze and fog removing from videos and images has got massive concentration in the field of video and image processing because videos and images are severely affected by fog in tracking and surveillance system, object detection. Different defogging techniques proposed so far are based on polarisation, colour-line model, anisotropic diffusion, dark channel prior (DCP) etc. However, these methods are unable to produce output image with desirable quality in the presence of dense fog and sky region. In this study, the authors have proposed a novel fog removal technique where DCP is applied on the low-frequency component of empirical wavelet transformation coefficients of the foggy input image. They apply unsharp masking on wavelet coefficients of the embedded wavelet transformed image for improving the sharpness of the output image. Later contrast limited adaptive histogram equalisation technique is used as a post-processing task to the inverse transformed image for producing the sharp and high contrast output. Finally, the colour and intensity of the contrast-enhanced image are uplifted through S-channel and V-channel gain adjustment. The proposed method provides significant improvement to the overall quality of the output image compared to contemporary techniques. The quantitative and qualitative measurements confirm the claims.},
  archive      = {J_IETIP},
  author       = {Manas Sarkar and Priyanka Rakshit Sarkar and Ujjwal Mondal and Debashis Nandi},
  doi          = {10.1049/iet-ipr.2019.0496},
  journal      = {IET Image Processing},
  month        = {5},
  number       = {6},
  pages        = {1170-1179},
  shortjournal = {IET Image Process.},
  title        = {Empirical wavelet transform-based fog removal via dark channel prior},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Image DAEs based on residual entropy maximum.
<em>IETIP</em>, <em>14</em>(6), 1164–1169. (<a
href="https://doi.org/10.1049/iet-ipr.2018.5929">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image denoising under low signal-to-noise ratio (SNR) and non-Gaussian noise is still a challenging problem in image processing. In this study, the authors prose a kind of improved convolution neural network auto-encoders for image denoising. Different from other priors based methods, the denoising auto-encoders (DAEs) can learn end-to-end mappings from noisy images to the target ones. This study research statistical features of image residual between the restored images and target images. According to the maximum entropy principle, the training loss function of the ordinary DAEs was modified with residual statistics as the constraint condition, and an improved training algorithm was proposed based on augmented Lagrange function method. Thus, the quality of restored image can be improved through removing image information from residual more efficiently. Experiments show not only the denoising effects of improved DAEs is superior to the original mean-square-error loss function DAEs in both peak SNR and Riesz feature similarity metric indexes, but also has the ability to suppress the different types of noises with different levels through a single model.},
  archive      = {J_IETIP},
  author       = {Qian Xiang and Likun Peng and Xueliang Pang},
  doi          = {10.1049/iet-ipr.2018.5929},
  journal      = {IET Image Processing},
  month        = {5},
  number       = {6},
  pages        = {1164-1169},
  shortjournal = {IET Image Process.},
  title        = {Image DAEs based on residual entropy maximum},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Efficient symmetric image encryption by using a novel 2D
chaotic system. <em>IETIP</em>, <em>14</em>(6), 1157–1163. (<a
href="https://doi.org/10.1049/iet-ipr.2019.0551">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The authors know that a common and effective way to protect digital images security are to encrypt these images into white noise image. In this study, the authors have designed a new two-dimensional (2D) chaotic system which is derived from two existing one-dimensional (1D) chaotic maps. The simulation results show that the new 2D chaotic system is able to produce many 2D chaotic maps by selecting different 1D chaotic maps, and which have the wider chaotic ranges and more complex chaotic behaviours compared with the existing 1D chaotic maps. In order to investigate its applications, using the proposed 2D chaotic maps, the authors design a novel image encryption algorithm. First of all, the original image is scrambled by using the chaotic sequences which are generated by new 2D chaotic maps, Arnold transform and Hilbert curve. Then the scrambled image is confused and diffused by chaotic sequences. Finally, the performance of the proposed encryption algorithm is simulated and the experimental results show that the validity and reliability of the proposed algorithm is validated.},
  archive      = {J_IETIP},
  author       = {Huiqing Huang and Shouzhi Yang and Ruisong Ye},
  doi          = {10.1049/iet-ipr.2019.0551},
  journal      = {IET Image Processing},
  month        = {5},
  number       = {6},
  pages        = {1157-1163},
  shortjournal = {IET Image Process.},
  title        = {Efficient symmetric image encryption by using a novel 2D chaotic system},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Joint rain and atmospheric veil removal from single image.
<em>IETIP</em>, <em>14</em>(6), 1150–1156. (<a
href="https://doi.org/10.1049/iet-ipr.2019.0952">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In natural rainy scenes, visibility is significantly degraded by two types of phenomena: specular highlights of nearby individual rain streaks and atmospheric veiling effect caused by distant accumulated rain. However, most existing deraining methods only take the first kind of degradation into consideration, which limits their potential application in heavy rain. In this study, a joint rain and atmospheric veil removal framework is proposed to address this problem. Since rain streaks and rain accumulation are entangled with each other, which is intractable to simulate, causing clean/rainy image pairs of real-world are hard to generate. Hence, after introducing a generalised rain model, which can represent both rain streaks and atmospheric veil physically, the authors do not learn the mapping function between image pairs using deep-learning architecture, but estimate the rain streaks, transmission, and atmospheric light via Gaussian mixture model patch prior and dark channel prior to solve the rain model instead. According to the comprehensive experimental evaluations, the proposed method outperforms other state-of-the-art methods in terms of both high visibility and vivid colour, especially in natural heavy rain scenario.},
  archive      = {J_IETIP},
  author       = {Zetian Mi and Yafei Wang and Congcong Zhao and Fengming Du and Xianping Fu},
  doi          = {10.1049/iet-ipr.2019.0952},
  journal      = {IET Image Processing},
  month        = {5},
  number       = {6},
  pages        = {1150-1156},
  shortjournal = {IET Image Process.},
  title        = {Joint rain and atmospheric veil removal from single image},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Smoke detection in ship engine rooms based on video images.
<em>IETIP</em>, <em>14</em>(6), 1141–1149. (<a
href="https://doi.org/10.1049/iet-ipr.2018.5305">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fire detection systems in ships are based on smoke and heat detection in accordance with safety regulations. The rapid advancement of machine vision technology has led to the development of video smoke detection (VSD) systems. In this study, a VSD system is applied to smoke detection within the engine room of the ship. A dataset for a range of scenarios was created with a smoke generator. The method for smoke detection was based on motion detection and a support vector machine classifier, which was used to make candidate regions and perform classification. A local binary pattern descriptor was used to extract the feature vector. A training set was made from a variety of video frames, randomly. Experimental results seldom produced false positive windows in the non-smoke region. However, if the greyscale value of difference image between background and the smoke is lower than the setting value for motion detection, the system could not detect smoke. Processing time is sufficiently fast for use in real-time smoke detection systems. To install a VSD system on-board a vessel, the authors recommend a performance standard of the system which must be met.},
  archive      = {J_IETIP},
  author       = {Kyung-Min Park and Cherl-O Bae},
  doi          = {10.1049/iet-ipr.2018.5305},
  journal      = {IET Image Processing},
  month        = {5},
  number       = {6},
  pages        = {1141-1149},
  shortjournal = {IET Image Process.},
  title        = {Smoke detection in ship engine rooms based on video images},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Two-stage image smoothing based on edge-patch histogram
equalisation and patch decomposition. <em>IETIP</em>, <em>14</em>(6),
1132–1140. (<a href="https://doi.org/10.1049/iet-ipr.2019.0484">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Part of important structural edges in the image is smoothed due to the small gradients, while the others are preserved with greater gradients. Therefore, the authors propose a two-stage image smoothing method based on edge-patch histogram equalisation and patch decomposition. The authors&#39; purpose is to increase the gradient of important structural edges while reducing the gradient of the texture region. Therefore, they divide the image into edge-patches where the structural edges are concentrated or non-edge-patches where the texture details are concentrated by image segmentation. The edge-patch needs to be equalised by the histograms for increasing the gradient of the edge pixels. All patches are decomposed to extract the smooth component for reducing the gradient of pixels. The smooth component of each patch is smoothed via gradient minimisation. In order to ensure the continuity of the patch boundaries, the edge-patch is inversely equalised. Finally, the whole image is smoothed via gradient minimisation for removing residual textures and seams. Experimental results demonstrate that the proposed method is more competitive in maintaining important structural edges and removing texture details than the state-of-the-art approaches. The proposed method can be applied to many areas of image processing.},
  archive      = {J_IETIP},
  author       = {Yepeng Liu and Xiang Ma and Xuemei Li and Caiming Zhang},
  doi          = {10.1049/iet-ipr.2019.0484},
  journal      = {IET Image Processing},
  month        = {5},
  number       = {6},
  pages        = {1132-1140},
  shortjournal = {IET Image Process.},
  title        = {Two-stage image smoothing based on edge-patch histogram equalisation and patch decomposition},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Image encryption using a combination of grain-128a algorithm
and zaslavsky chaotic map. <em>IETIP</em>, <em>14</em>(6), 1120–1131.
(<a href="https://doi.org/10.1049/iet-ipr.2019.0671">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Encryption is a very important way to secure data in storage and communication, and it is a process of encoding messages or information in such a manner that only authorised persons can access it. Different techniques are used to protect confidential image data against illicit access. In image encryption using chaotic systems, most authors use or design algorithms to generate the initial parameters’ values from the secret key. However, as the key size depends on the number of these parameters, the used algorithms show little sensitivity to small changes in the key. To enhance both security and sensitivity in the choice of the initial parameters, this work combines the use of the Grain-128a stream cipher algorithm with two-dimensional Zaslavsky chaotic map. Firstly, the Grain-128a algorithm is applied to generate the required parameters of Zaslavsky&#39;s chaotic map from a fixed length 256-bit secret key. Secondly, the sequences generated by the chaotic map are used to encrypt the image using a bit confusion and diffusion process. The simulation results on greyscale, colour, binary, indexed, and medical images together with the scores obtained in the evaluation of the algorithm show that the proposed method is very sure and effective in encrypting images of any size and any type.},
  archive      = {J_IETIP},
  author       = {Nawel Balaska and Zahir Ahmida and Aissa Belmeguenai and Selma Boumerdassi},
  doi          = {10.1049/iet-ipr.2019.0671},
  journal      = {IET Image Processing},
  month        = {5},
  number       = {6},
  pages        = {1120-1131},
  shortjournal = {IET Image Process.},
  title        = {Image encryption using a combination of grain-128a algorithm and zaslavsky chaotic map},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). New adaptive histogram equalisation heuristic approach for
contrast enhancement. <em>IETIP</em>, <em>14</em>(6), 1110–1119. (<a
href="https://doi.org/10.1049/iet-ipr.2019.0106">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Contrast enhancement of an image can be performed by using a simple histogram equalisation (HE) technique. However, there are some drawbacks of HE like immense brightness change, artificial effects, over-enhancement, which make it unsuitable to be used in many applications. To resolve these issues a new adaptive heuristic HE approach is proposed in this study. First, probability distribution function (PDF) of the image is calculated. Second, an adaptive parameter is calculated based on the mean and maximum values of that PDF. Thereafter, PDF and cumulative distribution function (CDF) are modified by applying a threshold limit to that adaptive parameter. Finally, another adaptive parameter is finding out by using modified CDF and a new CDF is obtained by using this second adaptive parameter. Traditional HE is then applied with the new CDF to getting the enhanced image. The visual and quantitative results of the proposed method outperform all other state-of-the-art papers and works well both for low and bright contrast images simultaneously. After rigorous experiment, it is concluded that the authors’ method enhances the image contrast very well with no over-enhancement or artificial effects in the images and also preserves the original characteristics of the input images.},
  archive      = {J_IETIP},
  author       = {Shubhi Kansal and Rajiv Kumar Tripathi},
  doi          = {10.1049/iet-ipr.2019.0106},
  journal      = {IET Image Processing},
  month        = {5},
  number       = {6},
  pages        = {1110-1119},
  shortjournal = {IET Image Process.},
  title        = {New adaptive histogram equalisation heuristic approach for contrast enhancement},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Writer identification with n-tuple direction feature from
contour. <em>IETIP</em>, <em>14</em>(6), 1101–1109. (<a
href="https://doi.org/10.1049/iet-ipr.2018.6391">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study introduces an effective solution for text-independent writer identification by generalising contour-hinge feature, which is called n -tuple direction feature. For extracting n -tuple direction feature, the authors first obtain all contours from connected components, then n + 1 points are considered on the contour with a certain distance apart, and next, the directions of the fragments connecting two successive points are computed. The n + 1 points move on the contour and the n -dimensional histogram of directions is computed. The proposed method is evaluated on large Farsi and English databases. A correct writer identification rate of 92.2% for English handwritings from 900 persons and 97.7% for Farsi handwritings from 600 persons are achieved. Comparison between the proposed method and other studies shows the promising performance and superiority of the proposed method.},
  archive      = {J_IETIP},
  author       = {Alireza Ghanbarian and Golnaz Ghiasi and Reza Safabakhsh and Narges Arastouie},
  doi          = {10.1049/iet-ipr.2018.6391},
  journal      = {IET Image Processing},
  month        = {5},
  number       = {6},
  pages        = {1101-1109},
  shortjournal = {IET Image Process.},
  title        = {Writer identification with n-tuple direction feature from contour},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). DRAN: Deep recurrent adversarial network for automated
pancreassegmentation. <em>IETIP</em>, <em>14</em>(6), 1091–1100. (<a
href="https://doi.org/10.1049/iet-ipr.2019.0399">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Automated pancreas segmentation in abdominal computed tomography (CT) scans is of high clinical relevance (i.e. pancreas cancer diagnosis and prognosis), but extremely difficult because the pancreas is a soft, small, and flexible abdominal organ with high anatomical variability, which causes the previous segmentation methods to result in low precision. In this study, the authors present a new deep recurrent adversarial network (DRAN) to tackle this challenge. DRAN contains three steps: (i) preserving global resolution of CT scans and modifying the receptive field of kernel adaptively through a dilated convolution autoencoder module; (ii) modelling contextual spatial correlation between neighbouring CT scan patches benefits from a specially designed local long short-term memory module; and (iii) improving the performance and generalisation by leveraging an adversarial module, which can constrain the spatial smoothness consistency between continuous CT scans based on the long-range spatial interaction. The system is evaluated on a dataset of 80 manually segmented CT volumes, using four-fold cross-validation. Its performance surpasses other state-of-the-art methods, with the Dice similarity coefficient of and pixel-wise accuracy of . Also, they perform a qualitative evaluation by an expert further revealing the effectiveness and potential of their DRAN as a clinical segmentation tool.},
  archive      = {J_IETIP},
  author       = {Yang Ning and Zhongyi Han and Li Zhong and Caiming Zhang},
  doi          = {10.1049/iet-ipr.2019.0399},
  journal      = {IET Image Processing},
  month        = {5},
  number       = {6},
  pages        = {1091-1100},
  shortjournal = {IET Image Process.},
  title        = {DRAN: Deep recurrent adversarial network for automated pancreassegmentation},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). CPGAN: Conditional patch-based generative adversarial
network for retinal vesselsegmentation. <em>IETIP</em>, <em>14</em>(6),
1081–1090. (<a href="https://doi.org/10.1049/iet-ipr.2019.1007">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Retinal blood vessels, the diagnostic bio-marker of ophthalmologic and diabetic retinopathy, utilise thick and thin vessels for diagnostic and monitoring purposes. The existing deep learning methods attempt to segment the retinal vessels using a unified loss function. However, a difference in spatial features of thick and thin vessels and a biased distribution creates an imbalanced thickness, rendering the unified loss function to be useful only for thick vessels. To address this challenge, a patch-based generative adversarial network-based technique is proposed which iteratively learns both thick and thin vessels in fundoscopic images. It introduces an additional loss function that allows the generator network to learn thin and thick vessels, while the discriminator network assists in segmenting out both vessels as a combined objective function. Compared with state-of-the-art techniques, the proposed model demonstrates the enhanced accuracy, sensitivity, specificity, and area under the receiver operating characteristic curves on STARE, DRIVE, and CHASEDB1 datasets.},
  archive      = {J_IETIP},
  author       = {Sadaqat Ali Rammy and Waseem Abbas and Naqy-Ul Hassan and Asif Raza and Wu Zhang},
  doi          = {10.1049/iet-ipr.2019.1007},
  journal      = {IET Image Processing},
  month        = {5},
  number       = {6},
  pages        = {1081-1090},
  shortjournal = {IET Image Process.},
  title        = {CPGAN: Conditional patch-based generative adversarial network for retinal vesselsegmentation},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Cooperation: A new force for boosting generative adversarial
nets with dual-networkstructure. <em>IETIP</em>, <em>14</em>(6),
1073–1080. (<a href="https://doi.org/10.1049/iet-ipr.2019.0149">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The principle of generative adversarial net is to fit the given data distribution by combining a generative model and discriminative model. There are two major challenges to conventional systems – they are difficult to train and they easily fall into ‘mode collapse’. To improve it, this study describes a novel network structure with dual generators. A ‘cooperation’ mechanism is introduced to help the generators work together. During training, generators not only learn from discriminative feedback but also from each other (like a study group). Compared with a single-generator network, a dual-generator network could capture many more ‘modes’ and eventually reduce the impact of ‘mode collapse.’ Dual networks also require extra computational resources. However, our experiment shows that even with network parameters of similar size, dual networks still achieved better results. Additionally, a dual-generator structure could be extended to multiple generators. The proposed network structure is also very robust and flexible. It can be adapted to various application scenarios, such as high-resolution image generation, domain adaptation and 3D model generation. The experimental results showed that with the same computing resources, multiple generators can generate better quality synthetic data, including 2D images, 3D objects, style transferring etc.},
  archive      = {J_IETIP},
  author       = {Long Zhang and Jieyu Zhao and Xulun Ye and Yu Chen},
  doi          = {10.1049/iet-ipr.2019.0149},
  journal      = {IET Image Processing},
  month        = {5},
  number       = {6},
  pages        = {1073-1080},
  shortjournal = {IET Image Process.},
  title        = {Cooperation: A new force for boosting generative adversarial nets with dual-networkstructure},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). End-to-end learning interpolation for object tracking in low
frame-rate video. <em>IETIP</em>, <em>14</em>(6), 1066–1072. (<a
href="https://doi.org/10.1049/iet-ipr.2019.0944">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In many scenarios, where videos are transmitted through bandwidth-limited channels for subsequent semantic analytics, the choice of frame rates has to balance between bandwidth constraints and analytics performance. Faced with this practical challenge, this study focuses on enhancing object tracking at low frame rates and proposes a learning Interpolation for tracking framework. This framework embeds an implicit video frame interpolation sub-network, which is concatenated and jointly trained with another object tracking sub-network. Once a low frame-rate video is an input, it is first mapped into a high frame-rate latent video, based on which the tracker is learned. Novel strategies and loss functions are derived to ensure the effective end-to-end optimisation of the authors’ network. On several challenging benchmarks and settings, their method achieves a highly competitive tradeoff between frame rate and tracking accuracy. As is known, the implications of interpolation on semantic video analytics and tracking remain unexplored, and the authors expect their method to find many applications in mobile embedded vision, Internet of Things and edge computing.},
  archive      = {J_IETIP},
  author       = {Liqiang Liu and Jianzhong Cao},
  doi          = {10.1049/iet-ipr.2019.0944},
  journal      = {IET Image Processing},
  month        = {5},
  number       = {6},
  pages        = {1066-1072},
  shortjournal = {IET Image Process.},
  title        = {End-to-end learning interpolation for object tracking in low frame-rate video},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Dual attention convolutional network for action recognition.
<em>IETIP</em>, <em>14</em>(6), 1059–1065. (<a
href="https://doi.org/10.1049/iet-ipr.2019.0963">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Action recognition has been an active research area for many years. Extracting discriminative spatial and temporal features of different actions plays a key role in accomplishing this task. Current popular methods of action recognition are mainly based on two-stream Convolutional Networks (ConvNets) or 3D ConvNets. However, the computational cost of two-stream ConvNets is high for the requirement of optical flow while 3D ConvNets takes too much memory because they have a large amount of parameters. To alleviate such problems, the authors propose a Dual Attention ConvNet (DANet) based on dual attention mechanism which consists of spatial attention and temporal attention. The former concentrates on main motion objects in a video frame by using ConvNet structure and the latter captures related information of multiple video frames by adopting self-attention. Their network is entirely based on 2D ConvNet and takes in only RGB frames. Experimental results on UCF-101 and HMDB-51 benchmarks demonstrate that DANet gets comparable results among leading methods, which proves the effectiveness of the dual attention mechanism.},
  archive      = {J_IETIP},
  author       = {Xiaoqiang Li and Miao Xie and Yin Zhang and Guangtai Ding and Weiqin Tong},
  doi          = {10.1049/iet-ipr.2019.0963},
  journal      = {IET Image Processing},
  month        = {5},
  number       = {6},
  pages        = {1059-1065},
  shortjournal = {IET Image Process.},
  title        = {Dual attention convolutional network for action recognition},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Image reflection removal using end-to-end convolutional
neural network. <em>IETIP</em>, <em>14</em>(6), 1047–1058. (<a
href="https://doi.org/10.1049/iet-ipr.2019.0247">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Single image reflection removal is an ill-posed problem. To solve this problem, this study develops a network structure based on a deep encoder–decoder RRnet. Unlike most deep learning strategies applied in this context, the authors find that redundant information increases the difficulty of predicting images on the network; thus, the proposed method uses mixed reflection image cascaded edges as input to the network. The proposed network structure is divided into two parts: the first part is a deep convolutional encoder–decoder network. Its function uses the mixed reflection image and the target edge as input to predict the target layer. The second part is an identical encoder–decoder network structure. Its function uses the mixed reflection image and the reflection edge as input to predict the image reflection layer. In addition, the authors use joint loss to optimise the network model. To train the neural network, they also create an image dataset for reflection removal, which includes a true mixed reflection image and a synthetic mixed reflection image. They use four evaluation indicators to evaluate the proposed method and the other six methods. The experimental results indicate that the proposed method is superior to previous methods.},
  archive      = {J_IETIP},
  author       = {Jinjiang Li and Guihui Li and Hui Fan},
  doi          = {10.1049/iet-ipr.2019.0247},
  journal      = {IET Image Processing},
  month        = {5},
  number       = {6},
  pages        = {1047-1058},
  shortjournal = {IET Image Process.},
  title        = {Image reflection removal using end-to-end convolutional neural network},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Inhomogeneous morphological PDEs for robust and adaptive
image shock filters. <em>IETIP</em>, <em>14</em>(6), 1035–1046. (<a
href="https://doi.org/10.1049/iet-ipr.2019.0086">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Classical morphological filters suffer from well performing in a noisy environment, and intrinsic image structures are not taken into account. The authors propose here an alternative to overcome such weaknesses, by properly using robust shock filters and inhomogeneity. Thus, they obtain multiscale morphological operators by using image edge functions as local weights in inhomogeneous Hamiltonians in classical multiscale dilations/erosions formulated with partial differential equations (PDEs). They provide the equivalent sup–inf-based formulations, and derive sharpening/enhancement methods. In addition, they establish the PDE associated with the asymptotical iterations of the proposed robust and adaptive filters. The good behaviours of the proposed sup–inf and PDE-based methods are illustrated on synthetic, greyscale, and colour images; results are analysed both qualitatively and quantitatively.},
  archive      = {J_IETIP},
  author       = {El Hadji S. Diop and Jesùs Angulo},
  doi          = {10.1049/iet-ipr.2019.0086},
  journal      = {IET Image Processing},
  month        = {5},
  number       = {6},
  pages        = {1035-1046},
  shortjournal = {IET Image Process.},
  title        = {Inhomogeneous morphological PDEs for robust and adaptive image shock filters},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Blind quality assessment for 3D synthesised video with
binocular asymmetric distortion. <em>IETIP</em>, <em>14</em>(6),
1027–1034. (<a href="https://doi.org/10.1049/iet-ipr.2019.1085">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {During the process of watching 3D synthesised video (3D-SV) and switching viewpoints, there is a case of asymmetric distortion, the left(right) viewpoint is a synthesised video generated by rendering technique, and the right(left) viewpoint is a real video taken by the camera. How to accurately estimate the quality of 3D-SV with binocular asymmetric distortions is a new and challenging problem. Aiming at this problem, a blind quality assessment method for 3D-SV with binocular asymmetric distortions is proposed. Firstly, the local edge deformations of synthesised videos at different scales are measured by calculating their standard deviations. Secondly, the global naturalness of synthesised videos is computed by analysing their natural statistical characteristics. Thirdly, a strategy for fusing left and right quality scores is proposed, which considers their texture information in different directions. Finally, the random forest is used to obtain an objective quality score. The experimental results show the superiority of the proposed method on asymmetry 3D-SV database.},
  archive      = {J_IETIP},
  author       = {Shuainan Cui and Zongju Peng and Fen Chen and Wenhui Zou and Gangyi Jiang and Mei Yu},
  doi          = {10.1049/iet-ipr.2019.1085},
  journal      = {IET Image Processing},
  month        = {5},
  number       = {6},
  pages        = {1027-1034},
  shortjournal = {IET Image Process.},
  title        = {Blind quality assessment for 3D synthesised video with binocular asymmetric distortion},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Parallel strength pareto evolutionary algorithm-II based
image encryption. <em>IETIP</em>, <em>14</em>(6), 1015–1026. (<a
href="https://doi.org/10.1049/iet-ipr.2019.0587">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, many image encryption approaches have been proposed on the basis of chaotic maps. The various types of chaotic maps such as one-dimensional and multi-dimensional have been used to generate the secret keys. Chaotic maps require some parameters and value assignment to these parameters is very crucial. Because, poor value assignments may make the chaotic map un-chaotic. Therefore, hyper-parameter tuning of chaotic maps is required. Recently, meta-heuristic based image encryption approaches have been designed by researchers to resolve this issue. However, the majority of the techniques suffer from poor computational speed and stuck in local optima problems. Therefore, in this study, a strength Pareto evolutionary algorithm-II based meta-heuristic approach is proposed to tune the hyper-parameters of the four-dimensional chaotic map. The proposed approach is also implemented in a parallel fashion to enhance the computational speed. The effectiveness of the proposed approach is evaluated through extensive experiments. Comparative analyses show that the proposed approach outperforms the competitive approaches in terms of entropy, NPCR, UACI, and PSNR by , and , respectively.},
  archive      = {J_IETIP},
  author       = {Manjit Kaur and Dilbag Singh and Raminder Singh Uppal},
  doi          = {10.1049/iet-ipr.2019.0587},
  journal      = {IET Image Processing},
  month        = {5},
  number       = {6},
  pages        = {1015-1026},
  shortjournal = {IET Image Process.},
  title        = {Parallel strength pareto evolutionary algorithm-II based image encryption},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020a). Lossless digital image watermarking in sparse domain by
using k-singular value decomposition algorithm. <em>IETIP</em>,
<em>14</em>(6), 1005–1014. (<a
href="https://doi.org/10.1049/iet-ipr.2018.6040">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The crucial hurdle faced by the watermarking technique is to maintain the steadiness corresponding to several attacks while assisting a sufficient level of security. In this study, a robust lossless sparse domain-based watermarking approach combined with discrete cosine transform (DCT) is introduced to hide the secret message in the selected significant sparse elements of the host image. The proposed method takes advantage of a sparse representation-based dictionary learning process. To enhance the security of the original image, the authors first apply the DCT on a secret message. These DCT coefficients with some regularised parameters will be inserted into the selected significant sparse coefficients. At the extraction stage, the secret message is extracted from those significant sparse coefficients by employing the sparse domain orthogonal matching pursuit algorithm. Finally, the inverse DCT is applied to extract the secret message without any information loss. To show the effectiveness of the proposed method, different commonly used attacks are simulated. Simulation results in terms of peak signal-to-noise ratio, structural similarity, normal correlation, and feature similarity indicate that the proposed method can recover the hidden secret message accurately against seven different types of attacks including speckle, Gaussian, salt and pepper, rotate, crop, fold, and blur attack.},
  archive      = {J_IETIP},
  author       = {Farah Deeba and She Kun and Fayaz Ali Dharejo and Yuanchun Zhou},
  doi          = {10.1049/iet-ipr.2018.6040},
  journal      = {IET Image Processing},
  month        = {5},
  number       = {6},
  pages        = {1005-1014},
  shortjournal = {IET Image Process.},
  title        = {Lossless digital image watermarking in sparse domain by using K-singular value decomposition algorithm},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Flow driven attention network for video salient object
detection. <em>IETIP</em>, <em>14</em>(6), 997–1004. (<a
href="https://doi.org/10.1049/iet-ipr.2019.0836">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Salient object detection has been revolutionised by convolutional neural network (CNN) recently. However, it is hard to transfer the state-of-the-art still-image based saliency detectors to videos directly, owing to the neglect of temporal contexts between frames. In this study, the authors propose a flow-driven attention network (FDAN) to exploit motion information for video salient object detection. FDAN consists of an appearance feature extractor, a motion-guided attention module and a saliency map regression module. It extracts the appearance feature per frame, refines appearance feature with optical flow and infers the ultimate saliency map, respectively. Motion-guided attention module is the core of FDAN, which extracts motion information in the form of attention. This attention mechanism is a two-branch CNN, fusing optical flow and appearance features. In addition, a shortcut connection is applied to the attention multiplied feature map for noise suppression intensively. Experimental results show that the proposed method can achieve performance on par with the state-of-the-art method flow-guided recurrent neural encoder on challenging benchmarks of Densely Annotated Video Segmentation and Freiburg–Berkeley Motion Segmentation while being two times faster in detection.},
  archive      = {J_IETIP},
  author       = {Feng Zhou and Hui Shuai and Qingshan Liu and Guodong Guo},
  doi          = {10.1049/iet-ipr.2019.0836},
  journal      = {IET Image Processing},
  month        = {5},
  number       = {6},
  pages        = {997-1004},
  shortjournal = {IET Image Process.},
  title        = {Flow driven attention network for video salient object detection},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020a). Corrigendum: Automatic detection of acute lymphoblastic
leukaemia based on extending the multifractal features. <em>IETIP</em>,
<em>14</em>(5), 995. (<a
href="https://doi.org/10.1049/iet-ipr.2020.0350">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_IETIP},
  doi          = {10.1049/iet-ipr.2020.0350},
  journal      = {IET Image Processing},
  month        = {4},
  number       = {5},
  pages        = {995},
  shortjournal = {IET Image Process.},
  title        = {Corrigendum: Automatic detection of acute lymphoblastic leukaemia based on extending the multifractal features},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Multi-directional local adjacency descriptors (MDLAD) for
heterogeneous face recognition. <em>IETIP</em>, <em>14</em>(5), 982–994.
(<a href="https://doi.org/10.1049/iet-ipr.2019.0199">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents new image descriptors for heterogeneous face recognition (HFR). The proposed descriptors combine directional and neighborhood information using a rotating spoke and concentric rings concept. We name the descriptors as multi-directional local adjacency descriptors (MDLAD). This family of descriptor captures the directional information through successive rotations of a pair of orthogonal spokes. Likewise, they capture the adjacency information through a comparison against the central pixel of a window with concentric rings around the central pixel. The MDLAD is found to describe the face images well for recognition purposes, which when matched using the chi-squared distance. The face recognition performance with MDLAD improves with its use as a layer in a deep neural network, which yields a robust classification for heterogeneous face recognition with respect to the state-of-the-art methods. The MDLADNET deep network is easily trainable with few hyperparameters and limited data samples as compared to existing similar deep networks. We have experimented on different heterogeneous modalities viz. Extended Yale B, CASIA, CUFSF, IIITD, LFW, Multi-PIE, and CARL, and have found proficient results.},
  archive      = {J_IETIP},
  author       = {Shubhobrata Bhattacharya and Anirban Dasgupta and Aurobinda Routray},
  doi          = {10.1049/iet-ipr.2019.0199},
  journal      = {IET Image Processing},
  month        = {4},
  number       = {5},
  pages        = {982-994},
  shortjournal = {IET Image Process.},
  title        = {Multi-directional local adjacency descriptors (MDLAD) for heterogeneous face recognition},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Image encryption with cross colour field algorithm and
improved cascade chaos systems. <em>IETIP</em>, <em>14</em>(5), 973–981.
(<a href="https://doi.org/10.1049/iet-ipr.2019.0310">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this study, the critical factor of the chaos system has been analysed to improve the randomicity of the encryption keyspace. Several chaos systems have been integrated together with a linear function to form a much more efficient key sequence generator. This study also presents the cross colour field confusion method which scrambles the pixels among the R G B colour matrixes. In this way, the range of the pixels scrambling map is extended to three matrixes compared to the traditional pixels scrambling schemes which do pixels scrambling in its own colour matrix. The experiment results show that the improved cascade system has better bifurcation diagrams. The cross colour field diffusion algorithm makes the encrypted image has little pixel correlation and no information leakage. The experiment results justify that the novel cross colour confusion scheme with improved chaos systems has a good ability to resist brute force attack, known plain attack, chosen plain text attack, chosen ciphertext attack and differential attack. The security analysis demonstrates that the proposed approach has satisfactory properties in image encryption.},
  archive      = {J_IETIP},
  author       = {Meng Jia},
  doi          = {10.1049/iet-ipr.2019.0310},
  journal      = {IET Image Processing},
  month        = {4},
  number       = {5},
  pages        = {973-981},
  shortjournal = {IET Image Process.},
  title        = {Image encryption with cross colour field algorithm and improved cascade chaos systems},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Segmentation-based recognition system for handwritten bangla
and devanagari words using conventional classification and transfer
learning. <em>IETIP</em>, <em>14</em>(5), 959–972. (<a
href="https://doi.org/10.1049/iet-ipr.2019.0208">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Offline recognition of handwritten text in Indian regional scripts is a major area of research as nearly 910 million people use such scripts in India. Most of the reported research works on Indian script-based optical character recognition (OCR) system have focused on a single script only. Research for developing methodologies that are capable of handling more than one Indian script is yet to be focused. As such, this has motivated us to study and experiment on creating a recognition system that can handle two most popular Indian scripts, namely Bangla and Devanagari. The authors propose a system that first detects and corrects skew present in Bangla and Devanagari handwritten words, estimates the headline, and further segments the words into meaningful pseudo-characters. This is followed by extraction of three different statistical features and combination of these features with off-the-shelf classifiers to study and identify the exemplary combination. Moreover, they employ state-of-the-art convolutional neural network-based transfer learning architectures and delineate a comparison with the extracted hand-crafted features. Finally, they amalgamate the identified pseudo-characters to provide the final result. On experimentation, the proposed segmentation methodology is discerned to provide good accuracy when compared with existing methods.},
  archive      = {J_IETIP},
  author       = {Rahul Pramanik and Soumen Bag},
  doi          = {10.1049/iet-ipr.2019.0208},
  journal      = {IET Image Processing},
  month        = {4},
  number       = {5},
  pages        = {959-972},
  shortjournal = {IET Image Process.},
  title        = {Segmentation-based recognition system for handwritten bangla and devanagari words using conventional classification and transfer learning},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). CNN based localisation of forged region in object-based
forgery for HD videos. <em>IETIP</em>, <em>14</em>(5), 947–958. (<a
href="https://doi.org/10.1049/iet-ipr.2019.0397">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The location of the smallest object in a scene plays an essential role in the perception of a viewer. Any tampering with it, may evolve in adverse consequences especially with surveillance videos of banks, ATMs, traffic monitoring etc. Therefore, a scientific approach is required to thoroughly observe the fine details of tampering (forgery) in a video. A spatio-temporal detection method is proposed using convolutional neural network (CNN) to detect as well as localise the forged region in a forged video frame. The proposed method is employed in two stages. The first stage is detecting forged frames using proposed temporal CNN, while the second stage is localising the forged region in a novel way using proposed spatial CNN. The vital element of a video, i.e. motion residual is used to train the proposed network. Thus, making the network comprehensive in detecting the object-based forgery in HD videos. The performance of the proposed method is evaluated on SYSU-OBJFORG dataset (object-based video forgery dataset) and a derived test dataset of variable length and frame size videos. The results are compared with state-of-the-art methods to prove the efficacy of the proposed method.},
  archive      = {J_IETIP},
  author       = {Aditi Kohli and Abhinav Gupta and Divya Singhal},
  doi          = {10.1049/iet-ipr.2019.0397},
  journal      = {IET Image Processing},
  month        = {4},
  number       = {5},
  pages        = {947-958},
  shortjournal = {IET Image Process.},
  title        = {CNN based localisation of forged region in object-based forgery for HD videos},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Two-stage traffic sign detection and recognition based on
SVM and convolutional neural networks. <em>IETIP</em>, <em>14</em>(5),
939–946. (<a href="https://doi.org/10.1049/iet-ipr.2019.0634">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nowadays, traffic sign recognition is the most important task of advanced driver assistance systems since it improves the safety and comfort of drivers. However, it remains a challenging task due to the complexity of road traffic scenes. In this study, a novel two-stage approach for real-time traffic sign detection and recognition in a real traffic situation was proposed. The first stage aims to detect and classify the detected traffic signs into circular and triangular shape using HOG features and linear support vector machines (SVMs). The main objective of the second stage is to recognise the traffic signs using a convolutional neural network into their subclasses. The performance of the whole process is tested on German traffic sign detection benchmark (GTSDB) and German traffic sign recognition benchmark (GTSRB) datasets. Experimental results show that the obtained detection and recognition rate is comparable with those reported in the literature with much less complexity. Furthermore, the average processing time demonstrates its suitability for real-time processing applications.},
  archive      = {J_IETIP},
  author       = {Ahmed Hechri and Abdellatif Mtibaa},
  doi          = {10.1049/iet-ipr.2019.0634},
  journal      = {IET Image Processing},
  month        = {4},
  number       = {5},
  pages        = {939-946},
  shortjournal = {IET Image Process.},
  title        = {Two-stage traffic sign detection and recognition based on SVM and convolutional neural networks},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Structure tensor-based SIFT algorithm for SAR image
registration. <em>IETIP</em>, <em>14</em>(5), 929–938. (<a
href="https://doi.org/10.1049/iet-ipr.2019.0568">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The scale-invariant feature transform (SIFT) algorithm is the most widely used feature extraction as well as a feature matching method in remote sensing image registration. However, the performance of this algorithm is affected by the influence of speckle noise in synthetic aperture radar (SAR) images. It reduces the number of correct matches as well as the correct matching rate in SAR image registration. Moreover, SAR image registration is considered to be a challenging task as the images generally have significant geometric as well as intensity variations. To address these problems, a structure tensor-based SIFT algorithm is proposed to register the SAR images. At first, the tensor diffusion technique is used to construct the scale layers. Then, the features are extracted in the scale layers. Finally, feature matching is performed between the input SAR images and correct matches are identified. The proposed method can increase the number of correct matches as well as position accuracy in registration. Experiments have been conducted on five SAR image pairs to verify the effectiveness of the method.},
  archive      = {J_IETIP},
  author       = {Divya S and Sourabh Paul and Umesh Chandra Pati},
  doi          = {10.1049/iet-ipr.2019.0568},
  journal      = {IET Image Processing},
  month        = {4},
  number       = {5},
  pages        = {929-938},
  shortjournal = {IET Image Process.},
  title        = {Structure tensor-based SIFT algorithm for SAR image registration},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Active contour image segmentation model with de-hazing
constraints. <em>IETIP</em>, <em>14</em>(5), 921–928. (<a
href="https://doi.org/10.1049/iet-ipr.2018.5987">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Images captured in hazy or foggy weather conditions can be seriously degraded by scattering of atmospheric particles, which makes the objects and their features difficult to be identified by computer vision systems. In the past decades, image de-hazing is used to remove the influence of weather factors and improve image visualisation in hazy scenes by providing easy image post-processing towards human assistance systems benefit. In this study, the authors present a variational segmentation model equipped with de-hazing constraint terms in a new coupled dehazing-segmentation model. The proposed hybrid formulation not only recovers/restores the fog/haze degradation but at the same time segments image degraded object/objects by solving in this way the difficulties of simultaneously performed dehazing and segmentation pre/post-processing. This combination takes into account the image structure boundaries and the image quality, leading in this way to a robust dehazing segmentation scheme. The advantages of the proposed method are the suitability of the model for grey and vector-valued images, a small number of parameters involved, and a rather good speed of the algorithm. Experiments show that their approach outperforms the state-of-the-art algorithms in terms of segmentation accuracy while avoiding a dehazing preprocessing which reflects an extended CPU time.},
  archive      = {J_IETIP},
  author       = {Haider Ali and Awal Sher and Maryam Saeed and Lavdie Rada},
  doi          = {10.1049/iet-ipr.2018.5987},
  journal      = {IET Image Processing},
  month        = {4},
  number       = {5},
  pages        = {921-928},
  shortjournal = {IET Image Process.},
  title        = {Active contour image segmentation model with de-hazing constraints},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Depth data and fusion of feature descriptors for static
gesture recognition. <em>IETIP</em>, <em>14</em>(5), 909–920. (<a
href="https://doi.org/10.1049/iet-ipr.2019.0230">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this study, the authors propose a novel methodology for static gesture recognition in a complex background using only depth map from Microsoft&#39;s Kinect camera. Four different types of features are extracted and analysed on two public static gesture datasets. The features extracted from the segmented hand are geometrical, local binary patterns, number of fingers ( Num ) raised in a gesture and distance of hand palm centre from the fingertips and the valley between the fingers. The hand region is first segmented from the image using depth data followed by the forearm removal. Four multi-class support vector machine (SVM) kernels are also compared and used for recognition of gestures with extracted feature vector as an input. The experimental results achieved recognition accuracy of 99 and on two public complex static gesture datasets using Gaussian SVM kernel function as a classifier. The proposed approach is found to be comparable and even outperforms some of the state-of-the-art techniques in terms of high recognition accuracies, even after using a single cue for hand segmentation and extraction of features in the complex background which results in non-dependency on too many cues and much hardware.},
  archive      = {J_IETIP},
  author       = {Prachi Sharma and Radhey Shyam Anand},
  doi          = {10.1049/iet-ipr.2019.0230},
  journal      = {IET Image Processing},
  month        = {4},
  number       = {5},
  pages        = {909-920},
  shortjournal = {IET Image Process.},
  title        = {Depth data and fusion of feature descriptors for static gesture recognition},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Robust image hashing with visual attention model and
invariant moments. <em>IETIP</em>, <em>14</em>(5), 901–908. (<a
href="https://doi.org/10.1049/iet-ipr.2019.1157">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image hashing is an efficient technique of multimedia processing for many applications, such as image copy detection, image authentication, and social event detection. In this study, the authors propose a novel image hashing with visual attention model and invariant moments. An important contribution is the weighted DWT (discrete wavelet transform) representation by incorporating a visual attention model called Itti saliency model into LL sub-band. Since the Itti saliency model can efficiently extract saliency map reflecting regions of attention focus, perceptual robustness of the proposed hashing is achieved. In addition, as invariant moments are robust and discriminative features, hash construction with invariant moments extracted from the weighted DWT representation ensures good classification performance between robustness and discrimination. Extensive experiments with open image datasets are done to validate the performances of the proposed hashing. The results demonstrate that the proposed hashing is robust and discriminative. Performance comparisons with some hashing algorithms are also conducted, and the receiver operating characteristic results illustrate that the proposed hashing outperforms the compared hashing algorithms in classification performance between robustness and discrimination.},
  archive      = {J_IETIP},
  author       = {Zhenjun Tang and Hanyun Zhang and Chi-Man Pun and Mengzhu Yu and Chunqiang Yu and Xianquan Zhang},
  doi          = {10.1049/iet-ipr.2019.1157},
  journal      = {IET Image Processing},
  month        = {4},
  number       = {5},
  pages        = {901-908},
  shortjournal = {IET Image Process.},
  title        = {Robust image hashing with visual attention model and invariant moments},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Kernel-based bayesian clustering of computed tomography
images for lung nodule segmentation. <em>IETIP</em>, <em>14</em>(5),
890–900. (<a href="https://doi.org/10.1049/iet-ipr.2018.5748">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Lung nodule segmentation is an interesting research topic, and it serves as an effective solution for the diagnosis of Lung cancer. The existing methods of lung nodule segmentation suffer from accuracy issues due to the heterogeneity of the nodules in the lungs and the presence of visual deviations in the nodules. Thus, there is a requirement for an effective lung nodule segmentation, which assists the physicians in making accurate decisions. Accordingly, this study proposes a lung nodule segmentation process based on the kernel-based Bayesian fuzzy clustering (BFC), which is the integration of kernel functions in the BFC. Initially, the input computed tomography image is pre-processed for ensuring the effective segmentation, and the lobes are identified using the adaptive thresholding strategy. Then, the dominant areas in the lobes are identified using a scale-invariant feature transform descriptor, and the significant nodules are extracted using the grid-based segmentation. Finally, the lung nodules are segmented using the proposed kernel-based BFC. The proposed algorithm is evaluated using the Lung Image Database Consortium and Image Database Resource Initiative database, and it acquires the accuracy, sensitivity, and false positive rate of 0.955, 0.999, and 0.025, respectively.},
  archive      = {J_IETIP},
  author       = {Yadhu Rajan Baby and Vinod Kumar Ramayyan Sumathy},
  doi          = {10.1049/iet-ipr.2018.5748},
  journal      = {IET Image Processing},
  month        = {4},
  number       = {5},
  pages        = {890-900},
  shortjournal = {IET Image Process.},
  title        = {Kernel-based bayesian clustering of computed tomography images for lung nodule segmentation},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). CapsNet topology to classify tumours from brain images and
comparative evaluation. <em>IETIP</em>, <em>14</em>(5), 882–889. (<a
href="https://doi.org/10.1049/iet-ipr.2019.0312">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Visual evaluation of many magnetic resonance images is a difficult task. Therefore,computer-assisted brain tumor classification techniques have been proposed.These techniques have several drawbacks or limitations. Capsule based neuralnetworks are new approaches that can preserve spatial relationships of learnedfeatures using dynamic routing algorithm. By this way, not only performance oftumor recognition increases but also sampling efficiency and generalisationcapability improves. Therefore, in this work, a Capsule Network (CapsNet) isused to achieve fully automated classification of tumors from brain magneticresonance images. In this work, prevalent three types of tumors (pituitary,glioma and meningioma) have been handled. The main contributions in this paperare as follows: 1) A comprehensive review on CapsNet based methods is presented.2) A new CapsNet topology is designed by using a Sobolev gradient-basedoptimisation, expectation-maximisation based dynamic routing and tumor boundaryinformation. 3) The network topology is applied to categorise three types ofbrain tumors. 4) Comparative evaluations of the results obtained by othermethods are performed. According to the experimental results, the proposedCapsNet based technique can achieve extraction of desired features from imagedata sets and provides tumor classification automatically with 92.65%accuracy.},
  archive      = {J_IETIP},
  author       = {Evgin Goceri},
  doi          = {10.1049/iet-ipr.2019.0312},
  journal      = {IET Image Processing},
  month        = {4},
  number       = {5},
  pages        = {882-889},
  shortjournal = {IET Image Process.},
  title        = {CapsNet topology to classify tumours from brain images and comparative evaluation},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Privacy preserving search index for image databases based on
SURF and order preserving encryption. <em>IETIP</em>, <em>14</em>(5),
874–881. (<a href="https://doi.org/10.1049/iet-ipr.2019.0575">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Managing large personal image databases requires efficient privacy preserving indexing methods to allow for their outsourcing to possibly curious cloud servers. To construct a secure inverted index in this paper, first, visual words are extracted from stored images based on the Speeded-Up and Robust Features (SURF). Next, Order Preserving Encryption (OPE) is used to encipher the frequencies of occurrence of the extracted visual words. Another scale and rotation invariant feature, which is the local HSV histogram, is included for comparison. From the obtained results, it is apparent that SURF achieves more precise results. Aggregation of both features is considered to further improve the accuracy. The effects of the weighting scheme of the visual words and their number on the performance are investigated. Weighted term frequency inverse document frequency (tf-idf) together with the Jaccard similarity measure yield the best performance. OPE encryption is shown to have minor impact on the retrieval accuracy. To reduce encryption time, a lookup table is constructed. The inverted index reduces the search time significantly compared to a sequential search scheme as apparent from the results. A comparative study with recent related schemes demonstrates the competitiveness of the implemented system in terms of computational efficiency and accuracy.},
  archive      = {J_IETIP},
  author       = {Safaa Magdy and Yasmine Abouelseoud and Mervat Mikhail},
  doi          = {10.1049/iet-ipr.2019.0575},
  journal      = {IET Image Processing},
  month        = {4},
  number       = {5},
  pages        = {874-881},
  shortjournal = {IET Image Process.},
  title        = {Privacy preserving search index for image databases based on SURF and order preserving encryption},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Hybrid higher-order total variation model for multiplicative
noise removal. <em>IETIP</em>, <em>14</em>(5), 862–873. (<a
href="https://doi.org/10.1049/iet-ipr.2018.5930">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As an important, challenging, and difficult problem in image processing, multiplicative noise removal (MNR) has attracted great attention. To this end, many variational methods have been effectively proposed in the past few decades. Among these variational methods, total variation (TV) and its higher-order extensions are very effective, where the former can preserve sharp edges but cause some undesirable staircase effects and the latter can better reduce the staircase effects but sometimes smooth the image details. To overcome the drawbacks while taking full use of their merits, the authors propose a novel hybrid higher-order TV regularisation model for MNR, in which the novelty of the proposed model consists of combining the image prior information of first-order and second-order derivatives to propose a novel higher-order regulariser, named as hybrid higher-order TV (HHTV). More specifically, a more preferable equivalent formulation of HHTV is derived. Then, they use the derived equivalent formulation to design an efficient alternating iterative algorithm to solve the proposed model. Finally, the experimental results demonstrate that the proposed HHTV method outperforms several state-of-the-art methods in terms of image quality and convergence speed.},
  archive      = {J_IETIP},
  author       = {Pengfei Liu},
  doi          = {10.1049/iet-ipr.2018.5930},
  journal      = {IET Image Processing},
  month        = {4},
  number       = {5},
  pages        = {862-873},
  shortjournal = {IET Image Process.},
  title        = {Hybrid higher-order total variation model for multiplicative noise removal},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). KSR-BOF: A new and exemplified method (as KSRs method) for
image classification. <em>IETIP</em>, <em>14</em>(5), 853–861. (<a
href="https://doi.org/10.1049/iet-ipr.2019.0613">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image classification is very important in pattern recognition and computer vision, where, for integrating final representation, feature pooling methods of the max-pooling, sum-pooling and average-pooling have been widely used. In this study, the authors propose a new method called K -strongest responses (KSRs) on the dictionary atoms for integrating the coding coefficients to generate the final representation that is compared with the previous pooling methods, produces better performance for the image classification task. On the basis of the KSR method, to improve classification accuracy and generate more compact and discriminative final representation, a new framework consisting of two-part KSR and bag-of-features is proposed. To evaluate the performance of the proposed method and framework, they apply it to locality-constrained linear coding, linear distance coding and sparse coding by using two datasets from benchmarks of scene classification: 19-class satellite scene and UC Merced Land. The results show that the coding coefficients integrated by their method and framework are more discriminative than other methods.},
  archive      = {J_IETIP},
  author       = {Mohammad Hassan Maleki and Ghosheh Abed Hodtani and Seyed Hesam Odin Hashemi},
  doi          = {10.1049/iet-ipr.2019.0613},
  journal      = {IET Image Processing},
  month        = {4},
  number       = {5},
  pages        = {853-861},
  shortjournal = {IET Image Process.},
  title        = {KSR-BOF: A new and exemplified method (as KSRs method) for image classification},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Blind video quality assessment via spatiotemporal
statistical analysis of adaptive cube size 3D-DCT coefficients.
<em>IETIP</em>, <em>14</em>(5), 845–852. (<a
href="https://doi.org/10.1049/iet-ipr.2019.0275">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {There is an urgent need for a robust video quality assessment (VQA) model that can efficiently evaluate the quality of a video content varying in terms of the distortion and content type in the absence of the reference video. Considering this need, a novel no reference (NR) model relying on the spatiotemporal statistics of the distorted video in a three-dimensional (3D)-discrete cosine transform (DCT) domain is proposed in this study. While developing the model, as the first contribution, the video contents are adaptively segmented into the cubes of different sizes and spatiotemporal contents in line with the human visual system (HVS) properties. Then, the 3D-DCT is applied to these cubes. Following that, as the second contribution, different efficient features (i.e. spectral behaviour, energy variation, distances between spatiotemporal frequency bands, and DC variation) associated with the contents of these cubes are extracted. After that, these features are associated with the subjective experimental results obtained from the EPFL-PoliMi video database using the linear regression analysis for building the model. The evaluation results present that the proposed model, unlike many top-performing NR-VQA models (e.g. V-BLIINDS, VIIDEO, and SSEQ), achieves high and stable performance across the videos with different contents and distortions.},
  archive      = {J_IETIP},
  author       = {Enes Cemiloglu and Gokce Nur Yilmaz},
  doi          = {10.1049/iet-ipr.2019.0275},
  journal      = {IET Image Processing},
  month        = {4},
  number       = {5},
  pages        = {845-852},
  shortjournal = {IET Image Process.},
  title        = {Blind video quality assessment via spatiotemporal statistical analysis of adaptive cube size 3D-DCT coefficients},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Image patch prior learning based on random neighbourhood
resampling for image denoising. <em>IETIP</em>, <em>14</em>(5), 838–844.
(<a href="https://doi.org/10.1049/iet-ipr.2018.5403">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image patch priors become a popular tool for image denoising. The Gaussian mixture model (GMM) is remarkably effective in modelling natural image patches. However, GMM prior learning using the expectation maximisation (EM) algorithm is sensitive to the initialisation, often leading to low convergence rate of parameter estimation. In this study, a novel sampling method called random neighbourhood resampling (RNR) is proposed to improve the accuracy and efficiency of parameter estimation. An enhanced GMM (EGMM) learning algorithm is further developed by incorporating RNR into the EM algorithm to initialise and update the GMM prior. The learned EGMM prior is applied in the expected patch log-likelihood (EPLL) framework for image denoising. The effectiveness and performance of the proposed RNR and EGMM algorithm are demonstrated via extensive experimental results comparing with the state-of-the-art image denoising methods, the experimental results show the higher PSNR result of the denoised images using the proposed method. Meanwhile, the authors verified that the proposed method can efficiently reduce the time of image denoising compared with the basic EPLL method.},
  archive      = {J_IETIP},
  author       = {Jian Ji and Jiajie Wei and Guoliang Fan and Mengqi Bai and Jingjing Huang and Qiguang Miao},
  doi          = {10.1049/iet-ipr.2018.5403},
  journal      = {IET Image Processing},
  month        = {4},
  number       = {5},
  pages        = {838-844},
  shortjournal = {IET Image Process.},
  title        = {Image patch prior learning based on random neighbourhood resampling for image denoising},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Stereo matching for infrared images using guided filtering
weighted by exponential moving average. <em>IETIP</em>, <em>14</em>(5),
830–837. (<a href="https://doi.org/10.1049/iet-ipr.2019.0144">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Infrared imaging is less susceptible to illumination conditions and haze than visible light imaging. The advantage makes infrared sensing suitable for providing remote visibility with reduced distortion. However, infrared images tend to have low resolution and lack rich textures that facilitate stereo matching. To enhance the applicability of infrared stereo imaging, the authors re-examine the guided-image techniques to include advanced edge-aware filters for aggregation and propose a novel guided-image filtering scheme here. Based on the exponential moving average, the weights are recursively calculated such that all pixels on the infrared image pair can contribute to a discrepancy cost. The arrangement allows additional pixels to be involved in the cost aggregation to reduce the demand for rich texture. Experimental results using the colour and thermal stereo (CATS) benchmark testbed demonstrate that the proposed approach outperforms several state-of-art approaches in generating accurate disparity maps.},
  archive      = {J_IETIP},
  author       = {Chengtao Zhu and Yau-Zen Chang},
  doi          = {10.1049/iet-ipr.2019.0144},
  journal      = {IET Image Processing},
  month        = {4},
  number       = {5},
  pages        = {830-837},
  shortjournal = {IET Image Process.},
  title        = {Stereo matching for infrared images using guided filtering weighted by exponential moving average},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Improved strategy for human action recognition; experiencing
a cascaded design. <em>IETIP</em>, <em>14</em>(5), 818–829. (<a
href="https://doi.org/10.1049/iet-ipr.2018.5769">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Human motion analysis has received a lot of attention in the computer vision community during the last few years. This research domain is supported by a wide spectrum of applications including video surveillance, patient monitoring systems, and pedestrian detection, to name a few. In this study, an improved cascaded design for human motion analysis is presented; it consolidates four phases: (i) acquisition and preprocessing, (ii) frame segmentation, (iii) features extraction and dimensionality reduction, and (iv) classification. The implemented architecture takes advantage of CIE-Lab and National Television System Committee colour spaces, and also performs contrast stretching using the proposed red–green–blue* colour space enhancement technique. A parallel design utilising attention-based motion estimation and segmentation module is also proposed in order to avoid the detection of false moving regions. In addition to these contributions, the proposed feature selection technique called entropy controlled principal components with weights minimisation, further improves the classification accuracy. The authors claims are supported with a comparison between six state-of-the-art classifiers tested on five standard benchmark data sets including Weizmann, KTH, UIUC, Muhavi, and WVU, where the results reveal an improved correct classification rate of 96.55, 99.50, 99.40, 100, and 100%, respectively.},
  archive      = {J_IETIP},
  author       = {Muhammad Attique Khan and Tallha Akram and Muhammad Sharif and Nazeer Muhammad and Muhammad Younus Javed and Syed Rameez Naqvi},
  doi          = {10.1049/iet-ipr.2018.5769},
  journal      = {IET Image Processing},
  month        = {4},
  number       = {5},
  pages        = {818-829},
  shortjournal = {IET Image Process.},
  title        = {Improved strategy for human action recognition; experiencing a cascaded design},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020a). Enhanced image no-reference quality assessment based on
colour space distribution. <em>IETIP</em>, <em>14</em>(5), 807–817. (<a
href="https://doi.org/10.1049/iet-ipr.2019.0856">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this study, the authors investigate the problem of enhanced image no-reference (NR) quality assessment. For resolving the problem of the enhanced images, it is difficult to obtain reference images, this study proposes an NR image quality assessment (IQA) model based on colour space distribution. Given an enhanced image, our method first uses a gist to select a clear target image in which the scene, colour and quality are similar to the hypothetical reference images. And then, the colour transfer is used between the input images and target images to construct the reference image. Next, the appropriate IQA method is used to assess enhanced image quality. The absolute colour difference and feature similarity (FSIM) are used to measure the colour and grey-scale image quality, respectively. Extensive experiments demonstrate that the proposed method is good at evaluating enhanced image quality for X-ray, dust, underwater and low-light images. The experimental results are consistent with human subjective evaluation and achieve good assessment effects.},
  archive      = {J_IETIP},
  author       = {Hao Liu and Ce Li and Dong Zhang and Yannan Zhou and Shaoyi Du},
  doi          = {10.1049/iet-ipr.2019.0856},
  journal      = {IET Image Processing},
  month        = {4},
  number       = {5},
  pages        = {807-817},
  shortjournal = {IET Image Process.},
  title        = {Enhanced image no-reference quality assessment based on colour space distribution},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Image contrast enhancement with brightness preservation
using an optimal gamma and logarithmic approach. <em>IETIP</em>,
<em>14</em>(4), 794–805. (<a
href="https://doi.org/10.1049/iet-ipr.2019.0921">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this study, a new enhancement framework is proposed for low contrast and dark images where traditional histogram equalisation (HE), gamma and logarithmic transformation are incorporated to achieve a visually pleasing image. Before the operation of HE on the input image, gamma and logarithmic transformation are performed in order to preserve the fine details of the image. A new gamma value of the proposed algorithm helps to restrain histogram spikes to avoid over-enhancement and noise artefacts effect. After that, a novel logarithmic transformation is used to map a narrow range of low-intensity values in the input image to a wider range of output levels. Thus, the dark input values are spread out into the higher intensity values, which improve the overall contrast and brightness of the image. The proposed method is compared with various state-of-the-art techniques. The large dataset has been used to check the feasibility of the technique. The subjective and objective analysis shows that the proposed algorithm outperforms most of the existing contrast-enhancement algorithms and the results are natural-looking, good contrast images with almost no artefacts.},
  archive      = {J_IETIP},
  author       = {Neha Singh and Ashish Kumar Bhandari},
  doi          = {10.1049/iet-ipr.2019.0921},
  journal      = {IET Image Processing},
  month        = {3},
  number       = {4},
  pages        = {794-805},
  shortjournal = {IET Image Process.},
  title        = {Image contrast enhancement with brightness preservation using an optimal gamma and logarithmic approach},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Application of optimal FrFT order for improving the azimuth
resolution of range doppler imaging algorithm. <em>IETIP</em>,
<em>14</em>(4), 789–793. (<a
href="https://doi.org/10.1049/iet-ipr.2019.1065">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The traditional range Doppler (RD) imaging algorithm has become the most intuitive and classical method in synthetic aperture radar (SAR) imaging processing because of its easy implementation and high processing efficiency. However, owing to the poor quality of the image produced by RD algorithm, this traditional imaging method is growing increasingly unable to meet practical needs. In an effort to solve the problem of poor azimuth imaging quality, this study proposes a high-resolution RD azimuth imaging algorithm. The expression of the optimal order of the SAR azimuth signals using fractional Fourier transform (FrFT) is derived in detail herein. A theoretical analysis shows that this optimal order is unique and depends on the azimuth imaging parameters. The experimental results based on airborne SAR simulation data and spaceborne SAR measurement data indicate that the azimuth resolution of the proposed algorithm is improved by 31.6 or 46.8% as against that of the traditional RD algorithm, whereas the peak side lobe ratio (PSLR) and integrated side lobe ratio (ISLR) values obtained via the proposed algorithm are approximately equivalent to those obtained via the traditional RD algorithm.},
  archive      = {J_IETIP},
  author       = {Zhenli Wang and Qun Wang},
  doi          = {10.1049/iet-ipr.2019.1065},
  journal      = {IET Image Processing},
  month        = {3},
  number       = {4},
  pages        = {789-793},
  shortjournal = {IET Image Process.},
  title        = {Application of optimal FrFT order for improving the azimuth resolution of range doppler imaging algorithm},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Learning a minimum similarity projection and lowest
correlation representation for image classification. <em>IETIP</em>,
<em>14</em>(4), 782–788. (<a
href="https://doi.org/10.1049/iet-ipr.2019.0683">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The projection and representation learning is an attractive tool for image classification problem due to its effectiveness and efficiency of extracting interior structure for data. However, the complexity and diversity of real data lead to the decline of classification performance. A novel image classification method is proposed by learning a minimum similarity projection and lowest correlation representation. This method attempts to produce a discriminative representation on a low-dimensional space for the data, which takes two steps: feature projection and feature representation. By learning a projection matrix, the feature projection aims to map the samples into a low-dimensional space which jointly minimises the similar within-class difference and maximises the dissimilar cross-class difference. A discriminative representation for the data on the new space is generated by using the de-correlated effect to the representation results of all classes. Therefore, the learned projection and representation simultaneously demonstrate discriminative properties in the learning of both steps. The extensive experiments conducted on different visual classification tasks consist of face recognition, object categorisation, and scene classification that the proposed method performs superior performance for image classification.},
  archive      = {J_IETIP},
  author       = {Shan Lu and Jun Zhang and Ying Gao},
  doi          = {10.1049/iet-ipr.2019.0683},
  journal      = {IET Image Processing},
  month        = {3},
  number       = {4},
  pages        = {782-788},
  shortjournal = {IET Image Process.},
  title        = {Learning a minimum similarity projection and lowest correlation representation for image classification},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). No-reference quality assessment for live broadcasting videos
in temporal and spatial domains. <em>IETIP</em>, <em>14</em>(4),
774–781. (<a href="https://doi.org/10.1049/iet-ipr.2019.1195">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nowadays, live broadcasting video has become increasingly popular and high-quality live broadcasting video is highly needed. In practice, live broadcasting videos usually undergo several processing stages, which inevitably introduce multiple distortions, e. g. frame freezing and intensity mutation, causing the degraded quality of experience. However, little work has been done to the quality evaluation of live broadcasting videos, which may hinder the further development of more advanced live broadcasting video delivery systems. Motivated by this, this study presents a no-reference quality evaluation model for live broadcasting videos (LBVQA) in temporal and spatial domains. In the temporal domain, statistic features are extracted to measure the frame freezing and intensity mutation, and the entropy-based feature is extracted to describe the global jitter. In the spatial domain, blurring is measured based on phase coherence, and abnormal exposure ratio is calculated based on an adaptive threshold. Finally, all features are fed into a backpropagation neural network to train the quality prediction model. Experimental results on the Live Broadcasting Video Database demonstrate the advantages of the proposed metric over the state-of-the-art image and video quality metrics.},
  archive      = {J_IETIP},
  author       = {Yipo Huang and Leida Li and Yu Zhou and Bo Hu},
  doi          = {10.1049/iet-ipr.2019.1195},
  journal      = {IET Image Processing},
  month        = {3},
  number       = {4},
  pages        = {774-781},
  shortjournal = {IET Image Process.},
  title        = {No-reference quality assessment for live broadcasting videos in temporal and spatial domains},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Variable step-size matching pursuit based on oblique
projection for compressed sensing. <em>IETIP</em>, <em>14</em>(4),
766–773. (<a href="https://doi.org/10.1049/iet-ipr.2019.0916">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The development of compressive sensing has focused on sparse signal reconstruction in recent years. Most existing greedy algorithms achieve satisfactory reconstruction performance only when the sparsity of the target signal has been known as prior information. Moreover, some greedy algorithms always involve either high-computational expenses or low-reconstruction accuracy caused by the process of adaptive adjustment of signal sparsity. To address these concerns, a novel variable step-size matching pursuit based on oblique projection (VSMPOP) for compressed sensing is proposed. The proposed VSMPOP algorithm estimates the initial sparsity based on the restricted isometry property criterion. The algorithm creates a support set of the target signal after a preliminary test and oblique projection test between the sensing matrix and the residual. VSMPOP realises a similar approach to the sparsity level with a variable step size. The experimental results demonstrated that the proposed VSMPOP algorithm provides superior performance in terms of computational complexity and reconstruction efficiency compared with most of the available matching pursuit algorithms.},
  archive      = {J_IETIP},
  author       = {Na Li and Xinghui Yin and Huanyin Guo and Sulan Zong and Wei Fu},
  doi          = {10.1049/iet-ipr.2019.0916},
  journal      = {IET Image Processing},
  month        = {3},
  number       = {4},
  pages        = {766-773},
  shortjournal = {IET Image Process.},
  title        = {Variable step-size matching pursuit based on oblique projection for compressed sensing},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Semantic combined network for zero-shot scene parsing.
<em>IETIP</em>, <em>14</em>(4), 757–765. (<a
href="https://doi.org/10.1049/iet-ipr.2019.0870">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, image-based scene parsing has attracted increasing attention due to its wide application. However, conventional models can only be valid on images with the same domain of the training set and are typically trained using discrete and meaningless labels. Inspired by the traditional zero-shot learning methods which employ auxiliary side information to bridge the source and target domains, the authors propose a novel framework called semantic combined network (SCN), which aims at learning a scene parsing model only from the images of the seen classes while targeting on the unseen ones. In addition, with the assistance of semantic embeddings of classes, the proposed SCN can further improve the performances of traditional fully supervised scene parsing methods. Extensive experiments are conducted on the data set Cityscapes, and the results show that the proposed SCN can perform well on both zero-shot scene parsing (ZSSP) and generalised ZSSP settings based on several state-of-the-art scenes parsing architectures. Furthermore, the authors test the proposed model under the traditional fully supervised setting and the results show that the proposed SCN can also significantly improve the performances of the original network models.},
  archive      = {J_IETIP},
  author       = {Yinduo Wang and Haofeng Zhang and Shidong Wang and Yang Long and Longzhi Yang},
  doi          = {10.1049/iet-ipr.2019.0870},
  journal      = {IET Image Processing},
  month        = {3},
  number       = {4},
  pages        = {757-765},
  shortjournal = {IET Image Process.},
  title        = {Semantic combined network for zero-shot scene parsing},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Normalised gamma transformation-based contrast-limited
adaptive histogram equalisation with colour correction for sand–dust
image enhancement. <em>IETIP</em>, <em>14</em>(4), 747–756. (<a
href="https://doi.org/10.1049/iet-ipr.2019.0992">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Images captured in the sand–dust weather often suffer from serious colour cast and poor contrast, and this has serious implications for outdoor computer vision systems. To address these problems, a normalised gamma transformation-based contrast-limited adaptive histogram equalisation (CLAHE) with colour correction in Lab colour space for sand–dust image enhancement is proposed in this study. This method consists of image contrast enhancement and image colour correction. To avoid producing new colour deviation, the input sand–dust images are first transformed from red, green, and blue colour space into Lab colour space. Then, the contrast of the lightness component (L channel) of the sand–dust image is enhanced using CLAHE. To avoid unbalanced contrast, as well as to reduce the overincreased brightness caused by CLAHE, a normalised gamma correction function is introduced to CLAHE. After that, the a and b chromatic components are recovered by a grey-world-based colour correction method. Experiments on real sand–dust images demonstrate that the proposed method can obtain the highest percentage of new visible edges for all testing images. The contrast restoration exhibits good colour fidelity and proper brightness.},
  archive      = {J_IETIP},
  author       = {Zhenghao Shi and Yaning Feng and Minghua Zhao and Erhu Zhang and Lifeng He},
  doi          = {10.1049/iet-ipr.2019.0992},
  journal      = {IET Image Processing},
  month        = {3},
  number       = {4},
  pages        = {747-756},
  shortjournal = {IET Image Process.},
  title        = {Normalised gamma transformation-based contrast-limited adaptive histogram equalisation with colour correction for sand–dust image enhancement},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Segmentation-based MAP despeckling of medical ultrasound
images in shearlet domain based on normal inverse gaussian distribution.
<em>IETIP</em>, <em>14</em>(4), 736–746. (<a
href="https://doi.org/10.1049/iet-ipr.2018.6347">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Speckle noise is an undesirable phenomenon that exhibits granular pattern which reduces the diagnostic capability of clinical ultrasound (US) images. In this study, a new speckle denoising method based on modelling of detail band shearlet coefficients of log-transformed US images is presented. In each detail shearlet subband, coefficients corresponding to signal and speckle noise are modelled as normal inverse Gaussian and Gaussian priors, respectively. These coefficients, based on image local statistics are segmented into different regions of heterogeneity viz. homogeneous, heterogeneous and strongly heterogeneous regions, respectively, so as to control over smoothing of denoised images. Then, using the prior distributions maximum a posteriori (MAP) estimation is performed over all regions of detail bands except those regions that represent strongly heterogeneous coefficients. For better performance, an adaptive weight function is also used in the MAP expression which reduces the loss of feature information. Experimentation is done on noise-free synthetic kidney and foetus US images and a set of 60 real US images. The results are presented for objective and subjective quality assessment of the proposed method and five other methods for speckle denoising. The potential of the proposed method in comparison to other methods can easily be ascertained from the obtained denoising results.},
  archive      = {J_IETIP},
  author       = {Amit Garg and Vineet Khandelwal},
  doi          = {10.1049/iet-ipr.2018.6347},
  journal      = {IET Image Processing},
  month        = {3},
  number       = {4},
  pages        = {736-746},
  shortjournal = {IET Image Process.},
  title        = {Segmentation-based MAP despeckling of medical ultrasound images in shearlet domain based on normal inverse gaussian distribution},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Novel visual tracking approach via ant lion optimiser.
<em>IETIP</em>, <em>14</em>(4), 727–735. (<a
href="https://doi.org/10.1049/iet-ipr.2018.5702">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ant lion optimiser (ALO) is a new nature-inspired swarm intelligence optimisation algorithm that mimics the hunting mechanism of antlions in nature. ALO has been proved to have the merits of high exploitation and convergence speed benefiting from adaptive boundary shrinking mechanism and elitism. In this work, visual tracking is expressed as searching for object in whole search space by interaction between antlions and ants. A novel ALO-based visual tracking framework is proposed and the adaptation and sensitivity of the parameters in ALO are discussed to improve tracking performance. In addition, considering that ALO tracker needs a lot of iteration consumption, kernel correlation filter with deep feature is integrated into the ALO tracking framework (ALOKCF) to improve track efficiency. Extensive experimental results prove that the ALO tracker is very competitive compared to other trackers, especially for abrupt motion tracking. At the same time, two visual tracking benchmarks are used to verify ALOKCF tracker achieves state-of-the-art performance.},
  archive      = {J_IETIP},
  author       = {Huanlong Zhang and Zeng Gao and Jie Zhang and Xiankai Lu and Jian Chen and Guohao Nie and Xiaoliang Qian},
  doi          = {10.1049/iet-ipr.2018.5702},
  journal      = {IET Image Processing},
  month        = {3},
  number       = {4},
  pages        = {727-735},
  shortjournal = {IET Image Process.},
  title        = {Novel visual tracking approach via ant lion optimiser},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Wavelet-based deep learning for skin lesion classification.
<em>IETIP</em>, <em>14</em>(4), 720–726. (<a
href="https://doi.org/10.1049/iet-ipr.2019.0553">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Skin lesions can be in malignant or benign forms. Benign skin lesion types are not deadly; however, malignant types of skin lesions can be fatal. Lethal forms are known as skin cancer. These types require urgent clinical treatment. Fast detection and diagnosis of malignant types of skin lesions might prevent life-threatening scenarios. This work presents two methods for the automatic classification of malignant melanoma and seborrhoeic keratosis lesions. The first method builds on modelling skin images together with wavelet coefficients. Approximate, horizontal, and vertical wavelet coefficients are obtained using the wavelet transform, and then deep learning (DL) models are generated for each of the representations and skin images. The second method builds on modelling skin images together with three approximate coefficients. This method utilises a sequential wavelet transformation to produce approximation coefficients. Then DL models are generated for each of the representations and skin images. Transfer learning-based ResNet-18 and ResNet-50 DL models provide model images and wavelet coefficients. Then skin lesion detection is achieved by fusing model output probabilities. Both proposed models outperform the methods only based on image data and other previously proposed methods.},
  archive      = {J_IETIP},
  author       = {Sertan Serte and Hasan Demirel},
  doi          = {10.1049/iet-ipr.2019.0553},
  journal      = {IET Image Processing},
  month        = {3},
  number       = {4},
  pages        = {720-726},
  shortjournal = {IET Image Process.},
  title        = {Wavelet-based deep learning for skin lesion classification},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Semi-supervised convolutional generative adversarial network
for hyperspectral image classification. <em>IETIP</em>, <em>14</em>(4),
709–719. (<a href="https://doi.org/10.1049/iet-ipr.2019.0869">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To solve the problem of insufficient annotated samples in hyperspectral image classification, the semi-supervised convolutional generative adversarial network classification model is proposed in this study. The generative adversarial framework constructs an adversarial game, where the generator captures data distribution and generates fake samples, while the discriminator determines whether the input comes from generated or training data. In the proposed method, a deep three-dimensional (3D) convolutional neural network is used to generate the so-called fake cube samples and another 3D deep residual network is designed to discriminate the inputs. Furthermore, the generated samples, labelled and unlabelled samples are put into the discriminator for joint training, and the trained discriminator can determine the authenticity of the sample and the class label. This semi-supervised generative adversarial training strategy can effectively improve the generalisation capability of the deep residual network where the labelled samples are limited. Three widely used hyperspectral images are utilised to evaluate the classification performance of the proposed method: Indian Pines, Pavia University, and Salinas-A. The classification results reveal that the proposed model can improve the classification performance and achieve competitive results compared with the state-of-art methods, especially when there are few training samples.},
  archive      = {J_IETIP},
  author       = {Zhixiang Xue},
  doi          = {10.1049/iet-ipr.2019.0869},
  journal      = {IET Image Processing},
  month        = {3},
  number       = {4},
  pages        = {709-719},
  shortjournal = {IET Image Process.},
  title        = {Semi-supervised convolutional generative adversarial network for hyperspectral image classification},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Wavelet transform modulus maxima-based robust logo
watermarking. <em>IETIP</em>, <em>14</em>(4), 697–708. (<a
href="https://doi.org/10.1049/iet-ipr.2018.5868">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Digital image watermarking is used to protect the copyright of digital images. In this study, a novel blind logo image watermarking technique for RGB images is proposed. The proposed technique exploits the error correction capabilities of the human visual system. It embeds two different watermarks in the wavelet/multiwavelet domains. The two watermarks are embedded in different sub-bands, are orthogonal, and serve different purposes. One is a high capacity multi-bit watermark used to embed the logo, and the other is a 1-bit watermark which is used for the detection and reversal of geometrical attacks. The two watermarks are both embedded using a spread spectrum approach, based on a pseudo-random noise sequence and a unique secret key. Robustness against geometric attacks such as rotation, scaling, and translation (RST) is achieved by embedding the 1-bit watermark in the wavelet transform modulus maximacoefficients of the wavelet transform. The experimental results show that the proposed watermarking technique has better distortion parameter detection capabilities and compares favourably against existing techniques in terms of robustness against geometrical attacks such as RST.},
  archive      = {J_IETIP},
  author       = {Mohammad Barr and Cristian Serdean},
  doi          = {10.1049/iet-ipr.2018.5868},
  journal      = {IET Image Processing},
  month        = {3},
  number       = {4},
  pages        = {697-708},
  shortjournal = {IET Image Process.},
  title        = {Wavelet transform modulus maxima-based robust logo watermarking},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Efficient inception v2 based deep convolutional neural
network for real-time hand action recognition. <em>IETIP</em>,
<em>14</em>(4), 688–696. (<a
href="https://doi.org/10.1049/iet-ipr.2019.0985">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The most effective and accurate deep convolutional neural network (faster region-based convolutional neural network (Faster R-CNN) Inception V2 model, single shot detector (SSD) Inception V2 model) based architectures for real-time hand gesture recognition is proposed. The proposed models are tested on standard data sets (NUS hand posture data set-II, Senz-3D) and custom-developed (MITI hand data set (MITI-HD)) data set. The performance metrics are analysed for intersection over union (IoU) ranges between 0.5 and 0.95. IoU value of 0.5 resulted in higher precision compared to other IoU values considered (0.5:0.95, 0.75). It is observed that the Faster R-CNN Inception V2 model resulted in higher precision (0.990 for AP all , IoU = 0.5) compared to SSD Inception V2 model (0.984 for all ) for MITI-HD 160. The computation time of Faster R-CNN Inception V2 is higher compared to SSD Inception V2 model and also resulted in less number of mispredictions. Increasing the size of samples (MITI-HD 300) resulted in improvement of AP all = 0.991. Improvement in large (APlarge) and medium (APmedium) size detections are not significant when compared to small (APsmall) detections. It is concluded that the Faster R-CNN Inception V2 model is highly suitable for real-time hand gesture recognition system under unconstrained environments.},
  archive      = {J_IETIP},
  author       = {S. Rubin Bose and V. Sathiesh Kumar},
  doi          = {10.1049/iet-ipr.2019.0985},
  journal      = {IET Image Processing},
  month        = {3},
  number       = {4},
  pages        = {688-696},
  shortjournal = {IET Image Process.},
  title        = {Efficient inception v2 based deep convolutional neural network for real-time hand action recognition},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). New design of adaptive gabor wavelet filter bank for medical
image retrieval. <em>IETIP</em>, <em>14</em>(4), 679–687. (<a
href="https://doi.org/10.1049/iet-ipr.2019.1024">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Gabor wavelet is widely used in the analysis of texture feature. This study presents a novel feature descriptor based on the design of adaptive Gabor wavelet filter-bank for medical image retrieval. The design of proposed Gabor wavelet provides flexibility to extract the dominant directional features from medical images. First, peaks in the spectrum of medical image are analysed to determine the dominant directions present in the image. With these dominant directions, a bank of Gabor-filters is designed to extract the directional features effectively. Next, feature vector is derived by computing the energy and standard deviation from the Gabor filtered coefficients at a particular scale and orientation. The use of maximum likelihood estimation (MLE) is suggested to measure the similarity between the feature vectors of heterogeneous medical images. The performance of the proposed method is evaluated on three different publicly available databases namely NEMA, OASIS and EXACT09. The performance in terms of average retrieval precision (ARP), average retrieval rate (ARR) and computational time are compared with well-known existing methods. It is observed from experimental results that the proposed approach achieved ARP of 85.32, 85.24 and 77% and ARR of 31.33, 14.05 and 23.78% for NEMA, OASIS and EXACT09 databases respectively.},
  archive      = {J_IETIP},
  author       = {Aswini K. Samantaray and Amol D. Rahulkar},
  doi          = {10.1049/iet-ipr.2019.1024},
  journal      = {IET Image Processing},
  month        = {3},
  number       = {4},
  pages        = {679-687},
  shortjournal = {IET Image Process.},
  title        = {New design of adaptive gabor wavelet filter bank for medical image retrieval},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Low light image enhancement with adaptive sigmoid transfer
function. <em>IETIP</em>, <em>14</em>(4), 668–678. (<a
href="https://doi.org/10.1049/iet-ipr.2019.0781">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Low light image enhancement algorithms intent to produce visually pleasant images and target to extract valuable information for computer vision applications. The task of improving the quality of low light images is a challenging one. The existing methods for quality improvement undeniably annoy the visual aesthetics and suffer the major drawback of high computational complexity and less efficiency. To improve the visual quality and lower the distortions, a simple and computationally efficient low light image enhancement framework is presented in this study. To achieve this, an adaptive sigmoid transfer function (ASTF) is used and is derived from the sigmoid activation function of neural networks. By combining ASTF with a Laplacian operator, colour and contrast-enhanced images are obtained. Experiments show the effectiveness of the proposed method with state-of-the-art methods.},
  archive      = {J_IETIP},
  author       = {Kankanala Srinivas and Ashish Kumar Bhandari},
  doi          = {10.1049/iet-ipr.2019.0781},
  journal      = {IET Image Processing},
  month        = {3},
  number       = {4},
  pages        = {668-678},
  shortjournal = {IET Image Process.},
  title        = {Low light image enhancement with adaptive sigmoid transfer function},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Medical fusion framework using discrete fractional wavelets
and non-subsampled directional filter banks. <em>IETIP</em>,
<em>14</em>(4), 658–667. (<a
href="https://doi.org/10.1049/iet-ipr.2019.0948">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image fusion in neuro diagnosis is intimidating due to its complexity. The heterogeneous natures of the original brain images make intermodal transmission difficult during fusion. Medical image fusion using complementary modalities results in loss of vital salient information. Poor fusion, colour deficiencies result due to similar processing for both the modalities. A dual technique is proposed using discrete fractional wavelet transform (FRWT) and non-subsampled directional filter banks for better extraction of salient image elements for improved diagnosis. The sparsity character of the coefficients FRWT is controlled by optimising the parity operator using Grey Wolf optimisation algorithm. Four sets of neurological multimodal magnetic resonance imaging and single photon emission computed tomography (CT) brain images are used from benchmark database for validation. The objective evaluation has been conducted using five metrics. The main values obtained from objective metrics based on the proposed technique are 6.3213 for Shannon entropy, mutual information is computed to be 2.7582, fusion factor is 1.9095, standard deviation is 0.1310, and edge strength is 0.76122 indicating improved diagnostic information and superior image quality. Subjective evaluation by a medico validates the findings with finer visual output and enhanced contrast in comparison with recent and state-of-the-art methods .},
  archive      = {J_IETIP},
  author       = {Gurpreet Kaur and Sukhwinder Singh and Renu Vig},
  doi          = {10.1049/iet-ipr.2019.0948},
  journal      = {IET Image Processing},
  month        = {3},
  number       = {4},
  pages        = {658-667},
  shortjournal = {IET Image Process.},
  title        = {Medical fusion framework using discrete fractional wavelets and non-subsampled directional filter banks},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Image quality assessment via spatial-transformed domains
multi-feature fusion. <em>IETIP</em>, <em>14</em>(4), 648–657. (<a
href="https://doi.org/10.1049/iet-ipr.2018.6417">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The basis of image processing is to evaluate and monitor image quality using algorithms rather than subjective methods. Conventional gradient operators have been popularly used in previous image quality assessment tasks to reflect the edge contour of an image, while there are some obvious defects in terms of the selection of mask scale and direction. Some improved versions are also less than ideal since they fail to consider the gradient information of the same pixel in different directions at the same time. The authors adopt a powerful gradient operator that can simultaneously capture edge information in all four directions at the same pixel point with more relevant values being considered instead of selecting the maximum in these four directions. Furthermore, four complementary types of features extracted from the spatial and transform domains are considered. A set of 12-dimensional feature vectors is generated for each image by multi-feature fusion. Ultimately, random forest regression technique is employed to train their model and then map the distortion effects to the prediction scores. The experimental results show that the proposed FVC-G has better overall performance, more powerful cross-database operation capability, and higher visual consistency than other advanced methods.},
  archive      = {J_IETIP},
  author       = {Miaomiao Yu and Yuanlin Zheng and Kaiyang Liao and Zhisen Tang},
  doi          = {10.1049/iet-ipr.2018.6417},
  journal      = {IET Image Processing},
  month        = {3},
  number       = {4},
  pages        = {648-657},
  shortjournal = {IET Image Process.},
  title        = {Image quality assessment via spatial-transformed domains multi-feature fusion},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Effective key-frame extraction approach using TSTBTC–BBA.
<em>IETIP</em>, <em>14</em>(4), 638–647. (<a
href="https://doi.org/10.1049/iet-ipr.2018.6361">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Video summarisation and key-frame extraction play a vital role in video processing and content-based video analysis. Key-frame extraction facilitates rapid browsing and proficient video ordering in numerous applications such as multimedia, gaming etc. The application of ‘effective key-frame extraction’ can be used in the field of object detection and tracking, and it is found to be helpful in providing some sort of signal to visually impaired persons. In this study, the advantages of Thepade&#39;s sorted ternary block truncation coding (TSTBTC) along with binary bat algorithm (BBA) are used to overcome the limitations by extracting an effective key frame from the videos. Static images in the form of frames are extracted from input video database and processed through TSTBTC algorithm for calculating similarity measures amongst two consecutive frames. The input data is processed using four colour spaces, namely red–green–blue colour space, Kekre&#39;s LUV colour space, YCbCr colour space, YUV colour space and BBA are used to optimise the threshold value. Furthermore, five similarity measures indices are used to calculate the number of frames, and the result obtained shows that TSTBTC–BBA algorithm deployed in YUV colour space provides better results compared with other existing techniques regarding precision, F-measure and colour spaces.},
  archive      = {J_IETIP},
  author       = {Yadwinder Singh and Lakhwinder Kaur},
  doi          = {10.1049/iet-ipr.2018.6361},
  journal      = {IET Image Processing},
  month        = {3},
  number       = {4},
  pages        = {638-647},
  shortjournal = {IET Image Process.},
  title        = {Effective key-frame extraction approach using TSTBTC–BBA},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Reference data preparation for complex satellite image
segmentation. <em>IETIP</em>, <em>14</em>(4), 628–637. (<a
href="https://doi.org/10.1049/iet-ipr.2019.0234">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The high spatial resolution satellite images covering multiple objects of the urban area are visually complex in nature. This visual complexity causes ambiguity during segmentation of such images when the targets are unknown. In such case, reference data are required to assess the segmentation methods. Due to excellence of humans in visual analysis, the presented work has attempted for a psycho-visual approach to prepare the reference segmented complex HRS images. The reference data have has prepared through the correlation of the quantified eye-tracking data (metrics) and corresponding concurrent think-aloud (CTA) data for each of the created segments. Segments get updated based on Gestalt principles and Gibson&#39;s theory. Those segments having the best correlation between metrics and CTA data have been opted as the final output. The results suggest that the functional grouping of objects while preferring perceptual grouping for segments drawing conforms the most to the participants&#39; verbal response. The final results have been compared with the existing notion of full segmentation used for complex images. The comparison also proffers the superiority of the proposed segmentation over full segmentation to be used as reference. In the future, a large number of images may be used to prepare better reference data.},
  archive      = {J_IETIP},
  author       = {Ashu Sharma and Jayanta Kumar Ghosh and Saptarshi Kolay},
  doi          = {10.1049/iet-ipr.2019.0234},
  journal      = {IET Image Processing},
  month        = {3},
  number       = {4},
  pages        = {628-637},
  shortjournal = {IET Image Process.},
  title        = {Reference data preparation for complex satellite image segmentation},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Deep convolution network for dense crowd counting.
<em>IETIP</em>, <em>14</em>(4), 621–627. (<a
href="https://doi.org/10.1049/iet-ipr.2019.0435">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Estimating the total number of people in a crowded situation is a challenging task due to numerous occlusions and perspective changes existing in crowd images. To address this issue, the authors have proposed a new deep learning framework for accurate and efficient crowd counting here. Inspired by multi-column convolutional neural network (MCNN) and contextual pyramid convolutional neural network (CP-CNN), the authors use a combination of a two branches, convolutional neutral network (CNN) and transposed convolutional layers, to generate a high-quality density map. The two-branch CNN for feature extraction generates a density map that is only a quarter of size of the original image Then a set of transposed convolutional layers and convolutional layers are combined with the network to make up for the detail loss of the density map conducted by stacked pooling. Compared with MCNN and CP-CNN, the authors’ approach employs fewer branches and simpler architecture. Experimental result shows that their approach achieves MAE 80.7 and MSE 131.2 in ShanghaiTech PartA dataset, MAE 15.6 and MSE 26.8 in ShanghaiTech PartB dataset, and MAE Average 7.1 in WorldExpo&#39;10 dataset.},
  archive      = {J_IETIP},
  author       = {Wei Zhang and Yongjie Wang and Yanyan Liu and Jianghua Zhu},
  doi          = {10.1049/iet-ipr.2019.0435},
  journal      = {IET Image Processing},
  month        = {3},
  number       = {4},
  pages        = {621-627},
  shortjournal = {IET Image Process.},
  title        = {Deep convolution network for dense crowd counting},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Detection and localisation of multiple brain tumours by
object counting and elimination. <em>IETIP</em>, <em>14</em>(4),
615–620. (<a href="https://doi.org/10.1049/iet-ipr.2019.0071">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Brain cancer is a major health problem that affects the lives of many people. Clinicians depend on their medical knowledge to analyse laboratory test results and clinical information extracted manually from medical images, to identify the essential characteristics of tumours such as size, shape, and location. The accurate diagnosis of brain tumours is very essential for deciding the most appropriate treatment protocols and procedures. Apart from being tedious, slow and time-consuming manual segmentation of brain image scans is also prone to human error. In order to enhance the accuracy and reliability of the diagnosis of brain tumours, rigorous image processing and segmentation is required. In this study the authors present an automated image segmentation method for detecting and localising multiple brain tumours of different sizes and intensities by using a technique of object counting coupled with an evaluation process. The object counting technique were carried out using multiple binary images with different threshold values, and the evaluation process was based on assessment criteria of specific tumour parameters. Different magnetic resonance images with multiple tumours were used for testing the proposed method. The results indicated that the proposed method is effective in detecting and localising multiple true tumours in the brain.},
  archive      = {J_IETIP},
  author       = {Mohamed Nasor and Walid Obaid},
  doi          = {10.1049/iet-ipr.2019.0071},
  journal      = {IET Image Processing},
  month        = {3},
  number       = {4},
  pages        = {615-620},
  shortjournal = {IET Image Process.},
  title        = {Detection and localisation of multiple brain tumours by object counting and elimination},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Context-based ensemble classification for the detection of
architectural distortion in a digitised mammogram. <em>IETIP</em>,
<em>14</em>(4), 603–614. (<a
href="https://doi.org/10.1049/iet-ipr.2019.0639">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The problem of computer-aided detection of architectural distortion (AD) in a digitised mammogram has been attempted in this manuscript. In examining a mammogram, the decision regarding a particular region of interest (RoI) is dependent on the appearance of the surrounding regions. However, in existing methods to detect AD the inference about an RoI is dependent on the appearance of this RoI alone. In addition, multiple radiologists infer the same mammogram in coming to a final decision about the mammogram. Contrary to popular ensemble classifiers like Adaboost and Random Forest, the authors propose an ensemble based method (imitating multiple radiologists by classifiers) for detecting AD such that the decision on a test RoI is dependent on the decisions of the surrounding RoIs in the proposed ensemble classifier. The proposed context-based ensemble classifier has been validated on two mammographic databases. The proposal shows promising results in both the databases.},
  archive      = {J_IETIP},
  author       = {Yusuf Akhtar and Dipti Prasad Mukherjee},
  doi          = {10.1049/iet-ipr.2019.0639},
  journal      = {IET Image Processing},
  month        = {3},
  number       = {4},
  pages        = {603-614},
  shortjournal = {IET Image Process.},
  title        = {Context-based ensemble classification for the detection of architectural distortion in a digitised mammogram},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Robust retinal optic disc and optic cup segmentation via
stationary wavelet transform and maximum vessel pixel sum.
<em>IETIP</em>, <em>14</em>(4), 592–602. (<a
href="https://doi.org/10.1049/iet-ipr.2019.0845">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Glaucoma leads to irreversible blindness and its diagnosis relies heavily on cup to disc ratio. This ratio can be calculated by segmenting the optic disc (OD) and optic cup (OC) from the fundus image. However, the segmentation of OD and OC is a complex process and should be carried out with utmost accuracy to screen the risk of glaucoma. In order to circumvent this complexity, this study presents two novel algorithms to segment the OD and OC boundaries, respectively by creating an automated region of interest (ROI). The first algorithm uses the inverse polar transform to segment OD where the horizontal coefficients of sixth level decomposition Daubechies stationary wavelet transform of ROI is processed. The second algorithm uses maximum vessel pixel sum to extract the complete OC region by extending the partial cup edges to the nasal side of the cup boundary. This approach covers the region under central retinal blood vessels also which were missing in earlier research. The proposed algorithms achieved an accuracy rate up to 99.70% for OD and 99.47% for OC segmentation, respectively even under severe retinal pathological conditions.},
  archive      = {J_IETIP},
  author       = {Birendra Biswal and Eadara Vyshnavi and Metta Venkata Satya Sairam and Pravat Kumar Rout},
  doi          = {10.1049/iet-ipr.2019.0845},
  journal      = {IET Image Processing},
  month        = {3},
  number       = {4},
  pages        = {592-602},
  shortjournal = {IET Image Process.},
  title        = {Robust retinal optic disc and optic cup segmentation via stationary wavelet transform and maximum vessel pixel sum},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Varied channels region proposal and classification network
for wildlife image classification under complex environment.
<em>IETIP</em>, <em>14</em>(4), 585–591. (<a
href="https://doi.org/10.1049/iet-ipr.2019.1042">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A varied channels region proposal and classification network (VCRPCN) is developed based on a deep convolutional neural network (DCNN) and the characteristics of the animals appearing for automatic wildlife animal classification in camera trapped images, the architecture of the network is improved by feeding different channels into different components of the network to accomplish different aims, i.e. the animal images and their background images are employed in the region proposal component to extract region candidates for the animal&#39;s location, and the animal images combined with the region candidates are fed into the classification component to identify their categories. This novel architecture considers changes to the image due to the animals&#39; appearances, and identifies potential animal regions in images and extracts their local features to describe and classify them. Five hundred low contrast animal images have been collected. All images have low contrast due to being acquired during the night. Cross-validation is employed to statistically measure the performance of the proposed algorithm. The experimental results demonstrate that in comparison with the well-known object detection network, faster R-CNN, the proposed VCRPCN achieved higher accuracy with the same dataset and training configuration with an average accuracy improvement of 21%.},
  archive      = {J_IETIP},
  author       = {Yanhui Guo and Thomas A. Rothfus and Amira S. Ashour and Lei Si and Chunlai Du and Tih-Fen Ting},
  doi          = {10.1049/iet-ipr.2019.1042},
  journal      = {IET Image Processing},
  month        = {3},
  number       = {4},
  pages        = {585-591},
  shortjournal = {IET Image Process.},
  title        = {Varied channels region proposal and classification network for wildlife image classification under complex environment},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Image segmentation algorithm based on neutrosophic fuzzy
clustering with non-local information. <em>IETIP</em>, <em>14</em>(3),
576–584. (<a href="https://doi.org/10.1049/iet-ipr.2018.5949">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To improve the boundary processing ability and anti-noise performance of image segmentation algorithm，a neutrosophic fuzzy clustering algorithm based on non-local information is proposed here. Initially, the proposed approach uses the data distribution of deterministic subset to determine the clustering centre of the fuzzy subset. Besides, the fuzzy non-local pixel correlation is introduced into the neutrosophic fuzzy mean clustering algorithm. The experimental results on synthetic images, medical images and natural images show that the proposed method is more robust and more accurate than the existing clustering segmentation methods.},
  archive      = {J_IETIP},
  author       = {Jinyu Wen and Shibin Xuan and Yuqi Li and Qihui Peng and Qing Gao},
  doi          = {10.1049/iet-ipr.2018.5949},
  journal      = {IET Image Processing},
  month        = {2},
  number       = {3},
  pages        = {576-584},
  shortjournal = {IET Image Process.},
  title        = {Image segmentation algorithm based on neutrosophic fuzzy clustering with non-local information},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Fast search real-time face recognition based on DCT
coefficients distribution. <em>IETIP</em>, <em>14</em>(3), 570–575. (<a
href="https://doi.org/10.1049/iet-ipr.2018.6175">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The authors propose an adaptive face recognition algorithm based on the discrete cosine transform (DCT) coefficients approach. For the database&#39;s establishment, the face images are pre-processed with colour transform, hair cutting, and background removing to eliminate non-face information. The recognised kernel applied the weights of DCT coefficient distribution with the entire image transformation, to avoid position mismatch and reduce the light effect. The key coefficients of DCT are chosen from the training database by maximum variance. The fast search mode can reject 90% weak candidates with few coefficients to fasten the processing speed. The significant coefficients weighting methods are used to enhance face features. Only using 50 coefficients per picture, the recognition rate can achieve 95% for ORL face database testing. For real-time recognition, camera imaging is processed with algorithms using C-programming based on Windows system. The recognition rate can achieve 95% and the speed is about nine frames per second for real-time recognition in practice.},
  archive      = {J_IETIP},
  author       = {Shih-Chang Hsia and Szu-Hong Wang and Chia-Jung Chen},
  doi          = {10.1049/iet-ipr.2018.6175},
  journal      = {IET Image Processing},
  month        = {2},
  number       = {3},
  pages        = {570-575},
  shortjournal = {IET Image Process.},
  title        = {Fast search real-time face recognition based on DCT coefficients distribution},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Design of fuzzy inference system for apple ripeness
estimation using gradient method. <em>IETIP</em>, <em>14</em>(3),
561–569. (<a href="https://doi.org/10.1049/iet-ipr.2018.6524">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this study, a fuzzy classification approach based on colour features has been investigated to estimate the ripeness of apple fruits according to three maturity stages; unripe, turning-ripe and ripe. The K nearest neighbour algorithm was applied in order to segment the fruit image into four regions namely background, green area, yellow area and red area. The last three regions represent the colour features and were subsequently given as inputs to the fuzzy classifier. Gradient method has been used for tuning the fuzzy classifier in order to obtain the best performance. Image database used for simulation has been collected and exploited for the training and testing phases using cross-validation. Simulation results indicate that the best classifier parameters can be obtained. The efficiency of the proposed system compared with the non-use of the gradient method has been proved by the confusion matrix and the most known classification evaluation metrics. Moreover, the trained fuzzy classifier demonstrates its outperformance in terms of accuracy and execution time compared with other existing methods.},
  archive      = {J_IETIP},
  author       = {Raja Hamza and Mohamed Chtourou},
  doi          = {10.1049/iet-ipr.2018.6524},
  journal      = {IET Image Processing},
  month        = {2},
  number       = {3},
  pages        = {561-569},
  shortjournal = {IET Image Process.},
  title        = {Design of fuzzy inference system for apple ripeness estimation using gradient method},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Image denoising using 2D orthogonal locality preserving
discriminant projection. <em>IETIP</em>, <em>14</em>(3), 552–560. (<a
href="https://doi.org/10.1049/iet-ipr.2019.0436">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image denoising using a combination of non-local self-similarity and transformed domain techniques has become popular in past few years. Instead of working on independent pixels, patches extracted from the noisy image are grouped together based on structural similarity and noise elimination is performed in transformed domain. Orthogonal locality preserving projection and its variant that processes the images directly in matrix format have been used for image denoising recently. Locality preserving nature of these techniques takes care of similarity within image patches while learning the basis, hence reducing the task of grouping patches explicitly. Non-local self-similarity based image denoising approaches perform patch grouping based on structural similarity. Discriminant information, if considered can play pivotal role in achieving superior clustering of data and thereby is expected to enhance the quality of denoising. With this aim in mind, two-dimensional (2D) orthogonal locality preserving discriminant projection is formulated in this study. While learning the basis, along with the similarity, proposed approach also takes into account dissimilarity between patches. A global basis thus learnt from the noisy image is used for denoising and comparable denoising performance is shown relative to the state-of-the-art methods.},
  archive      = {J_IETIP},
  author       = {Gitam Shikkenawis and Suman K. Mitra},
  doi          = {10.1049/iet-ipr.2019.0436},
  journal      = {IET Image Processing},
  month        = {2},
  number       = {3},
  pages        = {552-560},
  shortjournal = {IET Image Process.},
  title        = {Image denoising using 2D orthogonal locality preserving discriminant projection},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Quality assessment framework for video contextualisation of
personal videos. <em>IETIP</em>, <em>14</em>(3), 545–551. (<a
href="https://doi.org/10.1049/iet-ipr.2018.6022">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The world is witnessing rapid transformations in hardware technology. This will keep on improving day by day. The processing power of every handheld device is significantly improved as well as the storage capacity is increased. With all these advancements, the personal video capture and usage of videos have tremendously increased across many applications. The quality assessment of personal videos has become very important. It is a vital task to design a model for assessing the video quality. A novel methodology for detecting damaged video frames is proposed here. The primary objective of the research is to detect uni-coloured frames and frames with ice effect. The novel histograms bin comparison technique is proposed for inter- and intraframe analysis. The shakiness of the video is calculated using motion estimation. The video quality is also assessed using blur detection as well as contrast calculation to spot useful portion in the video. The proposed framework generates the video quality metadata and supports the contextualisation process.},
  archive      = {J_IETIP},
  author       = {Pratap Sanap and Shaila D. Apte},
  doi          = {10.1049/iet-ipr.2018.6022},
  journal      = {IET Image Processing},
  month        = {2},
  number       = {3},
  pages        = {545-551},
  shortjournal = {IET Image Process.},
  title        = {Quality assessment framework for video contextualisation of personal videos},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Automatic thresholding using a modified valley emphasis.
<em>IETIP</em>, <em>14</em>(3), 536–544. (<a
href="https://doi.org/10.1049/iet-ipr.2019.0176">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Otsu&#39;s method is one of the most well-known methods for automatic thresholding, which serves as an important algorithm category for image segmentation. However, it fails if the histogram is close to unimodal or has large intra-class variances. To alleviate this limitation, improved Otsu&#39;s methods such as the valley emphasis method and weighted object variances method have been proposed, which still yield non-optimal segmentation performance in some cases. In this study, a modified valley metric using second-order derivative is proposed to improve the Otsu&#39;s algorithm. Experiments are firstly conducted on five typical test images whose histograms are unimodal, multimodal or have large intra-class variances, and then expanded to a larger data set consisting of 22 cell images. The proposed algorithm is compared with original Otsu&#39;s method and existing improved algorithms. Four evaluation metrics including misclassification error, foreground recall, Dice similarity coefficient and Jaccard index are adopted to quantitatively measure the segmentation performance. Results show that the proposed algorithm achieves best segmentation results on both data sets quantitatively and qualitatively. The proposed algorithm adapts the Otsu&#39;s method to more image subtypes, indicating a wider application in automatic thresholding and image segmentation field.},
  archive      = {J_IETIP},
  author       = {Jiangwa Xing and Pei Yang and Letu Qingge},
  doi          = {10.1049/iet-ipr.2019.0176},
  journal      = {IET Image Processing},
  month        = {2},
  number       = {3},
  pages        = {536-544},
  shortjournal = {IET Image Process.},
  title        = {Automatic thresholding using a modified valley emphasis},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Application of random elements in ISS. <em>IETIP</em>,
<em>14</em>(3), 530–535. (<a
href="https://doi.org/10.1049/iet-ipr.2018.5648">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The -threshold image secret sharing (ISS) encodes a secret image into n shares. When k or more shares are obtained, the secret image can be decoded; however, less than k shares could decode none of the secret image. ISS primarily includes polynomial-based ISS and visual secret sharing (VSS). In this study, the authors find that the random elements in ISS can be used not only to hide information but also to obtain more features such as multiple decryptions and comprehensible share. They have established an application model of random elements that is suitable for both polynomial-based ISS and VSS. On the basis of the model, they have extended three algorithms to achieve information hiding, multiple decryptions and comprehensible share. Experiments indicate the effectiveness of these algorithms.},
  archive      = {J_IETIP},
  author       = {Xuehu Yan and Yuliang Lu and Lintao Liu and Xia Li and Jingju Liu and Guozheng Yang},
  doi          = {10.1049/iet-ipr.2018.5648},
  journal      = {IET Image Processing},
  month        = {2},
  number       = {3},
  pages        = {530-535},
  shortjournal = {IET Image Process.},
  title        = {Application of random elements in ISS},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020a). Novel multiple images encryption algorithm using CML system
and DNA encoding. <em>IETIP</em>, <em>14</em>(3), 518–529. (<a
href="https://doi.org/10.1049/iet-ipr.2019.0771">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this study, the authors advance a novel multiple images encryption algorithm using the coupled map lattices (CML) system and DNA encoding. The algorithm adopts a permutation-diffusion structure. First, the initial values and parameters of the CML system are determined by given values and SHA hash key of original images. Secondly, several plain images are grouped and permuted among groups. Next, different groups are combined into eight blocks of the same size, each of which is independently coded and scrambled. Thirdly, in the diffusion process, a DNA-level multiplication operation is redefined, and required key matrices are resulted from a small key connected with original images. Finally, dynamic DNA coding/decoding is adopted, and the coding and decoding rules are determined by original images. The evaluation of the simulation experiment shows that the proposed algorithm is safe and feasible, and has good encryption effect.},
  archive      = {J_IETIP},
  author       = {Hao Zhang and Xiao-qing Wang and Xing-yuan Wang and Peng-fei Yan},
  doi          = {10.1049/iet-ipr.2019.0771},
  journal      = {IET Image Processing},
  month        = {2},
  number       = {3},
  pages        = {518-529},
  shortjournal = {IET Image Process.},
  title        = {Novel multiple images encryption algorithm using CML system and DNA encoding},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Left ventricular segmentation based on a parallel watershed
transformation towards an accurate heart function evaluation.
<em>IETIP</em>, <em>14</em>(3), 506–517. (<a
href="https://doi.org/10.1049/iet-ipr.2018.6379">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Magnetic resonance imaging (MRI) has emerged as the golden reference for cardiac examination. This modality allows the assessment of human cardiovascular morphology, functioning, and perfusion. Although a couple of challenging issues, such as the cardiac magnetic resonance (MR) image&#39;s features and the large variability of images among several patients, still influences the cardiac cavities’ segmentation and needs to be carried out. In this study, the authors have profoundly reviewed and fully compared semi-automated segmentation methods performed on cardiac cine-MR short-axis images for the evaluation of the left ventricular functions. However, the number of parameters handled by the synthesised works is limited if not null. For the sake of ensuring the highest coverage of the left ventricle parameters computing, they have introduced a parallel watershed-based approach to segment the left ventricular allowing hence the computation of six parameters (end-diastolic volume, end-systolic volume, ejection fraction, cardiac output, stroke volume, and left ventricular mass). An algorithm is associated with the main considered measurements. The experimental results that were obtained through studying 20 patients’ MRI data base demonstrate their approach&#39;s accuracy in estimating real values of the parameters’ set thanks to a faithful segmentation of the myocardium.},
  archive      = {J_IETIP},
  author       = {Ramzi Mahmoudi and Narjes Ben Ameur and Asma Ammari and Mohamed Akil and Rachida Saouli and Badii Hmida and Momahed Hedi Bedoui},
  doi          = {10.1049/iet-ipr.2018.6379},
  journal      = {IET Image Processing},
  month        = {2},
  number       = {3},
  pages        = {506-517},
  shortjournal = {IET Image Process.},
  title        = {Left ventricular segmentation based on a parallel watershed transformation towards an accurate heart function evaluation},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Fully automated scheme for computer-aided detection and
breast cancer diagnosis using digitised mammograms. <em>IETIP</em>,
<em>14</em>(3), 495–505. (<a
href="https://doi.org/10.1049/iet-ipr.2018.5953">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Breast cancer becomes a significant public health problem in the world. During the early detection of breast cancer, it is a very challenging task to classify accurately the benign–malignant patterns in digital mammograms. This study proposes a new fully automated computer-aided diagnosis (CAD) system for breast cancer diagnosis with high-accuracy and low-computational requirements. The expectation–maximisation algorithm is investigated to extract automatically the region of interests (ROIs) within mammograms. The standard shape, statistical, and textural features of ROIs are extracted and combined with multi-resolution and multi-orientation features derived from a new feature extraction technique based on wavelet-based contourlet transform. A hybrid feature selection approach based on combining the support vector machine recursive feature elimination with correlation bias reduction algorithm is proposed. Also, the authors investigate a new similarity-based learning algorithm, called Q, for benign–malignant classification. The proposed CAD system is applied to real clinical mammograms, and the experimental results demonstrate the superior performance of the proposed CAD system over other existing CAD systems in terms of accuracy 98.16%, sensitivity 98.63%, specificity 97.80%, and computational time 2.2 s. This reveals the effectiveness of the proposed CAD system in improving the accuracy of breast cancer diagnosis in real-time systems.},
  archive      = {J_IETIP},
  author       = {Ahmed S. Eltrass and Mohamed S. Salama},
  doi          = {10.1049/iet-ipr.2018.5953},
  journal      = {IET Image Processing},
  month        = {2},
  number       = {3},
  pages        = {495-505},
  shortjournal = {IET Image Process.},
  title        = {Fully automated scheme for computer-aided detection and breast cancer diagnosis using digitised mammograms},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Image classification using SLIC superpixel and FAAGKFCM
image segmentation. <em>IETIP</em>, <em>14</em>(3), 487–494. (<a
href="https://doi.org/10.1049/iet-ipr.2019.0255">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image classification is one of the popular fields for researchers in computer vision. This study highlights the use of simple linear iterative clustering (SLIC) superpixel in combination with fast and automatically adjustable Gaussian radial basis function kernel-based fuzzy C-means (FAAGKFCM) for image segmentation along with the deep learning techniques. Bag-of-feature with speeded up robust feature along with deep features are used for classification of 101 classes of the image and 256 classes of the image from Caltech 101, Caltech 256 and MIT 67 image datasets. The combination of SLIC superpixel with FAAGKFCM image segmentation acts as the pre-processing step for image classification, which in turn provides a better result in the classification of images. This method has achieved an accuracy of 94% in Caltech 101 dataset, 85% in Caltech 256 dataset and 84% in MIT 67 dataset.},
  archive      = {J_IETIP},
  author       = {Nongmeikapam Kishorjit Singh and Ningthoujam Johny Singh and Wahengbam Kanan Kumar},
  doi          = {10.1049/iet-ipr.2019.0255},
  journal      = {IET Image Processing},
  month        = {2},
  number       = {3},
  pages        = {487-494},
  shortjournal = {IET Image Process.},
  title        = {Image classification using SLIC superpixel and FAAGKFCM image segmentation},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Effect of pooling strategy on convolutional neural network
for classification of hyperspectral remote sensing images.
<em>IETIP</em>, <em>14</em>(3), 480–486. (<a
href="https://doi.org/10.1049/iet-ipr.2019.0561">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The deep convolutional neural network (CNN) has recently attracted the researchers for classification of hyperspectral remote sensing images. The CNN mainly consists of convolution layer, pooling layer and fully connected layer. The pooling is a regularisation technique and improves the performance of CNN while reducing the computation time. Various pooling strategies have been developed in literature. This study shows the effect of pooling strategy on the performance of deep CNN for classification of hyperspectral remote sensing images. The authors have compared the performance of various pooling strategies such as max pooling, average pooling, stochastic pooling, rank-based average pooling and rank-based weighted pooling. The experiments were performed on three well-known hyperspectral remote sensing datasets: Indian Pines, University of Pavia and Kennedy Space Center. The proposed experimental results show that max pooling has produced better results for all the three considered datasets.},
  archive      = {J_IETIP},
  author       = {Somenath Bera and Vimal K. Shrivastava},
  doi          = {10.1049/iet-ipr.2019.0561},
  journal      = {IET Image Processing},
  month        = {2},
  number       = {3},
  pages        = {480-486},
  shortjournal = {IET Image Process.},
  title        = {Effect of pooling strategy on convolutional neural network for classification of hyperspectral remote sensing images},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Edge-aware image filtering using a structure-guided CNN.
<em>IETIP</em>, <em>14</em>(3), 472–479. (<a
href="https://doi.org/10.1049/iet-ipr.2018.6691">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image filtering is a fundamental preprocessing step for accurate, robust computer vision applications such as image segmentation, object classification, and reconstruction. However, many convolutional neural network (CNN)-based methods tend to lose significant edge information in the output layer, and generate undesired artefacts in the feature extraction layers. This study presents a deep CNN model for edge-aware image filtering. The proposed network model consists of three sub-networks: (i) feature extraction, (ii) convolution artefact removal, and (iii) structure extraction networks. The proposed network model has an end-to-end trainable architecture that does not need any post-processing steps. Especially, the structure extraction network can successfully preserve significant edges. The proposed filter outperforms state-of-the-art denoising filters in terms of both objective and subjective measures, and can be used for various image enhancement and restoration problems such as edge-preserving smoothing, image denoising, deblurring, and deblocking.},
  archive      = {J_IETIP},
  author       = {Sijung Kim and Changho Song and Jinbeum Jang and Joonki Paik},
  doi          = {10.1049/iet-ipr.2018.6691},
  journal      = {IET Image Processing},
  month        = {2},
  number       = {3},
  pages        = {472-479},
  shortjournal = {IET Image Process.},
  title        = {Edge-aware image filtering using a structure-guided CNN},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020a). Enhanced copy–paste forgery detection in digital images
using scale-invariant feature transform. <em>IETIP</em>, <em>14</em>(3),
462–471. (<a href="https://doi.org/10.1049/iet-ipr.2019.0842">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image forgery detection and localisation is one of the principal problems in digital forensics. Copy–paste forgery in digital images is a type of forgery in which an image region is copied and pasted at another location within the same image. In this work, the authors propose a methodology to detect and localise copy-pasted regions in images based on scale-invariant feature transform (SIFT). Existing copy-paste forgery detection in images using SIFT and clustering techniques such as hierarchical agglomerative and density-based spatial clustering of applications with noise resulted many false pixel detections. They have introduced sensitivity-based clustering along with SIFT features to identify copy–pasted pixels and disregard the false pixels. Experimental evaluation on public image datasets MICC-F220, MICC-F2000 and MICC-F8 multi shows that the proposed method is showing improved performance in detecting and localising copy-paste forgeries in images than the existing works. Also the proposed work detects multiple copy–pasted regions in the images and is robust to attacks such as geometrical transformation of copied regions such as scaling and rotation.},
  archive      = {J_IETIP},
  author       = {Priyadharsini Selvaraj and Muneeswaran Karuppiah},
  doi          = {10.1049/iet-ipr.2019.0842},
  journal      = {IET Image Processing},
  month        = {2},
  number       = {3},
  pages        = {462-471},
  shortjournal = {IET Image Process.},
  title        = {Enhanced copy–paste forgery detection in digital images using scale-invariant feature transform},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Soft biometrics-based face image retrieval using improved
grey wolf optimisation. <em>IETIP</em>, <em>14</em>(3), 451–461. (<a
href="https://doi.org/10.1049/iet-ipr.2019.0271">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Humans often use faces to recognise and identify individuals. Face recognition is one of the important tasks carried out by forensic examiners manually during their investigation, when there is an evidence image/video available from a crime scene. There is a growing demand for face recognition from unconstrained images, which is valuable for criminal investigators in identifying the victims. When an input face image is given to the proposed system, it filters from large scale face dataset to find the top-k similar faces. Deep convolutional neural network approach is employed to extract important features present in the input face image and improved grey wolf optimisation approach is proposed to select optimal features from the extracted features. It is then preceded by a soft biometric-based face matcher that helps in retrieving exact face image from the top-k similar faces matched using approximate nearest neighbour. The performance of the proposed system is evaluated using LFW, CASIA, Multi-pie and Color-Feret datasets. The proposed system addresses the challenge of searching face images from a large collection of unconstrained images by incorporating feature retrieval using DCNN and IGWO with soft biometric face matcher in cascaded framework which improves accuracy and reduces computation and retrieval time.},
  archive      = {J_IETIP},
  author       = {Haseena Sikkandar and Revathi Thiyagarajan},
  doi          = {10.1049/iet-ipr.2019.0271},
  journal      = {IET Image Processing},
  month        = {2},
  number       = {3},
  pages        = {451-461},
  shortjournal = {IET Image Process.},
  title        = {Soft biometrics-based face image retrieval using improved grey wolf optimisation},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Integrating taylor–krill herd-based SVM to fuzzy-based
adaptive filter for medical image denoising. <em>IETIP</em>,
<em>14</em>(3), 442–450. (<a
href="https://doi.org/10.1049/iet-ipr.2018.6434">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Medical imaging systems contribute much towards effective decision-making by the physicians, which is highly essential in the day-to-day life of humans. In this study, Taylor–Krill herd (KH)-based support vector machine (SVM) is proposed for medical image denoising. The Taylor–KH-based SVM is the integration of Taylor series in KH optimisation algorithm, which is used for tuning the optimal weights of the SVM classifier. The efficiency of KH is due to two global and two local optimisers, and the adaptive operators ensure the adaptive nature of KH. Above all, KH never uses the derivative information as it employs the stochastic search and thereby, reduces the complexity of the algorithm. The proposed method tunes the hyperplane parameters of SVM optimally so that the optimal identification of the noisy pixels in the image is ensured and replaced with adaptive weights. The proposed method is analysed based on the metrics, such as peak signal-to-noise ratio (PSNR), structural similarity (SSIM) and the comparative analysis is done with existing methods for showing the effectiveness of the proposed method. The simulation result shows that the proposed method acquired a PSNR of 30.36 dB and SSIM of 0.89, respectively.},
  archive      = {J_IETIP},
  author       = {C. Narasimha and A. Nagaraja Rao},
  doi          = {10.1049/iet-ipr.2018.6434},
  journal      = {IET Image Processing},
  month        = {2},
  number       = {3},
  pages        = {442-450},
  shortjournal = {IET Image Process.},
  title        = {Integrating Taylor–Krill herd-based SVM to fuzzy-based adaptive filter for medical image denoising},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). BM3D-GT&amp;AD: An improved BM3D denoising algorithm based
on gaussian threshold and angular distance. <em>IETIP</em>,
<em>14</em>(3), 431–441. (<a
href="https://doi.org/10.1049/iet-ipr.2019.0469">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Block-matching and three-dimensional filtering (BM3D) is generally considered as a milestone for its outstanding performance in the area of image denoising. However, it still suffers from the loss of image detail due to the utilisation of hard thresholding on transform domain during the phase of the basic estimate. In the frequency domain, a large amount of image detail information is in high frequency, which tends to be mixed with noise. Since its low amplitude is below the threshold, some image detail is filtered out with the noise. To retain more details, this study proposes an improved BM3D. It adopts an adaptable threshold with the core of Gaussian function during hard thresholding, which can filter out more noise while retaining more high-frequency information. When grouping, the normalised angular distance is taken as a measure of similarity to relieve the interference of noise further and achieve a higher peak signal-to-noise ratio (PSNR). The experimental results show that under the background of Gaussian noise with standard deviation of 20–60, the PSNR of denoised images (with a large amount of detail), applied with the authors’ improved algorithm, can be improved by compared with original BM3D.},
  archive      = {J_IETIP},
  author       = {Qinping Feng and Shuping Tao and Chao Xu and Guang Jin},
  doi          = {10.1049/iet-ipr.2019.0469},
  journal      = {IET Image Processing},
  month        = {2},
  number       = {3},
  pages        = {431-441},
  shortjournal = {IET Image Process.},
  title        = {BM3D-GT&amp;AD: An improved BM3D denoising algorithm based on gaussian threshold and angular distance},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Design of vision-based indoor positioning based on embedded
system. <em>IETIP</em>, <em>14</em>(3), 423–430. (<a
href="https://doi.org/10.1049/iet-ipr.2018.6285">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Indoor positioning techniques have become very important in recent years. Due to the wide deployment of surveillance cameras, it has become feasible to use the videos for indoor positioning. The success of using this approach can also reduce the load of security persons of watching the monitors all the time. In this study, the authors propose a vision-based indoor positioning system. The proposed method uses a frame processing technique and applies the Gaussian mixture learning for video background model. The foreground object can be extracted by using the background subtraction. Based on the foreground object, the objects can be tracked and used in the direct linear transform, and generate a bird&#39;s-eye map with camera information. A real-time demonstration has been also provided. It shows the tracing of the moving objects and the bird&#39;s-eye view.},
  archive      = {J_IETIP},
  author       = {Tsung-Han Tsai and Chih-Hao Chang and Shih-Wei Chen and Chia-Hsiang Yao},
  doi          = {10.1049/iet-ipr.2018.6285},
  journal      = {IET Image Processing},
  month        = {2},
  number       = {3},
  pages        = {423-430},
  shortjournal = {IET Image Process.},
  title        = {Design of vision-based indoor positioning based on embedded system},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). CAD: Concatenated action descriptor for one and two
person(s), using silhouette and silhouette’s skeleton. <em>IETIP</em>,
<em>14</em>(3), 417–422. (<a
href="https://doi.org/10.1049/iet-ipr.2018.6437">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study introduces an action descriptor that has the ability to perform human action recognition efficiently for one and two person(s). The authors’ proposed descriptor computes information like motion, spatial–temporal, diversion with respect to the centroid, critical point and keypoint detection, whereas the existing approaches lack to address this information efficiently. Action descriptors are developed from signature-based optical flow, signature-based corner points and binary robust invariant scalable keypoints. These action descriptors are applied to silhouette and silhouette&#39;s skeleton frames. These aforementioned action descriptors lead to developing the concatenated action descriptor (CAD). In order to develop action descriptors, the reference video frame plays an important role. Weizmann (one person) and both clean and noise versions of SBU Kinect Interaction (two persons) datasets are used for the evaluation of their proposed descriptors. On the other hand, classifications are performed by using support vector machine. Experimental results demonstrate that CAD not only outperforms among the entire proposed descriptors, but also provides better performance as compared to state-of-the-art approaches.},
  archive      = {J_IETIP},
  author       = {M. Shujah Islam and Mansoor Iqbal and Nuzhat Naqvi and Khush Bakhat and M. Mattah Islam and Shamsa Kanwal and Zhongfu Ye},
  doi          = {10.1049/iet-ipr.2018.6437},
  journal      = {IET Image Processing},
  month        = {2},
  number       = {3},
  pages        = {417-422},
  shortjournal = {IET Image Process.},
  title        = {CAD: Concatenated action descriptor for one and two person(s), using silhouette and silhouette&#39;s skeleton},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Video segmentation scheme based on AMC. <em>IETIP</em>,
<em>14</em>(3), 407–416. (<a
href="https://doi.org/10.1049/iet-ipr.2018.6659">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Video segmentation has become a fundamental of various multimedia applications. Spatiotemporal coherence is important for video segmentation. In this study, to balance the spatiotemporal coherence in scenes with deformation or large motion, the authors propose a novel segmentation scheme based on the absorbing Markov chain (AMC) model named directed graph segmentation based on AMC. In their study, they first generate primary proposals per frame. Then, they train weight models by using a part of primary proposals with their features and feature scores. Next, they construct a directed AMC graph, in which states are the generated primary proposals and edge weights are decided by trained weight models. They subsequently perform the first proposal selection per frame by thresholding the modified absorbed time. Afterwards, they design a reselection algorithm to filter the selected proposals and ensure the proposals, which are the most likely to be the motion object in each frame, to be selected as candidates. Finally, they employ the graph-cuts based optimisation algorithm to generate refined per pixel segmentation by using object and background models built by candidate proposals under the concept of Gaussian mixture models. Experimental results demonstrate that the proposed scheme shows competitive performance compared with advanced algorithms.},
  archive      = {J_IETIP},
  author       = {Ying Cao and Lijuan Sun and Chong Han and Jian Guo},
  doi          = {10.1049/iet-ipr.2018.6659},
  journal      = {IET Image Processing},
  month        = {2},
  number       = {3},
  pages        = {407-416},
  shortjournal = {IET Image Process.},
  title        = {Video segmentation scheme based on AMC},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). New video encryption schemes based on chaotic maps.
<em>IETIP</em>, <em>14</em>(2), 397–406. (<a
href="https://doi.org/10.1049/iet-ipr.2018.5250">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Two new encryption algorithms for secure video transmission are proposed in this paper. The two algorithms employ different types of chaotic maps to generate the keystream for encrypting the video frames. Both algorithms involve a substitution step and a permutation step to achieve confusion and diffusion requirements. For efficient transmission, the video file is compressed before being encrypted. In the basic implementation of both algorithms, MPEG-2 standard is used for compression. However, the algorithms are shown to be compliant with other compression techniques. In the permutation step, the effect of the block size used in the shuffling process is examined. Smaller blocks result in increasing the processing time, while reducing both the correlation between adjacent pixels and the peak-signal-to noise ratio of an encrypted frame. The use of a Feistel structure is investigated to enhance security and its negative impact on the encryption time is demonstrated. The experimental results of the two proposed schemes confirm that they represent different tradeoffs between security and computational efficiency. Both schemes are sensitive to slight variations in the encryption key as apparent from the obtained differential measures. The conducted comparative study shows the competitiveness of the proposed schemes to existing schemes in literature.},
  archive      = {J_IETIP},
  author       = {Hassan Elkamchouchi and Wessam M. Salama and Yasmine Abouelseoud},
  doi          = {10.1049/iet-ipr.2018.5250},
  journal      = {IET Image Processing},
  month        = {2},
  number       = {2},
  pages        = {397-406},
  shortjournal = {IET Image Process.},
  title        = {New video encryption schemes based on chaotic maps},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). No-reference image quality assessment via structural
information fluctuation. <em>IETIP</em>, <em>14</em>(2), 384–396. (<a
href="https://doi.org/10.1049/iet-ipr.2019.0750">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image quality assessment (IQA) is a meaningful research topic to meet the increasing demand of high-quality image. The degradation of image quality will cause changes in image structural information. Meanwhile, human visual system is sensitive to changes in structural information. This finding motivates us to utilise structural information for proposing IQA method which is consistent with human visual perception. Recently, IQA methods are mainly focused on individual image type, e.g. natural image or screen content image (SCI), thus, the authors proposed a novel no-reference IQA method which can be suitable for both natural image and SCI. The proposed method is based on structural information analysis. For each image, they first obtain the grey-scale fluctuation maps (GFMs) in four detection directions. After that, the grey-scale fluctuation direction map (GFD) of certain image can be acquired via its GFMs. Based on the GFMs and GFD, the structural features of each image are extracted, and then collected and transformed to feature vectors. Subsequently, the IQA model is trained by support vector regression. The experimental results on the public databases demonstrate the proposed method can predict image quality accurately for both natural image and SCI, and the performance is competitive with prevalent methods.},
  archive      = {J_IETIP},
  author       = {Xichen Yang and Tianshu Wang and Genlin Ji},
  doi          = {10.1049/iet-ipr.2019.0750},
  journal      = {IET Image Processing},
  month        = {2},
  number       = {2},
  pages        = {384-396},
  shortjournal = {IET Image Process.},
  title        = {No-reference image quality assessment via structural information fluctuation},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Fast and robust video stabilisation with preserved
intentional camera motion and smear removal for infrared video.
<em>IETIP</em>, <em>14</em>(2), 376–383. (<a
href="https://doi.org/10.1049/iet-ipr.2019.0764">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Border military surveillance is one of the demanding and challenging tasks for any nation. Thermal (infrared) camera, which works on the infrared domain, provides complete visual sequences even in pitch dark night conditions. When the video is recorded from a thermal camera mounted on a vehicle, the output video is unstabilised with poor visual quality due to unintentional camera motion. These unwanted motions also introduce smear. Furthermore, there may be situations when a camera is moved intentionally to capture target. This study proposes a fast and robust algorithm for auto stabilisation of videos with smear removal while keeping the intentional motion of camera. This algorithm is developed under the framework of speeded up robust features matching. The proposed algorithm is capable of correcting both motions, i.e. translation as well as rotational. Quality improvement of up to 21 dB is achieved in the stabilised output videos.},
  archive      = {J_IETIP},
  author       = {Sudhir Khare and Manvendra Singh and Brajesh Kumar Kaushik},
  doi          = {10.1049/iet-ipr.2019.0764},
  journal      = {IET Image Processing},
  month        = {2},
  number       = {2},
  pages        = {376-383},
  shortjournal = {IET Image Process.},
  title        = {Fast and robust video stabilisation with preserved intentional camera motion and smear removal for infrared video},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Non-redundant frame identification and keyframe selection in
DWT-PCA domain for authentication of video. <em>IETIP</em>,
<em>14</em>(2), 366–375. (<a
href="https://doi.org/10.1049/iet-ipr.2019.0341">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study is intended to protect video data and watermark from unauthorised access. The proposed methodology accentuates two new algorithms, namely structural similarity index metric–absolute difference metric (SSIM-AMD) based non-redundant frame identification (NRFI) and entropy–AMD based keyframe selection (KFS) to reduce the challenges posed by traditional discrete wavelet transform–singular value decomposition. Traditional techniques embed the entire watermark to all existing frames in the video, which is cumbersome and time-consuming. In this methodology, NRFI algorithm is applied to segregate the redundant and non-redundant frames to specific database. The KFS algorithm is used to identify suitable keyframes. DWT is applied into keyframes, which decomposes the frames into subbands. The middle band is selected for embedding. The principal component of watermark image block is embedded into identified keyframes in the video. The chaotic map is adapted to reorder the watermark block for improving the authentication level of the watermarking. The ant colony optimization (ACO) technique is adapted to select the suitable scaling factor for watermarking process. The principal component analysis technique is employed for avoiding false-positive attacks. Experimental results show the proposed methodology can withstand image processing, video processing, false-positive attacks and produces good results in terms of perceptual quality and robustness.},
  archive      = {J_IETIP},
  author       = {Sethuraman Ponni alias Sathya and Srinivasan Ramakrishnan},
  doi          = {10.1049/iet-ipr.2019.0341},
  journal      = {IET Image Processing},
  month        = {2},
  number       = {2},
  pages        = {366-375},
  shortjournal = {IET Image Process.},
  title        = {Non-redundant frame identification and keyframe selection in DWT-PCA domain for authentication of video},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Highly efficient neoteric histogram–entropy-based rapid and
automatic thresholding method for moving vehicles and pedestrians
detection. <em>IETIP</em>, <em>14</em>(2), 354–365. (<a
href="https://doi.org/10.1049/iet-ipr.2018.5555">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Thresholding for segmentation is an important key step and necessary process in various applications. Estimating an accurate threshold value for a complex and coarse image is computationally expensive and lacks accuracy and stability. This study is aimed at developing a general histogram–entropy-based thresholding method, referred as our HEBT method, for fast and efficient automatic threshold value evaluation. In the proposed method, the probability density function and Shannon entropy derived from 1D bimodal histogram have been used to find the optimal threshold values automatically. The proposed method implemented with a three-frame differencing segmentation technique has been tested on real-time datasets – change detection 2012, change detection 2014, and Wallflower – to identify pedestrians and vehicles. The performance of our HEBT method has been compared with six state-of-the-art automatic thresholding methods. The experimental segmented image results confirmed that our HEBT method is more adaptable and better suited for real-time systems with severe challenging conditions of great variations. Further, the new HEBT method achieved the best segmentation results with highest values of several performance parameters, i.e. recall, precision, similarity, and f-measure. Interestingly, the computation time is the lowest for the proposed method than the state-of-the-art methods, promising its application for a fast and effective image segmentation.},
  archive      = {J_IETIP},
  author       = {Karnam Silpaja Chandrasekar and Planisamy Geetha},
  doi          = {10.1049/iet-ipr.2018.5555},
  journal      = {IET Image Processing},
  month        = {2},
  number       = {2},
  pages        = {354-365},
  shortjournal = {IET Image Process.},
  title        = {Highly efficient neoteric histogram–entropy-based rapid and automatic thresholding method for moving vehicles and pedestrians detection},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Identification of wool and mohair fibres with texture
feature extraction and deep learning. <em>IETIP</em>, <em>14</em>(2),
348–353. (<a href="https://doi.org/10.1049/iet-ipr.2019.0907">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Wool and mohair fibres are both animal-based fibres and having circular scales on their microscopic images from the longitudinal view. Although they look very similar in their microscopic view, they show different physical/chemical properties which determine their usage area. Thus, in textile industry, they need to be separated carefully from each other. The separation of wool/mohair fibres is an important issue and can be performed with human eye by using the microscopic images, that is not time/cost effective and not objective. The novelty of the presented study is to design an objective, easy, rapid, time and cost-effective method in order to separate wool fibre from mohair fibre by using a texture analysis based identification method. For this purpose, microscopic images of both wool and mohair fibres were preprocessed as the texture images. Local binary pattern-based feature extraction process and deep learning were separately used to get determinative information from the fibres. In order to identify the samples, the classification based method was completed. Experimental results indicated that an accurate texture analysis for this kind of animal fibres is possible to identify wool and mohair fibres by using deep learning and machine learning with 99.8% and 90.25% accuracy rates, respectively.},
  archive      = {J_IETIP},
  author       = {Kazim Yildiz},
  doi          = {10.1049/iet-ipr.2019.0907},
  journal      = {IET Image Processing},
  month        = {2},
  number       = {2},
  pages        = {348-353},
  shortjournal = {IET Image Process.},
  title        = {Identification of wool and mohair fibres with texture feature extraction and deep learning},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Investigation of filtering of rain streaks affected video
sequences under various quantisation parameter in HEVC encoder using an
enhanced v-BM4D algorithm. <em>IETIP</em>, <em>14</em>(2), 337–347. (<a
href="https://doi.org/10.1049/iet-ipr.2018.6005">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study proposes an enhanced video block matching four-dimensional (V-BM4D) denoising algorithm for highly correlated rain streaks pattern video sequence in high-efficiency video coding (HEVC) encoder. Removal of structural rain streaks from high definition (HD) video sequences are challenging issues in real-time video application. Temporal information of original video sequences is distorted among the successive video frames due to the presence of natural degradation parameters such as fog, smog and rain streaks. Rain streaks are highly correlated structural noise pattern, which affects the nature of video frame compared to other natural degradations. Existing HEVC encoder is employed with in-loop filtering to protect the temporal information of original video sequences from rain streaks pattern. However, in-loop filtering is unable to remove the highly directional oriented rain pattern from video frames. To retain temporal information among the successive video frames, enhanced block matching denoising block is adopted in HEVC coder. The proposed enhanced VBM4D- based in-loop filtering eliminates the various level of rain streaks from the HD video sequences by temporal information prediction in 4D platform. Experimental results demonstrate that the proposed denoising algorithm yields better peak signal to noise ratio value and provides better bit rate saving for various HEVC configuration.},
  archive      = {J_IETIP},
  author       = {Thiyagarajan Jayaraman and Gowri Shankar Chinnusamy},
  doi          = {10.1049/iet-ipr.2018.6005},
  journal      = {IET Image Processing},
  month        = {2},
  number       = {2},
  pages        = {337-347},
  shortjournal = {IET Image Process.},
  title        = {Investigation of filtering of rain streaks affected video sequences under various quantisation parameter in HEVC encoder using an enhanced V-BM4D algorithm},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Reversible contrast enhancement for medical images with
background segmentation. <em>IETIP</em>, <em>14</em>(2), 327–336. (<a
href="https://doi.org/10.1049/iet-ipr.2019.0423">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Contrast enhancement (CE) of medical images is helpful to bring out the unclear content in the interested regions. Recently, reversible CE has been proposed so that the original version of a contrast-changed image can be exactly recovered. This property can be used to save storage space or facilitate the archiving system. To enhance the regions of interest (ROI) without introducing visual distortions, the technique of image segmentation (e.g. using Otsu&#39;s method) has been used to obtain the background before conducting the CE process. To segment the ROI more accurately, an interactive algorithm called GrabCut is employed in the proposed scheme. In addition, a new preprocessing strategy is adopted to preserve the image quality through the CE process. Consequently, the content in the selected regions can be better brought out while the reversibility of the CE process is achieved. The experimental results on 30 chest radiograph images and 20 magnetic resonance images have demonstrated the efficacy of the proposed scheme for reversible CE. The evaluation results are provided to show the better performances of the proposed method in achieving CE effects and preserving image quality.},
  archive      = {J_IETIP},
  author       = {Hao-Tian Wu and Qi Huang and Yiu-ming Cheung and Lingling Xu and Shaohua Tang},
  doi          = {10.1049/iet-ipr.2019.0423},
  journal      = {IET Image Processing},
  month        = {2},
  number       = {2},
  pages        = {327-336},
  shortjournal = {IET Image Process.},
  title        = {Reversible contrast enhancement for medical images with background segmentation},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Ensemble of deep convolutional neural networks based
multi-modality images for alzheimer’s disease diagnosis. <em>IETIP</em>,
<em>14</em>(2), 318–326. (<a
href="https://doi.org/10.1049/iet-ipr.2019.0617">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Alzheimer&#39;s disease (AD) is one of the most common progressive neurodegenerative diseases. Structural magnetic resonance imaging (MRI) would provide abundant information on the anatomical structure of human organs. Fluorodeoxy-glucose positron emission tomography (PET) obtains the metabolic activity of the brain. Previous studies have demonstrated that multi-modality images could contribute to improve diagnosis of AD. However, these methods need to extract the handcrafted features that demand domain specific knowledge and image processing stage is time consuming. In order to tackle these problems, in this study, the authors propose a novel framework that ensembles three state-of-the-art deep convolutional neural networks (DCNNs) with multi-modality images for AD classification. In detail, they extract some slices from each subject of each modality, and every DCNN generates a probabilistic score for the input slices. Furthermore, a ‘dropout’ mechanism is introduced to discard low discrimination slices of the category probabilities. Then average reserved slices of each subject are acquired as a new feature. Finally, they train the Adaboost ensemble classifier based on single decision tree classifier with the MRI and PET probabilistic scores of each DCNN. Evaluations on Alzheimer&#39;s Disease Neuroimaging Initiative database show that the proposed algorithm has better performance compared to existing method, the algorithm proposed in this study significantly improved the classification accuracy.},
  archive      = {J_IETIP},
  author       = {Xusheng Fang and Zhenbing Liu and Mingchang Xu},
  doi          = {10.1049/iet-ipr.2019.0617},
  journal      = {IET Image Processing},
  month        = {2},
  number       = {2},
  pages        = {318-326},
  shortjournal = {IET Image Process.},
  title        = {Ensemble of deep convolutional neural networks based multi-modality images for alzheimer&#39;s disease diagnosis},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Improving 3D reconstruction accuracy in wavelet transform
profilometry by reducing shadow effects. <em>IETIP</em>, <em>14</em>(2),
310–317. (<a href="https://doi.org/10.1049/iet-ipr.2019.0854">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Wavelet transform profilometry is a three-dimensional (3D) reconstruction method based on the structured light technique of fringe pattern projection, widely used because it is a non-invasive, high-performance 3D reconstruction method. The presence of shadows created by the object in the image capture process is an obstacle in obtaining accurate 3D reconstructions, as they add noise to the phase data, leading to artefacts in object reconstruction, even when using robust phase-unwrapping algorithms. Since shadows present diverse intensities and shapes, detecting and eliminating their effects are challenging tasks. This work presents a novel method to detect shadow regions and reduce their effects in 3D reconstruction. The proposed method uses coloured fringe patterns to detect the shadows and mathematical morphology to condition the outlines of the shadow regions. The shadow outline information is used to interpolate the background-plane fringe pattern onto the captured scene, where the shadows are detected. The mean squared error (MSE) of the reconstructed objects is reduced to 25% of the MSE without shadow removal, on an average, when using the Bioucas phase-unwrapping method. When using the Ghiglia phase-unwrapping method, the MSE reduction is to 8.3%, on an average, of the MSE in the shadow case.},
  archive      = {J_IETIP},
  author       = {Claudia-Victoria López-Torres and Sebastián Salazar Colores and Kevin Kells and Jesús-Carlos Pedraza-Ortega and Juan-Manuel Ramos-Arreguin},
  doi          = {10.1049/iet-ipr.2019.0854},
  journal      = {IET Image Processing},
  month        = {2},
  number       = {2},
  pages        = {310-317},
  shortjournal = {IET Image Process.},
  title        = {Improving 3D reconstruction accuracy in wavelet transform profilometry by reducing shadow effects},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Blur parameter locus curve and its applications.
<em>IETIP</em>, <em>14</em>(2), 297–309. (<a
href="https://doi.org/10.1049/iet-ipr.2019.0577">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Conventionally, the point spread function (PSF) is understood as a characteristic function of any optical system. It captures the information about the amount of blur present along all the directions for a point in the scene. However, the dependence of blur on the PSF is in the form of convolution for any object other than a point source present in the scene and hence their relationship is less explicit. The authors propose a blur parameter locus curve (BPLC) as a system representation which has a one to one relationship with blur. BPLC simply is a chart of blur amounts in all directions of a given PSF with respect to the selected measurement function. They further characterise the PSF by decomposing the variation of BPLC across all directions based on the study performed for different possible forms of the blur kernels. Such decomposition provides powerful tools for various analysis. As PSF can be anisotropic, the computation of BPLC becomes an essential intermediate step to obtain the scale map as at the same scale, blur is different in different directions. Furthermore, they demonstrate the use of BPLC to obtain other system characteristics function such as PSF.},
  archive      = {J_IETIP},
  author       = {Himanshu Kumar and Sumana Gupta and Venkatesh K. Subramanian},
  doi          = {10.1049/iet-ipr.2019.0577},
  journal      = {IET Image Processing},
  month        = {2},
  number       = {2},
  pages        = {297-309},
  shortjournal = {IET Image Process.},
  title        = {Blur parameter locus curve and its applications},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Effective hybrid genetic algorithm for removing salt and
pepper noise. <em>IETIP</em>, <em>14</em>(2), 289–296. (<a
href="https://doi.org/10.1049/iet-ipr.2019.0566">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study presents a new approach for recovering an image perturbed by salt and pepper noise (SPN) using a hybrid genetic algorithm (HGA) at all densities, called effective HGA (EHGA). The main contribution of the proposed algorithm is combining the genetic algorithm with image denoising methods that are integrated into the population to achieve rapid convergence. The idea is to evolve a group of individuals into a number of iterations using crossover and mutation operators. This approach evolves a set of images rather than a set of parameters from the filters. Experimental results of simulation on different images using peak signal-to-noise ratio, structural similarity index metric, image enhancement factor and Universal Quality Index show that the proposed algorithm outperforms other methods in removing the SPN qualitatively and quantitatively if the noise density is moderate and high. EHGA also preserves important features such as texture and corners of the image.},
  archive      = {J_IETIP},
  author       = {Nail Alaoui and Amel Baha Houda Adamou-Mitiche and Lahcène Mitiche},
  doi          = {10.1049/iet-ipr.2019.0566},
  journal      = {IET Image Processing},
  month        = {2},
  number       = {2},
  pages        = {289-296},
  shortjournal = {IET Image Process.},
  title        = {Effective hybrid genetic algorithm for removing salt and pepper noise},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Assessment of sentinel-2A multispectral image for benthic
habitat composition mapping. <em>IETIP</em>, <em>14</em>(2), 279–288.
(<a href="https://doi.org/10.1049/iet-ipr.2018.6044">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sentinel-2A accuracy for benthic habitat composition mapping was tested and compared to ALOS AVNIR-2. Aerial image acquired using custom-made unmanned aerial vehicle was used to train and validate the model. The mapping was conducted regardless of the benthic class and at individual benthic class. Benthic habitat class spatial distribution was obtained using the combination of image segmentation and classification tree analysis. The aerial image was interpreted based on the percentage of the constructed and non-constructed classes. The constructed class includes coral reefs, dead coral, seagrass, and macroalgae, while non-constructed class covers carbonate sand, rock, and rubble. Sentinel-2A produced higher accuracy (92%) than ALOS AVNIR-2 (78%) for benthic habitat spatial distribution mapping. However, in the empirical modelling of benthic habitat composition, ALOS AVNIR-2 (SE 23–24%) produced slightly better accuracy than Sentinel-2A (SE 23–27%). Several factors affected the low accuracy, which include the sub-pixel mixing of benthic habitat and constructed class, the delay between dates of acquisition, and radiometric quality of the images. Since the fundamental relationship between reflectance value and the percentage of the constructed class has been justified and consistent, given more experiments it has the potential to predict benthic habitat composition with higher accuracy in the future.},
  archive      = {J_IETIP},
  author       = {Pramaditya Wicaksono and Muhammad Afif Fauzan and Septian Galih Widhi Asta},
  doi          = {10.1049/iet-ipr.2018.6044},
  journal      = {IET Image Processing},
  month        = {2},
  number       = {2},
  pages        = {279-288},
  shortjournal = {IET Image Process.},
  title        = {Assessment of sentinel-2A multispectral image for benthic habitat composition mapping},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Heat diffusion embedded level set evolution for infrared
image segmentation. <em>IETIP</em>, <em>14</em>(2), 267–278. (<a
href="https://doi.org/10.1049/iet-ipr.2018.6629">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this study, the authors present a novel level set method for infrared image segmentation. Local region-based models can fit intensity inhomogeneity partly but they are sensitive to local window scale. To deal with it, they embed an heat diffusion process in conventional level set evolution and convert heat to a part of data term in level set energy function. Besides, bias field model can extract the local intensity clustering property of the image. Therefore, the proposed method can deal with the interference of intensity inhomogeneity and complex background if appropriate seeded pixels are selected. Finally, the energy functional is minimised by a combinatorial optimal algorithm in a graph model to get a global optimal solution and accelerate the level set evolution implementation. The experiments show that the proposed method is robust to parameter setting, noise, and initial contour position. The comparisons on a large quantity of infrared image datasets with standard level set methods also demonstrate the efficiency of the proposed method.},
  archive      = {J_IETIP},
  author       = {Ziwei Wei and Yulong Qiao and Xiao Lu},
  doi          = {10.1049/iet-ipr.2018.6629},
  journal      = {IET Image Processing},
  month        = {2},
  number       = {2},
  pages        = {267-278},
  shortjournal = {IET Image Process.},
  title        = {Heat diffusion embedded level set evolution for infrared image segmentation},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Training approach using the shallow model and hard triplet
mining for person re-identification. <em>IETIP</em>, <em>14</em>(2),
256–266. (<a href="https://doi.org/10.1049/iet-ipr.2019.0334">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-target tracking in a non-overlapping camera network is an active research field, and one of the important problems in it is the person re-identification problem. In this study, the authors propose an approach to improve the performance of the backbone model in the person re-identification. Their approach focuses on training a fusion model with a shallow model and making hard triplets with relationship matrices quickly and efficiently. The proposed approach is simple, but it improves the performance of the backbone. In addition, the hard triplet mining in their process is much faster than the conventional approach. Experimental evaluation shows that the proposed approach can improve the performances of the backbone model. The proposed approach improves rank-1 and mean average precision (mAP) performance by more than 12.54 and 15.44%, respectively, over the backbone models in the Market1501 and DukeMTMC-reID dataset. The approach also achieves competitive performances compared with state-of-the-art approaches.},
  archive      = {J_IETIP},
  author       = {Hyunguk Choi and Kin Choong Yow and Moongu Jeon},
  doi          = {10.1049/iet-ipr.2019.0334},
  journal      = {IET Image Processing},
  month        = {2},
  number       = {2},
  pages        = {256-266},
  shortjournal = {IET Image Process.},
  title        = {Training approach using the shallow model and hard triplet mining for person re-identification},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Background subtraction in dynamic scenes using the dynamic
principal component analysis. <em>IETIP</em>, <em>14</em>(2), 245–255.
(<a href="https://doi.org/10.1049/iet-ipr.2018.6095">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study presents a foreground detection method capable of robustly estimating the background under the presence of dynamic effects. The key contribution of this study is the use of the dynamic principal component analysis to model the serial correlation between successive frames and construct a robust pixel-based background model. The frames are normalised in hue, saturation and value colour space to reduce the effect of illumination changes. To restrict the background model, kernel density estimation is used to identify the distribution of the background time-lagged data matrix and then confidence interval limits are used to determine the corresponding detection thresholds. The foreground is detected using background subtraction. This method is tested on several common sequences such as CDnet 2014, ETSI 2014 and MULTIVISION 2013. The authors also hold comparisons based on quantitative metrics with several state-of-the-art methods. Experimental results show that their method outperforms some state-of-the-art methods and has comparable performance with some depth-based methods.},
  archive      = {J_IETIP},
  author       = {Achraf Djerida and Zhonghua Zhao and Jiankang Zhao},
  doi          = {10.1049/iet-ipr.2018.6095},
  journal      = {IET Image Processing},
  month        = {2},
  number       = {2},
  pages        = {245-255},
  shortjournal = {IET Image Process.},
  title        = {Background subtraction in dynamic scenes using the dynamic principal component analysis},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Real-time multi-trajectory matching for dynamic hand gesture
recognition. <em>IETIP</em>, <em>14</em>(2), 236–244. (<a
href="https://doi.org/10.1049/iet-ipr.2019.1068">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Focus on the field of dynamic gesture recognition, there is a problem of the action, is difficult to recognise when multiple fingertips move in a small range (rotation, grabbing), the authors proposed a method to get the recognition results with high robustness in real-time. Firstly, they proposed the concavity kernel accumulation algorithm (CKA) to cluster corners in an image. Secondly, they deem CKA as a region proposal generator and combined it with convolutional neural network to detect fingertips. Thirdly, they proposed the global nearest neighbour point matching algorithm to match fingertips from two frames. Finally, the long short-term memory is used in multi-trajectory recognition to get the results of gesture recognition. Experiments show that their method could recognise multi-trajectory gestures accurately, furthermore, it can run in real time (20 FPS) without graphics processing unit (GPU).},
  archive      = {J_IETIP},
  author       = {Chengfeng Jian and Junjie Li},
  doi          = {10.1049/iet-ipr.2019.1068},
  journal      = {IET Image Processing},
  month        = {2},
  number       = {2},
  pages        = {236-244},
  shortjournal = {IET Image Process.},
  title        = {Real-time multi-trajectory matching for dynamic hand gesture recognition},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Research on fingerprint classification based on twin support
vector machine. <em>IETIP</em>, <em>14</em>(2), 231–235. (<a
href="https://doi.org/10.1049/iet-ipr.2018.5977">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fingerprint classification is one of the core steps of fingerprint recognition and directly relates to the accuracy of recognition. For this reason, a fingerprint classification method based on Twin Support Vector Machine (TWSVM) is studied. First, the Gabor filter is used to extract texture features from fingerprint images. Second, a multi-class model based on TWSVM is constructed by using the ‘one-versus-all’ strategy and the binary tree method, respectively. The quantum particle swarm optimisation algorithm is used to optimise the parameters in the model. Then the fingerprints are divided into five categories using the optimised model. Finally, the classification model is evaluated using fingerprint images from the NIST-4 database. The experimental results show that applying the TWSVM to fingerprint classification can get good classification results.},
  archive      = {J_IETIP},
  author       = {Shifei Ding and Songhui Shi and Weikuan Jia},
  doi          = {10.1049/iet-ipr.2018.5977},
  journal      = {IET Image Processing},
  month        = {2},
  number       = {2},
  pages        = {231-235},
  shortjournal = {IET Image Process.},
  title        = {Research on fingerprint classification based on twin support vector machine},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Hyperspectral image restoration by subspace representation
with low-rank constraint and spatial-spectral total variation.
<em>IETIP</em>, <em>14</em>(2), 220–230. (<a
href="https://doi.org/10.1049/iet-ipr.2019.0803">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hyperspectral images (HSIs) restoration is an important preprocessing step. The spectral vectors in HSI can be separated into different classification based on the land-covers, which means the spectral space can be regarded as the union of several low-rank subspaces. Subspace low-rank representation (SLRR) is powerful in exploring the inner low-rank structure and has been applied for HSI restoration. However, the traditional SLRR only seek for the rank-minimum representation under a given dictionary, which may treat the structured sparse noise as inherent low-rank components. In addition, the SLRR framework cannot make full use of the spatial information. In this study, a framework named subspace representation with low-rank constraint and spatial-spectral total variation is proposed for HSI restoration. In which, an artificial rank constraint is introduced to control the rank of the representation result, which can improve the removal of the structured sparse noise and exploit the intrinsic structure of spectral space more effectively. Meanwhile, the spatial-spectral total variation regularisation is applied to enhance the spatial and spectral smoothness. Several experiments conducted in simulated and real HSI datasets demonstrate that the proposed method can achieve a state-of-the-art performance both in visual quality and quantitative assessments.},
  archive      = {J_IETIP},
  author       = {Jun Ye and Xian Zhang},
  doi          = {10.1049/iet-ipr.2019.0803},
  journal      = {IET Image Processing},
  month        = {2},
  number       = {2},
  pages        = {220-230},
  shortjournal = {IET Image Process.},
  title        = {Hyperspectral image restoration by subspace representation with low-rank constraint and spatial-spectral total variation},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Backlit images enhancement using global tone mappings and
image fusion. <em>IETIP</em>, <em>14</em>(2), 211–219. (<a
href="https://doi.org/10.1049/iet-ipr.2019.0814">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The authors present a method for the enhancement of backlit images, i.e. images in which the main source of light is behind the photography subject. These images contain, simultaneously, very dark and very bright regions. In this situation, a single tone mapping function is unable to enhance the whole image. They propose the use of several such tone mappings, some of them enhancing the dark regions while others enhancing the bright regions, and then the combination of all these results using an image fusion algorithm. Qualitative and quantitative results confirm the validity of the proposed method.},
  archive      = {J_IETIP},
  author       = {Antoni Buades and Jose-Luis Lisani and Ana Belen Petro and Catalina Sbert},
  doi          = {10.1049/iet-ipr.2019.0814},
  journal      = {IET Image Processing},
  month        = {2},
  number       = {2},
  pages        = {211-219},
  shortjournal = {IET Image Process.},
  title        = {Backlit images enhancement using global tone mappings and image fusion},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Optimisation of linear dependence energy for object
co-segmentation in a set of images with heterogeneous contents.
<em>IETIP</em>, <em>14</em>(1), 201–210. (<a
href="https://doi.org/10.1049/iet-ipr.2018.5176">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This work proposes a framework for simultaneously segmenting foreground objects in a collection of images having heterogeneous contents. Rather than resorting to image co-segmentation to segment similar objects in multiple images, which requires the use of categorised images, the authors’ idea disseminates segmentation information within images. In this way, it becomes easier to detect foreground objects in all of them simultaneously, mainly under the hypothesis of using similar or different images. General information is aggregated, on foregrounds as well as on backgrounds, from a set of images for joint segmentation of category-independent objects. The key idea is to estimate the linear dependence of the foreground histograms of the input images to optimise a Markov random field-based energy function. Iterative optimisation of each image permits after that the enhancement of the final segmentation results. Extensive experiments demonstrate that the proposed method (PM) enables full-object segmentation of foreground objects within a collection of images composed of different classes. Indeed, the validation of the accuracy on five challenging datasets (iCoseg, Oxford Flowers, MicroSoft Research Cambridge (MSRC), Caltech101 and Berkeley) shows that the PM achieves satisfactory results as compared with state-of-the-art methods. Besides, it has the challenging ability to efficiently deal with uncategorised objects.},
  archive      = {J_IETIP},
  author       = {Hager Merdassi and Walid Barhoumi and Ezzeddine Zagrouba},
  doi          = {10.1049/iet-ipr.2018.5176},
  journal      = {IET Image Processing},
  month        = {1},
  number       = {1},
  pages        = {201-210},
  shortjournal = {IET Image Process.},
  title        = {Optimisation of linear dependence energy for object co-segmentation in a set of images with heterogeneous contents},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). DRU-net: A novel u-net for biomedical image segmentation.
<em>IETIP</em>, <em>14</em>(1), 192–200. (<a
href="https://doi.org/10.1049/iet-ipr.2019.0025">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the wide applications of biomedical images in the medical field, the segmentation of biomedical images plays an important role in clinical diagnosis, pathological analysis, and medical intervention. Full convolutional neural networks, especially U-net, have improved the performance of segmentation greatly in recent years. However, due to their regular geometric structure, the standard convolutions that they use are inherently limited in dealing with geometric transformations while biomedical objects have huge variations in shape and size. In this study, the authors propose the DRU-net, which is a novel U-net with deformable encoder and reshaping upsampling convolution decoder, for biomedical image segmentation. First, deformable convolutional networks are applied and improved to enhance the learning ability of the encoder for geometric transformations. Second, a novel upsampling method named reshape upsampling convolution is proposed for better-restoring resolution and fusion features. Furthermore, focal loss is used to address class imbalance and model overwhelmed problems in biomedical image segmentation tasks. Theoretic analysis and experimental results have shown that the proposed algorithm not only reduces the number of parameters of U-Net, but also achieves produces competitive results compared with the state-of-the-art algorithms in terms of various quantitative measures on Drosophila electron microscopy dataset and Warwick-QU dataset.},
  archive      = {J_IETIP},
  author       = {Xuegang Hu and Hongguang Yang},
  doi          = {10.1049/iet-ipr.2019.0025},
  journal      = {IET Image Processing},
  month        = {1},
  number       = {1},
  pages        = {192-200},
  shortjournal = {IET Image Process.},
  title        = {DRU-net: A novel U-net for biomedical image segmentation},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Level set based shape prior and deep learning for image
segmentation. <em>IETIP</em>, <em>14</em>(1), 183–191. (<a
href="https://doi.org/10.1049/iet-ipr.2018.6622">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep convolutional neural network can effectively extract hidden patterns in images and learn realistic image priors from the training set. And fully convolutional networks (FCNs) have achieved state-of-the-art performance in the image segmentation. However, these methods have the disadvantages of noise, boundary roughness and no prior shape. Therefore, this study proposes a level set with the deep prior method for the image segmentation based on the priors learned by FCNs. The FCNs can learn high-level semantic patterns from the training set. Also, the output of the FCNs represents the high-level semantic information as a probability map and the global affine transformation can obtain the optimal affine transformation of the intrinsic prior shape. Moreover, the improved level set method integrates the information of the original image, the probability map and the corrected prior shape to achieve the image segmentation. Compared with the traditional level set method of simple scenes, the proposed method solves the disadvantage of FCNs by using the high-level semantic information to segment images of complex scenes. Finally, Portrait data set are used to verify the effectiveness of the proposed method. The experimental results show that the proposed method can obtain more accurate segmentation results than the traditional FCNs.},
  archive      = {J_IETIP},
  author       = {Yongming Han and Shuheng Zhang and Zhiqing Geng and Qin Wei and Zhi Ouyang},
  doi          = {10.1049/iet-ipr.2018.6622},
  journal      = {IET Image Processing},
  month        = {1},
  number       = {1},
  pages        = {183-191},
  shortjournal = {IET Image Process.},
  title        = {Level set based shape prior and deep learning for image segmentation},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Fusing HOG and convolutional neural network spatial–temporal
features for video-based facial expression recognition. <em>IETIP</em>,
<em>14</em>(1), 176–182. (<a
href="https://doi.org/10.1049/iet-ipr.2019.0293">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Video-based facial expression recognition (VFER) is the fundamental feature of various computer vision applications. Visual features are the key factors for facial expression recognition. However, the gap between the visual features and the emotions is large. In order to bridge the gap, the proposed method utilises convolutional neural networks (CNNs) and histogram of oriented gradient (HOG) to obtain the more comprehensive feature for VFER. Firstly, it extracts shallow features from the video frame through a number of convolutional kernels in CNNs, which has the characteristics of displacement, scale and deformation invariance. Then, the HOG is employed to extract HOG features from CNN&#39;s shallow features, which are strongly correlated with facial expressions. Finally, the support vector machine (SVM) is employed to conduct the task of facial expression recognition. The extensive experiments on RML, CK+ and AFEW5.0 database show that this framework takes on the promising performance and outperforming the state of the arts.},
  archive      = {J_IETIP},
  author       = {Xianzhang Pan},
  doi          = {10.1049/iet-ipr.2019.0293},
  journal      = {IET Image Processing},
  month        = {1},
  number       = {1},
  pages        = {176-182},
  shortjournal = {IET Image Process.},
  title        = {Fusing HOG and convolutional neural network spatial–temporal features for video-based facial expression recognition},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Novel image restoration method based on multi-frame
super-resolution for atmospherically distorted images. <em>IETIP</em>,
<em>14</em>(1), 168–175. (<a
href="https://doi.org/10.1049/iet-ipr.2019.0319">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this study, the authors propose a novel multi-frame super-resolution method using frame selection and multiple fusions for atmospherically distorted, zoomed-in, image-quality enhancement. When a small part of the image captured by placing a target several kilometres away from the fixed camera is enlarged, the quality of the part becomes poor owing to low resolution, spatial deformations and noise that are mainly caused by long distance and atmospheric turbulence. Thus, the authors propose an adaptive frame selection method that selects only a few frames with small blur based on the corresponding images with relatively clear edges. Further, they propose multiple fusion schemes to reconstruct the selected frames, thereby suppressing the influence of deformation. By converting all the frames into high-resolution based on each frame and integrating them, deformation and noise are effectively removed without high computation cost using the multiple fusion scheme. The proposed method, which enhances the quality of atmospherically distorted zoomed-in images, exhibits superior performance than the state-of-the-art image super-resolution methods with regard to high accuracy, efficiency and ease of implementation, ensuring that the proposed method is suitable for enhancing the quality of an image captured using a general digital camera or a smartphone.},
  archive      = {J_IETIP},
  author       = {Yinhao Li and Katsuhisa Ogawa and Yutaro Iwamoto and Yen-Wei Chen},
  doi          = {10.1049/iet-ipr.2019.0319},
  journal      = {IET Image Processing},
  month        = {1},
  number       = {1},
  pages        = {168-175},
  shortjournal = {IET Image Process.},
  title        = {Novel image restoration method based on multi-frame super-resolution for atmospherically distorted images},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Correction of complex purple fringing by green-channel
compensation and local luminance adaptation. <em>IETIP</em>,
<em>14</em>(1), 154–167. (<a
href="https://doi.org/10.1049/iet-ipr.2019.0732">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In natural photography, defects in camera imaging pipeline often result in some form of colour noise or distortion. Nature of this distortion is generally intertwined with scene dependent variables such as positioning, intensity and composition of light source and local object colour reflectivity. One such defect is called purple fringing aberration (PFA). PFA problems are of two types, one of which corresponds to a localised fringing effect near high contrast zones (termed as Isolated PFA or IS-PFA) and the second which corresponds to a widespread semi-transparent purple haze over a large part of natural scene (termed as complex PFA or C-PFA). Much of the PFA-correction solutions have been driven towards IS-PFA and very little towards C-PFA. Based on a premise that in C-PFA, green channel is heavily suppressed and noisy, while colour information in red and blue channels are largely conserved, authors propose a green-channel compensation algorithm for restoring true natural colours in fringe affected region. To correct white-tuft produced by proposed compensation algorithm, they also devise a suitable localised luminance adaptation procedure to equalise perceived changes in luminance profile. Comparisons with state-of-the-art methods devised to combat this purple haze effect yield promising results for a majority of test cases.},
  archive      = {J_IETIP},
  author       = {Parveen Malik and Kannan Karthik},
  doi          = {10.1049/iet-ipr.2019.0732},
  journal      = {IET Image Processing},
  month        = {1},
  number       = {1},
  pages        = {154-167},
  shortjournal = {IET Image Process.},
  title        = {Correction of complex purple fringing by green-channel compensation and local luminance adaptation},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Integrated system for automatic detection of representative
video frames in wireless capsule endoscopy using adaptive sliding window
singular value decomposition. <em>IETIP</em>, <em>14</em>(1), 147–153.
(<a href="https://doi.org/10.1049/iet-ipr.2019.0251">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Wireless capsule endoscopy (WCE) is a non-invasive diagnosis method that allows recording a video as the capsule travels through the gastrointestinal (GI) tract. The practical drawback is producing a long clinical video in which the review process by an experienced specialist is tedious. Automated summarisation methods can reduce the evaluation time by experts as well as errors in manual interpretation. The proposed approach consists of three main steps as follows: First, an adaptive sliding window singular value decomposition is employed to extract representative video frames. Then, adaptive contrast diffusion is utilised to increase the visibility of WCE frames. At the end stage, a novel knowledge-based method is developed to segment video frames into four topographic zones of GI tract, which are oesophagus, stomach, small intestine and large intestine. The authors have evaluated the proposed framework in the presence of 30 local datasets as well as publicly available KID database. The average recall and precision were estimated by 0.86 and 0.83, and by 0.82 and 0.83 for KID database, respectively. Their results reveal that significant reduction in the review time is feasible using the proposed technique. Quantitative results of summarisation show that the proposed method is more effective than three methods in the literature.},
  archive      = {J_IETIP},
  author       = {Abbas Biniaz and Fatemeh Abdolali and Reza Aghaeizadeh Zoroofi},
  doi          = {10.1049/iet-ipr.2019.0251},
  journal      = {IET Image Processing},
  month        = {1},
  number       = {1},
  pages        = {147-153},
  shortjournal = {IET Image Process.},
  title        = {Integrated system for automatic detection of representative video frames in wireless capsule endoscopy using adaptive sliding window singular value decomposition},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Image retrieval based on ASIFT features in a hadoop
clustered system. <em>IETIP</em>, <em>14</em>(1), 138–146. (<a
href="https://doi.org/10.1049/iet-ipr.2019.0229">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For image matching, the scale invariant feature transform (SIFT) algorithm is a commonly used one. They are invariant to image rotation, scale zooming, and partially invariant to change in illumination and 3D camera viewpoint. Affine SIFT (ASIFT) is an extension of SIFT, which solves the problem when images are captured at different angles. However, ASIFT has higher computational complexity than SIFT, due to a huge amount of features in the images. Therefore, in this study, a Hadoop-based image retrieval system is proposed to solve the ASIFT shortcomings of high computation by the MapReduce technology. The system uses a combination of the Bag-of-Words method and support vector machine. Finally, the experimental results verify that the proposed method is more effective than the other state-of-the-art methods for a variety of datasets.},
  archive      = {J_IETIP},
  author       = {Yin-Fu Huang and Huan-Yu Wu},
  doi          = {10.1049/iet-ipr.2019.0229},
  journal      = {IET Image Processing},
  month        = {1},
  number       = {1},
  pages        = {138-146},
  shortjournal = {IET Image Process.},
  title        = {Image retrieval based on ASIFT features in a hadoop clustered system},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Automatic detection of acute lymphoblastic leukaemia based
on extending the multifractal features. <em>IETIP</em>, <em>14</em>(1),
132–137. (<a href="https://doi.org/10.1049/iet-ipr.2018.5910">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The main purpose of this study is to introduce a new species of features to improve the diagnosis efficiency of acute lymphoblastic leukaemia from microscopic images. First, the authors segmented nuclei by the k -means and watershed algorithms. They extracted three sets of geometrical, statistical, and chaotic features from nuclei images. Six chaotic features were extracted by calculating the fractal dimension from five sub-images driven from the nuclei images, with their grey levels being modified. The authors classified the images into binary and multiclass types via the support vector machine algorithm. They conducted principal component analysis for dimensional reduction of feature space and then evaluated the proposed algorithm for the overfitting problem. The obtained overall results represent 99% accuracy, 99% specificity, and 97% sensitivity values in the classification of six-cell groups. The difference between the train and test errors was &lt;3%, which proves that the classification performance had improved by using the multifractal features.},
  archive      = {J_IETIP},
  author       = {Mohamadreza Abbasi and Saeed Kermani and Ardeshir Tajebib and Morteza Moradi Amin and Manije Abbasi},
  doi          = {10.1049/iet-ipr.2018.5910},
  journal      = {IET Image Processing},
  month        = {1},
  number       = {1},
  pages        = {132-137},
  shortjournal = {IET Image Process.},
  title        = {Automatic detection of acute lymphoblastic leukaemia based on extending the multifractal features},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Higher precision range estimation for context-based adaptive
binary arithmetic coding. <em>IETIP</em>, <em>14</em>(1), 125–131. (<a
href="https://doi.org/10.1049/iet-ipr.2018.6602">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Lagrangian rate distortion optimisation is widely employed in modern video encoders, such as high-efficiency video coding (H.265/HEVC). In this work, the authors propose a more accurate context-based adaptive binary arithmetic coding look-up table that can enhance compression quality and provide substantially better accuracy of range estimation, by employing one-more bit with 64 probability states. For the hardware implementation, they propose a higher precision look-up table instead of the HEVC Test Model (HM) standard table. The authors also define a new finite-state machine to handle the probability changing in real-time. The significant BD-RATE gain of the proposed context modelling is up to 6.0% for all-intra mode and 13.0% for inter mode. This finite state machine offers no divergence from the H.265/HEVC standards and can be used in the current systems.},
  archive      = {J_IETIP},
  author       = {Sio-Kei Im and Ka-Hou Chan},
  doi          = {10.1049/iet-ipr.2018.6602},
  journal      = {IET Image Processing},
  month        = {1},
  number       = {1},
  pages        = {125-131},
  shortjournal = {IET Image Process.},
  title        = {Higher precision range estimation for context-based adaptive binary arithmetic coding},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Low-rank tensor completion for visual data recovery via the
tensor train rank-1 decomposition. <em>IETIP</em>, <em>14</em>(1),
114–124. (<a href="https://doi.org/10.1049/iet-ipr.2018.6594">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this study, the authors study the problem of tensor completion, in particular for three-dimensional arrays such as visual data. Previous works have shown that the low-rank constraint can produce impressive performances for tensor completion. These works are often solved by means of Tucker rank. However, Tucker rank does not capture the intrinsic correlation of the tensor entries. Therefore, the authors propose a new proximal operator for the approximation of tensor nuclear norms based on tensor-train rank-1 decomposition via the singular value decomposition. The proximal operator will perform a soft-thresholding operation on tensor singular values. In addition, the low-rank constraint can capture the global structure of data well, but it does not exploit local smooth of visual data. Therefore, they integrate total variation as a regularisation term into low-rank tensor completion. Finally, they use a primal–dual splitting to achieve optimisation. Experimental results have shown that the proposed method, can preserve the multi-dimensional nature inherent in the data, and thus provide superior results over many state-of-the-art tensor completion techniques.},
  archive      = {J_IETIP},
  author       = {Xiaohua Liu and Xiao-Yuan Jing and Guijin Tang and Fei Wu and Xiwei Dong},
  doi          = {10.1049/iet-ipr.2018.6594},
  journal      = {IET Image Processing},
  month        = {1},
  number       = {1},
  pages        = {114-124},
  shortjournal = {IET Image Process.},
  title        = {Low-rank tensor completion for visual data recovery via the tensor train rank-1 decomposition},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). XNORCONV: CNNs accelerator implemented on FPGA using a
hybrid CNNs structure and an inter-layer pipeline method.
<em>IETIP</em>, <em>14</em>(1), 105–113. (<a
href="https://doi.org/10.1049/iet-ipr.2019.0385">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nowadays, convolutional neural networks (CNNs) have become a research hotspot because of their high performance in computer vision and pattern recognition. However, as the high energy consumption of traditional graphic processing units-based CNNs, it is difficult to deploy them into portable devices. To deal with this problem, a hybrid CNN structure (XNORCONV) was proposed and implemented on field-programmable gate array (FPGA) in this study. Two improvements have been applied in XNORCONV. Firstly, the multiplications in the convolutional layer (CONV) were replaced by XNOR operations to save the multiplier and reduce computational complexity. Secondly, an inter-layer pipeline was designed to further accelerate the calculation. XNORCONV was implemented on Xilinx Zynq-7000 xc7z020clg400-1 under the clock frequency of 150 MHz and tested with MNIST dataset. The results of the experiment show that XNORCONV can classify each picture from MNIST in , and achieve 98.4% recognition accuracy. Compared with traditional Lenet-5 on different platforms, XNORCONV reduced multiplication by 85.6% with only 0.4% accuracy loss.},
  archive      = {J_IETIP},
  author       = {Lin Zhang and Xiaokang Bu and Bing Li},
  doi          = {10.1049/iet-ipr.2019.0385},
  journal      = {IET Image Processing},
  month        = {1},
  number       = {1},
  pages        = {105-113},
  shortjournal = {IET Image Process.},
  title        = {XNORCONV: CNNs accelerator implemented on FPGA using a hybrid CNNs structure and an inter-layer pipeline method},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Regularised IHS-based pan-sharpening approach using spectral
consistency constraint and total variation. <em>IETIP</em>,
<em>14</em>(1), 94–104. (<a
href="https://doi.org/10.1049/iet-ipr.2019.0283">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this study, the authors address the fusion of low-resolution multi-spectral image with the corresponding high-resolution panchromatic image to provide high-resolution multi-spectral (HRM) one, i.e. pan sharpening. The intensity–hue–saturation (IHS)-based pan-sharpening methods are popular because they are simple, efficient, and of high-spatial quality. However, their frameworks are unavoidably subject to spectral distortion. To reduce the inevitable spectral distortion of IHS-based pan-sharpening approaches, the spectral consistency constraint is used in the proposed method. Moreover, to stabilise fusion results obtained from the ill-posed pan-sharpening problem and to keep the smoothness of the HRM image, a total variation regularisation term is considered. These considerations are formulated in a non-quadratic optimisation problem. To solve this problem, a kind of variable splitting method, known as half-quadratic approximation is utilised, and also an alternating optimisation procedure is used to reconstruct HRM image. To gain convenient control on the local spectral and the spatial information, and also to reduce the required memory, in the optimisation stage, the patch-based strategy is employed. The proposed method was tested on two datasets acquired by GeoEye-1 and Pleiades satellites. To evaluate the proposed method, visual assessment, as well as quantitative comparison with different pan-sharpening methods, was carried out.},
  archive      = {J_IETIP},
  author       = {Mohammad Khateri and Fahim Shabanzade and Fardin Mirzapour},
  doi          = {10.1049/iet-ipr.2019.0283},
  journal      = {IET Image Processing},
  month        = {1},
  number       = {1},
  pages        = {94-104},
  shortjournal = {IET Image Process.},
  title        = {Regularised IHS-based pan-sharpening approach using spectral consistency constraint and total variation},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Gradient-based kernel selection technique for tumour
detection and extraction of medical images using graph cut.
<em>IETIP</em>, <em>14</em>(1), 84–93. (<a
href="https://doi.org/10.1049/iet-ipr.2018.6615">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Magnetic resonance imaging is a powerful, ubiquitous imaging technique that provides detailed high-contrast images differentiating soft tissues. The low radio-frequency bias field creates intensity inhomogeneity generating low contrast that often creates difficulty for quantitative and qualitative analyses. Segmentation aids in analysis of changes occurring in brain, where bias effect severely affects performance. The graph-cut (GC) segmentation provides supervised computer-assisted diagnosis and treatment. GC&#39;s interactive nature requires manual selection of kernels for initialisation. The shrinkage behaviour of GC creates inaccurate and fallacious extraction. On the basis of these problems, this study proposes gradient-based kernel selection GC method that simultaneously removes shrinkage problem and locates tumour in image, eliminating human interaction with accurate segmentation for even bias field images. The proposed method addresses these problems by emphasising on directive inclination of intensity scales of symmetrical halves of images. The proposed method is evaluated for high-grade glioma and low-grade glioma images with and without bias field. The average performance metrics evaluated for these images depict remarkable improvement in comparison with existing techniques. The proposed technique is validated by applying on real-time dataset of tumour images obtained from State Government Hospital, Shimla, India.},
  archive      = {J_IETIP},
  author       = {Jyotsna Dogra and Shruti Jain and Meenakshi Sood},
  doi          = {10.1049/iet-ipr.2018.6615},
  journal      = {IET Image Processing},
  month        = {1},
  number       = {1},
  pages        = {84-93},
  shortjournal = {IET Image Process.},
  title        = {Gradient-based kernel selection technique for tumour detection and extraction of medical images using graph cut},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Automatic rectification of warped bangla document images.
<em>IETIP</em>, <em>14</em>(1), 74–83. (<a
href="https://doi.org/10.1049/iet-ipr.2019.0831">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this study, a robust algorithm for dewarping of camera-captured document images, mainly in Bangla script, is proposed. The algorithm can handle various types of warped document images and they are generated due to different types of document surfaces (convex, concave or multi-folded). The proposed algorithm is independent of font type, font size, font style and camera view angle. After initial preprocessing, the method first demarcates the text lines present in the document image. Then, the headline ( shirorekha ) position of each text line is estimated. Based on the headline position and shape, each text line is dewarped. If the document is highly warped, distorted text (e.g. thinner and shorter characters) is generated after dewarping. Special care has been taken to minimise this distortion based on most undistorted character information. Exhaustive testing shows the robustness and shape improvement of the proposed algorithm. Finally, for shape quality evaluation, some new measures are defined.},
  archive      = {J_IETIP},
  author       = {Arpan Garai and Samit Biswas and Sekhar Mandal and Bidyut B. Chaudhuri},
  doi          = {10.1049/iet-ipr.2019.0831},
  journal      = {IET Image Processing},
  month        = {1},
  number       = {1},
  pages        = {74-83},
  shortjournal = {IET Image Process.},
  title        = {Automatic rectification of warped bangla document images},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Automatic optical-to-SAR image registration using a
structural descriptor. <em>IETIP</em>, <em>14</em>(1), 62–73. (<a
href="https://doi.org/10.1049/iet-ipr.2019.0389">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Optical-to-synthetic aperture radar (SAR) image registration is a challenging task in remote sensing as the images have significant non-linear intensity variations as well as large geometric differences. Moreover, the influence of speckle noise in the SAR image further affects the registration result. The structural descriptors are very effective to handle the non-linear intensity variations between optical and SAR images. Although a number of optical-to-SAR image registration methods have been proposed in the past few years based on the structural information of image, most of them are ineffective for the images having large geometric differences. To address these problems, a novel optical-to-SAR image registration algorithm is proposed by using a new structural descriptor. Initially, the corner features are extracted from the optical and SAR images. Then, the proposed structural descriptor is constructed for the extracted features. Finally, feature matching is performed between the optical and SAR images and the correct match are identified. The proposed method is very effective to register the optical and SAR images having significant intensity variations and large geometric differences. It can increase the number of correct match between the images. Experiments on six sets of optical and SAR image pairs demonstrate the effectiveness of the proposed method.},
  archive      = {J_IETIP},
  author       = {Sourabh Paul and Umesh C. Pati},
  doi          = {10.1049/iet-ipr.2019.0389},
  journal      = {IET Image Processing},
  month        = {1},
  number       = {1},
  pages        = {62-73},
  shortjournal = {IET Image Process.},
  title        = {Automatic optical-to-SAR image registration using a structural descriptor},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Real-time least-squares ensemble visual tracking.
<em>IETIP</em>, <em>14</em>(1), 53–61. (<a
href="https://doi.org/10.1049/iet-ipr.2018.6037">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this study, the authors present a novel ensemble tracking system by formulating the tracking task in terms of a linear regression which is a least-squares problem. A set of weak classifiers are trained using least squares which are solved efficiently using the Moore–Penrose inverse. Then, these weak classifiers are combined into a strong classifier using bagging. The strong classifier is used to recognise the target and locate its position, which is obtained efficiently in the Fourier domain. For obtaining a good ensemble, a novel sampling strategy is proposed to train accurate and diverse weak classifiers. By exploiting historical targets to monitor the training process, pose change and occlusion are well-handled. The proposed method is extensively evaluated using a variety of evaluation protocols on the recent standard datasets including OTB50, OTB100 and VOT2016. Experimental results show that the proposed methodology performs favourably against state-of-the-art methods in terms of efficiency, accuracy and robustness.},
  archive      = {J_IETIP},
  author       = {Ridong Zhu and Xiaoyuan Yang and Jingkai Wang and Zhengze Li},
  doi          = {10.1049/iet-ipr.2018.6037},
  journal      = {IET Image Processing},
  month        = {1},
  number       = {1},
  pages        = {53-61},
  shortjournal = {IET Image Process.},
  title        = {Real-time least-squares ensemble visual tracking},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Colour image encryption scheme based on enhanced quadratic
chaotic map. <em>IETIP</em>, <em>14</em>(1), 40–52. (<a
href="https://doi.org/10.1049/iet-ipr.2019.0123">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this study, an enhanced quadratic map (EQM) is proposed and has been applied in a new colour image encryption scheme. The performance evaluations show that the EQM has excellent performances such as better Lyapunov exponent and larger chaotic ranges when compared with the classical quadratic map. The sequences generated from this EQM are successfully used in a new proposed colour image encryption scheme with excellent confusion and diffusion properties. The encryption structure is based on the permutation–diffusion process, and then adopted on the classical permutation, it is characterised by a high speed of diffusion, which enables the encryption of the three components of the plaintext image at the same time, and these encrypted components are simultaneously related to each other. The proposed scheme is tested on the USC-SIPI image dataset and on the real-life image dataset; its effectiveness is also compared with five latterly proposed image encryption schemes. The simulation results indicate that the proposed scheme has the properties of large key space, a weaker correlation between neighbouring pixels, higher sensitivity towards key, greater randomness of pixels and the capacity to withstand statistical analysis, plaintext/chosen-plaintext attacks, and differential attacks, thus that it has higher security and can be appropriate for image encryption.},
  archive      = {J_IETIP},
  author       = {Djamel Herbadji and Aissa Belmeguenai and Nadir Derouiche and Hongjung Liu},
  doi          = {10.1049/iet-ipr.2019.0123},
  journal      = {IET Image Processing},
  month        = {1},
  number       = {1},
  pages        = {40-52},
  shortjournal = {IET Image Process.},
  title        = {Colour image encryption scheme based on enhanced quadratic chaotic map},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Image sorting via a reduction in travelling salesman
problem. <em>IETIP</em>, <em>14</em>(1), 31–39. (<a
href="https://doi.org/10.1049/iet-ipr.2018.5880">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The authors define and approximately solve the problem of unsupervised image sorting that is considered as a kind of content-based image clustering. The content-based image sorting is the creation of a route that passes through all the images once, in such an order that the next one from the previous image has similar content. In the end, an image ordering (e.g. slideshow) is automatically produced, so that the images with similar content should be close to each other. This problem resembles the problem known in the literature as ‘travelling salesman problem’ (TSP). In this work, the authors have proposed two classes of methods (the nearest-neighbour and genetic methods) that have also been applied on the TSP problem. Their benefits on computational efficiency and accuracy are discussed over six datasets that have been created from the GHIM-10K dataset. The experimental results demonstrate that the proposed methods efficiently solve the image sorting problem, producing image sequences that almost agree with human intuition.},
  archive      = {J_IETIP},
  author       = {Smaragda Markaki and Costas Panagiotakis and Dimitra Lasthiotaki},
  doi          = {10.1049/iet-ipr.2018.5880},
  journal      = {IET Image Processing},
  month        = {1},
  number       = {1},
  pages        = {31-39},
  shortjournal = {IET Image Process.},
  title        = {Image sorting via a reduction in travelling salesman problem},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Binary tomography on the isometric tessellation involving
pixel shape orientation. <em>IETIP</em>, <em>14</em>(1), 25–30. (<a
href="https://doi.org/10.1049/iet-ipr.2019.0099">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this study, a tomography reconstruction problem of binary images is considered on the isometric grid. On this grid, the triangle pixels have two types of orientations, accordingly, the authors call them delta or nabla shape pixels. The proposed reconstruction method uses data of projections of three natural directions. They are the lane directions of the triangular tessellation (these directions are somewhat analogous to row/column directions on the rectangular grids). The projection ray, penetrating through a grid lane, now not passing through the middle of pixels (i.e. through the middle line of triangle shape pixels), as usually taken, but little bit shifted from the middle parallel to the lane. This method provides the exact information about the number of nabla and delta shape triangle pixels in each lane of the image. This additional information is included in the reconstruction process to improve the quality of reconstruction. They formulate the suggested model into an energy-minimisation problem and apply a gradient-based approach for its minimisation. They show and analyse various experimental results on test images. The presented approach shows both better quality reconstructions and shorter running time than the earlier approaches.},
  archive      = {J_IETIP},
  author       = {Benedek Nagy and Tibor Lukić},
  doi          = {10.1049/iet-ipr.2019.0099},
  journal      = {IET Image Processing},
  month        = {1},
  number       = {1},
  pages        = {25-30},
  shortjournal = {IET Image Process.},
  title        = {Binary tomography on the isometric tessellation involving pixel shape orientation},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Image-independent optimal non-negative integer bit
allocation technique for the DCT-based image transform coders.
<em>IETIP</em>, <em>14</em>(1), 11–24. (<a
href="https://doi.org/10.1049/iet-ipr.2018.6302">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The optimum non-negative integer bit allocation (ONIBA) is an important technique, which provides optimal quantisation of transform coefficients for the image transform coders (ITCs). However, the existing ONIBA algorithms are still not popular for the discrete cosine transform (DCT)-based ITCs, due to their image-dependent nature and additional side information requirements. Therefore, this study presents a novel image-independent ONIBA (IIONIBA) technique to achieve efficient quantisation for the DCT-ITCs. For the development of the proposed IIONIBA technique, initially, an image-dependent ONIBA algorithm is proposed, which is then mapped into desired image-independent solution via utilisation of a prepared combined image and proposed modified step size mapping technique. Thereafter, a new lookup table for the elements of quantisation tables, obtained from the proposed IIONIBA technique, is established using non-linear regression analysis, to reduce the problem of additional side information requirements. Several experiments are performed to evaluate the performance of the proposed IIONIBA technique based on the visual quality assessment of reconstructed images and the image quality indexes peak signal to noise ratio (PSNR) and mean structural similarity index (MSSIM). The results show that the proposed IIONIBA technique delivers better quantisation and provides significant gains in the image quality indexes as compared to the recent quantisation techniques.},
  archive      = {J_IETIP},
  author       = {Vikrant Singh Thakur and Kavita Thakur and Shubhrata Gupta and Kamisetty R. Rao},
  doi          = {10.1049/iet-ipr.2018.6302},
  journal      = {IET Image Processing},
  month        = {1},
  number       = {1},
  pages        = {11-24},
  shortjournal = {IET Image Process.},
  title        = {Image-independent optimal non-negative integer bit allocation technique for the DCT-based image transform coders},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Vehicle detection in intelligent transport system under a
hazy environment: A survey. <em>IETIP</em>, <em>14</em>(1), 1–10. (<a
href="https://doi.org/10.1049/iet-ipr.2018.5351">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Developing an intelligent transportation system has attracted a lot of attention in the recent past. Moreover, with the growing number of vehicles on the road most nations are adopting an intelligent transport system (ITS) for handling issues like traffic flow density, queue length, the average speed of the traffic, and total vehicles passing through a point in a specific time interval and so on. ITS by capturing traffic images and videos through cameras, helps the traffic control centres in monitoring and managing the traffic. Efficient and unfailing vehicle detection is a crucial step for the ITS. This study reviews different techniques and applications used around the world for vehicle detection under various environmental conditions based on video processing systems. This study also discusses the types of cameras used for vehicle detections, and the classification of vehicles for traffic monitoring and controlling. This study finally highlights the problems encountered during surveillance under extreme weather conditions.},
  archive      = {J_IETIP},
  author       = {Agha Asim Husain and Tanmoy Maity and Ravindra Kumar Yadav},
  doi          = {10.1049/iet-ipr.2018.5351},
  journal      = {IET Image Processing},
  month        = {1},
  number       = {1},
  pages        = {1-10},
  shortjournal = {IET Image Process.},
  title        = {Vehicle detection in intelligent transport system under a hazy environment: A survey},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
</ul>

</body>
</html>
