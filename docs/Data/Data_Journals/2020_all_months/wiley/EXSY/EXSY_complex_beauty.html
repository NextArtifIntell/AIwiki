<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>EXSY_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="exsy---118">EXSY - 118</h2>
<ul>
<li><details>
<summary>
(2020). Deep learning techniques for recommender systems based on
collaborative filtering. <em>EXSY</em>, <em>37</em>(6), e12647. (<a
href="https://doi.org/10.1111/exsy.12647">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the Big Data Era, recommender systems perform a fundamental role in data management and information filtering. In this context, Collaborative Filtering (CF) persists as one of the most prominent strategies to effectively deal with large datasets and is capable of offering users interesting content in a recommendation fashion. Nevertheless, it is well-known CF recommenders suffer from data sparsity, mainly in cold-start scenarios, substantially reducing the quality of recommendations. In the vast literature about the aforementioned topic, there are numerous solutions, in which the state-of-the-art contributions are, in some sense, conditioned or associated with traditional CF methods such as Matrix Factorization (MF), that is, they rely on linear optimization procedures to model users and items into low-dimensional embeddings. To overcome the aforementioned challenges, there has been an increasing number of studies exploring deep learning techniques in the CF context for latent factor modelling. In this research, authors conduct a systematic review focusing on state-of-the-art literature on deep learning techniques applied in collaborative filtering recommendation, and also featuring primary studies related to mitigating the cold start problem. Additionally, authors considered the diverse non-linear modelling strategies to deal with rating data and side information, the combination of deep learning techniques with traditional CF-based linear methods, and an overview of the most used public datasets and evaluation metrics concerning CF scenarios.},
  archive      = {J_EXSY},
  author       = {Guilherme Brandão Martins and João Paulo Papa and Hojjat Adeli},
  doi          = {10.1111/exsy.12647},
  journal      = {Expert Systems},
  month        = {12},
  number       = {6},
  pages        = {e12647},
  shortjournal = {Expert Syst.},
  title        = {Deep learning techniques for recommender systems based on collaborative filtering},
  volume       = {37},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). The impact engineer—weaving the golden braid. <em>EXSY</em>,
<em>37</em>(6), e12646. (<a
href="https://doi.org/10.1111/exsy.12646">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_EXSY},
  author       = {Gareth Davies and Jon Hall},
  doi          = {10.1111/exsy.12646},
  journal      = {Expert Systems},
  month        = {12},
  number       = {6},
  pages        = {e12646},
  shortjournal = {Expert Syst.},
  title        = {The impact engineer—Weaving the golden braid},
  volume       = {37},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). The use of machine learning and deep learning algorithms in
functional magnetic resonance imaging—a systematic review.
<em>EXSY</em>, <em>37</em>(6), e12644. (<a
href="https://doi.org/10.1111/exsy.12644">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Functional Magnetic Resonance Imaging (fMRI) is presently one of the most popular techniques for analysing the dynamic states in brain images using various kinds of algorithms. From the last decade, there is an exponential rise in the use of the machine and deep learning algorithms of artificial intelligence for analysing fMRI data. However, it is a big challenge for every researcher to choose a suitable machine or deep learning algorithm for analysing fMRI data due to the availability of a large number of algorithms in the literature. It takes much time for each researcher to know about the various approaches and algorithms which are in use for fMRI data. This paper provides a review in a systematic manner for the present literature of fMRI data that makes use of the machine and deep learning algorithms. The major goals of this review paper are to (a) identify machine learning and deep learning research trends for the implementation of fMRI; (b) identify usage of Machine Learning Algorithms and deep learning in fMRI, and (c) help new researchers based on fMRI to put their new findings appropriately in existing domain of fMRI research. The results of this systematic review identified various fMRI studies and classified them based on fMRI types, mental diseases, use of machine learning and deep learning algorithms. The authors have provided the studies with the best performance of machine learning and deep learning algorithms used in fMRI. The authors believe that this systematic review will help incoming researchers on fMRI in their future works.},
  archive      = {J_EXSY},
  author       = {Mamoon Rashid and Harjeet Singh and Vishal Goyal},
  doi          = {10.1111/exsy.12644},
  journal      = {Expert Systems},
  month        = {12},
  number       = {6},
  pages        = {e12644},
  shortjournal = {Expert Syst.},
  title        = {The use of machine learning and deep learning algorithms in functional magnetic resonance imaging—A systematic review},
  volume       = {37},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Special issue on new trends and challenges of bio-inspired
computational intelligence algorithms in massively complex systems.
<em>EXSY</em>, <em>37</em>(6), e12637. (<a
href="https://doi.org/10.1111/exsy.12637">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_EXSY},
  author       = {Antonio Gonzalez-Pardo and Antonio J. Tallón-Ballesteros and Hujun Yin},
  doi          = {10.1111/exsy.12637},
  journal      = {Expert Systems},
  month        = {12},
  number       = {6},
  pages        = {e12637},
  shortjournal = {Expert Syst.},
  title        = {Special issue on new trends and challenges of bio-inspired computational intelligence algorithms in massively complex systems},
  volume       = {37},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Fifth special issue on knowledge discovery and business
intelligence. <em>EXSY</em>, <em>37</em>(6), e12628. (<a
href="https://doi.org/10.1111/exsy.12628">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_EXSY},
  author       = {Paulo Cortez and Albert Bifet},
  doi          = {10.1111/exsy.12628},
  journal      = {Expert Systems},
  month        = {12},
  number       = {6},
  pages        = {e12628},
  shortjournal = {Expert Syst.},
  title        = {Fifth special issue on knowledge discovery and business intelligence},
  volume       = {37},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). From mobility data to habits and common pathways.
<em>EXSY</em>, <em>37</em>(6), e12627. (<a
href="https://doi.org/10.1111/exsy.12627">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many aspects of our lives are associated with places and the activities we perform on a daily basis. Most of them are recurrent and demand displacement of the individual between regular places like going to work, school or other important personal locations. To accomplish these recurrent daily activities, people tend to follow regular paths with similar temporal and spatial characteristics, especially because humans are frequently looking for uniformity to support their decisions and make their actions easier or even automatic. In this work, we propose a method for discovering common pathways across users&#39; habits from human mobility data. By using a density-based clustering algorithm, we identify the most preferable locations the users visit, we apply a Gaussian mixture model over these places to automatically separate among all traces, the trajectories that follow patterns in order to discover the representations of individual&#39;s habits. By using the longest common sub-sequence algorithm, we search for the trajectories that are more similar over the set of users&#39; habits trips by considering the distance that pairs of users or habits share on the same path. The proposed method is evaluated over two real-world GPS datasets and the results show that the approach is able to detect the most important places in a user&#39;s life, detect the routine activities and identify common routes between users that have similar habits paving the way for research techniques in carpooling, recommendation and prediction systems.},
  archive      = {J_EXSY},
  author       = {Thiago Andrade and Brais Cancela and João Gama},
  doi          = {10.1111/exsy.12627},
  journal      = {Expert Systems},
  month        = {12},
  number       = {6},
  pages        = {e12627},
  shortjournal = {Expert Syst.},
  title        = {From mobility data to habits and common pathways},
  volume       = {37},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Semantic segmentation and colorization of grayscale aerial
imagery with w-net models. <em>EXSY</em>, <em>37</em>(6), e12622. (<a
href="https://doi.org/10.1111/exsy.12622">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The semantic segmentation of remotely sensed aerial imagery is nowadays an extensively explored task, concerned with determining, for each pixel in an input image, the most likely class label from a finite set of possible labels. Most previous work in the area has addressed the analysis of high-resolution modern images, although the semantic segmentation of historical grayscale aerial photos can also have important applications. Examples include supporting the development of historical road maps, or the development of dasymetric disaggregation approaches leveraging historical building footprints. Following recent work in the area related to the use of fully-convolutional neural networks for semantic segmentation, and specifically envisioning the segmentation of grayscale aerial imagery, we evaluated the performance of an adapted version of the W-Net architecture, which has achieved very good results on other types of image segmentation tasks. Our W-Net model is trained to simultaneously segment images and reconstruct, or predict, the colour of the input images from intermediate representations. Through experiments with distinct data sets frequently used in previous studies, we show that the proposed W-Net architecture is quite effective in colouring and segmenting the input images. The proposed approach outperforms a baseline corresponding to the U-Net model for the segmentation of both coloured and grayscale imagery, and it also outperforms some of the other recently proposed approaches when considering coloured imagery.},
  archive      = {J_EXSY},
  author       = {Maria Dias and João Monteiro and Jacinto Estima and Joel Silva and Bruno Martins},
  doi          = {10.1111/exsy.12622},
  journal      = {Expert Systems},
  month        = {12},
  number       = {6},
  pages        = {e12622},
  shortjournal = {Expert Syst.},
  title        = {Semantic segmentation and colorization of grayscale aerial imagery with W-net models},
  volume       = {37},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Visual interpretation of regression error. <em>EXSY</em>,
<em>37</em>(6), e12621. (<a
href="https://doi.org/10.1111/exsy.12621">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Several sophisticated machine learning tools (e.g., ensembles or deep networks) have shown outstanding performance in different regression forecasting tasks. In many real world application domains the numeric predictions of the models drive important and costly decisions. Nevertheless, decision makers frequently require more than a black box model to be able to “trust” the predictions up to the point that they base their decisions on them. In this context, understanding these black boxes has become one of the hot topics in Machine Learning research. This paper proposes a series of visualization tools that explain the relationship between the expected predictive performance of black box regression models and the values of the input variables of any given test case. This type of information thus allows end-users to correctly assess the risks associated with the use of a model, by showing how concrete values of the predictors may affect the performance of the model. Our illustrations with different real world data sets and learning algorithms provide insights on the type of usage and information these tools bring to both the data analyst and the end-user. Furthermore, a thorough evaluation of the proposed tools is performed to showcase the reliability of this approach.},
  archive      = {J_EXSY},
  author       = {Inês Areosa and Luís Torgo},
  doi          = {10.1111/exsy.12621},
  journal      = {Expert Systems},
  month        = {12},
  number       = {6},
  pages        = {e12621},
  shortjournal = {Expert Syst.},
  title        = {Visual interpretation of regression error},
  volume       = {37},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Sibilant consonants classification comparison with multi-
and single-class neural networks. <em>EXSY</em>, <em>37</em>(6), e12620.
(<a href="https://doi.org/10.1111/exsy.12620">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many children with speech sound disorders cannot pronounce the sibilant consonants correctly. We have developed a serious game, which is controlled by the children&#39;s voices in real time, with the purpose of helping children on practicing the production of European Portuguese (EP) sibilant consonants. For this, the game uses a sibilant consonant classifier. Since the game does not require any type of adult supervision, children can practice producing these sounds more often, which may lead to faster improvements of their speech. Recently, the use of deep neural networks has given considerable improvements in the classification of a variety of use cases, from image classification to speech and language processing. Here, we propose to use deep convolutional neural networks to classify sibilant phonemes of EP in our serious game for speech and language therapy. We compared the performance of several different artificial neural networks that used Mel frequency cepstral coefficients or log Mel filterbanks. Our best deep learning model achieves classification scores of 95.48% using a 2D convolutional model with log Mel filterbanks as input features. Such results are then further improved for specific classes with simple binary classifiers.},
  archive      = {J_EXSY},
  author       = {Ivo Anjos and Nuno Cavalheiro Marques and Margarida Grilo and Isabel Guimarães and João Magalhães and Sofia Cavaco},
  doi          = {10.1111/exsy.12620},
  journal      = {Expert Systems},
  month        = {12},
  number       = {6},
  pages        = {e12620},
  shortjournal = {Expert Syst.},
  title        = {Sibilant consonants classification comparison with multi- and single-class neural networks},
  volume       = {37},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A context-aware recommender method based on text and opinion
mining. <em>EXSY</em>, <em>37</em>(6), e12618. (<a
href="https://doi.org/10.1111/exsy.12618">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A recommender system is an information filtering technology that can be used to recommend items that may be of interest to users. Additionally, there are the context-aware recommender systems that consider contextual information to generate the recommendations. Reviews can provide relevant information that can be used by recommender systems, including contextual and opinion information. In a previous work, we proposed a context-aware recommendation method based on text mining (CARM-TM). The method includes two techniques to extract context from reviews: CIET.5 embed , a technique based on word embeddings; and RulesContext , a technique based on association rules. In this work, we have extended our previous method by including CEOM , a new technique which extracts context by using aspect-based opinions. We call our extension of CARM-TOM (context-aware recommendation method based on text and opinion mining). To generate recommendations, our method makes use of the CAMF algorithm, a context-aware recommender based on matrix factorization. To evaluate CARM-TOM, we ran an extensive set of experiments in a dataset about restaurants, comparing CARM-TOM against the MF algorithm, an uncontextual recommender system based on matrix factorization; and against a context extraction method proposed in literature. The empirical results strongly indicate that our method is able to improve a context-aware recommender system.},
  archive      = {J_EXSY},
  author       = {Camila Vaccari Sundermann and Renan de Padua and Vítor Rodrigues Tonon and Ricardo Marcondes Marcacini and Marcos Aurélio Domingues and Solange Oliveira Rezende},
  doi          = {10.1111/exsy.12618},
  journal      = {Expert Systems},
  month        = {12},
  number       = {6},
  pages        = {e12618},
  shortjournal = {Expert Syst.},
  title        = {A context-aware recommender method based on text and opinion mining},
  volume       = {37},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A bi-objective procedure to deliver actionable knowledge in
sport services. <em>EXSY</em>, <em>37</em>(6), e12617. (<a
href="https://doi.org/10.1111/exsy.12617">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The increase in retention of customers in gyms and health clubs is nowadays a challenge that requires concrete and personalized actions. Traditional data mining studies focused essentially on predictive analytics, neglecting the business domain. This work presents an actionable knowledge discovery system that uses the following pipeline (data collection, predictive model and retention interventions). In the first step, it extracts and transforms existing real data from databases of the sports facilities. In the second step, predictive models are applied to identify user profiles more susceptible to dropout, where actionable withdrawal rules are based on actionable attributes. Finally, in the third step, based on the previous actionable knowledge, some of the values of the actionable attributes should be changed in order to increase retention. Simulation of scenarios is carried out, with test and control groups, where business utility and associated cost are measured. This document presents a bi-objective study in order to choose the more efficient scenarios.},
  archive      = {J_EXSY},
  author       = {Paulo Pinheiro and Luís Cavique},
  doi          = {10.1111/exsy.12617},
  journal      = {Expert Systems},
  month        = {12},
  number       = {6},
  pages        = {e12617},
  shortjournal = {Expert Syst.},
  title        = {A bi-objective procedure to deliver actionable knowledge in sport services},
  volume       = {37},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Face similarity linkage: A novel biometric approach to
sexually motivated serial killer victims. <em>EXSY</em>, <em>37</em>(6),
e12597. (<a href="https://doi.org/10.1111/exsy.12597">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Some sexually motivated serial killers target victims on the basis of appearance. Therefore, multiple victims of a single serial killer are likely to have some facial features and geometries that are similar. The current research was undertaken to propose a technique, termed face similarity linkage, to evaluate whether victims of a serial killer have statistically more similar facial measurements than a randomly chosen person of the same gender. To test this, three of Ted Bundy&#39;s victims were randomly selected and anatomical landmarks were located and measured to produce proportionality indices of their faces. A random subject from an online database was used as a comparison. The results showed there were no statistically significant differences between the three of Bundy&#39;s victims, however there was significant difference between 11 of the 17 facial measurements of Bundy&#39;s victims when compared to a random person. This research serves as a proof of concept that, with more advanced means of data collection, may be a useful tool for law enforcement for linking serial homicides. The current method is relatively novel, and in need of expert systems interfaces to improve speed and application, which is outlined in the current study.},
  archive      = {J_EXSY},
  author       = {Sarah Bernadette Hackett and David Keatley and Brendan Chapman},
  doi          = {10.1111/exsy.12597},
  journal      = {Expert Systems},
  month        = {12},
  number       = {6},
  pages        = {e12597},
  shortjournal = {Expert Syst.},
  title        = {Face similarity linkage: A novel biometric approach to sexually motivated serial killer victims},
  volume       = {37},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). An integrated probabilistic linguistic projection method for
MCGDM based on ELECTRE III and the weighted convex median voting rule.
<em>EXSY</em>, <em>37</em>(6), e12593. (<a
href="https://doi.org/10.1111/exsy.12593">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the multi-criteria group decision-making (MCGDM) problems with great uncertainty, making full use of participants&#39; evaluation information could help improve the accuracy and reliability of decision results. Probabilistic linguistic term set (PLTS) is an effective tool to represent qualitative data and can fully express the hesitation and preference of decision makers. Therefore, this paper aims to propose an MCGDM method based on PLTSs. In the proposed method, the projection of PLTSs is explored to measure the distance and angle differences between two objects, and Bayesian best–worst method (Bayesian BWM) is used to determine the aggregated final weights of criteria. Besides, the elimination and choice translating reality III (ELECTRE III) method combined with distillation algorithm deals with the projection of PLTSs to obtain the alternatives&#39; ranking of each decision maker. Then, the weighted convex median voting rule is developed to integrate the rankings results regarding all decision makers, which can solve the conflict of ranking results among experts and ensure that the comprehensive ranking results are reasonable and practical. Finally, a case study of health-care waste management is designed and comparative analyses are implemented to show the effectiveness and advantages of the proposed method.},
  archive      = {J_EXSY},
  author       = {Zi-yu Chen and Xiao-kang Wang and Juan-juan Peng and Hong-yu Zhang and Jian-qiang Wang},
  doi          = {10.1111/exsy.12593},
  journal      = {Expert Systems},
  month        = {12},
  number       = {6},
  pages        = {e12593},
  shortjournal = {Expert Syst.},
  title        = {An integrated probabilistic linguistic projection method for MCGDM based on ELECTRE III and the weighted convex median voting rule},
  volume       = {37},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Bone age estimation from carpal radiography images using
deep learning. <em>EXSY</em>, <em>37</em>(6), e12584. (<a
href="https://doi.org/10.1111/exsy.12584">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Bone age estimation has been used in medicine to verify whether the bone structure development degree of a person corresponds to their chronological age. Such estimate is useful for prognosis about the development of children and adolescents, as well as for the diagnosis of endocrinological diseases. This work proposes a fully automated methodology for bone age estimation from carpal radiography images. The methodology comprises two steps, the preprocessing of the image and the classification using a convolutional neural network. The system accuracy for different types of preprocessing is evaluated. We compare the accuracy achieved using the full radiography image as input for the neural network and using only parts of the image corresponding to the Phalangeal region, the Epiphyseal region, and the concatenation of these parts with a crop around the wrist. Digital image processing techniques are employed to segment these regions. Experiments are performed using radiography images from the California University Database. The impact of using different pre-trained neural networks for transfer learning is evaluated.},
  archive      = {J_EXSY},
  author       = {Yih An Ding and Filipe Mutz and Klaus F. Côco and Luiz A. Pinto and Karin S. Komati},
  doi          = {10.1111/exsy.12584},
  journal      = {Expert Systems},
  month        = {12},
  number       = {6},
  pages        = {e12584},
  shortjournal = {Expert Syst.},
  title        = {Bone age estimation from carpal radiography images using deep learning},
  volume       = {37},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). The fixed set search applied to the power dominating set
problem. <em>EXSY</em>, <em>37</em>(6), e12559. (<a
href="https://doi.org/10.1111/exsy.12559">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we focus on solving the power dominating set problem and its connected version. These problems are frequently used for finding optimal placements of phasor measurement units in power systems. We present an improved integer linear program (ILP) for both problems. In addition, a greedy constructive algorithm and a local search are developed. A greedy randomised adaptive search procedure (GRASP) algorithm is created to find near optimal solutions for large scale problem instances. The performance of the GRASP is further enhanced by extending it to the novel fixed set search (FSS) metaheuristic. Our computational results show that the proposed ILP has a significantly lower computational cost than existing ILPs for both versions of the problem. The proposed FSS algorithm manages to find all the optimal solutions that have been acquired using the ILP. In the last group of tests, it is shown that the FSS can significantly outperform the GRASP in both solution quality and computational cost.},
  archive      = {J_EXSY},
  author       = {Raka Jovanovic and Stefan Voss},
  doi          = {10.1111/exsy.12559},
  journal      = {Expert Systems},
  month        = {12},
  number       = {6},
  pages        = {e12559},
  shortjournal = {Expert Syst.},
  title        = {The fixed set search applied to the power dominating set problem},
  volume       = {37},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Finding weaknesses in networks using greedy randomized
adaptive search procedure and path relinking. <em>EXSY</em>,
<em>37</em>(6), e12540. (<a
href="https://doi.org/10.1111/exsy.12540">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, the relevance of cybersecurity has been increasingly evident to companies and institutions, as well as to final users. Because of that, it is important to ensure the robustness of a network. With the aim of improving the security of the network, it is desirable to find out which are the most critical nodes in order to protect them from external attackers. This work tackles this problem, named the α -separator problem, from a heuristic perspective, proposing an algorithm based on the Greedy Randomized Adaptive Search Procedure (GRASP). In particular, a novel approach for the constructive procedure is proposed, where centrality metrics derived from social network analysis are used as a greedy criterion. Furthermore, the quality of the provided solutions is improved by means of a combination method based on Path Relinking (PR). This work explores different variants of PR, also adapting the most recent one, Exterior PR, for the problem under consideration. The combination of GRASP + PR allows the algorithm to obtain high-quality solutions within a reasonable computing time. The proposal is supported by a set of intensive computational experiments that show the quality of the proposal, comparing it with the most competitive algorithm found in the state of art.},
  archive      = {J_EXSY},
  author       = {Sergio Pérez-Peló and Jesús Sánchez-Oro and Abraham Duarte},
  doi          = {10.1111/exsy.12540},
  journal      = {Expert Systems},
  month        = {12},
  number       = {6},
  pages        = {e12540},
  shortjournal = {Expert Syst.},
  title        = {Finding weaknesses in networks using greedy randomized adaptive search procedure and path relinking},
  volume       = {37},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). On the design of hybrid bio-inspired meta-heuristics for
complex multiattribute vehicle routing problems. <em>EXSY</em>,
<em>37</em>(6), e12528. (<a
href="https://doi.org/10.1111/exsy.12528">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper addresses a multiattribute vehicle routing problem, the rich vehicle routing problem, with time constraints, heterogeneous fleet, multiple depots, multiple routes, and incompatibilities of goods. Four different approaches are presented and applied to 15 real datasets. They are based on two meta-heuristics, ant colony optimization (ACO) and genetic algorithm (GA), that are applied in their standard formulation and combined as hybrid meta-heuristics to solve the problem. As such ACO-GA is a hybrid meta-heuristic using ACO as main approach and GA as local search. GA-ACO is a memetic algorithm using GA as main approach and ACO as local search. The results regarding quality and computation time are compared with two commercial tools currently used to solve the problem. Considering the number of customers served, one of the tools and the ACO-GA approach outperforms the others. Considering the cost, ACO, GA, and GA-ACO provide better results. Regarding computation time, GA and GA-ACO have been found the most competitive among the benchmark.},
  archive      = {J_EXSY},
  author       = {Ana-Maria Nogareda and Javier Del Ser and Eneko Osaba and David Camacho},
  doi          = {10.1111/exsy.12528},
  journal      = {Expert Systems},
  month        = {12},
  number       = {6},
  pages        = {e12528},
  shortjournal = {Expert Syst.},
  title        = {On the design of hybrid bio-inspired meta-heuristics for complex multiattribute vehicle routing problems},
  volume       = {37},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). StarTroper, a film trope rating optimizer using machine
learning and evolutionary algorithms. <em>EXSY</em>, <em>37</em>(6),
e12525. (<a href="https://doi.org/10.1111/exsy.12525">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Designing a story is widely considered a crafty yet critical task that requires deep specific human knowledge in order to reach a minimum quality and originality. This includes designing at a high level different elements of the film; these high-level elements are called tropes when they become patterns. The present paper proposes and evaluates a methodology to automatically synthesize sets of tropes in a way that they maximize the potential rating of a film that conforms to them. We use machine learning to create a surrogate model that maps film ratings from tropes, trained with the data extracted and processed from huge film databases in Internet, and then we use a genetic algorithm that uses that surrogate model as evaluator to optimize the combination of tropes in a film. In order to evaluate the methodology, we analyse the nature of the tropes and their distributions in existing films, the performance of the models and the quality of the sets of tropes synthesized. The results of this proof of concept show that the methodology works and is able to build sets of tropes that maximize the rating and that these sets are genuine. The work has revealed that the methodology and tools developed are directly suitable for assisting in the plots generation as an authoring tool and, ultimately, for supporting the automatic generation of stories, for example, in massively populated videogames.},
  archive      = {J_EXSY},
  author       = {Rubén Héctor García-Ortega and Pablo García-Sánchez and Juan Julián Merelo-Guervós},
  doi          = {10.1111/exsy.12525},
  journal      = {Expert Systems},
  month        = {12},
  number       = {6},
  pages        = {e12525},
  shortjournal = {Expert Syst.},
  title        = {StarTroper, a film trope rating optimizer using machine learning and evolutionary algorithms},
  volume       = {37},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Transfer learning and information retrieval applied to fall
detection. <em>EXSY</em>, <em>37</em>(6), e12522. (<a
href="https://doi.org/10.1111/exsy.12522">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Detecting falls in the elderly population is a very important issue that is related with the time of recovery. This study focuses on using wearable smart watches to monitor the movements of the user in order to detect patterns that might be related to fall events. The proposed solution explores Symbolic Aggregate approXimation (SAX) Time Series representation, together with two information retrieval techniques enriched with transfer learning (TL). The solution is user centred; that is, a model is developed for each specific user. Basically, the fall detection approach makes use of a finite-state machine to detect peaks; the time series window embedding these peaks are represented using SAX. Assuming the data from the public fall detection data sets are valid, a dictionary is prepared using the most relevant words. This dictionary is then introduced as previous knowledge to an online learning classifier that is trained with normal activities of daily living. The two classifiers are evaluated and compared with two classical approaches. Before this comparison, two clustering approaches are studied to produce the bag of relevant words. A complete experimentation is included, which makes use of several publicly available data sets and also with a data set developed by the research group. Comparisons are performed for all the data sets, showing how the TL stage empowers the classifier. The results show that this solution produces high detection rates and at the same time performed similarly for all the individuals tested. Furthermore, the positive effects of TL in this context are clearly remarked.},
  archive      = {J_EXSY},
  author       = {Mirko Fañez and José R. Villar and Enrique de la Cal and Javier Sedano and Victor M. González},
  doi          = {10.1111/exsy.12522},
  journal      = {Expert Systems},
  month        = {12},
  number       = {6},
  pages        = {e12522},
  shortjournal = {Expert Syst.},
  title        = {Transfer learning and information retrieval applied to fall detection},
  volume       = {37},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Distributed artificial bee colony approach for connected
appliances in smart home energy management system. <em>EXSY</em>,
<em>37</em>(6), e12521. (<a
href="https://doi.org/10.1111/exsy.12521">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this study, we propose a computational intelligence model for the Internet of Things applications by applying the concept of swarm intelligence (SI) into connected devices. Particularly, decentralized management of smart home energy management system (HEMS) is taken into account in which connected appliances, by sharing information with each other, make the individual decisions for optimizing electricity prices of smart HEMS. Specifically, the study includes two main issues: (a) We propose a framework for decentralized management in smart HEMS; and (b) artificial bee colony (ABC) algorithm, a typical algorithm of SI techniques, has been applied for connected appliances in terms of communication and collaboration with each other to optimize the performance of the energy management system. Moreover, regarding the implementation, we develop and simulate a connected environment of smart home systems to evaluate the proposed approach. The simulation indicates the promising results in terms of optimizing the load balancing problem comparing with the conventional approach of the decentralized management system in smart home applications.},
  archive      = {J_EXSY},
  author       = {Khac-Hoai N. Bui and Israel E. Agbehadji and Richard Millham and David Camacho and Jason J. Jung},
  doi          = {10.1111/exsy.12521},
  journal      = {Expert Systems},
  month        = {12},
  number       = {6},
  pages        = {e12521},
  shortjournal = {Expert Syst.},
  title        = {Distributed artificial bee colony approach for connected appliances in smart home energy management system},
  volume       = {37},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A new system for automatic analysis and quality adjustment
in audiovisual subtitled-based contents by means of genetic algorithms.
<em>EXSY</em>, <em>37</em>(6), e12512. (<a
href="https://doi.org/10.1111/exsy.12512">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In Spain, the subtitling service on television for the deaf has been improving in quantity since the General Law on Audiovisual Communication was enacted in 2010. This law establishes a series of quality standards that must be followed in the subtitling process. One of the most relevant aspects of subtitle quality is the speed at which they are shown on the screen, due to the fact that a too high speed (less time on screen) will make them difficult to read and the information hard to understand. In order to determine whether the speed at which the subtitles are being shown is adequate, first, it is necessary to process all the information associated with the broadcast of the digital TV channels including data from different sources. In this research, the authors have worked with the data obtained within the time period between July 2012 and December 2017, that is, with more than 950 million records. This article presents a framework for integration and processing of heterogeneous information associated with the subtitling of audiovisual content from different sources. Moreover, the framework will provide an automatic adjustment of subtitles in broadcasting regarding quality indicators by means of a genetic algorithm approach. The results show that the system is able to estimate the best relationship between the time and size of the subtitles and maintaining the quality levels established for this research. These results have been validated by experts and users of this domain.},
  archive      = {J_EXSY},
  author       = {Monica Souto-Rico and Israel González-Carrasco and José-Luis López-Cuadrado and Belén Ruíz-Mezcua},
  doi          = {10.1111/exsy.12512},
  journal      = {Expert Systems},
  month        = {12},
  number       = {6},
  pages        = {e12512},
  shortjournal = {Expert Syst.},
  title        = {A new system for automatic analysis and quality adjustment in audiovisual subtitled-based contents by means of genetic algorithms},
  volume       = {37},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Special issue on “advances in visual analytics and mining
visual data.” <em>EXSY</em>, <em>37</em>(5), e12607. (<a
href="https://doi.org/10.1111/exsy.12607">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_EXSY},
  author       = {Victor Chang and Shadi A. Aljawarneh and Chung-Sheng Li},
  doi          = {10.1111/exsy.12607},
  journal      = {Expert Systems},
  month        = {10},
  number       = {5},
  pages        = {e12607},
  shortjournal = {Expert Syst.},
  title        = {Special issue on “advances in visual analytics and mining visual data”},
  volume       = {37},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Domain problem-solving expert identification in community
question answering. <em>EXSY</em>, <em>37</em>(5), e12582. (<a
href="https://doi.org/10.1111/exsy.12582">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Question-Answering (Q&amp;A) services provide internet users with platforms to exchange knowledge and ideas. The development of Q&amp;A sites, or Community Question Answering (CQA), mainly depends on the high-quality content continuously contributed by users with high-level expertise, who can be recognized as experts. Expert finding is an important task for the authorities of Q&amp;A communities to encourage commitment. In a highly competitive market environment, CQA managers have to take measures to retain and nurture users, especially superior contributors. However, current expertise scoring techniques adopted in CQA often give much credit to very active users and fail to identify real experts. This study aims to develop a robust and practical expert identification framework for Q&amp;A communities, by combining well-designed expertise scoring technique and probabilistic clustering model. With regard to expert identification, a numerical metric of users&#39; expertise is developed as the optimal expert finding strategy, and a clustering algorithm based on Gaussian-Gamma mixture model (GGMM) is proposed to efficiently distinguish experts from nonexperts. In the experiments, the proposed method is applied to real-world datasets collected from subcommunities of Stack Exchange Q&amp;A networks. Results obtained from comparative experiments show that our method achieves better performance than the state-of-the-art methods and demonstrate the effectiveness of the proposed framework. The analysis shows that the framework which combines the proposed expertise scoring technique and Gaussian–Gamma mixture clustering model is capable of detecting excellent domain problem-solving experts who exhibit both domain interest and expertise.},
  archive      = {J_EXSY},
  author       = {Weizhao Tang and Tun Lu and Hansu Gu and Peng Zhang and Ning Gu},
  doi          = {10.1111/exsy.12582},
  journal      = {Expert Systems},
  month        = {10},
  number       = {5},
  pages        = {e12582},
  shortjournal = {Expert Syst.},
  title        = {Domain problem-solving expert identification in community question answering},
  volume       = {37},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Developing a novel inverse data envelopment analysis (DEA)
model for evaluating after-sales units. <em>EXSY</em>, <em>37</em>(5),
e12579. (<a href="https://doi.org/10.1111/exsy.12579">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes a novel model of inverse data envelopment analysis (IDEA) based on the slack-based measure (SBM) approach. The developed inverse SBM model can maintain relative efficiency of decision making units (DMUs) with new input and output. This model can also measure the input and output volumes when a decision maker (DM) increases efficiency score. The inverse SBM model is a kind of multi-objective non-linear programming (MONLP) problem, which is not easy to solve. Therefore, we suggest a linear programming model for solving inverse SBM model. In this model efficiency score of DMU under evaluation remains unchanged. Furthermore, we suggest an optimal combination of inputs and outputs in the production possibility set (PPS). A case study is presented to demonstrate the efficacy of our proposed model.},
  archive      = {J_EXSY},
  author       = {Seyed S. S. Hosseininia and Reza F. Saen},
  doi          = {10.1111/exsy.12579},
  journal      = {Expert Systems},
  month        = {10},
  number       = {5},
  pages        = {e12579},
  shortjournal = {Expert Syst.},
  title        = {Developing a novel inverse data envelopment analysis (DEA) model for evaluating after-sales units},
  volume       = {37},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Towards an automatic coding of observational studies: Coding
neurofeedback therapies of children with autism. <em>EXSY</em>,
<em>37</em>(5), e12572. (<a
href="https://doi.org/10.1111/exsy.12572">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The coding of observational data is commonly used to analyse and evaluate human behaviours. The technique can help researchers inform the design and impact of, for example, an Ubicomp system by studying specific behaviours of interest. There are some tools that can alleviate the burden of observational coding, like those that help to collect and organise data, but can still be error-prone and time-consuming. Moreover, most of these tools lack automation, requiring intense human interaction. In order to mitigate these issues, computer vision (CV) and machine learning (ML) techniques could be used to automate observational coding, but little work has focused on analysing the feasibility of such an approach, with the goal of reducing the total coding time while maintaining accuracy. In this work, we address this question by proposing an automated approach for a real-world case study and compare it to manual coding. The study is composed of 10 videos with an average duration of 17 min each, where the goal is to determine the attention of children with autism that participate in a neurofeedback therapy session. Each video was hand-coded by three human observers to define the ground truth and to measure the manual coding time. Results show that it is feasible to automate the coding of observational behaviours and obtain a noticeable reduction in coding time, but with a slight loss in accuracy. Moreover, we illustrate that the best solution would be a hybrid approach, using a semi-automated system that combines human expertise and ML predictions Keywords Observational studies, Coding behaviours, Automatic coding, Computer-Vision, Machine Learning.},
  archive      = {J_EXSY},
  author       = {Víctor R. López-López and Lizbeth Escobedo and Leonardo Trujillo},
  doi          = {10.1111/exsy.12572},
  journal      = {Expert Systems},
  month        = {10},
  number       = {5},
  pages        = {e12572},
  shortjournal = {Expert Syst.},
  title        = {Towards an automatic coding of observational studies: Coding neurofeedback therapies of children with autism},
  volume       = {37},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Evaluation of an interactive educational system in urban
knowledge acquisition and representation based on students’ profiles.
<em>EXSY</em>, <em>37</em>(5), e12570. (<a
href="https://doi.org/10.1111/exsy.12570">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The aim of this article is to study how students currently understand the conception of space through different media and how that understanding helps them to intervene in the space. Firstly, the process of teaching and learning as well as innovative supporting technologies are analysed, revealing the characteristics of the contemporary student profile and better ways of teaching according to it. Secondly, we describe the evaluation of an experiment with a virtual reality (VR) system for urban project design with students of architecture from two universities. The premise is that the technology used in VR is familiar to the current profile of students. This paper aims to study the advantages and disadvantages of this trend and to find a balance. To do so, we use a quantitative method to evaluate students&#39; profiles and their level of satisfaction with the system. The results were obtained by a questionnaire and a survey, which show the role and use of technologies in the students&#39; environment and the degree of satisfaction when using it in the educational processes. In line with our assumptions, the value of satisfaction in the use of an advanced visualization technology in the classroom reveals a high level of motivation, in general, with differentiation between students in their first and last phases of studies.},
  archive      = {J_EXSY},
  author       = {Monica V. Sanchez-Sepulveda and David Fonseca and Alicia García-Holgado and Francisco José García-Peñalvo and Jordi Franquesa and Ernesto Redondo and Fernando Moreira},
  doi          = {10.1111/exsy.12570},
  journal      = {Expert Systems},
  month        = {10},
  number       = {5},
  pages        = {e12570},
  shortjournal = {Expert Syst.},
  title        = {Evaluation of an interactive educational system in urban knowledge acquisition and representation based on students&#39; profiles},
  volume       = {37},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Development of optimal water supply plan using integrated
fuzzy delphi and fuzzy ELECTRE III methods—case study of the gamasiab
basin. <em>EXSY</em>, <em>37</em>(5), e12568. (<a
href="https://doi.org/10.1111/exsy.12568">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a novel method for the development of an optimal water supply plan showcased using data from the Gamasiab basin, located in Kermanshah province, Iran, concerning new dams that are being constructed in this semi-arid region. In this paper, a new group multi-criteria decision-making (MCDM) plan is proposed by combining two MCDM methods based on the fuzzy Delphi and fuzzy ELECTRE III methods that convert the experts&#39; opinions to triangular fuzzy numbers based on the level of uncertainty associated with various quantitative and qualitative criteria. Considering the opinions of four non-stakeholder experts and data analysis using the fuzzy Delphi method, the criteria were evaluated. Then, by analysing the results using the fuzzy ELECTRE III method, the final ranking of scenarios is obtained. A sensitivity analysis was conducted to assess the effect of uncertainty on the performance of the decision-making system in scenarios ranking. The total expense, flood control, reservoir capacity and diversion and water transfer played a significant role in selecting the optimal scenario. Additionally, a hydrologic model was developed to evaluate the performance of the optimal scenario in terms of qualitative criteria. The data indicated that there was a good agreement between the results obtained from the hydrological model and the scenario ranking by the employed method. Altogether, a comparison of the proposed method with other MCDM methods, including fuzzy analytic hierarchy process and fuzzy technique for order preference by simulation of ideal solution, indicated that the results of the employed method matched more closely to the local experts&#39; opinion.},
  archive      = {J_EXSY},
  author       = {Amir Noori and Hossein Bonakdari and Khosro Morovati and Bahram Gharabaghi},
  doi          = {10.1111/exsy.12568},
  journal      = {Expert Systems},
  month        = {10},
  number       = {5},
  pages        = {e12568},
  shortjournal = {Expert Syst.},
  title        = {Development of optimal water supply plan using integrated fuzzy delphi and fuzzy ELECTRE III methods—Case study of the gamasiab basin},
  volume       = {37},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Solving predictive control problem of fast-varying
multivariable systems by incorporating unknown active dynamics generated
by real-time adaptive learning machine. <em>EXSY</em>, <em>37</em>(5),
e12567. (<a href="https://doi.org/10.1111/exsy.12567">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Not only adaptive predictive control of switched systems is a computationally intensive procedure, it also involves various challenges in addressing the problems of robust stabilization and precise tracking. This study proposes new strategies to deal with the aforementioned issues (namely safe and precise control alongside with reduction of computational burden). The first contribution of this work is reduction of conservatism for described class of systems. Control of switched systems with undetectable switching signals is often conducted in worst case switching configuration to ensure robustness, which potentially results in conservative design. The issue of conservativeness is intensified in multi input-multi output (MIMO) dynamical systems due to increased dimensions. However, attaining a robust control scheme for all switching configurations while ensuring precise response is inherently paradoxical. To overcome this issue, this study proposes a new dual-mode algorithm where control modes corresponding to safety and precision are activated at appropriate stages of system response. This is conducted based on incorporation of an adaptive fuzzy-wavelet neural network identification scheme in predictive control of MIMO switched systems. However, as convergence of the adaptive algorithm to actual system is attained after a finite period of time, a safe-mode control algorithm is proposed to maintain quality of transient response in convergence period. In other words, the proposed algorithm operates in safe and precise control modes to ensure robust stability in the convergence period and non-conservative design in steady-state. Second major contribution of the work is reduction of calculation burden based on incorporation of a suboptimal control algorithm. To this end, we propose a predictive control scheme based on a suboptimal gradient-descent based controller, calculating feasible stabilizing inputs instead of optimal inputs. Effects of dynamical variations are incorporated in the model predictive control framework for increased compatibility with high-speed switching dynamics. Then, based on incorporation of dual-mode algorithm, precise steady-state performance is attained while preventing notable perturbations in dynamical discontinuities at switching.},
  archive      = {J_EXSY},
  author       = {Sadeq Yaqubi and Mohammad Reza Homaeinezhad},
  doi          = {10.1111/exsy.12567},
  journal      = {Expert Systems},
  month        = {10},
  number       = {5},
  pages        = {e12567},
  shortjournal = {Expert Syst.},
  title        = {Solving predictive control problem of fast-varying multivariable systems by incorporating unknown active dynamics generated by real-time adaptive learning machine},
  volume       = {37},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Deep OCR for arabic script-based language like pastho.
<em>EXSY</em>, <em>37</em>(5), e12565. (<a
href="https://doi.org/10.1111/exsy.12565">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Developing cursive script recognition systems have always been a challenging task for researchers. This article proposes a ligature-based recognition system for the cursive Pashto script using four pre-trained CNN models using a fine-tuned approach. The SqueezeNet, ResNet, MobileNet and DenseNet models have been observed for the classification and the recognition of Pashto sub-word (ligature). Overall, the proposed system is divided into two domains (Source and Target). The source domain contains the pre-trained models used on the ImageNet Dataset. These models are later fine-tuned using the transfer learning approach to be used for the Pashto ligature recognition. The data augmentation techniques of negative and contour are used to increase the representation of ligature images and the dataset size. The CNN models have been evaluated on the benchmarks Pashto ligatures FAST-NU dataset. The proposed system achieved the highest recognition rate of up to 99.31% using the DenseNet architecture of Convolutional Neural Network for Pashto ligature.},
  archive      = {J_EXSY},
  author       = {Saeeda Naz and Naila H. Khan and Shizza Zahoor and Muhammad I. Razzak},
  doi          = {10.1111/exsy.12565},
  journal      = {Expert Systems},
  month        = {10},
  number       = {5},
  pages        = {e12565},
  shortjournal = {Expert Syst.},
  title        = {Deep OCR for arabic script-based language like pastho},
  volume       = {37},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Implementation of an optimized binary classification by
GMDH-type neural network algorithm for predicting the blast produced
ground vibration. <em>EXSY</em>, <em>37</em>(5), e12563. (<a
href="https://doi.org/10.1111/exsy.12563">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ground vibration is one of the most important undesired phenomena resulting from blasting operations imposing damages to facilities and buildings on the one hand, and creating environmental problems in open pit mining on the other. Therefore, the present study aims to provide an optimized classification binary model to identify the blasting patterns with an acceptable ground vibration intensity to reduce the damages resulting from this artificial phenomenon. This study uses a binary method to provide an optimized classification model for predicting and evaluating the blasting patterns with the minimum ground vibration. Group Method of Data Handling-Type Neural Network is used as one of the most practical optimization algorithms to solve complicated and uncertain problems in this modelling. In this study, by collecting the data of 52 different blasting patterns from Soungun copper mine, some of the most important geometric properties and the amount of ammonium nitrate fuel oil consumed in each blasting pattern are recorded. In addition, based on expertise and experience of experts, the degree of ground vibration produced by each blasting is qualitatively classified into four different ranges of very high, high, normal and low in the form of unacceptable (very high and High) and acceptable (normal and low) clusters. Based on the results obtained from the analyses, the developed model has a high flexibility and ability in the binary prediction of blasting patterns with an acceptable vibration magnitude.},
  archive      = {J_EXSY},
  author       = {Davood Mohammadi and Reza Mikaeil and Jafar Abdollahi-Sharif},
  doi          = {10.1111/exsy.12563},
  journal      = {Expert Systems},
  month        = {10},
  number       = {5},
  pages        = {e12563},
  shortjournal = {Expert Syst.},
  title        = {Implementation of an optimized binary classification by GMDH-type neural network algorithm for predicting the blast produced ground vibration},
  volume       = {37},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Assessing the importance of autistic attributes for autism
screening. <em>EXSY</em>, <em>37</em>(5), e12562. (<a
href="https://doi.org/10.1111/exsy.12562">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Autistic Spectrum Disorder (ASD) is a cognitive disease which leads to the loss of linguistic, communicative, cognitive, and social skills and abilities. Patients with ASD have diverse troubles such as sleeping problems. The role of genetic and environmental factors is of great importance in its pathophysiology. Early diagnosis provides an improved overall mental health of the patients. This study aimed to identify the important attributes for the best detection of this disorder in children, adolescents and adults. To achieve this aim, Recursive Feature Elimination and Stability Selection methods that consider important attributes for target class were proposed. The attributes collected from autism screening methods and other attributes such as age and gender were examined for the disease. The results verified the combining of feature selection method and machine learning algorithm within the frame of accuracy, sensitivity and specificity evaluation metrics.},
  archive      = {J_EXSY},
  author       = {Kemal Akyol},
  doi          = {10.1111/exsy.12562},
  journal      = {Expert Systems},
  month        = {10},
  number       = {5},
  pages        = {e12562},
  shortjournal = {Expert Syst.},
  title        = {Assessing the importance of autistic attributes for autism screening},
  volume       = {37},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Modelling the vibration response of a gas turbine using
machine learning. <em>EXSY</em>, <em>37</em>(5), e12560. (<a
href="https://doi.org/10.1111/exsy.12560">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This work deals with modelling the vibration response of a gas turbine obtained during the start-up process until reaching the nominal speed for power generation. Analysing the vibrations of a complex systems like a gas turbine is useful for the diagnostic of faults or damages in the internal mechanical components of the different stages that integrate a turbine. This work focuses on the study of the shaft vibrations of the bearing radial type mounted between the shaft and the bearing compressor associated with the speed of the turbine. This relationship is studied using experimental data collected from a particular gas turbine model. In particular, we propose a methodology to synthesize a computational model following a supervised learning approach implemented through different machine learning techniques, including a multi-layers perceptron network, support vector machine (SVM), random forest (RF) and genetic programming (GP) with local search. Results show that SVM, RF and GP perform very well in this task, producing accurate predictive models. Moreover, there are some interesting trade-offs between the methods, regarding generalization error, overfitting and model interpretability that are relevant for future applications and research.},
  archive      = {J_EXSY},
  author       = {Josué Zárate and Perla Juárez-Smith and Javier Carmona and Leonardo Trujillo and Salvador de Lara},
  doi          = {10.1111/exsy.12560},
  journal      = {Expert Systems},
  month        = {10},
  number       = {5},
  pages        = {e12560},
  shortjournal = {Expert Syst.},
  title        = {Modelling the vibration response of a gas turbine using machine learning},
  volume       = {37},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Uncertain location–allocation decisions for a bi-objective
two-stage supply chain network design problem with environmental
impacts. <em>EXSY</em>, <em>37</em>(5), e12558. (<a
href="https://doi.org/10.1111/exsy.12558">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the cases that the historical data of an uncertain event is not available, belief degree-based uncertainty theory is a useful tool to reflect such uncertainty. This study focuses on uncertain bi-objective supply chain network design problem with cost and environmental impacts under uncertainty. As such network may be designed for the first time in a geographical region, this problem is modelled by the concepts of belief degree-based uncertainty theory. This article is almost the first study on belief degree-based uncertain supply chain network design problem with environmental impacts. Two approaches such as expected value model and chance-constrained model are applied to convert the proposed uncertain problem to its crisp form. The obtained crisp forms are solved by some multi-objective optimization approaches of the literature such as TH, Niroomand, MMNV. A deep computational study with several test problems are performed to study the performance of the crisp models and the solution approaches. According to the results, the obtained crisp formulations are highly sensitive to the changes in the value of the cost parameters. On the other hand, Niroomand and MMNV solution approaches perform better than other solution approaches from the solution quality point of view.},
  archive      = {J_EXSY},
  author       = {Ali Mahmoodirad and Sadegh Niroomand},
  doi          = {10.1111/exsy.12558},
  journal      = {Expert Systems},
  month        = {10},
  number       = {5},
  pages        = {e12558},
  shortjournal = {Expert Syst.},
  title        = {Uncertain location–allocation decisions for a bi-objective two-stage supply chain network design problem with environmental impacts},
  volume       = {37},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A machine learning approach for imputation and anomaly
detection in IoT environment. <em>EXSY</em>, <em>37</em>(5), e12556. (<a
href="https://doi.org/10.1111/exsy.12556">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The problem of anomaly and attack detection in IoT environment is one of the prime challenges in the domain of internet of things that requires an immediate concern. For example, anomalies and attacks in IoT environment such as scan, malicious operation, denial of service, spying, data type probing, wrong setup, malicious control can lead to failure of an IoT system. Datasets generated in an IoT environment usually have missing values. The presence of missing values makes the classifier unsuitable for classification task. This article introduces (a) a novel imputation technique for imputation of missing data values (b) a classifier which is based on feature transformation to perform classification (c) imputation measure for similarity computation between any two instances that can also be used as similarity measure. The performance of proposed classifier is studied by using imputed datasets obtained through applying Kmeans, F-Kmeans and proposed imputation methods. Experiments are also conducted by applying existing and proposed classifiers on the imputed dataset obtained using proposed imputation technique. For experimental study in this article, we have used an open source dataset named distributed smart space orchestration system publicly available from Kaggle. Experiment results obtained are also validated using Wilcoxon non-parametric statistical test. It is proved that the performance of proposed approach is better when compared to existing classifiers when the imputation process is performed using F-Kmeans and K-Means imputation techniques. It is also observed that accuracies for attack classes scan, malicious operation, denial of service, spying, data type probing, wrong setup are 100% while it is 99% for malicious control attack class when the proposed imputation and classification technique are applied.},
  archive      = {J_EXSY},
  author       = {Radhakrishna Vangipuram and Rajesh Kumar Gunupudi and Veereswara Kumar Puligadda and Janaki Vinjamuri},
  doi          = {10.1111/exsy.12556},
  journal      = {Expert Systems},
  month        = {10},
  number       = {5},
  pages        = {e12556},
  shortjournal = {Expert Syst.},
  title        = {A machine learning approach for imputation and anomaly detection in IoT environment},
  volume       = {37},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Fast and improved backpropagation learning of multi-layer
artificial neural network using adaptive activation function.
<em>EXSY</em>, <em>37</em>(5), e12555. (<a
href="https://doi.org/10.1111/exsy.12555">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the conventional backpropagation (BP) learning algorithm used for the training of the connecting weights of the artificial neural network (ANN), a fixed slope−based sigmoidal activation function is used. This limitation leads to slower training of the network because only the weights of different layers are adjusted using the conventional BP algorithm. To accelerate the rate of convergence during the training phase of the ANN, in addition to updates of weights, the slope of the sigmoid function associated with artificial neuron can also be adjusted by using a newly developed learning rule. To achieve this objective, in this paper, new BP learning rules for slope adjustment of the activation function associated with the neurons have been derived. The combined rules both for connecting weights and slopes of sigmoid functions are then applied to the ANN structure to achieve faster training. In addition, two benchmark problems: classification and nonlinear system identification are solved using the trained ANN. The results of simulation-based experiments demonstrate that, in general, the proposed new BP learning rules for slope and weight adjustments of ANN provide superior convergence performance during the training phase as well as improved performance in terms of root mean square error and mean absolute deviation for classification and nonlinear system identification problems.},
  archive      = {J_EXSY},
  author       = {Sashmita Panda and Ganapati Panda},
  doi          = {10.1111/exsy.12555},
  journal      = {Expert Systems},
  month        = {10},
  number       = {5},
  pages        = {e12555},
  shortjournal = {Expert Syst.},
  title        = {Fast and improved backpropagation learning of multi-layer artificial neural network using adaptive activation function},
  volume       = {37},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Investigations on adaptive connectivity and shape prior
based fuzzy graph-cut colour image segmentation. <em>EXSY</em>,
<em>37</em>(5), e12554. (<a
href="https://doi.org/10.1111/exsy.12554">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image segmentation is a challenging problem in computer vision with wide application. It is a process which considers the similarity criterion required to separate an image into different homogenous connected regions. First, an Optimized Adaptive Connectivity and Shape Prior in Modified Graph Cut Segmentation method has been applied to handle the structural irregularities in images. Second, an Optimized Adaptive Connectivity and Shape Prior in Modified Fuzzy Graph Cut Segmentation (Opac-MFGseg) is proposed to partition the images based on feature values. In this method, a fuzzy rule based system is used with optimization algorithm to provide the information on how much a specific feature is involved in image boundaries. The graph obtained from this fuzzy approach is further used in adaptive shape prior in modified graph cuts framework. Moreover, this method supports moving images (videos). In such a situation, a fully dynamic method called Optimized Adaptive Connectivity and Shape Prior in Dynamic Fuzzy Graph Cut Segmentation (Opac-DFGseg) method is proposed for the image segmentation. The effectiveness of the Opac-MFGseg and Opac-DFGseg methods is tested in terms of average sensitivity, precision, area overlap measure, relative error, and accuracy and computation time.},
  archive      = {J_EXSY},
  author       = {Adonu Celestine and J Dinesh Peter},
  doi          = {10.1111/exsy.12554},
  journal      = {Expert Systems},
  month        = {10},
  number       = {5},
  pages        = {e12554},
  shortjournal = {Expert Syst.},
  title        = {Investigations on adaptive connectivity and shape prior based fuzzy graph-cut colour image segmentation},
  volume       = {37},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Ensemble feature selection in medical datasets: Combining
filter, wrapper, and embedded feature selection results. <em>EXSY</em>,
<em>37</em>(5), e12553. (<a
href="https://doi.org/10.1111/exsy.12553">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Feature selection is a process aimed at filtering out unrepresentative features from a given dataset, usually allowing the later data mining and analysis steps to produce better results. However, different feature selection algorithms use different criteria to select representative features, making it difficult to find the best algorithm for different domain datasets. The limitations of single feature selection methods can be overcome by the application of ensemble methods, combining multiple feature selection results. In the literature, feature selection algorithms are classified as filter, wrapper, or embedded techniques. However, to the best of our knowledge, there has been no study focusing on combining these three types of techniques to produce ensemble feature selection. Therefore, the aim here is to answer the question as to which combination of different types of feature selection algorithms offers the best performance for different types of medical data including categorical, numerical, and mixed data types. The experimental results show that a combination of filter (i.e., principal component analysis) and wrapper (i.e., genetic algorithms) techniques by the union method is a better choice, providing relatively high classification accuracy and a reasonably good feature reduction rate.},
  archive      = {J_EXSY},
  author       = {Chih-Wen Chen and Yi-Hong Tsai and Fang-Rong Chang and Wei-Chao Lin},
  doi          = {10.1111/exsy.12553},
  journal      = {Expert Systems},
  month        = {10},
  number       = {5},
  pages        = {e12553},
  shortjournal = {Expert Syst.},
  title        = {Ensemble feature selection in medical datasets: Combining filter, wrapper, and embedded feature selection results},
  volume       = {37},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Contracting in brazilian public administration: A machine
learning approach. <em>EXSY</em>, <em>37</em>(5), e12550. (<a
href="https://doi.org/10.1111/exsy.12550">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The risk of non-fulfilment of a contract can harm public administration or even interrupt public services. Therefore, models that assist manager decision making in the audit and control of contracts with a higher disqualification risk may be important tools, with economic and even social repercussions. In this article, public contracts are classified with respect to the risk of non-compliance with their terms of delivery. The quantitative tools used are statistical and machine learning models, similar to credit risk rating of loans. As dependent variables, the models use data found in electronic databases present in e-government implementations. A previously classified listing of suspended companies is used as a proxy for risky contracts, as it contains private companies which failed with their contractual obligations. The classification techniques utilized are logistic regression, k-nearest neighbours, discriminant analysis, support vector machine and random forests. Although the methods can be applied to any government with electronic procurement and contracts systems, Brazilian data is used to illustrate the benefits of contract governance for emerging economies. It is concluded that the credit rating techniques used directly apply to contractual risk in public administration. Considering real public administration contract data, the classification algorithm that generates the best performance is k-nearest neighbours.},
  archive      = {J_EXSY},
  author       = {Bruno M. Henrique and Vinicius A. Sobreiro and Herbert Kimura},
  doi          = {10.1111/exsy.12550},
  journal      = {Expert Systems},
  month        = {10},
  number       = {5},
  pages        = {e12550},
  shortjournal = {Expert Syst.},
  title        = {Contracting in brazilian public administration: A machine learning approach},
  volume       = {37},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Cascade convolutional neural network-long short-term memory
recurrent neural networks for automatic tonal and nontonal
preclassification-based indian language identification. <em>EXSY</em>,
<em>37</em>(5), e12544. (<a
href="https://doi.org/10.1111/exsy.12544">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This work presents an automatic tonal/nontonal preclassification-based Indian language identification (LID) system. Languages are firstly classified into tonal and nontonal categories, and then, individual languages are identified from the languages of the respective categories. This work proposes the use of pitch Chroma and formant features for this task, and also investigates how Mel-frequency Cepstral Coefficients (MFCCs) complement these features. It further explores block processing (BP), pitch synchronous analysis (PSA)- and glottal closure regions (GCRs)-based approaches for feature extraction, using syllables as basic units. Cascade convolutional neural network (CNN)-long short-term memory (LSTM) model using syllable-level features has been developed. National Institute of Technology Silchar language database (NITS-LD) and OGI-Multilingual Telephone Speech Corpus (OGI-MLTS) have been used for experimental validation. The proposed system based on the score combination of Cascade CNN-LSTM models of Chroma (extracted from BP method), first two formants and MFCCs (both extracted from GCR method) reports the highest accuracies. In the preclassification stage, the observed accuracies are 91%, 87.3%, and 85.1% for NITS-LD, for 30 s, 10 s, and 3 s test data respectively. For OGI-MLTS database, the respective accuracies are 86.7%, 83.1%, and 80.6%. That amounts to absolute improvements of 11.6%, 12.3%, and 13.9% for NITS-LD, and 12.5%, 11.9%, and 12.6% for OGI-MLTS database with respect to that of the baseline system. The proposed preclassification-based LID system shows improvements of 7.3%, 6.4%, and 7.4% for NITS-LD and 6.1%, 6.7%, and 7.2% for OGI-MLTS database over the baseline system for the three respective test data conditions.},
  archive      = {J_EXSY},
  author       = {Chuya China Bhanja and Mohammad A. Laskar and Rabul H. Laskar},
  doi          = {10.1111/exsy.12544},
  journal      = {Expert Systems},
  month        = {10},
  number       = {5},
  pages        = {e12544},
  shortjournal = {Expert Syst.},
  title        = {Cascade convolutional neural network-long short-term memory recurrent neural networks for automatic tonal and nontonal preclassification-based indian language identification},
  volume       = {37},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). An objective and interactive-information-based feedback
mechanism for the consensus-reaching process considering a non-support
degree for minority opinions. <em>EXSY</em>, <em>37</em>(5), e12543. (<a
href="https://doi.org/10.1111/exsy.12543">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The consensus-reaching process (CRP) to achieve higher unanimity and ensure common agreement before deriving a final decision has become an important procedure in group decision-making problems. The demand for high-quality decision results has motivated the development of large-scale group decision-making (LGDM). In such cases, the issue of minority opinion has gained awareness due to the related effects on enhancing consensus and decision quality. A minority opinion cannot exert an effect unless the majority attach importance to whether that opinion is supported or not. To reflect the effect of minority opinions on consensus, this paper establishes a LGDM framework with an objective and interactive-information-based feedback mechanism for the CRP. Given the natural forms of human expression, multi-granular linguistic information and a 2-tuple linguistic model are used. First, initial weights are objectively assigned to decision-makers (DMs) to weaken the impact of the majority. Subsequently, a non-support degree function is newly defined to reflect the extent to which other DMs dissent from a minority opinion. More importantly, feedback rules are constructed to make corresponding adjustments to the powers of discourse among all DMs in the attempt to reach consensus. Finally, the proposed three-phase LGDM framework is applied to new product development (NPD), and simulation experiments are conducted based on two algorithms to verify the framework&#39;s applicability and feasibility.},
  archive      = {J_EXSY},
  author       = {Ru-xin Nie and Zhang-peng Tian and Jian-qiang Wang and Han-yang Luo},
  doi          = {10.1111/exsy.12543},
  journal      = {Expert Systems},
  month        = {10},
  number       = {5},
  pages        = {e12543},
  shortjournal = {Expert Syst.},
  title        = {An objective and interactive-information-based feedback mechanism for the consensus-reaching process considering a non-support degree for minority opinions},
  volume       = {37},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Neutrosophic structured element. <em>EXSY</em>,
<em>37</em>(5), e12542. (<a
href="https://doi.org/10.1111/exsy.12542">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a new concept in neutrosophic sets (NS) called neutrosophic structured element (NSE). Based on this concept, we define the operational laws, score function, and some aggregation operators of NS. Finally, as an application of this concept, we propose a decision-making method for a multi-attribute decision making (MADM) problem under NSE information. The results indicate that this concept is a useful tool for dealing with neutrosophic decision problems.},
  archive      = {J_EXSY},
  author       = {Seyyed Ahmad Edalatpanah},
  doi          = {10.1111/exsy.12542},
  journal      = {Expert Systems},
  month        = {10},
  number       = {5},
  pages        = {e12542},
  shortjournal = {Expert Syst.},
  title        = {Neutrosophic structured element},
  volume       = {37},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Allocating the fixed cost based on data envelopment analysis
in view of the shapley value. <em>EXSY</em>, <em>37</em>(5), e12539. (<a
href="https://doi.org/10.1111/exsy.12539">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper considers fixed cost allocation in view of cooperative game theory and proposes an approach based on data envelopment analysis while incorporating the perspectives of coalition efficiency and the Shapley value. To do this, we first build two models to evaluate coalition efficiencies before and after cost allocation, and we prove that all coalitions can be efficient after fixed cost allocation. Then, following the premise that each coalition makes itself efficient without reducing the efficiencies of other decision-making units&#39; preallocation efficiency, we propose a model that determines the acceptable range of each coalition&#39;s allocated fixed cost. Furthermore, a model is constructed to determine the final cost allocation based on three principles: efficiency, monotonicity, and similarity. Moreover, the Shapley value is employed to obtain the cost allocated to each decision-making unit (DMU). The proposed approach considers the relationships among DMUs across their forming coalitions to determine their interaction types and then generates a fixed cost allocation result that possesses the features of the Shapley value. This process makes the fixed cost allocation more acceptable. Finally, a simple numerical example and an empirical case are provided to illustrate the calculation process of the proposed approach and compare our approach with the traditional methods.},
  archive      = {J_EXSY},
  author       = {Fanyong Meng and Li Wu and Junfei Chu},
  doi          = {10.1111/exsy.12539},
  journal      = {Expert Systems},
  month        = {10},
  number       = {5},
  pages        = {e12539},
  shortjournal = {Expert Syst.},
  title        = {Allocating the fixed cost based on data envelopment analysis in view of the shapley value},
  volume       = {37},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Mobile phone data statistics as a dynamic proxy indicator in
assessing regional economic activity and human commuting patterns.
<em>EXSY</em>, <em>37</em>(5), e12530. (<a
href="https://doi.org/10.1111/exsy.12530">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Various studies demonstrate that data on mobile phone use are useful when analysing problems in the fields of human activity or population dynamics, including tourism, transportation planning, public administration, etc. However, one of the biggest challenges is related to the restrictions contained in the General Data Protection Regulation that force the use of statistics about mobile operator client activities instead of allowing the analysis of mobile operator data. Therefore, a data analytics approach that does not involve information on the mobility of particular persons was developed, providing economically relevant data on aggregate mobility while protecting personal data. The activity data aggregation was conducted at 15-min intervals in the area of each cellular base station; “activity” is defined as the number of outgoing and incoming calls and sent and received text messages (short message service or SMS) and, in some instances, as the count of unique users. The case study examines all of Latvia&#39;s municipalities, analysing the economic activity level in each municipality in comparison to the mobile phone activity in three periods: 2015–2016, 2017, and 2018. It was concluded that the economic activity in municipalities can be estimated, and positive dynamics of regional development have been detected. Such data and the data analytics method, which provides an understanding of how economic activities evolve in real time in particular locations and economic activity centres, can improve regional development planning and plan implementation. In order to assess which are the centres of economic activity in each municipality and its sphere of influence, the patterns of human commuting and fluctuations of internal activity on workdays and weekends/holidays in 2017–2018 were determined. In general, there is a shortage of reliable data on human commuting within Latvia and its specific regions; therefore, the method described here provides a practical tool for regional governments to keep track of strategy implementation and for strategic gap analysis.},
  archive      = {J_EXSY},
  author       = {Irina Arhipova and Gundars Berzins and Edgars Brekis and Juris Binde and Martins Opmanis and Aldis Erglis and Evija Ansonska},
  doi          = {10.1111/exsy.12530},
  journal      = {Expert Systems},
  month        = {10},
  number       = {5},
  pages        = {e12530},
  shortjournal = {Expert Syst.},
  title        = {Mobile phone data statistics as a dynamic proxy indicator in assessing regional economic activity and human commuting patterns},
  volume       = {37},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Online eigenvector transformation reflecting concept drift
for improving network intrusion detection. <em>EXSY</em>,
<em>37</em>(5), e12477. (<a
href="https://doi.org/10.1111/exsy.12477">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Currently, large data streams are constantly being generated in diverse environments, and continuous storage of the data and periodic batch-type principal component analysis (PCA) are becoming increasingly difficult. Various online PCA algorithms have been proposed to solve this problem. In this study, we propose an online PCA methodology based on online eigenvector transformation with the moving average of the data stream that can reflect concept drift. We compared the network intrusion detection performance based on online transformation of eigenvectors with that of offline methods by applying three machine learning algorithms. Both online and offline methods demonstrated excellent performance in terms of precision. However, in terms of the recall ratio, the performance of the proposed methodology with integrated online eigenvector transformation was better; thus, the F1-measure also indicated better performance. The visualization of the principal component score shows the effectiveness of our method.},
  archive      = {J_EXSY},
  author       = {Seongchul Park and Sanghyun Seo and Changhoon Jeong and Juntae Kim},
  doi          = {10.1111/exsy.12477},
  journal      = {Expert Systems},
  month        = {10},
  number       = {5},
  pages        = {e12477},
  shortjournal = {Expert Syst.},
  title        = {Online eigenvector transformation reflecting concept drift for improving network intrusion detection},
  volume       = {37},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A method for outlier detection based on cluster analysis and
visual expert criteria. <em>EXSY</em>, <em>37</em>(5), e12473. (<a
href="https://doi.org/10.1111/exsy.12473">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Outlier detection is an important problem occurring in a wide range of areas. Outliers are the outcome of fraudulent behaviour, mechanical faults, human error, or simply natural deviations. Many data mining applications perform outlier detection, often as a preliminary step in order to filter out outliers and build more representative models. In this paper, we propose an outlier detection method based on a clustering process. The aim behind the proposal outlined in this paper is to overcome the specificity of many existing outlier detection techniques that fail to take into account the inherent dispersion of domain objects. The outlier detection method is based on four criteria designed to represent how human beings (experts in each domain) visually identify outliers within a set of objects after analysing the clusters. This has an advantage over other clustering-based outlier detection techniques that are founded on a purely numerical analysis of clusters. Our proposal has been evaluated, with satisfactory results, on data (particularly time series) from two different domains: stabilometry, a branch of medicine studying balance-related functions in human beings and electroencephalography (EEG), a neurological exploration used to diagnose nervous system disorders. To validate the proposed method, we studied method outlier detection and efficiency in terms of runtime. The results of regression analyses confirm that our proposal is useful for detecting outlier data in different domains, with a false positive rate of less than 2% and a reliability greater than 99%.},
  archive      = {J_EXSY},
  author       = {Juan A. Lara and David Lizcano and Víctor Rampérez and Javier Soriano},
  doi          = {10.1111/exsy.12473},
  journal      = {Expert Systems},
  month        = {10},
  number       = {5},
  pages        = {e12473},
  shortjournal = {Expert Syst.},
  title        = {A method for outlier detection based on cluster analysis and visual expert criteria},
  volume       = {37},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). An efficient hybrid similarity measure based on user
interests for recommender systems. <em>EXSY</em>, <em>37</em>(5),
e12471. (<a href="https://doi.org/10.1111/exsy.12471">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recommender systems are used to suggest items to users based on their interests. They have been used widely in various domains, including online stores, web advertisements, and social networks. As part of their process, recommender systems use a set of similarity measurements that would assist in finding interesting items. Although many similarity measurements have been proposed in the literature, they have not concentrated on actual user interests. This paper proposes a new efficient hybrid similarity measure for recommender systems based on user interests. This similarity measure is a combination of two novel base similarity measurements: the user interest–user interest similarity measure and the user interest–item similarity measure. This hybrid similarity measure improves the existing work in three aspects. First, it improves the current recommender systems by using actual user interests. Second, it provides a comprehensive evaluation of an efficient solution to the cold start problem. Third, this similarity measure works well even when no corated items exist between two users. Our experiments show that our proposed similarity measure is efficient in terms of accuracy, execution time, and applicability. Specifically, our proposed similarity measure achieves a mean absolute error (MAE) as low as 0.42, with 64% applicability and an execution time as low as 0.03 s, whereas the existing similarity measures from the literature achieve an MAE of 0.88 at their best; these results demonstrate the superiority of our proposed similarity measure in terms of accuracy, as well as having a high applicability percentage and a very short execution time.},
  archive      = {J_EXSY},
  author       = {Bilal Hawashin and Mohammad Lafi and Tarek Kanan and Ayman Mansour},
  doi          = {10.1111/exsy.12471},
  journal      = {Expert Systems},
  month        = {10},
  number       = {5},
  pages        = {e12471},
  shortjournal = {Expert Syst.},
  title        = {An efficient hybrid similarity measure based on user interests for recommender systems},
  volume       = {37},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A proposal for a 360° information system model for private
health care organizations. <em>EXSY</em>, <em>37</em>(5), e12420. (<a
href="https://doi.org/10.1111/exsy.12420">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {At a time when communication, new media, and digitalization are transversal to the whole of society, private health care organizations have the possibility of making their business processes evolve. The objective is thus to seize the benefits associated to the active use of patients&#39; electronic health records (EHRs) as the basis for personalized health care. In order to initially validate the health care sector acceptance of a 360° health care information system (HIS), focused on collecting patients&#39; data to create the necessary knowledge for delivering personalized health care procedures and initiatives, a focus group involving a set of health-related professionals was performed. Despite recognizing the immense possibilities associated to EHR and its direct incorporation on a 360° HIS, the referred professionals still highlighted their concerns relative to the maintenance of adequate security and privacy levels. With this in mind, a proposal for a 360° HIS model is presented, and its main functional blocks are described with a focus on triggering patient/customer loyalty.},
  archive      = {J_EXSY},
  author       = {Duarte Magalhães and José Martins and Frederico Branco and Manuel Au-Yong-Oliveira and Ramiro Gonçalves and Fernando Moreira},
  doi          = {10.1111/exsy.12420},
  journal      = {Expert Systems},
  month        = {10},
  number       = {5},
  pages        = {e12420},
  shortjournal = {Expert Syst.},
  title        = {A proposal for a 360° information system model for private health care organizations},
  volume       = {37},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Dynamic framework to mining internet of things for
multimedia services. <em>EXSY</em>, <em>37</em>(5), e12404. (<a
href="https://doi.org/10.1111/exsy.12404">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The rapid and unprecedented technological advancements are currently dominated by two technologies. At one hand, we witness the rise of the Internet of Things (IoT) as the next evolution of the Internet. At the other hand, we witness a vast spread of social networks that connects people together socially and opens the door for people to share and express ideas, thoughts, and information. IoT is overpopulated by a vast number of objects, millions of multimedia services, and interactions. Therefore, the search of the right object that can provide the specific multimedia service is considered as an important issue. The merge of these two technologies resulted in new paradigm called Social IoT (SIoT). The main idea in SIoT is that every object can mine IoT in search for certain multimedia service. We investigate the issue of friends&#39; management in SIoT and propose a framework to manage friends&#39; requests. The proposed framework employs several mechanisms to better manage friends&#39; relationships. The proposed framework consists of friend selection, friendship removal, and an update module. It proposes a weight-based algorithm and Naïve Bayes Classifier-based algorithm for the selection component. Moreover, a random service allocation model is proposed to construct service-specific network model. This model is then used in the simulation setup to examine the performance of different friends&#39; management algorithms. The performance of the proposed framework is evaluated using simulation under different scenarios. The obtained simulation results show improvement over other strategies in terms of average degree of connections, average path length, local cluster coefficients, and throughput.},
  archive      = {J_EXSY},
  author       = {Yaser Khamayseh and Wail Mardini and J. William Atwood and Monther Aldwairi},
  doi          = {10.1111/exsy.12404},
  journal      = {Expert Systems},
  month        = {10},
  number       = {5},
  pages        = {e12404},
  shortjournal = {Expert Syst.},
  title        = {Dynamic framework to mining internet of things for multimedia services},
  volume       = {37},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Tribute to professor miltos petridis. <em>EXSY</em>,
<em>37</em>(4), e12600. (<a
href="https://doi.org/10.1111/exsy.12600">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_EXSY},
  doi          = {10.1111/exsy.12600},
  journal      = {Expert Systems},
  month        = {8},
  number       = {4},
  pages        = {e12600},
  shortjournal = {Expert Syst.},
  title        = {Tribute to professor miltos petridis},
  volume       = {37},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Special issue on “machine learning challenges and
applications for industry 4.0.” <em>EXSY</em>, <em>37</em>(4), e12595.
(<a href="https://doi.org/10.1111/exsy.12595">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_EXSY},
  author       = {Victor Rodriguez-Fernandez and David Camacho},
  doi          = {10.1111/exsy.12595},
  journal      = {Expert Systems},
  month        = {8},
  number       = {4},
  pages        = {e12595},
  shortjournal = {Expert Syst.},
  title        = {Special issue on “Machine learning challenges and applications for industry 4.0”},
  volume       = {37},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020b). New data envelopment analysis models for assessing
sustainability part 2: A static data envelopment analysis approach.
<em>EXSY</em>, <em>37</em>(4), e12549. (<a
href="https://doi.org/10.1111/exsy.12549">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_EXSY},
  author       = {Reza Farzipoor Saen and Malin Song and Ron Fisher},
  doi          = {10.1111/exsy.12549},
  journal      = {Expert Systems},
  month        = {8},
  number       = {4},
  pages        = {e12549},
  shortjournal = {Expert Syst.},
  title        = {New data envelopment analysis models for assessing sustainability part 2: A static data envelopment analysis approach},
  volume       = {37},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Data envelopment analysis and robust optimization: A review.
<em>EXSY</em>, <em>37</em>(4), e12534. (<a
href="https://doi.org/10.1111/exsy.12534">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper reviews the milestone approaches for handling uncertainty in data envelopment analysis (DEA). This paper presents the detailed classifications of robust data envelopment analysis (RDEA). RDEA is appropriate for measuring the efficiencies of decision-making units in the presence of the data and distributional uncertainties. This paper reviews scenario-based and uncertainty set of DEA models. It covers 73 studies from 2008 to 2019. The paper concludes with suggestions about the guidelines for future researches in the field of RDEA.},
  archive      = {J_EXSY},
  author       = {Pejman Peykani and Emran Mohammadi and Reza Farzipoor Saen and Seyed Jafar Sadjadi and Mohsen Rostamy-Malkhalifeh},
  doi          = {10.1111/exsy.12534},
  journal      = {Expert Systems},
  month        = {8},
  number       = {4},
  pages        = {e12534},
  shortjournal = {Expert Syst.},
  title        = {Data envelopment analysis and robust optimization: A review},
  volume       = {37},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Design of highly effective multilayer feedforward neural
network by using genetic algorithm. <em>EXSY</em>, <em>37</em>(4),
e12532. (<a href="https://doi.org/10.1111/exsy.12532">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a highly effective and precise neural network method for choosing the activation functions (AFs) and tuning the learning parameters (LPs) of a multilayer feedforward neural network by using a genetic algorithm (GA). The performance of the neural network mainly depends on the learning algorithms and the network structure. The backpropagation learning algorithm is used for tuning the network connection weights, and the LPs are obtained by the GA to provide both fast and reliable learning. Also, the AFs of each neuron in the network are automatically chosen by a GA. The present study consists of 10 different functions to accomplish a better convergence of the desired input–output mapping. Test studies are performed to solve a set of two-dimensional regression problems for the proposed genetic-based neural network (GNN) and conventional neural network having sigmoid AFs and constant learning parameters. The proposed GNN has also been tested by applying it to three real problems in the fields of environment, medicine, and economics. Obtained results prove that the proposed GNN is more effective and reliable when compared with the classical neural network structure.},
  archive      = {J_EXSY},
  author       = {Oğuz Üstün and Erdal Bekiroğlu and Mithat Önder},
  doi          = {10.1111/exsy.12532},
  journal      = {Expert Systems},
  month        = {8},
  number       = {4},
  pages        = {e12532},
  shortjournal = {Expert Syst.},
  title        = {Design of highly effective multilayer feedforward neural network by using genetic algorithm},
  volume       = {37},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A novel completeness definition of event logs and
corresponding generation algorithm. <em>EXSY</em>, <em>37</em>(4),
e12529. (<a href="https://doi.org/10.1111/exsy.12529">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As the promotion of technologies and applications of Big Data, the research of business process management (BPM) has gradually deepened to consider the impacts and challenges of big business data on existing BPM technologies. Recently, parallel business process mining (e.g. discovering business models from business visual data, integrating runtime business data with interactive business process monitoring visualisation systems and summarising and visualising historical business data for further analysis, etc.) and multi-perspective business data analytics (e.g. pattern detecting, decision-making and process behaviour predicting, etc.) have been intensively studied considering the steep increase in business data size and type. However, comprehensive and in-depth testing is needed to ensure their quality. Testing based solely on existing business processes and their system logs is far from sufficient. Large-scale randomly generated models and corresponding complete logs should be used in testing. To test parallel algorithms for discovering process models, different log completeness and generation algorithms were proposed. However, they suffer from either state space explosion or non-full-covering task dependencies problem. Besides, most existing generation algorithms rely on random executing strategy, which leads to low and unstable efficiency. In this paper, we propose a novel log completeness type, that is, #TAR completeness, as well as its generation algorithm. The experimental results based on a series of randomly generated process models show that the #TAR complete logs outperform the state-of-the-art ones with lower capacity, fuller dependencies covering and higher generating efficiency.},
  archive      = {J_EXSY},
  author       = {Chuanyi Li and Jidong Ge and Lijie Wen and Li Kong and Victor Chang and Liguo Huang and Bin Luo},
  doi          = {10.1111/exsy.12529},
  journal      = {Expert Systems},
  month        = {8},
  number       = {4},
  pages        = {e12529},
  shortjournal = {Expert Syst.},
  title        = {A novel completeness definition of event logs and corresponding generation algorithm},
  volume       = {37},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Enriched latent dirichlet allocation for sentiment analysis.
<em>EXSY</em>, <em>37</em>(4), e12527. (<a
href="https://doi.org/10.1111/exsy.12527">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {One of the main benefits of unsupervised learning is that there is no need for labelled data. As a method of this category, latent Dirichlet allocation (LDA) estimates the semantic relations between the words of the text effectively and can play an important role in solving various issues, including emotional analysis in combination with other parameters. In this study, three novel topic models called date sentiment LDA (DSLDA), author–date sentiment LDA (ADSLDA), and pack–author–date sentiment LDA (PADSLDA) are proposed. The proposed models extend LDA through some extra parameters such as date, author, helpfulness, sentiment, and subtopic. The proposed models use helpfulness in the Gibbs sampling algorithm. Helpfulness is a part of readers who found the review helpful. The proposed models divide the words into two categories: the words more affected by the distribution of subtopic and the words more affected by the main topic. In this study, a new concept called pack is introduced, and a new model called PADSLDA is proposed for sentiment analysis at pack level. The proposed models outperformed the baseline models because according to evaluations results, the extra parameters can appropriately affect the generating process of words in a review. Sentiment analysis at the document level, perplexity, and topic coherence are the main parameters used in the evaluations.},
  archive      = {J_EXSY},
  author       = {Amjad Osmani and Jamshid Bagherzadeh Mohasefi and Farhad Soleimanian Gharehchopogh},
  doi          = {10.1111/exsy.12527},
  journal      = {Expert Systems},
  month        = {8},
  number       = {4},
  pages        = {e12527},
  shortjournal = {Expert Syst.},
  title        = {Enriched latent dirichlet allocation for sentiment analysis},
  volume       = {37},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A supervised learning approach for heading detection.
<em>EXSY</em>, <em>37</em>(4), e12520. (<a
href="https://doi.org/10.1111/exsy.12520">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As the popularity of the portable document format (PDF) file format increases, research that facilitates PDF text analysis or extraction is necessary. Heading detection is a crucial component of PDF-based text classification processes. This research involves training a supervised learning model to detect headings by systematically testing and selecting classifier features using recursive feature elimination . Results indicate that decision tree is the best classifier with an accuracy of 95.83%, sensitivity of 0.981, and a specificity of 0.946. This research into heading detection contributes to the field of PDF-based text extraction and can be applied to the automation of large scale PDF text analysis in a variety of professional and policy-based contexts.},
  archive      = {J_EXSY},
  author       = {Sahib Singh Budhiraja and Vijay Mago},
  doi          = {10.1111/exsy.12520},
  journal      = {Expert Systems},
  month        = {8},
  number       = {4},
  pages        = {e12520},
  shortjournal = {Expert Syst.},
  title        = {A supervised learning approach for heading detection},
  volume       = {37},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A survey on semanticized and personalized health recommender
systems. <em>EXSY</em>, <em>37</em>(4), e12519. (<a
href="https://doi.org/10.1111/exsy.12519">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Health 3.0 is a health-related extension of the Web 3.0 concept. It is based on the semantic Web which provides for semantically organizing electronic health records of individuals. Health 3.0 is rapidly gaining ground as a new research topic in many academic and industrial disciplines. Due to the recent rapid spread of wearable sensors and smart devices with access to social media, migrating health services from the traditional centre-based health system to personal health care is inevitable. In this current era of greater personalization, treating patients&#39; health problems according to their profile and medical data gathered is possible using the latest information technologies. Consequently, personalized health recommender systems have gained importance. Empowering the utility of advanced Web technology in personalized health systems is still challenging due to pressing issues, such as lack of low cost and accurate smart medical sensors and wearable devices, existing investment in legacy Web system architecture in health sector, heterogeneity of medical data gathered by myriad health care institutions and isolated health services, and interoperability issues as well as multi-dimensionality of medical data. By tracing recent developments, this paper offers a systematic review through recent research on semantic Web-enabled personalized health systems, namely, semanticized personalized health recommender systems with the key enabling technologies, major applications, and successful case studies. Critical questions derived from the research studies were discussed, and main directions of open issues were identified leading to recommendations for future study in the field of personalized health recommender systems.},
  archive      = {J_EXSY},
  author       = {Duygu Çelik Ertuğrul and Atilla Elçi},
  doi          = {10.1111/exsy.12519},
  journal      = {Expert Systems},
  month        = {8},
  number       = {4},
  pages        = {e12519},
  shortjournal = {Expert Syst.},
  title        = {A survey on semanticized and personalized health recommender systems},
  volume       = {37},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Simulating and modelling the DAX index and the USO etf
financial time series by using a simple agent-based learning
architecture. <em>EXSY</em>, <em>37</em>(4), e12516. (<a
href="https://doi.org/10.1111/exsy.12516">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This work presents an extensive case study on modelling the DAX (Deutscher Aktienindex) index and United States Oil Fund (USO) exchange-traded fund (Etf) time series with the financial agent-based system learning financial agent-based simulator (L-FABS) that exploits simulated annealing as a learning method. The USO Etf time series is highly correlated with oil price behaviour, and the DAX index is based on the weighted and accumulated behaviour of the share prices of some of the largest companies traded on the Frankfurt Stock Exchange. These two time series are driven by completely different economic factors and thus provide two diverse empirical settings to evaluate the effectiveness of our methodology. Our experimentation shows that a relatively simple computational representation of real financial markets is effective in capturing the overall behaviour of the time series with varying approximation levels while the prediction target is moved into the future. The reported experimental investigation of L-FABS shows that it is robust notwithstanding the learning method used and the data sets exploited. L-FABS indeed produced a relatively low approximation error in several settings even when evaluated with respect to other modelling approaches, for example, 0.88% and 1.61% errors on average for 1 day ahead experiments in, respectively, DAX index and USO Etf.},
  archive      = {J_EXSY},
  author       = {Filippo Neri and Iván García-Magariño},
  doi          = {10.1111/exsy.12516},
  journal      = {Expert Systems},
  month        = {8},
  number       = {4},
  pages        = {e12516},
  shortjournal = {Expert Syst.},
  title        = {Simulating and modelling the DAX index and the USO etf financial time series by using a simple agent-based learning architecture},
  volume       = {37},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A simplified neutrosophic multiplicative set-based TODIM
using water-filling algorithm for the determination of weights.
<em>EXSY</em>, <em>37</em>(4), e12515. (<a
href="https://doi.org/10.1111/exsy.12515">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to the technological developments around the world, the amount of information that researchers work on increases continuously. This information density contains incomplete and uncertain data that cannot be fully expressed with crisp numbers. Fuzzy sets, intuitionistic fuzzy sets, and neutrosophic sets are useful tools to manage such information, but these concepts use a symmetrical and uniform scale to express data, whereas real-life problems contain nonsymmetrical and non-uniform information. Intuitionistic multiplicative sets (IMSs) are effective tools for dealing with these real-life problems. However, IMSs cannot handle real-life problems completely because indeterminate information depends on membership and non-membership information of IMSs, which is a restriction for decision makers and also for decision problems. To overcome this limitation, this paper generalizes the IMSs by using simplified neutrosophic set and introduces a novel approach that is called simplified neutrosophic multiplicative sets (SNMSs). Firstly, we define SNMS, show their set-based operations, and then give a description of simplified neutrosophic multiplicative numbers (SNMNs). Based on SNMNs, we develop two simplified neutrosophic multiplicative aggregation operators on SNMNs that are called simplified neutrosophic multiplicative weighted arithmetic average operator and simplified neutrosophic multiplicative weighted geometric average operator. Furthermore, we define some simplified neutrosophic multiplicative distance measures. Finally, using a model based on water-filling algorithm for determining criteria weights, we give a numerical example to demonstrate the effectiveness of the introduced concept with the proposed simplified neutrosophic multiplicative simplified neutrosophic multiplicative-TODIM method.},
  archive      = {J_EXSY},
  author       = {Ali Köseoğlu and Rıdvan Şahin and Mehmet Merdan},
  doi          = {10.1111/exsy.12515},
  journal      = {Expert Systems},
  month        = {8},
  number       = {4},
  pages        = {e12515},
  shortjournal = {Expert Syst.},
  title        = {A simplified neutrosophic multiplicative set-based TODIM using water-filling algorithm for the determination of weights},
  volume       = {37},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Trend following deep q-learning strategy for stock trading.
<em>EXSY</em>, <em>37</em>(4), e12514. (<a
href="https://doi.org/10.1111/exsy.12514">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Computers and algorithms are widely used to help in stock market decision making. A few questions with regards to the profitability of algorithms for stock trading are can computers be trained to beat the markets? Can an algorithm take decisions for optimal profits? And so forth. In this research work, our objective is to answer some of these questions. We propose an algorithm using deep Q-Reinforcement Learning techniques to make trading decisions. Trading in stock markets involves potential risk because the price is affected by various uncertain events ranging from political influences to economic constraints. Models that trade using predictions may not always be profitable mainly due to the influence of various unknown factors in predicting the future stock price. Trend Following is a trading idea in which, trading decisions, like buying and selling, are taken purely according to the observed market trend. A stock trend can be up, down, or sideways. Trend Following does not predict the stock price but follows the reversals in the trend direction. A trend reversal can be used to trigger a buy or a sell of a certain stock. In this research paper, we describe a deep Q-Reinforcement Learning agent able to learn the Trend Following trading by getting rewarded for its trading decisions. Our results are based on experiments performed on the actual stock market data of the American and the Indian stock markets. The results indicate that the proposed model outperforms forecasting-based methods in terms of profitability. We also limit risk by confirming trading actions with the trend before actual trading.},
  archive      = {J_EXSY},
  author       = {Jagdish Chakole and Manish Kurhekar},
  doi          = {10.1111/exsy.12514},
  journal      = {Expert Systems},
  month        = {8},
  number       = {4},
  pages        = {e12514},
  shortjournal = {Expert Syst.},
  title        = {Trend following deep Q-learning strategy for stock trading},
  volume       = {37},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Study of haze emission efficiency based on new co-opetition
data envelopment analysis. <em>EXSY</em>, <em>37</em>(4), e12466. (<a
href="https://doi.org/10.1111/exsy.12466">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As haze intensifies in China, controlling haze emission has become the country&#39;s top priority for environmental protection. Because haze moves across different regions, it is necessary to develop a data envelopment analysis (DEA) model underpinned by both competition and cooperation to evaluate the haze emission efficiency in different provinces. This study innovatively adopts the spatial econometrics to construct the co-opetition matrices of Chinese provinces, then builds the co-opetition DEA model to evaluate the haze emission efficiency of them, and finally uses the haze data of 2015 as an example to assess the applicability of the model. The results of the study include the following: First, compared with the traditional CCR (A. Charnes &amp; W. W. Cooper &amp; E. Rhodes) model, this study constructs the co-opetition DEA cross-efficiency model that integrates haze&#39;s feature of cross-border moving; thus, it is more in line with the reality of haze emission and movement. Second, compared with the efficiency value gained from the CCR model, the haze emission efficiency values for Tianjin and Guangdong, two decision-making units, register greater variance when using the DEA model. The reason might lie in that they have a different spatial transportation relationship with their surrounding provinces. Third, the haze emission efficiency of provinces, according to the evaluation based on the co-opetition DEA method, varies greatly: Those with high efficiency are mostly inland provinces with slow economic growth and adverse climatic conditions, whereas many of the provinces with low efficiency are located in the relatively prosperous East China. The specific co-opetition DEA model constructed in this study enriches the research on the DEA model, which can be applied to the emission efficiency evaluation of similar pollutants around the world and can contribute empirical support to the haze reducing efforts of the government with its empirical results.},
  archive      = {J_EXSY},
  author       = {Xianhua Wu and Yufeng Chen and Peng Zhao and Ji Guo and Zhanxin Ma},
  doi          = {10.1111/exsy.12466},
  journal      = {Expert Systems},
  month        = {8},
  number       = {4},
  pages        = {e12466},
  shortjournal = {Expert Syst.},
  title        = {Study of haze emission efficiency based on new co-opetition data envelopment analysis},
  volume       = {37},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A hybrid data envelopment analysis and multi-attribute
decision making approach to sustainability assessment. <em>EXSY</em>,
<em>37</em>(4), e12347. (<a
href="https://doi.org/10.1111/exsy.12347">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The concept of sustainability consists of three main dimensions: environmental, techno-economic, and social. Measuring the sustainability status of a system or technology is a significant challenge, especially when it needs to consider a large number of attributes in each dimension of sustainability. In this study, we first propose a hybrid approach, involving data envelopment analysis (DEA) and a multi-attribute decision making (MADM) methodologies, for computing an index for each dimension of sustainability, and then we define the overall sustainability index as the mean of the three measured indexes. Towards this end, we define new concepts of efficiency and cross-efficiency of order ( p , q ) where p and q are the number of inputs and outputs, respectively. For a given ( p , q ) , we address the problem of finding efficiency of order ( p , q ) by developing a novel DEA-based selecting method. Finally, we define the sustainability index as a weighted sum of all possible cross-efficiencies of order ( p , q ) . Form a computational viewpoint, the proposed selecting model significantly decreases the computational burden in comparison with the successive solving of traditional DEA models. A case study of the electricity-generation technologies in the United Kingdom is taken as a real-world example to illustrate the potential application of our method.},
  archive      = {J_EXSY},
  author       = {Esmaeil Keshavarz and Mehdi Toloo},
  doi          = {10.1111/exsy.12347},
  journal      = {Expert Systems},
  month        = {8},
  number       = {4},
  pages        = {e12347},
  shortjournal = {Expert Syst.},
  title        = {A hybrid data envelopment analysis and multi-attribute decision making approach to sustainability assessment},
  volume       = {37},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Cross-efficiency evaluation method based on the conservative
point of view. <em>EXSY</em>, <em>37</em>(4), e12336. (<a
href="https://doi.org/10.1111/exsy.12336">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traditional cross-efficiency evaluation models have ignored the problem that large differences may exist among cross-efficiencies, which may make decision making units (DMUs) unwilling to accept cross-efficiency evaluation results. Aimed at solving this problem, this paper proposes an altruism cross-efficiency model based on the conservative point of view. Compared with other cross-efficiency evaluation models, the proposed model mainly exhibits the following advantages. First, the proposed model no longer guarantees the DMU&#39;s self-evaluation efficiency, allowing the self-evaluation efficiency to adaptively change. Therefore, the peer-evaluation process is flexible and adaptable to the actual situation. Second, the proposed model abandons the traditional max-average secondary objective function and proposes to use a max–min objective function. Thus, the proposed model can maximize the peer-efficiency of the worst performing peer-DMU, achieving the effects of reducing the gaps among cross-efficiencies. Third, the cross-efficiency evaluation model is based on the conservative point of view, which helps the DMU to distinguish potential competitors among peer-DMUs. Lastly, to solve the non-linear model proposed in this paper, the algorithm is designed to describe how to solve the model. The case of a flexible manufacturing system is used to show the appropriateness of the suggested model and algorithm.},
  archive      = {J_EXSY},
  author       = {Jiasen Sun and Jie Wu and Yingming Wang and Lei Li and Yuhong Wang},
  doi          = {10.1111/exsy.12336},
  journal      = {Expert Systems},
  month        = {8},
  number       = {4},
  pages        = {e12336},
  shortjournal = {Expert Syst.},
  title        = {Cross-efficiency evaluation method based on the conservative point of view},
  volume       = {37},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Measuring environmental sustainability performance of
freight transportation seaports in china: A data envelopment analysis
approach based on the closest targets. <em>EXSY</em>, <em>37</em>(4),
e12334. (<a href="https://doi.org/10.1111/exsy.12334">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Because of China&#39;s rapid economic development, its freight transportation system has grown to become one of China&#39;s high-pollution-emission sectors. However, there are few studies that pay close attention to measuring and improving the environmental performance of China&#39;s freight transportation system, especially in regard to seaports. In this paper, data envelopment analysis (DEA) is applied to measure the environmental performance of freight transportation seaports. In addition, we also provide benchmarking information to point the way to improving environmental performance effectively. Our proposed DEA model is based on the closest targets, which satisfies the strong monotonicity and can yield the most relevant solution for the inefficient seaports. An empirical study of 21 of China&#39;s primary freight transportation seaports shows that most of them have relatively good environmental performance. Among the five coastal port groups, the Bohai-rim port group had the best environmental performance, whereas the Pearl River port group had the worst. Our data show significant differences between the best and worst performances, indicating that more measures should be taken to balance and coordinate the development between the five coastal port groups.},
  archive      = {J_EXSY},
  author       = {Xingchen Li and Feng Li and Nenggui Zhao and Qingyuan Zhu},
  doi          = {10.1111/exsy.12334},
  journal      = {Expert Systems},
  month        = {8},
  number       = {4},
  pages        = {e12334},
  shortjournal = {Expert Syst.},
  title        = {Measuring environmental sustainability performance of freight transportation seaports in china: A data envelopment analysis approach based on the closest targets},
  volume       = {37},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). The global system-ranking efficiency model and calculating
examples with consideration of the nonhomogeneity of decision-making
units. <em>EXSY</em>, <em>37</em>(4), e12272. (<a
href="https://doi.org/10.1111/exsy.12272">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data envelopment analyses have been widely used to evaluate the relative efficiency of decision-making units (DMUs). However, the traditional data envelopment analysis model has not considered the problem of DMUs&#39; nonhomogeneity. If nonhomogeneous DMUs are evaluated under the same production frontier, conclusions may not be precise. For example, some DMUs&#39; input redundancy and output deficit cannot be adjusted as per planning results, which may lead to mistakes in management. This paper loosens the assumption of DMU homogeneity and builds a global system-ranking efficiency model based on existing literature, which divides the problem of DMUs&#39; nonhomogeneity into external nonhomogeneity and internal homogeneity. Data have been collected from 114 listed enterprises in China&#39;s solar power industry, and the analysis results indicate that this paper&#39;s model is stable and reliable and can be used as a reference for production managers.},
  archive      = {J_EXSY},
  author       = {Shuhong Wang and Xiaoli Sun},
  doi          = {10.1111/exsy.12272},
  journal      = {Expert Systems},
  month        = {8},
  number       = {4},
  pages        = {e12272},
  shortjournal = {Expert Syst.},
  title        = {The global system-ranking efficiency model and calculating examples with consideration of the nonhomogeneity of decision-making units},
  volume       = {37},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020a). New data envelopment analysis models for assessing
sustainability part 1: A dynamic data envelopment analysis approach.
<em>EXSY</em>, <em>37</em>(3), e12548. (<a
href="https://doi.org/10.1111/exsy.12548">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_EXSY},
  author       = {Reza Farzipoor Saen and Malin Song and Ron Fisher},
  doi          = {10.1111/exsy.12548},
  journal      = {Expert Systems},
  month        = {6},
  number       = {3},
  pages        = {e12548},
  shortjournal = {Expert Syst.},
  title        = {New data envelopment analysis models for assessing sustainability part 1: A dynamic data envelopment analysis approach},
  volume       = {37},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A fuzzy-clustering-based hierarchical i-vector/probabilistic
linear discriminant analysis system for text-dependent speaker
verification. <em>EXSY</em>, <em>37</em>(3), e12524. (<a
href="https://doi.org/10.1111/exsy.12496">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the i-vector/probabilistic linear discriminant analysis (PLDA) technique, the PLDA backend classifier is modelled on i-vectors. PLDA defines an i-vector subspace that compensates the unwanted variability and helps to discriminate among speaker-phrase pairs. The channel or session variability manifested in i-vectors are known to be nonlinear in nature. PLDA training, however, assumes the variability to be linearly separable, thereby causing loss of important discriminating information. Besides, the i-vector estimation, itself, is known to be poor in case of short utterances. This paper attempts to address these issues using a simple hierarchy-based system. A modified fuzzy-clustering technique is employed to divide the feature space into more characteristic feature subspaces using vocal source features. Thereafter, a separate i-vector/PLDA model is trained for each of the subspaces. The sparser alignment owing to subspace-specific universal background model and the relatively reduced dimensions of variability in individual subspaces help to train more effective i-vector/PLDA models. Also, vocal source features are complementary to mel frequency cepstral coefficients, which are transformed into i-vectors using mixture model technique. As a consequence, vocal source features and i-vectors tend to have complementary information. Thus using vocal source features for classification in a hierarchy tree may help to differentiate some of the speaker-phrase classes, which otherwise are not easily discriminable based on i-vectors. The proposed technique has been validated on Part 1 of RSR2015 database, and it shows a relative equal error rate reduction of up to 37.41% with respect to the baseline i-vector/PLDA system.},
  archive      = {J_EXSY},
  author       = {Mohammad Azharuddin Laskar and Rabul Hussain Laskar},
  doi          = {10.1111/exsy.12496},
  journal      = {Expert Systems},
  month        = {6},
  number       = {3},
  pages        = {e12524},
  shortjournal = {Expert Syst.},
  title        = {A fuzzy-clustering-based hierarchical i-vector/probabilistic linear discriminant analysis system for text-dependent speaker verification},
  volume       = {37},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). An outranking method for multicriteria decision making with
probabilistic hesitant information. <em>EXSY</em>, <em>37</em>(3),
e12513. (<a href="https://doi.org/10.1111/exsy.12513">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Defects of hesitant fuzzy set (HFS) manifest in actual decision-making process, so adding probabilities to the values in HFS is necessary. The probabilistic HFS (PHFS) is a useful tool to describe the uncertainty of elements in HFS by introducing occurrence probabilities. However, some important issues in PHFS utilization remain to be addressed. In this study, an outranking method for multicriteria decision making (MCDM) with probabilistic hesitant information is presented. First, the binary relations between two probabilistic hesitant fuzzy elements (PHFEs) are defined on the basis of the elimination and choice translating reality method. Some outranking relations between the alternatives are then introduced. Second, we provide a Hausdorff distance between two PHFEs. The main characteristic of the proposed Hausdorff distance is that it does not require the same length and arrangement of the PHFEs. Third, a maximizing Hausdorff distance deviation method is developed to obtain the evaluation criteria weights under a probabilistic hesitant fuzzy environment. Finally, an illustrative example in conjunction with comparative analysis is used to demonstrate that the proposed method is feasible for practical MCDM problems.},
  archive      = {J_EXSY},
  author       = {Jian Li and Qiongxia Chen},
  doi          = {10.1111/exsy.12513},
  journal      = {Expert Systems},
  month        = {6},
  number       = {3},
  pages        = {e12513},
  shortjournal = {Expert Syst.},
  title        = {An outranking method for multicriteria decision making with probabilistic hesitant information},
  volume       = {37},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Air pollution forecasting based on attention-based LSTM
neural network and ensemble learning. <em>EXSY</em>, <em>37</em>(3),
e12511. (<a href="https://doi.org/10.1111/exsy.12511">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With air pollution having become a global concern, scientists are committed to working on its amelioration. In the field of air pollution prediction, there have been good results in experimental research so far, but few studies have integrated weather forecast information and the properties of air pollution drift. In this work, we propose a novel wind-sensitive attention mechanism with a long short-term memory (LSTM) neural network model to predict the air pollution - PM2.5 concentrations by considering the influence of wind direction and speed on the changes of spatial–temporal PM2.5 concentrations in neighbouring areas. Preliminary predictions for PM2.5 are then made by an LSTM neural network regarding neighbouring pollution; these predictions are “paid attention to” and we finally apply an ensemble learning method based on e X treme G radient B oosting (XGBoost) to combine the preliminary predictions with weather forecasting to make second phase predictions of PM2.5. The experiment is conducted using PM2.5 data and weather forecast data. Our results illustrate that the proposed method is superior to other methods in predicting PM2.5 concentrations, including multi-layer perceptron, support vector regression, LSTM neural network, and extreme gradient boosting algorithm.},
  archive      = {J_EXSY},
  author       = {Duen-Ren Liu and Shin-Jye Lee and Yang Huang and Chien-Ju Chiu},
  doi          = {10.1111/exsy.12511},
  journal      = {Expert Systems},
  month        = {6},
  number       = {3},
  pages        = {e12511},
  shortjournal = {Expert Syst.},
  title        = {Air pollution forecasting based on attention-based LSTM neural network and ensemble learning},
  volume       = {37},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Qualitative hesitant fuzzy group decision making: An
additively consistent probability and consensus-based perspective.
<em>EXSY</em>, <em>37</em>(3), e12510. (<a
href="https://doi.org/10.1111/exsy.12510">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hesitant fuzzy linguistic preference relations (HFLPRs) can efficiently denote the hesitant qualitative judgments of decision makers. Consistency and consensus are two critical topics in group decision making (GDM) with preference relations. This paper uses the additively consistent concept for linguistic fuzzy preference relations (LFPRs) to give an additive consistency definition for HFLPRs. To judge the additive consistency of HFLPRs, 0-1 mixed programming models (0-1-MPMs) are constructed. Meanwhile, additive-consistency-based 0-1-MPMs to ascertain missing values in incomplete HFLPRs are established. Following the consistent probability of LFPRs, an algorithm to calculate the linguistic priority weighting vector is presented. In consideration of the consensus of GDM, a consistency-probability-distance-measure-based consensus index is defined, and an interactive improving consensus method is provided. Finally, a method for GDM with HFLPRs is offered that can address incomplete and inconsistent cases. Meanwhile, numerical examples are offered, and comparative analysis is made.},
  archive      = {J_EXSY},
  author       = {Jie Tang and Fanyong Meng and Zeshui Xu and Ruiping Yuan},
  doi          = {10.1111/exsy.12510},
  journal      = {Expert Systems},
  month        = {6},
  number       = {3},
  pages        = {e12510},
  shortjournal = {Expert Syst.},
  title        = {Qualitative hesitant fuzzy group decision making: An additively consistent probability and consensus-based perspective},
  volume       = {37},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). BlockU: Extended usage control in and for blockchain.
<em>EXSY</em>, <em>37</em>(3), e12507. (<a
href="https://doi.org/10.1111/exsy.12507">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {An electronic business transaction among untrusted bodies without consulting a mutually trusted party has remained widely accepted problem. Blockchain resolves this problem by introducing peer-to-peer network with a consensus algorithm and trusted ledger. Blockchain originally introduced for cryptocurrency that came with proof-of-work consensus algorithm. Due to some performance issues, scientists brought concept of permissioned Blockchain. Hyperledger Fabric is a permissioned Blockchain targeting business-oriented problems for industry. It is designed for efficient transaction execution over Blockchain with pluggable consensus model; however, there is limitation of rapid application development. Hyperledger introduced a new layer called Hyperledger Composer on top of the Fabric layer, which provides an abstract layer to model the business application readily and quickly. Composer provides a smart contract to extend the functionality and flexibility of Fabric layer and provides a way of communication with other systems to meet business requirements. Hyperledger Composer uses role-based access control (RBAC) model to secure access to its valuable assets. However, RBAC is not enough because many business deals require continuous assets monitoring. Our proposed model, BlockU, covers all possible access control models required by a business. BlockU can monitor assets continuously during transactions and updates attributes accordingly. Moreover, we incorporate hooks in Hyperledger Composer to implement extended permission model that provides extensive permission management capability on an asset. Subsequently, our proposed enhanced access control model is implemented with a minimal change to existing Composer code base and is backward compatible with the current security mechanism.},
  archive      = {J_EXSY},
  author       = {Yasar Khan and Toqeer Ali and Megat Fariz and Fernando Moreira and Frederico Branco and José Martins and Ramiro Gonçalves},
  doi          = {10.1111/exsy.12507},
  journal      = {Expert Systems},
  month        = {6},
  number       = {3},
  pages        = {e12507},
  shortjournal = {Expert Syst.},
  title        = {BlockU: Extended usage control in and for blockchain},
  volume       = {37},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A class of monotone kernelized classifiers on the basis of
the choquet integral. <em>EXSY</em>, <em>37</em>(3), e12506. (<a
href="https://doi.org/10.1111/exsy.12506">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The key property of monotone classifiers is that increasing (decreasing) input values lead to increasing (decreasing) the output value. Preserving monotonicity for a classifier typically requires many constraints to be respected by modelling approaches such as artificial intelligence techniques. The type of constraints strongly depends on the modelling assumptions. Of course, for sophisticated models, such conditions might be very complex. In this study, we present a new family of kernels that we call it Choquet kernels. Henceforth, it allows for employing popular kernel-based methods, such as support vector machines. Instead of a naïve approach with exponential computational complexity, we propose an equivalent formulation with quadratic time in the number of attributes. Furthermore, because coefficients derived from kernel solutions are not necessarily monotone in the dual form, different approaches are proposed to monotonize coefficients. Finally, experiments illustrate beneficial properties of the Choquet kernels.},
  archive      = {J_EXSY},
  author       = {Ali Fallah Tehrani and Marc Strickert and Diane Ahrens},
  doi          = {10.1111/exsy.12506},
  journal      = {Expert Systems},
  month        = {6},
  number       = {3},
  pages        = {e12506},
  shortjournal = {Expert Syst.},
  title        = {A class of monotone kernelized classifiers on the basis of the choquet integral},
  volume       = {37},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Assessment of text coherence using an ontology-based
relatedness measurement method. <em>EXSY</em>, <em>37</em>(3), e12505.
(<a href="https://doi.org/10.1111/exsy.12505">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes a novel method for assessing text coherence. Central to this approach is an ontology-based representation of text, which captures the level of relatedness between consecutive sentences via ontologies. Our method encompasses annotating text using ontological concepts and assessing text coherence based on relatedness measurement among these concepts. The ontology-based relatedness measurement method used in this study considers various types of relationships in ontologies and derived relationships via an inference engine for computing relatedness. We hypothesized that rich variety of relationships and inferred facts in ontologies would improve the success of text coherence assessment. Our results demonstrate that the use of ontologies yields to coherence values that have a higher correlation with human ratings.},
  archive      = {J_EXSY},
  author       = {Görkem Giray and Murat Osman Ünalır},
  doi          = {10.1111/exsy.12505},
  journal      = {Expert Systems},
  month        = {6},
  number       = {3},
  pages        = {e12505},
  shortjournal = {Expert Syst.},
  title        = {Assessment of text coherence using an ontology-based relatedness measurement method},
  volume       = {37},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Endoscopy report mining for intelligent gastric cancer
screening. <em>EXSY</em>, <em>37</em>(3), e12504. (<a
href="https://doi.org/10.1111/exsy.12504">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Endoscopy is an important tool for gastric cancer screening. Due to the lack of effective decision support system for endoscopy, the detection of gastric cancer in the clinic is usually with low sensitivity. In this paper, we propose a Genetic Algorithm optimized Neural Network (GAoNN) approach for gastric cancer detection based on endoscopy reports mining. Considering the fact that gastric cancer sensitivity can significantly improve the 5-year survival rate of patients, both the prediction accuracy and the sensitivity are employed to construct a multiobjective optimization model for enhancing the classification performance of GAoNN. In particular, we extended an effective genetic algorithm Nondominated Sorting Genetic Algorithm II (NSGA-II) to train a neural network and reduced the complexity in training hyperparameters and improved the efficiency by substituting the computationally intensive stochastic gradient descent (SGD) algorithm in a neural network. Specifically, we designed the novel crossover and mutation operators and modified the nondominated ranking and crowding distance sorting procedures in NSGA-II for GAoNN. Through testing on 8,546 real-world endoscopy reports, we show that GAoNN achieves a prediction accuracy up to 83.74%, which is better than several competitors by significantly increasing sensitivity to 83.14%. GAoNN also reduces the training time by 30.94% when compared with conventional SGD-based training, which indicates the feasibility of GAoNN in clinical practice.},
  archive      = {J_EXSY},
  author       = {Jinxin Pan and Shuai Ding and Shanlin Yang and Gang Li and Xiao Liu},
  doi          = {10.1111/exsy.12504},
  journal      = {Expert Systems},
  month        = {6},
  number       = {3},
  pages        = {e12504},
  shortjournal = {Expert Syst.},
  title        = {Endoscopy report mining for intelligent gastric cancer screening},
  volume       = {37},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A robust and efficient convolutional deep learning framework
for age-invariant face recognition. <em>EXSY</em>, <em>37</em>(3),
e12503. (<a href="https://doi.org/10.1111/exsy.12503">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Extensive research has been carried out in the past on face recognition, face detection, and age estimation. However, age-invariant face recognition (AIFR) has not been explored that thoroughly. The facial appearance of a person changes considerably over time that results in introducing significant intraclass variations, which makes AIFR a very challenging task. Most of the face recognition studies that have addressed the ageing problem in the past have employed complex models and handcrafted features with strong parametric assumptions. In this work, we propose a novel deep learning framework that extracts age-invariant and generalized features from facial images of the subjects. The proposed model trained on facial images from a minor part (20–30%) of lifespan of subjects correctly identifies them throughout their lifespan. A variety of pretrained 2D convolutional neural networks are compared in terms of accuracy, time, and computational complexity to select the most suitable network for AIFR. Extensive experimental results are carried out on the popular and challenging face and gesture recognition network ageing dataset. The proposed method achieves promising results and outperforms the state-of-the-art AIFR models by achieving an accuracy of 99%, which proves the effectiveness of deep learning in facial ageing research.},
  archive      = {J_EXSY},
  author       = {Adeel Yousaf and Muhammad Junaid Khan and Muhammad Jaleed Khan and Adil M. Siddiqui and Khurram Khurshid},
  doi          = {10.1111/exsy.12503},
  journal      = {Expert Systems},
  month        = {6},
  number       = {3},
  pages        = {e12503},
  shortjournal = {Expert Syst.},
  title        = {A robust and efficient convolutional deep learning framework for age-invariant face recognition},
  volume       = {37},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Assessing stationarity in web analytics: A study of bounce
rates. <em>EXSY</em>, <em>37</em>(3), e12502. (<a
href="https://doi.org/10.1111/exsy.12502">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Evidence-based methods for evaluating marketing interventions such as A / B testing have become standard practice. However, the pitfalls associated with the misuse of this decision-making instrument are not well understood by managers and analytics professionals. In this study, we assess the impact of stationarity on the validity of samples from conditioned time series, which are abundant in web metrics. Such a prominent metric is the bounce rate, which is prevalent in assessing engagement with web content as well as the performance of marketing touchpoints. In this study, we show how to control for stationarity using an algorithmic transformation to calculate the optimum sampling period. This distance is based on a novel stationary ergodic process that considers that a stationary series presents reversible symmetric features and is calculated using a dynamic time warping algorithm in a self-correlation procedure. This study contributes to the expert and intelligent systems literature by demonstrating a robust method for sub-sampling time-series data, which are critical in decision making.},
  archive      = {J_EXSY},
  author       = {Marios Poulos and Nikolaos Korfiatis and Sozon Papavlassopoulos},
  doi          = {10.1111/exsy.12502},
  journal      = {Expert Systems},
  month        = {6},
  number       = {3},
  pages        = {e12502},
  shortjournal = {Expert Syst.},
  title        = {Assessing stationarity in web analytics: A study of bounce rates},
  volume       = {37},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A homogeneous ensemble method for predicting gastric cancer
based on gastroscopy reports. <em>EXSY</em>, <em>37</em>(3), e12499. (<a
href="https://doi.org/10.1111/exsy.12499">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Gastroscopy is important for finding suspicious stomach lesions, screening for gastric cancer, and providing early diagnoses. Due to the differences in the levels of diagnosis and treatment among gastroscope doctors, clinical diagnosis based on gastroscopy is limited by low diagnostic sensitivity and specificity to gastric cancer. An assistive system for gastroscopy report analysis can be helpful to improve the success rate of gastric cancer detection. In this study, a homogeneous ensemble decision support system for gastric cancer screening (Endo-GCS) that performs word segmentation, feature extraction, and gastric cancer screening on text-based gastroscopy reports is proposed. The proposed Endo-GCS method establishes a progressive local weighting algorithm that improves the overall prediction performance of the homogeneous ensemble model in gastric cancer screening. An optimal threshold estimation algorithm is developed to minimize the negative impact of misdiagnosis and missed diagnoses. Through a comparative experimental study using real gastroscopy report data, the pathological examination conclusion is the gold standard. The sensitivity of the proposed Endo-GCS method is 88.27%, the specificity is 77.84%, and the accuracy is 82.11%, which significantly improved the sensitivity 65.49% and the accuracy 80.5% of the gastroscopic diagnosis results, respectively.},
  archive      = {J_EXSY},
  author       = {Shuai Ding and Shikang Hu and Jinxin Pan and Xiaojian Li and Gang Li and Xiao Liu},
  doi          = {10.1111/exsy.12499},
  journal      = {Expert Systems},
  month        = {6},
  number       = {3},
  pages        = {e12499},
  shortjournal = {Expert Syst.},
  title        = {A homogeneous ensemble method for predicting gastric cancer based on gastroscopy reports},
  volume       = {37},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). An efficient neurodynamic model to solve nonconvex nonlinear
optimization problems and its applications. <em>EXSY</em>,
<em>37</em>(3), e12498. (<a
href="https://doi.org/10.1111/exsy.12498">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a recurrent neural network for solving nonconvex nonlinear optimization problems subject to nonlinear inequality constraints. First, the p -power transformation is exploited for local convexification of the Lagrangian function in nonconvex nonlinear optimization problem. Next, the proposed neural network is constructed based on the Karush–Kuhn–Tucker (KKT) optimality conditions and the projection function. An important property of this neural network is that its equilibrium point corresponds to the optimal solution of the original problem. By utilizing an appropriate Lyapunov function, it is shown that the proposed neural network is stable in the sense of Lyapunov and convergent to the global optimal solution of the original problem. Also, the sensitivity of the convergence is analysed by changing the scaling factors. Compared with other existing neural networks for such problem, the proposed neural network has more advantages such as high accuracy of the obtained solutions, fast convergence, and low complexity. Finally, simulation results are provided to show the benefits of the proposed model, which compare to or outperform existing models.},
  archive      = {J_EXSY},
  author       = {Mohammad Moghaddas and Ghasem Tohidi},
  doi          = {10.1111/exsy.12498},
  journal      = {Expert Systems},
  month        = {6},
  number       = {3},
  pages        = {e12498},
  shortjournal = {Expert Syst.},
  title        = {An efficient neurodynamic model to solve nonconvex nonlinear optimization problems and its applications},
  volume       = {37},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). EBNO: Evolution of cost-sensitive bayesian networks.
<em>EXSY</em>, <em>37</em>(3), e12495. (<a
href="https://doi.org/10.1111/exsy.12495">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The last decade has seen an increase in the attention paid to the development of cost-sensitive learning algorithms that aim to minimize misclassification costs while still maintaining accuracy. Most of this attention has been on cost-sensitive decision tree learning, whereas relatively little attention has been paid to assess if it is possible to develop better cost-sensitive classifiers based on Bayesian networks. Hence, this paper presents EBNO, an algorithm that utilizes Genetic algorithms to learn cost-sensitive Bayesian networks, where genes are utilized to represent the links between the nodes in Bayesian networks and the expected cost is used as a fitness function. An empirical comparison of the new algorithm has been carried out with respect to (a) an algorithm that induces cost-insensitive Bayesian networks to provide a base line, (b) ICET, a well-known algorithm that uses Genetic algorithms to induce cost-sensitive decision trees, (c) use of MetaCost to induce cost-sensitive Bayesian networks via bagging (d) use of AdaBoost to induce cost-sensitive Bayesian networks, and (e) use of XGBoost, a gradient boosting algorithm, to induce cost-sensitive decision trees. An empirical evaluation on 28 data sets reveals that EBNO performs well in comparison with the algorithms that produce single interpretable models and performs just as well as algorithms that use bagging and boosting methods.},
  archive      = {J_EXSY},
  author       = {Eman Nashnush and Sunil Vadera},
  doi          = {10.1111/exsy.12495},
  journal      = {Expert Systems},
  month        = {6},
  number       = {3},
  pages        = {e12495},
  shortjournal = {Expert Syst.},
  title        = {EBNO: Evolution of cost-sensitive bayesian networks},
  volume       = {37},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Analysis of the environmental efficiency in china based on
the DEA cross-efficiency approach under different policy objectives.
<em>EXSY</em>, <em>37</em>(3), e12461. (<a
href="https://doi.org/10.1111/exsy.12461">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The existing studies on environmental efficiency evaluation generally have the problem of efficiency overestimation. To solve this problem, a new data envelopment analysis (DEA) cross-efficiency approach with undesirable outputs is developed to evaluate environmental efficiency from the perspectives of both self-evaluation and peer evaluation. Then, three new evaluation strategies, namely, economic development strategy, environmental protection strategy, and win–win strategy, are proposed to reflect the needs of decision makers under different policy objectives. The proposed cross-efficiency approach with different evaluation strategies not only realizes the cross evaluation of environmental efficiency, but also guarantees the relative uniqueness of the optimal solution on the basis of the preferences of decision makers. Combining the metafrontier DEA approach and DEA window analysis, a new cross-efficiency analytical framework is constructed to gradually analyse the influences of policy objectives, technology heterogeneity, and dynamic correlation on the environmental efficiency. Subsequently, the environmental efficiency of China&#39;s economic development during 2006–2015 is in-depth analysed on the basis of the proposed analytical framework, and some interesting conclusions, and some useful suggestions are obtained.},
  archive      = {J_EXSY},
  author       = {Lei Chen and Fei-Mei Wu and Ying-Ming Wang and Mei-Juan Li},
  doi          = {10.1111/exsy.12461},
  journal      = {Expert Systems},
  month        = {6},
  number       = {3},
  pages        = {e12461},
  shortjournal = {Expert Syst.},
  title        = {Analysis of the environmental efficiency in china based on the DEA cross-efficiency approach under different policy objectives},
  volume       = {37},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Sustainability assessment of iranian petrochemical companies
in stock exchange: A data envelopment analysis-based approach.
<em>EXSY</em>, <em>37</em>(3), e12359. (<a
href="https://doi.org/10.1111/exsy.12359">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The assessment of efficiency is always of particular importance according to different indicators from different perspectives. There are various techniques for evaluating petrochemical companies, among which the data envelopment analysis technique is one of the best techniques that can be used to calculate the relative efficiency of a set of decision-making units with network structures. In the present paper, seven petrochemical companies listed in the Iranian stock exchange were analysed. These companies were evaluated in terms of financial performance and sustainable development, and their relative efficiency was calculated during 2015–2016. According to the obtained results, only Marun Petrochemical Co. was found to be efficient in all areas and years. The results also showed that four companies were efficient in financial terms over the period under study. In the general conclusion regarding the companies&#39; performance, Marun was ranked first, Jam was ranked second, and Zagros was ranked third.},
  archive      = {J_EXSY},
  author       = {Mohammad Fallah and Farhad Hosseinzadeh Lotfi},
  doi          = {10.1111/exsy.12359},
  journal      = {Expert Systems},
  month        = {6},
  number       = {3},
  pages        = {e12359},
  shortjournal = {Expert Syst.},
  title        = {Sustainability assessment of iranian petrochemical companies in stock exchange: A data envelopment analysis-based approach},
  volume       = {37},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). How does environmental regulation affect environmental
performance? A case study of china’s regional energy efficiency.
<em>EXSY</em>, <em>37</em>(3), e12326. (<a
href="https://doi.org/10.1111/exsy.12326">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Environmental regulation has been recognized as an important way to directly improve environmental performance or indirectly impact environmental performance through increasing environmental innovation. The present paper constructs an energy and carbon emission total factor productivity index using the Malmquist–Luenberger productivity index and data envelopment analysis. The proposed technique is used to measure the environmental performance of 30 Chinese provinces during the period 2010–2015. The energy and carbon emission total factor productivity measure is divided into a pure technical efficiency index and a technical progress index to provide detailed environmental performance information. Then an ordinary least squares model is adopted to analyse the impact of environmental regulation on environmental innovation operation and environmental performance by hypothesis testing. The empirical results show that environmental regulation and environmental innovation have positive direct effects on environmental performance and that market-based environmental regulation has a positive indirect effect on environmental performance by increasing environmental innovation.},
  archive      = {J_EXSY},
  author       = {Jie Wu and Jiangjiang Yang and Zhixiang Zhou},
  doi          = {10.1111/exsy.12326},
  journal      = {Expert Systems},
  month        = {6},
  number       = {3},
  pages        = {e12326},
  shortjournal = {Expert Syst.},
  title        = {How does environmental regulation affect environmental performance? a case study of china&#39;s regional energy efficiency},
  volume       = {37},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Sustainability assessment in the presence of undesirable
factors over time: A case on gas companies. <em>EXSY</em>,
<em>37</em>(3), e12316. (<a
href="https://doi.org/10.1111/exsy.12316">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sustainability is an essential ingredient for long-term success of firms, and its assessment has a significant impact on decision making and sustainability management. In the current paper, a data envelopment analysis-based approach is proposed to assess the sustainability of systems over several periods when undesirable outputs are present in the process. Indeed, sustainability is assessed in each period and, as a whole, simultaneously. Furthermore, social, economic, and environmental efficiency scores of multiperiod systems are evaluated in each period. The weak disposability assumption is used to handle undesirable outputs. Also, the current study concerns socio-economic, socioenvironmental, and eco-environmental factors in addition to economic, social, and environmental aspects. A real-world application on Iranian gas companies is used to illustrate the applicability of the proposed approach.},
  archive      = {J_EXSY},
  author       = {Hamzeh Amirteimoori and Alireza Amirteimoori and Arash Amirteimoori},
  doi          = {10.1111/exsy.12316},
  journal      = {Expert Systems},
  month        = {6},
  number       = {3},
  pages        = {e12316},
  shortjournal = {Expert Syst.},
  title        = {Sustainability assessment in the presence of undesirable factors over time: A case on gas companies},
  volume       = {37},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Sustainability of chinese airlines: A modified slack-based
measure model for CO2 emissions. <em>EXSY</em>, <em>37</em>(3), e12302.
(<a href="https://doi.org/10.1111/exsy.12302">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sustainable airline operations have become an increasingly important issue in recent years. With this respect, several initiatives for reducing pollutant emissions—such as carbon dioxide (CO 2 )—in the airline industry are now under consideration by regulators, policymakers, and companies. The impact of these initiatives upon efficiency levels of airline operations is still being analysed by different authors. This article is focused on the efficiency assessment of 13 major Chinese airlines from 2008 to 2015, applying a modified slack-based measure model to account for CO 2 emissions. The impact of contextual variables related to the airline&#39;s age, fleet mix, stock market governance, ownership type, network span, and whether or not it has undergone a previous merger and acquisition process is tested by means of a stochastic non-linear robust regression approach. Findings suggest that sustainability in Chinese airline operations is dependent upon a number of economic factors such as learning curve, economies of scale, technology type, and network management. Policy implications are derived for Chinese airlines.},
  archive      = {J_EXSY},
  author       = {Abdollah Hadi-Vencheh and Peter Wanke and Ali Jamshidi and Zhongfei Chen},
  doi          = {10.1111/exsy.12302},
  journal      = {Expert Systems},
  month        = {6},
  number       = {3},
  pages        = {e12302},
  shortjournal = {Expert Syst.},
  title        = {Sustainability of chinese airlines: A modified slack-based measure model for CO2 emissions},
  volume       = {37},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Dynamic efficiency evaluation of state-level business
incubators in china by using a slacks-based measure approach.
<em>EXSY</em>, <em>37</em>(3), e12285. (<a
href="https://doi.org/10.1111/exsy.12285">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {It is important to evaluate the efficiency of business incubators for their performance improvement. Because few enterprises can successfully graduate from the incubation process in one incubation period, the incubating enterprises will be carried over to the successive periods. In this context, the number of incubating enterprises can be regarded as a carry-over variable linking different incubation periods, which can also be treated as an undesirable output in the current period. This paper proposes a dynamic slacks-based measure model to evaluate the efficiency of China&#39;s state-level business incubators during 2010–2012. The empirical results show that neglecting new entrants and the typical carry-over variable may underestimate the incubation system&#39;s efficiency. Moreover, the operational efficiency of China&#39;s state-level business incubators is relatively low, which is largely caused by the lower pure technical efficiency. There exist great disparities between pure technical efficiency and scale efficiency for all considered incubators in China. Some important insights and policy suggestions are presented.},
  archive      = {J_EXSY},
  author       = {Xiangyang Sun and Yu Cheng and Qicheng Lu and Miao Hu},
  doi          = {10.1111/exsy.12285},
  journal      = {Expert Systems},
  month        = {6},
  number       = {3},
  pages        = {e12285},
  shortjournal = {Expert Syst.},
  title        = {Dynamic efficiency evaluation of state-level business incubators in china by using a slacks-based measure approach},
  volume       = {37},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Iwinac 2017: Assistive intelligence for the elderly.
<em>EXSY</em>, <em>37</em>(2), e12535. (<a
href="https://doi.org/10.1111/exsy.12535">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_EXSY},
  author       = {Mariano Rincón-Zamorano and Rafael Martínez-Tomás and José Manuel Ferrández},
  doi          = {10.1111/exsy.12535},
  journal      = {Expert Systems},
  month        = {4},
  number       = {2},
  pages        = {e12535},
  shortjournal = {Expert Syst.},
  title        = {Iwinac 2017: Assistive intelligence for the elderly},
  volume       = {37},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Advanced social media sentiment analysis for short-term
cryptocurrency price prediction. <em>EXSY</em>, <em>37</em>(2), e12493.
(<a href="https://doi.org/10.1111/exsy.12493">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, the scrutiny of bitcoin and other cryptocurrencies as legal and regulated components of financial systems has been increasing. Bitcoin is currently one of the largest cryptocurrencies in terms of capital market share. Therefore, this study proposes that sentiment analysis can be used as a computational tool to predict the prices of bitcoin and other cryptocurrencies for different time intervals. A key characteristic of the cryptocurrency market is that the fluctuation of currency prices depends on people&#39;s perceptions and opinions, not institutional money regulation. Therefore, analysing the relationship between social media and web search is crucial for cryptocurrency price prediction. This study uses Twitter and Google Trends to forecast the short-term prices of the primary cryptocurrencies, as these social media platforms are used to influence purchasing decisions. The study adopts and interpolates a unique multimodel approach to analyse the impact of social media on cryptocurrency prices. Our results prove that people&#39;s psychological and behavioural attitudes have a significant impact on the highly speculative cryptocurrency prices.},
  archive      = {J_EXSY},
  author       = {Krzysztof Wołk},
  doi          = {10.1111/exsy.12493},
  journal      = {Expert Systems},
  month        = {4},
  number       = {2},
  pages        = {e12493},
  shortjournal = {Expert Syst.},
  title        = {Advanced social media sentiment analysis for short-term cryptocurrency price prediction},
  volume       = {37},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Toxicity modelling of nanomaterials by origin evaluation of
their physicochemical descriptors using a combination of principal
component analysis and support vector machine methods. <em>EXSY</em>,
<em>37</em>(2), e12492. (<a
href="https://doi.org/10.1111/exsy.12492">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the present study, the performance of physicochemical descriptors of metal oxide nanomaterials on the basis of their origins, including descriptors related to element/ion, the metal oxide in bulk, and metal oxide in the media for toxicity modelling has been evaluated. Three published experimental nanomaterial data sets were selected for the study. The data set was divided into three subsets on the basis of the origin of descriptors; thereafter, each of them was analysed using principal component analysis for visual discrimination of toxic versus nontoxic nanomaterials in the principal component (PC) space. The metal oxide in media-based descriptor subset results in the best visual clustering of toxicity of nanomaterials compared with the rest two subsets. It was also confirmed with the class separability measures in the PC space and classification accuracy of the support vector machine (SVM) method. PC scores of the metal oxide in media-related descriptors results in the maximum value of class separability index ( J = 0.0049) and the maximum classification accuracy of 96.43% of SVM classifier (sensitivity of 100%). A toxicity classification model of nanomaterials has been established using PC scores of optimal descriptor subset and SVM method.},
  archive      = {J_EXSY},
  author       = {Sunil Kr Jha and Tae Hyun Yoon},
  doi          = {10.1111/exsy.12492},
  journal      = {Expert Systems},
  month        = {4},
  number       = {2},
  pages        = {e12492},
  shortjournal = {Expert Syst.},
  title        = {Toxicity modelling of nanomaterials by origin evaluation of their physicochemical descriptors using a combination of principal component analysis and support vector machine methods},
  volume       = {37},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A novel optimal PID controller autotuning design based on
the SLP algorithm. <em>EXSY</em>, <em>37</em>(2), e12489. (<a
href="https://doi.org/10.1111/exsy.12489">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A novel optimal proportional integral derivative (PID) autotuning controller design based on a new algorithm approach, the “swarm learning process” (SLP) algorithm, is proposed. It improves the convergence and performance of the autotuning PID parameter by applying the swarm and learning algorithm concepts. Its convergence is verified by two methods, global convergence and characteristic convergence. In the case of global convergence, the convergence rule of a random search algorithm is employed to judge, and Markov chain modelling is used to analyse. The superiority of the proposed method, in terms of characteristic convergence and performance, is verified through the simulation based on the automatic voltage regulator and direct current motor control system. Verification is performed by comparing the results of the proposed model with those of other algorithms, that is, the ant colony optimization with a new constrained Nelder–Mead algorithm, the genetic algorithm (GA), the particle swarm optimization (PSO) algorithm, and a neural network (NN). According to the global convergence analysis, the proposed method satisfies the convergence rule of the random search algorithm. With respect to the characteristic convergence and performance, the proposed method provides a better response than the GA, the PSO, and the NN for both control systems.},
  archive      = {J_EXSY},
  author       = {Jirapun Pongfai and Xiaojie Su and Huiyan Zhang and Wudhichai Assawinchaichote},
  doi          = {10.1111/exsy.12489},
  journal      = {Expert Systems},
  month        = {4},
  number       = {2},
  pages        = {e12489},
  shortjournal = {Expert Syst.},
  title        = {A novel optimal PID controller autotuning design based on the SLP algorithm},
  volume       = {37},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Building bagging on critical instances. <em>EXSY</em>,
<em>37</em>(2), e12486. (<a
href="https://doi.org/10.1111/exsy.12486">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The ensemble method is a powerful data mining paradigm, which builds a classification model by integrating multiple diversified component learners. Bagging is one of the most successful ensemble methods. It is made of bootstrap-inspired classifiers and uses these classifiers to get an aggregated classifier. However, in bagging, bootstrapped training sets become more and more similar as redundancy is increasing. Besides redundancy, any training set is usually subject to noise. Moreover, the training set might be imbalanced. Thus, each training instance has a different impact on the learning process. This paper explores some properties of the ensemble margin and its use in improving the performance of bagging. We introduce a new approach to measure the importance of training data in learning, based on the margin theory. Then, a new bagging method concentrating on critical instances is proposed. This method is more accurate than bagging and more robust than boosting. Compared to bagging, it reduces the bias while generally keeping the same variance. Our findings suggest that (a) examples with low margins tend to be more critical for the classifier performance; (b) examples with higher margins tend to be more redundant; (c) misclassified examples with high margins tend to be noisy examples. Our experimental results on 15 various data sets show that the generalization error of bagging can be reduced up to 2.5% and its resilience to noise strengthened by iteratively removing both typical and noisy training instances, reducing the training set size by up to 75%.},
  archive      = {J_EXSY},
  author       = {Li Guo and Samia Boukir and Alexandre Aussem},
  doi          = {10.1111/exsy.12486},
  journal      = {Expert Systems},
  month        = {4},
  number       = {2},
  pages        = {e12486},
  shortjournal = {Expert Syst.},
  title        = {Building bagging on critical instances},
  volume       = {37},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Improving the risk management of type 2 diabetes mellitus in
china from the perspective of social relationships. <em>EXSY</em>,
<em>37</em>(2), e12484. (<a
href="https://doi.org/10.1111/exsy.12484">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In China, Type 2 diabetes mellitus (T2DM) is increasingly affecting people&#39;s health. Although many risk factors related to T2DM have been researched, the association between social relationships and risk management of T2DM in China has not been fully researched. Therefore, we obtained 2,969 valid cases from the National Chinese Medicine Clinical Research Base-Key Disease of Diabetes Mellitus Study to evaluate the role of social relationships in the risk management of T2DM. We first establish an indicators system of social relationship factors and then propose a comprehensive method that integrates subjective (analytical network process) and objective (entropy weight method) evaluations to rank the importance of the 17 social relationship factors that were the most important and commonly used. The results suggest that different social relationship factors have different effects on the risk management of T2DM. Patients and health workers should pay more attention to the high-benefit factors and thus improve the efficiency of the risk management of T2DM. These findings provided theoretical support for patients and health workers by developing the positive effects of social relationships in improving the risk management of T2DM to the fullest degree.},
  archive      = {J_EXSY},
  author       = {Xiaojia Wang and Mi Chen and Wei Xia and Keyu Zhu and Shanshan Zhang and Weiqun Xu and Yuxiang Guan},
  doi          = {10.1111/exsy.12484},
  journal      = {Expert Systems},
  month        = {4},
  number       = {2},
  pages        = {e12484},
  shortjournal = {Expert Syst.},
  title        = {Improving the risk management of type 2 diabetes mellitus in china from the perspective of social relationships},
  volume       = {37},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Emergency decision making with extended axiomatic design
approach under picture fuzzy environment. <em>EXSY</em>, <em>37</em>(2),
e12482. (<a href="https://doi.org/10.1111/exsy.12482">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Timely and effective emergency decision making (EDM) is the key to control the spread of disasters and reduce the casualties and property losses caused by emergencies. However, due to limited time and insufficient data, it is difficult for decision makers to provide accurate information about emergency incidents. Moreover, the EDM problems become complicated and unstructured requiring the deployment of advanced mathematical techniques to derive the most acceptable response. In this paper, we propose a new EDM approach by using picture fuzzy sets and axiomatic design technique for determining the optimal rescue plan to reduce the damages of emergencies. The contribution of this paper is to apply the picture fuzzy sets to handle the uncertainty and ambiguity of decision makers&#39; assessments on emergency alternatives, apply the picture fuzzy hybrid averaging operator to aggregate decision makers&#39; opinions into a group evaluation matrix and extend the axiomatic design technique to identify the best emergency solution for EDM. Finally, a real example is provided, and the result is compared with existing methods to demonstrate the feasibility and practicability of our proposed EDM approach.},
  archive      = {J_EXSY},
  author       = {Xue-Feng Ding and Ling Zhang and Hu-Chen Liu},
  doi          = {10.1111/exsy.12482},
  journal      = {Expert Systems},
  month        = {4},
  number       = {2},
  pages        = {e12482},
  shortjournal = {Expert Syst.},
  title        = {Emergency decision making with extended axiomatic design approach under picture fuzzy environment},
  volume       = {37},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A context-awareness model for activity recognition in
robot-assisted scenarios. <em>EXSY</em>, <em>37</em>(2), e12481. (<a
href="https://doi.org/10.1111/exsy.12481">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Context awareness in ambient assisted living programmes for the elderly is a cornerstone in the current scenario of noncustomized service robots distributed around the world. This research proposes a context-awareness system for a human–robot scene interpretation based on seven primary contexts and the American Occupational Therapy Association. The context-awareness system defined here proposes an inference mechanism for the activity recognition supported on hierarchical Bayesian networks. However, when the information from sensors increases, the computational cost associated also increases. Thus, an evaluation of different Bayesian network models is necessary for decreasing its impact over the robot performance. Two topological models have been modelled and tested using OpenMarkov application: a two-level approach of an input–observations layer and the activity recognition layer, and a three-layer model setting apart a primary contexts layer, the input–observations layer, and the activity recognition layer. The qualitative and quantitative results presented here show better performance in terms of memory and memory in a three-layer model. Besides, its effect on a hybrid architecture of a robotic platform is presented.},
  archive      = {J_EXSY},
  author       = {Francisco J. Rodriguez Lera and Francisco Martín Rico and Angel Manuel Guerrero Higueras and Vicente Matellán Olivera},
  doi          = {10.1111/exsy.12481},
  journal      = {Expert Systems},
  month        = {4},
  number       = {2},
  pages        = {e12481},
  shortjournal = {Expert Syst.},
  title        = {A context-awareness model for activity recognition in robot-assisted scenarios},
  volume       = {37},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A multi-agent affective interactive MAGDM approach and its
applications. <em>EXSY</em>, <em>37</em>(2), e12480. (<a
href="https://doi.org/10.1111/exsy.12480">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traditional multi-attribute group decision-making (MAGDM) methods focus on weights calculation of sub-attributes and experts&#39; preferences, but lack the discussion on the decision-makers&#39; affective interaction, and its influence on the decision preference and group consistency. To address this problem, the present study proposed a new multilayer affective computing model based on “ personality–mood–emotion ” pattern, under the multi-agent decision system framework. In addition, we introduced the group trending index and affection-preference incentive mechanism, which can help simulate MAGDM process and learn group experts&#39; decision preferences. Further, we proposed a new multi-agent affective interactive MAGDM (MAAI-MAGDM) method, where we defined a novel group convergence index and an alternative decision entropy to explain the convergence process of decision and group consistency. Compared to the traditional MAGDM approaches, the proposed MAAI-MAGDM method fully considered the affective features of each expert, reduced the dependence on aggregation operators and weight analysis, alleviated the workload of group experts, and effectively reduced the complexity of decision-making calculation process. Finally, we verified that the proposed method can effectively assist the decision-making processes by employing two numerical cases.},
  archive      = {J_EXSY},
  author       = {Cheng Peng and Chong Su},
  doi          = {10.1111/exsy.12480},
  journal      = {Expert Systems},
  month        = {4},
  number       = {2},
  pages        = {e12480},
  shortjournal = {Expert Syst.},
  title        = {A multi-agent affective interactive MAGDM approach and its applications},
  volume       = {37},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Distance-based intuitionistic multiplicative multiple
criteria decision-making methods for healthcare management in west china
hospital. <em>EXSY</em>, <em>37</em>(2), e12479. (<a
href="https://doi.org/10.1111/exsy.12479">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Intuitionistic multiplicative sets use an asymmetric, unbalanced scale to express information from positive, negative, and indeterminate information. They have been found capable of comprehensively and objectively representing a person&#39;s intuitive understanding and hence have attracted much attention. Distance techniques are widely used to measure the degree to which arguments deviate from one another. Several fuzzy set extensions have been developed, but little research has been conducted on measures of distance between intuitionistic multiplicative sets. In this paper, we start by presenting a variety of measures of the distance between intuitionistic multiplicative sets, including Hausdorff distance measures, weighted distance measures, ordered weighted distance measures, and continuous weighted distance measures. We then develop a distance-based intuitionistic multiplicative-technique for order preference by similarity to ideal solution method and a distance-based intuitionistic multiplicative-Vlsekriterijumska Optimizacija I Kompromisno Resenje method for handling multiple criteria decision-making problems with intuitionistic multiplicative evaluation information. To demonstrate the practical application of these distance measures and the proposed methods, we provide a case study of hospital management of inpatient admission. The paper ends with comparative analyses of the two methods and some concluding remarks.},
  archive      = {J_EXSY},
  author       = {Huchang Liao and Cheng Zhang and Li Luo and Zeshui Xu and Jian-Bo Yang and Dong-Ling Xu},
  doi          = {10.1111/exsy.12479},
  journal      = {Expert Systems},
  month        = {4},
  number       = {2},
  pages        = {e12479},
  shortjournal = {Expert Syst.},
  title        = {Distance-based intuitionistic multiplicative multiple criteria decision-making methods for healthcare management in west china hospital},
  volume       = {37},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A new clustering method based on the bio-inspired cuttlefish
optimization algorithm. <em>EXSY</em>, <em>37</em>(2), e12478. (<a
href="https://doi.org/10.1111/exsy.12478">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most of the well-known clustering methods based on distance measures, distance metrics and similarity functions have the main problem of getting stuck in the local optima and their performance strongly depends on the initial values of the cluster centers. This paper presents a new approach to enhance the clustering problems with the bio-inspired Cuttlefish Algorithm (CFA) by searching the best cluster centers that can minimize the clustering metrics. Various UCI Machine Learning Repository datasets are used to test and evaluate the performance of the proposed method. For the sake of comparison, we have also analysed several algorithms such as K-means, Genetic Algorithm and the Particle Swarm Optimization (PSO) Algorithm. The simulations and obtained results demonstrate that the performance of the proposed CFA-Clustering method is superior to the other counterpart algorithms in most cases. Therefore, the CFA can be considered as an alternative stochastic method to solve clustering problems.},
  archive      = {J_EXSY},
  author       = {Adel Sabry Eesa and Zeynep Orman},
  doi          = {10.1111/exsy.12478},
  journal      = {Expert Systems},
  month        = {4},
  number       = {2},
  pages        = {e12478},
  shortjournal = {Expert Syst.},
  title        = {A new clustering method based on the bio-inspired cuttlefish optimization algorithm},
  volume       = {37},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). ArA*summarizer: An arabic text summarization system based on
subtopic segmentation and using an a* algorithm for reduction.
<em>EXSY</em>, <em>37</em>(2), e12476. (<a
href="https://doi.org/10.1111/exsy.12476">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Automatic text summarization is a field situated at the intersection of natural language processing and information retrieval. Its main objective is to automatically produce a condensed representative form of documents. This paper presents ArA*summarizer, an automatic system for Arabic single document summarization. The system is based on an unsupervised hybrid approach that combines statistical, cluster-based, and graph-based techniques. The main idea is to divide text into subtopics then select the most relevant sentences in the most relevant subtopics. The selection process is done by an A* algorithm executed on a graph representing the different lexical–semantic relationships between sentences. Experimentation is conducted on Essex Arabic summaries corpus and using recall-oriented understudy for gisting evaluation, automatic summarization engineering, merged model graphs, and n-gram graph powered evaluation via regression evaluation metrics. The evaluation results showed the good performance of our system compared with existing works.},
  archive      = {J_EXSY},
  author       = {Belahcene Bahloul and Hassina Aliane and Mohamed Benmohammed},
  doi          = {10.1111/exsy.12476},
  journal      = {Expert Systems},
  month        = {4},
  number       = {2},
  pages        = {e12476},
  shortjournal = {Expert Syst.},
  title        = {ArA*summarizer: An arabic text summarization system based on subtopic segmentation and using an a* algorithm for reduction},
  volume       = {37},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Fuzzy-description logic for supporting the rehabilitation of
the elderly. <em>EXSY</em>, <em>37</em>(2), e12464. (<a
href="https://doi.org/10.1111/exsy.12464">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {According to the latest statistics, the proportion of the elderly (+65) is increasing and is expected to double within the European Union in a period of 50 years. This ageing is due to the improvement of quality of life and advances in medicine in the last decades. Gerontechnology is receiving a great deal of attention as a way of providing the elderly with sustainable products, environments, and services combining gerontology and technology. One of the most important aspects to consider by gerontechnology is the mobility/rehabilitation technologies, because there is an important relationship between mobility and the elderly&#39;s quality of life. Telerehabilitation systems have emerged to allow the elderly to perform their rehabilitation exercises remotely. However, in many cases, the proposed systems assist neither the patients nor the experts about the progress of the rehabilitation. To address this problem, we propose in this paper, a fuzzy-semantic system for evaluating patient&#39;s physical state during the rehabilitation process based on well-known standard for patients&#39; evaluation. Moreover, a tool called FINE has been developed that facilitates the evaluation be accomplished in a semi-automatic way first asking patients to carry out a set of standard tests and then inferencing their state by means of a fuzzy-semantic approach using the data captured during the rehabilitation tasks.},
  archive      = {J_EXSY},
  author       = {Alejandro Moya and Elena Navarro and Javier Jaén and Pascual González},
  doi          = {10.1111/exsy.12464},
  journal      = {Expert Systems},
  month        = {4},
  number       = {2},
  pages        = {e12464},
  shortjournal = {Expert Syst.},
  title        = {Fuzzy-description logic for supporting the rehabilitation of the elderly},
  volume       = {37},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A multimodal conversational coach for active ageing based on
sentient computing and m-health. <em>EXSY</em>, <em>37</em>(2), e12454.
(<a href="https://doi.org/10.1111/exsy.12454">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As life expectancy increases, it has become more necessary to find ways to support healthy ageing. A number of active ageing initiatives are being developed nowadays to foster healthy habits in the population. This paper presents our contribution to these initiatives in the form of a multimodal conversational coach that acts as a coach for physical activities. The agent can be developed as an Android app running on smartphones and coupled with cheap widely available sport sensors in order to provide meaningful coaching. It can be employed to prepare exercise sessions, provide feedback during the sessions, and discuss the results after the exercise. It incorporates an affective component that informs dynamic user models to produce adaptive interaction strategies.},
  archive      = {J_EXSY},
  author       = {David Griol and José Manuel Molina and Araceli Sanchis},
  doi          = {10.1111/exsy.12454},
  journal      = {Expert Systems},
  month        = {4},
  number       = {2},
  pages        = {e12454},
  shortjournal = {Expert Syst.},
  title        = {A multimodal conversational coach for active ageing based on sentient computing and m-health},
  volume       = {37},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Modelling hospital readmissions under frailty conditions for
healthy aging. <em>EXSY</em>, <em>37</em>(2), e12437. (<a
href="https://doi.org/10.1111/exsy.12437">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the current context of an aging population in many developed countries, the issue of healthy aging is at the forefront of the political, scientific, and technological concerns. The frailty accompanying the late years of elderly people (&gt;70 years old) deserves special consideration due to its great economical and personal costs and the workload imposed on the health care system. Hospital readmissions under a short time after hospital discharge are one of the sources of concern, and much effort is being devoted to their prediction for better care of the elder and optimized resource management. In this paper, we consider the prediction of readmissions for patients that are evaluated positively in the frailty scales. The computational experiments are carried out over a gender-balanced cohort of 645 patients recruited at the University Hospital of Alava. We report machine-learning prediction results of the readmission before the standard readmission limit of 30 days. We apply an upsampling technique to correct for class imbalance. Results are positive, encouraging further research and the creation of larger cohorts in international efforts.},
  archive      = {J_EXSY},
  author       = {Manuel Graña and Jose Manuel Lopez-Guede and Jon Irazusta and Idoia Labayen and Ariadna Besga},
  doi          = {10.1111/exsy.12437},
  journal      = {Expert Systems},
  month        = {4},
  number       = {2},
  pages        = {e12437},
  shortjournal = {Expert Syst.},
  title        = {Modelling hospital readmissions under frailty conditions for healthy aging},
  volume       = {37},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Film mood induction and emotion classification using
physiological signals for health and wellness promotion in older adults
living alone. <em>EXSY</em>, <em>37</em>(2), e12425. (<a
href="https://doi.org/10.1111/exsy.12425">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper introduces a wearable hardware/software system specifically tailored to detect seven emotions (neutral, tenderness, amusement, anger, disgust, fear, and sadness) aimed at promoting health and wellness in older adults living alone at home. The complete software and hardware architectures acquiring and processing electrodermal activity and photoplethysmography signals are introduced. The wearable emotion detection system is trained by eliciting the desired emotions on 39 older adults through a film mood induction procedure. Seventeen features are calculated on skin conductance response and heart rate variability data, grouped into five statistical, four temporal, and eight morphological features. Then, these features are used to run emotion classification considering support vector machines, decision trees, and quadratic discriminant analysis. In line with psychological findings, the results offer a global accuracy of 82% in negative emotion (anger, disgust, fear, and sadness) classification. For positive emotions (tenderness and amusement), also in conformity with previous psychological outcomes, amusement shows the highest ratio of hits (92%) but tenderness the lowest one (66%). These results demonstrate that our wearable emotion detection system can be used by ageing adults, especially for detecting negative emotions that usually damage health and wellness and lead to social isolation.},
  archive      = {J_EXSY},
  author       = {Arturo Martínez-Rodrigo and Luz Fernández-Aguilar and Roberto Zangróniz and José M. Latorre and José M. Pastor and Antonio Fernández-Caballero},
  doi          = {10.1111/exsy.12425},
  journal      = {Expert Systems},
  month        = {4},
  number       = {2},
  pages        = {e12425},
  shortjournal = {Expert Syst.},
  title        = {Film mood induction and emotion classification using physiological signals for health and wellness promotion in older adults living alone},
  volume       = {37},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020b). Ontology-based smart medical solutions. <em>EXSY</em>,
<em>37</em>(1), e12518. (<a
href="https://doi.org/10.1111/exsy.12518">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_EXSY},
  author       = {Atilla Elçi and Duygu Çelik Ertuğrul},
  doi          = {10.1111/exsy.12518},
  journal      = {Expert Systems},
  month        = {2},
  number       = {1},
  pages        = {e12518},
  shortjournal = {Expert Syst.},
  title        = {Ontology-based smart medical solutions},
  volume       = {37},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020a). Disease classification for smart health. <em>EXSY</em>,
<em>37</em>(1), e12517. (<a
href="https://doi.org/10.1111/exsy.12517">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_EXSY},
  author       = {Atilla Elçi and Duygu Çelik Ertuğrul},
  doi          = {10.1111/exsy.12517},
  journal      = {Expert Systems},
  month        = {2},
  number       = {1},
  pages        = {e12517},
  shortjournal = {Expert Syst.},
  title        = {Disease classification for smart health},
  volume       = {37},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Situation-centered goal reinforcement of activities of daily
living in smart home environments. <em>EXSY</em>, <em>37</em>(1),
e12487. (<a href="https://doi.org/10.1111/exsy.12487">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Older adults with early-stage dementia (ED) can experience confusion or lack clarity when performing routine activities of daily living (ADLs). These circumstances predispose the older adult to safety-critical and often risky situations. A safety-critical risky situation is one that constitutes a hazard. To support independent living, a sensor-laden smart environment can be employed to mitigate such hazards. In this paper, we propose a situation-centered goal reinforcement framework that supports older adults with ED in their decision making, and guides them through their ADL in order to fulfill their goal or intention and avoid hazards. First, we employ an LSTM (Long Short-Term Memory) model to infer the current goal of the resident, using their previously observed normal ADL patterns. Secondly, we identify potentially risky situations in their currently observed goal path. We then incorporate a situ-learning agent (SLA) that helps an inhabitant to make the right decision, thus preventing adverse events while guiding her through the task sequence that leads to her goal state. In addition, we use a naïve agent to simulate episodes of confusion similar to those that might be experienced by older adults with ED. We validated our method against an open-source dementia dataset (Quesada et al., 2015) by considering four types of ADLs as case studies. We achieved an accuracy of 90.1% for our goal inference model, higher than the accuracies reported by related studies. We also reported other metrics including precision, recall and f1-score for goal inference model. Finally, SLA&#39;s action recommendations relevance was evaluated accordingly.},
  archive      = {J_EXSY},
  author       = {Richard O. Oyeleke and Carl K. Chang and Jennifer Margrett},
  doi          = {10.1111/exsy.12487},
  journal      = {Expert Systems},
  month        = {2},
  number       = {1},
  pages        = {e12487},
  shortjournal = {Expert Syst.},
  title        = {Situation-centered goal reinforcement of activities of daily living in smart home environments},
  volume       = {37},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Parameterizing neural networks for disease classification.
<em>EXSY</em>, <em>37</em>(1), e12465. (<a
href="https://doi.org/10.1111/exsy.12465">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Neural networks are one option to implement decision support systems for health care applications. In this paper, we identify optimal settings of neural networks for medical diagnoses: The study involves the application of supervised machine learning using an artificial neural network to distinguish between gout and leukaemia patients. With the objective to improve the base accuracy (calculated from the initial set-up of the neural network model), several enhancements are analysed, such as the use of hyperbolic tangent activation function instead of the sigmoid function, the use of two hidden layers instead of one, and transforming the measurements with linear regression to obtain a smoothened data set. Another setting we study is the impact on the accuracy when using a data set of reduced size but with higher data quality. We also discuss the tradeoff between accuracy and runtime efficiency.},
  archive      = {J_EXSY},
  author       = {Guryash Bahra and Lena Wiese},
  doi          = {10.1111/exsy.12465},
  journal      = {Expert Systems},
  month        = {2},
  number       = {1},
  pages        = {e12465},
  shortjournal = {Expert Syst.},
  title        = {Parameterizing neural networks for disease classification},
  volume       = {37},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Wearable ECG signal processing for automated cardiac
arrhythmia classification using CFASE-based feature selection.
<em>EXSY</em>, <em>37</em>(1), e12432. (<a
href="https://doi.org/10.1111/exsy.12432">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Classification of electrocardiogram (ECG) signals is obligatory for the automatic diagnosis of cardiovascular disease. With the recent advancement of low-cost wearable ECG device, it becomes more feasible to utilize ECG for cardiac arrhythmia classification in daily life. In this paper, we propose a lightweight approach to classify five types of cardiac arrhythmia, namely, normal beat (N), atrial premature contraction (A), premature ventricular contraction (V), left bundle branch block beat (L), and right bundle branch block beat (R). The combined method of frequency analysis and Shannon entropy is applied to extract appropriate statistical features. Information gain criterion is employed to select features that the results show that 10 highly effective features can obtain performance measures comparable to those obtained by using the complete features. The selected features are then fed to the input of Random Forest, K-Nearest Neighbour, and J48 for classification. To evaluate classification performance, tenfold cross validation is used to verify the effectiveness of our method. Experimental results show that Random Forest classifier demonstrates significant performance with the highest sensitivity of 98.1%, the specificity of 99.5%, the precision of 98.1%, and the accuracy of 98.08%, outperforming other representative approaches for automated cardiac arrhythmia classification.},
  archive      = {J_EXSY},
  author       = {Yuwei Zhang and Yuan Zhang and Benny Lo and Wenyao Xu},
  doi          = {10.1111/exsy.12432},
  journal      = {Expert Systems},
  month        = {2},
  number       = {1},
  pages        = {e12432},
  shortjournal = {Expert Syst.},
  title        = {Wearable ECG signal processing for automated cardiac arrhythmia classification using CFASE-based feature selection},
  volume       = {37},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A secure mHealth application for attention deficit and
hyperactivity disorder. <em>EXSY</em>, <em>37</em>(1), e12431. (<a
href="https://doi.org/10.1111/exsy.12431">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nowadays, many people have smartphones, the fact that encourages the development of new tools to address different problems. One of its consequences is the recent growth of mHealth, a term that refers to the practice of medicine based on the use of mobile devices for medical and health purposes. This work describes a new mHealth tool to improve memory and cognitive abilities through gamification and serious games. In particular, a mobile application here is proposed to help children that suffer from attention deficit hyperactivity disorder (ADHD). This application integrates the four profiles involved in this disorder: children, parents, teachers, and medical staff. With it, parents can discover if their children suffer the disorder, and children can improve their cognitive abilities through games of different types. Besides, the security aspect of the proposal is emphasized to highlight its importance in mHealth. Thus, the developed tool includes various cryptographic mechanisms to protect the confidentiality of communications and the authenticity of users and data.},
  archive      = {J_EXSY},
  author       = {Nayra Rodríguez-Pérez and Pino Caballero-Gil and Alexandra Rivero-García and Josué Toledo-Castro},
  doi          = {10.1111/exsy.12431},
  journal      = {Expert Systems},
  month        = {2},
  number       = {1},
  pages        = {e12431},
  shortjournal = {Expert Syst.},
  title        = {A secure mHealth application for attention deficit and hyperactivity disorder},
  volume       = {37},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Improving privacy in health care with an ontology-based
provenance management system. <em>EXSY</em>, <em>37</em>(1), e12427. (<a
href="https://doi.org/10.1111/exsy.12427">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Provenanc refers to the origin of information. Therefore, provenance is the metadata that record the history of data. As provenance is the derivation history of an object starting from its original source, the provenance information is used to analyse processes that are performed on an object and to track by whom these processes are performed. Thus, provenance shows the trustworthiness and quality of data. In a provenance management system in order to verify the trustworthy of provenance information, security needs must be also fulfilled. In this work, an ontology-based privacy-aware provenance management model is proposed. The proposed model is based on the Open Provenance Model, which is a common model for provenance. The proposed model aims to detect privacy violations, to reduce privacy risks by using permissions and prohibitions, and also to query the provenance data. The proposed model is implemented with Semantic Web technologies and demonstrated for the health care domain in order to preserve patients&#39; privacy. Also, an infectious disease ontology and a vaccination ontology are integrated to the system in order to track the patients&#39; vaccination history, to improve the quality of medical processes, the reliability of medical data, and the decision making in the health care domain.},
  archive      = {J_EXSY},
  author       = {Ozgu Can and Dilek Yilmazer},
  doi          = {10.1111/exsy.12427},
  journal      = {Expert Systems},
  month        = {2},
  number       = {1},
  pages        = {e12427},
  shortjournal = {Expert Syst.},
  title        = {Improving privacy in health care with an ontology-based provenance management system},
  volume       = {37},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Predicting and reducing “hospital-acquired infections” using
a knowledge-based e-surveillance system. <em>EXSY</em>, <em>37</em>(1),
e12402. (<a href="https://doi.org/10.1111/exsy.12402">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The use of automated computer methods when detecting hospital-acquired infections (HAIs) enhances the validity of the surveillance in an effective manner. This is because manual infection control systems used by hospitals are time consuming and are often restricted to intensive care units. This paper proposes a new knowledge-based electronic surveillance system to predict and reduce HAIs. The system can gather patient-associated data from hospital databanks to automatically predict patient injury based on the standard central line-associated bloodstream infection algorithm for HAI detection rules. The application of the proposed electronic system reduces the number of central lines associated with infection of the bloodstream and reduces the length of stay for patient treatment and thus reduces costs. The proposed system has several advantages: (a) It is a web-based system that collects actual data from patients from several IT sources, which will help collect patient data safely and quickly, thereby predicting HAIs effectively. (b) It has an integrated simulator to generate patient records, providing the ability to train medical personnel and nurses to enhance their skills. (c) It is a multimedia-based system to improve patient health reporting. (d) It assists policymakers in reviewing and approving control plans and policies to reduce and prevent hospital injuries. (e) The investigational results of the system showed an enhancement value equal to 87%.},
  archive      = {J_EXSY},
  author       = {Amin Y. Noaman and Abdul Hamid M. Ragab and Nabeela Al-Abdullah and Arwa Jamjoom and Farrukh Nadeem and Anser G. Ali},
  doi          = {10.1111/exsy.12402},
  journal      = {Expert Systems},
  month        = {2},
  number       = {1},
  pages        = {e12402},
  shortjournal = {Expert Syst.},
  title        = {Predicting and reducing “hospital-acquired infections” using a knowledge-based e-surveillance system},
  volume       = {37},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A knowledge construction methodology to automate case-based
learning using clinical documents. <em>EXSY</em>, <em>37</em>(1),
e12401. (<a href="https://doi.org/10.1111/exsy.12401">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The case-based learning (CBL) approach has gained attention in medical education as an alternative to traditional learning methodology. However, current CBL systems do not facilitate and provide computer-based domain knowledge to medical students for solving real-world clinical cases during CBL practice. To automate CBL, clinical documents are beneficial for constructing domain knowledge. In the literature, most systems and methodologies require a knowledge engineer to construct machine-readable knowledge. Keeping in view these facts, we present a knowledge construction methodology (KCM-CD) to construct domain knowledge ontology (i.e., structured declarative knowledge) from unstructured text in a systematic way using artificial intelligence techniques, with minimum intervention from a knowledge engineer. To utilize the strength of humans and computers, and to realize the KCM-CD methodology, an interactive case-based learning system (iCBLS) was developed. Finally, the developed ontological model was evaluated to evaluate the quality of domain knowledge in terms of coherence measure. The results showed that the overall domain model has positive coherence values, indicating that all words in each branch of the domain ontology are correlated with each other and the quality of the developed model is acceptable.},
  archive      = {J_EXSY},
  author       = {Maqbool Ali and Jamil Hussain and Sungyoung Lee and Byeong Ho Kang and Kashif Sattar},
  doi          = {10.1111/exsy.12401},
  journal      = {Expert Systems},
  month        = {2},
  number       = {1},
  pages        = {e12401},
  shortjournal = {Expert Syst.},
  title        = {A knowledge construction methodology to automate case-based learning using clinical documents},
  volume       = {37},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Head mouse control system for people with disabilities.
<em>EXSY</em>, <em>37</em>(1), e12398. (<a
href="https://doi.org/10.1111/exsy.12398">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, a human–machine interface for disabled people with spinal cord injuries is proposed. The designed human–machine interface is an assistive system that uses head movements and blinking for mouse control. In the proposed system, by moving one&#39;s head, the user moves the mouse pointer to the required coordinates and then blinks to send commands. The considered head mouse control is based on image processing including facial recognition, in particular, the recognition of the eyes, mouth, and nose. The proposed recognition system is based on the convolutional neural network, which uses the low-quality images that are captured by a computer&#39;s camera. The convolutional neural network (CNN) includes convolutional layers, a pooling layer, and a fully connected network. The CNN transforms the head movements to the actual coordinates of the mouse. The designed system allows people with disabilities to control a mouse pointer with head movements and to control mouse buttons with blinks. The results of the experiments demonstrate that this system is robust and accurate. This invention allows people with disabilities to freely control mouse cursors and mouse buttons without wearing any equipment.},
  archive      = {J_EXSY},
  author       = {Rahib H. Abiyev and Murat Arslan},
  doi          = {10.1111/exsy.12398},
  journal      = {Expert Systems},
  month        = {2},
  number       = {1},
  pages        = {e12398},
  shortjournal = {Expert Syst.},
  title        = {Head mouse control system for people with disabilities},
  volume       = {37},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). AUDIT: AnomaloUs data detection and isolation approach for
mobile healThcare systems. <em>EXSY</em>, <em>37</em>(1), e12390. (<a
href="https://doi.org/10.1111/exsy.12390">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mobile health care systems highly depend on collected physiological data through medical sensors to provide high-quality care services. However, inaccurate physiological data from sensors pose a major challenge for health care providers when making decisions, whereas an erroneous decision can affect the user&#39;s life. We propose, in this paper, an anomalous data detection and isolation approach for mobile health care systems. Our approach, called AUDIT, detects inaccurate measurements in real time and distinguishes between faults or errors and health events. To do so, we propose reduced time and space complexities algorithms based on dimension reduction within the context of resource constraints. Furthermore, a decision algorithm is proposed while exploring the spatio-temporal correlation between physiological attributes. First, we describe our approach. Then, we give its implementation details. Finally, to demonstrate the effectiveness of our approach, we show different experiments related to its detection performances and its time and space complexities.},
  archive      = {J_EXSY},
  author       = {Lamia Ben Amor and Imene Lahyani and Mohamed Jmaiel},
  doi          = {10.1111/exsy.12390},
  journal      = {Expert Systems},
  month        = {2},
  number       = {1},
  pages        = {e12390},
  shortjournal = {Expert Syst.},
  title        = {AUDIT: AnomaloUs data detection and isolation approach for mobile healThcare systems},
  volume       = {37},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). On the use of ear and profile faces for distinguishing
identical twins and nontwins. <em>EXSY</em>, <em>37</em>(1), e12389. (<a
href="https://doi.org/10.1111/exsy.12389">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study aims to measure the efficiency of ear and profile face in distinguishing identical twins under identification and verification modes. In addition, to distinguish identical twins by ear and profile face separately, we propose to fuse these traits with all possible binary combinations of left ear, left profile face, right ear, and right profile face. Fusion is implemented by score-level fusion and decision-level fusion techniques in the proposed method. Additionally, feature-level fusion is used for comparison. All experiments in this paper are also implemented on nontwins individuals, and the recognition performance of twins and nontwins are compared. Local binary patterns, local phase quantization, and binarized statistical image features approaches are used as texture-based descriptors for feature extraction process. Images under controlled and uncontrolled lighting are tested. Ear and profile images from ND-TWINS-2009-2010 dataset are used in the experiments. The experimental results show that the proposed method is more accurate and reliable than using ear or profile face images separately. The performance of the proposed method for recognizing identical twins as recognition rate is 100% and 99.45%, and equal error rates are 0.54% and 1.63% in controlled and uncontrolled illumination conditions, respectively.},
  archive      = {J_EXSY},
  author       = {Önsen Toygar and Esraa Alqaralleh and Ayman Afaneh},
  doi          = {10.1111/exsy.12389},
  journal      = {Expert Systems},
  month        = {2},
  number       = {1},
  pages        = {e12389},
  shortjournal = {Expert Syst.},
  title        = {On the use of ear and profile faces for distinguishing identical twins and nontwins},
  volume       = {37},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A hybrid knowledge and ensemble classification approach for
prediction of venous thromboembolism. <em>EXSY</em>, <em>37</em>(1),
e12388. (<a href="https://doi.org/10.1111/exsy.12388">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Clinical narratives such as progress summaries, lab reports, surgical reports, and other narrative texts contain key biomarkers about a patient&#39;s health. Evidence-based preventive medicine needs accurate semantic and sentiment analysis to extract and classify medical features as the input to appropriate machine learning classifiers. However, the traditional approach of using single classifiers is limited by the need for dimensionality reduction techniques, statistical feature correlation, a faster learning rate, and the lack of consideration of the semantic relations among features. Hence, extracting semantic and sentiment-based features from clinical text and combining multiple classifiers to create an ensemble intelligent system overcomes many limitations and provides a more robust prediction outcome. The selection of an appropriate approach and its interparameter dependency becomes key for the success of the ensemble method. This paper proposes a hybrid knowledge and ensemble learning framework for prediction of venous thromboembolism (VTE) diagnosis consisting of the following components: a VTE ontology, semantic extraction and sentiment assessment of risk factor framework, and an ensemble classifier. Therefore, a component-based analysis approach was adopted for evaluation using a data set of 250 clinical narratives where knowledge and ensemble achieved the following results with and without semantic extraction and sentiment assessment of risk factor, respectively: a precision of 81.8% and 62.9%, a recall of 81.8% and 57.6%, an F measure of 81.8% and 53.8%, and a receiving operating characteristic of 80.1% and 58.5% in identifying cases of VTE.},
  archive      = {J_EXSY},
  author       = {Susan Sabra and Khalid Mahmood Malik and Muhammad Afzal and Vian Sabeeh and Ahmad Charaf Eddine},
  doi          = {10.1111/exsy.12388},
  journal      = {Expert Systems},
  month        = {2},
  number       = {1},
  pages        = {e12388},
  shortjournal = {Expert Syst.},
  title        = {A hybrid knowledge and ensemble classification approach for prediction of venous thromboembolism},
  volume       = {37},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Frame logic-based specification and discovery of semantic
web services with application to medical appointments. <em>EXSY</em>,
<em>37</em>(1), e12387. (<a
href="https://doi.org/10.1111/exsy.12387">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Matching web services and client requirements in the form of goals is a significant challenge in the discovery of semantic web services. The most common but unsatisfactory approach to matching is set-based, where both the client and web services declare what objects they require and what objects they can provide. Matching then becomes the simple task of comparing sets of objects. This approach is inadequate because it says nothing about the functionality required by the client or the functionality provided by the web service. As an alternative, we use the Frame Logic as implemented in Flora-2 to specify web service capabilities and client requirements, including their preconditions, postconditions, and ontologies, implement a logic-based discovery agent using Flora-2, demonstrate its usefulness in a medical appointment making scenario, and show its efficiency both theoretically and by benchmarking. The result is an expressive yet concise representation scheme for semantic web services, and a practical, efficient, powerful, and fully implemented matching engine based purely on logical inference for web service discovery, with direct applicability to Web Service Modeling Ontology and Web Service Modeling Language, because both are based on Frame Logic.},
  archive      = {J_EXSY},
  author       = {Omid Sharifi and Shahin Mehdipour Ataee and Zeki Bayram},
  doi          = {10.1111/exsy.12387},
  journal      = {Expert Systems},
  month        = {2},
  number       = {1},
  pages        = {e12387},
  shortjournal = {Expert Syst.},
  title        = {Frame logic-based specification and discovery of semantic web services with application to medical appointments},
  volume       = {37},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Onyx: A new canvas-based tool for visualizing biomedical and
health ontologies. <em>EXSY</em>, <em>37</em>(1), e12380. (<a
href="https://doi.org/10.1111/exsy.12380">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ontologies provide formal, machine-readable, and human-interpretable representations of domain knowledge. Therefore, ontologies have come into question with the development of Semantic Web technologies. People who want to use ontologies need an understanding of the ontology, but this understanding is very difficult to attain if the ontology user lacks the background knowledge necessary to comprehend the ontology or if the ontology is very large. Thus, software tools that facilitate the understanding of ontologies are needed. Ontology visualization is an important research area because visualization can help in the development, exploration, verification, and comprehension of ontologies. This paper introduces the design of a new ontology visualization tool, which differs from traditional visualization tools by providing important metrics and analytics about ontology concepts and warning the ontology developer about potential ontology design errors. The tool, called Onyx, also has advantages in terms of speed and readability. Thus, Onyx offers a suitable environment for the representation of large ontologies, especially those used in biomedical and health information systems and those that contain many terms. It is clear that these additional functionalities will increase the value of traditional ontology visualization tools during ontology exploration and evaluation.},
  archive      = {J_EXSY},
  author       = {Övünç Öztürk and Hasan Gökhan Açikgöz},
  doi          = {10.1111/exsy.12380},
  journal      = {Expert Systems},
  month        = {2},
  number       = {1},
  pages        = {e12380},
  shortjournal = {Expert Syst.},
  title        = {Onyx: A new canvas-based tool for visualizing biomedical and health ontologies},
  volume       = {37},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Architecture and implementation of a smart-pregnancy
monitoring system using web-based application. <em>EXSY</em>,
<em>37</em>(1), e12379. (<a
href="https://doi.org/10.1111/exsy.12379">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Today, people use web-based technologies to meet their information needs, socialise, communicate, and deal with formal and informal processes. At the same time, mobile versions of these applications provide people with great convenience in daily life. These applications include blood-pressure monitors, blood-glucose monitors, body-analysis scales, pulse oximeters, and activity and sleep trackers. Many of these products sync directly with a free mobile app that makes monitoring, viewing, storing, and sharing of health vitals simple and comprehensive. The data collected from the user is stored in a cloud-based application, then trained by intelligent algorithms that use machine learning for health aims so that the user can instantly see his or her status and development. In this study, the aim was to construct a cloud-based application specific to women for monitoring pregnancy. In the web-based application working with membership logic, members can access machine learning assisted calculators of the baby percentile, period tracker, pregnancy calendar, and baby vaccination schedule. Moreover, they can access augmented/virtual-reality-assisted visual training.},
  archive      = {J_EXSY},
  author       = {Yunus Santur and Sinem Güven Santur and Mehmet Karaköse},
  doi          = {10.1111/exsy.12379},
  journal      = {Expert Systems},
  month        = {2},
  number       = {1},
  pages        = {e12379},
  shortjournal = {Expert Syst.},
  title        = {Architecture and implementation of a smart-pregnancy monitoring system using web-based application},
  volume       = {37},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Converness: Ontology-driven conversational awareness and
context understanding in multimodal dialogue systems. <em>EXSY</em>,
<em>37</em>(1), e12378. (<a
href="https://doi.org/10.1111/exsy.12378">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dialogue-based systems often consist of several components, such as communication analysis, dialogue management, domain reasoning, and language generation. In this paper, we present Converness, an ontology-driven, rule-based framework to facilitate domain reasoning for conversational awareness in multimodal dialogue-based agents. Converness uses Web Ontology Language 2 (OWL 2) ontologies to capture and combine the conversational modalities of the domain, for example, deictic gestures and spoken utterances, fuelling conversational topic understanding, and interpretation using description logics and rules. At the same time, defeasible rules are used to couple domain and user-centred knowledge to further assist the interaction with end users, facilitating advanced conflict resolution and personalised context disambiguation. We illustrate the capabilities of the framework through its integration into a multimodal dialogue-based agent that serves as an intelligent interface between users (elderly, caregivers, and health experts) and an ambient assistive living platform in real home settings.},
  archive      = {J_EXSY},
  author       = {Georgios Meditskos and Efstratios Kontopoulos and Stefanos Vrochidis and Ioannis Kompatsiaris},
  doi          = {10.1111/exsy.12378},
  journal      = {Expert Systems},
  month        = {2},
  number       = {1},
  pages        = {e12378},
  shortjournal = {Expert Syst.},
  title        = {Converness: Ontology-driven conversational awareness and context understanding in multimodal dialogue systems},
  volume       = {37},
  year         = {2020},
}
</textarea>
</details></li>
</ul>

</body>
</html>
