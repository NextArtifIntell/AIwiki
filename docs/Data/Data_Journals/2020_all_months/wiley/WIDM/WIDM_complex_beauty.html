<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>WIDM_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="widm---50">WIDM - 50</h2>
<ul>
<li><details>
<summary>
(2020). Smart city and resilient city: Differences and connections.
<em>WIDM</em>, <em>10</em>(6), e1388. (<a
href="https://doi.org/10.1002/widm.1388">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Smart city (SC) and resilient city (RC) have been studied and practiced over the years in terms of the increasing urban problems and disasters. However, there is a large overlap between their meanings and relationships. With an increasing concern for both SC and RC in urban development and hazard mitigation, a review was conducted to explore the differences and connections between SC and RC with scientometric analysis. There are far more literatures about SC than RC, and very few papers discuss SC and RC together. The research trend, category, and hotspots from research clusters are illustrated and compared. Major differences are discussed from their objectives, driving force, current research focus, and criticism. The literatures both related to SC and RC are used to explore their connections, which are very limited. The results revealed that the RC&#39;s impact on SC are positive from physical, social, and environmental aspects, while SC&#39;s impacts on RC could be both positive and negative from the above three aspects. It is indicated that SC and RC are both important for urban planning and can be complementary to each other through proper design and governance, which implies the need for building a resilient smart city (RSC). This article is categorized under:},
  archive      = {J_WIDM},
  author       = {Shiyao Zhu and Dezhi Li and Haibo Feng and Tiantian Gu and Kasun Hewage and Rehan Sadiq},
  doi          = {10.1002/widm.1388},
  journal      = {Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
  month        = {11},
  number       = {6},
  pages        = {e1388},
  shortjournal = {WIREs Data Mining Knowl. Discov.},
  title        = {Smart city and resilient city: Differences and connections},
  volume       = {10},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Expression of concern: Wang, c., zhang, q., liu, w., liu, y.
&amp; miao, l. Facial feature discovery for ethnicity recognition. WIREs
data mining knowl. Discov. 9, e1278 (2019).
Https://doi.org/10.1002/widm.1278. <em>WIDM</em>, <em>10</em>(6), e1386.
(<a href="https://doi.org/10.1002/widm.1386">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_WIDM},
  doi          = {10.1002/widm.1386},
  journal      = {Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
  month        = {11},
  number       = {6},
  pages        = {e1386},
  shortjournal = {WIREs Data Mining Knowl. Discov.},
  title        = {Expression of concern: wang, c., zhang, q., liu, w., liu, y. &amp; miao, l. facial feature discovery for ethnicity recognition. WIREs data mining knowl. discov. 9, e1278 (2019). https://doi.org/10.1002/widm.1278},
  volume       = {10},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Combating disinformation in a social media age.
<em>WIDM</em>, <em>10</em>(6), e1385. (<a
href="https://doi.org/10.1002/widm.1385">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The creation, dissemination, and consumption of disinformation and fabricated content on social media is a growing concern, especially with the ease of access to such sources, and the lack of awareness of the existence of such false information. In this article, we present an overview of the techniques explored to date for the combating of disinformation with various forms. We introduce different forms of disinformation, discuss factors related to the spread of disinformation, elaborate on the inherent challenges in detecting disinformation, and show some approaches to mitigating disinformation via education, research, and collaboration. Looking ahead, we present some promising future research directions on disinformation. This article is categorized under:},
  archive      = {J_WIDM},
  author       = {Kai Shu and Amrita Bhattacharjee and Faisal Alatawi and Tahora H. Nazer and Kaize Ding and Mansooreh Karami and Huan Liu},
  doi          = {10.1002/widm.1385},
  journal      = {Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
  month        = {11},
  number       = {6},
  pages        = {e1385},
  shortjournal = {WIREs Data Mining Knowl. Discov.},
  title        = {Combating disinformation in a social media age},
  volume       = {10},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Uncertainty quantification for multilabel text
classification. <em>WIDM</em>, <em>10</em>(6), e1384. (<a
href="https://doi.org/10.1002/widm.1384">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep neural networks have recently achieved impressive performance on multilabel text classification. However, the uncertainty in multilabel text classification tasks and their application in the model are often overlooked. To better understand and evaluate the uncertainty in multilabel text classification tasks, we propose a general framework called Uncertainty Quantification for Multilabel Text Classification framework. Based on the prediction results produced by traditional neural networks, the aleatory uncertainty of each classification label and the epistemic uncertainty of the prediction result can further be obtained by this framework. We design experiments to characterize the properties of aleatory uncertainty and epistemic uncertainty from the data characteristics and model features. The experimental results show that this framework is reasonable. Furthermore, we demonstrate how this framework allows us to define the model optimization criterion to identify policies that balance the expected training cost, model performance, and uncertainty sensitivity. This article is categorized under:},
  archive      = {J_WIDM},
  author       = {Wenshi Chen and Bowen Zhang and Mingyu Lu},
  doi          = {10.1002/widm.1384},
  journal      = {Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
  month        = {11},
  number       = {6},
  pages        = {e1384},
  shortjournal = {WIREs Data Mining Knowl. Discov.},
  title        = {Uncertainty quantification for multilabel text classification},
  volume       = {10},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Automatic question generation. <em>WIDM</em>,
<em>10</em>(6), e1382. (<a
href="https://doi.org/10.1002/widm.1382">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Automatic generation of semantically well-formed questions from a given text can contribute to various domains, including education, dialogues/interactive question answering systems, search engines, and more. It is well-known as a challenging task, which involves the common obstacles of other natural language processing (NLP) activities. We start this advanced review with a brief overview of the most common automatic question generation (AQG) applications. Then we describe the main steps of a typical AQG pipeline, namely question construction, ranking, and evaluation. Finally, we discuss the open challenges of the AQG field that still need to be addressed by NLP researchers. This article is categorized under:},
  archive      = {J_WIDM},
  author       = {Mark Last and Guy Danon},
  doi          = {10.1002/widm.1382},
  journal      = {Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
  month        = {11},
  number       = {6},
  pages        = {e1382},
  shortjournal = {WIREs Data Mining Knowl. Discov.},
  title        = {Automatic question generation},
  volume       = {10},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). An overview of unsupervised drift detection methods.
<em>WIDM</em>, <em>10</em>(6), e1381. (<a
href="https://doi.org/10.1002/widm.1381">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Practical applications involving big data, such as weather monitoring, identification of customer preferences, Internet log analysis, and sensors warnings require challenging data analysis, since these are examples of problems whose data are generated in streams and usually demand real-time analytics. Patterns in such data stream problems may change quickly. Consequently, machine learning models that operate in this context must be updated over time. This phenomenon is called concept drift in machine learning and data mining literature. Several different directions have been pursued to learn from data stream and to deal with concept drift. However, most drift detection methods consider that an instance&#39;s class label is available right after its prediction, since these methods work by monitoring the prediction results of a base classifier or an ensemble of classifiers. Nevertheless, this constraint is unrealistic in several practical problems. To cope with this constraint, some works are focused on proposing efficient unsupervised or semi-supervised concept drift detectors. While interesting and recent overview papers dedicated to supervised drift detectors have been published, the scenario is not the same in terms of unsupervised methods. Therefore, this work presents a comprehensive overview of approaches that tackle concept drift in classification problems in an unsupervised manner. Additional contribution includes a proposed taxonomy of state-of-the-art approaches for concept drift detection based on unsupervised strategies. This article is categorized under:},
  archive      = {J_WIDM},
  author       = {Rosana Noronha Gemaque and Albert França Josuá Costa and Rafael Giusti and Eulanda Miranda dos Santos},
  doi          = {10.1002/widm.1381},
  journal      = {Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
  month        = {11},
  number       = {6},
  pages        = {e1381},
  shortjournal = {WIREs Data Mining Knowl. Discov.},
  title        = {An overview of unsupervised drift detection methods},
  volume       = {10},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A survey of pattern mining in dynamic graphs. <em>WIDM</em>,
<em>10</em>(6), e1372. (<a
href="https://doi.org/10.1002/widm.1372">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph data is found in numerous domains such as for the analysis of social networks, sensor networks, bioinformatics, industrial systems, and chemistry. Analyzing graphs to identify useful and interesting patterns is an important research area. It helps understanding graphs, and hence support decision making. Since two decades, many graph mining algorithms have been proposed to identify patterns such as frequent subgraphs, paths, cliques, and trees. But most of them assume that graphs are static. This simplifying assumption makes it easy to design algorithms but discard information about how graphs evolve. This article provides a detailed survey of techniques for mining interesting patterns in dynamic graphs, which can serve both as an introduction and as a guide to recent advances and opportunities in this research area. The main tasks related to mining patterns in dynamic graphs are reviewed such as discovering frequent subgraphs, evolution rules, motifs, subgraph sequences, recurrent and triggering patterns, and trend sequences. In addition, an overview of strategies and approaches to solve dynamic graph mining problems is presented, and their advantages and limitations are highlighted. Various extensions are also discussed such as to discover patterns in data streams and big data. Finally, the article mentions several research opportunities. This article is categorized under:},
  archive      = {J_WIDM},
  author       = {Philippe Fournier-Viger and Ganghuan He and Chao Cheng and Jiaxuan Li and Min Zhou and Jerry Chun-Wei Lin and Unil Yun},
  doi          = {10.1002/widm.1372},
  journal      = {Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
  month        = {11},
  number       = {6},
  pages        = {e1372},
  shortjournal = {WIREs Data Mining Knowl. Discov.},
  title        = {A survey of pattern mining in dynamic graphs},
  volume       = {10},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Explainable artificial intelligence and machine learning: A
reality rooted perspective. <em>WIDM</em>, <em>10</em>(6), e1368. (<a
href="https://doi.org/10.1002/widm.1368">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As a consequence of technological progress, nowadays, one is used to the availability of big data generated in nearly all fields of science. However, the analysis of such data possesses vast challenges. One of these challenges relates to the explainability of methods from artificial intelligence (AI) or machine learning. Currently, many of such methods are nontransparent with respect to their working mechanism and for this reason are called black box models, most notably deep learning methods. However, it has been realized that this constitutes severe problems for a number of fields including the health sciences and criminal justice and arguments have been brought forward in favor of an explainable AI (XAI). In this paper, we do not assume the usual perspective presenting XAI as it should be, but rather provide a discussion what XAI can be . The difference is that we do not present wishful thinking but reality grounded properties in relation to a scientific theory beyond physics. This article is categorized under:},
  archive      = {J_WIDM},
  author       = {Frank Emmert-Streib and Olli Yli-Harja and Matthias Dehmer},
  doi          = {10.1002/widm.1368},
  journal      = {Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
  month        = {11},
  number       = {6},
  pages        = {e1368},
  shortjournal = {WIREs Data Mining Knowl. Discov.},
  title        = {Explainable artificial intelligence and machine learning: A reality rooted perspective},
  volume       = {10},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). The use of machine learning in sport outcome prediction: A
review. <em>WIDM</em>, <em>10</em>(5), e1380. (<a
href="https://doi.org/10.1002/widm.1380">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The increase in the volume of structured and unstructured data related to more than just sport events leads to the development and increased use of techniques that extract information and employ machine-learning algorithms in predicting process outcomes based on input but not necessarily output data. Taking sports into consideration, predicting outcomes, and extracting valuable information has become appealing not only to sports workers but also to the wider audience, particularly in the areas of team management and sports betting. The aim of this article is to review the existing machine learning (ML) algorithms in predicting sport outcomes. Over 100 papers were analyzed and only some of these papers were taken into consideration. Almost all of the analyzed papers use some sort of feature selection and feature extraction, most often prior to using the machine-learning algorithm. As an evaluation method of ML algorithms, researchers, in most cases, use data segmentation with data being chronologically distributed. In addition to data segmentation, researchers also use the k -cross-evaluation method. Sport predictions are usually treated as a classification problem with one class being predicted and rare cases being predicted as numerical values. Mostly used ML models are neural networks using data segmentation. This article is categorized under:},
  archive      = {J_WIDM},
  author       = {Tomislav Horvat and Josip Job},
  doi          = {10.1002/widm.1380},
  journal      = {Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
  month        = {9},
  number       = {5},
  pages        = {e1380},
  shortjournal = {WIREs Data Mining Knowl. Discov.},
  title        = {The use of machine learning in sport outcome prediction: A review},
  volume       = {10},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Interpretability of machine learning-based prediction models
in healthcare. <em>WIDM</em>, <em>10</em>(5), e1379. (<a
href="https://doi.org/10.1002/widm.1379">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {There is a need of ensuring that learning (ML) models are interpretable. Higher interpretability of the model means easier comprehension and explanation of future predictions for end-users. Further, interpretable ML models allow healthcare experts to make reasonable and data-driven decisions to provide personalized decisions that can ultimately lead to higher quality of service in healthcare. Generally, we can classify interpretability approaches in two groups where the first focuses on personalized interpretation (local interpretability) while the second summarizes prediction models on a population level (global interpretability). Alternatively, we can group interpretability methods into model-specific techniques, which are designed to interpret predictions generated by a specific model, such as a neural network, and model-agnostic approaches, which provide easy-to-understand explanations of predictions made by any ML model. Here, we give an overview of interpretability approaches using structured data and provide examples of practical interpretability of ML in different areas of healthcare, including prediction of health-related outcomes, optimizing treatments, or improving the efficiency of screening for specific conditions. Further, we outline future directions for interpretable ML and highlight the importance of developing algorithmic solutions that can enable ML driven decision making in high-stakes healthcare problems. This article is categorized under:},
  archive      = {J_WIDM},
  author       = {Gregor Stiglic and Primoz Kocbek and Nino Fijacko and Marinka Zitnik and Katrien Verbert and Leona Cilar},
  doi          = {10.1002/widm.1379},
  journal      = {Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
  month        = {9},
  number       = {5},
  pages        = {e1379},
  shortjournal = {WIREs Data Mining Knowl. Discov.},
  title        = {Interpretability of machine learning-based prediction models in healthcare},
  volume       = {10},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Knowledge discovery from remote sensing images: A review.
<em>WIDM</em>, <em>10</em>(5), e1371. (<a
href="https://doi.org/10.1002/widm.1371">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The development of Earth observation (EO) technology has made the volume of remote sensing data archiving continually larger, but the knowledge hidden in massive remote sensing images has not been fully exploited. Through in-depth research on the artificial intelligence (AI)-based knowledge discovery approaches from remote sensing images, we divided them into four typical types according to their development stage, including rule-based approaches, data-driven approaches, reinforcement learning approaches, and ensemble methods. The basic principles, typical applications, advantages, and disadvantages have been detailed for commonly used algorithms within each category. Conclusions include the following: (a) Rule-based, data-driven and reinforcement learning algorithms form a trilogy from knowledge to data, and to capabilities. (b) Rule-based data mining algorithms can provide prior knowledge for data-driven approaches, the knowledge discovered by data-driven models can be as an important complement to expert knowledge and rule sets, and reinforcement learning approaches can effectively make up for the lack of training samples or small training sample in data-driven models. (c) The traditional data-driven machine learning approaches and their ensemble methods are the current and may be the future mainstream methods for large regional and even global scale long time series remote sensing data mining and analysis, and improving their computing efficiency is the key research direction. (d) Deep learning, deep reinforcement learning, transfer learning, and an ensemble approach of the three may be the main means for small-area scope, short time series, and key geoscience information extraction from remote sensing images within a long time of the future. This article is categorized under:},
  archive      = {J_WIDM},
  author       = {Lizhe Wang and Jining Yan and Lin Mu and Liang Huang},
  doi          = {10.1002/widm.1371},
  journal      = {Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
  month        = {9},
  number       = {5},
  pages        = {e1371},
  shortjournal = {WIREs Data Mining Knowl. Discov.},
  title        = {Knowledge discovery from remote sensing images: A review},
  volume       = {10},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Publisher’s note: Wang, c., zhang, q., liu, w., liu, y.
&amp; miao, l. Facial feature discovery for ethnicity recognition. WIREs
data mining knowl. Discov. 9, e1278 (2019).
Https://doi.org/10.1002/widm.1278. <em>WIDM</em>, <em>10</em>(5), e1370.
(<a href="https://doi.org/10.1002/widm.1370">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_WIDM},
  doi          = {10.1002/widm.1370},
  journal      = {Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
  month        = {9},
  number       = {5},
  pages        = {e1370},
  shortjournal = {WIREs Data Mining Knowl. Discov.},
  title        = {Publisher&#39;s note: wang, c., zhang, q., liu, w., liu, y. &amp; miao, l. facial feature discovery for ethnicity recognition. WIREs data mining knowl. discov. 9, e1278 (2019). https://doi.org/10.1002/widm.1278},
  volume       = {10},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Data-driven approach to application programming interface
documentation mining: A review. <em>WIDM</em>, <em>10</em>(5), e1369.
(<a href="https://doi.org/10.1002/widm.1369">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Application programming interface (API) is an important form of software reuse. API documentations, such as API specifications, tutorials, and online forums, are valuable learning resources for reusing the APIs. In recent years, many data-driven API documentation mining (ADM) methods have been proposed. These methods mine API documentations and return API-related information to help developers better understand and reuse APIs. These methods treat documentations as unstructured data and apply various data mining techniques to analyze the documentation data. Currently, there is no comprehensive review of the data-driven approach to API documentation mining. This review aims to fill in this gap by analyzing and discussing the state of the art ADM papers. We survey 32 representative papers published in prominent software engineering journals and conferences in recent 5 years (January 2014–July 2019). We analyze their mining tasks, mined data, problems, data mining techniques, and evaluation metrics. Based on the survey results, we point out research challenges and future research directions in this area. This article is categorized under:},
  archive      = {J_WIDM},
  author       = {Di Wu and Xiao-Yuan Jing and Hongyu Zhang and Xiaohui Kong and Yu Xie and Zhiguo Huang},
  doi          = {10.1002/widm.1369},
  journal      = {Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
  month        = {9},
  number       = {5},
  pages        = {e1369},
  shortjournal = {WIREs Data Mining Knowl. Discov.},
  title        = {Data-driven approach to application programming interface documentation mining: A review},
  volume       = {10},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Role of sentiment analysis in social media security and
analytics. <em>WIDM</em>, <em>10</em>(5), e1366. (<a
href="https://doi.org/10.1002/widm.1366">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Social media, in recent times, has with eased an explosion of data with so many social media platforms available to interact and express opinions freely. This has led to easy access to the privacy of social media users which raise broader security concerns and issues. The present paper provides an overview of various sentiment analysis approaches and techniques for social media security and analytics. The multiple security application domains like deception detection, anomaly detection, risk management, and disaster relief have been identified where sentiment analysis is used for social media security. An in-depth study on security issues related to data provenance, distrust, e-commerce security, consumer security breaches, market surveillance, credibility, and risk assessment in social media have been presented. A comparison of various techniques, methodologies, dataset, and application domain where sentiment analysis is used has been discussed. The present work discusses the results of different machine learning techniques based on the performance metrics that have been used for the implementation of sentiment analysis in the respective security domains. It identifies the various gaps, issues, and the recent advancements in the field and presents a line of work that needs to be carried forward in future. This article is categorized under:},
  archive      = {J_WIDM},
  author       = {Sanur Sharma and Anurag Jain},
  doi          = {10.1002/widm.1366},
  journal      = {Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
  month        = {9},
  number       = {5},
  pages        = {e1366},
  shortjournal = {WIREs Data Mining Knowl. Discov.},
  title        = {Role of sentiment analysis in social media security and analytics},
  volume       = {10},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A review of mammographic region of interest classification.
<em>WIDM</em>, <em>10</em>(5), e1357. (<a
href="https://doi.org/10.1002/widm.1357">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Early detection of breast cancer is important and highly valuable in clinical practice. X-ray mammography is broadly used for prescreening the breast and is also attractive due to its noninvasive nature. However, experts can misdiagnose a significant proportion of the cases, which may either cause redundant examinations or cancer. In order to reduce false positive and negative rates of mammography screening, computer-aided breast cancer detection has been studied for more than 30 years and many methods have been proposed by the researchers. In this review, region of interest (ROI) classification methods, which operate on a predefined or segmented ROIs with a focus on mass classification are surveyed. A total of 72 high quality journal and conference papers are selected from the Web of Science (WOS) database that meet several inclusion criteria. A comparative analysis is provided based on ROI extraction methods, data sets and machine learning techniques employed, the prediction accuracies, and usage frequency statistics. Based on the performances obtained on publicly available data sets, the ROI classification problem from mammogram images can be considered as approaching to be solved. Nonetheless, it can still be used as complementary information in breast cancer detection from the whole mammograms, which has room for improvement. This article is categorized under:},
  archive      = {J_WIDM},
  author       = {Sena B. Yengec Tasdemir and Kasim Tasdemir and Zafer Aydin},
  doi          = {10.1002/widm.1357},
  journal      = {Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
  month        = {9},
  number       = {5},
  pages        = {e1357},
  shortjournal = {WIREs Data Mining Knowl. Discov.},
  title        = {A review of mammographic region of interest classification},
  volume       = {10},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Temporal association rule mining: An overview considering
the time variable as an integral or implied component. <em>WIDM</em>,
<em>10</em>(4), e1367. (<a
href="https://doi.org/10.1002/widm.1367">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Association rules are commonly used to provide decision-makers with knowledge that helps them to make good decisions. Most of the published proposals mine association rules without paying particular attention to temporal information. However, in real-life applications data usually change over time or presenting different temporal situations. Therefore, the extracted knowledge may not be useful, since we may not know whether the rules are currently applicable or whether they will be applicable in the future. For this reason, in recent years, many methods have been proposed in the literature for mining temporal association rules, which introduce a greater predictive and descriptive power providing an additional degree of interestingness. One of the main problems in this research field is the lack of visibility most works suffer since there is no standard terminology to refer to it, making it difficult to find and compare proposals and studies in the field. This contribution attempts to offer a well-defined framework that allows researchers both to easily locate the previous proposals and to propose well-grounded methods in the future. To accomplish both objectives, a two-level taxonomy is proposed according to whether the time variable is considered to provide order to the data collection and to locate some temporal constraints, or whether it is considered as an attribute within the learning process. Some recent applications, available software tools, and a bibliographical analysis in accordance with the Web of Science are also shown. Finally, some critical considerations and potential further directions are discussed. This article is categorized under:},
  archive      = {J_WIDM},
  author       = {Alberto Segura-Delgado and María José Gacto and Rafael Alcalá and Jesús Alcalá-Fdez},
  doi          = {10.1002/widm.1367},
  journal      = {Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
  month        = {7},
  number       = {4},
  pages        = {e1367},
  shortjournal = {WIREs Data Mining Knowl. Discov.},
  title        = {Temporal association rule mining: An overview considering the time variable as an integral or implied component},
  volume       = {10},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Continuous authentication using biometrics: An advanced
review. <em>WIDM</em>, <em>10</em>(4), e1365. (<a
href="https://doi.org/10.1002/widm.1365">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The shortcomings of conventional access control systems for high-security environments have led to the concert of continuous authentication. Contrary to traditional verification, in which users are authenticated only once at the start of their session, continuous authentication systems regularly check users&#39; identities to prevent hijackings. The challenges in this area involve balancing the security of protected assets by quickly detecting intruders with the system usability for genuine users. Biometric recognition plays a major role within this context, as it is the main way to assure that users are who they claim to be. A comparative analysis of the latest works revealed different aspects of this problem. First, some biometrics traits among those applied for continuous authentication are more suitable for this task than others. Second, systems combining multiple traits have advantages over those relying on a single one. Finally, many works fail to report proper evaluation metrics. With this in mind, we were able to identify new opportunities for researchers in the field. We highlight the potential for mining new datasets on the internet, which would benefit validation and benchmarking, and how recent deep learning techniques could address some of the open challenges in the area. This article is categorized under:},
  archive      = {J_WIDM},
  author       = {Gabriel Dahia and Leone Jesus and Maurício Pamplona Segundo},
  doi          = {10.1002/widm.1365},
  journal      = {Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
  month        = {7},
  number       = {4},
  pages        = {e1365},
  shortjournal = {WIREs Data Mining Knowl. Discov.},
  title        = {Continuous authentication using biometrics: An advanced review},
  volume       = {10},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Online streaming feature selection with incremental feature
grouping. <em>WIDM</em>, <em>10</em>(4), e1364. (<a
href="https://doi.org/10.1002/widm.1364">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Today, the dimensionality of data is increasing in a massive way. Thus, traditional feature selection techniques are not directly applicable. Consequently, recent research has led to the development of a more efficient approach to the selection of features from a feature stream, known as streaming feature selection. Another active research area, related to feature selection, is feature grouping. Feature grouping selects relevant features by evaluating the hidden information of selected features. However, although feature grouping is a promising technique, it is not directly applicable to feature streams. In this paper, we propose a novel and efficient algorithm that uses online feature grouping, embedded within a new incremental technique, to select features from a feature stream. This technique groups similar features together; it assigns new incoming features to an existing group or creates a new group. To the best of our knowledge, this is the first approach that proposes the use of incremental feature grouping to perform feature selection from features. We have implemented this algorithm and evaluated it, using benchmark datasets, against state-of-the-art streaming feature selection algorithms that use feature grouping or incremental selection techniques. The results show superior performance by the proposed technique through combining the online selection and grouping, in terms of prediction accuracy and running time. This article is categorized under:},
  archive      = {J_WIDM},
  author       = {Noura Al Nuaimi and Mohammad M. Masud},
  doi          = {10.1002/widm.1364},
  journal      = {Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
  month        = {7},
  number       = {4},
  pages        = {e1364},
  shortjournal = {WIREs Data Mining Knowl. Discov.},
  title        = {Online streaming feature selection with incremental feature grouping},
  volume       = {10},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Designing and deploying insurance recommender systems using
machine learning. <em>WIDM</em>, <em>10</em>(4), e1363. (<a
href="https://doi.org/10.1002/widm.1363">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recommender systems have become extremely important to various types of industries where customer interaction and feedback is paramount to the success of the business. For companies that face changes that arise with ever-growing markets, providing product recommendations to new and existing customers is a challenge. Our goal is to give our customers personalized recommendations based on what other similar people with similar portfolios have, in order to make sure they are adequately covered for their needs. Our system uses customer characteristics in addition to customer portfolio data. Since the number of possible recommendable products is relatively small, compared to other recommender domains, and missing data is relatively frequent, we chose to use Bayesian Networks for modeling our systems. We also present a deep-learning-based approach to provide recommendations to prospects (potential customers) where only external marketing data is available at the time of prediction. This article is categorized under:},
  archive      = {J_WIDM},
  author       = {Maleeha Qazi and Kaya Tollas and Teja Kanchinadam and Joseph Bockhorst and Glenn Fung},
  doi          = {10.1002/widm.1363},
  journal      = {Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
  month        = {7},
  number       = {4},
  pages        = {e1363},
  shortjournal = {WIREs Data Mining Knowl. Discov.},
  title        = {Designing and deploying insurance recommender systems using machine learning},
  volume       = {10},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). On the determinants of uber accessibility and its spatial
distribution: Evidence from uber in philadelphia. <em>WIDM</em>,
<em>10</em>(4), e1362. (<a
href="https://doi.org/10.1002/widm.1362">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study investigates the impact of socioeconomics and demographic factors (e.g., population density, minority rate, age, gender, education, wealth, and crime) and transportation infrastructure (e.g., walk score, transit score, and bike score) on the accessibility of Uber in the city of Philadelphia. K -means clustering is applied for initial data exploration. Based on the spatial model selection diagnostic tests, we developed maximum likelihood spatial lag models with queen contiguity spatial weight matrix. The results show that Uber accessibility is not balanced in different neighborhoods in Philadelphia. Uber is more accessible in denser areas with the high male population, better public transportation access and less access to amenities in the walkable distances. Moreover, we observed that Uber is more accessible in areas with a high crime rate. This observation shows that Uber has made it easier to get out of high crime rate areas. Finally, contribution in the literature on accessibility in ride-sourcing networks is discussed. Findings are additionally used to provide managerial implications to mitigate discrimination in ride-sourcing platforms. This article is categorized under:},
  archive      = {J_WIDM},
  author       = {Sina Shokoohyar and Anae Sobhani and Saeed R. Ramezanpour Nargesi},
  doi          = {10.1002/widm.1362},
  journal      = {Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
  month        = {7},
  number       = {4},
  pages        = {e1362},
  shortjournal = {WIREs Data Mining Knowl. Discov.},
  title        = {On the determinants of uber accessibility and its spatial distribution: Evidence from uber in philadelphia},
  volume       = {10},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). O-MedAL: Online active deep learning for medical image
analysis. <em>WIDM</em>, <em>10</em>(4), e1353. (<a
href="https://doi.org/10.1002/widm.1353">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Active learning (AL) methods create an optimized labeled training set from unlabeled data. We introduce a novel online active deep learning method for medical image analysis. We extend our MedAL AL framework to present new results in this paper. A novel sampling method queries the unlabeled examples that maximize the average distance to all training set examples. Our online method enhances performance of its underlying baseline deep network. These novelties contribute to significant performance improvements, including improving the model&#39;s underlying deep network accuracy by 6.30%, using only 25% of the labeled dataset to achieve baseline accuracy, reducing backpropagated images during training by as much as 67%, and demonstrating robustness to class imbalance in binary and multiclass tasks. This article is categorized under: Technologies &gt; Machine Learning Technologies &gt; Classification Application Areas &gt; Health Care},
  archive      = {J_WIDM},
  author       = {Asim Smailagic and Pedro Costa and Alex Gaudio and Kartik Khandelwal and Mostafa Mirshekari and Jonathon Fagert and Devesh Walawalkar and Susu Xu and Adrian Galdran and Pei Zhang and Aurélio Campilho and Hae Young Noh},
  doi          = {10.1002/widm.1353},
  journal      = {Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
  month        = {7},
  number       = {4},
  pages        = {e1353},
  shortjournal = {WIREs Data Mining Knowl. Discov.},
  title        = {O-MedAL: Online active deep learning for medical image analysis},
  volume       = {10},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A survey and taxonomy of adversarial neural networks for
text-to-image synthesis. <em>WIDM</em>, <em>10</em>(4), e1345. (<a
href="https://doi.org/10.1002/widm.1345">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Text-to-image synthesis refers to computational methods which translate human written textual descriptions, in the form of keywords or sentences, into images with similar semantic meaning to the text. In earlier research, image synthesis relied mainly on word to image correlation analysis combined with supervised methods to find best alignment of the visual content matching to the text. Recent progress in deep learning (DL) has brought a new set of unsupervised DL methods, particularly deep generative models which are able to generate realistic visual images using suitably trained neural network models. The change of direction from the computer vision-based approaches to artificial intelligence (AI)-driven methods ignited the intense interest in industry, such as virtual reality, recreational &amp; professional (eSports) gaming, and computer-aided design, and so on, to automatically generate compelling images from text-based natural language descriptions. In this paper, we review the most recent development in the text-to-image synthesis research domain. Our goal is to provide value by delivering a comparative review of the state-of-the-art models in terms of their architecture and design. The survey first introduces image synthesis and its challenges, and then reviews key concepts such as generative adversarial networks (GANs) and deep convolutional encoder-decoder neural networks (DCNNs). After that, we propose a taxonomy to summarize GAN-based text-to-image synthesis into four major categories: semantic enhancement GANs, resolution enhancement GANs, diversity enhancement GANS, and motion enhancement GANs. We elaborate on the main objective of each group, and further review typical GAN architectures in each group. The taxonomy and the review outline the techniques and the evolution of different approaches, and eventually provide a roadmap to summarize the list of contemporaneous solutions that utilize GANs and DCNNs to generate enthralling results in categories such as human faces, birds, flowers, room interiors, object reconstruction from edge maps (games), and so on. The survey concludes with a comparison of the proposed solutions, challenges that remain unresolved, and future developments in the text-to-image synthesis domain. This article is categorized under:},
  archive      = {J_WIDM},
  author       = {Jorge Agnese and Jonathan Herrera and Haicheng Tao and Xingquan Zhu},
  doi          = {10.1002/widm.1345},
  journal      = {Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
  month        = {7},
  number       = {4},
  pages        = {e1345},
  shortjournal = {WIREs Data Mining Knowl. Discov.},
  title        = {A survey and taxonomy of adversarial neural networks for text-to-image synthesis},
  volume       = {10},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). How to cheat the page limit. <em>WIDM</em>, <em>10</em>(3),
e1361. (<a href="https://doi.org/10.1002/widm.1361">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Every conference imposing a limit on the length of submissions must deal with the problem of page limit cheating: authors tweaking the parameters of the game such that they can squeeze more content into their paper. We claim that this problem is endemic, although we lack the data to formally prove this. Instead, this paper provides a far from exhaustive summary of ways to cheat the page limit, a case study involving the papers accepted for the Research and Applied Data Science tracks at the European Conference on Machine Learning and Principles and Practice of Knowledge Discovery in Databases (ECMLPKDD) 2019, and a discussion of ways for program chairs to tackle this problem. Of the 130 accepted papers in these two ECMLPKDD 2019 tracks, 68 satisfied the page limit; 62 (47.7%) turned out to spill over the page limit, by up to as much as 50%. To misappropriate a phrase from Darrell Huff&#39;s “How to Lie with Statistics,” we intend for this paper not to be a manual for swindlers; instead, nefarious paper authors already know these tricks, and honest program chairs must learn them in self-defense. This article is categorized under:},
  archive      = {J_WIDM},
  author       = {Wouter Duivesteijn and Sibylle Hess and Xin Du},
  doi          = {10.1002/widm.1361},
  journal      = {Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
  month        = {5},
  number       = {3},
  pages        = {e1361},
  shortjournal = {WIREs Data Mining Knowl. Discov.},
  title        = {How to cheat the page limit},
  volume       = {10},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Data mining powered by the gene ontology. <em>WIDM</em>,
<em>10</em>(3), e1359. (<a
href="https://doi.org/10.1002/widm.1359">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The gene ontology (GO) is a widely used resource for describing molecular functions, biological processes, and cellular components of gene products. Since its inception in 2006, the GO has been used to describe millions of gene products resulting in a massive data store of over 6 million annotations. The staggering amount of data that has resulted from annotating gene products with GO terms has led the way and opened new avenues for a wide variety of large-scale computational analyses. Specifically, a variety of data mining techniques such as association rule mining, clustering etc. have been applied successfully to a range of biological applications. This article provides a review of four data mining applications/techniques for GO data mining gene expression data, association rule mining, clustering, and text mining and highlights future directions in each of these areas. This article is categorized under: Algorithmic Development &gt; Association Rules Algorithmic Development &gt; Biological Data Mining Ensemble Methods &gt; Text Mining},
  archive      = {J_WIDM},
  author       = {Prashanti Manda},
  doi          = {10.1002/widm.1359},
  journal      = {Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
  month        = {5},
  number       = {3},
  pages        = {e1359},
  shortjournal = {WIREs Data Mining Knowl. Discov.},
  title        = {Data mining powered by the gene ontology},
  volume       = {10},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Design and analysis of computer experiments with
quantitative and qualitative inputs: A selective review. <em>WIDM</em>,
<em>10</em>(3), e1358. (<a
href="https://doi.org/10.1002/widm.1358">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Computer experiment refers to the study of complex systems by using computer simulations to emulate the physical system. Design and analysis of computer experiments have attracted great attention in past decades. However, many computer experiments involve not only quantitative inputs, but also qualitative inputs, which make the design and analysis more challenging. The Latin hypercube design and its variants are widely used in computer experiments, but mainly for the quantitative inputs. Constructing desirable emulators for computer experiments with qualitative inputs also remains a challenging problem due to the discrete nature of qualitative inputs. In this review, we describe a set of statistical approaches for design and analysis of computer experiments with both quantitative and qualitative factors. This article is categorized under: Fundamental Concepts of Data and Knowledge &gt; Key Design Issues in Data Mining Algorithmic Development &gt; Statistics},
  archive      = {J_WIDM},
  author       = {Xiaoning Kang and Xinwei Deng},
  doi          = {10.1002/widm.1358},
  journal      = {Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
  month        = {5},
  number       = {3},
  pages        = {e1358},
  shortjournal = {WIREs Data Mining Knowl. Discov.},
  title        = {Design and analysis of computer experiments with quantitative and qualitative inputs: A selective review},
  volume       = {10},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Bias in data-driven artificial intelligence systems—an
introductory survey. <em>WIDM</em>, <em>10</em>(3), e1356. (<a
href="https://doi.org/10.1002/widm.1356">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Artificial Intelligence (AI)-based systems are widely employed nowadays to make decisions that have far-reaching impact on individuals and society. Their decisions might affect everyone, everywhere, and anytime, entailing concerns about potential human rights issues. Therefore, it is necessary to move beyond traditional AI algorithms optimized for predictive performance and embed ethical and legal principles in their design, training, and deployment to ensure social good while still benefiting from the huge potential of the AI technology. The goal of this survey is to provide a broad multidisciplinary overview of the area of bias in AI systems, focusing on technical challenges and solutions as well as to suggest new research directions towards approaches well-grounded in a legal frame. In this survey, we focus on data-driven AI, as a large part of AI is powered nowadays by (big) data and powerful machine learning algorithms. If otherwise not specified, we use the general term bias to describe problems related to the gathering or processing of data that might result in prejudiced decisions on the bases of demographic features such as race, sex, and so forth. This article is categorized under:},
  archive      = {J_WIDM},
  author       = {Eirini Ntoutsi and Pavlos Fafalios and Ujwal Gadiraju and Vasileios Iosifidis and Wolfgang Nejdl and Maria-Esther Vidal and Salvatore Ruggieri and Franco Turini and Symeon Papadopoulos and Emmanouil Krasanakis and Ioannis Kompatsiaris and Katharina Kinder-Kurlanda and Claudia Wagner and Fariba Karimi and Miriam Fernandez and Harith Alani and Bettina Berendt and Tina Kruegel and Christian Heinze and Klaus Broelemann and Gjergji Kasneci and Thanassis Tiropanis and Steffen Staab},
  doi          = {10.1002/widm.1356},
  journal      = {Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
  month        = {5},
  number       = {3},
  pages        = {e1356},
  shortjournal = {WIREs Data Mining Knowl. Discov.},
  title        = {Bias in data-driven artificial intelligence systems—An introductory survey},
  volume       = {10},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Educational data mining and learning analytics: An updated
survey. <em>WIDM</em>, <em>10</em>(3), e1355. (<a
href="https://doi.org/10.1002/widm.1355">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This survey is an updated and improved version of the previous one published in 2013 in this journal with the title “data mining in education”. It reviews in a comprehensible and very general way how Educational Data Mining and Learning Analytics have been applied over educational data. In the last decade, this research area has evolved enormously and a wide range of related terms are now used in the bibliography such as Academic Analytics, Institutional Analytics, Teaching Analytics, Data-Driven Education, Data-Driven Decision-Making in Education, Big Data in Education, and Educational Data Science. This paper provides the current state of the art by reviewing the main publications, the key milestones, the knowledge discovery cycle, the main educational environments, the specific tools, the free available datasets, the most used methods, the main objectives, and the future trends in this research area. This article is categorized under: Application Areas &gt; Education and Learning},
  archive      = {J_WIDM},
  author       = {Cristobal Romero and Sebastian Ventura},
  doi          = {10.1002/widm.1355},
  journal      = {Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
  month        = {5},
  number       = {3},
  pages        = {e1355},
  shortjournal = {WIREs Data Mining Knowl. Discov.},
  title        = {Educational data mining and learning analytics: An updated survey},
  volume       = {10},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Forensic intelligence and the analytical process.
<em>WIDM</em>, <em>10</em>(3), e1354. (<a
href="https://doi.org/10.1002/widm.1354">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A review was undertaken of the developments made with integrating forensic evidence into the analytical process to support police investigations. Evidence such as DNA, fingerprints, fibers, accelerants, tyre marks, and so forth, can support to differing degrees the various working theories or hypotheses about the nature of the alleged crime, the persons of interest and the modus operandi. Investigators however, either forensic or detective, bring various biases to evidence capture and analysis, biases which are better understood in the intelligence community. Structured analytical techniques have a long history in intelligence analysis, for example analysis of competing hypotheses, which serves several purposes: information sharing, clarity of communication, and to highlight the common forms of bias brought to bear in an investigation. We illustrate the representation of links based on traces and intelligence, and how these can be stored in databases permitting better “reasoning” with evidence. We also present some recommendations for integration of forensic intelligence into the investigative analytic process and review information systems in this area. This article is categorized under:},
  archive      = {J_WIDM},
  author       = {Giles Oatley and Brendan Chapman and James Speers},
  doi          = {10.1002/widm.1354},
  journal      = {Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
  month        = {5},
  number       = {3},
  pages        = {e1354},
  shortjournal = {WIREs Data Mining Knowl. Discov.},
  title        = {Forensic intelligence and the analytical process},
  volume       = {10},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Learning decision trees through monte carlo tree search: An
empirical evaluation. <em>WIDM</em>, <em>10</em>(3), e1348. (<a
href="https://doi.org/10.1002/widm.1348">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Decision trees (DTs) are a widely used prediction tool, owing to their interpretability. Standard learning methods follow a locally optimal approach that trades off prediction performance for computational efficiency. Such methods can however be far from optimal, and it may pay off to spend more computational resources to increase performance. Monte Carlo tree search (MCTS) is an approach to approximate optimal choices in exponentially large search spaces. We propose a DT learning approach based on the Upper Confidence Bound applied to tree (UCT) algorithm, including procedures to expand and explore the space of DTs. To mitigate the computational cost of our method, we employ search pruning strategies that discard some branches of the search tree. The experiments show that proposed approach outperformed the C4.5 algorithm in 20 out of 31 datasets, with statistically significant improvements in the trade-off between prediction performance and DT complexity. The approach improved locally optimal search for datasets with more than 1,000 instances, or for smaller datasets likely arising from complex distributions. This article is categorized under:},
  archive      = {J_WIDM},
  author       = {Cecília Nunes and Mathieu De Craene and Hélène Langet and Oscar Camara and Anders Jonsson},
  doi          = {10.1002/widm.1348},
  journal      = {Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
  month        = {5},
  number       = {3},
  pages        = {e1348},
  shortjournal = {WIREs Data Mining Knowl. Discov.},
  title        = {Learning decision trees through monte carlo tree search: An empirical evaluation},
  volume       = {10},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Extraction, correlation, and abstraction of event data for
process mining. <em>WIDM</em>, <em>10</em>(3), e1346. (<a
href="https://doi.org/10.1002/widm.1346">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Process mining provides a rich set of techniques to discover valuable knowledge of business processes based on data that was recorded in different types of information systems. It enables analysis of end-to-end processes to facilitate process re-engineering and process improvement. Process mining techniques rely on the availability of data in the form of event logs. In order to enable process mining in diverse environments, the recorded data need to be located and transformed to event logs. The journey from raw data to event logs suitable for process mining can be addressed by a variety of methods and techniques, which are the focus of this article. In particular, techniques proposed in the literature to support the creation of event logs from raw data are reviewed and classified. This includes techniques for identification and extraction of the required event data from diverse sources as well as their correlation and abstraction. This article is categorized under:},
  archive      = {J_WIDM},
  author       = {Kiarash Diba and Kimon Batoulis and Matthias Weidlich and Mathias Weske},
  doi          = {10.1002/widm.1346},
  journal      = {Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
  month        = {5},
  number       = {3},
  pages        = {e1346},
  shortjournal = {WIREs Data Mining Knowl. Discov.},
  title        = {Extraction, correlation, and abstraction of event data for process mining},
  volume       = {10},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Internet of things and data analytics: A current review.
<em>WIDM</em>, <em>10</em>(3), e1341. (<a
href="https://doi.org/10.1002/widm.1341">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the advent of Internet and computing, we entered into an era with more people exchanging information over the Internet using devices like desktops, laptops, tablets, mobile phones, and similar data transmitting and receiving gadgets. This was a host centric communication approach. Internet of Things (IoT) is the next stage of technological advancement in computation, networking and communication with physical objects around the world getting connected to the network and exchanging data. This is an information centric approach. Thus it can be defined as an expanding physical network of dynamically increasing physical objects. The objects share information derived from their environments, reliably, and securely over the communication medium leveraging multiple protocols. The protocols involved are getting standardized to address the compatibility and interoperability issues. Each object connected is uniquely identified and controlled in the network. IoT finds its applications in many fields as environment monitoring, logistics, health care, automobile, controlled industrial environment, smart cities, and many more. As the devices, their cardinality and alignment, the data type, the data rate and specifics of the domain of the IoT applications vary; there is also a need to define architectures that incorporate the devices, communication mediums, storage, and analysis capabilities and the consumers of the derived output. Advancements in data analytics, machine learning, and digital technologies offer possibilities of derive meaningful intelligence for actionable output and in creating useful and context aware applications. This article is categorized under:},
  archive      = {J_WIDM},
  author       = {Gaurav Mohindru and Koushik Mondal and Haider Banka},
  doi          = {10.1002/widm.1341},
  journal      = {Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
  month        = {5},
  number       = {3},
  pages        = {e1341},
  shortjournal = {WIREs Data Mining Knowl. Discov.},
  title        = {Internet of things and data analytics: A current review},
  volume       = {10},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Facial feature discovery for ethnicity recognition.
<em>WIDM</em>, <em>10</em>(2), e1351. (<a
href="https://doi.org/10.1002/widm.1351">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_WIDM},
  doi          = {10.1002/widm.1351},
  journal      = {Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
  month        = {3},
  number       = {2},
  pages        = {e1351},
  shortjournal = {WIREs Data Mining Knowl. Discov.},
  title        = {Facial feature discovery for ethnicity recognition},
  volume       = {10},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Healthcare 4.0: A review of frontiers in digital health.
<em>WIDM</em>, <em>10</em>(2), e1350. (<a
href="https://doi.org/10.1002/widm.1350">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Healthcare 4.0 is a term that has emerged recently and derived from Industry 4.0. Today, the health care sector is more digital than in past decades; for example, spreading from x-rays and magnetic resonance imaging to computed tomography and ultrasound scans to electric medical records. With the wide spectrum of digital technologies underpinning Healthcare 4.0 to deliver more effective and efficient health care services, in this article, we use the wisdom pyramid methodology to conduct a systematic review of current digital frontiers in Healthcare 4.0. This article is categorized under:},
  archive      = {J_WIDM},
  author       = {Prem Prakash Jayaraman and Abdur Rahim Mohammad Forkan and Ahsan Morshed and Pari Delir Haghighi and Yong-Bin Kang},
  doi          = {10.1002/widm.1350},
  journal      = {Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
  month        = {3},
  number       = {2},
  pages        = {e1350},
  shortjournal = {WIREs Data Mining Knowl. Discov.},
  title        = {Healthcare 4.0: A review of frontiers in digital health},
  volume       = {10},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Surveying the reach and maturity of machine learning and
artificial intelligence in astronomy. <em>WIDM</em>, <em>10</em>(2),
e1349. (<a href="https://doi.org/10.1002/widm.1349">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Machine learning (automated processes that learn by example in order to classify, predict, discover, or generate new data) and artificial intelligence (methods by which a computer makes decisions or discoveries that would usually require human intelligence) are now firmly established in astronomy. Every week, new applications of machine learning and artificial intelligence are added to a growing corpus of work. Random forests, support vector machines, and neural networks are now having a genuine impact for applications as diverse as discovering extrasolar planets, transient objects, quasars, and gravitationally lensed systems, forecasting solar activity, and distinguishing between signals and instrumental effects in gravitational wave astronomy. This review surveys contemporary, published literature on machine learning and artificial intelligence in astronomy and astrophysics. Applications span seven main categories of activity: classification, regression, clustering, forecasting, generation, discovery, and the development of new scientific insights. These categories form the basis of a hierarchy of maturity, as the use of machine learning and artificial intelligence emerges, progresses, or becomes established. This article is categorized under:},
  archive      = {J_WIDM},
  author       = {Christopher J. Fluke and Colin Jacobs},
  doi          = {10.1002/widm.1349},
  journal      = {Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
  month        = {3},
  number       = {2},
  pages        = {e1349},
  shortjournal = {WIREs Data Mining Knowl. Discov.},
  title        = {Surveying the reach and maturity of machine learning and artificial intelligence in astronomy},
  volume       = {10},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). An enhanced feature-based sentiment analysis approach.
<em>WIDM</em>, <em>10</em>(2), e1347. (<a
href="https://doi.org/10.1002/widm.1347">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the last few years, online reviews where individuals express their thoughts, interests, experiences, and opinions have broadly spread over the internet. Sentiment analysis has evolved to analyze these online reviews and provide valuable insights for both individuals and organizations that may help them in making decisions. Unfortunately the performance of sentiment analysis process is affected by the nature of online reviews&#39; content that may contain emoticons and negation words. Moreover, spam reviews have been written for the purpose of deceiving others. Therefore, there is a need to develop an approach that considers these issues. In this paper, an enhanced approach for sentiment analysis is proposed which aims to enhance the performance of classifying reviews based on their features and assigning accurate sentiment score to features. This enhanced approach is achieved by handling negation, detecting emoticons, and detecting spam reviews using a combination of different types of properties which leads to achieving better predictive performance. The proposed approach has been verified against three datasets of different sizes. The results indicate that the proposed approach achieves a maximum accuracy of about 99.06% in detecting spam reviews and a maximum accuracy of about 97.13% in classifying reviews. This article is categorized under:},
  archive      = {J_WIDM},
  author       = {Nagwa M. K. Saeed and Nivin A. Helal and Nagwa L. Badr and Tarek F. Gharib},
  doi          = {10.1002/widm.1347},
  journal      = {Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
  month        = {3},
  number       = {2},
  pages        = {e1347},
  shortjournal = {WIREs Data Mining Knowl. Discov.},
  title        = {An enhanced feature-based sentiment analysis approach},
  volume       = {10},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Density-based clustering. <em>WIDM</em>, <em>10</em>(2),
e1343. (<a href="https://doi.org/10.1002/widm.1343">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Clustering refers to the task of identifying groups or clusters in a data set. In density-based clustering , a cluster is a set of data objects spread in the data space over a contiguous region of high density of objects. Density-based clusters are separated from each other by contiguous regions of low density of objects. Data objects located in low-density regions are typically considered noise or outliers. In this review article we discuss the statistical notion of density-based clusters, classic algorithms for deriving a flat partitioning of density-based clusters, methods for hierarchical density-based clustering, and methods for semi-supervised clustering. We conclude with some open challenges related to density-based clustering. This article is categorized under:},
  archive      = {J_WIDM},
  author       = {Ricardo J. G. B. Campello and Peer Kröger and Jörg Sander and Arthur Zimek},
  doi          = {10.1002/widm.1343},
  journal      = {Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
  month        = {3},
  number       = {2},
  pages        = {e1343},
  shortjournal = {WIREs Data Mining Knowl. Discov.},
  title        = {Density-based clustering},
  volume       = {10},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A review of keyphrase extraction. <em>WIDM</em>,
<em>10</em>(2), e1339. (<a
href="https://doi.org/10.1002/widm.1339">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Keyphrase extraction is a textual information processing task concerned with the automatic extraction of representative and characteristic phrases from a document that express all the key aspects of its content. Keyphrases constitute a succinct conceptual summary of a document, which is very useful in digital information management systems for semantic indexing, faceted search, document clustering and classification. This article introduces keyphrase extraction, provides a well-structured review of the existing work, offers interesting insights on the different evaluation approaches, highlights open issues and presents a comparative experimental study of popular unsupervised techniques on five datasets. This article is categorized under:},
  archive      = {J_WIDM},
  author       = {Eirini Papagiannopoulou and Grigorios Tsoumakas},
  doi          = {10.1002/widm.1339},
  journal      = {Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
  month        = {3},
  number       = {2},
  pages        = {e1339},
  shortjournal = {WIREs Data Mining Knowl. Discov.},
  title        = {A review of keyphrase extraction},
  volume       = {10},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Method evaluation, parameterization, and result validation
in unsupervised data mining: A critical survey. <em>WIDM</em>,
<em>10</em>(2), e1330. (<a
href="https://doi.org/10.1002/widm.1330">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Machine Learning (ML) and Data Mining (DM) build tools intended to help users solve data-related problems that are infeasible for “unaugmented” humans. Tools need manuals, however, and in the case of ML/DM methods, this means guidance with respect to which technique to choose, how to parameterize it, and how to interpret derived results to arrive at knowledge about the phenomena underlying the data. While such information is available in the literature, it has not yet been collected in one place. We survey three types of work for clustering and pattern mining: (1) comparisons of existing techniques, (2) evaluations of different parameterization options and studies providing guidance for setting parameter values, and (3) work comparing mining results with the ground truth. We find that although interesting results exist, as a whole the body of work on these questions is too limited. In addition, we survey recent studies in the field of community detection, as a contrasting example. We argue that an objective obstacle for performing needed studies is a lack of data and survey the state of available data, pointing out certain limitations. As a solution, we propose to augment existing data by artificially generated data, review the state-of-the-art in data generation in unsupervised mining, and identify shortcomings. In more general terms, we call for the development of a true “Data Science” that—based on work in other domains, results in ML, and existing tools—develops needed data generators and builds up the knowledge needed to effectively employ unsupervised mining techniques. This article is categorized under:},
  archive      = {J_WIDM},
  author       = {Albrecht Zimmermann},
  doi          = {10.1002/widm.1330},
  journal      = {Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
  month        = {3},
  number       = {2},
  pages        = {e1330},
  shortjournal = {WIREs Data Mining Knowl. Discov.},
  title        = {Method evaluation, parameterization, and result validation in unsupervised data mining: A critical survey},
  volume       = {10},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). No free lunch theorem for concept drift detection in
streaming data classification: A review. <em>WIDM</em>, <em>10</em>(2),
e1327. (<a href="https://doi.org/10.1002/widm.1327">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many real-world data mining applications have to deal with unlabeled streaming data. They are unlabeled because the sheer volume of the stream makes it impractical to label a significant portion of the data. The data streams can evolve over time and these changes are called concept drifts. Concept drifts have different characteristics, which can be used to categorize them into different types. A trade-off between performance and cost exists among many concept drift detection approaches. On the one hand, high accuracy detection approach usually requires labeled data, possibly involving high cost for labeling. On the other hand, a variety of methods have been devoted to the topic of concept drift detection with unlabeled data, but these approaches often are most suited for only a subset of the concept drift types. The objective of this survey is to present these methods, categorize them and give recommendations of usage based on their behaviors under different types of concept drift. This article is categorized under:},
  archive      = {J_WIDM},
  author       = {Hanqing Hu and Mehmed Kantardzic and Tegjyot S. Sethi},
  doi          = {10.1002/widm.1327},
  journal      = {Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
  month        = {3},
  number       = {2},
  pages        = {e1327},
  shortjournal = {WIREs Data Mining Knowl. Discov.},
  title        = {No free lunch theorem for concept drift detection in streaming data classification: A review},
  volume       = {10},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Hierarchical third-order tensor decomposition through
inverse difference pyramid based on the three-dimensional walsh–hadamard
transform with applications in data mining. <em>WIDM</em>,
<em>10</em>(2), e1314. (<a
href="https://doi.org/10.1002/widm.1314">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A new approach is presented for hierarchical decomposition of third-order tensors through their transformation into the generalized three-dimensional (3D) spectrum space based on the inverse difference pyramid (IDP). For this, we choose the 3D Walsh–Hadamard transform (3D-WHT). As result, each tensor is represented as a spectral tensor of m hierarchical levels which contains selected low-frequency 3D-WHT coefficients. Calculating sequentially the inverse 3D-WHT for the coefficients from each pyramid level starting from its top, the tensor is approximated with increasing accuracy until its full restoration is achieved. To illustrate the new approach, given is the algorithm for hierarchical three-level tensor decomposition based on the reduced IDP. The proposed approach permits simultaneous decorrelation of tensor elements in three mutually orthogonal directions. The energy of the tensor elements is concentrated in a small number of spectral coefficients which build the top of the inverse pyramid. The use of the 3D-WHT permits to achieve minimum computational complexity, compared to deterministic 3D orthogonal transforms. The main applications of the new method for data mining in the contemporary intelligent systems are in the processing and analysis of large sets of different kinds of data/images/videos in the following areas: Compression of correlated image sequences, computer tomography, thermo vision, ultrasound and multichannel medical signals; search of 3D objects in image databases; extraction of features for recognition of 3D objects; multidimensional data denoising; multilayer watermarking of video sequences; and so on. This article is categorized under:},
  archive      = {J_WIDM},
  author       = {Roumen K. Kountchev and Barna L. Iantovics and Roumiana A. Kountcheva},
  doi          = {10.1002/widm.1314},
  journal      = {Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
  month        = {3},
  number       = {2},
  pages        = {e1314},
  shortjournal = {WIREs Data Mining Knowl. Discov.},
  title        = {Hierarchical third-order tensor decomposition through inverse difference pyramid based on the three-dimensional Walsh–Hadamard transform with applications in data mining},
  volume       = {10},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). The benefits and drawbacks of data mining technologies.
<em>WIDM</em>, <em>10</em>(1), e1344. (<a
href="https://doi.org/10.1002/widm.1344">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_WIDM},
  author       = {Witold Pedrycz},
  doi          = {10.1002/widm.1344},
  journal      = {Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
  month        = {1},
  number       = {1},
  pages        = {e1344},
  shortjournal = {WIREs Data Mining Knowl. Discov.},
  title        = {The benefits and drawbacks of data mining technologies},
  volume       = {10},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). On the dynamics of user engagement in news comment media.
<em>WIDM</em>, <em>10</em>(1), e1342. (<a
href="https://doi.org/10.1002/widm.1342">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many news outlets allow users to contribute comments on topics about daily world events. News articles are the seeds that spring users&#39; interest to contribute content, that is, comments. A news outlet may allow users to contribute comments on all their articles or a selected number of them. The topic of an article may lead to an apathetic user commenting activity (several tens of comments) or to a spontaneous fervent one (several thousands of comments). This environment creates a social dynamic that is little studied. The social dynamics around articles have the potential to reveal interesting facets of the user population at a news outlet. In this paper, we report the salient findings about these social media from 15 months worth of data collected from 17 news outlets comprising of over 38,000 news articles and about 21 million user comments. Analysis of the data reveals interesting insights such as there is an uneven relationship between news outlets and their user populations across outlets. Such observations and others have not been revealed, to our knowledge. We believe our analysis in this paper can contribute to news predictive analytics (e.g., user reaction to a news article or predicting the volume of comments posted to an article). This article is categorized under:},
  archive      = {J_WIDM},
  author       = {Lihong He and Chao Han and Arjun Mukherjee and Zoran Obradovic and Eduard Dragut},
  doi          = {10.1002/widm.1342},
  journal      = {Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
  month        = {1},
  number       = {1},
  pages        = {e1342},
  shortjournal = {WIREs Data Mining Knowl. Discov.},
  title        = {On the dynamics of user engagement in news comment media},
  volume       = {10},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Blockchain for explainable and trustworthy artificial
intelligence. <em>WIDM</em>, <em>10</em>(1), e1340. (<a
href="https://doi.org/10.1002/widm.1340">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The increasing computational power and proliferation of big data are now empowering Artificial Intelligence (AI) to achieve massive adoption and applicability in many fields. The lack of explanation when it comes to the decisions made by today&#39;s AI algorithms is a major drawback in critical decision-making systems. For example, deep learning does not offer control or reasoning over its internal processes or outputs. More importantly, current black-box AI implementations are subject to bias and adversarial attacks that may poison the learning or the inference processes. Explainable AI (XAI) is a new trend of AI algorithms that provide explanations of their AI decisions. In this paper, we propose a framework for achieving a more trustworthy and XAI by leveraging features of blockchain, smart contracts, trusted oracles, and decentralized storage. We specify a framework for complex AI systems in which the decision outcomes are reached based on decentralized consensuses of multiple AI and XAI predictors. The paper discusses how our proposed framework can be utilized in key application areas with practical use cases. This article is categorized under:},
  archive      = {J_WIDM},
  author       = {Mohamed Nassar and Khaled Salah and Muhammad Habib ur Rehman and Davor Svetinovic},
  doi          = {10.1002/widm.1340},
  journal      = {Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
  month        = {1},
  number       = {1},
  pages        = {e1340},
  shortjournal = {WIREs Data Mining Knowl. Discov.},
  title        = {Blockchain for explainable and trustworthy artificial intelligence},
  volume       = {10},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Churn prediction in turkey’s telecommunications sector: A
proposed multiobjective–cost-sensitive ant colony optimization.
<em>WIDM</em>, <em>10</em>(1), e1338. (<a
href="https://doi.org/10.1002/widm.1338">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Players in the telecommunications sector struggle against the competition to keep customers, and therefore they need effective churn management. Most classification algorithms either ignore misclassification cost or assume that the costs of all incorrect classification errors are equal. But as in real life, many classification problems have different misclassification costs and this difference cannot be ignored. For this reason, studies on cost-sensitive classification approaches have gained importance in recent years. The characteristics of telecommunications datasets such as high dimensionality and imbalance are making it difficult to achieve the desired performance for churn prediction. By taking this into consideration, we propose a multiobjective–cost-sensitive ant colony optimization (MOC-ACO-Miner) approach which integrates the cost-based nondominated sorted genetic algorithm feature selection and multiobjective ACO based cost-sensitive learning. MOC-ACO-Miner is applied to one of Turkey&#39;s top 100 information technology companies for customer churn-prediction. Finally, experiments find out that the model performs quite well with the area under receiver operating characteristic curve values of 0.9998 for predicting churners and therefore it can be beneficial for the highly competitive telecommunications sector. This article is categorized under:},
  archive      = {J_WIDM},
  author       = {Mihrimah Özmen and Emel K. Aydoğan and Yılmaz Delice and M. Duran Toksarı},
  doi          = {10.1002/widm.1338},
  journal      = {Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
  month        = {1},
  number       = {1},
  pages        = {e1338},
  shortjournal = {WIREs Data Mining Knowl. Discov.},
  title        = {Churn prediction in turkey&#39;s telecommunications sector: A proposed multiobjective–cost-sensitive ant colony optimization},
  volume       = {10},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Cable joint fault detection for the ring main unit based on
an adaptive TNPE algorithm. <em>WIDM</em>, <em>10</em>(1), e1336. (<a
href="https://doi.org/10.1002/widm.1336">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The ring main unit (RMU) is the main equipment of the electrical power distribution network. The deterioration process of the RMU cable joint is multivariable and nonstationary over time. Existing fault detection models for the RMU cable joint are single-variable and static over time. A novel algorithm of adaptive time neighborhood preserving embedding (ATNPE) is proposed in this paper, a multivariable time series statistical algorithm with online learning ability to improve the robustness and generalization ability of cable joint fault detection. This method combines an approximate linear dependence (ALD) condition with the time neighborhood preserving embedding (TNPE). New independent samples were determined by calculating the ALD value between new samples and modeling samples. In addition, the new independent samples were used to update the TNPE model. The offline training module was used to update RMU fault detection based on the TNPE model by training new samples with historical offline data. The online updating module trained the historical model to ensure the correct rate of online detection. The experimental results indicated that the method has the ability to adapt to new samples and the squared prediction error statistic limit responded to changes in statistical characteristics of monitoring data. The method can effectively reduce the false alarm rate and has the same fault detection rate compared to the existing model. This article is categorized under:},
  archive      = {J_WIDM},
  author       = {Lei Tan and Peng Li and Fei Tao and Aimin Miao and Min Cao},
  doi          = {10.1002/widm.1336},
  journal      = {Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
  month        = {1},
  number       = {1},
  pages        = {e1336},
  shortjournal = {WIREs Data Mining Knowl. Discov.},
  title        = {Cable joint fault detection for the ring main unit based on an adaptive TNPE algorithm},
  volume       = {10},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Core-sets: An updated survey. <em>WIDM</em>, <em>10</em>(1),
e1335. (<a href="https://doi.org/10.1002/widm.1335">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In optimization or machine learning problems we are given a set of items, usually points in some metric space, and the goal is to minimize or maximize an objective function over some space of candidate solutions. For example, in clustering problems, the input is a set of points in some metric space, and a common goal is to compute a set of centers in some other space (points, lines) that will minimize the sum of distances to these points. In database queries, we may need to compute such a some for a specific query set of k centers. However, traditional algorithms cannot handle modern systems that require parallel real-time computations of infinite distributed streams from sensors such as GPS, audio or video that arrive to a cloud, or networks of weaker devices such as smartphones or robots. Core-set is a “small data” summarization of the input “big data,” where every possible query has approximately the same answer on both data sets. Generic techniques enable efficient coreset maintenance of streaming, distributed and dynamic data. Traditional algorithms can then be applied on these coresets to maintain the approximated optimal solutions. The challenge is to design coresets with provable tradeoff between their size and approximation error. This survey summarizes such constructions in a retrospective way, that aims to unified and simplify the state-of-the-art. This article is categorized under},
  archive      = {J_WIDM},
  author       = {Dan Feldman},
  doi          = {10.1002/widm.1335},
  journal      = {Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
  month        = {1},
  number       = {1},
  pages        = {e1335},
  shortjournal = {WIREs Data Mining Knowl. Discov.},
  title        = {Core-sets: An updated survey},
  volume       = {10},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). An introduction to algorithmic differentiation.
<em>WIDM</em>, <em>10</em>(1), e1334. (<a
href="https://doi.org/10.1002/widm.1334">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Algorithmic differentiation (AD), also known as automatic differentiation, is a technology for accurate and efficient evaluation of derivatives of a function given as a computer model. The evaluations of such models are essential building blocks in numerous scientific computing and data analysis applications, including optimization, parameter identification, sensitivity analysis, uncertainty quantification, nonlinear equation solving, and integration of differential equations. We provide an introduction to AD and present its basic ideas and techniques, some of its most important results, the implementation paradigms it relies on, the connection it has to other domains including machine learning and parallel computing, and a few of the major open problems in the area. Topics we discuss include: forward mode and reverse mode of AD, higher-order derivatives, operator overloading and source transformation, sparsity exploitation, checkpointing, cross-country mode, and differentiating iterative processes. This article is categorized under:},
  archive      = {J_WIDM},
  author       = {Assefaw H. Gebremedhin and Andrea Walther},
  doi          = {10.1002/widm.1334},
  journal      = {Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
  month        = {1},
  number       = {1},
  pages        = {e1334},
  shortjournal = {WIREs Data Mining Knowl. Discov.},
  title        = {An introduction to algorithmic differentiation},
  volume       = {10},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Sentiment analysis for mining texts and social networks
data: Methods and tools. <em>WIDM</em>, <em>10</em>(1), e1333. (<a
href="https://doi.org/10.1002/widm.1333">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Social networks (SNs) represent an established environment in which users share daily emotions and opinions. Therefore, they have become an essential source of big data related to sentiment/opinion sphere. Sentiment analysis (SA) aims to extract sentiments, emotions or opinions from texts, made available by different data sources like SNs. This review presents a depth study relative to the methods and the main tools for SA. The analysis was performed by defining four criteria and several variables to compare 24 tools with objective criteria. Specifically, the tools have been analyzed and tested to verify their usability, flexibility of use, and other specifications related to the type of analysis performed. The majority of tools can detect positive, negative, and neutral polarity, while few tools only detect positive and negative polarity. Moreover, seven tools were able to recognize emotions, and only one provides a visual map for geo-referenced data. Except for one, remaining 23 tools offer service through the web interface. Finally, only nine tools provide both application program interfaces and a client for common programming languages to allow potential developer end-users to integrate a specific SA tool into their application. Differently, from other recent surveys, the paper presents and discusses both methods and tools for analyzing texts and SN data sources to extract sentiment. Moreover, it contains a comprehensive comparison with other recent surveys. The comparative analysis of the tools completed according to objective criteria allows to highlight some limits on main tools that need to be faced with enhancing the end-user experience. This article is categorized under:},
  archive      = {J_WIDM},
  author       = {Chiara Zucco and Barbara Calabrese and Giuseppe Agapito and Pietro H. Guzzi and Mario Cannataro},
  doi          = {10.1002/widm.1333},
  journal      = {Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
  month        = {1},
  number       = {1},
  pages        = {e1333},
  shortjournal = {WIREs Data Mining Knowl. Discov.},
  title        = {Sentiment analysis for mining texts and social networks data: Methods and tools},
  volume       = {10},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A review on sentiment discovery and analysis of educational
big-data. <em>WIDM</em>, <em>10</em>(1), e1328. (<a
href="https://doi.org/10.1002/widm.1328">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sentiment discovery and analysis (SDA) aims to automatically identify the underlying attitudes, sentiments, and subjectivity towards a certain entity such as learners and learning resources. Due to its enormous potential for smart education, SDA has been deemed as a powerful technique for identifying and classifying sentiments from multimodal and multisource data over the whole process of education. For big educational data streams, SDA faces challenges in unimodal feature selection, sentiment classification, and multimodal fusion. As such, a large body of studies in the literature explores diverse approaches to SDA for educational applications. This paper provides a self-contained, uniform overview of the SDA techniques for education. In particular, we focus on prominent studies in unimodal sentiment features and classifications (e.g., text, audio, and visual). In addition, we present a novel SDA framework of multimodal fusions, together with description of their crucial components. Based on this framework, we review different approaches to SDA on education from the perspectives of approaches and applications. After comprehensively reviewing the SDA techniques on education, we present the trends and prospectives of the future SDA research under ubiquitous education contexts. This article is categorized under:},
  archive      = {J_WIDM},
  author       = {Zhongmei Han and Jiyi Wu and Changqin Huang and Qionghao Huang and Meihua Zhao},
  doi          = {10.1002/widm.1328},
  journal      = {Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
  month        = {1},
  number       = {1},
  pages        = {e1328},
  shortjournal = {WIREs Data Mining Knowl. Discov.},
  title        = {A review on sentiment discovery and analysis of educational big-data},
  volume       = {10},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Data mining in foreign language learning. <em>WIDM</em>,
<em>10</em>(1), e1287. (<a
href="https://doi.org/10.1002/widm.1287">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Educational data mining (EDM) combines the techniques of data mining with educational data in order to provide students, instructors, and researchers with knowledge that can benefit academic processes. Due to globalization, foreign language learning (FLL) has become increasingly important. This work seeks to gain insight as to how data mining (DM) is being used to benefit FLL. For this purpose, an advanced review of pertinent research published from 2012 to 2017 was performed. After applying our screening method, 208 papers were selected for the exhaustive analysis. This analysis was divided into four aspects: context (educational environments, educational level), number of items, DM methods, and DM applications. The results indicated that 54% of studies were conducted in traditional environments, while only 3% of studies were performed in an m-learning environment. In addition, 25 and 72% of the research was conducted in either a primary or secondary level, or in tertiary or adult level, respectively. Likewise, 76% of studies contained datasets of less than 1,000 items. The most utilized EDM methods were: factor analysis, regression, text mining, correlation mining, and causal DM. In addition, the studies analyzed showed that DM is mainly employed to predict the performance of students, to check learners&#39; motivation, and to provide feedback for instructors. These results seem to indicate that although DM has much to offer the increasing number of language students, it is not being used to its full potential. This article is categorized under:},
  archive      = {J_WIDM},
  author       = {Javier Bravo-Agapito and Claire Frances Bonilla and Isaac Seoane},
  doi          = {10.1002/widm.1287},
  journal      = {Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
  month        = {1},
  number       = {1},
  pages        = {e1287},
  shortjournal = {WIREs Data Mining Knowl. Discov.},
  title        = {Data mining in foreign language learning},
  volume       = {10},
  year         = {2020},
}
</textarea>
</details></li>
</ul>

</body>
</html>
