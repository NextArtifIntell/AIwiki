<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>OR_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="or---104">OR - 104</h2>
<ul>
<li><details>
<summary>
(2020). Simple bayesian algorithms for best-arm identification.
<em>OR</em>, <em>68</em>(6), ii–iv. (<a
href="https://doi.org/10.1287/opre.2019.1911">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper considers the optimal adaptive allocation of measurement effort for identifying the best among a finite set of options or designs. An experimenter sequentially chooses designs to measure and observes noisy signals of their quality with the goal of confidently identifying the best design after a small number of measurements. This paper proposes three simple and intuitive Bayesian algorithms for adaptively allocating measurement effort and formalizes a sense in which these seemingly naive rules are the best possible. One proposal is top-two probability sampling, which computes the two designs with the highest posterior probability of being optimal and then randomizes to select among these two. One is a variant of top-two sampling that considers not only the probability that a design is optimal, but the expected amount by which its quality exceeds that of other designs. The final algorithm is a modified version of Thompson sampling that is tailored for identifying the best design. We prove that these simple algorithms satisfy a sharp optimality property. In a frequentist setting where the true quality of the designs is fixed, one hopes that the posterior definitively identifies the optimal design, in the sense that that the posterior probability assigned to the event that some other design is optimal converges to zero as measurements are collected. We show that under the proposed algorithms, this convergence occurs at an exponential rate, and the corresponding exponent is the best possible among all allocation rules. It should be highlighted that the proposed algorithms depend on a single tuning parameter, which determines the probability used when randomizing among the top-two designs. Attaining the optimal rate of posterior convergence requires either that this parameter is set optimally or is tuned adaptively toward the optimal value. The paper goes further, characterizing the exponent attained on any problem instance and for any value of the tunable parameter. This exponent is interpreted as being optimal among a constrained class of allocation rules. Finally, considerable robustness to this parameter is established through numerical experiments and theoretical results. When this parameter is set to 1/2, the exponent attained is within a factor of 2 of best possible across all problem instances.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2019.1911},
  journal      = {Operations Research},
  number       = {6},
  pages        = {ii-iv},
  shortjournal = {Oper. Res.},
  title        = {Simple bayesian algorithms for best-arm identification},
  volume       = {68},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Reducing delay in retrial queues by simultaneously
differentiating service and retrial rates. <em>OR</em>, <em>68</em>(6),
ii–iv. (<a href="https://doi.org/10.1287/opre.2019.1933">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we introduce a service grade differentiation policy for queueing models with customer retrials. We show that the average waiting time can be reduced through strategically allocating the rates of service and retrial times without needing additional service capacity. Countering to the intuition that higher service variability usually yields a larger delay, we show that the benefits of our simultaneous service-and-retrial differentiation policy outweigh the impact of the increased service variability. We present a necessary and sufficient condition under which the proposed policy reduces the waiting time and a closed-form expression for the optimal allocation policy. In heavy traffic, our policy can asymptotically reduce both the delay and the number of customer retrials before entering service by a significant factor, which is a function of the ratio of the service rate to the retrial rate.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2019.1933},
  journal      = {Operations Research},
  number       = {6},
  pages        = {ii-iv},
  shortjournal = {Oper. Res.},
  title        = {Reducing delay in retrial queues by simultaneously differentiating service and retrial rates},
  volume       = {68},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Technical note—on the optimality of reflection control.
<em>OR</em>, <em>68</em>(6), ii–iv. (<a
href="https://doi.org/10.1287/opre.2019.1935">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The goal of this paper is to illustrate the optimality of reflection control in three different settings, to bring out their connections and to contrast their distinctions. First, we study the control of a Brownian motion with a negative drift, so as to minimize a long-run average cost objective. We prove the optimality of the reflection control, which prevents the Brownian motion from dropping below a certain level by cancelling out from time to time part of the negative drift; and we show that the optimal reflection level can be derived as the fixed point that equates the long-run average cost to the holding cost. Second, we establish the asymptotic optimality of the reflection control when it is applied to a discrete production-inventory system driven by (delayed) renewal processes; and we do so via identifying the limiting regime of the system under diffusion scaling. Third, in the case of controlling a birth–death model, we establish the optimality of the reflection control directly via a linear programming–based approach. In all three cases, we allow an exponentially bounded holding cost function, which appears to be more general than what’s allowed in prior studies. This general cost function reveals some previously unknown technical fine points on the optimality of the reflection control, and extends significantly its domain of applications.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2019.1935},
  journal      = {Operations Research},
  number       = {6},
  pages        = {ii-iv},
  shortjournal = {Oper. Res.},
  title        = {Technical Note—On the optimality of reflection control},
  volume       = {68},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Optimistic monte carlo tree search with sampled information
relaxation dual bounds. <em>OR</em>, <em>68</em>(6), ii–iv. (<a
href="https://doi.org/10.1287/opre.2019.1939">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Monte Carlo tree search (MCTS), most famously used in game-play artificial intelligence (e.g., the game of Go), is a well-known strategy for constructing approximate solutions to sequential decision problems. Its primary innovation is the use of a heuristic, known as a default policy , to obtain Monte Carlo estimates of downstream values for states in a decision tree. This information is used to iteratively expand the tree toward regions of states and actions that an optimal policy might visit. However, to guarantee convergence to the optimal action, MCTS requires the entire tree to be expanded asymptotically. In this paper, we propose a new “optimistic” tree search technique called primal-dual MCTS that uses sampled information relaxation upper bounds on potential actions to make tree expansion decisions, creating the possibility of ignoring parts of the tree that stem from highly suboptimal choices. The core contribution of this paper is to prove that despite converging to a partial decision tree in the limit, the recommended action from primal-dual MCTS is optimal. The new approach shows promise when used to optimize the behavior of a single driver navigating a graph while operating on a ride-sharing platform. Numerical experiments on a real data set of taxi trips in New Jersey suggest that primal-dual MCTS improves on standard MCTS (upper confidence trees) and other policies while exhibiting a reduced sensitivity to the size of the action space.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2019.1939},
  journal      = {Operations Research},
  number       = {6},
  pages        = {ii-iv},
  shortjournal = {Oper. Res.},
  title        = {Optimistic monte carlo tree search with sampled information relaxation dual bounds},
  volume       = {68},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Information and memory in dynamic resource allocation.
<em>OR</em>, <em>68</em>(6), ii–iv. (<a
href="https://doi.org/10.1287/opre.2019.1940">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a general framework, dubbed stochastic processing under imperfect information , to study the impact of information constraints and memories on dynamic resource allocation. The framework involves a stochastic processing network (SPN) scheduling problem in which the scheduler may access the system state only through a noisy channel, and resource allocation decisions must be carried out through the interaction between an encoding policy (that observes the state) and allocation policy (that chooses the allocation). Applications in the management of large-scale data centers and human-in-the-loop service systems are among our chief motivations. We quantify the degree to which information constraints reduce the size of the capacity region in general SPNs and how such reduction depends on the amount of memories available to the encoding and allocation policies. Using a novel metric, capacity factor , our main theorem characterizes the reduction in capacity region (under “optimal” policies) for all nondegenerate channels and across almost all combinations of memory sizes. Notably, the theorem demonstrates, in substantial generality, that (1) the presence of a noisy channel always reduces capacity, (2) more memory for the allocation policy always improves capacity, and (3) more memory for the encoding policy has little to no effect on capacity. Finally, all of our positive (achievability) results are established through constructive, implementable policies. Our proof program involves the development of a host of new techniques, largely from first principles, by combining ideas from information theory, learning and queuing theory. As a submodule of one of the policies proposed, we create a simple yet powerful generalization of the maximum-weight (max-weight) policy, in which individual Markov chains are selected dynamically, in a manner analogous to how schedules are used in a conventional max-weight policy.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2019.1940},
  journal      = {Operations Research},
  number       = {6},
  pages        = {ii-iv},
  shortjournal = {Oper. Res.},
  title        = {Information and memory in dynamic resource allocation},
  volume       = {68},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Technical note—there’s no free lunch: On the hardness of
choosing a correct big-m in bilevel optimization. <em>OR</em>,
<em>68</em>(6), ii–iv. (<a
href="https://doi.org/10.1287/opre.2019.1944">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {One of the most frequently used approaches to solve linear bilevel optimization problems consists in replacing the lower-level problem with its Karush–Kuhn–Tucker (KKT) conditions and by reformulating the KKT complementarity conditions using techniques from mixed-integer linear optimization. The latter step requires to determine some big- M constant in order to bound the lower level’s dual feasible set such that no bilevel-optimal solution is cut off. In practice, heuristics are often used to find a big- M although it is known that these approaches may fail. In this paper, we consider the hardness of two proxies for the above mentioned concept of a bilevel-correct big- M . First, we prove that verifying that a given big- M does not cut off any feasible vertex of the lower level’s dual polyhedron cannot be done in polynomial time unless P = NP. Second, we show that verifying that a given big- M does not cut off any optimal point of the lower level’s dual problem (for any point in the projection of the high-point relaxation onto the leader’s decision space) is as hard as solving the original bilevel problem.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2019.1944},
  journal      = {Operations Research},
  number       = {6},
  pages        = {ii-iv},
  shortjournal = {Oper. Res.},
  title        = {Technical Note—There’s no free lunch: On the hardness of choosing a correct big-M in bilevel optimization},
  volume       = {68},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Diffusion in random networks: Impact of degree distribution.
<em>OR</em>, <em>68</em>(6), ii–iv. (<a
href="https://doi.org/10.1287/opre.2019.1945">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Motivated by viral marketing on social networks, we study the diffusion process of a new product on a network where each agent is connected to a random subset of others. The number of contacts (i.e., degree) varies across agents, and the firm knows the degree of each agent. Further, the firm can seed a fraction of the population and incurs a fixed cost per contact. Under any bounded degree distribution and for any target adoption proportion, we compute both the cost and the time it takes to reach the target in the limit of network size. Our characterization of the diffusion process in such a general setting is the first of its kind for a problem that is generally deemed intractable and solved using approximation methods such as mean-field. Our solution indicates that the degree distribution impacts the diffusion process even beyond its first and second moments. Using our limit results, we conduct comparative statics on degree distribution and uncover a trade-off between cost-efficiency and fast growth. Fixing the average degree, a minimum-variance degree distribution incurs the minimum cost to reach any adoption proportion. On the other hand, higher variance results in faster growth for low or moderately high target adoption proportions, but it incurs higher cost. This trade-off arises partly because of an endogenous effect of diffusion on the distribution of adopters’ neighbors: as the diffusion progresses, adopters become more likely to be connected to other adopters. This highlights the benefit of our exact analysis compared to mean-field approximation methods that rely on perfect mixing of adopters and non-adopters (i.e., there is no change in the distribution of adopters’ neighbors with the progress of diffusion). Further, we study the impact of the degree distribution on optimal seeding strategies for a given seeding budget. Somewhat surprisingly, we show that to minimize cost, it is optimal to seed low-degree agents. Even if the objective is to minimize time, for certain regimes, the optimal seeding strategy is a mixture of low- and high-degree agents.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2019.1945},
  journal      = {Operations Research},
  number       = {6},
  pages        = {ii-iv},
  shortjournal = {Oper. Res.},
  title        = {Diffusion in random networks: Impact of degree distribution},
  volume       = {68},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Asynchronous schemes for stochastic and misspecified
potential games and nonconvex optimization. <em>OR</em>, <em>68</em>(6),
ii–iv. (<a href="https://doi.org/10.1287/opre.2019.1946">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The distributed computation of equilibria and optima has seen growing interest in a broad collection of networked problems. We consider the computation of Nash equilibria of convex stochastic noncooperative games characterized by a possibly nonconvex potential function. Since any stationary point of the potential function is a Nash equilibrium, there is an equivalence between asynchronous best-response (BR) schemes applied on a noncooperative game and block-coordinate descent (BCD) schemes implemented on the associated potential function. We focus on two classes of such games: (Problem 1): a potential game, in which each player solves a parameterized stochastic convex program, and (Problem 2): a misspecified generalization, in which the player-specific stochastic program is complicated by a parametric misspecification with the unknown parameter being the solution to a stochastic convex optimization problem. In both settings, exact proximal BR solutions are generally unavailable in finite time because they necessitate solving stochastic programs. Consequently, we design two asynchronous inexact proximal BR schemes to solve Problems 1 and 2, respectively, in which in each iteration, a single player is randomly chosen to compute an inexact proximal BR solution (via stochastic approximation) with delayed rival information while the other players keep their strategies invariant. In the misspecified regime (Problem 2), each player possesses an extra estimate of the misspecified parameter by using a projected stochastic gradient algorithm with an increasing batch of sampled gradients. By imposing suitable conditions on the inexactness sequences, we prove that the iterates produced by both schemes converge almost surely to a connected subset of the set of Nash equilibria. When the player-specific problems are strongly convex, an inexact pure BR scheme (without a proximal term) is shown to be convergent. In effect, we provide what we believe is among the first randomized BCD schemes for stochastic nonconvex (but block-wise convex) optimization with almost sure convergence properties. We further show that the associated gap function converges to zero in mean. These statements can be extended to allow for accommodating weighted potential games and generalized potential games. Finally, we present preliminary numerics by applying the proposed schemes to congestion control and Nash–Cournot competition.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2019.1946},
  journal      = {Operations Research},
  number       = {6},
  pages        = {ii-iv},
  shortjournal = {Oper. Res.},
  title        = {Asynchronous schemes for stochastic and misspecified potential games and nonconvex optimization},
  volume       = {68},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Network-based approximate linear programming for discrete
optimization. <em>OR</em>, <em>68</em>(6), ii–iv. (<a
href="https://doi.org/10.1287/opre.2019.1953">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present relaxations for discrete optimization problems using approximate linear programs (ALPs) defined on multiple networks that represent different state-space aggregations. Our network ALP leverages information across these networks using a piecewise-constant value function approximation, and its optimistic bound is theoretically guaranteed to weakly improve upon the bounds from the individual networks used in its construction. Solving network ALPs is challenging because of its large number of constraints—a well-known issue when employing approximate linear programming. We side-step this issue by using a clique-based graph representation to design a network ALP restriction that admits a polynomial-time solvable extended formulation, which we show to also deliver a weakly better bound than individual networks. We execute a computational study on a challenging bilinear problem arising in marketing analytics and a routing application encountered in the preemptive maintenance of energy or city-owned assets. When used within a branch-and-bound scheme, the network ALP restriction significantly outperforms in both solution quality and time the following benchmarks: a state-of-the-art mathematical programming solver, a generic aggregation/disaggregation scheme applied to a single network, and a heuristic that chooses the best bound among individual networks.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2019.1953},
  journal      = {Operations Research},
  number       = {6},
  pages        = {ii-iv},
  shortjournal = {Oper. Res.},
  title        = {Network-based approximate linear programming for discrete optimization},
  volume       = {68},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Algorithms for online matching, assortment, and pricing with
tight weight-dependent competitive ratios. <em>OR</em>, <em>68</em>(6),
ii–iv. (<a href="https://doi.org/10.1287/opre.2019.1957">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Motivated by the dynamic assortment offerings and item pricings occurring in e-commerce, we study a general problem of allocating finite inventories to heterogeneous customers arriving sequentially. We analyze this problem under the framework of competitive analysis, where the sequence of customers is unknown and does not necessarily follow any pattern. Previous work in this area, studying online matching, advertising, and assortment problems, has focused on the case where each item can only be sold at a single price, resulting in algorithms which achieve the best-possible competitive ratio of 1 − 1/ e . In this paper, we extend all of these results to allow for items having multiple feasible prices. Our algorithms achieve the best-possible weight-dependent competitive ratios, which depend on the sets of feasible prices given in advance. Our algorithms are also simple and intuitive; they are based on constructing a class of universal value functions that integrate the selection of items and prices offered. Finally, we test our algorithms on the publicly available hotel data set of Bodea et al. [Bodea T, Ferguson M, Garrow L (2009) Data set—Choice-based revenue management: Data from a major hotel chain. Manufacturing Service Oper. Management 11(2):356–361.], where there are multiple items (hotel rooms), each with multiple prices (fares at which the room could be sold). We find that applying our algorithms, as a hybrid with algorithms that attempt to forecast and learn the future transactions, results in the best performance.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2019.1957},
  journal      = {Operations Research},
  number       = {6},
  pages        = {ii-iv},
  shortjournal = {Oper. Res.},
  title        = {Algorithms for online matching, assortment, and pricing with tight weight-dependent competitive ratios},
  volume       = {68},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Operational risk management: A stochastic control framework
with preventive and corrective controls. <em>OR</em>, <em>68</em>(6),
ii–iv. (<a href="https://doi.org/10.1287/opre.2019.1960">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose a general modeling framework for operational risk management of financial firms. We consider operational risk events as shocks to a financial firm’s value process and then study capital investments under preventive and corrective controls to mitigate risk losses. The optimal decisions are made in three scenarios: (i) preventive control only, (ii) corrective control only, and (iii) joint controls. We characterize the optimal control policies within a general modeling framework that comprises these three scenarios and then discuss an exponential risk reduction function. We conclude our work with an application of our model to a data set from a commercial bank. We find that, through a proper investment strategy, we can achieve a significant performance improvement, especially when the risk severity level is high. Moreover, with controls, the value of the firm tends to increase relative to the value of the firm without controls. Hence, the controls are essentially smoothing out the jump losses and increasing the value of the firm. At the bank we analyze we find that with a joint control strategy the bank can achieve profit increases from 7.45\% to 11.62\% when the risk reduction efficiencies of the two controls are high. In general, our modeling framework, which combines a typical operational risk process with stochastic control, may suggest a new research direction in operations management and operational risk management.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2019.1960},
  journal      = {Operations Research},
  number       = {6},
  pages        = {ii-iv},
  shortjournal = {Oper. Res.},
  title        = {Operational risk management: A stochastic control framework with preventive and corrective controls},
  volume       = {68},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Efficient solution of maximum-entropy sampling problems.
<em>OR</em>, <em>68</em>(6), ii–iv. (<a
href="https://doi.org/10.1287/opre.2019.1962">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider a new approach for the maximum-entropy sampling problem (MESP) that is based on bounds obtained by maximizing a function of the form ldet M ( x ) over linear constraints, where M ( x ) is linear in the n -vector x . These bounds can be computed very efficiently and are superior to all previously known bounds for MESP on most benchmark test problems. A branch-and-bound algorithm using the new bounds solves challenging instances of MESP to optimality for the first time.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2019.1962},
  journal      = {Operations Research},
  number       = {6},
  pages        = {ii-iv},
  shortjournal = {Oper. Res.},
  title        = {Efficient solution of maximum-entropy sampling problems},
  volume       = {68},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Input efficiency measures: A generalized, encompassing
formulation. <em>OR</em>, <em>68</em>(6), ii–iv. (<a
href="https://doi.org/10.1287/opre.2019.1963">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This contribution defines a new generalized input efficiency measure which encompasses and thus links four well-known input efficiency measures: the Debreu-Farrell measure, the Färe-Lovell measure, the asymmetric Färe measure, and the multiplicative Färe-Lovell measure. The axiomatic properties of this new measure are studied. The generalized input efficiency measure naturally leads to the definition of new measures as special cases. It also provides a general framework for testing the choice of efficiency measures. Examples of mathematical programming models in specific cases are established to illustrate this new measure.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2019.1963},
  journal      = {Operations Research},
  number       = {6},
  pages        = {ii-iv},
  shortjournal = {Oper. Res.},
  title        = {Input efficiency measures: A generalized, encompassing formulation},
  volume       = {68},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Parallel bayesian global optimization of expensive
functions. <em>OR</em>, <em>68</em>(6), ii–iv. (<a
href="https://doi.org/10.1287/opre.2019.1966">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider parallel global optimization of derivative-free expensive-to-evaluate functions, and propose an efficient method based on stochastic approximation for implementing a conceptual Bayesian optimization algorithm proposed by Ginsbourger in 2008. At the heart of this algorithm is maximizing the information criterion called the “multipoints expected improvement,” or the q - EI . To accomplish this, we use infinitesimal perturbation analysis (IPA) to construct a stochastic gradient estimator and show that this estimator is unbiased. We also show that the stochastic gradient ascent algorithm using the constructed gradient estimator converges to a stationary point of the q - EI surface, and therefore, as the number of multiple starts of the gradient ascent algorithm and the number of steps for each start grow large, the one-step Bayes-optimal set of points is recovered. We show in numerical experiments using up to 128 parallel evaluations that our method for maximizing the q - EI is faster than methods based on closed-form evaluation using high-dimensional integration, when considering many parallel function evaluations, and is comparable in speed when considering few. We also show that the resulting one-step Bayes-optimal algorithm for parallel global optimization finds high-quality solutions with fewer evaluations than a heuristic based on approximately maximizing the q - EI . A high-quality open source implementation of this algorithm is available in the open source Metrics Optimization Engine (MOE).},
  archive      = {J_OR},
  doi          = {10.1287/opre.2019.1966},
  journal      = {Operations Research},
  number       = {6},
  pages        = {ii-iv},
  shortjournal = {Oper. Res.},
  title        = {Parallel bayesian global optimization of expensive functions},
  volume       = {68},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Why is maximum clique often easy in practice? <em>OR</em>,
<em>68</em>(6), ii–iv. (<a
href="https://doi.org/10.1287/opre.2019.1970">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To this day, the maximum clique problem remains a computationally challenging problem. Indeed, despite researchers’ best efforts, there exist unsolved benchmark instances with 1,000 vertices. However, relatively simple algorithms solve real-life instances with millions of vertices in a few seconds. Why is this the case? Why is the problem apparently so easy in many naturally occurring networks? In this paper, we provide an explanation. First, we observe that the graph’s clique number ω is very near to the graph’s degeneracy d in most real-life instances. This observation motivates a main contribution of this paper, which is an algorithm for the maximum clique problem that runs in time polynomial in the size of the graph, but exponential in the gap g ≔ ( d + 1 ) − ω between the clique number ω and its degeneracy-based upper bound d +1. When this gap g can be treated as a constant, as is often the case for real-life graphs, the proposed algorithm runs in time O ( d m ) = O ( m 1.5 ) . This provides a rigorous explanation for the apparent easiness of these instances despite the intractability of the problem in the worst case. Further, our implementation of the proposed algorithm is actually practical—competitive with the best approaches from the literature.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2019.1970},
  journal      = {Operations Research},
  number       = {6},
  pages        = {ii-iv},
  shortjournal = {Oper. Res.},
  title        = {Why is maximum clique often easy in practice?},
  volume       = {68},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Maximum likelihood estimation by monte carlo simulation:
Toward data-driven stochastic modeling. <em>OR</em>, <em>68</em>(6),
ii–iv. (<a href="https://doi.org/10.1287/opre.2019.1978">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a gradient-based simulated maximum likelihood estimation to estimate unknown parameters in a stochastic model without assuming that the likelihood function of the observations is available in closed form. A key element is to develop Monte Carlo–based estimators for the density and its derivatives for the output process, using only knowledge about the dynamics of the model. We present the theory of these estimators and demonstrate how our approach can handle various types of model structures. We also support our findings and illustrate the merits of our approach with numerical results.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2019.1978},
  journal      = {Operations Research},
  number       = {6},
  pages        = {ii-iv},
  shortjournal = {Oper. Res.},
  title        = {Maximum likelihood estimation by monte carlo simulation: Toward data-driven stochastic modeling},
  volume       = {68},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). On the consistent path problem. <em>OR</em>, <em>68</em>(6),
ii–iv. (<a href="https://doi.org/10.1287/opre.2020.1979">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The application of decision diagrams in combinatorial optimization has proliferated in the last decade. In recent years, authors have begun to investigate how to use not one, but a set of diagrams, to model constraints and objective function terms. Optimizing over a collection of decision diagrams, the problem we refer to as the consistent path problem (CPP) can be addressed by associating a network-flow model with each decision diagram, jointly linked through channeling constraints. A direct application of integer programming to the ensuing model has already been shown to result in algorithms that provide orders-of-magnitude performance gains over classical methods. Lacking, however, is a careful study of dedicated solution methods designed to solve the CPP. This paper provides a detailed study of the CPP, including a discussion on complexity results and a complete polyhedral analysis. We propose a cut-generation algorithm, which, under a structured ordering property, finds a cut, if one exists, through an application of the classical maximum flow problem, albeit in an exponentially sized network. We use this procedure to fuel a cutting-plane algorithm that is applied to unconstrained binary cubic optimization and a variant of the market split problem, resulting in an algorithm that compares favorably with CPLEX, using standard integer programming formulations for both problems.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2020.1979},
  journal      = {Operations Research},
  number       = {6},
  pages        = {ii-iv},
  shortjournal = {Oper. Res.},
  title        = {On the consistent path problem},
  volume       = {68},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Optimal monitoring schedule in dynamic contracts.
<em>OR</em>, <em>68</em>(5), iii–vi. (<a
href="https://doi.org/10.1287/opre.2019.1968">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Consider a setting in which a principal induces effort from an agent to reduce the arrival rate of a Poisson process of adverse events. The effort is costly to the agent and unobservable to the principal unless the principal is monitoring the agent. Monitoring ensures effort but is costly to the principal. The optimal contract involves monetary payments and monitoring sessions that depend on past arrival times. We formulate the problem as a stochastic optimal control model and solve the problem analytically. The optimal schedules of payment and monitoring demonstrate different structures depending on model parameters. Overall, the optimal dynamic contracts are simple to describe, easy to compute and implement, and intuitive to explain.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2019.1968},
  journal      = {Operations Research},
  number       = {5},
  pages        = {iii-vi},
  shortjournal = {Oper. Res.},
  title        = {Optimal monitoring schedule in dynamic contracts},
  volume       = {68},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Ambulance emergency response optimization in developing
countries. <em>OR</em>, <em>68</em>(5), iii–vi. (<a
href="https://doi.org/10.1287/opre.2019.1969">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The lack of emergency medical transportation is viewed as the main barrier to the access and availability of emergency medical care in low- and middle-income countries (LMICs). In this paper, we present a robust optimization approach to optimize both the location and routing of emergency response vehicles, accounting for uncertainty in travel times and spatial demand characteristic of LMICs. We traveled to Dhaka, Bangladesh, the sixth largest and third most densely populated city in the world, to conduct field research resulting in the collection of two unique data sets that inform our approach. These data are leveraged to estimate demand for emergency medical services in an LMIC setting and to predict the travel time between any two locations in the road network for different times of day and days of the week. We combine our prediction-optimization framework with a simulation model and real data to provide an in-depth investigation into three policy-related questions. First, we demonstrate that outpost locations optimized for weekday rush hour lead to good performance for all times of day and days of the week. Second, we find that the performance of the current system could be replicated using one third of the current outpost locations and one half of the current number of ambulances. Finally, we show that a fleet of small ambulances has the potential to significantly outperform traditional ambulance vans. In particular, they are able to capture approximately three times more demand while reducing the median average response time by roughly 10\%–18\% over the entire week and 24\%–35\% during rush hour because of increased routing flexibility offered by more nimble vehicles on a larger road network. Our results provide practical insights for emergency response optimization that can be leveraged by hospital-based and private ambulance providers in Dhaka and other urban centers in developing countries.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2019.1969},
  journal      = {Operations Research},
  number       = {5},
  pages        = {iii-vi},
  shortjournal = {Oper. Res.},
  title        = {Ambulance emergency response optimization in developing countries},
  volume       = {68},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Dynamic inventory and price controls involving unknown
demand on discrete nonperishable items. <em>OR</em>, <em>68</em>(5),
iii–vi. (<a href="https://doi.org/10.1287/opre.2019.1974">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study adaptive policies that handle dynamic inventory and price controls when the random demand for discrete nonperishable items is unknown. Pure inventory control is achieved by targeting newsvendor ordering quantities that correspond to empirical demand distributions learned over time. On this basis we conduct the more complex joint inventory-price control, where demand-affecting prices await to be evaluated as well. We identify policies that strive to balance between exploration and exploitation, and measure their performances via regrets, that is, the prices to pay for not knowing demand distributions a priori over a given horizon. Multiple bounds are derived on regrets’ growth rates; they vary with how thoroughly unknown the demand distributions are and whether nonperishability has indeed been accounted for. Our simulation study illustrates order-of-magnitude differences between pure inventory and joint inventory-price controls.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2019.1974},
  journal      = {Operations Research},
  number       = {5},
  pages        = {iii-vi},
  shortjournal = {Oper. Res.},
  title        = {Dynamic inventory and price controls involving unknown demand on discrete nonperishable items},
  volume       = {68},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Technical note—waterfall and agile product development
approaches: Disjunctive stochastic programming formulations.
<em>OR</em>, <em>68</em>(5), iii–vi. (<a
href="https://doi.org/10.1287/opre.2019.1977">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The periodic selection of new product development (NPD) projects is a crucial operational decision. The main goals of start-up companies in NPD are to attain a reliable return level and deliver this return level fast. Achieving these goals is complicated because of uncertainties in projects’ returns and durations. We develop new disjunctive stochastic programming models that capture the above-mentioned NPD goals. The first stochastic model is static, representing the traditional waterfall product development process, whereas the second one is dynamic, representing the agile product development process. We design a reformulation method and a decomposition algorithm to solve a problem encountered by a U.S.-based software start-up company. Our results indicate counterintuitively that high reliability in attaining a targeted return may be achieved by investing in projects with a longer development time and higher risk. Furthermore, we show that if the capability to make dynamic decisions is overlooked while available, the time to attain the targeted return is overestimated.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2019.1977},
  journal      = {Operations Research},
  number       = {5},
  pages        = {iii-vi},
  shortjournal = {Oper. Res.},
  title        = {Technical Note—Waterfall and agile product development approaches: Disjunctive stochastic programming formulations},
  volume       = {68},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Technical note—pricing and prioritization in a duopoly with
self-selecting, heterogeneous, time-sensitive customers under low
utilization. <em>OR</em>, <em>68</em>(5), iii–vi. (<a
href="https://doi.org/10.1287/opre.2020.1983">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Time is often used as a differentiating factor in several service operations contexts by service providers (SPs) who prioritize their customers. We use a three-stage game to investigate the competition between two SPs providing service with relatively low utilization to impatient and patient customers. In the first stage, the SPs decide whether to offer single service in which customers are seen on a first-come-first-serve basis or differentiated service with prioritization. In the second stage, they set their prices. In the third stage, customers self-select and make their purchase decisions. We use a novel approach to model customers’ self-selection through an optimization problem with appropriate individual rationality (IR) and incentive compatibility (IC) conditions. In the pricing subgame, we focus on scenarios in which all customers get a positive net utility from using the service. We analyze the following three kinds of service delivery by the SPs, and we characterize different types of equilibrium associated with them: (i) S S in which both SPs provide single service, (ii) S D in which one SP provides single service and the other SP provides differentiated service, and (iii) D D in which both of them provide differentiated service. We then use these results to determine the overall equilibrium. We derive conditions (on customer heterogeneity and the fraction of impatient customers) for S S , S D , or D D to result in the overall equilibrium.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2020.1983},
  journal      = {Operations Research},
  number       = {5},
  pages        = {iii-vi},
  shortjournal = {Oper. Res.},
  title        = {Technical Note—Pricing and prioritization in a duopoly with self-selecting, heterogeneous, time-sensitive customers under low utilization},
  volume       = {68},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A stochastic integer programming approach to air traffic
scheduling and operations. <em>OR</em>, <em>68</em>(5), iii–vi. (<a
href="https://doi.org/10.1287/opre.2020.1985">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Air traffic management measures comprise tactical operating procedures to minimize delay costs and strategic scheduling interventions to control overcapacity scheduling. Although interdependent, these problems have been treated in isolation. This paper proposes an integrated model of scheduling and operations in airport networks that jointly optimizes scheduling interventions and ground-holding operations across airports networks under operating uncertainty. It is formulated as a two-stage stochastic program with integer recourse. To solve it, we develop an original decomposition algorithm with provable solution quality guarantees. The algorithm relies on new optimality cuts— dual integer cuts —that leverage the reduced costs of the dual linear programming relaxation of the second-stage problem. The algorithm also incorporates neighborhood constraints , which shift from exploration to exploitation at later stages. We also use a scenario generation approach to construct representative scenarios from historical records of operations—using integer programming. Computational experiments show that our algorithm yields near-optimal solutions for the entire U.S. National Airspace System network. Ultimately, the proposed approach enhances airport demand management models through scale integration (by capturing network-wide interdependencies) and scope integration (by capturing interdependencies between scheduling and operations).},
  archive      = {J_OR},
  doi          = {10.1287/opre.2020.1985},
  journal      = {Operations Research},
  number       = {5},
  pages        = {iii-vi},
  shortjournal = {Oper. Res.},
  title        = {A stochastic integer programming approach to air traffic scheduling and operations},
  volume       = {68},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Adaptive matching for expert systems with uncertain task
types. <em>OR</em>, <em>68</em>(5), iii–vi. (<a
href="https://doi.org/10.1287/opre.2019.1954">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A matching in a two-sided market often incurs an externality: a matched resource may become unavailable to the other side of the market, at least for a while. This is especially an issue in online platforms involving human experts, as the expert resources are often scarce. The efficient utilization of experts in these platforms is made challenging by the fact that the information available about the parties involved is usually limited. To address this challenge, we develop a model of a task-expert matching system where a task is matched to an expert using not only the prior information about the task but also the feedback obtained from the past matches. In our model, the tasks arrive online while the experts are fixed and constrained by a finite service capacity. For this model, we characterize the maximum task resolution throughput a platform can achieve. We show that the natural greedy approach where each expert is assigned a task most suitable to his or her skill is suboptimal, as it does not internalize the aforementioned externality. We develop a throughput-optimal backpressure algorithm which does so by accounting for the “congestion” among different task types. Finally, we validate our model and confirm our theoretical findings with data-driven simulations via logs of Math.StackExchange.com, a Stack Overflow forum dedicated to mathematics.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2019.1954},
  journal      = {Operations Research},
  number       = {5},
  pages        = {iii-vi},
  shortjournal = {Oper. Res.},
  title        = {Adaptive matching for expert systems with uncertain task types},
  volume       = {68},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Production scheduling for strategic open pit mine planning:
A mixed-integer programming approach. <em>OR</em>, <em>68</em>(5),
iii–vi. (<a href="https://doi.org/10.1287/opre.2019.1965">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Given a discretized representation of an ore body known as a block model, the open pit mining production scheduling problem that we consider consists of defining which blocks to extract, when to extract them, and how or whether to process them, in such a way as to comply with operational constraints and maximize net present value. Although it has been established that this problem can be modeled with mixed-integer programming, the number of blocks used to represent real-world mines (millions) has made solving large instances nearly impossible in practice. In this article, we introduce a new methodology for tackling this problem and conduct computational tests using real problem sets ranging in size from 20,000 to 5,000,000 blocks and spanning 20 to 50 time periods. We consider both direct block scheduling and bench-phase scheduling problems, with capacity, blending, and minimum production constraints. Using new preprocessing and cutting planes techniques, we are able to reduce the linear programming relaxation value by up to 33\%, depending on the instance. Then, using new heuristics, we are able to compute feasible solutions with an average gap of 1.52\% relative to the previously computed bound. Moreover, after four hours of running a customized branch-and-bound algorithm on the problems with larger gaps, we are able to further reduce the average from 1.52\% to 0.71\%.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2019.1965},
  journal      = {Operations Research},
  number       = {5},
  pages        = {iii-vi},
  shortjournal = {Oper. Res.},
  title        = {Production scheduling for strategic open pit mine planning: A mixed-integer programming approach},
  volume       = {68},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Technical note—data-based dynamic pricing and inventory
control with censored demand and limited price changes. <em>OR</em>,
<em>68</em>(5), iii–vi. (<a
href="https://doi.org/10.1287/opre.2020.1993">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A firm makes pricing and inventory replenishment decisions for a product over T periods to maximize its expected total profit. Demand is random and price sensitive, and unsatisfied demands are lost and unobservable (censored demand). The firm knows the demand process up to some parameters and needs to learn them through pricing and inventory experimentation. However, because of business constraints, the firm is prevented from making frequent price changes, leading to correlated and dependent sales data. We develop data-driven algorithms by actively experimenting inventory and pricing decisions and construct maximum likelihood estimator with censored and correlated samples for parameter estimation. We analyze the algorithms using the T -period regret, defined as the profit loss of the algorithms over T periods compared with the clairvoyant optimal policy that knew the parameters a priori. For a so-called well-separated case, we show that the regret of our algorithm is O ( T 1 / ( m+1 ) ) when the number of price changes is limited by m ≥ 1 and is O ( log T ) when limited by β log T for some positive constant β &gt; 0 , whereas for a more general case, the regret is O ( T 1 / 2 ) when the underlying demand is bounded and O ( T 1 / 2 ⁡ log T ) when the underlying demand is unbounded. We further prove that our algorithm for each case is the best possible in the sense that its regret rate matches with the theoretical lower bound.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2020.1993},
  journal      = {Operations Research},
  number       = {5},
  pages        = {iii-vi},
  shortjournal = {Oper. Res.},
  title        = {Technical Note—Data-based dynamic pricing and inventory control with censored demand and limited price changes},
  volume       = {68},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Robust contract designs: Linear contracts and moral hazard.
<em>OR</em>, <em>68</em>(5), iii–vi. (<a
href="https://doi.org/10.1287/opre.2020.1994">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider incentive compensation where the firm has ambiguity on the effort-contingent output distribution: The parameters of the output probability distribution are in an ellipsoidal uncertainty set. The firm evaluates any contract by its worst-case performance over all possible parameters in the uncertainty set. Similarly, the incentive compatible condition for the agent must hold for all possible parameters in the uncertainty set. The firm is financially risk neutral and the agent has limited liability. We find that when the agent is financially risk neutral, the optimal robust contract is a linear contract—paying the agent a base payment and a fixed share of the output. Moreover, the linear contract is the only type of contract that is robust to the parameter uncertainty. When there is model uncertainty over a general effort-contingent output distribution, we show that a generalized linear contract is uniquely optimal. When the agent is risk averse and has a piecewise linear utility, the only optimal contract is a piecewise linear contract that consists of progressive fixed payments and linear rewards with progressive commission rates. We also provide the analysis for the trade-off between robustness and worst-case performance and show that our results are robust to a variety of settings, including cases with general ℓ p ‐norm uncertainty sets, multiple effort levels, and so on. Our paper provides a new explanation for the popularity of linear contracts and piecewise linear contracts in practice and introduces a flexible modeling approach for robust contract designs with model uncertainty.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2020.1994},
  journal      = {Operations Research},
  number       = {5},
  pages        = {iii-vi},
  shortjournal = {Oper. Res.},
  title        = {Robust contract designs: Linear contracts and moral hazard},
  volume       = {68},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Interior-point-based online stochastic bin packing.
<em>OR</em>, <em>68</em>(5), iii–vi. (<a
href="https://doi.org/10.1287/opre.2019.1914">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Bin packing is an algorithmic problem that arises in diverse applications such as remnant inventory systems, shipping logistics, and appointment scheduling. In its simplest variant, a sequence of T items (e.g., orders for raw material, packages for delivery) is revealed one at a time, and each item must be packed on arrival in an available bin (e.g., remnant pieces of raw material in inventory, shipping containers). The sizes of items are independent and identically distributed (i.i.d.) samples from an unknown distribution, but the sizes are known when the items arrive. The goal is to minimize the number of nonempty bins (equivalently waste, defined to be the total unused space in nonempty bins). This problem has been extensively studied in the operations research and theoretical computer science communities, yet all existing heuristics either rely on learning the distribution or exhibit o( T ) additive suboptimality compared with the optimal offline algorithm only for certain classes of distributions (those with sublinear optimal expected waste). In this paper, we propose a family of algorithms that are the first truly distribution-oblivious algorithms for stochastic bin packing, and achieve O ( T ) additive suboptimality for all item size distributions. Our algorithms are inspired by approximate interior-point algorithms for convex optimization. In addition to regret guarantees for discrete i.i.d. sequences, we extend our results to continuous item size distribution with bounded density, and prove a family of novel regret bounds for non-i.i.d. input sequences. To the best of our knowledge, these are the first such results for non-i.i.d. and non-random-permutation input sequences for online stochastic packing.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2019.1914},
  journal      = {Operations Research},
  number       = {5},
  pages        = {iii-vi},
  shortjournal = {Oper. Res.},
  title        = {Interior-point-based online stochastic bin packing},
  volume       = {68},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Nonstationary bandits with habituation and recovery
dynamics. <em>OR</em>, <em>68</em>(5), iii–vi. (<a
href="https://doi.org/10.1287/opre.2019.1918">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many settings involve sequential decision making where a set of actions can be chosen at each time step, each action provides a stochastic reward, and the distribution for the reward provided by each action is initially unknown. However, frequent selection of a specific action may reduce the expected reward for that action, whereas abstaining from choosing an action may cause its expected reward to increase. Such nonstationary phenomena are observed in many real-world settings such as personalized healthcare adherence–improving interventions and targeted online advertising. Though finding an optimal policy for general models with nonstationarity is PSPACE-complete, we propose and analyze a new class of models called reducing or gaining unknown efficacy (ROGUE) bandits, which we show in this paper can capture these phenomena and are amenable to the design of policies with provable properties. We first present a consistent maximum likelihood approach to estimate the parameters of these models and conduct a statistical analysis to construct finite sample concentration bounds. Using this analysis, we develop and analyze two different algorithms for optimizing ROGUE models: an upper confidence bound algorithm (ROGUE-UCB) and an ɛ-greedy algorithm (ɛ-ROGUE). Our theoretical analysis shows that under proper conditions, the ROGUE-UCB and ɛ-ROGUE algorithms can achieve logarithmic in time regret, unlike existing algorithms, which result in linear regret. We conclude with a numerical experiment using real-world data from a personalized healthcare adherence–improving intervention to increase physical activity. In this intervention, the goal is to optimize the selection of messages (e.g., confidence increasing versus knowledge increasing) to send to each individual each day to increase adherence and physical activity. Our results show that ROGUE-UCB and ɛ-ROGUE perform better in terms of aggregated regret and average reward when compared with state-of-the-art algorithms, and in the context of this intervention, the use of ROGUE-UCB increases daily step counts by roughly 1,000 steps a day (about a half-mile more of walking) compared with other algorithms in a simulation experiment.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2019.1918},
  journal      = {Operations Research},
  number       = {5},
  pages        = {iii-vi},
  shortjournal = {Oper. Res.},
  title        = {Nonstationary bandits with habituation and recovery dynamics},
  volume       = {68},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Fast best subset selection: Coordinate descent and local
combinatorial optimization algorithms. <em>OR</em>, <em>68</em>(5),
iii–vi. (<a href="https://doi.org/10.1287/opre.2019.1919">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The L 0 -regularized least squares problem (a.k.a. best subsets ) is central to sparse statistical learning and has attracted significant attention across the wider statistics, machine learning, and optimization communities. Recent work has shown that modern mixed integer optimization (MIO) solvers can be used to address small to moderate instances of this problem. In spite of the usefulness of L 0 -based estimators and generic MIO solvers, there is a steep computational price to pay when compared with popular sparse learning algorithms (e.g., based on L 1 regularization). In this paper, we aim to push the frontiers of computation for a family of L 0 -regularized problems with additional convex penalties. We propose a new hierarchy of necessary optimality conditions for these problems. We develop fast algorithms, based on coordinate descent and local combinatorial optimization, that are guaranteed to converge to solutions satisfying these optimality conditions. From a statistical viewpoint, an interesting story emerges. When the signal strength is high, our combinatorial optimization algorithms have an edge in challenging statistical settings. When the signal is lower, pure L 0 benefits from additional convex regularization. We empirically demonstrate that our family of L 0 -based estimators can outperform the state-of-the-art sparse learning algorithms in terms of a combination of prediction, estimation, and variable selection metrics under various regimes (e.g., different signal strengths, feature correlations, number of samples and features). Our new open-source sparse learning toolkit L0Learn (available on CRAN and GitHub) reaches up to a threefold speedup (with p up to 10 6 ) when compared with competing toolkits such as glmnet and ncvreg.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2019.1919},
  journal      = {Operations Research},
  number       = {5},
  pages        = {iii-vi},
  shortjournal = {Oper. Res.},
  title        = {Fast best subset selection: Coordinate descent and local combinatorial optimization algorithms},
  volume       = {68},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Optimal online learning for nonlinear belief models using
discrete priors. <em>OR</em>, <em>68</em>(5), iii–vi. (<a
href="https://doi.org/10.1287/opre.2019.1921">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider an optimal learning problem where we are trying to learn a function that is nonlinear in unknown parameters in an online setting. We formulate the problem as a dynamic program, provide the optimality condition using Bellman’s equation, and propose a multiperiod lookahead policy to overcome the nonconcavity in the value of information. We adopt a sampled belief model, which we refer to as a discrete prior. For an infinite-horizon problem with discounted cumulative rewards, we prove asymptotic convergence properties under the proposed policy, a rare result for online learning. We then demonstrate the approach in three different settings: a health setting where we make medical decisions to maximize healthcare response over time, a dynamic pricing setting where we make pricing decisions to maximize the cumulative revenue, and a clinical pharmacology setting where we make dosage controls to minimize the deviation between actual and target effects.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2019.1921},
  journal      = {Operations Research},
  number       = {5},
  pages        = {iii-vi},
  shortjournal = {Oper. Res.},
  title        = {Optimal online learning for nonlinear belief models using discrete priors},
  volume       = {68},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Technical note—central limit theorems for estimated
functions at estimated points. <em>OR</em>, <em>68</em>(5), iii–vi. (<a
href="https://doi.org/10.1287/opre.2019.1922">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We provide a simple proof of the central limit theorem (CLT) for estimated functions at estimated points. Such estimators arise in a number of different simulation-based computational settings. We illustrate the methodology via applications to quantile estimation and related sensitivity analysis, as well as to computation of conditional value-at-risk.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2019.1922},
  journal      = {Operations Research},
  number       = {5},
  pages        = {iii-vi},
  shortjournal = {Oper. Res.},
  title        = {Technical Note—Central limit theorems for estimated functions at estimated points},
  volume       = {68},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Decision making when things are only a matter of time.
<em>OR</em>, <em>68</em>(5), iii–vi. (<a
href="https://doi.org/10.1287/opre.2019.1923">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article gives a comprehensive treatment of preferences regarding time risk—the risk of something happening sooner or later—within the expected discounted utility model. We characterize the signs of the discount function’s derivatives of all orders and show how these signs are decisive for time risk preferences. We introduce the notions of prudent and temperate discounting and illustrate their importance for economic behavior. Several applications in which an important event is “only a matter of time” are presented. The seminal net present value rule to evaluate investment opportunities is generalized to take into account the timing uncertainty of cash flows.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2019.1923},
  journal      = {Operations Research},
  number       = {5},
  pages        = {iii-vi},
  shortjournal = {Oper. Res.},
  title        = {Decision making when things are only a matter of time},
  volume       = {68},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Technical note—time inconsistency of optimal policies of
distributionally robust inventory models. <em>OR</em>, <em>68</em>(5),
iii–vi. (<a href="https://doi.org/10.1287/opre.2019.1932">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we investigate optimal policies of distributionally robust (risk averse) inventory models. We demonstrate that if the respective risk measures are not strictly monotone, then there may exist infinitely many optimal policies that are not base-stock and not time consistent. This is in a sharp contrast with the risk neutral formulation of the inventory model where all optimal policies are time consistent. This also extends previous studies of time inconsistency in the robust setting.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2019.1932},
  journal      = {Operations Research},
  number       = {5},
  pages        = {iii-vi},
  shortjournal = {Oper. Res.},
  title        = {Technical Note—Time inconsistency of optimal policies of distributionally robust inventory models},
  volume       = {68},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Learning in combinatorial optimization: What and how to
explore. <em>OR</em>, <em>68</em>(5), iii–vi. (<a
href="https://doi.org/10.1287/opre.2019.1926">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study dynamic decision making under uncertainty when, at each period, a decision maker implements a solution to a combinatorial optimization problem. The objective coefficient vectors of said problem, which are unobserved before implementation, vary from period to period. These vectors, however, are known to be random draws from an initially unknown distribution with known range. By implementing different solutions, the decision maker extracts information about the underlying distribution but at the same time experiences the cost associated with said solutions. We show that resolving the implied exploration versus exploitation tradeoff efficiently is related to solving a lower-bound problem (LBP), which simultaneously answers the questions of what to explore and how to do so. We establish a fundamental limit on the asymptotic performance of any admissible policy that is proportional to the optimal objective value of the LBP problem. We show that such a lower bound might be asymptotically attained by policies that adaptively reconstruct and solve the LBP at an exponentially decreasing frequency. Because the LBP is likely intractable in practice, we propose policies that instead reconstruct and solve a proxy for the LBP, which we call the optimality cover problem (OCP). We provide strong evidence of the practical tractability of the OCP, which implies that the proposed policies can be implemented in real time. We test the performance of the proposed policies through extensive numerical experiments, and we show that they significantly outperform relevant benchmarks in the long-term and are competitive in the short-term.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2019.1926},
  journal      = {Operations Research},
  number       = {5},
  pages        = {iii-vi},
  shortjournal = {Oper. Res.},
  title        = {Learning in combinatorial optimization: What and how to explore},
  volume       = {68},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Optimization of tree ensembles. <em>OR</em>, <em>68</em>(5),
iii–vi. (<a href="https://doi.org/10.1287/opre.2019.1928">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Tree ensemble models such as random forests and boosted trees are among the most widely used and practically successful predictive models in applied machine learning and business analytics. Although such models have been used to make predictions based on exogenous, uncontrollable independent variables, they are increasingly being used to make predictions where the independent variables are controllable and are also decision variables. In this paper, we study the problem of tree ensemble optimization: given a tree ensemble that predicts some dependent variable using controllable independent variables, how should we set these variables so as to maximize the predicted value? We formulate the problem as a mixed-integer optimization problem. We theoretically examine the strength of our formulation, provide a hierarchy of approximate formulations with bounds on approximation quality and exploit the structure of the problem to develop two large-scale solution methods, one based on Benders decomposition and one based on iteratively generating tree split constraints. We test our methodology on real data sets, including two case studies in drug design and customized pricing, and show that our methodology can efficiently solve large-scale instances to near or full optimality, and outperforms solutions obtained by heuristic approaches.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2019.1928},
  journal      = {Operations Research},
  number       = {5},
  pages        = {iii-vi},
  shortjournal = {Oper. Res.},
  title        = {Optimization of tree ensembles},
  volume       = {68},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Regime classification and stock loan valuation. <em>OR</em>,
<em>68</em>(4), ii–v. (<a
href="https://doi.org/10.1287/opre.2019.1934">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For traditional perpetual American put options under regime-switching models, the risk-free interest rates are typically assumed to be positive, and optimal stopping usually can occur in any regime. However, this may not hold true when the risk-free interest rates are allowed to be equal to zero (the interest rate may drop to zero in reality, e.g., in certain periods in Japan); there may exist “continuation regimes” within which optimal stopping can never occur, that is, within which stopping is never optimal. In this paper, we develop a unified, fixed point approach to determining all continuation regimes explicitly for the pricing problem of perpetual American put options under general regime-switching exponential Lévy models with any finite numbers of regimes and general Lévy types, in which the risk-free interest rate in each regime is nonnegative and the discounted stock price with the dividends reinvested is not necessarily a martingale. As an important application of this regime classification result, we provide a unified framework for the valuation of infinite maturity stock loans under general regime-switching exponential Lévy models with any finite numbers of regimes and general Lévy types, which can be formulated as the pricing problem of perpetual American call options with negative interest rates. Applying this unified approach yields analytical solutions to the infinite maturity stock loan prices under both the general exponential Lévy models without regime switching and the regime-switching phase-type jump diffusion models with any finite numbers of regimes and with or without dividends, which include some related results in the literature as special cases.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2019.1934},
  journal      = {Operations Research},
  number       = {4},
  pages        = {ii-v},
  shortjournal = {Oper. Res.},
  title        = {Regime classification and stock loan valuation},
  volume       = {68},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Fleet coordination in decentralized humanitarian operations
funded by earmarked donations. <em>OR</em>, <em>68</em>(4), ii–v. (<a
href="https://doi.org/10.1287/opre.2019.1941">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study incentive alignment for the coordination of humanitarian operations. Transportation is the second-largest overhead cost (after personnel) for international humanitarian organizations (IHOs). Yet, the management of IHO vehicle fleets faces obstacles to reach efficiency, due to a structural conflict between the incentives of an IHO’s individual program (which prioritize prompt service and minimal program cost) and the IHO’s central headquarters (which on top of the objective of the program is also responsible for the running costs of the entire fleet). In recent years, the headquarters has been further limited by a trend toward donors earmarking their funding. This problem is further complicated by information asymmetry in the system due to the dispersed geographical locations of the programs. Assuming these real-world conditions, grounded in field research, we design a novel mechanism to align incentives. Interestingly, the standard result of “no output distortion and positive information rent for the efficient type” in adverse selection models does not hold in our setting. We obtain three parameter-dependent regions for truth revelation and link these regions to the type of activities carried out by different programs.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2019.1941},
  journal      = {Operations Research},
  number       = {4},
  pages        = {ii-v},
  shortjournal = {Oper. Res.},
  title        = {Fleet coordination in decentralized humanitarian operations funded by earmarked donations},
  volume       = {68},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Planning for end-user substitution in agribusiness.
<em>OR</em>, <em>68</em>(4), ii–v. (<a
href="https://doi.org/10.1287/opre.2019.1943">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we consider the problem in which a firm offers a portfolio of products (agricultural seeds) to multiple customer segments comprising farmers under aggressive fill-rate constraints, and some, but not all, customers will accept a substitute to their preferred choice. This business situation is not adequately represented by traditional inventory-management models, where a firm initiates a substitution based on its monetary considerations. By exploiting some recent results on polyhedral expectations, we develop a decomposition-based approach to determine optimal inventory levels for the firm’s seed portfolio under aggressive fill-rate targets. The approach provides an exact solution that is implementable in managerial-friendly environments and permits a what-if analysis for real-time decision support. Subsequently we extend the technical development to establish: (i) a simple computable bound on the value of substitution, (ii) a procedure for determining implied penalty costs for substitutable products, and (iii) comparative static results for the product portfolio. We also discuss the implementation of the technical development at a Fortune 100 firm that has resulted in significant monetary savings. Finally, we provide geography- and climate-specific managerial insights for managing seed substitution by end-users.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2019.1943},
  journal      = {Operations Research},
  number       = {4},
  pages        = {ii-v},
  shortjournal = {Oper. Res.},
  title        = {Planning for end-user substitution in agribusiness},
  volume       = {68},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Dynamic assortment personalization in high dimensions.
<em>OR</em>, <em>68</em>(4), ii–v. (<a
href="https://doi.org/10.1287/opre.2019.1948">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study the problem of dynamic assortment personalization with large, heterogeneous populations and wide arrays of products, and demonstrate the importance of structural priors for effective, efficient large-scale personalization. Assortment personalization is the problem of choosing, for each individual (type), a best assortment of products, ads, or other offerings (items) so as to maximize revenue. This problem is central to revenue management in e-commerce and online advertising where both items and types can number in the millions. We formulate the dynamic assortment personalization problem as a discrete-contextual bandit with m contexts (types) and exponentially many arms (assortments of the n items). We assume that each type’s preferences follow a simple parametric model with n parameters. In all, there are mn parameters, and existing literature suggests that order optimal regret scales as mn . However, the data required to estimate so many parameters is orders of magnitude larger than the data available in most revenue management applications; and the optimal regret under these models is unacceptably high. In this paper, we impose a natural structure on the problem – a small latent dimension, or low rank. In the static setting, we show that this model can be efficiently learned from surprisingly few interactions, using a time- and memory-efficient optimization algorithm that converges globally whenever the model is learnable. In the dynamic setting, we show that structure-aware dynamic assortment personalization can have regret that is an order of magnitude smaller than structure-ignorant approaches. We validate our theoretical results empirically.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2019.1948},
  journal      = {Operations Research},
  number       = {4},
  pages        = {ii-v},
  shortjournal = {Oper. Res.},
  title        = {Dynamic assortment personalization in high dimensions},
  volume       = {68},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Technical note—dynamic pricing with heterogeneous patience
levels. <em>OR</em>, <em>68</em>(4), ii–v. (<a
href="https://doi.org/10.1287/opre.2019.1951">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider the problem of dynamic pricing in the presence of patient consumers. We call a consumer patient if she is willing to wait a certain number of periods for a lower price and will purchase as soon as the price is equal to or below her valuation. We allow for arbitrary joint distributions of patience levels and valuations. We propose an efficient dynamic programming algorithm for finding optimal pricing policies. We find numerically that optimal policies can take the form of incomplete cyclic policies, mixing features of nested sales policies and of decreasing cyclic policies.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2019.1951},
  journal      = {Operations Research},
  number       = {4},
  pages        = {ii-v},
  shortjournal = {Oper. Res.},
  title        = {Technical Note—Dynamic pricing with heterogeneous patience levels},
  volume       = {68},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Technical note—robust newsvendor games with ambiguity in
demand distributions. <em>OR</em>, <em>68</em>(4), ii–v. (<a
href="https://doi.org/10.1287/opre.2019.1955">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In classical newsvendor games, vendors collaborate to serve their aggregate demand whose joint distribution is assumed known with certainty. We investigate a new class of newsvendor games with ambiguity in the joint demand distributions, which is represented by a Fréchet class of distributions with some, possibly overlapping, marginal information. To model this new class of games, we use ideas from distributionally robust optimization to handle distributional ambiguity and study the robust newsvendor games. We provide conditions for the existence of core solutions of these games using the structural analysis of the worst-case joint demand distributions of the corresponding distributionally robust newsvendor optimization problem.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2019.1955},
  journal      = {Operations Research},
  number       = {4},
  pages        = {ii-v},
  shortjournal = {Oper. Res.},
  title        = {Technical Note—Robust newsvendor games with ambiguity in demand distributions},
  volume       = {68},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Technical note—constant-order policies for lost-sales
inventory models with random supply functions: Asymptotics and
heuristic. <em>OR</em>, <em>68</em>(4), ii–v. (<a
href="https://doi.org/10.1287/opre.2019.1971">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider an infinite-horizon lost-sales inventory model where the supply takes positive lead times and is a random function of the order quantity (e.g., random yield/capacity). The optimal policy for this model is computationally intractable, and no heuristic has been proposed in the literature. In this paper, we focus on a simple class of constant-order policies (COPs) that place the same order in every period regardless of the system state. Under some assumptions on the random supply function, we prove that the best COP is asymptotically optimal with large lead times, and the optimality gap converges to zero exponentially fast in the lead time. We also prove that if the mean supply capacity is less than the mean demand, then the best COP is also asymptotically optimal with large penalty costs; otherwise, the long-run average cost of the best COP asymptotically increases at the rate of the square root of the penalty cost. Further, we construct a simple heuristic COP and show that it performs very close to the best COP. Finally, we provide a numerical study to derive further insights into the performance of the best COP.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2019.1971},
  journal      = {Operations Research},
  number       = {4},
  pages        = {ii-v},
  shortjournal = {Oper. Res.},
  title        = {Technical Note—Constant-order policies for lost-sales inventory models with random supply functions: Asymptotics and heuristic},
  volume       = {68},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Robust auctions for revenue via enhanced competition.
<em>OR</em>, <em>68</em>(4), ii–v. (<a
href="https://doi.org/10.1287/opre.2019.1929">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most results in revenue-maximizing mechanism design hinge on “getting the price right”—selling goods to bidders at prices low enough to encourage a sale but high enough to garner nontrivial revenue. This approach is difficult to implement when the seller has little or no a priori information about bidder valuations or when the setting is sufficiently complex, such as matching markets with heterogeneous goods. In this paper, we apply a robust approach to designing auctions for revenue. Instead of relying on prior knowledge regarding bidder valuations, we “let the market do the work” and let prices emerge from competition for scarce goods. We analyze the revenue guarantees of one of the simplest imaginable implementations of this idea: first, we enhance competition in the market by increasing demand (or alternatively, by limiting supply), and second, we run a standard second price (Vickrey) auction. In their renowned work from 1996 , Bulow and Klemperer [Bulow J, Klemperer P (1996) Auctions vs. negotiations. Amer. Econom. Rev. 86(1):180–194.] apply this method to markets with single goods. As our main result, we give the first application beyond single-parameter settings, proving that, simultaneously for many valuation distributions, this method achieves expected revenue at least as good as the optimal revenue in the original market. Our robust and simple approach provides a handle on the elusive optimal revenue in multiitem matching markets and shows when the use of welfare-maximizing Vickrey auctions is justified, even if revenue is a priority. By establishing quantitative tradeoffs, our work provides guidelines for a seller in choosing among two different revenue-extracting strategies: sophisticated pricing based on market research or advertising to draw additional bidders.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2019.1929},
  journal      = {Operations Research},
  number       = {4},
  pages        = {ii-v},
  shortjournal = {Oper. Res.},
  title        = {Robust auctions for revenue via enhanced competition},
  volume       = {68},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Selling passes to strategic customers. <em>OR</em>,
<em>68</em>(4), ii–v. (<a
href="https://doi.org/10.1287/opre.2019.1936">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Passes are prepaid packages of multiple units of goods or services with flexible consumption times. They may take a variety of forms such as commuter passes in transportation, capped quotas in telecommunications, or memberships in health or beauty clubs. We consider a monopolist selling products or services to strategic customers by dynamically pricing passes in conjunction with individual items. The strategic behavior is captured by a dynamic choice model that endogenizes strategic purchase, utilization, and renewal of the pass. Under the control-theoretic framework, we find that the optimal pricing policy has a turnpike property; the optimal price trajectories stay near the steady state for most of the sales horizon, and the fixed-pricing policy performs very well when the horizon is long enough. In the turnpike, we show that passes should offer a quantity discount when customers are not fully strategic. As a form of advance purchase, passes allow the firm to capitalize on strategic behavior without limiting the supply. From the revenue-recognition principle, we show that a passholder can generate a higher revenue rate than a nonpassholder customer.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2019.1936},
  journal      = {Operations Research},
  number       = {4},
  pages        = {ii-v},
  shortjournal = {Oper. Res.},
  title        = {Selling passes to strategic customers},
  volume       = {68},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Becoming strategic: Endogenous consumer time preferences and
multiperiod pricing. <em>OR</em>, <em>68</em>(4), ii–v. (<a
href="https://doi.org/10.1287/opre.2019.1937">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Pricing over multiple periods under forward-looking, strategic consumer purchasing behavior has received significant recent research attention; however, whether consumers actually benefit from this behavior and would voluntarily choose to be strategic has not been previously considered. We explore this question by developing a model of endogenous time preferences, consistent with microeconomic theories of boundedly rational intertemporal decision making, in which consumers choose to become strategic by exerting costly effort. We show three key implications of this choice. First, considering the consumer choice to be strategic can have a significant impact on firm and consumer decisions—in particular, qualitatively impacting the firm’s optimal pricing policy. Second, it is possible to increase firm profit, consumer surplus, and social welfare simultaneously by increasing the cost of strategic behavior, suggesting that firms can, essentially, force consumers to be myopic and make all parties better off; this helps explain how firms that do the most to make strategic behavior difficult are able to attract more demand and be successful in the marketplace. And third, efforts to mitigate strategic consumer waiting by committing to future prices instead of pricing dynamically may decrease the cost of strategic behavior and backfire, encouraging more consumers to be strategic; hence, in contrast to most previous research, price commitment may yield lower profit than dynamic pricing if consumers can choose to be strategic.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2019.1937},
  journal      = {Operations Research},
  number       = {4},
  pages        = {ii-v},
  shortjournal = {Oper. Res.},
  title        = {Becoming strategic: Endogenous consumer time preferences and multiperiod pricing},
  volume       = {68},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Bayesian incentive-compatible bandit exploration.
<em>OR</em>, <em>68</em>(4), ii–v. (<a
href="https://doi.org/10.1287/opre.2019.1949">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As self-interested individuals (“agents”) make decisions over time, they utilize information revealed by other agents in the past and produce information that may help agents in the future. This phenomenon is common in a wide range of scenarios in the Internet economy, as well as in medical decisions. Each agent would like to exploit : select the best action given the current information, but would prefer the previous agents to explore : try out various alternatives to collect information. A social planner, by means of a carefully designed recommendation policy, can incentivize the agents to balance the exploration and exploitation so as to maximize social welfare. We model the planner’s recommendation policy as a multiarm bandit algorithm under incentive-compatibility constraints induced by agents’ Bayesian priors. We design a bandit algorithm which is incentive-compatible and has asymptotically optimal performance, as expressed by regret. Further, we provide a black-box reduction from an arbitrary multiarm bandit algorithm to an incentive-compatible one, with only a constant multiplicative increase in regret. This reduction works for very general bandit settings that incorporate contexts and arbitrary partial feedback.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2019.1949},
  journal      = {Operations Research},
  number       = {4},
  pages        = {ii-v},
  shortjournal = {Oper. Res.},
  title        = {Bayesian incentive-compatible bandit exploration},
  volume       = {68},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Information disclosure and pricing policies for sales of
network goods. <em>OR</em>, <em>68</em>(4), ii–v. (<a
href="https://doi.org/10.1287/opre.2019.1950">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study a two-period model in which a firm faces the problem of deciding whether to commit to sales volume disclosure under market size uncertainty when selling a network good to forward-looking customers who time their purchases. If the first-period sales volume is disclosed, the second-period customers will base their purchase decisions on this information. If the sales volume is not disclosed, all customers will make purchase decisions based on their estimate of market size. We identify two countervailing effects of sales disclosure: (1) a prodisclosure “Matthew effect” (the benefit of a realized large market size tends to outweigh the loss of a realized small one) and (2) an antidisclosure saturation effect (for a sufficiently large market, customers would make a purchase anyway even without knowing the realized market size but might be discouraged if observing a realized small one). With exogenous prices, we show that committing to sales disclosure is a dominating (dominated) policy when the expected network benefit is relatively weak (strong). We also examine three endogenous pricing scenarios. First, under state-independent pricing, committing to sales disclosure is better off if the customer valuation distribution has a high probability of reaching very high values (i.e., a heavy tail). Second, if a firm can credibly preannounce a contingent pricing policy, committing to sales disclosure is always better off. Third, under contingent pricing without commitment, we show that committing to sales disclosure is better off when delaying the purchase decision to the second period does not reduce the value much.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2019.1950},
  journal      = {Operations Research},
  number       = {4},
  pages        = {ii-v},
  shortjournal = {Oper. Res.},
  title        = {Information disclosure and pricing policies for sales of network goods},
  volume       = {68},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Social and monopoly optimization in observable queues.
<em>OR</em>, <em>68</em>(4), ii–v. (<a
href="https://doi.org/10.1287/opre.2019.1907">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Naor’s celebrated paper studies customer decisions in an observable M/M/1 queue in which joining-customers utility is linearly decreasing with the joining position. Naor derives the optimal threshold strategies for the individuals, social planner, and monopolist and proves that the monopoly optimal threshold is (weakly) smaller than the socially optimal threshold, which is (weakly) smaller than the individually optimal one. Studies show, based on numerical observations and/or ad hoc proof techniques, that this triangular relation holds within various specific setups, in which the queuing process is not M/M/1 and/or when the utility is not linear. We point out properties that imply the aforementioned result in Naor’s model and its extensions and suggest model applications for our findings. Our formulation gives strictly stronger results than those currently appearing in the literature. We further provide simple examples in which the inequality does not hold.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2019.1907},
  journal      = {Operations Research},
  number       = {4},
  pages        = {ii-v},
  shortjournal = {Oper. Res.},
  title        = {Social and monopoly optimization in observable queues},
  volume       = {68},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Pseudo-valid cutting planes for two-stage mixed-integer
stochastic programs with right-hand-side uncertainty. <em>OR</em>,
<em>68</em>(4), ii–v. (<a
href="https://doi.org/10.1287/opre.2019.1905">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a novel way of applying cutting plane techniques to two-stage mixed-integer stochastic programs with uncertainty in the right-hand side. Instead of using cutting planes that are always valid, our idea is to apply pseudo-valid cutting planes to the second-stage feasible regions that may cut away feasible integer second-stage solutions for some scenarios and may be overly conservative for others. The advantage is that it allows us to use cutting planes that are affine in the first-stage decision variables, so that the approximation is convex and can be efficiently solved using techniques from convex optimization. We derive tight performance guarantees for using particular types of pseudo-valid cutting planes for simple integer recourse models. Moreover, we show in general that using pseudo-valid cutting planes leads to good first-stage solutions if the total variations of the one-dimensional conditional probability density functions of the random variables in the model converge to zero.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2019.1905},
  journal      = {Operations Research},
  number       = {4},
  pages        = {ii-v},
  shortjournal = {Oper. Res.},
  title        = {Pseudo-valid cutting planes for two-stage mixed-integer stochastic programs with right-hand-side uncertainty},
  volume       = {68},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Dynamic scheduling of multiclass many-server queues with
abandonment: The generalized cμ/h rule. <em>OR</em>, <em>68</em>(4),
ii–v. (<a href="https://doi.org/10.1287/opre.2019.1908">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study the fluid model of a many-server queue with multiple customer classes and obtain optimality results for this model. For the purpose of minimizing the long-run average queue-length costs and abandon penalties, we propose three scheduling policies to cope with any general cost functions and general patience-time distributions. First, we introduce the target-allocation policy, which assigns higher priority to customer classes with larger deviation from the desired allocation of the service capacity and prove its optimality for any general queue-length cost functions and patience-time distributions. The Gcμ / h rule, which extends the well-known Gcμ rule by taking abandonment into account, is shown to be optimal for the case of convex queue-length costs and nonincreasing hazard rates of patience. For the case of concave queue-length costs but nondecreasing hazard rates of patience, it is optimal to apply a fixed-priority policy, and a knapsack-like problem is developed to determine the optimal priority order efficiently. As a motivating example of the operations of emergency departments, a hybrid of the Gcμ / h rule and the fixed-priority policy is suggested to reduce crowding and queue abandonment. Numerical experiments show that this hybrid policy performs satisfactorily. We also prove the asymptotic optimality of policies in the original queueing system using the fluid results.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2019.1908},
  journal      = {Operations Research},
  number       = {4},
  pages        = {ii-v},
  shortjournal = {Oper. Res.},
  title        = {Dynamic scheduling of multiclass many-server queues with abandonment: The generalized cμ/h rule},
  volume       = {68},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Technical note—cyclic variables and markov decision
processes. <em>OR</em>, <em>68</em>(4), ii–v. (<a
href="https://doi.org/10.1287/opre.2019.1913">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper I develop a cyclic value function iteration, which is an adjustment to the standard value function iteration. When using this algorithm, the inclusion of cyclic variables of any size into the state space of an infinite horizon Markov decision process does not increase the computational complexity of solving for the value function. This result is proven theoretically and shown to closely hold in practice using Monte Carlo simulations.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2019.1913},
  journal      = {Operations Research},
  number       = {4},
  pages        = {ii-v},
  shortjournal = {Oper. Res.},
  title        = {Technical Note—Cyclic variables and markov decision processes},
  volume       = {68},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Managing supply in the on-demand economy: Flexible workers,
full-time employees, or both? <em>OR</em>, <em>68</em>(4), ii–v. (<a
href="https://doi.org/10.1287/opre.2019.1916">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {There are different workforce models in the “gig” economy. Although some on-demand service providers rely strictly on either traditional employees or independent contractors, others rely on a blended workforce, which melds a layer of contingent workers with a core of permanent employees. In deciding on the “right number of right people to staff at the right time,” managers must appropriately weigh the pertinent tradeoffs. In this paper, we study cost-minimizing staffing decisions in service systems where the manager must decide on how many flexible (contractors) and/or fixed (full-time) agents to staff in order to effectively balance operating costs, varying customer demand patterns, and supply-side uncertainty while not compromising on the quality of service offered to customers. We consider a queueing-theoretic framework where the number of servers is random, because part of the workforce is flexible. Because the staffing problem with a random number of servers is analytically intractable, we formulate two problem relaxations based on fluid and stochastic fluid formulations, and we establish their accuracies in large systems by relying on an asymptotic, many-server mode of analysis. We derive the optimal staffing policy and glean insights into the appropriateness of alternative workforce models in on-demand services. We also shed light on the distinction between demand-side (customer arrival rates) and supply-side (number of servers) uncertainties in queueing systems.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2019.1916},
  journal      = {Operations Research},
  number       = {4},
  pages        = {ii-v},
  shortjournal = {Oper. Res.},
  title        = {Managing supply in the on-demand economy: Flexible workers, full-time employees, or both?},
  volume       = {68},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Diffusion approximation for efficiency-driven queues when
customers are patient. <em>OR</em>, <em>68</em>(4), ii–v. (<a
href="https://doi.org/10.1287/opre.2019.1917">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Motivated by call center practice, we propose a tractable approximate model for queues with general service and patience time assumptions in the efficiency-driven (ED) regime, when customers’ patience times are relatively long compared with their service times. We use a one-dimensional diffusion process to approximate the virtual waiting time process that is scaled in both magnitude and time, with the number of servers and the mean patience time as the respective scaling factors. Using this diffusion model, we obtain the steady-state distributions of virtual waiting time and queue length, which in turn, yield simple formulas for performance measures, such as service levels and effective abandonment fractions. These formulas can work well when the mean patience time is several times longer than the mean service time. To justify the diffusion model, we formulate an asymptotic framework by considering a sequence of queues, where both the number of servers and the mean patience time go to infinity. The centered and scaled virtual waiting time process converges in distribution to the one-dimensional diffusion process under this framework. To prove the diffusion limit, a functional central limit theorem is established for the superposition of renewal processes. From a macroscopic perspective, we demonstrate that the dynamics of a many-server queue in the ED regime could be as simple as the dynamics of a single-server queue when customers are patient.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2019.1917},
  journal      = {Operations Research},
  number       = {4},
  pages        = {ii-v},
  shortjournal = {Oper. Res.},
  title        = {Diffusion approximation for efficiency-driven queues when customers are patient},
  volume       = {68},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Dynamic nonlinear pricing of inventories over finite sales
horizons. <em>OR</em>, <em>68</em>(3), ii–iv. (<a
href="https://doi.org/10.1287/opre.2019.1891">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider a finite-horizon, finite-capacity dynamic pricing model where consumers may purchase multiple units of the same product. We present three models that differ in their complexity and revenue potential. The dynamic nonlinear pricing (DNP) model allows the seller to dynamically selecting a price for each bundle size. The dynamic linear pricing model restricts the seller to dynamically select a unit price for all bundle sizes. There can be a significant revenue gap between the two models, but the additional revenues require a nonlinear policy that may be more challenging to implement. This motivates the study of dynamic block pricing as an intermediate pricing model where prices are linear within each block. A heuristic for this last model provides almost as much revenue as DNP while avoiding its complexity.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2019.1891},
  journal      = {Operations Research},
  number       = {3},
  pages        = {ii-iv},
  shortjournal = {Oper. Res.},
  title        = {Dynamic nonlinear pricing of inventories over finite sales horizons},
  volume       = {68},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Technical note—worst-case benefit of restocking for the
vehicle routing problem with stochastic demands. <em>OR</em>,
<em>68</em>(3), ii–iv. (<a
href="https://doi.org/10.1287/opre.2019.1901">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The extant literature on the vehicle routing problem with stochastic demands indicates that the restocking strategy yields moderate percentage expected cost reductions relative to the a priori approach but lacks theoretical support for this improvement. We conduct a worst-case analysis that corroborates the observed restocking benefits and enhances our understanding of a foundational model in logistics under uncertainty.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2019.1901},
  journal      = {Operations Research},
  number       = {3},
  pages        = {ii-iv},
  shortjournal = {Oper. Res.},
  title        = {Technical Note—Worst-case benefit of restocking for the vehicle routing problem with stochastic demands},
  volume       = {68},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Real-time dynamic pricing for revenue management with
reusable resources, advance reservation, and deterministic service time
requirements. <em>OR</em>, <em>68</em>(3), ii–iv. (<a
href="https://doi.org/10.1287/opre.2019.1906">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider a dynamic pricing problem in a system with reusable resources. Customers arrive randomly over time according to a specified nonstationary rate, and each customer requests a service that uses a combination of different types of resources for a deterministic duration of time. The resources are reusable in the sense that they can be immediately used to serve a new customer on the completion of the previous service. Our objective is to construct a dynamic pricing control that maximizes expected total revenues. This is a fundamental problem faced by firms in many industries. We develop real-time heuristic controls based on the solution of the deterministic relaxation of the original stochastic problem and show that they are near optimal in the regime of large demand and large resource capacity. We further show that our results can be extended to a more general setting with heterogeneous service time and advance reservation.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2019.1906},
  journal      = {Operations Research},
  number       = {3},
  pages        = {ii-iv},
  shortjournal = {Oper. Res.},
  title        = {Real-time dynamic pricing for revenue management with reusable resources, advance reservation, and deterministic service time requirements},
  volume       = {68},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Modeling and design of container terminal operations.
<em>OR</em>, <em>68</em>(3), ii–iv. (<a
href="https://doi.org/10.1287/opre.2019.1920">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The design of container terminal operations is complex because multiple factors affect operational performance. These factors include numerous choices for handling technology, terminal topology, and design parameters and stochastic interactions between the quayside, stackside, and vehicle transport processes. In this research, we propose new integrated queuing network models for rapid design evaluation of container terminals with automated lift vehicles and automated guided vehicles. These models offer the flexibility to analyze alternate design variations and develop insights. For instance, the effect of different vehicle dwell point policies is analyzed using state-dependent queues, whereas the efficient terminal layout is determined using variation in the service time expressions at the stations. We show the relation among the dwell point–dependent waiting times and also show their asymptotic equivalence at heavy traffic conditions. These models form the building blocks for design and analysis of large-scale terminal operations. We test the model efficacy using detailed in-house simulation experiments and real-terminal validation by partnering with an external party.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2019.1920},
  journal      = {Operations Research},
  number       = {3},
  pages        = {ii-iv},
  shortjournal = {Oper. Res.},
  title        = {Modeling and design of container terminal operations},
  volume       = {68},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). The distributionally robust chance-constrained vehicle
routing problem. <em>OR</em>, <em>68</em>(3), ii–iv. (<a
href="https://doi.org/10.1287/opre.2019.1924">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study a variant of the capacitated vehicle routing problem (CVRP), which asks for the cost-optimal delivery of a single product to geographically dispersed customers through a fleet of capacity-constrained vehicles. Contrary to the classical CVRP, which assumes that the customer demands are deterministic, we model the demands as a random vector whose distribution is only known to belong to an ambiguity set. We then require the delivery schedule to be feasible with a probability of at least 1 − ε, where ε characterizes the risk tolerance of the decision maker. We argue that the emerging distributionally robust CVRP can be solved efficiently with standard branch-and-cut algorithms whenever the ambiguity set satisfies a subadditivity condition. We then show that this subadditivity condition holds for a large class of moment ambiguity sets. We derive efficient cut generation schemes for ambiguity sets that specify the support as well as (bounds on) the first and second moments of the customer demands. Our numerical results indicate that the distributionally robust CVRP has favorable scaling properties and can often be solved in runtimes comparable to those of the deterministic CVRP.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2019.1924},
  journal      = {Operations Research},
  number       = {3},
  pages        = {ii-iv},
  shortjournal = {Oper. Res.},
  title        = {The distributionally robust chance-constrained vehicle routing problem},
  volume       = {68},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Technical note—options portfolio selection. <em>OR</em>,
<em>68</em>(3), ii–iv. (<a
href="https://doi.org/10.1287/opre.2019.1925">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We develop a new method to optimize portfolios of options in a market where European calls and puts are available with many exercise prices for each of several potentially correlated underlying assets. We identify the combination of asset-specific option payoffs that maximizes the Sharpe ratio of the overall portfolio: such payoffs form the unique solution to a system of integral equations, which reduces to a linear matrix equation under discrete representations of the underlying probabilities. Even when risk-neutral volatilities are all higher than physical volatilities, it can be optimal to sell options on some assets while buying options on other assets, for which the positive hedging demand outweighs negative demand stemming from asset-specific returns.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2019.1925},
  journal      = {Operations Research},
  number       = {3},
  pages        = {ii-iv},
  shortjournal = {Oper. Res.},
  title        = {Technical Note—Options portfolio selection},
  volume       = {68},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Assortment optimization under the paired combinatorial logit
model. <em>OR</em>, <em>68</em>(3), ii–iv. (<a
href="https://doi.org/10.1287/opre.2019.1930">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider uncapacitated and capacitated assortment problems under the paired combinatorial logit model, where the goal is to find a set of products to offer to maximize the expected revenue obtained from a customer. In the uncapacitated setting, we can offer any set of products, whereas in the capacitated setting, there is an upper bound on the number of products that we can offer. We establish that even the uncapacitated assortment problem is strongly NP-hard. To develop an approximation framework for our assortment problems, we transform the assortment problem into an equivalent problem of finding the fixed point of a function, but computing the value of this function at any point requires solving a nonlinear integer program. Using a suitable linear programming relaxation of the nonlinear integer program and randomized rounding, we obtain a 0.6-approximation algorithm for the uncapacitated assortment problem. Using randomized rounding on a semidefinite programming relaxation, we obtain an improved 0.79-approximation algorithm, but the semidefinite programming relaxation can become difficult to solve in practice for large problem instances. Finally, using iterative rounding, we obtain a 0.25-approximation algorithm for the capacitated assortment problem. Our computational experiments on randomly generated problem instances demonstrate that our approximation algorithms, on average, yield expected revenues that are within 1.1\% of an efficiently computable upper bound on the optimal expected revenue.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2019.1930},
  journal      = {Operations Research},
  number       = {3},
  pages        = {ii-iv},
  shortjournal = {Oper. Res.},
  title        = {Assortment optimization under the paired combinatorial logit model},
  volume       = {68},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). An efficient frontier approach to scoring and ranking
hospital performance. <em>OR</em>, <em>68</em>(3), ii–iv. (<a
href="https://doi.org/10.1287/opre.2019.1972">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Centers for Medicare and Medicaid Services (CMS) star rating methodology for publicly evaluating hospitals uses a latent variable model that is based on the presumption of a single, but unobservable, hospital-specific quality factor shared across a group of performance measures. Performance measures are given higher weight if they statistically appear to be more strongly correlated with this hidden factor. We show how this approach, when applied to measures that are weakly or not correlated with each other, can effectively ignore measures and can exhibit “knife-edge” instability, so that even if hospitals improve relative to all other hospitals, they may nonetheless score lower overall because of weight shifting onto different measures than before. In contrast, we provide an approach to scoring and ranking hospitals that, under reasonable conditions, ensures that hospitals that improve relative to all other hospitals obtain higher scores, while also having the capability to autonomously adjust weights as measures are added or subtracted over time. Rather than exploit statistical correlation, we propose a conic optimization framework that offers a new integrated approach in data envelopment analysis for simultaneous efficiency analysis and performance evaluation. We develop theory that explains the behaviour of our approach, including various properties satisfied by hospital scores at optimality. Using data, we apply our approach to score and rank nearly every hospital in the United States and demonstrate the extent to which it agrees or disagrees with the existing approach to the CMS star ratings.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2019.1972},
  journal      = {Operations Research},
  number       = {3},
  pages        = {ii-iv},
  shortjournal = {Oper. Res.},
  title        = {An efficient frontier approach to scoring and ranking hospital performance},
  volume       = {68},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Sustainability planning for healthcare information exchanges
with supplier rebate program. <em>OR</em>, <em>68</em>(3), ii–iv. (<a
href="https://doi.org/10.1287/opre.2019.1912">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Healthcare information exchanges (HIEs) facilitate the electronic transfer of healthcare information among healthcare organizations. Several studies have shown that HIEs have the potential of saving billions of dollars per year and significantly improving healthcare delivery. Despite this economic benefit, financial sustainability is still a key challenge for many HIEs. Even though the issue of HIE sustainability is very important, it has not been analyzed rigorously. Hence the focus of our work is to understand the operational decisions of an HIE and provide insights into making rational decisions to ensure its long-term financial sustainability. Some recent studies argue that HIEs need to provide additional services in order to improve financial sustainability. In this context, some HIEs have started partnering with external suppliers for a supplier rebate program in which each member purchases services/supplies from specific suppliers via the network and receives a fixed percentage rebate. Our model is developed for one such HIE based in Austin, Texas (Critical Connection). Although we present our model in the context of Critical Connection, we ensure that our model is generic enough to apply in other HIE settings as well. We investigate a multiperiod two-service model in which an HIE offers (1) a healthcare information-sharing service and (2) a supplier rebate program. Our proposed model is based primarily on our interactions with a number of HIE providers in Texas. First, we present structural properties and an equilibrium solution for our model. Then, based on extensive computational experiments, we present several useful managerial insights for the HIE provider (for effectively managing its network) as well as policymakers (for improving the financial sustainability of HIEs and increasing the participation of healthcare organizations in HIE networks). We also analyze a variation of our model that allows healthcare providers to self-select the types of services.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2019.1912},
  journal      = {Operations Research},
  number       = {3},
  pages        = {ii-iv},
  shortjournal = {Oper. Res.},
  title        = {Sustainability planning for healthcare information exchanges with supplier rebate program},
  volume       = {68},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Optimal nonlinear pricing in social networks under
asymmetric network information. <em>OR</em>, <em>68</em>(3), ii–iv. (<a
href="https://doi.org/10.1287/opre.2019.1915">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study the optimal nonlinear pricing of products and services in social networks, in which customers are strategic and their consumption exhibits local externality. Customers know about their local network characteristics (which are positively affiliated across neighbors), but the selling firm only has knowledge of the global network. We develop a solution approach based on calculus of variations and positive neighbor affiliation to tackle this nonstandard principal–agent problem faced by the firm. We show that the optimal pricing compromises the capitalization of the susceptibility to neighbor consumption with the motivation of one’s own consumption, which gives rise to a menu of quantity premium or quantity discount. In the Erdös and Rényi graph (a special case of the social network model we use), we find that the pricing scheme does not screen network positions; consequently, the firm can offer a simple uniform price. We conduct robustness checks of our results with two-way connections, in which the firm-optimal consumption becomes linear in customer degree in the scale-free network. Compared with linear pricing, we show that nonlinear pricing allows the firm to respond more effectively to the changes of network topology and economic factors.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2019.1915},
  journal      = {Operations Research},
  number       = {3},
  pages        = {ii-iv},
  shortjournal = {Oper. Res.},
  title        = {Optimal nonlinear pricing in social networks under asymmetric network information},
  volume       = {68},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). An approximation algorithm for network revenue management
under nonstationary arrivals. <em>OR</em>, <em>68</em>(3), ii–iv. (<a
href="https://doi.org/10.1287/opre.2019.1931">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We provide an approximation algorithm for network revenue management problems. In our approximation algorithm, we construct an approximate policy using value function approximations that are expressed as linear combinations of basis functions. We use a backward recursion to compute the coefficients of the basis functions in the linear combinations. If each product uses at most L resources, then the total expected revenue obtained by our approximate policy is at least 1 / ( 1 + L ) of the optimal total expected revenue. In many network revenue management settings, although the number of resources and products can become large, the number of resources used by a product remains bounded. In this case, our approximate policy provides a constant-factor performance guarantee. Our approximate policy can handle nonstationarities in the customer arrival process. To our knowledge, our approximate policy is the first approximation algorithm for network revenue management problems under nonstationary arrivals. Our approach can incorporate the customer choice behavior among the products, and allows the products to use multiple units of a resource, while still maintaining the performance guarantee. In our computational experiments, we demonstrate that our approximate policy performs quite well, providing total expected revenues that are substantially better than its theoretical performance guarantee.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2019.1931},
  journal      = {Operations Research},
  number       = {3},
  pages        = {ii-iv},
  shortjournal = {Oper. Res.},
  title        = {An approximation algorithm for network revenue management under nonstationary arrivals},
  volume       = {68},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Adaptive submodular ranking and routing. <em>OR</em>,
<em>68</em>(3), ii–iv. (<a
href="https://doi.org/10.1287/opre.2019.1889">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study a general stochastic ranking problem in which an algorithm needs to adaptively select a sequence of elements so as to “cover” a random scenario (drawn from a known distribution) at minimum expected cost. The coverage of each scenario is captured by an individual submodular function, in which the scenario is said to be covered when its function value goes above a given threshold. We obtain a logarithmic factor approximation algorithm for this adaptive ranking problem, which is the best possible (unless P = NP ). This problem unifies and generalizes many previously studied problems with applications in search ranking and active learning. The approximation ratio of our algorithm either matches or improves the best result known in each of these special cases. Furthermore, we extend our results to an adaptive vehicle-routing problem, in which costs are determined by an underlying metric. This routing problem is a significant generalization of the previously studied adaptive traveling salesman and traveling repairman problems. Our approximation ratio nearly matches the best bound known for these special cases. Finally, we present experimental results for some applications of adaptive ranking.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2019.1889},
  journal      = {Operations Research},
  number       = {3},
  pages        = {ii-iv},
  shortjournal = {Oper. Res.},
  title        = {Adaptive submodular ranking and routing},
  volume       = {68},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). The benders dual decomposition method. <em>OR</em>,
<em>68</em>(3), ii–iv. (<a
href="https://doi.org/10.1287/opre.2019.1892">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many methods that have been proposed to solve large-scale mixed integer linear programing (MILP) problems rely on decomposition techniques. These methods exploit either the primal or the dual structure of the problem, yielding the Benders decomposition or Lagrangian dual decomposition methods. We propose a new and high-performance approach, called Benders dual decomposition (BDD), which combines the complementary advantages of both methods. The development of BDD is based on a specific reformulation of the Benders subproblem, where local copies of the master variables are introduced in the proposed subproblem formulation and then priced out into the objective function. We show that this method (i) generates stronger feasibility and optimality cuts compared with the classical Benders method, (ii) can converge to the optimal integer solution at the root node of the Benders master problem, and (iii) is capable of generating high-quality incumbent solutions at the early iterations of the algorithm. We report encouraging computational results on various benchmark MILP problems.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2019.1892},
  journal      = {Operations Research},
  number       = {3},
  pages        = {ii-iv},
  shortjournal = {Oper. Res.},
  title        = {The benders dual decomposition method},
  volume       = {68},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Near-optimal disjoint-path facility location through set
cover by pairs. <em>OR</em>, <em>68</em>(3), ii–iv. (<a
href="https://doi.org/10.1287/opre.2019.1956">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we consider two special cases of the “cover-by-pairs” optimization problem that arises when we need to place facilities so that each customer is served by two facilities that reach it by disjoint shortest paths. These problems arise in a network traffic-monitoring scheme proposed by Breslau et al. and have potential applications to content distribution. The “set-disjoint” variant applies to networks that use the open shortest path first routing protocol, and the “path-disjoint” variant applies when multiprotocol label switching routing is enabled, making better solutions possible at the cost of greater operational expense. Although we can prove that no polynomial-time algorithm can guarantee good solutions for either version, we are able to provide heuristics that do very well in practice on instances with real-world network structure. Fast implementations of the heuristics, made possible by exploiting mathematical observations about the relationship between the network instances and the corresponding instances of the cover-by-pairs problem, allow us to perform an extensive experimental evaluation of the heuristics and what the solutions they produce tell us about the effectiveness of the proposed monitoring scheme. For the set-disjoint variant, we validate our claim of near-optimality via a new lower-bounding integer programming formulation. Although computing this lower bound requires solving the NP-hard hitting set problem and can underestimate the optimal value by a linear factor in the worst case, it can be computed quickly by CPLEX, and it equals the optimal solution value for all the instances in our extensive test bed.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2019.1956},
  journal      = {Operations Research},
  number       = {3},
  pages        = {ii-iv},
  shortjournal = {Oper. Res.},
  title        = {Near-optimal disjoint-path facility location through set cover by pairs},
  volume       = {68},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Deadlines, offer timing, and the search for alternatives.
<em>OR</em>, <em>68</em>(3), ii–iv. (<a
href="https://doi.org/10.1287/opre.2019.1895">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We model two agents who can benefit from a mutual deal or partnership, yet are also searching for outside alternatives. This generic situation is observed in various settings (e.g., the job market for experts) and involves several decisions. The proposer decides not only on the timing, deadline, and value of her offer but also on how to handle her outside alternatives; the responder decides whether to accept the proposer’s offer (if any) and how to handle his own outside alternatives. A responder holding an offer with a long deadline becomes more selective when evaluating outside alternatives, an effect we call acceptance deterrence . The strength of this effect depends on the information structure of the game as well as on the ways in which the proposer and the responder can interact. A practical prescription for the proposer is to use exploding offers in the settings where acceptance deterrence is weak (e.g., stationary search, search with recall). In contrast, longer deadlines are beneficial in the settings where acceptance deterrence is strong (e.g., perfect information).},
  archive      = {J_OR},
  doi          = {10.1287/opre.2019.1895},
  journal      = {Operations Research},
  number       = {3},
  pages        = {ii-iv},
  shortjournal = {Oper. Res.},
  title        = {Deadlines, offer timing, and the search for alternatives},
  volume       = {68},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Computation of exact bootstrap confidence intervals:
Complexity and deterministic algorithms. <em>OR</em>, <em>68</em>(3),
ii–iv. (<a href="https://doi.org/10.1287/opre.2019.1904">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The bootstrap is a nonparametric approach for calculating quantities, such as confidence intervals, directly from data. Since calculating exact bootstrap quantities is believed to be intractable, randomized resampling algorithms are traditionally used. In this paper, we present a new perspective on the bootstrap method through the lens of counting integer points in polyhedra. Through this new perspective, we make several advances for the bootstrap method, both theoretically and algorithmically. First, we establish several computational complexity results for the exact bootstrap method in the case of the sample mean. Second, we present the first efficient deterministic approximation algorithm (fully polynomial time approximation scheme) for producing exact bootstrap confidence intervals which, unlike traditional methods, has guaranteed bounds on the approximation error. Third, we develop a simple exact algorithm for exact bootstrap confidence intervals based on polynomial multiplication. We provide empirical evidence on real and synthetic data sets with several hundreds of data points that the proposed deterministic algorithms can quickly produce confidence intervals that are substantially more accurate than those from randomized methods, and thus are practical alternatives in applications such as clinical trials.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2019.1904},
  journal      = {Operations Research},
  number       = {3},
  pages        = {ii-iv},
  shortjournal = {Oper. Res.},
  title        = {Computation of exact bootstrap confidence intervals: Complexity and deterministic algorithms},
  volume       = {68},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Confidence intervals for data-driven inventory policies with
demand censoring. <em>OR</em>, <em>68</em>(2), iii–vi. (<a
href="https://doi.org/10.1287/opre.2019.1883">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We revisit the classical dynamic inventory management problem of Scarf [Scarf H (1959b) The optimality of ( s , S ) policies in the dynamic inventory problem. Arrow KJ, Karlin S, Suppes P, eds. Mathematical Methods in the Social Science (Stanford University Press, Stanford, CA), 196–202.] from the perspective of a decision maker who has n historical selling seasons of data and must make ordering decisions for the upcoming season. We develop a nonparametric estimation procedure for the (S, s) policy that is consistent and then characterize the finite sample properties of the estimated (S, s) levels by deriving their asymptotic confidence intervals. We also consider having at least some of the past selling seasons of data censored from the absence of backlogging and show that the intuitive procedure of first correcting for censoring in the demand data yields inconsistent estimates. We then show how to correctly use the censored data to obtain consistent estimates and derive asymptotic confidence intervals for this policy using Stein’s method. We further show the confidence intervals can be used to effectively bound the difference between the expected total cost of an estimated policy and that of the optimal policy. We validate our results with extensive computations on simulated data. Our results extend to the repeated newsvendor problem and the base stock policy problem by appropriate parameter choices.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2019.1883},
  journal      = {Operations Research},
  number       = {2},
  pages        = {iii-vi},
  shortjournal = {Oper. Res.},
  title        = {Confidence intervals for data-driven inventory policies with demand censoring},
  volume       = {68},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). An optimal callback policy for general arrival processes: A
pathwise analysis. <em>OR</em>, <em>68</em>(2), iii–vi. (<a
href="https://doi.org/10.1287/opre.2019.1884">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper studies the callback option as an instrument to effectively mitigate congestion due to temporary surges in arrivals to a call center. The call arrival process can be an arbitrary point process, allowing uncertainty and temporary surges in the arrival rate, provided that the system is stable. When a customer arrives, the call center manager examines the system state and decides whether to offer the incoming customer the callback option. When the callback option is offered, the customer decides whether to accept the offer. The customer is routed to the offline queue (to be called back later) only if he is offered the callback option and accepts it. Otherwise, he is routed to the online queue. For each customer in the online queue, the call center manager incurs a waiting cost of h per time unit. Similarly, whenever she routes a customer to the offline queue (for a callback later), she incurs a one-time penalty of p . Initially, we allow complete foresight policies that look into the entire future. We show that a simple lookahead policy that looks into the future arrivals and service completion times for the next p / h time units and uses the current number of customers in the system who previously rejected a callback offer (but does not look into the accept/reject decisions of future customers) is pathwise optimal. Building on the insights gleaned from the optimal lookahead policies, we also propose a nonanticipating (and implementable) policy by interpreting the lookahead policy in the fluid model and show that it is optimal in the fluid model. In particular, we show that this policy reduces to the so-called line policy, if the arrival rate process follows a Cox-Ingersoll-Ross process. Last, we conduct a simulation study, which shows that the proposed policies perform well.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2019.1884},
  journal      = {Operations Research},
  number       = {2},
  pages        = {iii-vi},
  shortjournal = {Oper. Res.},
  title        = {An optimal callback policy for general arrival processes: A pathwise analysis},
  volume       = {68},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Mitigating interdiction risk with fortification.
<em>OR</em>, <em>68</em>(2), iii–vi. (<a
href="https://doi.org/10.1287/opre.2019.1890">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study a network fortification problem on a directed network that channels single-commodity resources to fulfill random demands delivered to a subset of the nodes. For given a realization of demands, the malicious interdictor would disrupt the network in a manner that would maximize the total demand shortfalls subject to the interdictor’s constraints. To mitigate the risk of such shortfalls, a network’s operator can fortify it by providing additional network capacity and/or protecting the nominal capacity. Given the stochastic nature of the demand uncertainty, the goal is to fortify the network, within the operator’s budget constraint, to minimize the expected disutility of the shortfalls in events of interdiction. We model this as a three-level, nonlinear stochastic optimization problem that can be solved via a robust stochastic approximation approach under which each iteration involves solving a linear mixed-integer program. We provide favorable computational results that demonstrate how our fortification strategy effectively mitigates interdiction risks. We also extend the model to multicommodity network with multiple sources and multiple sinks.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2019.1890},
  journal      = {Operations Research},
  number       = {2},
  pages        = {iii-vi},
  shortjournal = {Oper. Res.},
  title        = {Mitigating interdiction risk with fortification},
  volume       = {68},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Estimating uncertainties using judgmental forecasts with
expert heterogeneity. <em>OR</em>, <em>68</em>(2), iii–vi. (<a
href="https://doi.org/10.1287/opre.2019.1938">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we develop a new characterization of multiple-point forecasts provided by experts and use it in an optimization framework to deduce actionable signals, including the mean, standard deviation, or a combination of the two for underlying probability distributions. This framework consists of three steps: (1) calibrate experts’ point forecasts using historical data to determine which quantile they provide, on average, when asked for forecasts, (2) quantify the precision in the experts’ forecasts around their average quantile, and (3) use this calibration information in an optimization framework to deduce the signals of interest. We also show that precision and accuracy in expert judgments are complementary in terms of their informativeness. We also discuss implementation of the development and the realized benefits at a large government project in the agribusiness domain.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2019.1938},
  journal      = {Operations Research},
  number       = {2},
  pages        = {iii-vi},
  shortjournal = {Oper. Res.},
  title        = {Estimating uncertainties using judgmental forecasts with expert heterogeneity},
  volume       = {68},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A generalized black–litterman model. <em>OR</em>,
<em>68</em>(2), iii–vi. (<a
href="https://doi.org/10.1287/opre.2019.1893">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Black–Litterman model provides a framework for combining the forecasts of a backward-looking equilibrium model with the views of (several) forward-looking experts in a portfolio allocation decision. The classical version uses the capital asset pricing model to specify expected returns, and assumes that expert views are unbiased noisy observations of future returns. It combines the two using Bayes’ rule and the portfolio allocation decision is made on the basis of the updated forecast. The classical Black–Litterman model assumes that the equilibrium and expert models are accurately specified. This is generally not the case, however, and there may be substantial efficiency loss if misspecification is ignored. In this paper, we formulate a generalized Black–Litterman model that accounts for both misspecification and bias in the equilibrium and expert models. We show how to calibrate this model using historical view and return data, and study the value of our generalized model for portfolio construction. More generally, this paper shows how the views of multiple experts can be modeled as a Bayesian graphical model and estimated using historical data, which may be of interest in applications that involve the aggregation of expert opinions for the purpose of decision making.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2019.1893},
  journal      = {Operations Research},
  number       = {2},
  pages        = {iii-vi},
  shortjournal = {Oper. Res.},
  title        = {A generalized Black–Litterman model},
  volume       = {68},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). When is selfish routing bad? The price of anarchy in light
and heavy traffic. <em>OR</em>, <em>68</em>(2), iii–vi. (<a
href="https://doi.org/10.1287/opre.2019.1894">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper examines the behavior of the price of anarchy as a function of the traffic inflow in nonatomic congestion games with multiple origin/destination (O/D) pairs. Empirical studies in real-world networks show that the price of anarchy is close to 1 in both light and heavy traffic, thus raising the following question: can these observations be justified theoretically? We first show that this is not always the case: the price of anarchy may remain a positive distance away from 1 for all values of the traffic inflow, even in simple three-link networks with a single O/D pair and smooth, convex costs. On the other hand, for a large class of cost functions (including all polynomials) and inflow patterns, the price of anarchy does converge to 1 in both heavy and light traffic, irrespective of the network topology and the number of O/D pairs in the network. We also examine the rate of convergence of the price of anarchy, and we show that it follows a power law whose degree can be computed explicitly when the network’s cost functions are polynomials.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2019.1894},
  journal      = {Operations Research},
  number       = {2},
  pages        = {iii-vi},
  shortjournal = {Oper. Res.},
  title        = {When is selfish routing bad? the price of anarchy in light and heavy traffic},
  volume       = {68},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Knowledge sharing and learning among smallholders in
developing economies: Implications, incentives, and reward mechanisms.
<em>OR</em>, <em>68</em>(2), iii–vi. (<a
href="https://doi.org/10.1287/opre.2019.1869">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In developing economies, smallholders apply their own specialized knowledge and exert costly effort to manage their farms. To raise overall productivity, NGOs and governments are advocating various knowledge-sharing and learning platforms for farmers to exchange a variety of farming techniques. Putting altruism aside, we examine the overall economic implications for heterogeneous farmers sharing their private knowledge voluntarily with others under (implicit) competition. By analyzing a multiperson sequential game, we find that farmers with high knowledge are reluctant to share knowledge, and consequently, the voluntary shared level is always lower than or equal to the “efficient” shared level that maximizes farmer welfare under coordination. This finding is motivational in developing a reward mechanism to entice farmers to elevate their knowledge shared level in a decentralized system so as to maximize farmer welfare. On reviewing different mechanisms, we propose a quota-based reward mechanism that can entice farmers to share knowledge voluntarily up to the efficient shared level.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2019.1869},
  journal      = {Operations Research},
  number       = {2},
  pages        = {iii-vi},
  shortjournal = {Oper. Res.},
  title        = {Knowledge sharing and learning among smallholders in developing economies: Implications, incentives, and reward mechanisms},
  volume       = {68},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Pricing and assortment strategies with product exchanges.
<em>OR</em>, <em>68</em>(2), iii–vi. (<a
href="https://doi.org/10.1287/opre.2019.1871">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Lenient return policies enable consumers to return or exchange products they are unsatisfied with, which boosts sales. Unfortunately, they also increase retailer costs. We develop a search framework where consumers sequentially learn about products’ true value and evaluate whether to keep, exchange, or return them. Our formulation results in a tractable attraction demand model that can be used for optimization. We show that when pricing is not a decision, the assortment problem does not have a simple structure, but we provide an approximation algorithm to solve it. When prices and assortment can be controlled, the optimization becomes tractable: product prices can either be set so that potential return costs are added to the product price, be reduced to ensure that consumers choose to evaluate them after an exchange, or be set so high so that the items are effectively excluded from the assortment. We find that when prices and assortment can be jointly optimized, assortment size always increases when consumers pay a higher share of the return cost. Finally, retailers prefer to pass all return costs on to the consumers, which not only improves social welfare but also can raise consumer surplus.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2019.1871},
  journal      = {Operations Research},
  number       = {2},
  pages        = {iii-vi},
  shortjournal = {Oper. Res.},
  title        = {Pricing and assortment strategies with product exchanges},
  volume       = {68},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Assignment mechanisms under distributional constraints.
<em>OR</em>, <em>68</em>(2), iii–vi. (<a
href="https://doi.org/10.1287/opre.2019.1887">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We generalize the serial dictatorship (SD) and probabilistic serial (PS) mechanism for assigning indivisible objects (seats in a school) to agents (students) to accommodate distributional constraints. Such constraints are motivated by equity considerations. Our generalization of SD maintains several of its desirable properties, including strategyproofness, Pareto optimality, and computational tractability, while satisfying the distributional constraints with a small error. Our generalization of the PS mechanism finds an ordinally efficient and envy-free assignment while satisfying the distributional constraint with a small error. We show, however, that no ordinally efficient and envy-free mechanism is also weakly strategyproof. Both of our algorithms assign at least the same number of students as the optimum fractional assignment.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2019.1887},
  journal      = {Operations Research},
  number       = {2},
  pages        = {iii-vi},
  shortjournal = {Oper. Res.},
  title        = {Assignment mechanisms under distributional constraints},
  volume       = {68},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Optimal pricing in markets with nonconvex costs.
<em>OR</em>, <em>68</em>(2), iii–vi. (<a
href="https://doi.org/10.1287/opre.2019.1900">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider a market run by an operator who seeks to satisfy a given consumer demand for a commodity by purchasing the needed amount from a group of competing suppliers with nonconvex cost functions. The operator knows the suppliers’ cost functions and announces a price/payment function for each supplier, which determines the payment to that supplier for producing different quantities. Each supplier then makes an individual decision about how much to produce, in order to maximize its own profit. The key question is how to design the price functions. To that end, we propose a new pricing scheme, which is applicable to general nonconvex costs, and allows using general parametric pricing functions. Optimizing for the quantities and the price parameters simultaneously, and the ability to use general parametric pricing functions allows our scheme to find prices that are typically economically more efficient and less discriminatory than those of the existing schemes. In addition, we supplement the proposed method with a polynomial-time approximation algorithm, which can be used to approximate the optimal quantities and prices. Our framework extends to the case of networked markets, which, to the best of our knowledge, has not been considered in previous work.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2019.1900},
  journal      = {Operations Research},
  number       = {2},
  pages        = {iii-vi},
  shortjournal = {Oper. Res.},
  title        = {Optimal pricing in markets with nonconvex costs},
  volume       = {68},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Optimal signaling of content accuracy: Engagement
vs. misinformation. <em>OR</em>, <em>68</em>(2), iii–vi. (<a
href="https://doi.org/10.1287/opre.2019.1897">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper studies information design in social networks. We consider a setting, where agents’ actions exhibit positive local network externalities. There is uncertainty about the underlying state of the world, which impacts agents’ payoffs. The platform can commit to a signaling mechanism that sends informative signals to agents upon realization of this uncertainty, thereby influencing their actions. Although this abstract setting has many applications, we discuss our results in the context of a specific one: A platform can send informative signals to agents in a social network to influence their engagement decisions with the available content, based on the realization of the inaccuracy of the content. We investigate how the platform should design its signaling mechanism to maximize engagement/minimize misinformation. The optimal (in terms of engagement/misinformation) signaling mechanism admits a simple threshold structure: The platform recommends that agents “engage” with the content if its inaccuracy level is below a threshold and recommends “do not engage” otherwise. For the mechanism that maximizes engagement, these thresholds depend on agents’ network positions, which we capture through a novel centrality measure. In the case where the platform seeks only to minimize misinformation (regardless of the induced engagement), common threshold mechanisms with identical thresholds across agents are optimal. This is in contrast to the engagement maximization setting, where when agents are heterogeneous in terms of their network positions, common threshold mechanisms induce substantially lower engagement than the optimal mechanisms. We also study the frontier of the engagement/misinformation levels that can be achieved via different mechanisms and characterize when common threshold mechanisms achieve optimal trade-offs. Finally, we supplement our theoretical findings with numerical simulations on a Facebook subgraph.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2019.1897},
  journal      = {Operations Research},
  number       = {2},
  pages        = {iii-vi},
  shortjournal = {Oper. Res.},
  title        = {Optimal signaling of content accuracy: Engagement vs. misinformation},
  volume       = {68},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Bicriteria approximation of chance-constrained covering
problems. <em>OR</em>, <em>68</em>(2), iii–vi. (<a
href="https://doi.org/10.1287/opre.2019.1866">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A chance-constrained optimization problem involves constraints with random data that can be violated with probability bounded from above by a prespecified small risk parameter. Such constraints are used to model reliability requirements in a variety of application areas, such as finance, energy, service, and manufacturing. Except under very special conditions, chance-constrained problems are extremely difficult. There has been a great deal of elegant work on developing tractable approximations of chance constraints. Unfortunately, none of these approaches comes with a constant factor approximation guarantee. We show that such a guarantee is impossible by proving an inapproximability result. By contrast, for a large class of chance-constrained covering problems, we propose a bicriteria approximation scheme. Our scheme finds a solution whose violation probability may be larger than but is within a constant factor of the specified risk parameter and whose objective value is within a constant factor of the true optimal value. Key to our developments is the construction of a tractable convex relaxation of a chance-constrained problem and an appropriate scaling of a solution to this relaxation. We extend our approximation results to the setting in which the underlying distribution of the constraint data is not known. That is, we consider distributionally robust chance-constrained covering problems with convex moment and Wasserstein ambiguity sets and provide bicriteria approximation results.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2019.1866},
  journal      = {Operations Research},
  number       = {2},
  pages        = {iii-vi},
  shortjournal = {Oper. Res.},
  title        = {Bicriteria approximation of chance-constrained covering problems},
  volume       = {68},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Multiechelon lot sizing: New complexities and inequalities.
<em>OR</em>, <em>68</em>(2), iii–vi. (<a
href="https://doi.org/10.1287/opre.2019.1867">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study a multiechelon lot-sizing problem for a serial supply chain that consists of a production level and several transportation levels, where the demands can exist in the production echelon as well as in any transportation echelons. With the presence of stationary production capacity and general cost functions, our model integrates production, inventory, and transportation decisions and generalizes existing literature on many multiechelon lot-sizing models. First, we answer an open question in the literature by showing that multiechelon lot sizing with intermediate demands (MLS) is NP-hard. Second, we develop polynomial time algorithms for both uncapacitated and capacitated MLS with a fixed number of echelons. The run times of our algorithms improve on those of many known algorithms for different MLS models. Third, we present families of valid inequalities for MLS that generalize known inequalities. For the uncapacitated case, we develop a polynomial-time separation algorithm and efficient separation heuristics. Finally, we demonstrate the effectiveness of a branch-and-cut algorithm using proposed inequalities to solve large multi-item MLS problems.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2019.1867},
  journal      = {Operations Research},
  number       = {2},
  pages        = {iii-vi},
  shortjournal = {Oper. Res.},
  title        = {Multiechelon lot sizing: New complexities and inequalities},
  volume       = {68},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Fast or slow: Search in discrete locations with two search
modes. <em>OR</em>, <em>68</em>(2), iii–vi. (<a
href="https://doi.org/10.1287/opre.2019.1870">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {An object is hidden in one of several discrete locations according to some known probability distribution, and the goal is to discover the object in the minimum expected time by successive searches of individual locations. If there is only one way to search each location, this search problem is solved using Gittins indices. Motivated by modern search technology, we extend earlier work to allow two modes—fast and slow—to search each location. The fast mode takes less time, but the slow mode is more likely to find the object. An optimal policy is difficult to obtain in general, because it requires an optimal sequence of search modes for each location in addition to a set of sequence-dependent Gittins indices for choosing between locations. Our analysis begins by—for each mode—identifying a sufficient condition for a location to use only that search mode in an optimal policy. For locations meeting neither sufficient condition, an optimal choice of search mode is extremely complicated, depending on both the probability distribution of the object’s hiding location and the search parameters of the other locations. We propose several heuristic policies motivated by our analysis and demonstrate their near-optimal performance in an extensive numerical study.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2019.1870},
  journal      = {Operations Research},
  number       = {2},
  pages        = {iii-vi},
  shortjournal = {Oper. Res.},
  title        = {Fast or slow: Search in discrete locations with two search modes},
  volume       = {68},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A primal–dual lifting scheme for two-stage robust
optimization. <em>OR</em>, <em>68</em>(2), iii–vi. (<a
href="https://doi.org/10.1287/opre.2019.1873">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Two-stage robust optimization problems, in which decisions are taken both in anticipation of and in response to the observation of an unknown parameter vector from within an uncertainty set, are notoriously challenging. In this paper, we develop convergent hierarchies of primal (conservative) and dual (progressive) bounds for these problems that trade off the competing goals of tractability and optimality: Although the coarsest bounds recover a tractable but suboptimal affine decision rule approximation of the two-stage robust optimization problem, the refined bounds lift extreme points of the uncertainty set until an exact but intractable extreme point reformulation of the problem is obtained. Based on these bounds, we propose a primal–dual lifting scheme for the solution of two-stage robust optimization problems that accommodates for discrete, here-and-now decisions, infeasible problem instances, and the absence of a relatively complete recourse. The incumbent solutions in each step of our algorithm afford rigorous error bounds, and they can be interpreted as piecewise affine decision rules. We illustrate the performance of our algorithm on illustrative examples and on an inventory management problem.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2019.1873},
  journal      = {Operations Research},
  number       = {2},
  pages        = {iii-vi},
  shortjournal = {Oper. Res.},
  title        = {A Primal–Dual lifting scheme for two-stage robust optimization},
  volume       = {68},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Allocation of intensive care unit beds in periods of high
demand. <em>OR</em>, <em>68</em>(2), iii–vi. (<a
href="https://doi.org/10.1287/opre.2019.1876">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The objective of this paper is to use mathematical modeling and analysis to develop insights into and policies for making bed allocation decisions in an intensive care unit (ICU) of a hospital during periods when patient demand is high. We first develop a stylized mathematical model in which patients’ health conditions change over time according to a Markov chain. In this model, each patient is in one of two possible health stages, one representing the critical and the other representing the highly critical health stage. The ICU has limited bed availability and therefore when a patient arrives and no beds are available, a decision needs to be made as to whether the patient should be admitted to the ICU and if so, which patient in the ICU should be transferred to the general ward. With the objective of minimizing the long-run average mortality rate, we provide analytical characterizations of the optimal policy under certain conditions. Then, based on these analytical results, we propose heuristic methods, which can be used under assumptions that are more general than what is assumed for the mathematical model. Finally, we demonstrate that the proposed heuristic methods work well by a simulation study, which relaxes some of the restrictive assumptions of the mathematical model by considering a more complex transition structure for patient health and allowing for patients to be possibly queued for admission to the ICU and readmitted from the general ward after they are discharged.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2019.1876},
  journal      = {Operations Research},
  number       = {2},
  pages        = {iii-vi},
  shortjournal = {Oper. Res.},
  title        = {Allocation of intensive care unit beds in periods of high demand},
  volume       = {68},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Submodularity in conic quadratic mixed 0–1 optimization.
<em>OR</em>, <em>68</em>(2), iii–vi. (<a
href="https://doi.org/10.1287/opre.2019.1888">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We describe strong convex valid inequalities for conic quadratic mixed 0–1 optimization. These inequalities can be utilized for solving numerous practical nonlinear discrete optimization problems from value-at-risk minimization to queueing system design, from robust interdiction to assortment optimization through appropriate conic quadratic mixed 0–1 relaxations. The inequalities exploit the submodularity of the binary restrictions and are based on the polymatroid inequalities over binaries for the diagonal case. We prove that the convex inequalities completely describe the convex hull of a single conic quadratic constraint as well as the rotated cone constraint over binary variables and unbounded continuous variables. We then generalize and strengthen the inequalities by incorporating additional constraints of the optimization problem. Computational experiments on mean-risk optimization with correlations, assortment optimization, and robust conic quadratic optimization indicate that the new inequalities strengthen the convex relaxations substantially and lead to significant performance improvements.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2019.1888},
  journal      = {Operations Research},
  number       = {2},
  pages        = {iii-vi},
  shortjournal = {Oper. Res.},
  title        = {Submodularity in conic quadratic mixed 0–1 optimization},
  volume       = {68},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). On the taylor expansion of value functions. <em>OR</em>,
<em>68</em>(2), iii–vi. (<a
href="https://doi.org/10.1287/opre.2019.1903">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce a framework for approximate dynamic programming that we apply to discrete-time chains on ℤ d + ℤ+d with countable action sets. The framework is grounded in the approximation of the (controlled) chain’s generator by that of another Markov process. In simple terms, our approach stipulates applying a second-order Taylor expansion to the value function, replacing the Bellman equation with one in continuous space and time in which the transition matrix is reduced to its first and second moments. In some cases, the resulting equation can be interpreted as a Hamilton–Jacobi–Bellman equation for a Brownian control problem. When tractable, the “Taylored” equation serves as a useful modeling tool. More generally, it is a starting point for approximation algorithms. We develop bounds on the optimality gap—the suboptimality introduced by using the control produced by the Taylored equation. These bounds can be viewed as a conceptual underpinning, analytical rather than relying on weak convergence arguments, for the good performance of controls derived from Brownian approximations. We prove that under suitable conditions and for suitably “large” initial states, (1) the optimality gap is smaller than a 1 – α fraction of the optimal value, with which α ∈ (0, 1) is the discount factor, and (2) the gap can be further expressed as the infinite-horizon discounted value with a “lower-order” per-period reward. Computationally, our framework leads to an “aggregation” approach with performance guarantees. Although the guarantees are grounded in partial differential equation theory, the practical use of this approach requires no knowledge of that theory.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2019.1903},
  journal      = {Operations Research},
  number       = {2},
  pages        = {iii-vi},
  shortjournal = {Oper. Res.},
  title        = {On the taylor expansion of value functions},
  volume       = {68},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Modeling dependent outages of electric power plants.
<em>OR</em>, <em>68</em>(1), 1–307. (<a
href="https://doi.org/10.1287/opre.2019.1952">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a framework to model dependence of outages of electric power plants. Our framework allows for common factors, such as weather events and fuel shortages, to drive outages. We calibrate our model for power plants in the Electric Reliability Council of Texas and the Western Electricity Coordinating Council regions using a unique data set of actual outages from the North American Electric Reliability Corporation. We find strong evidence of dependence in power plant outages based on the input fuel of the plants and illustrate how our framework can be used to evaluate the reliability of the supply of electricity for both regions, and also the impact on reliability of building additional capacity.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2019.1952},
  journal      = {Operations Research},
  number       = {1},
  pages        = {1-307},
  shortjournal = {Oper. Res.},
  title        = {Modeling dependent outages of electric power plants},
  volume       = {68},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Ignorance is almost bliss: Near-optimal stochastic matching
with few queries. <em>OR</em>, <em>68</em>(1), 1–307. (<a
href="https://doi.org/10.1287/opre.2019.1856">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study the stochastic matching problem with the goal of finding a maximum matching in a graph whose edges are unknown but can be accessed via queries. This is a special case of stochastic k -cycle packing, in which the problem is to find a maximum packing of cycles, each of which exists with some probability. We provide polynomial-time adaptive and nonadaptive algorithms that provably yield a near-optimal solution, using a number of edge queries that is linear in the number of vertices. We are especially interested in kidney exchange, with which pairs of patients with end-stage renal failure and their willing but incompatible donors participate in a mechanism that performs compatibility tests between patients and donors and swaps the donors of some patients so that a large number of patients receive compatible kidneys. Because of the significant cost of performing compatibility tests, currently, kidney exchange programs perform at most one compatibility test per patient. Our theoretical results applied to kidney exchange show that, by increasing the number of compatibility tests performed per patient from one to a larger constant, we effectively get the full benefit of exhaustive testing at a fraction of the cost. We show, on both generated and real data from the UNOS nationwide kidney exchange, that even a small number of nonadaptive edge queries per vertex results in large gains in expected successful matches.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2019.1856},
  journal      = {Operations Research},
  number       = {1},
  pages        = {1-307},
  shortjournal = {Oper. Res.},
  title        = {Ignorance is almost bliss: Near-optimal stochastic matching with few queries},
  volume       = {68},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). On the inefficiency of forward markets in leader–follower
competition. <em>OR</em>, <em>68</em>(1), 1–307. (<a
href="https://doi.org/10.1287/opre.2019.1863">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Motivated by electricity markets, this paper studies the impact of forward contracting in situations where firms have capacity constraints and heterogeneous production lead times. We consider a model with two types of firms—leaders and followers—that choose production at two different times. Followers choose productions in the second stage but can sell forward contracts in the first stage. Our main result is an explicit characterization of the equilibrium outcomes. Classic results on forward contracting suggest that it can mitigate market power in simple settings; however, the results in this paper show that the impact of forward markets in this setting is delicate—forward contracting can enhance or mitigate market power. In particular, our results show that leader–follower interactions created by heterogeneous production lead times may cause forward markets to be inefficient, even when there are a large number of followers. In fact, symmetric equilibria do not necessarily exist due to differences in market power among the leaders and followers.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2019.1863},
  journal      = {Operations Research},
  number       = {1},
  pages        = {1-307},
  shortjournal = {Oper. Res.},
  title        = {On the inefficiency of forward markets in Leader–Follower competition},
  volume       = {68},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Hidden hamiltonian cycle recovery via linear programming.
<em>OR</em>, <em>68</em>(1), 1–307. (<a
href="https://doi.org/10.1287/opre.2019.1886">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce the problem of hidden Hamiltonian cycle recovery, where there is an unknown Hamiltonian cycle in an n -vertex complete graph that needs to be inferred from noisy edge measurements. The measurements are independent and distributed according to P n for edges in the cycle and Q n otherwise. This formulation is motivated by a problem in genome assembly, where the goal is to order a set of contigs (genome subsequences) according to their positions on the genome using long-range linking measurements between the contigs. Computing the maximum likelihood estimate in this model reduces to a traveling salesman problem (TSP). Despite the NP-hardness of TSP, we show that a simple linear programming (LP) relaxation—namely, the fractional 2-factor (F2F) LP—recovers the hidden Hamiltonian cycle with high probability as n → ∞ n→∞ provided that α n − log n → ∞ αn−log n→∞ , where α n ≜ − 2 log ∫ √ d P n d Q n αn≜−2log∫dPndQn is the Rényi divergence of order 1 2 12 . This condition is information-theoretically optimal in the sense that, under mild distributional assumptions, α n ≥ ( 1 + o ( 1 ) ) log n αn≥(1+o(1))log n is necessary for any algorithm to succeed regardless of the computational cost. Departing from the usual proof techniques based on dual witness construction, the analysis relies on the combinatorial characterization (in particular, the half-integrality) of the extreme points of the F2F polytope. Represented as bicolored multigraphs, these extreme points are further decomposed into simpler “blossom-type” structures for the large deviation analysis and counting arguments. Evaluation of the algorithm on real data shows improvements over existing approaches.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2019.1886},
  journal      = {Operations Research},
  number       = {1},
  pages        = {1-307},
  shortjournal = {Oper. Res.},
  title        = {Hidden hamiltonian cycle recovery via linear programming},
  volume       = {68},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Can public reporting cure healthcare? The role of quality
transparency in improving patient–provider alignment. <em>OR</em>,
<em>68</em>(1), 1–307. (<a
href="https://doi.org/10.1287/opre.2019.1868">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Public reporting of medical treatment outcomes is being widely adopted by policymakers in an effort to increase quality transparency and improve alignment between patient choices and provider capabilities. We examine the soundness of this approach by studying the effects of quality transparency on patient choices, hospital investments, societal outcomes (e.g., patients’ social welfare and inequality), and the healthcare market structure (e.g., medical or geographical specialization). Our results offer insights into why previous public reporting efforts have been less than fully successful and suggest ways in which future efforts can be more effective. Specifically, our analytical and simulation results calibrated with empirical data from the Centers for Medicare and Medicaid Services reveal that increasing quality transparency promotes increased medical specialization, results in decreased geographical specialization, and induces hospitals to invest in their strengths rather than their weakness. Furthermore, increasing quality transparency in the short-term typically improves social welfare and reduces inequality among patients. In the long-term, however, we find that increasing transparency can decrease social welfare, and fail to yield socially optimal outcomes, even under full transparency. Hence, a policymaker concerned with societal outcomes should accompany increasing quality transparency with other policies that correct the allocation of patients to hospitals. Among these, we find that policies that incentivize hospitals are generally more effective than policies that incentivize patients. Finally, our results indicate that, to achieve maximal benefits from increasing quality transparency, policymakers should target younger, more affluent, or urban (i.e., high hospital density area) patients, or those requiring nonemergency treatment.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2019.1868},
  journal      = {Operations Research},
  number       = {1},
  pages        = {1-307},
  shortjournal = {Oper. Res.},
  title        = {Can public reporting cure healthcare? the role of quality transparency in improving Patient–Provider alignment},
  volume       = {68},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Dynamic leveraging–deleveraging games. <em>OR</em>,
<em>68</em>(1), 1–307. (<a
href="https://doi.org/10.1287/opre.2019.1865">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce a new mechanism for leverage dynamics, based on a multiperiod game of lenders with differentiated beliefs about the firm’s fundamental returns. The game features strategic substitutability for low existing leverage and strategic complementarity for high existing leverage. The resulting leverage process exhibits a mean-reverting regime around a long-run level, as long as it stays below an instability level. Above the instability level, leverage becomes explosive. We validate our model empirically using aggregate returns of financial firms over the 10-year period 2001–2010. Our model is consistent with the leveraging/deleveraging of this period and with the 2008 collapse in short-term debt.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2019.1865},
  journal      = {Operations Research},
  number       = {1},
  pages        = {1-307},
  shortjournal = {Oper. Res.},
  title        = {Dynamic Leveraging–Deleveraging games},
  volume       = {68},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Optimal pricing under diffusion-choice models. <em>OR</em>,
<em>68</em>(1), 1–307. (<a
href="https://doi.org/10.1287/opre.2019.1947">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We develop a solution approach to the centralized pricing problem of a firm managing multiple substitutable products. Demand of these products undergoes a diffusion process, and customers choose among the products, with the choice probability of each product given by the logit model. We examine the firm’s optimal pricing problem when product demand can be described by such “diffusion-choice” models. In particular, we focus on two models with proven merits and study a generalized version of the two models. To our knowledge, our work is the first to study the multiproduct pricing problem under the integrated diffusion-choice models, which are of both theoretical appeal and practical advantage. We establish uniqueness of the optimal solution and propose an efficient solution approach in addition to characterizing the optimal prices and their time trend. We show that the price trend can be attributed to diffusion dynamics as well as intertemporal changes in product costs and customer price sensitivity, all of which are integrated into a unified framework in this paper. Our model applies to both simultaneous and sequential product introductions and adapts to stochastic demand.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2019.1947},
  journal      = {Operations Research},
  number       = {1},
  pages        = {1-307},
  shortjournal = {Oper. Res.},
  title        = {Optimal pricing under diffusion-choice models},
  volume       = {68},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Approximation algorithms for product framing and pricing.
<em>OR</em>, <em>68</em>(1), 1–307. (<a
href="https://doi.org/10.1287/opre.2019.1875">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose one of the first models of “product framing” and pricing. Product framing refers to the way consumer choice is influenced by how the products are framed or displayed. We present a model in which a set of products is displayed or framed into a set of virtual web pages. We assume that consumers consider only products in the top pages with different consumers willing to see different numbers of pages. Consumers select a product, if any, from these pages following a general choice model. We show that the product-framing problem is NP-hard. We derive algorithms with guaranteed performance relative to an optimal algorithm under reasonable assumptions. Our algorithms are fast and easy to implement. We also present structural results and design algorithms for pricing under framing effects for the multinomial logit model. We show that, for profit maximization problems, at optimality, products are displayed in descending order of their value gap and in ascending order of their markups.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2019.1875},
  journal      = {Operations Research},
  number       = {1},
  pages        = {1-307},
  shortjournal = {Oper. Res.},
  title        = {Approximation algorithms for product framing and pricing},
  volume       = {68},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Capacity analysis of sequential zone picking systems.
<em>OR</em>, <em>68</em>(1), 1–307. (<a
href="https://doi.org/10.1287/opre.2019.1885">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper develops a capacity model for sequential zone picking systems. These systems are popular internal transport and order-picking systems because of their scalability, flexibility, high-throughput ability, and fit for use for a wide range of products and order profiles. The major disadvantage of such systems is congestion and blocking under heavy use, leading to long order throughput times. To reduce blocking and congestion, most systems use the block-and-recirculate protocol to dynamically manage workload. In this paper, the various elements of the system, such as conveyor lanes and pick zones, are modeled as a multiclass block-and-recirculate queueing network with capacity constraints on subnetworks. Because of this blocking protocol, the stationary distribution of the queueing network is highly intractable. We propose an approximation method based on jump-over blocking. Multiclass jump-over queueing networks admit a product-form stationary distribution and can be efficiently evaluated by mean value analysis and Norton’s theorem. This method can be applied during the design phase of sequential zone picking systems to determine the number of segments, number and length of zones, buffer capacities, and storage allocation of products to zones to meet performance targets. For a wide range of parameters, the results show that the relative error in the system throughput is typically less than 1\% compared with simulation.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2019.1885},
  journal      = {Operations Research},
  number       = {1},
  pages        = {1-307},
  shortjournal = {Oper. Res.},
  title        = {Capacity analysis of sequential zone picking systems},
  volume       = {68},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). An exact solution framework for multitrip vehicle-routing
problems with time windows. <em>OR</em>, <em>68</em>(1), 1–307. (<a
href="https://doi.org/10.1287/opre.2019.1874">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multitrip vehicle - routing problems (MTVRPs) generalize the well-known VRP by allowing vehicles to perform multiple trips per day. MTVRPs have received a lot of attention lately because of their relevance in real-life applications—for example, in city logistics and last-mile delivery. Several variants of the MTVRP have been investigated in the literature, and a number of exact methods have been proposed. Nevertheless, the computational results currently available suggest that MTVRPs with different side constraints require ad hoc formulations and solution methods to be solved. Moreover, solving instances with just 25 customers can be out of reach for such solution methods. In this paper, we proposed an exact solution framework to address four different MTVRPs proposed in the literature. The exact solution framework is based on a novel formulation that has an exponential number of variables and constraints. It relies on column generation, column enumeration, and cutting plane. We show that this solution framework can solve instances with up to 50 customers of four MTVRP variants and outperforms the state-of-the-art methods from the literature.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2019.1874},
  journal      = {Operations Research},
  number       = {1},
  pages        = {1-307},
  shortjournal = {Oper. Res.},
  title        = {An exact solution framework for multitrip vehicle-routing problems with time windows},
  volume       = {68},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Failing to foresee the updating of the reference point leads
to time-inconsistent investment. <em>OR</em>, <em>68</em>(1), 1–307. (<a
href="https://doi.org/10.1287/opre.2019.1872">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The current literature on behavioral portfolio optimization with reference point updating assumes that the decision maker foresees how the reference point will evolve and thus solves a time-consistent problem formulation. Empirical findings, however, suggest that decision makers often fail to foresee the updating of the reference point and consequently make time-inconsistent decisions. We analyze and compare the optimal investment strategies for a discrete time behavioral portfolio optimization problem with loss-aversion and time-varying reference points under both the time-consistent and time-inconsistent framework and for different updating rules for the reference point. There is only one framework predicting realistic investment behavior: the decision maker fails to foresee the updating of the reference point and thus faces a time-inconsistent problem, solves for a dynamically optimal strategy, and updates the reference point in a nonrecursive manner.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2019.1872},
  journal      = {Operations Research},
  number       = {1},
  pages        = {1-307},
  shortjournal = {Oper. Res.},
  title        = {Failing to foresee the updating of the reference point leads to time-inconsistent investment},
  volume       = {68},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Online algorithms for multilevel aggregation. <em>OR</em>,
<em>68</em>(1), 1–307. (<a
href="https://doi.org/10.1287/opre.2019.1847">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the multilevel aggregation problem (MLAP), requests arrive at the nodes of an edge-weighted tree T and have to be served eventually. A service is defined as a subtree X of T that contains the root of T . This subtree X serves all requests that are pending in the nodes of X , and the cost of this service is equal to the total weight of X . Each request also incurs waiting cost between its arrival and service times. The objective is to minimize the total waiting cost of all requests plus the total cost of all service subtrees. MLAP is a generalization of some well-studied optimization problems; for example, for trees of depth 1, MLAP is equivalent to the Transmission Control Protocol acknowledgment problem, whereas for trees of depth 2, it is equivalent to the joint replenishment problem. Aggregation problems for trees of arbitrary depth arise in multicasting, sensor networks, communication in organization hierarchies, and supply chain management. The instances of MLAP associated with these applications are naturally online, in the sense that aggregation decisions need to be made without information about future requests. Constant-competitive online algorithms are known for MLAP with one or two levels. However, it has been open whether there exist constant-competitive online algorithms for trees of depth more than 2. Addressing this open problem, we give the first constant-competitive online algorithm for trees of arbitrary (fixed) depth. The competitive ratio is O ( D 4 2 D ) , where D is the depth of T . The algorithm works for arbitrary waiting cost functions, including the variant with deadlines.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2019.1847},
  journal      = {Operations Research},
  number       = {1},
  pages        = {1-307},
  shortjournal = {Oper. Res.},
  title        = {Online algorithms for multilevel aggregation},
  volume       = {68},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Faster kriging: Facing high-dimensional simulators.
<em>OR</em>, <em>68</em>(1), 1–307. (<a
href="https://doi.org/10.1287/opre.2019.1860">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Kriging is one of the most widely used emulation methods in simulation. However, memory and time requirements potentially hinder its application to data sets generated by high-dimensional simulators. We borrow from the machine learning literature to propose a new algorithmic implementation of kriging that, while preserving prediction accuracy, notably reduces time and memory requirements. The theoretical and computational foundations of the algorithm are provided. The work then reports results of extensive numerical experiments to compare the performance of the proposed algorithm against current kriging implementations, on simulators of increasing dimensionality. Findings show notable savings in time and memory requirements that allow one to handle inputs across more that 10,000 dimensions.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2019.1860},
  journal      = {Operations Research},
  number       = {1},
  pages        = {1-307},
  shortjournal = {Oper. Res.},
  title        = {Faster kriging: Facing high-dimensional simulators},
  volume       = {68},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Queueing models for patient-flow dynamics in inpatient
wards. <em>OR</em>, <em>68</em>(1), 1–307. (<a
href="https://doi.org/10.1287/opre.2019.1845">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hospital-related queues have unique features that are not captured by standard queueing assumptions, necessitating the development of specialized models. In this paper, we propose a queueing model that takes into account the most salient features of queues associated with patient-flow dynamics in inpatient wards, including the need for a physician’s approval to discharge patients and subsequent discharge delays. In this setting, fundamental quantities, such as the (effective) mean hospitalization time and the traffic intensity, become functions of the queueing model’s primitives. We, therefore, begin by characterizing these quantities and quantifying the impacts that the discharge policy has on the average bed utilization and maximal throughput. We then introduce a deterministic fluid model to approximate the nonstationary patient-flow dynamics. The fluid model is shown to possess a unique periodic equilibrium, which is guaranteed to be approached as time increases so that long-run performance analysis can be carried out by simply considering that equilibrium cycle. Consequently, evaluating the effects of policy changes on the system’s performance and optimizing long-run operating costs are facilitated considerably. The effectiveness of the fluid model is demonstrated via comparisons to data from a large hospital and simulation experiments.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2019.1845},
  journal      = {Operations Research},
  number       = {1},
  pages        = {1-307},
  shortjournal = {Oper. Res.},
  title        = {Queueing models for patient-flow dynamics in inpatient wards},
  volume       = {68},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Online decision making with high-dimensional covariates.
<em>OR</em>, <em>68</em>(1), 1–307. (<a
href="https://doi.org/10.1287/opre.2019.1902">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Big data have enabled decision makers to tailor decisions at the individual level in a variety of domains, such as personalized medicine and online advertising. Doing so involves learning a model of decision rewards conditional on individual-specific covariates. In many practical settings, these covariates are high dimensional ; however, typically only a small subset of the observed features are predictive of a decision’s success. We formulate this problem as a K -armed contextual bandit with high-dimensional covariates and present a new efficient bandit algorithm based on the LASSO estimator. We prove that our algorithm’s cumulative expected regret scales at most polylogarithmically in the covariate dimension d ; to the best of our knowledge, this is the first such bound for a contextual bandit. The key step in our analysis is proving a new tail inequality that guarantees the convergence of the LASSO estimator despite the non-i.i.d. data induced by the bandit policy. Furthermore, we illustrate the practical relevance of our algorithm by evaluating it on a simplified version of a medication dosing problem. A patient’s optimal medication dosage depends on the patient’s genetic profile and medical records; incorrect initial dosage may result in adverse consequences, such as stroke or bleeding. We show that our algorithm outperforms existing bandit methods and physicians in correctly dosing a majority of patients.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2019.1902},
  journal      = {Operations Research},
  number       = {1},
  pages        = {1-307},
  shortjournal = {Oper. Res.},
  title        = {Online decision making with high-dimensional covariates},
  volume       = {68},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Technical note—consistency analysis of sequential learning
under approximate bayesian inference. <em>OR</em>, <em>68</em>(1),
1–307. (<a href="https://doi.org/10.1287/opre.2019.1850">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Approximate Bayesian inference is a powerful methodology for constructing computationally efficient statistical mechanisms for sequential learning from incomplete or censored information. Approximate Bayesian learning models have proven successful in a variety of operations research and business problems; however, prior work in this area has been primarily computational, and the consistency of approximate Bayesian estimators has been a largely open problem. We develop a new consistency theory by interpreting approximate Bayesian inference as a form of stochastic approximation (SA) with an additional “bias” term. We prove the convergence of a general SA algorithm of this form and leverage this analysis to derive the first consistency proofs for a suite of approximate Bayesian models from the recent literature.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2019.1850},
  journal      = {Operations Research},
  number       = {1},
  pages        = {1-307},
  shortjournal = {Oper. Res.},
  title        = {Technical Note—Consistency analysis of sequential learning under approximate bayesian inference},
  volume       = {68},
  year         = {2020},
}
</textarea>
</details></li>
</ul>

</body>
</html>
