<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>IJOO_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="ijoo---16">IJOO - 16</h2>
<ul>
<li><details>
<summary>
(2020). Spatial separability in hub location problems with an
application to brain connectivity networks. <em>IJOO</em>,
<em>2</em>(4), 320–346. (<a
href="https://doi.org/10.1287/ijoo.2019.0031">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Motivated by the need to solve large hub location problems efficiently and accurately, we discover an important characteristic of optimal solutions to p -hub median problems that we call spatial separability. It refers to the partitioning of the network into allocation clusters with nonoverlapping convex hulls. We illustrate numerically that the property persists over a wide range of randomly generated instances and propose a data-driven approach based on an insight from the property to tackle very large problem sizes. Computational experiments corroborate the effectiveness of the proposed approach in generating high-quality solutions within reasonable computational times. We then explore a new application area of hub location problems in brain connectivity networks and introduce the largest and the first set of three-dimensional instances in the literature. Computational results demonstrate the capability of hub location models in successfully depicting the hub organization of the human brain, as validated by the medical literature, thus revealing that hub location models can play an important role in investigating the intricate connectivity of the human brain.},
  archive      = {J_IJOO},
  doi          = {10.1287/ijoo.2019.0031},
  journal      = {Informs Journal on Optimization},
  number       = {4},
  pages        = {320-346},
  shortjournal = {INFORMS J. Optim.},
  title        = {Spatial separability in hub location problems with an application to brain connectivity networks},
  volume       = {2},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A partially ranked choice model for large-scale data-driven
assortment optimization. <em>IJOO</em>, <em>2</em>(4), 297–319. (<a
href="https://doi.org/10.1287/ijoo.2019.0037">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The assortment of products carried by a store has a crucial impact on its success. However, finding the right mix of products to attract a large portion of the customers is a challenging task. Several mathematical models have been proposed to optimize assortments. Most of them are based on discrete choice models that represent the buying behavior of the customers. Among them, rank-based choice models have been acknowledged for representing well high-dimensional product substitution effects and, therefore, reflect customer preferences in a reasonably realistic manner. In this work, we extend the concept of (strictly) fully ranked choice models to models with partial ranking that additionally allow for indifference among subsets of products, that is, on which the customer does not have a strict preference. We show that partially ranked choice models are theoretically equivalent to fully ranked choice models. We then propose an embedded column-generation procedure to efficiently estimate partially ranked choice models from historical transaction and assortment data. The subproblems involved can be efficiently solved by using a growing preference tree that represents partially ranked preferences, enabling us to learn preferences and optimize assortments for thousands of products. Computational experiments on artificially generated data and a case study on real industrial retail data suggest that our proposed methods outperform existing algorithms in terms of scalability, prediction accuracy, and quality of the obtained assortments.},
  archive      = {J_IJOO},
  doi          = {10.1287/ijoo.2019.0037},
  journal      = {Informs Journal on Optimization},
  number       = {4},
  pages        = {297-319},
  shortjournal = {INFORMS J. Optim.},
  title        = {A partially ranked choice model for large-scale data-driven assortment optimization},
  volume       = {2},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Data-driven modeling and optimization of the order
consolidation problem in e-warehousing. <em>IJOO</em>, <em>2</em>(4),
273–296. (<a href="https://doi.org/10.1287/ijoo.2019.0039">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We analyze data emanating from a major e-commerce warehouse and provided by a third-party warehouse logistics management company to replicate flow diagrams, assess order fulfillment efficiency, identify bottlenecks, and suggest improvement strategies. Without access to actual layouts and process-flow diagrams and purely based on data, we are able to describe the processes in detail and prescribe changes. By investigating the characteristics of orders, the wave-sorting operation, and the order-preparation process, we find that products from different orders are picked in batches for efficiency. Similar products are picked in small containers called totes. Totes are then stored in a buffer area and routed to be emptied of their contents at induction lines. Orders are then consolidated at the put wall, where each order is accumulated in a cubby. This order consolidation process depends on the sequence in which totes are processed and has a huge impact on order-completion time. We, therefore, present a generalization of the parallel machine–scheduling problem that we call the order consolidation problem to determine the tote-processing sequence that minimizes total order completion time. We provide mathematical formulations and devise heuristic and exact solution methods. We propose a fast simulated annealing metaheuristic and a branch-and-price approach in which the subproblems are variants of the single machine-scheduling problem and are solved using dynamic programming. We also devise a new branching rule, compare it against the literature, and test it on randomly generated and industry data. Applied to the data and the warehouse under study, optimizing the order consolidation is found to decrease the completion time of 75.66\% of orders and achieve average improvements of up to 28.77\% in order consolidation time and 21.92\% in cubby usage.},
  archive      = {J_IJOO},
  doi          = {10.1287/ijoo.2019.0039},
  journal      = {Informs Journal on Optimization},
  number       = {4},
  pages        = {273-296},
  shortjournal = {INFORMS J. Optim.},
  title        = {Data-driven modeling and optimization of the order consolidation problem in E-warehousing},
  volume       = {2},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Computational aspects of bayesian solution estimators in
stochastic optimization. <em>IJOO</em>, <em>2</em>(4), 256–272. (<a
href="https://doi.org/10.1287/ijoo.2019.0035">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study a class of stochastic programs in which some of the elements in the objective function are random and their probability distribution has unknown parameters. The goal is to find a good estimate for the optimal solution of the stochastic program using data sampled from the distribution of the random elements. We investigate two common optimization criteria for evaluating the quality of a solution estimator—one based on the difference in objective values and the other based on the Euclidean distance between solutions. We use risk as the expected value of such criteria over the sample space. Under a Bayesian framework, where a prior distribution is assumed for the unknown parameters, two natural estimation-optimization strategies arise. A separate scheme first finds an estimator for the unknown parameters and then uses this estimator in the optimization problem. A joint scheme combines the estimation and optimization steps by directly adjusting the distribution in the stochastic program. We analyze the risk difference between the solutions obtained from these two schemes for several classes of stochastic programs and provide insight on the computational effort to solve these problems.},
  archive      = {J_IJOO},
  doi          = {10.1287/ijoo.2019.0035},
  journal      = {Informs Journal on Optimization},
  number       = {4},
  pages        = {256-272},
  shortjournal = {INFORMS J. Optim.},
  title        = {Computational aspects of bayesian solution estimators in stochastic optimization},
  volume       = {2},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A universal and structured way to derive dual optimization
problem formulations. <em>IJOO</em>, <em>2</em>(4), 229–255. (<a
href="https://doi.org/10.1287/ijoo.2019.0034">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The dual problem of a convex optimization problem can be obtained in a relatively simple and structural way by using a well-known result in convex analysis, namely Fenchel’s duality theorem. This alternative way of forming a strong dual problem is the subject of this paper. We recall some standard results from convex analysis and then discuss how the dual problem can be written in terms of the conjugates of the objective function and the constraint functions. This is a didactically valuable method to explicitly write the dual problem. We demonstrate the method by deriving dual problems for several classical problems and also for a practical model for radiotherapy treatment planning, for which deriving the dual problem using other methods is a more tedious task. Additional material is presented in the appendices, including useful tables for finding conjugate functions of many functions.},
  archive      = {J_IJOO},
  doi          = {10.1287/ijoo.2019.0034},
  journal      = {Informs Journal on Optimization},
  number       = {4},
  pages        = {229-255},
  shortjournal = {INFORMS J. Optim.},
  title        = {A universal and structured way to derive dual optimization problem formulations},
  volume       = {2},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Integer programming on the junction tree polytope for
influence diagrams. <em>IJOO</em>, <em>2</em>(3), 209–228. (<a
href="https://doi.org/10.1287/ijoo.2019.0036">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Influence diagrams (ID) and limited memory influence diagrams (LIMID) are flexible tools to represent discrete stochastic optimization problems, with the Markov decision process (MDP) and partially observable MDP as standard examples. More precisely, given random variables considered as vertices of an acyclic digraph, a probabilistic graphical model defines a joint distribution via the conditional distributions of vertices given their parents. In an ID, the random variables are represented by a probabilistic graphical model whose vertices are partitioned into three types: chance, decision, and utility vertices. The user chooses the distribution of the decision vertices conditionally to their parents in order to maximize the expected utility. Leveraging the notion of rooted junction tree, we present a mixed integer linear formulation for solving an ID, as well as valid inequalities, which lead to a computationally efficient algorithm. We also show that the linear relaxation yields an optimal integer solution for instances that can be solved by the “single policy update,” the default algorithm for addressing IDs.},
  archive      = {J_IJOO},
  doi          = {10.1287/ijoo.2019.0036},
  journal      = {Informs Journal on Optimization},
  number       = {3},
  pages        = {209-228},
  shortjournal = {INFORMS J. Optim.},
  title        = {Integer programming on the junction tree polytope for influence diagrams},
  volume       = {2},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). An alternating manifold proximal gradient method for sparse
principal component analysis and sparse canonical correlation analysis.
<em>IJOO</em>, <em>2</em>(3), 192–208. (<a
href="https://doi.org/10.1287/ijoo.2019.0032">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sparse principal component analysis and sparse canonical correlation analysis are two essential techniques from high-dimensional statistics and machine learning for analyzing large-scale data. Both problems can be formulated as an optimization problem with nonsmooth objective and nonconvex constraints. Because nonsmoothness and nonconvexity bring numerical difficulties, most algorithms suggested in the literature either solve some relaxations of them or are heuristic and lack convergence guarantees. In this paper, we propose a new alternating manifold proximal gradient method to solve these two high-dimensional problems and provide a unified convergence analysis. Numerical experimental results are reported to demonstrate the advantages of our algorithm.},
  archive      = {J_IJOO},
  doi          = {10.1287/ijoo.2019.0032},
  journal      = {Informs Journal on Optimization},
  number       = {3},
  pages        = {192-208},
  shortjournal = {INFORMS J. Optim.},
  title        = {An alternating manifold proximal gradient method for sparse principal component analysis and sparse canonical correlation analysis},
  volume       = {2},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Data association via set packing for computer vision
applications. <em>IJOO</em>, <em>2</em>(3), 167–191. (<a
href="https://doi.org/10.1287/ijoo.2019.0030">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Significant progress has been made in the field of computer vision because of the development of supervised machine learning algorithms, which efficiently extract information from high-dimensional data such as images and videos. Such techniques are particularly effective at recognizing the presence or absence of entities in the domains where labeled data are abundant. However, supervised learning is not sufficient in applications where one needs to annotate each unique entity in crowded scenes respecting known domain-specific structures of those entities. This problem, known as data association , provides fertile ground for the application of combinatorial optimization. In this review paper, we present a unified framework based on column generation for some computer vision applications, namely multiperson tracking, multiperson pose estimation, and multicell segmentation, which can be formulated as set packing problems with a massive number of variables. To solve them, column generation algorithms are applied to circumvent the need to enumerate all variables explicitly. To enhance the solution process, we provide a general approach for applying subset-row inequalities to tighten the formulations and introduce novel dual-optimal inequalities to reduce the dual search space. The proposed algorithms and their enhancements are successfully applied to solve the three aforementioned computer vision problems and achieve superior performance over benchmark approaches. The common framework presented allows us to leverage operations research methodologies to efficiently tackle computer vision problems.},
  archive      = {J_IJOO},
  doi          = {10.1287/ijoo.2019.0030},
  journal      = {Informs Journal on Optimization},
  number       = {3},
  pages        = {167-191},
  shortjournal = {INFORMS J. Optim.},
  title        = {Data association via set packing for computer vision applications},
  volume       = {2},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Mining optimal policies: A pattern recognition approach to
model analysis. <em>IJOO</em>, <em>2</em>(3), 145–166. (<a
href="https://doi.org/10.1287/ijoo.2019.0026">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper studies the application of machine learning (ML) algorithms for discovering structural properties of optimal policies in numerically obtained solutions to optimization problems. We propose a framework based on ML for conducting model analysis in a systematic way that complements theoretical and numerical methods. As a proof of concept, we apply the framework to core operations problems, such as inventory management, queuing admission control, multiarmed bandit (MAB), and revenue management problems. We demonstrate how this approach can be used to identify optimal threshold-based policies (inventory management and admission control) and index policies (MAB), as well as for developing new heuristics for revenue management problems. For the MAB problem, our approach leads to a new efficient algorithm for computing optimal index policies. The main contribution of this work is methodological, in proposing and demonstrating the potential of using ML algorithms to analyze optimization problems and devise interpretable policies.},
  archive      = {J_IJOO},
  doi          = {10.1287/ijoo.2019.0026},
  journal      = {INFORMS Journal on Optimization},
  number       = {3},
  pages        = {145-166},
  shortjournal = {INFORMS J. Optim.},
  title        = {Mining optimal policies: A pattern recognition approach to model analysis},
  volume       = {2},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Decremental clustering for the solution of p-dispersion
problems to proven optimality. <em>IJOO</em>, <em>2</em>(2), 134–144.
(<a href="https://doi.org/10.1287/ijoo.2019.0027">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Given n points, a symmetric dissimilarity matrix D of dimensions n × n , and an integer p ? 2 , the p -dispersion problem (pDP) consists of selecting a subset of exactly p points in such a way that the minimum dissimilarity between any pair of selected points is maximum. The pDP is NP when p is an input of the problem. We propose a decremental clustering method to reduce the problem to the solution of a series of smaller pDPs until reaching proven optimality. A k -means algorithm is used to construct and refine the clusterings along the algorithm’s execution. The proposed method can handle problems orders of magnitude larger than the limits of the state-of-the-art solver for the pDP for small values of p .},
  archive      = {J_IJOO},
  doi          = {10.1287/ijoo.2019.0027},
  journal      = {Informs Journal on Optimization},
  number       = {2},
  pages        = {134-144},
  shortjournal = {INFORMS J. Optim.},
  title        = {Decremental clustering for the solution of p-dispersion problems to proven optimality},
  volume       = {2},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). On robust fractional 0-1 programming. <em>IJOO</em>,
<em>2</em>(2), 96–133. (<a
href="https://doi.org/10.1287/ijoo.2019.0025">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study single- and multiple-ratio robust fractional 0-1 programming problems (RFPs). In particular, this work considers RFPs under a wide range of disjoint and joint uncertainty sets, where the former implies separate uncertainty sets for each numerator and denominator and the latter accounts for different forms of interrelatedness between them. We first demonstrate that unlike the deterministic case, a single-ratio RFP is nondeterministic polynomial-time hard under general polyhedral uncertainty sets. However, if the uncertainty sets are imbued with a certain structure, variants of the well-known budgeted uncertainty, the disjoint and joint single-ratio RFPs are polynomially solvable when the deterministic counterpart is. We also propose mixed-integer linear programming (MILP) formulations for multiple-ratio RFPs. We conduct extensive computational experiments using test instances based on real and synthetic data sets to evaluate the performance of our MILP reformulations as well as to compare the disjoint and joint uncertainty sets. Finally, we demonstrate the value of the robust approach by examining the robust solution in a deterministic setting and vice versa.},
  archive      = {J_IJOO},
  doi          = {10.1287/ijoo.2019.0025},
  journal      = {INFORMS Journal on Optimization},
  number       = {2},
  pages        = {96-133},
  shortjournal = {INFORMS J. Optim.},
  title        = {On robust fractional 0-1 programming},
  volume       = {2},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A robust approach to the capacitated vehicle routing problem
with uncertain costs. <em>IJOO</em>, <em>2</em>(2), 79–95. (<a
href="https://doi.org/10.1287/ijoo.2019.0021">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We investigate a robust approach for solving the capacitated vehicle routing problem (CVRP) with uncertain travel times. It is based on the concept of K -adaptability, which allows one to calculate a set of k feasible solutions in a preprocessing phase before the scenario is revealed. Once a scenario occurs, the corresponding best solution may be picked out of the set of candidates. The aim is to determine the k candidates by hedging against the worst-case scenario, as is common in robust optimization. This idea leads to a min-max-min problem. In this paper, we propose an oracle-based algorithm for solving the resulting min-max-min CVRP, calling an exact algorithm for the deterministic problem in each iteration. Moreover, we adjust this approach such that heuristics for the CVRP can also be used. In this way, we derive a heuristic algorithm for the min-max-min problem, which turns out to yield good solutions in a short running time. All algorithms are tested on standard benchmark instances of the CVRP.},
  archive      = {J_IJOO},
  doi          = {10.1287/ijoo.2019.0021},
  journal      = {Informs Journal on Optimization},
  number       = {2},
  pages        = {79-95},
  shortjournal = {INFORMS J. Optim.},
  title        = {A robust approach to the capacitated vehicle routing problem with uncertain costs},
  volume       = {2},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Managing online content to build a follower base: Model and
applications. <em>IJOO</em>, <em>2</em>(1), 57–77. (<a
href="https://doi.org/10.1287/ijoo.2019.0023">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Content providers manage their production by regulating the pace at which content is created and released. They have two types of consumers: existing ones, or followers ; and new visitors who may become followers. To maximize their effectiveness, providers must consider the direct, short-term effect of content and also its indirect, long-term effect on the retention and expansion of their follower base. We develop a simple model to study the dynamics of building up a follower base and then combine that model with stochastic dynamic programming to optimize the content provider’s profit. We find that optimal content policies exhibit trajectories whereby both content and followers increase until reaching a steady state at which revenues equal marginal costs. It is therefore preferable to start slow and then accelerate only after successful content has increased the follower base; thus past success makes the content provider work harder. We apply our model to bloggers by analyzing the traffic from new and returning visitors to several blogs. We find that optimized posting activity yields significantly higher profits than does a steady or seasonal content strategy. Our model contributes to the growing literature on data-driven optimization and prescriptive analytics, specifically for content management.},
  archive      = {J_IJOO},
  doi          = {10.1287/ijoo.2019.0023},
  journal      = {INFORMS Journal on Optimization},
  number       = {1},
  pages        = {57-77},
  shortjournal = {INFORMS J. Optim.},
  title        = {Managing online content to build a follower base: Model and applications},
  volume       = {2},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Comparison-based algorithms for one-dimensional stochastic
convex optimization. <em>IJOO</em>, <em>2</em>(1), 34–56. (<a
href="https://doi.org/10.1287/ijoo.2019.0022">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Stochastic optimization finds a wide range of applications in operations research and management science. However, existing stochastic optimization techniques usually require the information of random samples (e.g., demands in the newsvendor problem) or the objective values at the sampled points (e.g., the lost sales cost), which might not be available in practice. In this paper, we consider a new setup for stochastic optimization, in which the decision maker has access to only comparative information between a random sample and two chosen decision points in each iteration. We propose a comparison-based algorithm (CBA) to solve such problems in one dimension with convex objective functions. Particularly, the CBA properly chooses the two points in each iteration and constructs an unbiased gradient estimate for the original problem. We show that the CBA achieves the same convergence rate as the optimal stochastic gradient methods (with the samples observed). We also consider extensions of our approach to multidimensional quadratic problems as well as problems with nonconvex objective functions. Numerical experiments show that the CBA performs well in test problems.},
  archive      = {J_IJOO},
  doi          = {10.1287/ijoo.2019.0022},
  journal      = {INFORMS Journal on Optimization},
  number       = {1},
  pages        = {34-56},
  shortjournal = {INFORMS J. Optim.},
  title        = {Comparison-based algorithms for one-dimensional stochastic convex optimization},
  volume       = {2},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Improved linear programs for discrete barycenters.
<em>IJOO</em>, <em>2</em>(1), 14–33. (<a
href="https://doi.org/10.1287/ijoo.2019.0020">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Discrete barycenters are the optimal solutions to mass transport problems for a set of discrete measures. Such transport problems arise in many applications of operations research and statistics. The best-known algorithms for exact barycenters are based on linear programming, but these programs scale exponentially in the number of measures, making them prohibitive for practical purposes. In this paper, we improve on these algorithms. First, by using the optimality conditions to restrict the search space, we provide a reduced linear program that contains dramatically fewer variables compared with previous formulations. Second, we recall a proof from the literature that lends itself to a linear program that has not been considered for computations. We show that this second formulation is the best model for data in general position. Third, we combine the two programs into a single hybrid model that retains the best properties of both formulations for partially structured data. We study these models by analyzing their scaling in size, by showing the hardness of the required preprocessing, and through computational experiments. In doing so, we show that each of the improved linear programs becomes the best model for different types of data.},
  archive      = {J_IJOO},
  doi          = {10.1287/ijoo.2019.0020},
  journal      = {Informs Journal on Optimization},
  number       = {1},
  pages        = {14-33},
  shortjournal = {INFORMS J. Optim.},
  title        = {Improved linear programs for discrete barycenters},
  volume       = {2},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Novel target discovery of existing therapies: Path to
personalized cancer therapy. <em>IJOO</em>, <em>2</em>(1), 1–13. (<a
href="https://doi.org/10.1287/ijoo.2019.0019">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Discovering new drugs involves tremendous effort and financial resources, often at a significant risk of failed trials. Identifying new targets of existing drugs provides a promising direction, especially for molecular targeted cancer therapies. This paper presents a novel, machine learning, and optimization-based method that identifies potential targets of existing drugs to expand the treatable patient population. The method has the following advantages: (1) It is based on clinical and genomic data from a large national cancer hospital; (2) it incorporates state-of-the-art knowledge of cancer molecular biology and signaling pathways; and (3) it models patient heterogeneity explicitly outside genomics. The output is an ordered list of therapy–target pairs that our algorithm identifies as highly promising to be further tested. The results are highly accurate when validated against known mechanisms of action for existing drugs, where relationships such as pertuzumab–ERBB2, cetuximab–EGFR, and erlotinib–EGFR were independently identified. We found similar results in the external The Cancer Genome Atlas data set. The findings suggest that a data-driven optimization approach to precision cancer medicine may lead to breakthroughs in the drug-discovery process and recommend effective personalized cancer treatments given patient-specific genomic and phenotypic information.},
  archive      = {J_IJOO},
  doi          = {10.1287/ijoo.2019.0019},
  journal      = {Informs Journal on Optimization},
  number       = {1},
  pages        = {1-13},
  shortjournal = {INFORMS J. Optim.},
  title        = {Novel target discovery of existing therapies: Path to personalized cancer therapy},
  volume       = {2},
  year         = {2020},
}
</textarea>
</details></li>
</ul>

</body>
</html>
