<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>NECO_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="neco---78">NECO - 78</h2>
<ul>
<li><details>
<summary>
(2020). Analyzing and accelerating the bottlenecks of training deep
SNNs with backpropagation. <em>NECO</em>, <em>32</em>(12), 2557–2600.
(<a href="https://doi.org/10.1162/neco_a_01319">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Spiking neural networks (SNNs) with the event-driven manner of transmitting spikes consume ultra-low power on neuromorphic chips. However, training deep SNNs is still challenging compared to convolutional neural networks (CNNs). The SNN training algorithms have not achieved the same performance as CNNs. In this letter, we aim to understand the intrinsic limitations of SNN training to design better algorithms. First, the pros and cons of typical SNN training algorithms are analyzed. Then it is found that the spatiotemporal backpropagation algorithm (STBP) has potential in training deep SNNs due to its simplicity and fast convergence. Later, the main bottlenecks of the STBP algorithm are analyzed, and three conditions for training deep SNNs with the STBP algorithm are derived. By analyzing the connection between CNNs and SNNs, we propose a weight initialization algorithm to satisfy the three conditions. Moreover, we propose an error minimization method and a modified loss function to further improve the training performance. Experimental results show that the proposed method achieves 91.53\% accuracy on the CIFAR10 data set with 1\% accuracy increase over the STBP algorithm and decreases the training epochs on the MNIST data set to 15 epochs (over 13 times speed-up compared to the STBP algorithm). The proposed method also decreases classification latency by over 25 times compared to the CNN-SNN conversion algorithms. In addition, the proposed method works robustly for very deep SNNs, while the STBP algorithm fails in a 19-layer SNN.},
  archive      = {J_NECO},
  author       = {Chen, Ruizhi and Li, Ling},
  doi          = {10.1162/neco_a_01319},
  journal      = {Neural Computation},
  number       = {12},
  pages        = {2557-2600},
  shortjournal = {Neural Comput.},
  title        = {Analyzing and accelerating the bottlenecks of training deep SNNs with backpropagation},
  volume       = {32},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Redundancy-aware pruning of convolutional neural networks.
<em>NECO</em>, <em>32</em>(12), 2532–2556. (<a
href="https://doi.org/10.1162/neco_a_01330">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Pruning is an effective way to slim and speed up convolutional neural networks. Generally previous work directly pruned neural networks in the original feature space without considering the correlation of neurons. We argue that such a way of pruning still keeps some redundancy in the pruned networks. In this letter, we proposed to prune in the intermediate space in which the correlation of neurons is eliminated. To achieve this goal, the input and output of a convolutional layer are first mapped to an intermediate space by orthogonal transformation. Then neurons are evaluated and pruned in the intermediate space. Extensive experiments have shown that our redundancy-aware pruning method surpasses state-of-the-art pruning methods on both efficiency and accuracy. Notably, using our redundancy-aware pruning method, ResNet models with three times the speed-up could achieve competitive performance with fewer floating point operations per second even compared to DenseNet.},
  archive      = {J_NECO},
  author       = {Xie, Guotian},
  doi          = {10.1162/neco_a_01330},
  journal      = {Neural Computation},
  number       = {12},
  pages        = {2532-2556},
  shortjournal = {Neural Comput.},
  title        = {Redundancy-aware pruning of convolutional neural networks},
  volume       = {32},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Active learning for level set estimation under input
uncertainty and its extensions. <em>NECO</em>, <em>32</em>(12),
2486–2531. (<a href="https://doi.org/10.1162/neco_a_01332">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Testing under what conditions a product satisfies the desired properties is a fundamental problem in manufacturing industry. If the condition and the property are respectively regarded as the input and the output of a black-box function, this task can be interpreted as the problem called level set estimation (LSE): the problem of identifying input regions such that the function value is above (or below) a threshold. Although various methods for LSE problems have been developed, many issues remain to be solved for their practical use. As one of such issues, we consider the case where the input conditions cannot be controlled precisely—LSE problems under input uncertainty. We introduce a basic framework for handling input uncertainty in LSE problems and then propose efficient methods with proper theoretical guarantees. The proposed methods and theories can be generally applied to a variety of challenges related to LSE under input uncertainty such as cost-dependent input uncertainties and unknown input uncertainties. We apply the proposed methods to artificial and real data to demonstrate their applicability and effectiveness.},
  archive      = {J_NECO},
  author       = {Inatsu, Yu and Karasuyama, Masayuki and Inoue, Keiichi and Takeuchi, Ichiro},
  doi          = {10.1162/neco_a_01332},
  journal      = {Neural Computation},
  number       = {12},
  pages        = {2486-2531},
  shortjournal = {Neural Comput.},
  title        = {Active learning for level set estimation under input uncertainty and its extensions},
  volume       = {32},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Toward a unified framework for cognitive maps.
<em>NECO</em>, <em>32</em>(12), 2455–2485. (<a
href="https://doi.org/10.1162/neco_a_01326">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this study, we integrated neural encoding and decoding into a unified framework for spatial information processing in the brain. Specifically, the neural representations of self-location in the hippocampus (HPC) and entorhinal cortex (EC) play crucial roles in spatial navigation. Intriguingly, these neural representations in these neighboring brain areas show stark differences. Whereas the place cells in the HPC fire as a unimodal function of spatial location, the grid cells in the EC show periodic tuning curves with different periods for different subpopulations (called modules). By combining an encoding model for this modular neural representation and a realistic decoding model based on belief propagation, we investigated the manner in which self-location is encoded by neurons in the EC and then decoded by downstream neurons in the HPC. Through the results of numerical simulations, we first show the positive synergy effects of the modular structure in the EC. The modular structure introduces more coupling between heterogeneous modules with different periodicities, which provides increased error-correcting capabilities. This is also demonstrated through a comparison of the beliefs produced for decoding two- and four-module codes. Whereas the former resulted in a complete decoding failure, the latter correctly recovered the self-location even from the same inputs. Further analysis of belief propagation during decoding revealed complex dynamics in information updates due to interactions among multiple modules having diverse scales. Therefore, the proposed unified framework allows one to investigate the overall flow of spatial information, closing the loop of encoding and decoding self-location in the brain.},
  archive      = {J_NECO},
  author       = {Kim, Woori and Yoo, Yongseok},
  doi          = {10.1162/neco_a_01326},
  journal      = {Neural Computation},
  number       = {12},
  pages        = {2455-2485},
  shortjournal = {Neural Comput.},
  title        = {Toward a unified framework for cognitive maps},
  volume       = {32},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Synchrony and complexity in state-related EEG networks: An
application of spectral graph theory. <em>NECO</em>, <em>32</em>(12),
2422–2454. (<a href="https://doi.org/10.1162/neco_a_01327">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The brain may be considered as a synchronized dynamic network with several coherent dynamical units. However, concerns remain whether synchronizability is a stable state in the brain networks. If so, which index can best reveal the synchronizability in brain networks? To answer these questions, we tested the application of the spectral graph theory and the Shannon entropy as alternative approaches in neuroimaging. We specifically tested the alpha rhythm in the resting-state eye closed (rsEC) and the resting-state eye open (rsEO) conditions, a well-studied classical example of synchrony in neuroimaging EEG. Since the synchronizability of alpha rhythm is more stable during the rsEC than the rsEO, we hypothesized that our suggested spectral graph theory indices (as reliable measures to interpret the synchronizability of brain signals) should exhibit higher values in the rsEC than the rsEO condition. We performed two separate analyses of two different datasets (as elementary and confirmatory studies). Based on the results of both studies and in agreement with our hypothesis, the spectral graph indices revealed higher stability of synchronizability in the rsEC condition. The k-mean analysis indicated that the spectral graph indices can distinguish the rsEC and rsEO conditions by considering the synchronizability of brain networks. We also computed correlations among the spectral indices, the Shannon entropy, and the topological indices of brain networks, as well as random networks. Correlation analysis indicated that although the spectral and the topological properties of random networks are completely independent, these features are significantly correlated with each other in brain networks. Furthermore, we found that complexity in the investigated brain networks is inversely related to the stability of synchronizability. In conclusion, we revealed that the spectral graph theory approach can be reliably applied to study the stability of synchronizability of state-related brain networks.},
  archive      = {J_NECO},
  author       = {Ghaderi, Amir Hossein and Baltaretu, Bianca R. and Andevari, Masood Nemati and Bharmauria, Vishal and Balci, Fuat},
  doi          = {10.1162/neco_a_01327},
  journal      = {Neural Computation},
  number       = {12},
  pages        = {2422-2454},
  shortjournal = {Neural Comput.},
  title        = {Synchrony and complexity in state-related EEG networks: An application of spectral graph theory},
  volume       = {32},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Differential covariance: A new method to estimate functional
connectivity in fMRI. <em>NECO</em>, <em>32</em>(12), 2389–2421. (<a
href="https://doi.org/10.1162/neco_a_01323">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Measuring functional connectivity from fMRI recordings is important in understanding processing in cortical networks. However, because the brain&#39;s connection pattern is complex, currently used methods are prone to producing false functional connections. We introduce differential covariance analysis, a new method that uses derivatives of the signal for estimating functional connectivity. We generated neural activities from dynamical causal modeling and a neural network of Hodgkin-Huxley neurons and then converted them to hemodynamic signals using the forward balloon model. The simulated fMRI signals, together with the ground-truth connectivity pattern, were used to benchmark our method with other commonly used methods. Differential covariance achieved better results in complex network simulations. This new method opens an alternative way to estimate functional connectivity.},
  archive      = {J_NECO},
  author       = {Lin, Tiger W. and Chen, Yusi and Bukhari, Qasim and Krishnan, Giri P. and Bazhenov, Maxim and Sejnowski, Terrence J.},
  doi          = {10.1162/neco_a_01323},
  journal      = {Neural Computation},
  number       = {12},
  pages        = {2389-2421},
  shortjournal = {Neural Comput.},
  title        = {Differential covariance: A new method to estimate functional connectivity in fMRI},
  volume       = {32},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Resonator networks, 2: Factorization performance and
capacity compared to optimization-based methods. <em>NECO</em>,
<em>32</em>(12), 2332–2388. (<a
href="https://doi.org/10.1162/neco_a_01329">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We develop theoretical foundations of resonator networks, a new type of recurrent neural network introduced in Frady, Kent, Olshausen, and Sommer ( 2020 ), a companion article in this issue, to solve a high-dimensional vector factorization problem arising in Vector Symbolic Architectures. Given a composite vector formed by the Hadamard product between a discrete set of high-dimensional vectors, a resonator network can efficiently decompose the composite into these factors. We compare the performance of resonator networks against optimization-based methods, including Alternating Least Squares and several gradient-based algorithms, showing that resonator networks are superior in several important ways. This advantage is achieved by leveraging a combination of nonlinear dynamics and searching in superposition, by which estimates of the correct solution are formed from a weighted superposition of all possible solutions. While the alternative methods also search in superposition, the dynamics of resonator networks allow them to strike a more effective balance between exploring the solution space and exploiting local information to drive the network toward probable solutions. Resonator networks are not guaranteed to converge, but within a particular regime they almost always do. In exchange for relaxing the guarantee of global convergence, resonator networks are dramatically more effective at finding factorizations than all alternative approaches considered.},
  archive      = {J_NECO},
  author       = {Kent, Spencer J. and Frady, E. Paxon and Sommer, Friedrich T. and Olshausen, Bruno A.},
  doi          = {10.1162/neco_a_01329},
  journal      = {Neural Computation},
  number       = {12},
  pages        = {2332-2388},
  shortjournal = {Neural Comput.},
  title        = {Resonator networks, 2: Factorization performance and capacity compared to optimization-based methods},
  volume       = {32},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Resonator networks, 1: An efficient solution for factoring
high-dimensional, distributed representations of data structures.
<em>NECO</em>, <em>32</em>(12), 2311–2331. (<a
href="https://doi.org/10.1162/neco_a_01331">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The ability to encode and manipulate data structures with distributed neural representations could qualitatively enhance the capabilities of traditional neural networks by supporting rule-based symbolic reasoning, a central property of cognition. Here we show how this may be accomplished within the framework of Vector Symbolic Architectures (VSAs) (Plate, 1991 ; Gayler, 1998 ; Kanerva, 1996 ), whereby data structures are encoded by combining high-dimensional vectors with operations that together form an algebra on the space of distributed representations. In particular, we propose an efficient solution to a hard combinatorial search problem that arises when decoding elements of a VSA data structure: the factorization of products of multiple codevectors. Our proposed algorithm, called a resonator network, is a new type of recurrent neural network that interleaves VSA multiplication operations and pattern completion. We show in two examples—parsing of a tree-like data structure and parsing of a visual scene—how the factorization problem arises and how the resonator network can solve it. More broadly, resonator networks open the possibility of applying VSAs to myriad artificial intelligence problems in real-world domains. The companion article in this issue (Kent, Frady, Sommer, &amp; Olshausen, 2020 ) presents a rigorous analysis and evaluation of the performance of resonator networks, showing it outperforms alternative approaches.},
  archive      = {J_NECO},
  author       = {Frady, E. Paxon and Kent, Spencer J. and Olshausen, Bruno A. and Sommer, Friedrich T.},
  doi          = {10.1162/neco_a_01331},
  journal      = {Neural Computation},
  number       = {12},
  pages        = {2311-2331},
  shortjournal = {Neural Comput.},
  title        = {Resonator networks, 1: An efficient solution for factoring high-dimensional, distributed representations of data structures},
  volume       = {32},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Effect of top-down connections in hierarchical sparse
coding. <em>NECO</em>, <em>32</em>(11), 2279–2309. (<a
href="https://doi.org/10.1162/neco_a_01325">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hierarchical sparse coding (HSC) is a powerful model to efficiently represent multidimensional, structured data such as images. The simplest solution to solve this computationally hard problem is to decompose it into independent layer-wise subproblems. However, neuroscientific evidence would suggest interconnecting these subproblems as in predictive coding (PC) theory, which adds top-down connections between consecutive layers. In this study, we introduce a new model, 2-layer sparse predictive coding (2L-SPC), to assess the impact of this interlayer feedback connection. In particular, the 2L-SPC is compared with a hierarchical Lasso (Hi-La) network made out of a sequence of independent Lasso layers. The 2L-SPC and a 2-layer Hi-La networks are trained on four different databases and with different sparsity parameters on each layer. First, we show that the overall prediction error generated by 2L-SPC is lower thanks to the feedback mechanism as it transfers prediction error between layers. Second, we demonstrate that the inference stage of the 2L-SPC is faster to converge and generates a refined representation in the second layer compared to the Hi-La model. Third, we show that the 2L-SPC top-down connection accelerates the learning process of the HSC problem. Finally, the analysis of the emerging dictionaries shows that the 2L-SPC features are more generic and present a larger spatial extension.},
  archive      = {J_NECO},
  author       = {Boutin, Victor and Franciosini, Angelo and Ruffier, Franck and Perrinet, Laurent},
  doi          = {10.1162/neco_a_01325},
  journal      = {Neural Computation},
  number       = {11},
  pages        = {2279-2309},
  shortjournal = {Neural Comput.},
  title        = {Effect of top-down connections in hierarchical sparse coding},
  volume       = {32},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). ReLU networks are universal approximators via piecewise
linear or constant functions. <em>NECO</em>, <em>32</em>(11), 2249–2278.
(<a href="https://doi.org/10.1162/neco_a_01316">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This letter proves that a ReLU network can approximate any continuous function with arbitrary precision by means of piecewise linear or constant approximations. For univariate function f ( x ) ⁠ , we use the composite of ReLUs to produce a line segment; all of the subnetworks of line segments comprise a ReLU network, which is a piecewise linear approximation to f ( x ) ⁠ . For multivariate function f ( x ) ⁠ , ReLU networks are constructed to approximate a piecewise linear function derived from triangulation methods approximating f ( x ) ⁠ . A neural unit called TRLU is designed by a ReLU network; the piecewise constant approximation, such as Haar wavelets, is implemented by rectifying the linear output of a ReLU network via TRLUs. New interpretations of deep layers, as well as some other results, are also presented.},
  archive      = {J_NECO},
  author       = {Huang, Changcun},
  doi          = {10.1162/neco_a_01316},
  journal      = {Neural Computation},
  number       = {11},
  pages        = {2249-2278},
  shortjournal = {Neural Comput.},
  title        = {ReLU networks are universal approximators via piecewise linear or constant functions},
  volume       = {32},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020a). Bicomplex projection rule for complex-valued hopfield
neural networks. <em>NECO</em>, <em>32</em>(11), 2237–2248. (<a
href="https://doi.org/10.1162/neco_a_01320">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A complex-valued Hopfield neural network (CHNN) with a multistate activation function is a multistate model of neural associative memory. The weight parameters need a lot of memory resources. Twin-multistate activation functions were introduced to quaternion- and bicomplex-valued Hopfield neural networks. Since their architectures are much more complicated than that of CHNN, the architecture should be simplified. In this work, the number of weight parameters is reduced by bicomplex projection rule for CHNNs, which is given by the decomposition of bicomplex-valued Hopfield neural networks. Computer simulations support that the noise tolerance of CHNN with a bicomplex projection rule is equal to or even better than that of quaternion- and bicomplex-valued Hopfield neural networks. By computer simulations, we find that the projection rule for hyperbolic-valued Hopfield neural networks in synchronous mode maintains a high noise tolerance.},
  archive      = {J_NECO},
  author       = {Kobayashi, Masaki},
  doi          = {10.1162/neco_a_01320},
  journal      = {Neural Computation},
  number       = {11},
  pages        = {2237-2248},
  shortjournal = {Neural Comput.},
  title        = {Bicomplex projection rule for complex-valued hopfield neural networks},
  volume       = {32},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Repetitive control for multi-joint arm movements based on
virtual trajectories. <em>NECO</em>, <em>32</em>(11), 2212–2236. (<a
href="https://doi.org/10.1162/neco_a_01322">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {According to the neuromuscular model of virtual trajectory control, the postures and movements of limbs are performed by shifting the equilibrium positions determined by agonist and antagonist muscle activities. In this study, we develop virtual trajectory control for the reaching movements of a multi-joint arm, introducing a proportional-derivative feedback control scheme. In virtual trajectory control, it is crucial to design a suitable virtual trajectory such that the desired trajectory can be realized. To this end, we propose an algorithm for updating virtual trajectories in repetitive control, which can be regarded as a Newton-like method in a function space. In our repetitive control, the virtual trajectory is corrected without explicit calculation of the arm dynamics, and the actual trajectory converges to the desired trajectory. Using computer simulations, we assessed the proposed repetitive control for the trajectory tracking of a two-link arm. Our results confirmed that when the feedback gains were reasonably high and the sampling time was sufficiently small, the virtual trajectory was adequately updated, and the desired trajectory was almost achieved within approximately 10 iterative trials. We also propose a method for modifying the virtual trajectory to ensure that the formation of the actual trajectory is identical even when the feedback gains are changed. This modification method makes it possible to execute flexible control, in which the feedback gains are effectively altered according to motion tasks.},
  archive      = {J_NECO},
  author       = {Uno, Yoji and Suzuki, Takehiro and Kagawa, Takahiro},
  doi          = {10.1162/neco_a_01322},
  journal      = {Neural Computation},
  number       = {11},
  pages        = {2212-2236},
  shortjournal = {Neural Comput.},
  title        = {Repetitive control for multi-joint arm movements based on virtual trajectories},
  volume       = {32},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Inferring neuronal couplings from spiking data using a
systematic procedure with a statistical criterion. <em>NECO</em>,
<em>32</em>(11), 2187–2211. (<a
href="https://doi.org/10.1162/neco_a_01324">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent remarkable advances in experimental techniques have provided a background for inferring neuronal couplings from point process data that include a great number of neurons. Here, we propose a systematic procedure for pre- and postprocessing generic point process data in an objective manner to handle data in the framework of a binary simple statistical model, the Ising or generalized McCulloch–Pitts model. The procedure has two steps: (1) determining time bin size for transforming the point process data into discrete-time binary data and (2) screening relevant couplings from the estimated couplings. For the first step, we decide the optimal time bin size by introducing the null hypothesis that all neurons would fire independently, then choosing a time bin size so that the null hypothesis is rejected with the strict criteria. The likelihood associated with the null hypothesis is analytically evaluated and used for the rejection process. For the second postprocessing step, after a certain estimator of coupling is obtained based on the preprocessed data set (any estimator can be used with the proposed procedure), the estimate is compared with many other estimates derived from data sets obtained by randomizing the original data set in the time direction. We accept the original estimate as relevant only if its absolute value is sufficiently larger than those of randomized data sets. These manipulations suppress false positive couplings induced by statistical noise. We apply this inference procedure to spiking data from synthetic and in vitro neuronal networks. The results show that the proposed procedure identifies the presence or absence of synaptic couplings fairly well, including their signs, for the synthetic and experimental data. In particular, the results support that we can infer the physical connections of underlying systems in favorable situations, even when using a simple statistical model.},
  archive      = {J_NECO},
  author       = {Terada, Yu and Obuchi, Tomoyuki and Isomura, Takuya and Kabashima, Yoshiyuki},
  doi          = {10.1162/neco_a_01324},
  journal      = {Neural Computation},
  number       = {11},
  pages        = {2187-2211},
  shortjournal = {Neural Comput.},
  title        = {Inferring neuronal couplings from spiking data using a systematic procedure with a statistical criterion},
  volume       = {32},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Assessing goodness-of-fit in marked point process models of
neural population coding via time and rate rescaling. <em>NECO</em>,
<em>32</em>(11), 2145–2186. (<a
href="https://doi.org/10.1162/neco_a_01321">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Marked point process models have recently been used to capture the coding properties of neural populations from multiunit electrophysiological recordings without spike sorting. These clusterless models have been shown in some instances to better describe the firing properties of neural populations than collections of receptive field models for sorted neurons and to lead to better decoding results. To assess their quality, we previously proposed a goodness-of-fit technique for marked point process models based on time rescaling, which for a correct model produces a set of uniform samples over a random region of space. However, assessing uniformity over such a region can be challenging, especially in high dimensions. Here, we propose a set of new transformations in both time and the space of spike waveform features, which generate events that are uniformly distributed in the new mark and time spaces. These transformations are scalable to multidimensional mark spaces and provide uniformly distributed samples in hypercubes, which are well suited for uniformity tests. We discuss the properties of these transformations and demonstrate aspects of model fit captured by each transformation. We also compare multiple uniformity tests to determine their power to identify lack-of-fit in the rescaled data. We demonstrate an application of these transformations and uniformity tests in a simulation study. Proofs for each transformation are provided in the appendix.},
  archive      = {J_NECO},
  author       = {Yousefi, Ali and Amidi, Yalda and Nazari, Behzad and Eden, Uri. T.},
  doi          = {10.1162/neco_a_01321},
  journal      = {Neural Computation},
  number       = {11},
  pages        = {2145-2186},
  shortjournal = {Neural Comput.},
  title        = {Assessing goodness-of-fit in marked point process models of neural population coding via time and rate rescaling},
  volume       = {32},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Closed-loop deep learning: Generating forward models with
backpropagation. <em>NECO</em>, <em>32</em>(11), 2122–2144. (<a
href="https://doi.org/10.1162/neco_a_01317">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A reflex is a simple closed-loop control approach that tries to minimize an error but fails to do so because it will always react too late. An adaptive algorithm can use this error to learn a forward model with the help of predictive cues. For example, a driver learns to improve steering by looking ahead to avoid steering in the last minute. In order to process complex cues such as the road ahead, deep learning is a natural choice. However, this is usually achieved only indirectly by employing deep reinforcement learning having a discrete state space. Here, we show how this can be directly achieved by embedding deep learning into a closed-loop system and preserving its continuous processing. We show in z-space specifically how error backpropagation can be achieved and in general how gradient-based approaches can be analyzed in such closed-loop scenarios. The performance of this learning paradigm is demonstrated using a line follower in simulation and on a real robot that shows very fast and continuous learning.},
  archive      = {J_NECO},
  author       = {Daryanavard, Sama and Porr, Bernd},
  doi          = {10.1162/neco_a_01317},
  journal      = {Neural Computation},
  number       = {11},
  pages        = {2122-2144},
  shortjournal = {Neural Comput.},
  title        = {Closed-loop deep learning: Generating forward models with backpropagation},
  volume       = {32},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Reverse-engineering neural networks to characterize their
cost functions. <em>NECO</em>, <em>32</em>(11), 2085–2121. (<a
href="https://doi.org/10.1162/neco_a_01315">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This letter considers a class of biologically plausible cost functions for neural networks, where the same cost function is minimized by both neural activity and plasticity. We show that such cost functions can be cast as a variational bound on model evidence under an implicit generative model. Using generative models based on partially observed Markov decision processes (POMDP), we show that neural activity and plasticity perform Bayesian inference and learning, respectively, by maximizing model evidence. Using mathematical and numerical analyses, we establish the formal equivalence between neural network cost functions and variational free energy under some prior beliefs about latent states that generate inputs. These prior beliefs are determined by particular constants (e.g., thresholds) that define the cost function. This means that the Bayes optimal encoding of latent or hidden states is achieved when the network&#39;s implicit priors match the process that generates its inputs. This equivalence is potentially important because it suggests that any hyperparameter of a neural network can itself be optimized—by minimization with respect to variational free energy. Furthermore, it enables one to characterize a neural network formally, in terms of its prior beliefs.},
  archive      = {J_NECO},
  author       = {Isomura, Takuya and Friston, Karl},
  doi          = {10.1162/neco_a_01315},
  journal      = {Neural Computation},
  number       = {11},
  pages        = {2085-2121},
  shortjournal = {Neural Comput.},
  title        = {Reverse-engineering neural networks to characterize their cost functions},
  volume       = {32},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A cerebellar computational mechanism for delay conditioning
at precise time intervals. <em>NECO</em>, <em>32</em>(11), 2069–2084.
(<a href="https://doi.org/10.1162/neco_a_01318">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The cerebellum is known to have an important role in sensing and execution of precise time intervals, but the mechanism by which arbitrary time intervals can be recognized and replicated with high precision is unknown. We propose a computational model in which precise time intervals can be identified from the pattern of individual spike activity in a population of parallel fibers in the cerebellar cortex. The model depends on the presence of repeatable sequences of spikes in response to conditioned stimulus input. We emulate granule cells using a population of Izhikevich neuron approximations driven by random but repeatable mossy fiber input. We emulate long-term depression (LTD) and long-term potentiation (LTP) synaptic plasticity at the parallel fiber to Purkinje cell synapse. We simulate a delay conditioning paradigm with a conditioned stimulus (CS) presented to the mossy fibers and an unconditioned stimulus (US) some time later issued to the Purkinje cells as a teaching signal. We show that Purkinje cells rapidly adapt to decrease firing probability following onset of the CS only at the interval for which the US had occurred. We suggest that detection of replicable spike patterns provides an accurate and easily learned timing structure that could be an important mechanism for behaviors that require identification and production of precise time intervals.},
  archive      = {J_NECO},
  author       = {Sanger, Terence D. and Kawato, Mitsuo},
  doi          = {10.1162/neco_a_01318},
  journal      = {Neural Computation},
  number       = {11},
  pages        = {2069-2084},
  shortjournal = {Neural Comput.},
  title        = {A cerebellar computational mechanism for delay conditioning at precise time intervals},
  volume       = {32},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Active learning for enumerating local minima based on
gaussian process derivatives. <em>NECO</em>, <em>32</em>(10), 2032–2068.
(<a href="https://doi.org/10.1162/neco_a_01307">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study active learning (AL) based on gaussian processes (GPs) for efficiently enumerating all of the local minimum solutions of a black-box function. This problem is challenging because local solutions are characterized by their zero gradient and positive-definite Hessian properties, but those derivatives cannot be directly observed. We propose a new AL method in which the input points are sequentially selected such that the confidence intervals of the GP derivatives are effectively updated for enumerating local minimum solutions. We theoretically analyze the proposed method and demonstrate its usefulness through numerical experiments.},
  archive      = {J_NECO},
  author       = {Inatsu, Yu and Sugita, Daisuke and Toyoura, Kazuaki and Takeuchi, Ichiro},
  doi          = {10.1162/neco_a_01307},
  journal      = {Neural Computation},
  number       = {10},
  pages        = {2032-2068},
  shortjournal = {Neural Comput.},
  title        = {Active learning for enumerating local minima based on gaussian process derivatives},
  volume       = {32},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Active learning of bayesian linear models with
high-dimensional binary features by parameter confidence-region
estimation. <em>NECO</em>, <em>32</em>(10), 1998–2031. (<a
href="https://doi.org/10.1162/neco_a_01310">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this letter, we study an active learning problem for maximizing an unknown linear function with high-dimensional binary features. This problem is notoriously complex but arises in many important contexts. When the sampling budget, that is, the number of possible function evaluations, is smaller than the number of dimensions, it tends to be impossible to identify all of the optimal binary features. Therefore, in practice, only a small number of such features are considered, with the majority kept fixed at certain default values, which we call the working set heuristic . The main contribution of this letter is to formally study the working set heuristic and present a suite of theoretically robust algorithms for more efficient use of the sampling budget. Technically, we introduce a novel method for estimating the confidence regions of model parameters that is tailored to active learning with high-dimensional binary features. We provide a rigorous theoretical analysis of these algorithms and prove that a commonly used working set heuristic can identify optimal binary features with favorable sample complexity. We explore the performance of the proposed approach through numerical simulations and an application to a functional protein design problem.},
  archive      = {J_NECO},
  author       = {Inatsu, Yu and Karasuyama, Masayuki and Inoue, Keiichi and Kandori, Hideki and Takeuchi, Ichiro},
  doi          = {10.1162/neco_a_01310},
  journal      = {Neural Computation},
  number       = {10},
  pages        = {1998-2031},
  shortjournal = {Neural Comput.},
  title        = {Active learning of bayesian linear models with high-dimensional binary features by parameter confidence-region estimation},
  volume       = {32},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Analysis of regression algorithms with unbounded sampling.
<em>NECO</em>, <em>32</em>(10), 1980–1997. (<a
href="https://doi.org/10.1162/neco_a_01313">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this letter, we study a class of the regularized regression algorithms when the sampling process is unbounded. By choosing different loss functions, the learning algorithms can include a wide range of commonly used algorithms for regression. Unlike the prior work on theoretical analysis of unbounded sampling, no constraint on the output variables is specified in our setting. By an elegant error analysis, we prove consistency and finite sample bounds on the excess risk of the proposed algorithms under regular conditions.},
  archive      = {J_NECO},
  author       = {Tong, Hongzhi and Gao, Jiajing},
  doi          = {10.1162/neco_a_01313},
  journal      = {Neural Computation},
  number       = {10},
  pages        = {1980-1997},
  shortjournal = {Neural Comput.},
  title        = {Analysis of regression algorithms with unbounded sampling},
  volume       = {32},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Multiview alignment and generation in CCA via consistent
latent encoding. <em>NECO</em>, <em>32</em>(10), 1936–1979. (<a
href="https://doi.org/10.1162/neco_a_01309">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multiview alignment, achieving one-to-one correspondence of multiview inputs, is critical in many real-world multiview applications, especially for cross-view data analysis problems. An increasing amount of work has studied this alignment problem with canonical correlation analysis (CCA). However, existing CCA models are prone to misalign the multiple views due to either the neglect of uncertainty or the inconsistent encoding of the multiple views. To tackle these two issues, this letter studies multiview alignment from a Bayesian perspective. Delving into the impairments of inconsistent encodings, we propose to recover correspondence of the multiview inputs by matching the marginalization of the joint distribution of multiview random variables under different forms of factorization. To realize our design, we present adversarial CCA (ACCA), which achieves consistent latent encodings by matching the marginalized latent encodings through the adversarial training paradigm. Our analysis, based on conditional mutual information, reveals that ACCA is flexible for handling implicit distributions. Extensive experiments on correlation analysis and cross-view generation under noisy input settings demonstrate the superiority of our model.},
  archive      = {J_NECO},
  author       = {Shi, Yaxin and Pan, Yuangang and Xu, Donna and Tsang, Ivor W.},
  doi          = {10.1162/neco_a_01309},
  journal      = {Neural Computation},
  number       = {10},
  pages        = {1936-1979},
  shortjournal = {Neural Comput.},
  title        = {Multiview alignment and generation in CCA via consistent latent encoding},
  volume       = {32},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Modal principal component analysis. <em>NECO</em>,
<em>32</em>(10), 1901–1935. (<a
href="https://doi.org/10.1162/neco_a_01308">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Principal component analysis (PCA) is a widely used method for data processing, such as for dimension reduction and visualization. Standard PCA is known to be sensitive to outliers, and various robust PCA methods have been proposed. It has been shown that the robustness of many statistical methods can be improved using mode estimation instead of mean estimation, because mode estimation is not significantly affected by the presence of outliers. Thus, this study proposes a modal principal component analysis (MPCA), which is a robust PCA method based on mode estimation. The proposed method finds the minor component by estimating the mode of the projected data points. As a theoretical contribution, probabilistic convergence property, influence function, finite-sample breakdown point, and its lower bound for the proposed MPCA are derived. The experimental results show that the proposed method has advantages over conventional methods.},
  archive      = {J_NECO},
  author       = {Sando, Keishi and Hino, Hideitsu},
  doi          = {10.1162/neco_a_01308},
  journal      = {Neural Computation},
  number       = {10},
  pages        = {1901-1935},
  shortjournal = {Neural Comput.},
  title        = {Modal principal component analysis},
  volume       = {32},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Binless kernel machine: Modeling spike train transformation
for cognitive neural prostheses. <em>NECO</em>, <em>32</em>(10),
1863–1900. (<a href="https://doi.org/10.1162/neco_a_01306">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Modeling spike train transformation among brain regions helps in designing a cognitive neural prosthesis that restores lost cognitive functions. Various methods analyze the nonlinear dynamic spike train transformation between two cortical areas with low computational eficiency. The application of a real-time neural prosthesis requires computational eficiency, performance stability, and better interpretation of the neural firing patterns that modulate target spike generation. We propose the binless kernel machine in the point-process framework to describe nonlinear dynamic spike train transformations. Our approach embeds the binless kernel to eficiently capture the feedforward dynamics of spike trains and maps the input spike timings into reproducing kernel Hilbert space (RKHS). An inhomogeneous Bernoulli process is designed to combine with a kernel logistic regression that operates on the binless kernel to generate an output spike train as a point process. Weights of the proposed model are estimated by maximizing the log likelihood of output spike trains in RKHS, which allows a global-optimal solution. To reduce computational complexity, we design a streaming-based clustering algorithm to extract typical and important spike train features. The cluster centers and their weights enable the visualization of the important input spike train patterns that motivate or inhibit output neuron firing. We test the proposed model on both synthetic data and real spike train data recorded from the dorsal premotor cortex and the primary motor cortex of a monkey performing a center-out task. Performances are evaluated by discrete-time rescaling Kolmogorov-Smirnov tests. Our model outperforms the existing methods with higher stability regardless of weight initialization and demonstrates higher eficiency in analyzing neural patterns from spike timing with less historical input (50\%). Meanwhile, the typical spike train patterns selected according to weights are validated to encode output spike from the spike train of single-input neuron and the interaction of two input neurons.},
  archive      = {J_NECO},
  author       = {Qian, Cunle and Sun, Xuyun and Wang, Yueming and Zheng, Xiaoxiang and Wang, Yiwen and Pan, Gang},
  doi          = {10.1162/neco_a_01306},
  journal      = {Neural Computation},
  number       = {10},
  pages        = {1863-1900},
  shortjournal = {Neural Comput.},
  title        = {Binless kernel machine: Modeling spike train transformation for cognitive neural prostheses},
  volume       = {32},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A predictive-coding network that is both discriminative and
generative. <em>NECO</em>, <em>32</em>(10), 1836–1862. (<a
href="https://doi.org/10.1162/neco_a_01311">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Predictive coding (PC) networks are a biologically interesting class of neural networks. Their layered hierarchy mimics the reciprocal connectivity pattern observed in the mammalian cortex, and they can be trained using local learning rules that approximate backpropagation (Bogacz, 2017 ). However, despite having feedback connections that enable information to flow down the network hierarchy, discriminative PC networks are not typically generative. Clamping the output class and running the network to equilibrium yields an input sample that usually does not resemble the training input. This letter studies this phenomenon and proposes a simple solution that promotes the generation of input samples that resemble the training inputs. Simple decay, a technique already in wide use in neural networks, pushes the PC network toward a unique minimum two-norm solution, and that unique solution provably (for linear networks) matches the training inputs. The method also vastly improves the samples generated for nonlinear networks, as we demonstrate on MNIST.},
  archive      = {J_NECO},
  author       = {Sun, Wei and Orchard, Jeff},
  doi          = {10.1162/neco_a_01311},
  journal      = {Neural Computation},
  number       = {10},
  pages        = {1836-1862},
  shortjournal = {Neural Comput.},
  title        = {A predictive-coding network that is both discriminative and generative},
  volume       = {32},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Fast and accurate langevin simulations of stochastic
hodgkin-huxley dynamics. <em>NECO</em>, <em>32</em>(10), 1775–1835. (<a
href="https://doi.org/10.1162/neco_a_01312">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fox and Lu introduced a Langevin framework for discrete-time stochastic models of randomly gated ion channels such as the Hodgkin-Huxley (HH) system. They derived a Fokker-Planck equation with state-dependent diffusion tensor D and suggested a Langevin formulation with noise coefficient matrix S such that SS ⊤ = D ⁠ . Subsequently, several authors introduced a variety of Langevin equations for the HH system. In this article, we present a natural 14-dimensional dynamics for the HH system in which each directed edge in the ion channel state transition graph acts as an independent noise source, leading to a 14 × 28 noise coefficient matrix S ⁠ . We show that (1) the corresponding 14D system of ordinary differential equations is consistent with the classical 4D representation of the HH system; (2) the 14D representation leads to a noise coefficient matrix S that can be obtained cheaply on each time step, without requiring a matrix decomposition; (3) sample trajectories of the 14D representation are pathwise equivalent to trajectories of Fox and Lu&#39;s system, as well as trajectories of several existing Langevin models; (4) our 14D representation (and those equivalent to it) gives the most accurate interspike interval distribution, not only with respect to moments but under both the L 1 and L ∞ metric-space norms; and (5) the 14D representation gives an approximation to exact Markov chain simulations that are as fast and as efficient as all equivalent models. Our approach goes beyond existing models, in that it supports a stochastic shielding decomposition that dramatically simplifies S with minimal loss of accuracy under both voltage- and current-clamp conditions.},
  archive      = {J_NECO},
  author       = {Pu, Shusen and Thomas, Peter J.},
  doi          = {10.1162/neco_a_01312},
  journal      = {Neural Computation},
  number       = {10},
  pages        = {1775-1835},
  shortjournal = {Neural Comput.},
  title        = {Fast and accurate langevin simulations of stochastic hodgkin-huxley dynamics},
  volume       = {32},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Polynomial-time algorithms for multiple-arm identification
with full-bandit feedback. <em>NECO</em>, <em>32</em>(9), 1733–1773. (<a
href="https://doi.org/10.1162/neco_a_01299">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study the problem of stochastic multiple-arm identification, where an agent sequentially explores a size- k subset of arms (also known as a super arm ) from given n arms and tries to identify the best super arm. Most work so far has considered the semi-bandit setting, where the agent can observe the reward of each pulled arm or assumed each arm can be queried at each round. However, in real-world applications, it is costly or sometimes impossible to observe a reward of individual arms. In this study, we tackle the full-bandit setting, where only a noisy observation of the total sum of a super arm is given at each pull. Although our problem can be regarded as an instance of the best arm identification in linear bandits, a naive approach based on linear bandits is computationally infeasible since the number of super arms K is exponential. To cope with this problem, we first design a polynomial-time approximation algorithm for a 0-1 quadratic programming problem arising in confidence ellipsoid maximization. Based on our approximation algorithm, we propose a bandit algorithm whose computation time is O (log K ⁠ ), thereby achieving an exponential speedup over linear bandit algorithms. We provide a sample complexity upper bound that is still worst-case optimal. Finally, we conduct experiments on large-scale data sets with more than 10 10 super arms, demonstrating the superiority of our algorithms in terms of both the computation time and the sample complexity.},
  archive      = {J_NECO},
  author       = {Kuroki, Yuko and Xu, Liyuan and Miyauchi, Atsushi and Honda, Junya and Sugiyama, Masashi},
  doi          = {10.1162/neco_a_01299},
  journal      = {Neural Computation},
  number       = {9},
  pages        = {1733-1773},
  shortjournal = {Neural Comput.},
  title        = {Polynomial-time algorithms for multiple-arm identification with full-bandit feedback},
  volume       = {32},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Tensor least angle regression for sparse representations of
multidimensional signals. <em>NECO</em>, <em>32</em>(9), 1697–1732. (<a
href="https://doi.org/10.1162/neco_a_01304">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sparse signal representations have gained much interest recently in both signal processing and statistical communities. Compared to orthogonal matching pursuit (OMP) and basis pursuit, which solve the L 0 and L 1 constrained sparse least-squares problems, respectively, least angle regression (LARS) is a computationally efficient method to solve both problems for all critical values of the regularization parameter λ ⁠ . However, all of these methods are not suitable for solving large multidimensional sparse least-squares problems, as they would require extensive computational power and memory. An earlier generalization of OMP, known as Kronecker-OMP, was developed to solve the L 0 problem for large multidimensional sparse least-squares problems. However, its memory usage and computation time increase quickly with the number of problem dimensions and iterations. In this letter, we develop a generalization of LARS, tensor least angle regression (T-LARS) that could efficiently solve either large L 0 or large L 1 constrained multidimensional, sparse, least-squares problems (underdetermined or overdetermined) for all critical values of the regularization parameter λ and with lower computational complexity and memory usage than Kronecker-OMP. To demonstrate the validity and performance of our T-LARS algorithm, we used it to successfully obtain different sparse representations of two relatively large 3D brain images, using fixed and learned separable overcomplete dictionaries, by solving both L 0 and L 1 constrained sparse least-squares problems. Our numerical experiments demonstrate that our T-LARS algorithm is significantly faster (46 to 70 times) than Kronecker-OMP in obtaining K -sparse solutions for multilinear leastsquares problems. However, the K -sparse solutions obtained using Kronecker-OMP always have a slightly lower residual error (1.55\% to 2.25\%) than ones obtained by T-LARS. Therefore, T-LARS could be an important tool for numerous multidimensional biomedical signal processing applications.},
  archive      = {J_NECO},
  author       = {Wickramasingha, Ishan and Elrewainy, Ahmed and Sobhy, Michael and Sherif, Sherif S.},
  doi          = {10.1162/neco_a_01304},
  journal      = {Neural Computation},
  number       = {9},
  pages        = {1697-1732},
  shortjournal = {Neural Comput.},
  title        = {Tensor least angle regression for sparse representations of multidimensional signals},
  volume       = {32},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020b). Hyperbolic-valued hopfield neural networks in synchronous
mode. <em>NECO</em>, <em>32</em>(9), 1685–1696. (<a
href="https://doi.org/10.1162/neco_a_01303">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For most multistate Hopfield neural networks, the stability conditions in asynchronous mode are known, whereas those in synchronous mode are not. If they were to converge in synchronous mode, recall would be accelerated by parallel processing. Complex-valued Hopfield neural networks (CHNNs) with a projection rule do not converge in synchronous mode. In this work, we provide stability conditions for hyperbolic Hopfield neural networks (HHNNs) in synchronous mode instead of CHNNs. HHNNs provide better noise tolerance than CHNNs. In addition, the stability conditions are applied to the projection rule, and HHNNs with a projection rule converge in synchronous mode. By computer simulations, we find that the projection rule for HHNNs in synchronous mode maintains a high noise tolerance.},
  archive      = {J_NECO},
  author       = {Kobayashi, Masaki},
  doi          = {10.1162/neco_a_01303},
  journal      = {Neural Computation},
  number       = {9},
  pages        = {1685-1696},
  shortjournal = {Neural Comput.},
  title        = {Hyperbolic-valued hopfield neural networks in synchronous mode},
  volume       = {32},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Fine-grained 3D-attention prototypes for few-shot learning.
<em>NECO</em>, <em>32</em>(9), 1664–1684. (<a
href="https://doi.org/10.1162/neco_a_01302">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the real world, a limited number of labeled finely grained images per class can hardly represent the class distribution effectively. Due to the more subtle visual differences in fine-grained images than simple images with obvious objects, that is, there exist smaller interclass and larger intraclass variations. To solve these issues, we propose an end-to-end attention-based model for fine-grained few-shot image classification (AFG) with the recent episode training strategy. It is composed mainly of a feature learning module, an image reconstruction module, and a label distribution module. The feature learning module mainly devises a 3D-Attention mechanism, which considers both the spatial positions and different channel attentions of the image features, in order to learn more discriminative local features to better represent the class distribution. The image reconstruction module calculates the mappings between local features and the original images. It is constrained by a designed loss function as auxiliary supervised information, so that the learning of each local feature does not need extra annotations. The label distribution module is used to predict the label distribution of a given unlabeled sample, and we use the local features to represent the image features for classification. By conducting comprehensive experiments on Mini-ImageNet and three fine-grained data sets, we demonstrate that the proposed model achieves superior performance over the competitors.},
  archive      = {J_NECO},
  author       = {Hu, Xin and Liu, Jun and Ma, Jie and Pan, Yudai and Zhang, Lingling},
  doi          = {10.1162/neco_a_01302},
  journal      = {Neural Computation},
  number       = {9},
  pages        = {1664-1684},
  shortjournal = {Neural Comput.},
  title        = {Fine-grained 3D-attention prototypes for few-shot learning},
  volume       = {32},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Parallel neural multiprocessing with gamma frequency
latencies. <em>NECO</em>, <em>32</em>(9), 1635–1663. (<a
href="https://doi.org/10.1162/neco_a_01301">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Poisson variability in cortical neural responses has been typically modeled using spike averaging techniques, such as trial averaging and rate coding, since such methods can produce reliable correlates of behavior. However, mechanisms that rely on counting spikes could be slow and inefficient and thus might not be useful in the brain for computations at timescales in the 10 millisecond range. This issue has motivated a search for alternative spike codes that take advantage of spike timing and has resulted in many studies that use synchronized neural networks for communication. Here we focus on recent studies that suggest that the gamma frequency may provide a reference that allows local spike phase representations that could result in much faster information transmission. We have developed a unified model (gamma spike multiplexing) that takes advantage of a single cycle of a cell&#39;s somatic gamma frequency to modulate the generation of its action potentials. An important consequence of this coding mechanism is that it allows multiple independent neural processes to run in parallel, thereby greatly increasing the processing capability of the cortex. System-level simulations and preliminary analysis of mouse cortical cell data are presented as support for the proposed theoretical model.},
  archive      = {J_NECO},
  author       = {Zhang, Ruohan and Ballard, Dana H.},
  doi          = {10.1162/neco_a_01301},
  journal      = {Neural Computation},
  number       = {9},
  pages        = {1635-1663},
  shortjournal = {Neural Comput.},
  title        = {Parallel neural multiprocessing with gamma frequency latencies},
  volume       = {32},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A mean-field description of bursting dynamics in spiking
neural networks with short-term adaptation. <em>NECO</em>,
<em>32</em>(9), 1615–1634. (<a
href="https://doi.org/10.1162/neco_a_01300">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Bursting plays an important role in neural communication. At the population level, macroscopic bursting has been identified in populations of neurons that do not express intrinsic bursting mechanisms. For the analysis of phase transitions between bursting and non-bursting states, mean-field descriptions of macroscopic bursting behavior are a valuable tool. In this article, we derive mean-field descriptions of populations of spiking neurons and examine whether states of collective bursting behavior can arise from short-term adaptation mechanisms. Specifically, we consider synaptic depression and spike-frequency adaptation in networks of quadratic integrate-and-fire neurons. Analyzing the mean-field model via bifurcation analysis, we find that bursting behavior emerges for both types of short-term adaptation. This bursting behavior can coexist with steady-state behavior, providing a bistable regime that allows for transient switches between synchronized and nonsynchronized states of population dynamics. For all of these findings, we demonstrate a close correspondence between the spiking neural network and the mean-field model. Although the mean-field model has been derived under the assumptions of an infinite population size and all-to-all coupling inside the population, we show that this correspondence holds even for small, sparsely coupled networks. In summary, we provide mechanistic descriptions of phase transitions between bursting and steady-state population dynamics, which play important roles in both healthy neural communication and neurological disorders.},
  archive      = {J_NECO},
  author       = {Gast, Richard and Schmidt, Helmut and Knösche, Thomas R.},
  doi          = {10.1162/neco_a_01300},
  journal      = {Neural Computation},
  number       = {9},
  pages        = {1615-1634},
  shortjournal = {Neural Comput.},
  title        = {A mean-field description of bursting dynamics in spiking neural networks with short-term adaptation},
  volume       = {32},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Theory and algorithms for shapelet-based multiple-instance
learning. <em>NECO</em>, <em>32</em>(8), 1580–1613. (<a
href="https://doi.org/10.1162/neco_a_01297">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a new formulation of multiple-instance learning (MIL), in which a unit of data consists of a set of instances called a bag. The goal is to find a good classifier of bags based on the similarity with a “shapelet” (or pattern), where the similarity of a bag with a shapelet is the maximum similarity of instances in the bag. In previous work, some of the training instances have been chosen as shapelets with no theoretical justification. In our formulation, we use all possible, and thus infinitely many, shapelets, resulting in a richer class of classifiers. We show that the formulation is tractable, that is, it can be reduced through linear programming boosting (LPBoost) to difference of convex (DC) programs of finite (actually polynomial) size. Our theoretical result also gives justification to the heuristics of some previous work. The time complexity of the proposed algorithm highly depends on the size of the set of all instances in the training sample. To apply to the data containing a large number of instances, we also propose a heuristic option of the algorithm without the loss of the theoretical guarantee. Our empirical study demonstrates that our algorithm uniformly works for shapelet learning tasks on time-series classification and various MIL tasks with comparable accuracy to the existing methods. Moreover, we show that the proposed heuristics allow us to achieve the result in reasonable computational time.},
  archive      = {J_NECO},
  author       = {Suehiro, Daiki and Hatano, Kohei and Takimoto, Eiji and Yamamoto, Shuji and Bannai, Kenichi and Takeda, Akiko},
  doi          = {10.1162/neco_a_01297},
  journal      = {Neural Computation},
  number       = {8},
  pages        = {1580-1613},
  shortjournal = {Neural Comput.},
  title        = {Theory and algorithms for shapelet-based multiple-instance learning},
  volume       = {32},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). On a scalable entropic breaching of the overfitting barrier
for small data problems in machine learning. <em>NECO</em>,
<em>32</em>(8), 1563–1579. (<a
href="https://doi.org/10.1162/neco_a_01296">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Overfitting and treatment of small data are among the most challenging problems in machine learning (ML), when a relatively small data statistics size T is not enough to provide a robust ML fit for a relatively large data feature dimension D ⁠ . Deploying a massively parallel ML analysis of generic classification problems for different D and T ⁠ , we demonstrate the existence of statistically significant linear overfitting barriers for common ML methods. The results reveal that for a robust classification of bioinformatics-motivated generic problems with the long short-term memory deep learning classifier (LSTM), one needs in the best case a statistics T that is at least 13.8 times larger than the feature dimension D ⁠ . We show that this overfitting barrier can be breached at a 10 - 12 fraction of the computational cost by means of the entropy-optimal scalable probabilistic approximations algorithm (eSPA), performing a joint solution of the entropy-optimal Bayesian network inference and feature space segmentation problems. Application of eSPA to experimental single cell RNA sequencing data exhibits a 30-fold classification performance boost when compared to standard bioinformatics tools and a 7-fold boost when compared to the deep learning LSTM classifier.},
  archive      = {J_NECO},
  author       = {Horenko, Illia},
  doi          = {10.1162/neco_a_01296},
  journal      = {Neural Computation},
  number       = {8},
  pages        = {1563-1579},
  shortjournal = {Neural Comput.},
  title        = {On a scalable entropic breaching of the overfitting barrier for small data problems in machine learning},
  volume       = {32},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A discrete-time neurodynamic approach to
sparsity-constrained nonnegative matrix factorization. <em>NECO</em>,
<em>32</em>(8), 1531–1562. (<a
href="https://doi.org/10.1162/neco_a_01294">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sparsity is a desirable property in many nonnegative matrix factorization (NMF) applications. Although some level of sparseness of NMF solutions can be achieved by using regularization, the resulting sparsity depends highly on the regularization parameter to be valued in an ad hoc way. In this letter we formulate sparse NMF as a mixed-integer optimization problem with sparsity as binary constraints. A discrete-time projection neural network is developed for solving the formulated problem. Sufficient conditions for its stability and convergence are analytically characterized by using Lyapunov&#39;s method. Experimental results on sparse feature extraction are discussed to substantiate the superiority of this approach to extracting highly sparse features.},
  archive      = {J_NECO},
  author       = {Li, Xinqi and Wang, Jun and Kwong, Sam},
  doi          = {10.1162/neco_a_01294},
  journal      = {Neural Computation},
  number       = {8},
  pages        = {1531-1562},
  shortjournal = {Neural Comput.},
  title        = {A discrete-time neurodynamic approach to sparsity-constrained nonnegative matrix factorization},
  volume       = {32},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Stochastic multichannel ranking with brain dynamics
preferences. <em>NECO</em>, <em>32</em>(8), 1499–1530. (<a
href="https://doi.org/10.1162/neco_a_01293">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A driver&#39;s cognitive state of mental fatigue significantly affects his or her driving performance and more important, public safety. Previous studies have leveraged reaction time (RT) as the metric for mental fatigue and aim at estimating the exact value of RT using electroencephalogram (EEG) signals within a regression model. However, due to the easily corrupted and also nonsmooth properties of RTs during data collection, methods focusing on predicting the exact value of a noisy measurement, RT generally suffer from poor generalization performance. Considering that human RT is the reflection of brain dynamics preference (BDP) rather than a single regression output of EEG signals, we propose a novel channel-reliability-aware ranking (CArank) model for the multichannel ranking problem. CArank learns from BDPs using EEG data robustly and aims at preserving the ordering corresponding to RTs. In particular, we introduce a transition matrix to characterize the reliability of each channel used in the EEG data, which helps in learning with BDPs only from informative EEG channels. To handle large-scale EEG signals, we propose a stochastic-generalized expectation maximum (SGEM) algorithm to update CArank in an online fashion. Comprehensive empirical analysis on EEG signals from 40 participants shows that our CArank achieves substantial improvements in reliability while simultaneously detecting noisy or less informative EEG channels.},
  archive      = {J_NECO},
  author       = {Pan, Yuangang and Tsang, Ivor W. and Singh, Avinash K. and Lin, Chin-Teng and Sugiyama, Masashi},
  doi          = {10.1162/neco_a_01293},
  journal      = {Neural Computation},
  number       = {8},
  pages        = {1499-1530},
  shortjournal = {Neural Comput.},
  title        = {Stochastic multichannel ranking with brain dynamics preferences},
  volume       = {32},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Inference of a mesoscopic population model from population
spike trains. <em>NECO</em>, <em>32</em>(8), 1448–1498. (<a
href="https://doi.org/10.1162/neco_a_01292">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Understanding how rich dynamics emerge in neural populations requires models exhibiting a wide range of behaviors while remaining interpretable in terms of connectivity and single-neuron dynamics. However, it has been challenging to fit such mechanistic spiking networks at the single-neuron scale to empirical population data. To close this gap, we propose to fit such data at a mesoscale, using a mechanistic but low-dimensional and, hence, statistically tractable model. The mesoscopic representation is obtained by approximating a population of neurons as multiple homogeneous pools of neurons and modeling the dynamics of the aggregate population activity within each pool. We derive the likelihood of both single-neuron and connectivity parameters given this activity, which can then be used to optimize parameters by gradient ascent on the log likelihood or perform Bayesian inference using Markov chain Monte Carlo (MCMC) sampling. We illustrate this approach using a model of generalized integrate-and-fire neurons for which mesoscopic dynamics have been previously derived and show that both single-neuron and connectivity parameters can be recovered from simulated data. In particular, our inference method extracts posterior correlations between model parameters, which define parameter subsets able to reproduce the data. We compute the Bayesian posterior for combinations of parameters using MCMC sampling and investigate how the approximations inherent in a mesoscopic population model affect the accuracy of the inferred single-neuron parameters.},
  archive      = {J_NECO},
  author       = {René, Alexandre and Longtin, André and Macke, Jakob H.},
  doi          = {10.1162/neco_a_01292},
  journal      = {Neural Computation},
  number       = {8},
  pages        = {1448-1498},
  shortjournal = {Neural Comput.},
  title        = {Inference of a mesoscopic population model from population spike trains},
  volume       = {32},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Any target function exists in a neighborhood of any
sufficiently wide random network: A geometrical perspective.
<em>NECO</em>, <em>32</em>(8), 1431–1447. (<a
href="https://doi.org/10.1162/neco_a_01295">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {It is known that any target function is realized in a sufficiently small neighborhood of any randomly connected deep network, provided the width (the number of neurons in a layer) is sufficiently large. There are sophisticated analytical theories and discussions concerning this striking fact, but rigorous theories are very complicated. We give an elementary geometrical proof by using a simple model for the purpose of elucidating its structure. We show that high-dimensional geometry plays a magical role. When we project a high-dimensional sphere of radius 1 to a low-dimensional subspace, the uniform distribution over the sphere shrinks to a gaussian distribution with negligibly small variances and covariances.},
  archive      = {J_NECO},
  author       = {Amari, Shun-ichi},
  doi          = {10.1162/neco_a_01295},
  journal      = {Neural Computation},
  number       = {8},
  pages        = {1431-1447},
  shortjournal = {Neural Comput.},
  title        = {Any target function exists in a neighborhood of any sufficiently wide random network: A geometrical perspective},
  volume       = {32},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Minimal spiking neuron for solving multilabel classification
tasks. <em>NECO</em>, <em>32</em>(7), 1408–1429. (<a
href="https://doi.org/10.1162/neco_a_01290">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The multispike tempotron (MST) is a powersul, single spiking neuron model that can solve complex supervised classification tasks. It is also internally complex, computationally expensive to evaluate, and unsuitable for neuromorphic hardware. Here we aim to understand whether it is possible to simplify the MST model while retaining its ability to learn and process information. To this end, we introduce a family of generalized neuron models (GNMs) that are a special case of the spike response model and much simpler and cheaper to simulate than the MST. We find that over a wide range of parameters, the GNM can learn at least as well as the MST does. We identify the temporal autocorrelation of the membrane potential as the most important ingredient of the GNM that enables it to classify multiple spatiotemporal patterns. We also interpret the GNM as a chemical system, thus conceptually bridging computation by neural networks with molecular information processing. We conclude the letter by proposing alternative training approaches for the GNM, including error trace learning and error backpropagation.},
  archive      = {J_NECO},
  author       = {Fil, Jakub and Chu, Dominique},
  doi          = {10.1162/neco_a_01290},
  journal      = {Neural Computation},
  number       = {7},
  pages        = {1408-1429},
  shortjournal = {Neural Comput.},
  title        = {Minimal spiking neuron for solving multilabel classification tasks},
  volume       = {32},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Generation of scale-invariant sequential activity in linear
recurrent networks. <em>NECO</em>, <em>32</em>(7), 1379–1407. (<a
href="https://doi.org/10.1162/neco_a_01288">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sequential neural activity has been observed in many parts of the brain and has been proposed as a neural mechanism for memory. The natural world expresses temporal relationships at a wide range of scales. Because we cannot know the relevant scales a priori, it is desirable that memory, and thus the generated sequences, is scale invariant. Although recurrent neural network models have been proposed as a mechanism for generating sequences, the requirements for scale-invariant sequences are not known. This letter reports the constraints that enable a linear recurrent neural network model to generate scale-invariant sequential activity. A straightforward eigendecomposition analysis results in two independent conditions that are required for scale invariance for connectivity matrices with real, distinct eigenvalues. First, the eigenvalues of the network must be geometrically spaced. Second, the eigenvectors must be related to one another via translation. These constraints are easily generalizable for matrices that have complex and distinct eigenvalues. Analogous albeit less compact constraints hold for matrices with degenerate eigenvalues. These constraints, along with considerations on initial conditions, provide a general recipe to build linear recurrent neural networks that support scale-invariant sequential activity.},
  archive      = {J_NECO},
  author       = {Liu, Yue and Howard, Marc W.},
  doi          = {10.1162/neco_a_01288},
  journal      = {Neural Computation},
  number       = {7},
  pages        = {1379-1407},
  shortjournal = {Neural Comput.},
  title        = {Generation of scale-invariant sequential activity in linear recurrent networks},
  volume       = {32},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Shapley homology: Topological analysis of sample influence
for neural networks. <em>NECO</em>, <em>32</em>(7), 1355–1378. (<a
href="https://doi.org/10.1162/neco_a_01289">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data samples collected for training machine learning models are typically assumed to be independent and identically distributed (i.i.d.). Recent research has demonstrated that this assumption can be problematic as it simplifies the manifold of structured data. This has motivated different research areas such as data poisoning, model improvement, and explanation of machine learning models. In this work, we study the influence of a sample on determining the intrinsic topological features of its underlying manifold. We propose the Shapley homology framework, which provides a quantitative metric for the influence of a sample of the homology of a simplicial complex. Our proposed framework consists of two main parts: homology analysis, where we compute the Betti number of the target topological space, and Shapley value calculation, where we decompose the topological features of a complex built from data points to individual points. By interpreting the influence as a probability measure, we further define an entropy that reflects the complexity of the data manifold. Furthermore, we provide a preliminary discussion of the connection of the Shapley homology to the Vapnik-Chervonenkis dimension. Empirical studies show that when the zero-dimensional Shapley homology is used on neighboring graphs, samples with higher influence scores have a greater impact on the accuracy of neural networks that determine graph connectivity and on several regular grammars whose higher entropy values imply greater difficulty in being learned.},
  archive      = {J_NECO},
  author       = {Zhang, Kaixuan and Wang, Qinglong and Liu, Xue and Giles, C. Lee},
  doi          = {10.1162/neco_a_01289},
  journal      = {Neural Computation},
  number       = {7},
  pages        = {1355-1378},
  shortjournal = {Neural Comput.},
  title        = {Shapley homology: Topological analysis of sample influence for neural networks},
  volume       = {32},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A mathematical analysis of memory lifetime in a simple
network model of memory. <em>NECO</em>, <em>32</em>(7), 1322–1354. (<a
href="https://doi.org/10.1162/neco_a_01286">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study the learning of an external signal by a neural network and the time to forget it when this network is submitted to noise. The presentation of an external stimulus to the recurrent network of binary neurons may change the state of the synapses. Multiple presentations of a unique signal lead to its learning. Then, during the forgetting time, the presentation of other signals (noise) may also modify the synaptic weights. We construct an estimator of the initial signal using the synaptic currents and in this way define a probability of error. In our model, these synaptic currents evolve as Markov chains. We study the dynamics of these Markov chains and obtain a lower bound on the number of external stimuli that the network can receive before the initial signal is considered forgotten (probability of error above a given threshold). Our results are based on a finite-time analysis rather than large-time asymptotic. We finally present numerical illustrations of our results.},
  archive      = {J_NECO},
  author       = {Helson, Pascal},
  doi          = {10.1162/neco_a_01286},
  journal      = {Neural Computation},
  number       = {7},
  pages        = {1322-1354},
  shortjournal = {Neural Comput.},
  title        = {A mathematical analysis of memory lifetime in a simple network model of memory},
  volume       = {32},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A model for the study of the increase in stimulus and change
point detection with small and variable spiking delays. <em>NECO</em>,
<em>32</em>(7), 1277–1321. (<a
href="https://doi.org/10.1162/neco_a_01285">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Precise timing of spikes between different neurons has been found to convey reliable information beyond the spike count. In contrast, the role of small and variable spiking delays, as reported, for example, in the visual cortex, remains largely unclear. This issue becomes particularly important considering the high speed of neuronal information processing, which is assumed to be based on only a few milliseconds within each processing step. We investigate the role of small and variable spiking delays with a parsimonious stochastic spiking model that is strongly motivated by experimental observations. The model contains only two parameters for the response of a neuron to one stimulus, describing directly the rate and the delay, or phase. Within the theoretical model, we specifically investigate two quantities, the probability of correct stimulus detection and the probability of correct change point detection, as a function of these parameters and within short periods of time. Optimal combinations of the two parameters across stimuli are derived that maximize these probabilities and enable comparison of pure rate, pure phase, and combined codes. In particular, the gain in correct detection probability when adding small and variable spiking delays to pure rate coding increases with the number of stimuli. More interesting, small and variable spiking delays can considerably improve the process of detecting changes in the stimulus, while also decreasing the probability of false alarms and thus increasing robustness and speed of change point detection. The results are compared to empirical spike train recordings of neurons in the visual cortex reported earlier in response to a number of visual stimuli. The results suggest that near-optimal combinations of rate and phase parameters may be implemented in the brain and that adding phase information could particularly increase the quality of change point detection in cases of highly similar stimuli.},
  archive      = {J_NECO},
  author       = {Straub, Benjamin and Schneider, Gaby},
  doi          = {10.1162/neco_a_01285},
  journal      = {Neural Computation},
  number       = {7},
  pages        = {1277-1321},
  shortjournal = {Neural Comput.},
  title        = {A model for the study of the increase in stimulus and change point detection with small and variable spiking delays},
  volume       = {32},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Heterogeneous synaptic weighting improves neural coding in
the presence of common noise. <em>NECO</em>, <em>32</em>(7), 1239–1276.
(<a href="https://doi.org/10.1162/neco_a_01287">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Simultaneous recordings from the cortex have revealed that neural activity is highly variable and that some variability is shared across neurons in a population. Further experimental work has demonstrated that the shared component of a neuronal population&#39;s variability is typically comparable to or larger than its private component. Meanwhile, an abundance of theoretical work has assessed the impact that shared variability has on a population code. For example, shared input noise is understood to have a detrimental impact on a neural population&#39;s coding fidelity. However, other contributions to variability, such as common noise, can also play a role in shaping correlated variability. We present a network of linear-nonlinear neurons in which we introduce a common noise input to model—for instance, variability resulting from upstream action potentials that are irrelevant to the task at hand. We show that by applying a heterogeneous set of synaptic weights to the neural inputs carrying the common noise, the network can improve its coding ability as measured by both Fisher information and Shannon mutual information, even in cases where this results in amplification of the common noise. With a broad and heterogeneous distribution of synaptic weights, a population of neurons can remove the harmful effects imposed by afferents that are uninformative about a stimulus. We demonstrate that some nonlinear networks benefit from weight diversification up to a certain population size, above which the drawbacks from amplified noise dominate over the benefits of diversification. We further characterize these benefits in terms of the relative strength of shared and private variability sources. Finally, we studied the asymptotic behavior of the mutual information and Fisher information analytically in our various networks as a function of population size. We find some surprising qualitative changes in the asymptotic behavior as we make seemingly minor changes in the synaptic weight distributions.},
  archive      = {J_NECO},
  author       = {Sachdeva, Pratik S. and Livezey, Jesse A. and DeWeese, Michael R.},
  doi          = {10.1162/neco_a_01287},
  journal      = {Neural Computation},
  number       = {7},
  pages        = {1239-1276},
  shortjournal = {Neural Comput.},
  title        = {Heterogeneous synaptic weighting improves neural coding in the presence of common noise},
  volume       = {32},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Salient slices: Improved neural network training and
performance with image entropy. <em>NECO</em>, <em>32</em>(6),
1222–1237. (<a href="https://doi.org/10.1162/neco_a_01282">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As a training and analysis strategy for convolutional neural networks (CNNs), we slice images into tiled segments and use, for training and prediction, segments that both satisfy an information criterion and contain sufficient content to support classification. In particular, we use image entropy as the information criterion. This ensures that each tile carries as much information diversity as the original image and, for many applications, serves as an indicator of usefulness in classification. To make predictions, a probability aggregation framework is applied to probabilities assigned by the CNN to the input image tiles. This technique, which we call Salient Slices, facilitates the use of large, high-resolution images that would be impractical to analyze unmodified; provides data augmentation for training, which is particularly valuable when image availability is limited; and the ensemble nature of the input for prediction enhances its accuracy.},
  archive      = {J_NECO},
  author       = {Frank, Steven J. and Frank, Andrea M.},
  doi          = {10.1162/neco_a_01282},
  journal      = {Neural Computation},
  number       = {6},
  pages        = {1222-1237},
  shortjournal = {Neural Comput.},
  title        = {Salient slices: Improved neural network training and performance with image entropy},
  volume       = {32},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Independently interpretable lasso for generalized linear
models. <em>NECO</em>, <em>32</em>(6), 1168–1221. (<a
href="https://doi.org/10.1162/neco_a_01279">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sparse regularization such as ℓ 1 regularization is a quite powerful and widely used strategy for high-dimensional learning problems. The effectiveness of sparse regularization has been supported practically and theoretically by several studies. However, one of the biggest issues in sparse regularization is that its performance is quite sensitive to correlations between features. Ordinary ℓ 1 regularization selects variables correlated with each other under weak regularizations, which results in deterioration of not only its estimation error but also interpretability. In this letter, we propose a new regularization method, independently interpretable lasso (IILasso), for generalized linear models. Our proposed regularizer suppresses selecting correlated variables, so that each active variable affects the response independently in the model. Hence, we can interpret regression coefficients intuitively, and the performance is also improved by avoiding overfitting. We analyze the theoretical property of the IILasso and show that the proposed method is advantageous for its sign recovery and achieves almost minimax optimal convergence rate. Synthetic and real data analyses also indicate the effectiveness of the IILasso.},
  archive      = {J_NECO},
  author       = {Takada, Masaaki and Suzuki, Taiji and Fujisawa, Hironori},
  doi          = {10.1162/neco_a_01279},
  journal      = {Neural Computation},
  number       = {6},
  pages        = {1168-1221},
  shortjournal = {Neural Comput.},
  title        = {Independently interpretable lasso for generalized linear models},
  volume       = {32},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Efficient position decoding methods based on fluorescence
calcium imaging in the mouse hippocampus. <em>NECO</em>, <em>32</em>(6),
1144–1167. (<a href="https://doi.org/10.1162/neco_a_01281">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Large-scale fluorescence calcium imaging methods have become widely adopted for studies of long-term hippocampal and cortical neuronal dynamics. Pyramidal neurons of the rodent hippocampus show spatial tuning in freely foraging or head-fixed navigation tasks. Development of efficient neural decoding methods for reconstructing the animal&#39;s position in real or virtual environments can provide a fast readout of spatial representations in closed-loop neuroscience experiments. Here, we develop an efficient strategy to extract features from fluorescence calcium imaging traces and further decode the animal&#39;s position. We validate our spike inference-free decoding methods in multiple in vivo calcium imaging recordings of the mouse hippocampus based on both supervised and unsupervised decoding analyses. We systematically investigate the decoding performance of our proposed methods with respect to the number of neurons, imaging frame rate, and signal-to-noise ratio. Our proposed supervised decoding analysis is ultrafast and robust, and thereby appealing for real-time position decoding applications based on calcium imaging.},
  archive      = {J_NECO},
  author       = {Tu, Mengyu and Zhao, Ruohe and Adler, Avital and Gan, Wen-Biao and Chen, Zhe S.},
  doi          = {10.1162/neco_a_01281},
  journal      = {Neural Computation},
  number       = {6},
  pages        = {1144-1167},
  shortjournal = {Neural Comput.},
  title        = {Efficient position decoding methods based on fluorescence calcium imaging in the mouse hippocampus},
  volume       = {32},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). First passage time memory lifetimes for multistate,
filter-based synapses. <em>NECO</em>, <em>32</em>(6), 1069–1143. (<a
href="https://doi.org/10.1162/neco_a_01283">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Models of associative memory with discrete state synapses learn new memories by forgetting old ones. In contrast to non-integrative models of synaptic plasticity, models with integrative, filter-based synapses exhibit an initial rise in the fidelity of recall of stored memories. This rise to a peak is driven by a transient process and is then followed by a return to equilibrium. In a series of papers, we have employed a first passage time (FPT) approach to define and study memory lifetimes, incrementally developing our methods, from both simple and complex binary-strength synapses to simple multistate synapses. Here, we complete this work by analyzing FPT memory lifetimes in multistate, filter-based synapses. To achieve this, we integrate out the internal filter states so that we can work with transitions only in synaptic strength. We then generalize results on polysynaptic generating functions from binary strength to multistate synapses, allowing us to examine the dynamics of synaptic strength changes in an ensemble of synapses rather than just a single synapse. To derive analytical results for FPT memory lifetimes, we partition the synaptic dynamics into two distinct phases: the first, pre-peak phase studied with a drift-only approximation, and the second, post-peak phase studied with approximations to the full strength transition probabilities. These approximations capture the underlying dynamics very well, as demonstrated by the extremely good agreement between results obtained by simulating our model and results obtained from the Fokker-Planck or integral equation approaches to FPT processes.},
  archive      = {J_NECO},
  author       = {Elliott, Terry},
  doi          = {10.1162/neco_a_01283},
  journal      = {Neural Computation},
  number       = {6},
  pages        = {1069-1143},
  shortjournal = {Neural Comput.},
  title        = {First passage time memory lifetimes for multistate, filter-based synapses},
  volume       = {32},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Nonequilibrium statistical mechanics of continuous
attractors. <em>NECO</em>, <em>32</em>(6), 1033–1068. (<a
href="https://doi.org/10.1162/neco_a_01280">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Continuous attractors have been used to understand recent neuroscience experiments where persistent activity patterns encode internal representations of external attributes like head direction or spatial location. However, the conditions under which the emergent bump of neural activity in such networks can be manipulated by space and time-dependent external sensory or motor signals are not understood. Here, we find fundamental limits on how rapidly internal representations encoded along continuous attractors can be updated by an external signal. We apply these results to place cell networks to derive a velocity-dependent nonequilibrium memory capacity in neural networks.},
  archive      = {J_NECO},
  author       = {Zhong, Weishun and Lu, Zhiyue and Schwab, David J. and Murugan, Arvind},
  doi          = {10.1162/neco_a_01280},
  journal      = {Neural Computation},
  number       = {6},
  pages        = {1033-1068},
  shortjournal = {Neural Comput.},
  title        = {Nonequilibrium statistical mechanics of continuous attractors},
  volume       = {32},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). The stochastic delta rule: Faster and more accurate deep
learning through adaptive weight noise. <em>NECO</em>, <em>32</em>(5),
1018–1032. (<a href="https://doi.org/10.1162/neco_a_01276">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multilayer neural networks have led to remarkable performance on many kinds of benchmark tasks in text, speech, and image processing. Nonlinear parameter estimation in hierarchical models is known to be subject to overfitting and misspecification. One approach to these estimation and related problems (e.g., saddle points, colinearity, feature discovery) is called Dropout. The Dropout algorithm removes hidden units according to a binomial random variable with probability p prior to each update, creating random “shocks” to the network that are averaged over updates (thus creating weight sharing). In this letter, we reestablish an older parameter search method and show that Dropout is a special case of this more general model, stochastic delta rule (SDR), published originally in 1990. Unlike Dropout, SDR redefines each weight in the network as a random variable with mean μ w i j and standard deviation σ w i j ⁠ . Each weight random variable is sampled on each forward activation, consequently creating an exponential number of potential networks with shared weights (accumulated in the mean values). Both parameters are updated according to prediction error, thus resulting in weight noise injections that reflect a local history of prediction error and local model averaging. SDR therefore implements a more sensitive local gradient-dependent simulated annealing per weight converging in the limit to a Bayes optimal network. We run tests on standard benchmarks (CIFAR and ImageNet) using a modified version of DenseNet and show that SDR outperforms standard Dropout in top-5 validation error by approximately 13\% with DenseNet-BC 121 on ImageNet and find various validation error improvements in smaller networks. We also show that SDR reaches the same accuracy that Dropout attains in 100 epochs in as few as 40 epochs, as well as improvements in training error by as much as 80\%.},
  archive      = {J_NECO},
  author       = {Frazier-Logue, Noah and Hanson, Stephen José},
  doi          = {10.1162/neco_a_01276},
  journal      = {Neural Computation},
  number       = {5},
  pages        = {1018-1032},
  shortjournal = {Neural Comput.},
  title        = {The stochastic delta rule: Faster and more accurate deep learning through adaptive weight noise},
  volume       = {32},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). The discriminative kalman filter for bayesian filtering with
nonlinear and nongaussian observation models. <em>NECO</em>,
<em>32</em>(5), 969–1017. (<a
href="https://doi.org/10.1162/neco_a_01275">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Kalman filter provides a simple and efficient algorithm to compute the posterior distribution for state-space models where both the latent state and measurement models are linear and gaussian. Extensions to the Kalman filter, including the extended and unscented Kalman filters, incorporate linearizations for models where the observation model p ( observation | state ) is nonlinear. We argue that in many cases, a model for p ( state | observation ) proves both easier to learn and more accurate for latent state estimation. Approximating p ( state | observation ) as gaussian leads to a new filtering algorithm, the discriminative Kalman filter (DKF), which can perform well even when p ( observation | state ) is highly nonlinear and/or nongaussian. The approximation, motivated by the Bernstein–von Mises theorem, improves as the dimensionality of the observations increases. The DKF has computational complexity similar to the Kalman filter, allowing it in some cases to perform much faster than particle filters with similar precision, while better accounting for nonlinear and nongaussian observation models than Kalman-based extensions. When the observation model must be learned from training data prior to filtering, off-the-shelf nonlinear and nonparametric regression techniques can provide a gaussian model for p ( observation | state ) that cleanly integrates with the DKF. As part of the BrainGate2 clinical trial, we successfully implemented gaussian process regression with the DKF framework in a brain-computer interface to provide real-time, closed-loop cursor control to a person with a complete spinal cord injury. In this letter, we explore the theory underlying the DKF, exhibit some illustrative examples, and outline potential extensions.},
  archive      = {J_NECO},
  author       = {Burkhart, Michael C. and Brandman, David M. and Franco, Brian and Hochberg, Leigh R. and Harrison, Matthew T.},
  doi          = {10.1162/neco_a_01275},
  journal      = {Neural Computation},
  number       = {5},
  pages        = {969-1017},
  shortjournal = {Neural Comput.},
  title        = {The discriminative kalman filter for bayesian filtering with nonlinear and nongaussian observation models},
  volume       = {32},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Equivalence projective simulation as a framework for
modeling formation of stimulus equivalence classes. <em>NECO</em>,
<em>32</em>(5), 912–968. (<a
href="https://doi.org/10.1162/neco_a_01274">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Stimulus equivalence (SE) and projective simulation (PS) study complex behavior, the former in human subjects and the latter in artificial agents. We apply the PS learning framework for modeling the formation of equivalence classes. For this purpose, we first modify the PS model to accommodate imitating the emergence of equivalence relations. Later, we formulate the SE formation through the matching-to-sample (MTS) procedure. The proposed version of PS model, called the equivalence projective simulation (EPS) model, is able to act within a varying action set and derive new relations without receiving feedback from the environment. To the best of our knowledge, it is the first time that the field of equivalence theory in behavior analysis has been linked to an artificial agent in a machine learning context. This model has many advantages over existing neural network models. Briefly, our EPS model is not a black box model, but rather a model with the capability of easy interpretation and flexibility for further modifications. To validate the model, some experimental results performed by prominent behavior analysts are simulated. The results confirm that the EPS model is able to reliably simulate and replicate the same behavior as real experiments in various settings, including formation of equivalence relations in typical participants, nonformation of equivalence relations in language-disabled children, and nodal effect in a linear series with nodal distance five. Moreover, through a hypothetical experiment, we discuss the possibility of applying EPS in further equivalence theory research.},
  archive      = {J_NECO},
  author       = {Mofrad, Asieh Abolpour and Yazidi, Anis and Hammer, Hugo L. and Arntzen, Erik},
  doi          = {10.1162/neco_a_01274},
  journal      = {Neural Computation},
  number       = {5},
  pages        = {912-968},
  shortjournal = {Neural Comput.},
  title        = {Equivalence projective simulation as a framework for modeling formation of stimulus equivalence classes},
  volume       = {32},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Comparison of different spike train synchrony measures
regarding their robustness to erroneous data from bicuculline-induced
epileptiform activity. <em>NECO</em>, <em>32</em>(5), 887–911. (<a
href="https://doi.org/10.1162/neco_a_01277">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As synchronized activity is associated with basic brain functions and pathological states, spike train synchrony has become an important measure to analyze experimental neuronal data. Many measures of spike train synchrony have been proposed, but there is no gold standard allowing for comparison of results from different experiments. This work aims to provide guidance on which synchrony measure is best suited to quantify the effect of epileptiform-inducing substances (e.g., bicuculline, BIC) in in vitro neuronal spike train data. Spike train data from recordings are likely to suffer from erroneous spike detection, such as missed spikes (false negative) or noise (false positive). Therefore, different timescale-dependent (cross-correlation, mutual information, spike time tiling coefficient) and timescale-independent (Spike-contrast, phase synchronization (PS), A-SPIKE-synchronization, A-ISI-distance, ARI-SPIKE-distance) synchrony measures were compared in terms of their robustness to erroneous spike trains. For this purpose, erroneous spike trains were generated by randomly adding (false positive) or deleting (false negative) spikes (in silico manipulated data) from experimental data. In addition, experimental data were analyzed using different spike detection threshold factors in order to confirm the robustness of the synchrony measures. All experimental data were recorded from cortical neuronal networks on microelectrode array chips, which show epileptiform activity induced by the substance BIC. As a result of the in silico manipulated data, Spike-contrast was the only measure that was robust to false-negative as well as false-positive spikes. Analyzing the experimental data set revealed that all measures were able to capture the effect of BIC in a statistically significant way, with Spike-contrast showing the highest statistical significance even at low spike detection thresholds. In summary, we suggest using Spike-contrast to complement established synchrony measures because it is timescale independent and robust to erroneous spike trains.},
  archive      = {J_NECO},
  author       = {Ciba, Manuel and Bestel, Robert and Nick, Christoph and de Arruda, Guilherme Ferraz and Peron, Thomas and Henrique, Comin César and Costa, Luciano da Fontoura and Rodrigues, Francisco Aparecido and Thielemann, Christiane},
  doi          = {10.1162/neco_a_01277},
  journal      = {Neural Computation},
  number       = {5},
  pages        = {887-911},
  shortjournal = {Neural Comput.},
  title        = {Comparison of different spike train synchrony measures regarding their robustness to erroneous data from bicuculline-induced epileptiform activity},
  volume       = {32},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Performance limitations in sensorimotor control: Trade-offs
between neural computation and accuracy in tracking fast movements.
<em>NECO</em>, <em>32</em>(5), 865–886. (<a
href="https://doi.org/10.1162/neco_a_01272">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The ability to move fast and accurately track moving objects is fundamentally constrained by the biophysics of neurons and dynamics of the muscles involved. Yet the corresponding trade-offs between these factors and tracking motor commands have not been rigorously quantified. We use feedback control principles to quantify performance limitations of the sensorimotor control system (SCS) to track fast periodic movements. We show that (1) linear models of the SCS fail to predict known undesirable phenomena, including skipped cycles, overshoot and undershoot, produced when tracking signals in the “fast regime,” while nonlinear pulsatile control models can predict such undesirable phenomena, and (2) tools from nonlinear control theory allow us to characterize fundamental limitations in this fast regime. Using a validated and tractable nonlinear model of the SCS, we derive an analytical upper bound on frequencies that the SCS model can reliably track before producing such undesirable phenomena as a function of the neurons&#39; biophysical constraints and muscle dynamics. The performance limitations derived here have important implications in sensorimotor control. For example, if the primary motor cortex is compromised due to disease or damage, the theory suggests ways to manipulate muscle dynamics by adding the necessary compensatory forces using an assistive neuroprosthetic device to restore motor performance and, more important, fast and agile movements. Just how one should compensate can be informed by our SCS model and the theory developed here.},
  archive      = {J_NECO},
  author       = {Saxena, Shreya and Sarma, Sridevi V. and Dahleh, Munther},
  doi          = {10.1162/neco_a_01272},
  journal      = {Neural Computation},
  number       = {5},
  pages        = {865-886},
  shortjournal = {Neural Comput.},
  title        = {Performance limitations in sensorimotor control: Trade-offs between neural computation and accuracy in tracking fast movements},
  volume       = {32},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A survey on deep learning for multimodal data fusion.
<em>NECO</em>, <em>32</em>(5), 829–864. (<a
href="https://doi.org/10.1162/neco_a_01273">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the wide deployments of heterogeneous networks, huge amounts of data with characteristics of high volume, high variety, high velocity, and high veracity are generated. These data, referred to multimodal big data, contain abundant intermodality and cross-modality information and pose vast challenges on traditional data fusion methods. In this review, we present some pioneering deep learning models to fuse these multimodal big data. With the increasing exploration of the multimodal big data, there are still some challenges to be addressed. Thus, this review presents a survey on deep learning for multimodal data fusion to provide readers, regardless of their original community, with the fundamentals of multimodal deep learning fusion method and to motivate new multimodal data fusion techniques of deep learning. Specifically, representative architectures that are widely used are summarized as fundamental to the understanding of multimodal deep learning. Then the current pioneering multimodal data fusion deep learning models are summarized. Finally, some challenges and future topics of multimodal data fusion deep learning models are described.},
  archive      = {J_NECO},
  author       = {Gao, Jing and Li, Peng and Chen, Zhikui and Zhang, Jianing},
  doi          = {10.1162/neco_a_01273},
  journal      = {Neural Computation},
  number       = {5},
  pages        = {829-864},
  shortjournal = {Neural Comput.},
  title        = {A survey on deep learning for multimodal data fusion},
  volume       = {32},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Optimal multivariate tuning with neuron-level and
population-level energy constraints. <em>NECO</em>, <em>32</em>(4),
794–828. (<a href="https://doi.org/10.1162/neco_a_01267">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Optimality principles have been useful in explaining many aspects of biological systems. In the context of neural encoding in sensory areas, optimality is naturally formulated in a Bayesian setting as neural tuning which minimizes mean decoding error. Many works optimize Fisher information, which approximates the minimum mean square error (MMSE) of the optimal decoder for long encoding time but may be misleading for short encoding times. We study MMSE-optimal neural encoding of a multivariate stimulus by uniform populations of spiking neurons, under firing rate constraints for each neuron as well as for the entire population. We show that the population-level constraint is essential for the formulation of a well-posed problem having finite optimal tuning widths and optimal tuning aligns with the principal components of the prior distribution. Numerical evaluation of the two-dimensional case shows that encoding only the dimension with higher variance is optimal for short encoding times. We also compare direct MMSE optimization to optimization of several proxies to MMSE: Fisher information, maximum likelihood estimation error, and the Bayesian Cramér-Rao bound. We find that optimization of these measures yields qualitatively misleading results regarding MMSE-optimal tuning and its dependence on encoding time and energy constraints.},
  archive      = {J_NECO},
  author       = {Harel, Yuval and Meir, Ron},
  doi          = {10.1162/neco_a_01267},
  journal      = {Neural Computation},
  number       = {4},
  pages        = {794-828},
  shortjournal = {Neural Comput.},
  title        = {Optimal multivariate tuning with neuron-level and population-level energy constraints},
  volume       = {32},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Online learning based on online DCA and application to
online classification. <em>NECO</em>, <em>32</em>(4), 759–793. (<a
href="https://doi.org/10.1162/neco_a_01266">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We investigate an approach based on DC (Difference of Convex functions) programming and DCA (DC Algorithm) for online learning techniques. The prediction problem of an online learner can be formulated as a DC program for which online DCA is applied. We propose the two so-called complete/approximate versions of online DCA scheme and prove their logarithmic/sublinear regrets. Six online DCA-based algorithms are developed for online binary linear classification. Numerical experiments on a variety of benchmark classification data sets show the efficiency of our proposed algorithms in comparison with the state-of-the-art online classification algorithms.},
  archive      = {J_NECO},
  author       = {Le Thi, Hoai An and Ho, Vinh Thanh},
  doi          = {10.1162/neco_a_01266},
  journal      = {Neural Computation},
  number       = {4},
  pages        = {759-793},
  shortjournal = {Neural Comput.},
  title        = {Online learning based on online DCA and application to online classification},
  volume       = {32},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Feature extraction of surface electromyography based on
improved small-world leaky echo state network. <em>NECO</em>,
<em>32</em>(4), 741–758. (<a
href="https://doi.org/10.1162/neco_a_01270">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Surface electromyography (sEMG) is an electrophysiological reflection of skeletal muscle contractile activity that can directly reflect neuromuscular activity. It has been a matter of research to investigate feature extraction methods of sEMG signals. In this letter, we propose a feature extraction method of sEMG signals based on the improved small-world leaky echo state network (ISWLESN). The reservoir of leaky echo state network (LESN) is connected by a random network. First, we improved the reservoir of the echo state network (ESN) by these networks and used edge-added probability to improve these networks. That idea enhances the adaptability of the reservoir, the generalization ability, and the stability of ESN. Then we obtained the output weight of the network through training and used it as features. We recorded the sEMG signals during different activities: falling, walking, sitting, squatting, going upstairs, and going downstairs. Afterward, we extracted corresponding features by ISWLESN and used principal component analysis for dimension reduction. At the end, scatter plot, the class separability index, and the Davies-Bouldin index were used to assess the performance of features. The results showed that the ISWLESN clustering performance was better than those of LESN and ESN. By support vector machine, it was also revealed that the performance of ISWLESN for classifying the activities was better than those of ESN and LESN.},
  archive      = {J_NECO},
  author       = {Xi, Xugang and Jiang, Wenjun and Miran, Seyed M. and Hua, Xian and Zhao, Yun-Bo and Yang, Chen and Luo, Zhizeng},
  doi          = {10.1162/neco_a_01270},
  journal      = {Neural Computation},
  number       = {4},
  pages        = {741-758},
  shortjournal = {Neural Comput.},
  title        = {Feature extraction of surface electromyography based on improved small-world leaky echo state network},
  volume       = {32},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Neural model of coding stimulus orientation and adaptation.
<em>NECO</em>, <em>32</em>(4), 711–740. (<a
href="https://doi.org/10.1162/neco_a_01269">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The coding of line orientation in the visual system has been investigated extensively. During the prolonged viewing of a stimulus, the perceived orientation continuously changes (normalization effect). Also, the orientation of the adapting stimulus and the background stimuli influence the perceived orientation of the subsequently displayed stimulus: tilt after-effect (TAE) or tilt illusion (TI). The neural mechanisms of these effects are not fully understood. The proposed model includes many local analyzers, each consisting of two sets of neurons. The first set has two independent cardinal detectors (CDs), whose responses depend on stimulus orientation. The second set has many orientation detectors (OD) tuned to different orientations of the stimulus. The ODs sum up the responses of the two CDs with respective weightings and output a preferred orientation depending on the ratio of CD responses. It is suggested that during prolonged viewing, the responses of the CDs decrease: the greater the excitation of the detector, the more rapid the decrease in its response. Thereby, the ratio of CD responses changes during the adaptation, causing the normalization effect and the TAE. The CDs of the different local analyzers laterally inhibit each other and cause the TI. We show that the properties of this model are consistent with both psychophysical and neurophysiological findings related to the properties of orientation perception, and we investigate how these mechanisms can affect the orientation&#39;s sensitivity.},
  archive      = {J_NECO},
  author       = {Vaitkevičius, Henrikas and Švegžda, Algimantas and Stanikūnas, Rytis and Bliumas, Remigijus and Šoliūnas, Alvydas and Kulikowski, Janus J.},
  doi          = {10.1162/neco_a_01269},
  journal      = {Neural Computation},
  number       = {4},
  pages        = {711-740},
  shortjournal = {Neural Comput.},
  title        = {Neural model of coding stimulus orientation and adaptation},
  volume       = {32},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Center manifold analysis of plateau phenomena caused by
degeneration of three-layer perceptron. <em>NECO</em>, <em>32</em>(4),
683–710. (<a href="https://doi.org/10.1162/neco_a_01268">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A hierarchical neural network usually has many singular regions in the parameter space due to the degeneration of hidden units. Here, we focus on a three-layer perceptron, which has one-dimensional singular regions comprising both attractive and repulsive parts. Such a singular region is often called a Milnor-like attractor. It is empirically known that in the vicinity of a Milnor-like attractor, several parameters converge much faster than the rest and that the dynamics can be reduced to smaller-dimensional ones. Here we give a rigorous proof for this phenomenon based on a center manifold theory. As an application, we analyze the reduced dynamics near the Milnor-like attractor and study the stochastic effects of the online learning.},
  archive      = {J_NECO},
  author       = {Tsutsui, Daiji},
  doi          = {10.1162/neco_a_01268},
  journal      = {Neural Computation},
  number       = {4},
  pages        = {683-710},
  shortjournal = {Neural Comput.},
  title        = {Center manifold analysis of plateau phenomena caused by degeneration of three-layer perceptron},
  volume       = {32},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Classification from triplet comparison data. <em>NECO</em>,
<em>32</em>(3), 659–681. (<a
href="https://doi.org/10.1162/neco_a_01262">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Learning from triplet comparison data has been extensively studied in the context of metric learning, where we want to learn a distance metric between two instances, and ordinal embedding, where we want to learn an embedding in a Euclidean space of the given instances that preserve the comparison order as much as possible. Unlike fully labeled data, triplet comparison data can be collected in a more accurate and human-friendly way. Although learning from triplet comparison data has been considered in many applications, an important fundamental question of whether we can learn a classifier only from triplet comparison data without all the labels has remained unanswered. In this letter, we give a positive answer to this important question by proposing an unbiased estimator for the classification risk under the empirical risk minimization framework. Since the proposed method is based on the empirical risk minimization framework, it inherently has the advantage that any surrogate loss function and any model, including neural networks, can be easily applied. Furthermore, we theoretically establish an estimation error bound for the proposed empirical risk minimizer. Finally, we provide experimental results to show that our method empirically works well and outperforms various baseline methods.},
  archive      = {J_NECO},
  author       = {Cui, Zhenghang and Charoenphakdee, Nontawat and Sato, Issei and Sugiyama, Masashi},
  doi          = {10.1162/neco_a_01262},
  journal      = {Neural Computation},
  number       = {3},
  pages        = {659-681},
  shortjournal = {Neural Comput.},
  title        = {Classification from triplet comparison data},
  volume       = {32},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Switching in cerebellar stellate cell excitability in
response to a pair of inhibitory/excitatory presynaptic inputs: A
dynamical system perspective. <em>NECO</em>, <em>32</em>(3), 626–658.
(<a href="https://doi.org/10.1162/neco_a_01261">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cerebellar stellate cells form inhibitory synapses with Purkinje cells, the sole output of the cerebellum. Upon stimulation by a pair of varying inhibitory and fixed excitatory presynaptic inputs, these cells do not respond to excitation (i.e., do not generate an action potential) when the magnitude of the inhibition is within a given range, but they do respond outside this range. We previously used a revised Hodgkin–Huxley type of model to study the nonmonotonic first-spike latency of these cells and their temporal increase in excitability in whole cell configuration (termed run-up). Here, we recompute these latency profiles using the same model by adapting an efficient computational technique, the two-point boundary value problem, that is combined with the continuation method. We then extend the study to investigate how switching in responsiveness, upon stimulation with presynaptic inputs, manifests itself in the context of run-up. A three-dimensional reduced model is initially derived from the original six-dimensional model and then analyzed to demonstrate that both models exhibit type 1 excitability possessing a saddle-node on an invariant cycle (SNIC) bifurcation when varying the amplitude of I app ⁠ . Using slow-fast analysis, we show that the original model possesses three equilibria lying at the intersection of the critical manifold of the fast subsystem and the nullcline of the slow variable h A (the inactivation of the A-type K + channel), the middle equilibrium is of saddle type with two-dimensional stable manifold (computed from the reduced model) acting as a boundary between the responsive and non-responsive regimes, and the (ghost of) SNIC is formed when the h A -nullcline is (nearly) tangential to the critical manifold. We also show that the slow dynamics associated with (the ghost of) the SNIC and the lower stable branch of the critical manifold are responsible for generating the nonmonotonic first-spike latency. These results thus provide important insight into the complex dynamics of stellate cells.},
  archive      = {J_NECO},
  author       = {Farjami, Saeed and Alexander, Ryan P. D. and Bowie, Derek and Khadra, Anmar},
  doi          = {10.1162/neco_a_01261},
  journal      = {Neural Computation},
  number       = {3},
  pages        = {626-658},
  shortjournal = {Neural Comput.},
  title        = {Switching in cerebellar stellate cell excitability in response to a pair of Inhibitory/Excitatory presynaptic inputs: A dynamical system perspective},
  volume       = {32},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Evaluating the potential gain of auditory and audiovisual
speech-predictive coding using deep learning. <em>NECO</em>,
<em>32</em>(3), 596–625. (<a
href="https://doi.org/10.1162/neco_a_01264">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sensory processing is increasingly conceived in a predictive framework in which neurons would constantly process the error signal resulting from the comparison of expected and observed stimuli. Surprisingly, few data exist on the accuracy of predictions that can be computed in real sensory scenes. Here, we focus on the sensory processing of auditory and audiovisual speech. We propose a set of computational models based on artificial neural networks (mixing deep feedforward and convolutional networks), which are trained to predict future audio observations from present and past audio or audiovisual observations (i.e., including lip movements). Those predictions exploit purely local phonetic regularities with no explicit call to higher linguistic levels. Experiments are conducted on the multispeaker LibriSpeech audio speech database (around 100 hours) and on the NTCD-TIMIT audiovisual speech database (around 7 hours). They appear to be efficient in a short temporal range (25–50 ms), predicting 50\% to 75\% of the variance of the incoming stimulus, which could result in potentially saving up to three-quarters of the processing power. Then they quickly decrease and almost vanish after 250 ms. Adding information on the lips slightly improves predictions, with a 5\% to 10\% increase in explained variance. Interestingly the visual gain vanishes more slowly, and the gain is maximum for a delay of 75 ms between image and predicted sound.},
  archive      = {J_NECO},
  author       = {Hueber, Thomas and Tatulli, Eric and Girin, Laurent and Schwartz, Jean-Luc},
  doi          = {10.1162/neco_a_01264},
  journal      = {Neural Computation},
  number       = {3},
  pages        = {596-625},
  shortjournal = {Neural Comput.},
  title        = {Evaluating the potential gain of auditory and audiovisual speech-predictive coding using deep learning},
  volume       = {32},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Model-free robust optimal feedback mechanisms of biological
motor control. <em>NECO</em>, <em>32</em>(3), 562–595. (<a
href="https://doi.org/10.1162/neco_a_01260">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sensorimotor tasks that humans perform are often affected by different sources of uncertainty. Nevertheless, the central nervous system (CNS) can gracefully coordinate our movements. Most learning frameworks rely on the internal model principle, which requires a precise internal representation in the CNS to predict the outcomes of our motor commands. However, learning a perfect internal model in a complex environment over a short period of time is a nontrivial problem. Indeed, achieving proficient motor skills may require years of training for some difficult tasks. Internal models alone may not be adequate to explain the motor adaptation behavior during the early phase of learning. Recent studies investigating the active regulation of motor variability, the presence of suboptimal inference, and model-free learning have challenged some of the traditional viewpoints on the sensorimotor learning mechanism. As a result, it may be necessary to develop a computational framework that can account for these new phenomena. Here, we develop a novel theory of motor learning, based on model-free adaptive optimal control, which can bypass some of the difficulties in existing theories. This new theory is based on our recently developed adaptive dynamic programming (ADP) and robust ADP (RADP) methods and is especially useful for accounting for motor learning behavior when an internal model is inaccurate or unavailable. Our preliminary computational results are in line with experimental observations reported in the literature and can account for some phenomena that are inexplicable using existing models.},
  archive      = {J_NECO},
  author       = {Bian, Tao and Wolpert, Daniel M. and Jiang, Zhong-Ping},
  doi          = {10.1162/neco_a_01260},
  journal      = {Neural Computation},
  number       = {3},
  pages        = {562-595},
  shortjournal = {Neural Comput.},
  title        = {Model-free robust optimal feedback mechanisms of biological motor control},
  volume       = {32},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Hidden aspects of the research ADOS are bound to affect
autism science. <em>NECO</em>, <em>32</em>(3), 515–561. (<a
href="https://doi.org/10.1162/neco_a_01263">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The research-grade Autism Diagnostic Observational Schedule (ADOS) is a broadly used instrument that informs and steers much of the science of autism. Despite its broad use, little is known about the empirical variability inherently present in the scores of the ADOS scale or their appropriateness to define change and its rate, to repeatedly use this test to characterize neurodevelopmental trajectories. Here we examine the empirical distributions of research-grade ADOS scores from 1324 records in a cross-section of the population comprising participants with autism between five and 65 years of age. We find that these empirical distributions violate the theoretical requirements of normality and homogeneous variance, essential for independence between bias and sensitivity. Further, we assess a subset of 52 typical controls versus those with autism and find a lack of proper elements to characterize neurodevelopmental trajectories in a coping nervous system changing at nonuniform, nonlinear rates. Repeating the assessments over four visits in a subset of the participants with autism for whom verbal criteria retained the same appropriate ADOS modules over the time span of the four visits reveals that switching the clinician changes the cutoff scores and consequently influences the diagnosis, despite maintaining fidelity in the same test&#39;s modules, room conditions, and tasks&#39; fluidity per visit. Given the changes in probability distribution shape and dispersion of these ADOS scores, the lack of appropriate metric spaces to define similarity measures to characterize change and the impact that these elements have on sensitivity-bias codependencies and on longitudinal tracking of autism, we invite a discussion on readjusting the use of this test for scientific purposes.},
  archive      = {J_NECO},
  author       = {Torres, Elizabeth B. and Rai, Richa and Mistry, Sejal and Gupta, Brenda},
  doi          = {10.1162/neco_a_01263},
  journal      = {Neural Computation},
  number       = {3},
  pages        = {515-561},
  shortjournal = {Neural Comput.},
  title        = {Hidden aspects of the research ADOS are bound to affect autism science},
  volume       = {32},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Improving generalization via attribute selection on
out-of-the-box data. <em>NECO</em>, <em>32</em>(2), 485–514. (<a
href="https://doi.org/10.1162/neco_a_01256">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Zero-shot learning (ZSL) aims to recognize unseen objects (test classes) given some other seen objects (training classes) by sharing information of attributes between different objects. Attributes are artificially annotated for objects and treated equally in recent ZSL tasks. However, some inferior attributes with poor predictability or poor discriminability may have negative impacts on the ZSL system performance. This letter first derives a generalization error bound for ZSL tasks. Our theoretical analysis verifies that selecting the subset of key attributes can improve the generalization performance of the original ZSL model, which uses all the attributes. Unfortunately, previous attribute selection methods have been conducted based on the seen data, and their selected attributes have poor generalization capability to the unseen data, which is unavailable in the training stage of ZSL tasks. Inspired by learning from pseudo-relevance feedback, this letter introduces out-of-the-box data—pseudo-data generated by an attribute-guided generative model—to mimic the unseen data. We then present an iterative attribute selection (IAS) strategy that iteratively selects key attributes based on the out-of-the-box data. Since the distribution of the generated out-of-the-box data is similar to that of the test data, the key attributes selected by IAS can be effectively generalized to test data. Extensive experiments demonstrate that IAS can significantly improve existing attribute-based ZSL methods and achieve state-of-the-art performance.},
  archive      = {J_NECO},
  author       = {Xu, Xiaofeng and Tsang, Ivor W. and Liu, Chuancai},
  doi          = {10.1162/neco_a_01256},
  journal      = {Neural Computation},
  number       = {2},
  pages        = {485-514},
  shortjournal = {Neural Comput.},
  title        = {Improving generalization via attribute selection on out-of-the-box data},
  volume       = {32},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Scaled coupled norms and coupled higher-order tensor
completion. <em>NECO</em>, <em>32</em>(2), 447–484. (<a
href="https://doi.org/10.1162/neco_a_01254">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, a set of tensor norms known as coupled norms has been proposed as a convex solution to coupled tensor completion. Coupled norms have been designed by combining low-rank inducing tensor norms with the matrix trace norm. Though coupled norms have shown good performances, they have two major limitations: they do not have a method to control the regularization of coupled modes and uncoupled modes, and they are not optimal for couplings among higher-order tensors. In this letter, we propose a method that scales the regularization of coupled components against uncoupled components to properly induce the low-rankness on the coupled mode. We also propose coupled norms for higher-order tensors by combining the square norm to coupled norms. Using the excess risk-bound analysis, we demonstrate that our proposed methods lead to lower risk bounds compared to existing coupled norms. We demonstrate the robustness of our methods through simulation and real-data experiments.},
  archive      = {J_NECO},
  author       = {Wimalawarne, Kishan and Yamada, Makoto and Mamitsuka, Hiroshi},
  doi          = {10.1162/neco_a_01254},
  journal      = {Neural Computation},
  number       = {2},
  pages        = {447-484},
  shortjournal = {Neural Comput.},
  title        = {Scaled coupled norms and coupled higher-order tensor completion},
  volume       = {32},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Synaptic scaling improves the stability of neural mass
models capable of simulating brain plasticity. <em>NECO</em>,
<em>32</em>(2), 424–446. (<a
href="https://doi.org/10.1162/neco_a_01257">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Neural mass models offer a way of studying the development and behavior of large-scale brain networks through computer simulations. Such simulations are currently mainly research tools, but as they improve, they could soon play a role in understanding, predicting, and optimizing patient treatments, particularly in relation to effects and outcomes of brain injury. To bring us closer to this goal, we took an existing state-of-the-art neural mass model capable of simulating connection growth through simulated plasticity processes. We identified and addressed some of the model&#39;s limitations by implementing biologically plausible mechanisms. The main limitation of the original model was its instability, which we addressed by incorporating a representation of the mechanism of synaptic scaling and examining the effects of optimizing parameters in the model. We show that the updated model retains all the merits of the original model, while being more stable and capable of generating networks that are in several aspects similar to those found in real brains.},
  archive      = {J_NECO},
  author       = {Demšar, Jure and Forsyth, Rob},
  doi          = {10.1162/neco_a_01257},
  journal      = {Neural Computation},
  number       = {2},
  pages        = {424-446},
  shortjournal = {Neural Comput.},
  title        = {Synaptic scaling improves the stability of neural mass models capable of simulating brain plasticity},
  volume       = {32},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). From synaptic interactions to collective dynamics in random
neuronal networks models: Critical role of eigenvectors and transient
behavior. <em>NECO</em>, <em>32</em>(2), 395–423. (<a
href="https://doi.org/10.1162/neco_a_01253">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The study of neuronal interactions is at the center of several big collaborative neuroscience projects (including the Human Connectome Project, the Blue Brain Project, and the Brainome) that attempt to obtain a detailed map of the entire brain. Under certain constraints, mathematical theory can advance predictions of the expected neural dynamics based solely on the statistical properties of the synaptic interaction matrix. This work explores the application of free random variables to the study of large synaptic interaction matrices. Besides recovering in a straightforward way known results on eigenspectra in types of models of neural networks proposed by Rajan and Abbott ( 2006 ), we extend them to heavy-tailed distributions of interactions. More important, we analytically derive the behavior of eigenvector overlaps, which determine the stability of the spectra. We observe that on imposing the neuronal excitation/inhibition balance, despite the eigenvalues remaining unchanged, their stability dramatically decreases due to the strong nonorthogonality of associated eigenvectors. This leads us to the conclusion that understanding the temporal evolution of asymmetric neural networks requires considering the entangled dynamics of both eigenvectors and eigenvalues, which might bear consequences for learning and memory processes in these models. Considering the success of free random variables theory in a wide variety of disciplines, we hope that the results presented here foster the additional application of these ideas in the area of brain sciences.},
  archive      = {J_NECO},
  author       = {Gudowska-Nowak, E. and Nowak, M. A. and Chialvo, D. R. and Ochab, J. K. and Tarnowski, W.},
  doi          = {10.1162/neco_a_01253},
  journal      = {Neural Computation},
  number       = {2},
  pages        = {395-423},
  shortjournal = {Neural Comput.},
  title        = {From synaptic interactions to collective dynamics in random neuronal networks models: Critical role of eigenvectors and transient behavior},
  volume       = {32},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Transition scale-spaces: A computational theory for the
discretized entorhinal cortex. <em>NECO</em>, <em>32</em>(2), 330–394.
(<a href="https://doi.org/10.1162/neco_a_01255">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Although hippocampal grid cells are thought to be crucial for spatial navigation, their computational purpose remains disputed. Recently, they were proposed to represent spatial transitions and convey this knowledge downstream to place cells. However, a single scale of transitions is insufficient to plan long goal-directed sequences in behaviorally acceptable time. Here, a scale-space data structure is suggested to optimally accelerate retrievals from transition systems, called transition scale-space (TSS). Remaining exclusively on an algorithmic level, the scale increment is proved to be ideally 2 for biologically plausible receptive fields. It is then argued that temporal buffering is necessary to learn the scale-space online. Next, two modes for retrieval of sequences from the TSS are presented: top down and bottom up. The two modes are evaluated in symbolic simulations (i.e., without biologically plausible spiking neurons). Additionally, a TSS is used for short-cut discovery in a simulated Morris water maze. Finally, the results are discussed in depth with respect to biological plausibility, and several testable predictions are derived. Moreover, relations to other grid cell models, multiresolution path planning, and scale-space theory are highlighted. Summarized, reward-free transition encoding is shown here, in a theoretical model, to be compatible with the observed discretization along the dorso-ventral axis of the medial entorhinal cortex. Because the theoretical model generalizes beyond navigation, the TSS is suggested to be a general-purpose cortical data structure for fast retrieval of sequences and relational knowledge. Source code for all simulations presented in this paper can be found at https://github.com/rochus/transitionscalespace .},
  archive      = {J_NECO},
  author       = {Waniek, Nicolai},
  doi          = {10.1162/neco_a_01255},
  journal      = {Neural Computation},
  number       = {2},
  pages        = {330-394},
  shortjournal = {Neural Comput.},
  title        = {Transition scale-spaces: A computational theory for the discretized entorhinal cortex},
  volume       = {32},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Face representations via tensorfaces of various
complexities. <em>NECO</em>, <em>32</em>(2), 281–329. (<a
href="https://doi.org/10.1162/neco_a_01258">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Neurons selective for faces exist in humans and monkeys. However, characteristics of face cell receptive fields are poorly understood. In this theoretical study, we explore the effects of complexity, defined as algorithmic information (Kolmogorov complexity) and logical depth, on possible ways that face cells may be organized. We use tensor decompositions to decompose faces into a set of components, called tensorfaces, and their associated weights, which can be interpreted as model face cells and their firing rates. These tensorfaces form a high-dimensional representation space in which each tensorface forms an axis of the space. A distinctive feature of the decomposition algorithm is the ability to specify tensorface complexity. We found that low-complexity tensorfaces have blob-like appearances crudely approximating faces, while high-complexity tensorfaces appear clearly face-like. Low-complexity tensorfaces require a larger population to reach a criterion face reconstruction error than medium- or high-complexity tensorfaces, and thus are inefficient by that criterion. Low-complexity tensorfaces, however, generalize better when representing statistically novel faces, which are faces falling beyond the distribution of face description parameters found in the tensorface training set. The degree to which face representations are parts based or global forms a continuum as a function of tensorface complexity, with low and medium tensorfaces being more parts based. Given the computational load imposed in creating high-complexity face cells (in the form of algorithmic information and logical depth) and in the absence of a compelling advantage to using high-complexity cells, we suggest face representations consist of a mixture of low- and medium-complexity face cells.},
  archive      = {J_NECO},
  author       = {Lehky, Sidney R. and Phan, Anh Huy and Cichocki, Andrzej and Tanaka, Keiji},
  doi          = {10.1162/neco_a_01258},
  journal      = {Neural Computation},
  number       = {2},
  pages        = {281-329},
  shortjournal = {Neural Comput.},
  title        = {Face representations via tensorfaces of various complexities},
  volume       = {32},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Optimal sampling of parametric families: Implications for
machine learning. <em>NECO</em>, <em>32</em>(1), 261–279. (<a
href="https://doi.org/10.1162/neco_a_01251">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {It is well known in machine learning that models trained on a training set generated by a probability distribution function perform far worse on test sets generated by a different probability distribution function. In the limit, it is feasible that a continuum of probability distribution functions might have generated the observed test set data; a desirable property of a learned model in that case is its ability to describe most of the probability distribution functions from the continuum equally well. This requirement naturally leads to sampling methods from the continuum of probability distribution functions that lead to the construction of optimal training sets. We study the sequential prediction of Ornstein-Uhlenbeck processes that form a parametric family. We find empirically that a simple deep network trained on optimally constructed training sets using the methods described in this letter can be robust to changes in the test set distribution.},
  archive      = {J_NECO},
  author       = {Huber, Adrian E. G. and Anumula, Jithendar and Liu, Shih-Chii},
  doi          = {10.1162/neco_a_01251},
  journal      = {Neural Computation},
  number       = {1},
  pages        = {261-279},
  shortjournal = {Neural Comput.},
  title        = {Optimal sampling of parametric families: Implications for machine learning},
  volume       = {32},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Iterative retrieval and block coding in autoassociative and
heteroassociative memory. <em>NECO</em>, <em>32</em>(1), 205–260. (<a
href="https://doi.org/10.1162/neco_a_01247">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Neural associative memories (NAM) are perceptron-like single-layer networks with fast synaptic learning typically storing discrete associations between pairs of neural activity patterns. Gripon and Berrou ( 2011 ) investigated NAM employing block coding, a particular sparse coding method, and reported a significant increase in storage capacity. Here we verify and extend their results for both heteroassociative and recurrent autoassociative networks. For this we provide a new analysis of iterative retrieval in finite autoassociative and heteroassociative networks that allows estimating storage capacity for random and block patterns. Furthermore, we have implemented various retrieval algorithms for block coding and compared them in simulations to our theoretical results and previous simulation data. In good agreement of theory and experiments, we find that finite networks employing block coding can store significantly more memory patterns. However, due to the reduced information per block pattern, it is not possible to significantly increase stored information per synapse. Asymptotically, the information retrieval capacity converges to the known limits C = ln 2 ≈ 0 . 69 and C = ( ln 2 ) / 4 ≈ 0 . 17 also for block coding. We have also implemented very large recurrent networks up to n = 2 · 10 6 neurons, showing that maximal capacity C ≈ 0 . 2 bit per synapse occurs for finite networks having a size n ≈ 10 5 similar to cortical macrocolumns.},
  archive      = {J_NECO},
  author       = {Knoblauch, Andreas and Palm, Günther},
  doi          = {10.1162/neco_a_01247},
  journal      = {Neural Computation},
  number       = {1},
  pages        = {205-260},
  shortjournal = {Neural Comput.},
  title        = {Iterative retrieval and block coding in autoassociative and heteroassociative memory},
  volume       = {32},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). An FPGA implementation of deep spiking neural networks for
low-power and fast classification. <em>NECO</em>, <em>32</em>(1),
182–204. (<a href="https://doi.org/10.1162/neco_a_01245">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A spiking neural network (SNN) is a type of biological plausibility model that performs information processing based on spikes. Training a deep SNN effectively is challenging due to the nondifferention of spike signals. Recent advances have shown that high-performance SNNs can be obtained by converting convolutional neural networks (CNNs). However, the large-scale SNNs are poorly served by conventional architectures due to the dynamic nature of spiking neurons. In this letter, we propose a hardware architecture to enable efficient implementation of SNNs. All layers in the network are mapped on one chip so that the computation of different time steps can be done in parallel to reduce latency. We propose new spiking max-pooling method to reduce computation complexity. In addition, we apply approaches based on shift register and coarsely grained parallels to accelerate convolution operation. We also investigate the effect of different encoding methods on SNN accuracy. Finally, we validate the hardware architecture on the Xilinx Zynq ZCU102. The experimental results on the MNIST data set show that it can achieve an accuracy of 98.94\% with eight-bit quantized weights. Furthermore, it achieves 164 frames per second (FPS) under 150 MHz clock frequency and obtains 41 × speed-up compared to CPU implementation and 22 times lower power than GPU implementation.},
  archive      = {J_NECO},
  author       = {Ju, Xiping and Fang, Biao and Yan, Rui and Xu, Xiaoliang and Tang, Huajin},
  doi          = {10.1162/neco_a_01245},
  journal      = {Neural Computation},
  number       = {1},
  pages        = {182-204},
  shortjournal = {Neural Comput.},
  title        = {An FPGA implementation of deep spiking neural networks for low-power and fast classification},
  volume       = {32},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A robust model of gated working memory. <em>NECO</em>,
<em>32</em>(1), 153–181. (<a
href="https://doi.org/10.1162/neco_a_01249">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Gated working memory is defined as the capacity of holding arbitrary information at any time in order to be used at a later time. Based on electrophysiological recordings, several computational models have tackled the problem using dedicated and explicit mechanisms. We propose instead to consider an implicit mechanism based on a random recurrent neural network. We introduce a robust yet simple reservoir model of gated working memory with instantaneous updates. The model is able to store an arbitrary real value at random time over an extended period of time. The dynamics of the model is a line attractor that learns to exploit reentry and a nonlinearity during the training phase using only a few representative values. A deeper study of the model shows that there is actually a large range of hyperparameters for which the results hold (e.g., number of neurons, sparsity, global weight scaling) such that any large enough population, mixing excitatory and inhibitory neurons, can quickly learn to realize such gated working memory. In a nutshell, with a minimal set of hypotheses, we show that we can have a robust model of working memory. This suggests this property could be an implicit property of any random population, that can be acquired through learning. Furthermore, considering working memory to be a physically open but functionally closed system, we give account on some counterintuitive electrophysiological recordings.},
  archive      = {J_NECO},
  author       = {Strock, Anthony and Hinaut, Xavier and Rougier, Nicolas P.},
  doi          = {10.1162/neco_a_01249},
  journal      = {Neural Computation},
  number       = {1},
  pages        = {153-181},
  shortjournal = {Neural Comput.},
  title        = {A robust model of gated working memory},
  volume       = {32},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Storing object-dependent sparse codes in a willshaw
associative network. <em>NECO</em>, <em>32</em>(1), 136–152. (<a
href="https://doi.org/10.1162/neco_a_01243">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Willshaw networks are single-layered neural networks that store associations between binary vectors. Using only binary weights, these networks can be implemented efficiently to store large numbers of patterns and allow for fault-tolerant recovery of those patterns from noisy cues. However, this is only the case when the involved codes are sparse and randomly generated. In this letter, we use a recently proposed approach that maps visual patterns into informative binary features. By doing so, we manage to transform MNIST handwritten digits into well-distributed codes that we then store in a Willshaw network in autoassociation. We perform experiments with both noisy and noiseless cues and verify a tenuous impact on the recovered pattern&#39;s relevant information. More specifically, we were able to perform retrieval after filling the memory to several factors of its number of units while preserving the information of the class to which the pattern belongs.},
  archive      = {J_NECO},
  author       = {Sa-Couto, Luis and Wichert, Andreas},
  doi          = {10.1162/neco_a_01243},
  journal      = {Neural Computation},
  number       = {1},
  pages        = {136-152},
  shortjournal = {Neural Comput.},
  title        = {Storing object-dependent sparse codes in a willshaw associative network},
  volume       = {32},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). On kernel method–based connectionist models and supervised
deep learning without backpropagation. <em>NECO</em>, <em>32</em>(1),
97–135. (<a href="https://doi.org/10.1162/neco_a_01250">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a novel family of connectionist models based on kernel machines and consider the problem of learning layer by layer a compositional hypothesis class (i.e., a feedforward, multilayer architecture) in a supervised setting. In terms of the models, we present a principled method to “kernelize” (partly or completely) any neural network (NN). With this method, we obtain a counterpart of any given NN that is powered by kernel machines instead of neurons. In terms of learning, when learning a feedforward deep architecture in a supervised setting, one needs to train all the components simultaneously using backpropagation (BP) since there are no explicit targets for the hidden layers (Rumelhart, Hinton, &amp; Williams, 1986 ). We consider without loss of generality the two-layer case and present a general framework that explicitly characterizes a target for the hidden layer that is optimal for minimizing the objective function of the network. This characterization then makes possible a purely greedy training scheme that learns one layer at a time, starting from the input layer. We provide instantiations of the abstract framework under certain architectures and objective functions. Based on these instantiations, we present a layer-wise training algorithm for an l -layer feedforward network for classification, where l ≥ 2 can be arbitrary. This algorithm can be given an intuitive geometric interpretation that makes the learning dynamics transparent. Empirical results are provided to complement our theory. We show that the kernelized networks, trained layer-wise, compare favorably with classical kernel machines as well as other connectionist models trained by BP. We also visualize the inner workings of the greedy kernelized models to validate our claim on the transparency of the layer-wise algorithm.},
  archive      = {J_NECO},
  author       = {Duan, Shiyu and Yu, Shujian and Chen, Yunmei and Principe, Jose C.},
  doi          = {10.1162/neco_a_01250},
  journal      = {Neural Computation},
  number       = {1},
  pages        = {97-135},
  shortjournal = {Neural Comput.},
  title        = {On kernel Method–Based connectionist models and supervised deep learning without backpropagation},
  volume       = {32},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A continuous-time analysis of distributed stochastic
gradient. <em>NECO</em>, <em>32</em>(1), 36–96. (<a
href="https://doi.org/10.1162/neco_a_01248">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We analyze the effect of synchronization on distributed stochastic gradient algorithms. By exploiting an analogy with dynamical models of biological quorum sensing, where synchronization between agents is induced through communication with a common signal, we quantify how synchronization can significantly reduce the magnitude of the noise felt by the individual distributed agents and their spatial mean. This noise reduction is in turn associated with a reduction in the smoothing of the loss function imposed by the stochastic gradient approximation. Through simulations on model nonconvex objectives, we demonstrate that coupling can stabilize higher noise levels and improve convergence. We provide a convergence analysis for strongly convex functions by deriving a bound on the expected deviation of the spatial mean of the agents from the global minimizer for an algorithm based on quorum sensing, the same algorithm with momentum, and the elastic averaging SGD (EASGD) algorithm. We discuss extensions to new algorithms that allow each agent to broadcast its current measure of success and shape the collective computation accordingly. We supplement our theoretical analysis with numerical experiments on convolutional neural networks trained on the CIFAR-10 data set, where we note a surprising regularizing property of EASGD even when applied to the non-distributed case. This observation suggests alternative second-order in time algorithms for nondistributed optimization that are competitive with momentum methods.},
  archive      = {J_NECO},
  author       = {Boffi, Nicholas M. and Slotine, Jean-Jacques E.},
  doi          = {10.1162/neco_a_01248},
  journal      = {Neural Computation},
  number       = {1},
  pages        = {36-96},
  shortjournal = {Neural Comput.},
  title        = {A continuous-time analysis of distributed stochastic gradient},
  volume       = {32},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Toward training recurrent neural networks for lifelong
learning. <em>NECO</em>, <em>32</em>(1), 1–35. (<a
href="https://doi.org/10.1162/neco_a_01246">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Catastrophic forgetting and capacity saturation are the central challenges of any parametric lifelong learning system. In this work, we study these challenges in the context of sequential supervised learning with an emphasis on recurrent neural networks. To evaluate the models in the lifelong learning setting, we propose a curriculum-based, simple, and intuitive benchmark where the models are trained on tasks with increasing levels of difficulty. To measure the impact of catastrophic forgetting, the model is tested on all the previous tasks as it completes any task. As a step toward developing true lifelong learning systems, we unify gradient episodic memory (a catastrophic forgetting alleviation approach) and Net2Net (a capacity expansion approach). Both models are proposed in the context of feedforward networks, and we evaluate the feasibility of using them for recurrent networks. Evaluation on the proposed benchmark shows that the unified model is more suitable than the constituent models for lifelong learning setting.},
  archive      = {J_NECO},
  author       = {Sodhani, Shagun and Chandar, Sarath and Bengio, Yoshua},
  doi          = {10.1162/neco_a_01246},
  journal      = {Neural Computation},
  number       = {1},
  pages        = {1-35},
  shortjournal = {Neural Comput.},
  title        = {Toward training recurrent neural networks for lifelong learning},
  volume       = {32},
  year         = {2020},
}
</textarea>
</details></li>
</ul>

</body>
</html>
