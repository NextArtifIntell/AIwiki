<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>JAIR_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="jair---84">JAIR - 84</h2>
<ul>
<li><details>
<summary>
(2020). Diagnosis of deep discrete-event systems. <em>JAIR</em>,
<em>69</em>, 1473–1532. (<a
href="https://doi.org/10.1613/jair.1.12171">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {An abduction-based diagnosis technique for a class of discrete-event systems (DESs), called deep DESs (DDESs), is presented. A DDES has a tree structure, where each node is a network of communicating automata, called an active unit (AU). The interaction of components within an AU gives rise to emergent events. An emergent event occurs when specific components collectively perform a sequence of transitions matching a given regular language. Any event emerging in an AU triggers the transition of a component in its parent AU. We say that the DDES has a deep behavior, in the sense that the behavior of an AU is governed not only by the events exchanged by the components within the AU but also by the events emerging from child AUs. Deep behavior characterizes not only living beings, including humans, but also artifacts, such as robots that operate in contexts at varying abstraction levels. Surprisingly, experimental results indicate that the hierarchical complexity of the system translates into a decreased computational complexity of the diagnosis task. Hence, the diagnosis technique is shown to be (formally) correct as well as (empirically) efficient.},
  archive      = {J_JAIR},
  author       = {Gianfranco Lamperti and Marina Zanella and Xiangfu Zhao},
  doi          = {10.1613/jair.1.12171},
  journal      = {Journal of Artificial Intelligence Research},
  pages        = {1473-1532},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {Diagnosis of deep discrete-event systems},
  volume       = {69},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Deep reinforcement learning: A state-of-the-art walkthrough.
<em>JAIR</em>, <em>69</em>, 1421–1471. (<a
href="https://doi.org/10.1613/jair.1.12412">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep Reinforcement Learning is a topic that has gained a lot of attention recently, due to the unprecedented achievements and remarkable performance of such algorithms in various benchmark tests and environmental setups. The power of such methods comes from the combination of an already established and strong field of Deep Learning, with the unique nature of Reinforcement Learning methods. It is, however, deemed necessary to provide a compact, accurate and comparable view of these methods and their results for the means of gaining valuable technical and practical insights. In this work we gather the essential methods related to Deep Reinforcement Learning, extracting common property structures for three complementary core categories: a) Model-Free, b) Model-Based and c) Modular algorithms. For each category, we present, analyze and compare state-of-the-art Deep Reinforcement Learning algorithms that achieve high performance in various environments and tackle challenging problems in complex and demanding tasks. In order to give a compact and practical overview of their differences, we present comprehensive comparison figures and tables, produced by reported performances of the algorithms under two popular simulation platforms: the Atari Learning Environment and the MuJoCo physics simulation platform. We discuss the key differences of the various kinds of algorithms, indicate their potential and limitations, as well as provide insights to researchers regarding future directions of the field.},
  archive      = {J_JAIR},
  author       = {Aristotelis Lazaridis and Anestis Fachantidis and Ioannis Vlahavas},
  doi          = {10.1613/jair.1.12412},
  journal      = {Journal of Artificial Intelligence Research},
  pages        = {1421-1471},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {Deep reinforcement learning: A state-of-the-art walkthrough},
  volume       = {69},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Bounds on the size of PC and URC formulas. <em>JAIR</em>,
<em>69</em>, 1395–1420. (<a
href="https://doi.org/10.1613/jair.1.12006">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we investigate CNF encodings, for which unit propagation is strong enough to derive a contradiction if the encoding is not consistent with a partial assignment of the variables (unit refutation complete or URC encoding) or additionally to derive all implied literals if the encoding is consistent with the partial assignment (propagation complete or PC encoding). We prove an exponential separation between the sizes of PC and URC encodings without auxiliary variables and strengthen the known results on their relationship to the PC and URC encodings that can use auxiliary variables. Besides of this, we prove that the sizes of any two irredundant PC formulas representing the same function differ at most by a factor polynomial in the number of the variables and present an example of a function demonstrating that a similar statement is not true for URC formulas. One of the separations above implies that a q-Horn formula may require an exponential number of additional clauses to become a URC formula. On the other hand, for every q-Horn formula, we present a polynomial size URC encoding of the same function using auxiliary variables. This encoding is not q-Horn in general.},
  archive      = {J_JAIR},
  author       = {Petr Kučera and Petr Savický},
  doi          = {10.1613/jair.1.12006},
  journal      = {Journal of Artificial Intelligence Research},
  pages        = {1395-1420},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {Bounds on the size of PC and URC formulas},
  volume       = {69},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). An evaluation of communication protocol languages for
engineering multiagent systems. <em>JAIR</em>, <em>69</em>, 1351–1393.
(<a href="https://doi.org/10.1613/jair.1.12212">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Communication protocols are central to engineering decentralized multiagent systems. Modern protocol languages are typically formal and address aspects of decentralization, such as asynchrony. However, modern languages differ in important ways in their basic abstractions and operational assumptions. This diversity makes a comparative evaluation of protocol languages a challenging task. We contribute a rich evaluation of diverse and modern protocol languages. Among the selected languages, Scribble is based on session types; Trace-C and Trace-F on trace expressions; HAPN on hierarchical state machines, and BSPL on information causality. Our contribution is four-fold. One, we contribute important criteria for evaluating protocol languages. Two, for each criterion, we compare the languages on the basis of whether they are able to specify elementary protocols that go to the heart of the criterion. Three, for each language, we map our findings to a canonical architecture style for multiagent systems, highlighting where the languages depart from the architecture. Four, we identify design principles for protocol languages as guidance for future research.},
  archive      = {J_JAIR},
  author       = {Amit K Chopra and Samuel H Christie V and Munindar P. Singh},
  doi          = {10.1613/jair.1.12212},
  journal      = {Journal of Artificial Intelligence Research},
  pages        = {1351-1393},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {An evaluation of communication protocol languages for engineering multiagent systems},
  volume       = {69},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). On the complexity of learning a class ratio from unlabeled
data. <em>JAIR</em>, <em>69</em>, 1333–1349. (<a
href="https://doi.org/10.1613/jair.1.12013">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the problem of learning a class ratio from unlabeled data, which we call CR learning, the training data is unlabeled, and only the ratios, or proportions, of examples receiving each label are given. The goal is to learn a hypothesis that predicts the proportions of labels on the distribution underlying the sample. This model of learning is applicable to a wide variety of settings, including predicting the number of votes for candidates in political elections from polls. In this paper, we formally define this class and resolve foundational questions regarding the computational complexity of CR learning and characterize its relationship to PAC learning. Among our results, we show, perhaps surprisingly, that for finite VC classes what can be efficiently CR learned is a strict subset of what can be learned efficiently in PAC, under standard complexity assumptions. We also show that there exist classes of functions whose CR learnability is independent of ZFC, the standard set theoretic axioms. This implies that CR learning cannot be easily characterized (like PAC by VC dimension).},
  archive      = {J_JAIR},
  author       = {Benjamin Fish and Lev Reyzin},
  doi          = {10.1613/jair.1.12013},
  journal      = {Journal of Artificial Intelligence Research},
  pages        = {1333–1349},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {On the complexity of learning a class ratio from unlabeled data},
  volume       = {69},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Adapting behavior via intrinsic reward: A survey and
empirical study. <em>JAIR</em>, <em>69</em>, 1287–1332. (<a
href="https://doi.org/10.1613/jair.1.12087">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Learning about many things can provide numerous benefits to a reinforcement learning system. For example, learning many auxiliary value functions, in addition to optimizing the environmental reward, appears to improve both exploration and representation learning. The question we tackle in this paper is how to sculpt the stream of experience—how to adapt the learning system’s behavior—to optimize the learning of a collection of value functions. A simple answer is to compute an intrinsic reward based on the statistics of each auxiliary learner, and use reinforcement learning to maximize that intrinsic reward. Unfortunately, implementing this simple idea has proven difficult, and thus has been the focus of decades of study. It remains unclear which of the many possible measures of learning would work well in a parallel learning setting where environmental reward is extremely sparse or absent. In this paper, we investigate and compare different intrinsic reward mechanisms in a new bandit-like parallel-learning testbed. We discuss the interaction between reward and prediction learners and highlight the importance of introspective prediction learners: those that increase their rate of learning when progress is possible, and decrease when it is not. We provide a comprehensive empirical comparison of 14 different rewards, including well-known ideas from reinforcement learning and active learning. Our results highlight a simple but seemingly powerful principle: intrinsic rewards based on the amount of learning can generate useful behavior, if each individual learner is introspective.},
  archive      = {J_JAIR},
  author       = {Cam Linke and Nadia M. Ady and Martha White and Thomas Degris and Adam White},
  doi          = {10.1613/jair.1.12087},
  journal      = {Journal of Artificial Intelligence Research},
  pages        = {1287-1332},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {Adapting behavior via intrinsic reward: A survey and empirical study},
  volume       = {69},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Reviewing autoencoders for missing data imputation:
Technical trends, applications and outcomes. <em>JAIR</em>, <em>69</em>,
1255–1285. (<a href="https://doi.org/10.1613/jair.1.12312">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Missing data is a problem often found in real-world datasets and it can degrade the performance of most machine learning models. Several deep learning techniques have been used to address this issue, and one of them is the Autoencoder and its Denoising and Variational variants. These models are able to learn a representation of the data with missing values and generate plausible new ones to replace them. This study surveys the use of Autoencoders for the imputation of tabular data and considers 26 works published between 2014 and 2020. The analysis is mainly focused on discussing patterns and recommendations for the architecture, hyperparameters and training settings of the network, while providing a detailed discussion of the results obtained by Autoencoders when compared to other state-of-the-art methods, and of the data contexts where they have been applied. The conclusions include a set of recommendations for the technical settings of the network, and show that Denoising Autoencoders outperform their competitors, particularly the often used statistical methods.},
  archive      = {J_JAIR},
  author       = {Ricardo Cardoso Pereira and Miriam Seoane Santos and Pedro Pereira Rodrigues and Pedro Henriques Abreu},
  doi          = {10.1613/jair.1.12312},
  journal      = {Journal of Artificial Intelligence Research},
  pages        = {1255-1285},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {Reviewing autoencoders for missing data imputation: Technical trends, applications and outcomes},
  volume       = {69},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Lifted bayesian filtering in multiset rewriting systems.
<em>JAIR</em>, <em>69</em>, 1203–1254. (<a
href="https://doi.org/10.1613/jair.1.12066">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a model for Bayesian filtering (BF) in discrete dynamic systems where multiple entities (inter)-act, i.e. where the system dynamics is naturally described by a Multiset rewriting system (MRS). Typically, BF in such situations is computationally expensive due to the high number of discrete states that need to be maintained explicitly. We devise a lifted state representation, based on a suitable decomposition of multiset states, such that some factors of the distribution are exchangeable and thus afford an efficient representation. Intuitively, this representation groups together similar entities whose properties follow an exchangeable joint distribution. Subsequently, we introduce a BF algorithm that works directly on lifted states, without resorting to the original, much larger ground representation. This algorithm directly lends itself to approximate versions by limiting the number of explicitly represented lifted states in the posterior. We show empirically that the lifted representation can lead to a factorial reduction in the representational complexity of the distribution, and in the approximate cases can lead to a lower variance of the estimate and a lower estimation error compared to the original, ground representation.},
  archive      = {J_JAIR},
  author       = {Stefan Lüdtke and Thomas Kirste},
  doi          = {10.1613/jair.1.12066},
  journal      = {Journal of Artificial Intelligence Research},
  pages        = {1203-1254},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {Lifted bayesian filtering in multiset rewriting systems},
  volume       = {69},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Adaptive stress testing: Finding likely failure events with
reinforcement learning. <em>JAIR</em>, <em>69</em>, 1165–1201. (<a
href="https://doi.org/10.1613/jair.1.12190">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Finding the most likely path to a set of failure states is important to the analysis of safety-critical systems that operate over a sequence of time steps, such as aircraft collision avoidance systems and autonomous cars. In many applications such as autonomous driving, failures cannot be completely eliminated due to the complex stochastic environment in which the system operates. As a result, safety validation is not only concerned about whether a failure can occur, but also discovering which failures are most likely to occur. This article presents adaptive stress testing (AST), a framework for finding the most likely path to a failure event in simulation. We consider a general black box setting for partially observable and continuous-valued systems operating in an environment with stochastic disturbances. We formulate the problem as a Markov decision process and use reinforcement learning to optimize it. The approach is simulation-based and does not require internal knowledge of the system, making it suitable for black-box testing of large systems. We present different formulations depending on whether the state is fully observable or partially observable. In the latter case, we present a modified Monte Carlo tree search algorithm that only requires access to the pseudorandom number generator of the simulator to overcome partial observability. We also present an extension of the framework, called differential adaptive stress testing (DAST), that can find failures that occur in one system but not in another. This type of differential analysis is useful in applications such as regression testing, where we are concerned with finding areas of relative weakness compared to a baseline. We demonstrate the effectiveness of the approach on an aircraft collision avoidance application, where a prototype aircraft collision avoidance system is stress tested to find the most likely scenarios of near mid-air collision.},
  archive      = {J_JAIR},
  author       = {Ritchie Lee and Ole J. Mengshoel and Anshu Saksena and Ryan W. Gardner and Daniel Genin and Joshua Silbermann and Michael Owen and Mykel J. Kochenderfer},
  doi          = {10.1613/jair.1.12190},
  journal      = {Journal of Artificial Intelligence Research},
  pages        = {1165-1201},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {Adaptive stress testing: Finding likely failure events with reinforcement learning},
  volume       = {69},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A differential privacy mechanism that accounts for network
effects for crowdsourcing systems. <em>JAIR</em>, <em>69</em>,
1127–1164. (<a href="https://doi.org/10.1613/jair.1.12158">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In crowdsourcing systems, it is important for the crowdsource campaign initiator to incentivize users to share their data to produce results of the desired computational accuracy. This problem becomes especially challenging when users are concerned about the privacy of their data. To overcome this challenge, existing work often aims to provide users with differential privacy guarantees to incentivize privacy-sensitive users to share their data. However, this work neglects the network effect that a user enjoys greater privacy protection when he aligns his participation behaviour with that of other users. To explore this network effect, we formulate the interaction among users regarding their participation decisions as a population game, because a user’s welfare from the interaction depends not only on his own participation decision but also the distribution of others’ decisions. We show that the Nash equilibrium of this game consists of a threshold strategy, where all users whose privacy sensitivity is below a certain threshold will participate and the remaining users will not. We characterize the existence and uniqueness of this equilibrium, which depends on the privacy guarantee, the reward provided by the initiator and the population size. Based on this equilibria analysis, we design the PINE (Privacy Incentivization with Network Effects) mechanism and prove that it maximizes the initiator’s payoff while providing participating users with a guaranteed degree of privacy protection. Numerical simulations, on both real and synthetic data, show that (i) PINE improves the initiator’s expected payoff by up to 75\%, compared to state of the art mechanisms that do not consider this effect; (ii) the performance gain by exploiting the network effect is particularly good when the majority of users are flexible over their privacy attitudes and when there are a large number of low quality task performers.},
  archive      = {J_JAIR},
  author       = {Yuan Luo and Nicholas R. Jennings},
  doi          = {10.1613/jair.1.12158},
  journal      = {Journal of Artificial Intelligence Research},
  pages        = {1127-1164},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {A differential privacy mechanism that accounts for network effects for crowdsourcing systems},
  volume       = {69},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Epistemic argumentation framework: Theory and computation.
<em>JAIR</em>, <em>69</em>, 1103–1126. (<a
href="https://doi.org/10.1613/jair.1.12121">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The paper introduces the notion of an epistemic argumentation framework&amp;nbsp;(EAF) as a means to integrate the beliefs of a reasoner with argumentation. Intuitively, an EAF encodes the beliefs of an agent who reasons about arguments. Formally, an EAF is a pair of an argumentation framework and an epistemic constraint. The semantics of the EAF is defined by the notion of an ω-epistemic labelling set,&amp;nbsp;where ω is complete, stable, grounded, or preferred, which is a set of ω-labellings that collectively satisfies the epistemic constraint of the EAF. The paper shows how EAF can represent different views of reasoners on the same argumentation framework. It also includes representing preferences in EAF and multi-agent argumentation. Finally, the paper discusses complexity issues and computation using epistemic logic programming.},
  archive      = {J_JAIR},
  author       = {Chiaki Sakama and Tran Cao Son},
  doi          = {10.1613/jair.1.12121},
  journal      = {Journal of Artificial Intelligence Research},
  pages        = {1103-1126},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {Epistemic argumentation framework: Theory and computation},
  volume       = {69},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Representing fitness landscapes by valued constraints to
understand the complexity of local search. <em>JAIR</em>, <em>69</em>,
1077–1102. (<a href="https://doi.org/10.1613/jair.1.12156">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Local search is widely used to solve combinatorial optimisation problems and to model biological evolution, but the performance of local search algorithms on different kinds of fitness landscapes is poorly understood. Here we consider how fitness landscapes can be represented using valued constraints, and investigate what the structure of such representations reveals about the complexity of local search. &amp;nbsp; &amp;nbsp; &amp;nbsp;First, we show that for fitness landscapes representable by binary Boolean valued constraints there is a minimal necessary constraint graph that can be easily computed. Second, we consider landscapes as equivalent if they allow the same (improving) local search moves; we show that a minimal constraint graph still exists, but is NP-hard to compute. &amp;nbsp; &amp;nbsp; &amp;nbsp;We then develop several techniques to bound the length of any sequence of local search moves. We show that such a bound can be obtained from the numerical values of the constraints in the representation, and show how this bound may be tightened by considering equivalent representations. In the binary Boolean case, we prove that a degree 2 or treestructured constraint graph gives a quadratic bound on the number of improving moves made by any local search; hence, any landscape that can be represented by such a model will be tractable for any form of local search. &amp;nbsp; &amp;nbsp; &amp;nbsp;Finally, we build two families of examples to show that the conditions in our tractability results are essential. With domain size three, even just a path of binary constraints can model a landscape with an exponentially long sequence of improving moves. With a treewidth-two constraint graph, even with a maximum degree of three, binary Boolean constraints can model a landscape with an exponentially long sequence of improving moves.},
  archive      = {J_JAIR},
  author       = {Artem Kaznatcheev and David Cohen and Peter Jeavons},
  doi          = {10.1613/jair.1.12156},
  journal      = {Journal of Artificial Intelligence Research},
  pages        = {1077-1102},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {Representing fitness landscapes by valued constraints to understand the complexity of local search},
  volume       = {69},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Credibility-limited base revision: New classes and their
characterizations. <em>JAIR</em>, <em>69</em>, 1023–1075. (<a
href="https://doi.org/10.1613/jair.1.12298">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper we study a kind of operator —known as credibility-limited base revisions— which addresses two of the main issues that have been pointed out to the AGM model of belief change. Indeed, on the one hand, these operators are defined on belief bases (rather than belief sets) and, on the other hand, they are constructed with the underlying idea that not all new information is accepted. We propose twenty different classes of credibility-limited base revision operators and obtain axiomatic characterizations for each of them. Additionally we thoroughly investigate the interrelations (in the sense of inclusion) among all those classes. More precisely, we analyse whether each one of those classes is or is not (strictly) contained in each of the remaining ones.},
  archive      = {J_JAIR},
  author       = {Marco Garapa and Eduardo Fermé and Maurício Reis},
  doi          = {10.1613/jair.1.12298},
  journal      = {Journal of Artificial Intelligence Research},
  pages        = {1023-1075},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {Credibility-limited base revision: New classes and their characterizations},
  volume       = {69},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Modular structures and atomic decomposition in ontologies.
<em>JAIR</em>, <em>69</em>, 963–1021. (<a
href="https://doi.org/10.1613/jair.1.12151">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the growth of ontologies used in diverse application areas, the need for module extraction and modularisation techniques has risen. The notion of the modular structure of an ontology, which comprises a suitable set of base modules together with their logical dependencies, has the potential to help users and developers in comprehending, sharing, and maintaining an ontology. We have developed a new modular structure, called atomic decomposition (AD), which is based on modules that provide strong logical properties, such as locality-based modules. In this article, we present the theoretical foundations of AD, review its logical and computational properties, discuss its suitability as a modular structure, and report on an experimental evaluation of AD. In addition, we discuss the concept of a modular structure in ontology engineering and provide a survey of existing decomposition approaches.},
  archive      = {J_JAIR},
  author       = {Chiara Del Vescovo and Matthew Horridge and Bijan Parsia and Uli Sattler and Thomas Schneider and Haoruo Zhao},
  doi          = {10.1613/jair.1.12151},
  journal      = {Journal of Artificial Intelligence Research},
  pages        = {963–1021},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {Modular structures and atomic decomposition in ontologies},
  volume       = {69},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Qualitative numeric planning: Reductions and complexity.
<em>JAIR</em>, <em>69</em>, 923–961. (<a
href="https://doi.org/10.1613/jair.1.11865">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Qualitative numerical planning is classical planning extended with non-negative real variables that can be increased or decreased &quot;qualitatively&quot;, i.e., by positive indeterminate amounts. While deterministic planning with numerical variables is undecidable in general, qualitative numerical planning is decidable and provides a convenient abstract model for generalized planning. The solutions to qualitative numerical problems (QNPs) were shown to correspond to the strong cyclic solutions of an associated fully observable non-deterministic (FOND) problem that terminate. This leads to a generate-and-test algorithm for solving QNPs where solutions to a FOND problem are generated one by one and tested for termination. The computational shortcomings of this approach for solving QNPs, however, are that it is not simple to amend FOND planners to generate all solutions, and that the number of solutions to check can be doubly exponential in the number of variables. In this work we address these limitations while providing additional insights on QNPs. More precisely, we introduce two polynomial-time reductions, one from QNPs to FOND problems and the other from FOND problems to QNPs both of which do not involve termination tests. A result of these reductions is that QNPs are shown to have the same expressive power and the same complexity as FOND problems.},
  archive      = {J_JAIR},
  author       = {Blai Bonet and Hector Geffner},
  doi          = {10.1613/jair.1.11865},
  journal      = {Journal of Artificial Intelligence Research},
  pages        = {923–961},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {Qualitative numeric planning: Reductions and complexity},
  volume       = {69},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). To regulate or not: A social dynamics analysis of an
idealised AI race. <em>JAIR</em>, <em>69</em>, 881–921. (<a
href="https://doi.org/10.1613/jair.1.12225">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Rapid technological advancements in Artificial Intelligence (AI), as well as the growing deployment of intelligent technologies in new application domains, have generated serious anxiety and a fear of missing out among different stake-holders, fostering a racing narrative. Whether real or not, the belief in such a race for domain supremacy through AI, can make it real simply from its consequences, as put forward by the Thomas theorem. These consequences may be negative, as racing for technological supremacy creates a complex ecology of choices that could push stake-holders to underestimate or even ignore ethical and safety procedures. As a consequence, different actors are urging to consider both the normative and social impact of these technological advancements, contemplating the use of the precautionary principle in AI innovation and research. Yet, given the breadth and depth of AI and its advances, it is difficult to assess which technology needs regulation and when. As there is no easy access to data describing this alleged AI race, theoretical models are necessary to understand its potential dynamics, allowing for the identification of when procedures need to be put in place to favour outcomes beneficial for all. We show that, next to the risks of setbacks and being reprimanded for unsafe behaviour, the time-scale in which domain supremacy can be achieved plays a crucial role. When this can be achieved in a short term, those who completely ignore the safety precautions are bound to win the race but at a cost to society, apparently requiring regulatory actions. Our analysis reveals that imposing regulations for all risk and timing conditions may not have the anticipated effect as only for specific conditions a dilemma arises between what is individually preferred and globally beneficial. Similar observations can be made for the long-term development case. Yet different from the short-term situation, conditions can be identified that require the promotion of risk-taking as opposed to compliance with safety regulations in order to improve social welfare. These results remain robust both when two or several actors are involved in the race and when collective rather than individual setbacks are produced by risk-taking behaviour. When defining codes of conduct and regulatory policies for applications of AI, a clear understanding of the time-scale of the race is thus required, as this may induce important non-trivial effects. This article is part of the special track on AI and Society. &amp;nbsp;},
  archive      = {J_JAIR},
  author       = {The Anh Han and Luis Moniz Pereira and Francisco C. Santos and Tom Lenaerts},
  doi          = {10.1613/jair.1.12225},
  journal      = {Journal of Artificial Intelligence Research},
  pages        = {881-921},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {To regulate or not: A social dynamics analysis of an idealised AI race},
  volume       = {69},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Contrasting the spread of misinformation in online social
networks. <em>JAIR</em>, <em>69</em>, 847–879. (<a
href="https://doi.org/10.1613/jair.1.11509">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Online social networks are nowadays one of the most effective and widespread tools used to share information. In addition to being employed by individuals for communicating with friends and acquaintances, and by brands for marketing and customer service purposes, they constitute a primary source of daily news for a significant number of users. Unfortunately, besides legit news, social networks also allow to effectively spread inaccurate or even entirely fabricated ones. Also due to sensationalist claims, misinformation can spread from the original sources to a large number of users in a very short time, with negative consequences that, in extreme cases, can even put at risk public safety or health. In this work we discuss and propose methods to limit the spread of misinformation over online social networks. The issue is split in two separate sub-problems. We first aim to identify the most probable sources of the misinformation among the subset of users that have been reached by it. In the second step, assuming to know the misinformation sources, we want to locate a minimum number of monitors (that is, entities able to identify and block false information) in the network in order to prevent that the misinformation campaign reaches some “critical” nodes while maintaining low the number of nodes exposed to the infection. For each of the two issues, we provide both heuristics and mixed integer programming formulations. To verify the quality and efficiency of our suggested solutions, we conduct experiments on several real-world networks. The results of this extensive experimental phase validate our heuristics as effective tools to contrast the spread of misinformation in online social networks. Regarding the source identification step, our approach showed success rates above 80\% in most of the considered settings, and above 60\% in almost all of them. With respect to the second issue, our heuristic proved to be able to obtain solutions that exceeded (in terms of number of required monitors) the ones obtained through our MILP-based approach of more than 20\% in only few test scenarios. Our heuristics for both problems also proved to outperform significantly some previously proposed algorithms.},
  archive      = {J_JAIR},
  author       = {Marco Amoruso and Daniele Anello and Vincenzo Auletta and Raffaele Cerulli and Diodato Ferraioli and Andrea Raiconi},
  doi          = {10.1613/jair.1.11509},
  journal      = {Journal of Artificial Intelligence Research},
  pages        = {847-879},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {Contrasting the spread of misinformation in online social networks},
  volume       = {69},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Mapping the landscape of artificial intelligence
applications against COVID-19. <em>JAIR</em>, <em>69</em>, 807–845. (<a
href="https://doi.org/10.1613/jair.1.12162">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {COVID-19, the disease caused by the SARS-CoV-2 virus, has been declared a pandemic by the World Health Organization, which has reported over 18 million confirmed cases as of August 5, 2020. In this review, we present an overview of recent studies using Machine Learning and, more broadly, Artificial Intelligence, to tackle many aspects of the COVID19 crisis. We have identified applications that address challenges posed by COVID-19 at different scales, including: molecular, by identifying new or existing drugs for treatment; clinical, by supporting diagnosis and evaluating prognosis based on medical imaging and non-invasive measures; and societal, by tracking both the epidemic and the accompanying infodemic using multiple data sources. We also review datasets, tools, and resources needed to facilitate Artificial Intelligence research, and discuss strategic considerations related to the operational implementation of multidisciplinary partnerships and open science. We highlight the need for international cooperation to maximize the potential of AI in this and future pandemics.},
  archive      = {J_JAIR},
  author       = {Joseph Bullock and Alexandra Luccioni and Katherine Hoffman Pham and Cynthia Sin Nga Lam and Miguel Luengo-Oroz},
  doi          = {10.1613/jair.1.12162},
  journal      = {Journal of Artificial Intelligence Research},
  pages        = {807-845},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {Mapping the landscape of artificial intelligence applications against COVID-19},
  volume       = {69},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Using machine learning for decreasing state uncertainty in
planning. <em>JAIR</em>, <em>69</em>, 765–806. (<a
href="https://doi.org/10.1613/jair.1.11567">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a novel approach for decreasing state uncertainty in planning prior to solving the planning problem. This is done by making predictions about the state based on currently known information, using machine learning techniques. For domains where uncertainty is high, we define an active learning process for identifying which information, once sensed, will best improve the accuracy of predictions. We demonstrate that an agent is able to solve problems with uncertainties in the state with less planning effort compared to standard planning techniques. Moreover, agents can solve problems for which they could not find valid plans without using predictions. Experimental results also demonstrate that using our active learning process for identifying information to be sensed leads to gathering information that improves the prediction process.},
  archive      = {J_JAIR},
  author       = {Senka Krivic and Michael Cashmore and Daniele Magazzeni and Sandor Szedmak and Justus Piater},
  doi          = {10.1613/jair.1.11567},
  journal      = {Journal of Artificial Intelligence Research},
  pages        = {765-806},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {Using machine learning for decreasing state uncertainty in planning},
  volume       = {69},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Structure from randomness in halfspace learning with the
zero-one loss. <em>JAIR</em>, <em>69</em>, 733–764. (<a
href="https://doi.org/10.1613/jair.1.11506">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We prove risk bounds for halfspace learning when the data dimensionality is allowed to be larger than the sample size, using a notion of compressibility by random projection. In particular, we give upper bounds for the empirical risk minimizer learned efficiently from randomly projected data, as well as uniform upper bounds in the full high-dimensional space. Our main findings are the following: i) In both settings, the obtained bounds are able to discover and take advantage of benign geometric structure, which turns out to depend on the cosine similarities between the classifier and points of the input space, and provide a new interpretation of margin distribution type arguments. ii) Furthermore our bounds allow us to draw new connections between several existing successful classification algorithms, and we also demonstrate that our theory is predictive of empirically observed performance in numerical simulations and experiments. iii) Taken together, these results suggest that the study of compressive learning can improve our understanding of which benign structural traits – if they are possessed by the data generator – make it easier to learn an effective classifier from a sample.},
  archive      = {J_JAIR},
  author       = {Ata Kaban and Robert J. Durrant},
  doi          = {10.1613/jair.1.11506},
  journal      = {Journal of Artificial Intelligence Research},
  pages        = {733-764},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {Structure from randomness in halfspace learning with the zero-one loss},
  volume       = {69},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). The complexity landscape of outcome determination in
judgment aggregation. <em>JAIR</em>, <em>69</em>, 687–731. (<a
href="https://doi.org/10.1613/jair.1.11970">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We provide a comprehensive analysis of the computational complexity of the outcome determination problem for the most important aggregation rules proposed in the literature on logic-based judgment aggregation. Judgment aggregation is a powerful and flexible framework for studying problems of collective decision making that has attracted interest in a range of disciplines, including Legal Theory, Philosophy, Economics, Political Science, and Artificial Intelligence. The problem of computing the outcome for a given list of individual judgments to be aggregated into a single collective judgment is the most fundamental algorithmic challenge arising in this context. Our analysis applies to several different variants of the basic framework of judgment aggregation that have been discussed in the literature, as well as to a new framework that encompasses all existing such frameworks in terms of expressive power and representational succinctness.},
  archive      = {J_JAIR},
  author       = {Ulle Endriss and Ronald de Haan and Jérôme Lang and Marija Slavkovik},
  doi          = {10.1613/jair.1.11970},
  journal      = {Journal of Artificial Intelligence Research},
  pages        = {687–731},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {The complexity landscape of outcome determination in judgment aggregation},
  volume       = {69},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Belief change and 3-valued logics: Characterization of
19,683 belief change operators. <em>JAIR</em>, <em>69</em>, 657–685. (<a
href="https://doi.org/10.1613/jair.1.12091">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this work we introduce a 3-valued logic with modalities, with the aim of having a clear and precise representation of epistemic states, thus the formulas of this logic will be our epistemic states. Indeed, these formulas are identified with ranking functions of 3 values, a generalization of total preorders of three levels. In this framework we analyze some types of changes of these epistemic structures and give syntactical characterizations of them in the introduced logic. In particular, we introduce and study carefully a new operator called Cautious Improvement operator. We also characterize all operators that are definable in this framework.},
  archive      = {J_JAIR},
  author       = {Nerio Borges and Ramón Pino Pérez},
  doi          = {10.1613/jair.1.12091},
  journal      = {Journal of Artificial Intelligence Research},
  pages        = {657-685},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {Belief change and 3-valued logics: Characterization of 19,683 belief change operators},
  volume       = {69},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Maximin share allocations on cycles. <em>JAIR</em>,
<em>69</em>, 613–655. (<a
href="https://doi.org/10.1613/jair.1.11702">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The problem of fair division of indivisible goods is a fundamental problem of resource allocation in multi-agent systems, also studied extensively in social choice. Recently, the problem was generalized to the case when goods form a graph and the goal is to allocate goods to agents so that each agent’s bundle forms a connected subgraph. For the maximin share fairness criterion, researchers proved that if goods form a tree, an allocation offering each agent a bundle of at least her maximin share value always exists. Moreover, it can be found in polynomial time. In this paper we consider the problem of maximin share allocations of goods on a cycle. Despite the simplicity of the graph, the problem turns out to be significantly harder than its tree version. We present cases when maximin share allocations of goods on cycles exist and provide in this case results on allocations guaranteeing each agent a certain fraction of her maximin share. We also study algorithms for computing maximin share allocations of goods on cycles.},
  archive      = {J_JAIR},
  author       = {Miroslaw Truszczynski and Zbigniew Lonc},
  doi          = {10.1613/jair.1.11702},
  journal      = {Journal of Artificial Intelligence Research},
  pages        = {613-655},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {Maximin share allocations on cycles},
  volume       = {69},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). The bottleneck simulator: A model-based deep reinforcement
learning approach. <em>JAIR</em>, <em>69</em>, 571–612. (<a
href="https://doi.org/10.1613/jair.1.12463">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep reinforcement learning has recently shown many impressive successes. However, one major obstacle towards applying such methods to real-world problems is their lack of data-efficiency. To this end, we propose the Bottleneck Simulator: a model-based reinforcement learning method which combines a learned, factorized transition model of the environment with rollout simulations to learn an effective policy from few examples. The learned transition model employs an abstract, discrete (bottleneck) state, which increases sample efficiency by reducing the number of model parameters and by exploiting structural properties of the environment. We provide a mathematical analysis of the Bottleneck Simulator in terms of fixed points of the learned policy, which reveals how performance is affected by four distinct sources of error: an error related to the abstract space structure, an error related to the transition model estimation variance, an error related to the transition model estimation bias, and an error related to the transition model class bias. Finally, we evaluate the Bottleneck Simulator on two natural language processing tasks: a text adventure game and a real-world, complex dialogue response selection task. On both tasks, the Bottleneck Simulator yields excellent performance beating competing approaches.},
  archive      = {J_JAIR},
  author       = {Iulian Vlad Serban and Chinnadhurai Sankar and Michael Pieper and Joelle Pineau and Yoshua Bengio},
  doi          = {10.1613/jair.1.12463},
  journal      = {Journal of Artificial Intelligence Research},
  pages        = {571-612},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {The bottleneck simulator: A model-based deep reinforcement learning approach},
  volume       = {69},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Computing bayes-nash equilibria in combinatorial auctions
with verification. <em>JAIR</em>, <em>69</em>, 531–570. (<a
href="https://doi.org/10.1613/jair.1.11525">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a new algorithm for computing pure-strategy ε-Bayes-Nash equilibria (ε-BNEs) in combinatorial auctions. The main innovation of our algorithm is to separate the algorithm’s search phase (for finding the ε-BNE) from the verification phase (for computing the ε). Using this approach, we obtain an algorithm that is both very fast and provides theoretical guarantees on the ε it finds. Our main contribution is a verification method which, surprisingly, allows us to upper bound the ε across the whole continuous value space without making assumptions about the mechanism. Using our algorithm, we can now compute ε-BNEs in multi-minded domains that are significantly more complex than what was previously possible to solve. We release our code under an open-source license to enable researchers to perform algorithmic analyses of auctions, to enable bidders to analyze different strategies, and many other applications.},
  archive      = {J_JAIR},
  author       = {Vitor Bosshard and Benedikt Bünz and Benjamin Lubin and Sven Seuken},
  doi          = {10.1613/jair.1.11525},
  journal      = {Journal of Artificial Intelligence Research},
  pages        = {531-570},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {Computing bayes-nash equilibria in combinatorial auctions with verification},
  volume       = {69},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Properties of switch-list representations of boolean
functions. <em>JAIR</em>, <em>69</em>, 501–529. (<a
href="https://doi.org/10.1613/jair.1.12199">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we focus on a less usual way to represent Boolean functions, namely on representations by switch-lists, which are closely related to interval representations. Given a truth table representation of a Boolean function f the switch-list representation of f is a list of Boolean vectors from the truth table which have a different function value than the preceding Boolean vector in the truth table. The main aim of this paper is to include this type of representation in the Knowledge Compilation Map by Darwiche and Marquis and to argue that switch-lists may in certain situations constitute a reasonable choice for a target language in knowledge compilation. First, we compare switch-list representations with a number of standard representations (such as CNF, DNF, and OBDD) with respect to their relative succinctness. As a by-product of this analysis, we also give a short proof of a longstanding open question proposed by Darwiche and Marquis, namely the incomparability of MODS (models) and PI (prime implicates) representations. Next, using the succinctness result between switch-lists and OBDDs, we develop a polynomial time compilation algorithm from switch-lists to OBDDs. Finally, we analyze which standard transformations and queries (those considered by Darwiche and Marquis) can be performed in polynomial time with respect to the size of the input if the input knowledge is represented by a switch-list. We show that this collection is very broad and the combination of polynomial time transformations and queries is quite unique. Some of the queries can be answered directly using the switch-list input, others require a compilation of the input to OBDD representations which are then used to answer the queries.},
  archive      = {J_JAIR},
  author       = {Miloš Chromý and Ondřej Čepek},
  doi          = {10.1613/jair.1.12199},
  journal      = {Journal of Artificial Intelligence Research},
  pages        = {501–529},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {Properties of switch-list representations of boolean functions},
  volume       = {69},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). The PETLON algorithm to plan efficiently for
task-level-optimal navigation. <em>JAIR</em>, <em>69</em>, 471–500. (<a
href="https://doi.org/10.1613/jair.1.12181">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Intelligent mobile robots have recently become able to operate autonomously in large-scale indoor environments for extended periods of time. In this process, mobile robots need the capabilities of both task and motion planning. Task planning in such environments involves sequencing the robot’s high-level goals and subgoals, and typically requires reasoning about the locations of people, rooms, and objects in the environment, and their interactions to achieve a goal. One of the prerequisites for optimal task planning that is often overlooked is having an accurate estimate of the actual distance (or time) a robot needs to navigate from one location to another. State-of-the-art motion planning algorithms, though often computationally complex, are designed exactly for this purpose of finding routes through constrained spaces. In this article, we focus on integrating task and motion planning (TMP) to achieve task-level-optimal planning for robot navigation while maintaining manageable computational efficiency. To this end, we introduce TMP algorithm PETLON (Planning Efficiently for Task-Level-Optimal Navigation), including two configurations with different trade-offs over computational expenses between task and motion planning, for everyday service tasks using a mobile robot. Experiments have been conducted both in simulation and on a mobile robot using object delivery tasks in an indoor office environment. The key observation from the results is that PETLON is more efficient than a baseline approach that pre-computes motion costs of all possible navigation actions, while still producing plans that are optimal at the task level. We provide results with two different task planning paradigms in the implementation of PETLON, and offer TMP practitioners guidelines for the selection of task planners from an engineering perspective.},
  archive      = {J_JAIR},
  author       = {Shih-Yun Lo and Shiqi Zhang and Peter Stone},
  doi          = {10.1613/jair.1.12181},
  journal      = {Journal of Artificial Intelligence Research},
  pages        = {471-500},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {The PETLON algorithm to plan efficiently for task-level-optimal navigation},
  volume       = {69},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). AMP chain graphs: Minimal separators and structure learning
algorithms. <em>JAIR</em>, <em>69</em>, 419–470. (<a
href="https://doi.org/10.1613/jair.1.12101">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper deals with chain graphs (CGs) under the Andersson–Madigan–Perlman (AMP) interpretation. We address the problem of finding a minimal separator in an AMP CG, namely, finding a set Z of nodes that separates a given non-adjacent pair of nodes such that no proper subset of Z separates that pair. We analyze several versions of this problem and offer polynomial time algorithms for each. These include finding a minimal separator from a restricted set of nodes, finding a minimal separator for two given disjoint sets, and testing whether a given separator is minimal. To address the problem of learning the structure of AMP CGs from data, we show that the PC-like algorithm is order dependent, in the sense that the output can depend on the order in which the variables are given. We propose several modifications of the PC-like algorithm that remove part or all of this order-dependence. We also extend the decomposition-based approach for learning Bayesian networks (BNs) to learn AMP CGs, which include BNs as a special case, under the faithfulness assumption. We prove the correctness of our extension using the minimal separator results. Using standard benchmarks and synthetically generated models and data in our experiments demonstrate the competitive performance of our decomposition-based method, called LCD-AMP, in comparison with the (modified versions of) PC-like algorithm. The LCD-AMP algorithm usually outperforms the PC-like algorithm, and our modifications of the PC-like algorithm learn structures that are more similar to the underlying ground truth graphs than the original PC-like algorithm, especially in high-dimensional settings. In particular, we empirically show that the results of both algorithms are more accurate and stabler when the sample size is reasonably large and the underlying graph is sparse},
  archive      = {J_JAIR},
  author       = {Mohammad Ali Javidian and Marco Valtorta and Pooyan Jamshidi},
  doi          = {10.1613/jair.1.12101},
  journal      = {Journal of Artificial Intelligence Research},
  pages        = {419-470},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {AMP chain graphs: Minimal separators and structure learning algorithms},
  volume       = {69},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Neural machine translation: A review. <em>JAIR</em>,
<em>69</em>, 343–418. (<a
href="https://doi.org/10.1613/jair.1.12007">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The field of machine translation (MT), the automatic translation of written text from one natural language into another, has experienced a major paradigm shift in recent years. Statistical MT, which mainly relies on various count-based models and which used to dominate MT research for decades, has largely been superseded by neural machine translation (NMT), which tackles translation with a single neural network. In this work we will trace back the origins of modern NMT architectures to word and sentence embeddings and earlier examples of the encoder-decoder network family. We will conclude with a short survey of more recent trends in the field.},
  archive      = {J_JAIR},
  author       = {Felix Stahlberg},
  doi          = {10.1613/jair.1.12007},
  journal      = {Journal of Artificial Intelligence Research},
  pages        = {343-418},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {Neural machine translation: A review},
  volume       = {69},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Planning high-level paths in hostile, dynamic, and uncertain
environments. <em>JAIR</em>, <em>69</em>, 297–342. (<a
href="https://doi.org/10.1613/jair.1.12077">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper introduces and studies a graph-based variant of the path planning problem arising in hostile environments. We consider a setting where an agent (e.g. a robot) must reach a given destination while avoiding being intercepted by probabilistic entities which exist in the graph with a given probability and move according to a probabilistic motion pattern known a priori. Given a goal vertex and a deadline to reach it, the agent must compute the path to the goal that maximizes its chances of survival. We study the computational complexity of the problem, and present two algorithms for computing high quality solutions in the general case: an exact algorithm based on Mixed-Integer Nonlinear Programming, working well in instances of moderate size, and a pseudo-polynomial time heuristic algorithm allowing to solve large scale problems in reasonable time. We also consider the two limit cases where the agent can survive with probability 0 or 1, and provide specialized algorithms to detect these kinds of situations more efficiently.},
  archive      = {J_JAIR},
  author       = {Jacopo Banfi and Vikram Shree and Mark Campbell},
  doi          = {10.1613/jair.1.12077},
  journal      = {Journal of Artificial Intelligence Research},
  pages        = {297-342},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {Planning high-level paths in hostile, dynamic, and uncertain environments},
  volume       = {69},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Improved high dimensional discrete bayesian network
inference using triplet region construction. <em>JAIR</em>, <em>69</em>,
231–295. (<a href="https://doi.org/10.1613/jair.1.12198">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Performing efficient inference on high dimensional discrete Bayesian Networks (BNs) is challenging. When using exact inference methods the space complexity can grow exponentially with the tree-width, thus making computation intractable. This paper presents a general purpose approximate inference algorithm, based on a new region belief approximation method, called Triplet Region Construction (TRC). TRC reduces the cluster space complexity for factorized models from worst-case exponential to polynomial by performing graph factorization and producing clusters of limited size. Unlike previous generations of region-based algorithms, TRC is guaranteed to converge and effectively addresses the region choice problem that bedevils other region-based algorithms used for BN inference. Our experiments demonstrate that it also achieves significantly more accurate results than competing algorithms.},
  archive      = {J_JAIR},
  author       = {Peng Lin and Martin Neil and Norman Fenton},
  doi          = {10.1613/jair.1.12198},
  journal      = {Journal of Artificial Intelligence Research},
  pages        = {231-295},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {Improved high dimensional discrete bayesian network inference using triplet region construction},
  volume       = {69},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). The parameterized complexity of motion planning for
snake-like robots. <em>JAIR</em>, <em>69</em>, 191–229. (<a
href="https://doi.org/10.1613/jair.1.11864">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study the parameterized complexity of a variant of the classic video game Snake that models real-world problems of motion planning. Given a snake-like robot with an initial position and a final position in an environment (modeled by a graph), our objective is to determine whether the robot can reach the final position from the initial position without intersecting itself. Naturally, this problem models a wide-variety of scenarios, ranging from the transportation of linked wagons towed by a locomotor at an airport or a supermarket to the movement of a group of agents that travel in an “ant-like” fashion and the construction of trains in amusement parks. Unfortunately, already on grid graphs, this problem is PSPACE-complete. Nevertheless, we prove that even on general graphs, the problem is solvable in FPT time with respect to the size of the snake. In particular, this shows that the problem is fixed-parameter tractable (FPT). Towards this, we show how to employ color-coding to sparsify the configuration graph of the problem to reduce its size significantly. We believe that our approach will find other applications in motion planning. Additionally, we show that the problem is unlikely to admit a polynomial kernel even on grid graphs, but it admits a treewidth-reduction procedure. To the best of our knowledge, the study of the parameterized complexity of motion planning problems (where the intermediate configurations of the motion are of importance) has so far been largely overlooked. Thus, our work is pioneering in this regard.},
  archive      = {J_JAIR},
  author       = {Siddharth Gupta and Guy Sa&#39;ar and Meirav Zehavi},
  doi          = {10.1613/jair.1.11864},
  journal      = {Journal of Artificial Intelligence Research},
  pages        = {191-229},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {The parameterized complexity of motion planning for snake-like robots},
  volume       = {69},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Annotator rationales for labeling tasks in crowdsourcing.
<em>JAIR</em>, <em>69</em>, 143–189. (<a
href="https://doi.org/10.1613/jair.1.12012">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {When collecting item ratings from human judges, it can be difficult to measure and enforce data quality due to task subjectivity and lack of transparency into how judges make each rating decision. To address this, we investigate asking judges to provide a specific form of rationale supporting each rating decision. We evaluate this approach on an information retrieval task in which human judges rate the relevance of Web pages for different search topics. Cost-benefit analysis over 10,000 judgments collected on Amazon’s Mechanical Turk suggests a win-win. Firstly, rationales yield a multitude of benefits: more reliable judgments, greater transparency for evaluating both human raters and their judgments, reduced need for expert gold, the opportunity for dual-supervision from ratings and rationales, and added value from the rationales themselves. Secondly, once experienced in the task, crowd workers provide rationales with almost no increase in task completion time. Consequently, we can realize the above benefits with minimal additional cost.},
  archive      = {J_JAIR},
  author       = {Mucahid Kutlu and Tyler McDonnell and Tamer Elsayed and Matthew Lease},
  doi          = {10.1613/jair.1.12012},
  journal      = {Journal of Artificial Intelligence Research},
  pages        = {143-189},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {Annotator rationales for labeling tasks in crowdsourcing},
  volume       = {69},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Contiguous cake cutting: Hardness results and approximation
algorithms. <em>JAIR</em>, <em>69</em>, 109–141. (<a
href="https://doi.org/10.1613/jair.1.12222">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study the fair allocation of a cake, which serves as a metaphor for a divisible resource, under the requirement that each agent should receive a contiguous piece of the cake. While it is known that no finite envy-free algorithm exists in this setting, we exhibit efficient algorithms that produce allocations with low envy among the agents. We then establish NP-hardness results for various decision problems on the existence of envy-free allocations, such as when we fix the ordering of the agents or constrain the positions of certain cuts. In addition, we consider a discretized setting where indivisible items lie on a line and show a number of hardness results extending and strengthening those from prior work. Finally, we investigate connections between approximate and exact envy-freeness, as well as between continuous and discrete cake cutting.},
  archive      = {J_JAIR},
  author       = {Paul Goldberg and Alexandros Hollender and Warut Suksompong},
  doi          = {10.1613/jair.1.12222},
  journal      = {Journal of Artificial Intelligence Research},
  pages        = {109-141},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {Contiguous cake cutting: Hardness results and approximation algorithms},
  volume       = {69},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Incompatibilities between iterated and relevance-sensitive
belief revision. <em>JAIR</em>, <em>69</em>, 85–108. (<a
href="https://doi.org/10.1613/jair.1.11871">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The AGM paradigm for belief change, as originally introduced by Alchourron, Gärdenfors and Makinson, lacks any guidelines for the process of iterated revision. One of the most influential work addressing this problem is Darwiche and Pearl&#39;s approach (DP approach, for short), which, despite its well-documented shortcomings, remains to this date the most dominant. In this article, we make further observations on the DP approach. In particular, we prove that the DP postulates are, in a strong sense, inconsistent with Parikh&#39;s relevance-sensitive axiom (P), extending previous initial conflicts. Immediate consequences of this result are that an entire class of intuitive revision operators, which includes Dalal&#39;s operator, violates the DP postulates, as well as that the Independence postulate and Spohn&#39;s conditionalization are inconsistent with axiom (P). The whole study, essentially, indicates that two fundamental aspects of the revision process, namely, iteration and relevance, are in deep conflict, and opens the discussion for a potential reconciliation towards a comprehensive formal framework for knowledge dynamics.},
  archive      = {J_JAIR},
  author       = {Theofanis Aravanis and Pavlos Peppas and Mary-Anne Williams},
  doi          = {10.1613/jair.1.11871},
  journal      = {Journal of Artificial Intelligence Research},
  pages        = {85-108},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {Incompatibilities between iterated and relevance-sensitive belief revision},
  volume       = {69},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). On sparse discretization for graphical games. <em>JAIR</em>,
<em>69</em>, 67–84. (<a
href="https://doi.org/10.1613/jair.1.12391">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graphical games are one of the earliest examples of the impact that the general field of graphical models have had in other areas, and in this particular case, in classical mathematical models in game theory. Graphical multi-hypermatrix games, a concept formally introduced in this research note, generalize graphical games while allowing the possibility of further space savings in model representation to that of standard graphical games. The main focus of this research note is discretization schemes for computing approximate Nash equilibria, with emphasis on graphical games, but also briefly touching on normal-form and polymatrix games. The main technical contribution is a theorem that establishes sufficient conditions for a discretization of the players’ space of mixed strategies to contain an approximate Nash equilibrium. The result is actually stronger because every exact Nash equilibrium has a nearby approximate Nash equilibrium on the grid induced by the discretization. The sufficient conditions are weaker than those of previous results. In particular, a uniform discretization of size linear in the inverse of the approximation error and in the natural game-representation parameters suffices. The theorem holds for a generalization of graphical games, introduced here. The result has already been useful in the design and analysis of tractable algorithms for graphical games with parametric payoff functions and certain game-graph structures. For standard graphical games, under natural conditions, the discretization is logarithmic in the game-representation size, a substantial improvement over the linear dependency previously required. Combining the improved discretization result with old results on constraint networks in AI simplifies the derivation and analysis of algorithms for computing approximate Nash equilibria in graphical games.},
  archive      = {J_JAIR},
  author       = {Luis Ortiz},
  doi          = {10.1613/jair.1.12391},
  journal      = {Journal of Artificial Intelligence Research},
  pages        = {67-84},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {On sparse discretization for graphical games},
  volume       = {69},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Constraint and satisfiability reasoning for graph coloring.
<em>JAIR</em>, <em>69</em>, 33–65. (<a
href="https://doi.org/10.1613/jair.1.11313">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph coloring is an important problem in combinatorial optimization and a major component of numerous allocation and scheduling problems. In this paper we introduce a hybrid CP/SAT approach to graph coloring based on the addition-contraction recurrence of Zykov. Decisions correspond to either adding an edge between two non-adjacent vertices or contracting these two vertices, hence enforcing inequality or equality, respectively. This scheme yields a symmetry-free tree and makes learnt clauses stronger by not committing to a particular color. We introduce a new lower bound for this problem based on Mycielskian graphs; a method to produce a clausal explanation of this bound for use in a CDCL algorithm; a branching heuristic emulating Brélaz’ heuristic on the Zykov tree; and dedicated pruning techniques relying on marginal costs with respect to the bound and on reasoning about transitivity when unit propagating learnt clauses. The combination of these techniques in both a branch-and-bound and in a bottom-up search outperforms other SAT-based approaches and Dsatur on standard benchmarks both for finding upper bounds and for proving lower bounds.},
  archive      = {J_JAIR},
  author       = {Emmanuel Hebrard and George Katsirelos},
  doi          = {10.1613/jair.1.11313},
  journal      = {Journal of Artificial Intelligence Research},
  pages        = {33-65},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {Constraint and satisfiability reasoning for graph coloring},
  volume       = {69},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Point at the triple: Generation of text summaries from
knowledge base triples. <em>JAIR</em>, <em>69</em>, 1–31. (<a
href="https://doi.org/10.1613/jair.1.11694">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We investigate the problem of generating natural language summaries from knowledge base triples. Our approach is based on a pointer-generator network, which, in addition to generating regular words from a fixed target vocabulary, is able to verbalise triples in several ways. We undertake an automatic and a human evaluation on single and open-domain summaries generation tasks. Both show that our approach significantly outperforms other data-driven baselines.},
  archive      = {J_JAIR},
  author       = {Pavlos Vougiouklis and Eddy Maddalena and Jonathon Hare and Elena Simperl},
  doi          = {10.1613/jair.1.11694},
  journal      = {Journal of Artificial Intelligence Research},
  pages        = {1-31},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {Point at the triple: Generation of text summaries from knowledge base triples},
  volume       = {69},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Towards partial order reductions for strategic ability.
<em>JAIR</em>, <em>68</em>, 817–850. (<a
href="https://doi.org/10.1613/jair.1.11936">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a general semantics for strategic abilities of agents in asynchronous systems, with and without perfect information. Based on the semantics, we show some general complexity results for verification of strategic abilities in asynchronous interaction. More importantly, we develop a methodology for partial order reduction in verification of agents with imperfect information. We show that the reduction preserves an important subset of strategic properties, with as well as without the fairness assumption. We also demonstrate the effectiveness of the reduction on a number of benchmarks. Interestingly, the reduction does not work for strategic abilities under perfect information.},
  archive      = {J_JAIR},
  author       = {Wojciech Jamroga and Wojciech Penczek and Teofil Sidoruk and Piotr Dembiński and Antoni Mazurkiewicz},
  doi          = {10.1613/jair.1.11936},
  journal      = {Journal of Artificial Intelligence Research},
  pages        = {817-850},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {Towards partial order reductions for strategic ability},
  volume       = {68},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Gradient-based learning methods extended to smooth manifolds
applied to automated clustering. <em>JAIR</em>, <em>68</em>, 777–816.
(<a href="https://doi.org/10.1613/jair.1.12192">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Grassmann manifold based sparse spectral clustering is a classification technique that&amp;nbsp; consists in learning a latent representation of data, formed by a subspace basis, which&amp;nbsp; is sparse. In order to learn a latent representation, spectral clustering is formulated in&amp;nbsp; terms of a loss minimization problem over a smooth manifold known as Grassmannian.&amp;nbsp; Such minimization problem cannot be tackled by one of traditional gradient-based learning&amp;nbsp; algorithms, which are only suitable to perform optimization in absence of constraints among&amp;nbsp; parameters. It is, therefore, necessary to develop specific optimization/learning algorithms&amp;nbsp; that are able to look for a local minimum of a loss function under smooth constraints in&amp;nbsp; an efficient way. Such need calls for manifold optimization methods. In this paper, we&amp;nbsp; extend classical gradient-based learning algorithms on &amp;nbsp;&amp;nbsp;at parameter spaces (from classical&amp;nbsp; gradient descent to adaptive momentum) to curved spaces (smooth manifolds) by means&amp;nbsp; of tools from manifold calculus. We compare clustering performances of these methods&amp;nbsp; and known methods from the scientific literature. The obtained results confirm that the&amp;nbsp; proposed learning algorithms prove lighter in computational complexity than existing ones&amp;nbsp; without detriment in clustering efficacy.},
  archive      = {J_JAIR},
  author       = {Alkis Koudounas and Simone Fiori},
  doi          = {10.1613/jair.1.12192},
  journal      = {Journal of Artificial Intelligence Research},
  pages        = {777-816},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {Gradient-based learning methods extended to smooth manifolds applied to automated clustering},
  volume       = {68},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). How to do things with words: A bayesian approach.
<em>JAIR</em>, <em>68</em>, 753–776. (<a
href="https://doi.org/10.1613/jair.1.11951">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Communication changes the beliefs of the listener and of the speaker. The value of a communicative act stems from the valuable belief states which result from this act. To model this we build on the Interactive POMDP (IPOMDP) framework, which extends POMDPs to allow agents to model others in multi-agent settings, and we include communication that can take place between the agents to formulate Communicative IPOMDPs (CIPOMDPs). We treat communication as a type of action and therefore, decisions regarding communicative acts are based on decision-theoretic planning using the Bellman optimality principle and value iteration, just as they are for all other rational actions. As in any form of planning, the results of actions need to be precisely specified. We use the Bayes’ theorem to derive how agents update their beliefs in CIPOMDPs; updates are due to agents’ actions, observations, messages they send to other agents, and messages they receive from others. The Bayesian decision-theoretic approach frees us from the commonly made assumption of cooperative discourse – we consider agents which are free to be dishonest while communicating and are guided only by their selfish rationality. We use a simple Tiger game to illustrate the belief update, and to show that the ability to rationally communicate allows agents to improve efficiency of their interactions.},
  archive      = {J_JAIR},
  author       = {Piotr Gmytrasiewicz},
  doi          = {10.1613/jair.1.11951},
  journal      = {Journal of Artificial Intelligence Research},
  pages        = {753-776},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {How to do things with words: A bayesian approach},
  volume       = {68},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Subgoaling techniques for satisficing and optimal numeric
planning. <em>JAIR</em>, <em>68</em>, 691–752. (<a
href="https://doi.org/10.1613/jair.1.11875">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper studies novel subgoaling relaxations for automated planning with propositional and numeric state variables. Subgoaling relaxations address one source of complexity of the planning problem: the requirement to satisfy conditions simultaneously. The core idea is to relax this requirement by recursively decomposing conditions into atomic subgoals that are considered in isolation. Such relaxations are typically used for pruning, or as the basis for computing admissible or inadmissible heuristic estimates to guide optimal or satisificing heuristic search planners. In the last decade or so, the subgoaling principle has underpinned the design of an abundance of relaxation-based heuristics whose formulations have greatly extended the reach of classical planning. This paper extends subgoaling relaxations to support numeric state variables and numeric conditions. We provide both theoretical and practical results, with the aim of reaching a good trade-off between accuracy and computation costs within a heuristic state-space search planner. Our experimental results validate the theoretical assumptions, and indicate that subgoaling substantially improves on the state of the art in optimal and satisficing numeric planning via forward state-space search.},
  archive      = {J_JAIR},
  author       = {Enrico Scala and Patrik Haslum and Sylvie Thiébaux and Miquel Ramirez},
  doi          = {10.1613/jair.1.11875},
  journal      = {Journal of Artificial Intelligence Research},
  pages        = {691-752},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {Subgoaling techniques for satisficing and optimal numeric planning},
  volume       = {68},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Image captioning using facial expression and attention.
<em>JAIR</em>, <em>68</em>, 661–689. (<a
href="https://doi.org/10.1613/jair.1.12025">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Benefiting from advances in machine vision and natural language processing techniques, current image captioning systems are able to generate detailed visual descriptions. For the most part, these descriptions represent an objective characterisation of the image, although some models do incorporate subjective aspects related to the observer’s view of the image, such as sentiment; current models, however, usually do not consider the emotional content of images during the caption generation process. This paper addresses this issue by proposing novel image captioning models which use facial expression features to generate image captions. The models generate image captions using long short-term memory networks applying facial features in addition to other visual features at different time steps. We compare a comprehensive collection of image captioning models with and without facial features using all standard evaluation metrics. The evaluation metrics indicate that applying facial features with an attention mechanism achieves the best performance, showing more expressive and more correlated image captions, on an image caption dataset extracted from the standard Flickr 30K dataset, consisting of around 11K images containing faces. An analysis of the generated captions finds that, perhaps unexpectedly, the improvement in caption quality appears to come not from the addition of adjectives linked to emotional aspects of the images, but from more variety in the actions described in the captions.},
  archive      = {J_JAIR},
  author       = {Omid Mohamad Nezami and Mark Dras and Stephen Wan and Cecile Paris},
  doi          = {10.1613/jair.1.12025},
  journal      = {Journal of Artificial Intelligence Research},
  pages        = {661-689},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {Image captioning using facial expression and attention},
  volume       = {68},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). The effects of experience on deception in human-agent
negotiation. <em>JAIR</em>, <em>68</em>, 633–660. (<a
href="https://doi.org/10.1613/jair.1.11924">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Negotiation is the complex social process by which multiple parties come to mutual agreement over a series of issues. As such, it has proven to be a key challenge problem for designing adequately social AIs that can effectively navigate this space. Artificial AI agents that are capable of negotiating must be capable of realizing policies and strategies that govern offer acceptances, offer generation, preference elicitation, and more. But the next generation of agents must also adapt to reflect their users’ experiences. &amp;nbsp; &amp;nbsp; &amp;nbsp;The best human negotiators tend to have honed their craft through hours of practice and experience. But, not all negotiators agree on which strategic tactics to use, and endorsement of deceptive tactics in particular is a controversial topic for many negotiators. We examine the ways in which deceptive tactics are used and endorsed in non-repeated human negotiation and show that prior experience plays a key role in governing what tactics are seen as acceptable or useful in negotiation. Previous work has indicated that people that negotiate through artificial agent representatives may be more inclined to fairness than those people that negotiate directly. We present a series of three user studies that challenge this initial assumption and expand on this picture by examining the role of past experience. &amp;nbsp; &amp;nbsp; &amp;nbsp;This work constructs a new scale for measuring endorsement of manipulative negotiation tactics and introduces its use to artificial intelligence research. It continues by presenting the results of a series of three studies that examine how negotiating experience can change what negotiation tactics and strategies human endorse. Study #1 looks at human endorsement of deceptive techniques based on prior negotiating experience as well as representative effects. Study #2 further characterizes the negativity of prior experience in relation to endorsement of deceptive techniques. Finally, in Study #3, we show that the lessons learned from the empirical observations in Study #1 and #2 can in fact be induced—by designing agents that provide a specific type of negative experience, human endorsement of deception can be predictably manipulated.},
  archive      = {J_JAIR},
  author       = {Johnathan Mell and Gale Lucas and Sharon Mozgai and Jonathan Gratch},
  doi          = {10.1613/jair.1.11924},
  journal      = {Journal of Artificial Intelligence Research},
  pages        = {633-660},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {The effects of experience on deception in human-agent negotiation},
  volume       = {68},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A general approach to multimodal document quality
assessment. <em>JAIR</em>, <em>68</em>, 607–632. (<a
href="https://doi.org/10.1613/jair.1.11647">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The perceived quality of a document is affected by various factors, including grammat- icality, readability, stylistics, and expertise depth, making the task of document quality assessment a complex one. In this paper, we explore this task in the context of assessing the quality of Wikipedia articles and academic papers. Observing that the visual rendering of a document can capture implicit quality indicators that are not present in the document text — such as images, font choices, and visual layout — we propose a joint model that combines the text content with a visual rendering of the document for document qual- ity assessment. Our joint model achieves state-of-the-art results over five datasets in two domains (Wikipedia and academic papers), which demonstrates the complementarity of textual and visual features, and the general applicability of our model. To examine what kinds of features our model has learned, we further train our model in a multi-task learning setting, where document quality assessment is the primary task and feature learning is an auxiliary task. Experimental results show that visual embeddings are better at learning structural features while textual embeddings are better at learning readability scores, which further verifies the complementarity of visual and textual features.},
  archive      = {J_JAIR},
  author       = {Aili Shen and Bahar Salehi and Jianzhong Qi and Timothy Baldwin},
  doi          = {10.1613/jair.1.11647},
  journal      = {Journal of Artificial Intelligence Research},
  pages        = {607-632},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {A general approach to multimodal document quality assessment},
  volume       = {68},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Scalable planning with deep neural network learned
transition models. <em>JAIR</em>, <em>68</em>, 571–606. (<a
href="https://doi.org/10.1613/jair.1.11829">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In many complex planning problems with factored, continuous state and action spaces such as Reservoir Control, Heating Ventilation and Air Conditioning (HVAC), and Navigation domains, it is difficult to obtain a model of the complex nonlinear dynamics that govern state evolution. However, the ubiquity of modern sensors allows us to collect large quantities of data from each of these complex systems and build accurate, nonlinear deep neural network models of their state transitions. But there remains one major problem for the task of control – how can we plan with deep network learned transition models without resorting to Monte Carlo Tree Search and other black-box transition model techniques that ignore model structure and do not easily extend to continuous domains? In this paper, we introduce two types of planning methods that can leverage deep neural network learned transition models: Hybrid Deep MILP Planner (HD-MILP-Plan) and Tensorflow Planner (TF-Plan). In HD-MILP-Plan, we make the critical observation that the Rectified Linear Unit (ReLU) transfer function for deep networks not only allows faster convergence of model learning, but also permits a direct compilation of the deep network transition model to a Mixed-Integer Linear Program (MILP) encoding. Further, we identify deep network specific optimizations for HD-MILP-Plan that improve performance over a base encoding and show that we can plan optimally with respect to the learned deep networks. In TF-Plan, we take advantage of the efficiency of auto-differentiation tools and GPU-based computation where we encode a subclass of purely continuous planning problems as Recurrent Neural Networks and directly optimize the actions through backpropagation. We compare both planners and show that TF-Plan is able to approximate the optimal plans found by HD-MILP-Plan in less computation time. Hence this article offers two novel planners for continuous state and action domains with learned deep neural net transition models: one optimal method (HD-MILP-Plan) and a scalable alternative for large-scale problems (TF-Plan).},
  archive      = {J_JAIR},
  author       = {Ga Wu and Buser Say and Scott Sanner},
  doi          = {10.1613/jair.1.11829},
  journal      = {Journal of Artificial Intelligence Research},
  pages        = {571-606},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {Scalable planning with deep neural network learned transition models},
  volume       = {68},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Simulating offender mobility: Modeling activity nodes from
large-scale human activity data. <em>JAIR</em>, <em>68</em>, 541–570.
(<a href="https://doi.org/10.1613/jair.1.11831">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, simulation techniques have been applied to investigate the spatiotemporal dynamics of crime. Researchers have instantiated mobile offenders in agent-based simulations for theory testing, experimenting with crime prevention strategies, and exploring crime prediction techniques, despite facing challenges due to the complex dynamics of crime and the lack of detailed information about offender mobility. This paper presents a simulation model to explore offender mobility, focusing on the interplay between the agent&#39;s awareness space and activity nodes. The simulation generates patterns of individual mobility aiming to cumulatively match crime patterns. To instantiate a realistic urban environment, we use open data to simulate the urban structure, location-based social networks data to represent activity nodes as a proxy for human activity, and taxi trip data as a proxy for human movement between regions of the city. We analyze and systematically compare 35 different mobility strategies and demonstrate the benefits of using large-scale human activity data to simulate offender mobility. The strategies combining taxi trip data or historic crime data with popular activity nodes perform best compared to other strategies, especially for robbery. Our approach provides a basis for building agent-based crime simulations that infer offender mobility in urban areas from real-world data.},
  archive      = {J_JAIR},
  author       = {Raquel Rosés and Cristina Kadar and Charlotte Gerritsen and Ovi Chris Rouly},
  doi          = {10.1613/jair.1.11831},
  journal      = {Journal of Artificial Intelligence Research},
  pages        = {541-570},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {Simulating offender mobility: Modeling activity nodes from large-scale human activity data},
  volume       = {68},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Ontology reasoning with deep neural networks. <em>JAIR</em>,
<em>68</em>, 503–540. (<a
href="https://doi.org/10.1613/jair.1.11661">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The ability to conduct logical reasoning is a fundamental aspect of intelligent human behavior, and thus an important problem along the way to human-level artificial intelligence. Traditionally, logic-based symbolic methods from the field of knowledge representation and reasoning have been used to equip agents with capabilities that resemble human logical reasoning qualities. More recently, however, there has been an increasing interest in using machine learning rather than logic-based symbolic formalisms to tackle these tasks. In this paper, we employ state-of-the-art methods for training deep neural networks to devise a novel model that is able to learn how to effectively perform logical reasoning in the form of basic ontology reasoning. This is an important and at the same time very natural logical reasoning task, which is why the presented approach is applicable to a plethora of important real-world problems. We present the outcomes of several experiments, which show that our model is able to learn to perform highly accurate ontology reasoning on very large, diverse, and challenging benchmarks. Furthermore, it turned out that the suggested approach suffers much less from different obstacles that prohibit logic-based symbolic reasoning, and, at the same time, is surprisingly plausible from a biological point of view.},
  archive      = {J_JAIR},
  author       = {Patrick Hohenecker and Thomas Lukasiewicz},
  doi          = {10.1613/jair.1.11661},
  journal      = {Journal of Artificial Intelligence Research},
  pages        = {503–540},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {Ontology reasoning with deep neural networks},
  volume       = {68},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Preferences single-peaked on a circle. <em>JAIR</em>,
<em>68</em>, 463–502. (<a
href="https://doi.org/10.1613/jair.1.11732">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce the domain of preferences that are single-peaked on a circle, which is a generalization of the well-studied single-peaked domain. This preference restriction is useful, e.g., for scheduling decisions, certain facility location problems, and for one-dimensional decisions in the presence of extremist preferences. We give a fast recognition algorithm of this domain, provide a characterisation by finitely many forbidden subprofiles, and show that many popular single- and multi-winner voting rules are polynomial-time computable on this domain. In particular, we prove that Proportional Approval Voting can be computed in polynomial time for profiles that are single-peaked on a circle. In contrast, Kemeny&#39;s rule remains hard to evaluate, and several impossibility results from social choice theory can be proved using only profiles in this domain.},
  archive      = {J_JAIR},
  author       = {Dominik Peters and Martin Lackner},
  doi          = {10.1613/jair.1.11732},
  journal      = {Journal of Artificial Intelligence Research},
  pages        = {463-502},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {Preferences single-peaked on a circle},
  volume       = {68},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Automated conjecturing II: Chomp and reasoned game play.
<em>JAIR</em>, <em>68</em>, 447–461. (<a
href="https://doi.org/10.1613/jair.1.12188">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We demonstrate the use of a program that generates conjectures about positions of the combinatorial game Chomp—explanations of why certain moves are bad. These could be used in the design of a Chomp-playing program that gives reasons for its moves. We prove one of these Chomp conjectures—demonstrating that our conjecturing program can produce genuine Chomp knowledge. The conjectures are generated by a general purpose conjecturing program that was previously and successfully used to generate mathematical conjectures. Our program is initialized with Chomp invariants and example game boards—the conjectures take the form of invariant-relation statements interpreted to be true for all board positions of a certain kind. The conjectures describe a theory of Chomp positions. The program uses limited, natural input and suggests how theories generated on-the-fly might be used in a variety of situations where decisions—based on reasons—are required.},
  archive      = {J_JAIR},
  author       = {Alexander Bradford and J. Kain Day and Laura Hutchinson and Bryan Kaperick and Craig Larson and Matthew Mills and David Muncy and Nico Van Cleemput},
  doi          = {10.1613/jair.1.12188},
  journal      = {Journal of Artificial Intelligence Research},
  pages        = {447-461},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {Automated conjecturing II: Chomp and reasoned game play},
  volume       = {68},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Predicting strategic behavior from free text. <em>JAIR</em>,
<em>68</em>, 413–445. (<a
href="https://doi.org/10.1613/jair.1.11849">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The connection between messaging and action is fundamental both to web applications, such as web search and sentiment analysis, and to economics. However, while prominent online applications exploit messaging in natural (human) language in order to predict non-strategic action selection, the economics literature focuses on the connection between structured stylized messaging to strategic decisions in games and multi-agent encounters. This paper aims to connect these two strands of research, which we consider highly timely and important due to the vast online textual communication on the web. Particularly, we introduce the following question: Can free text expressed in natural language serve for the prediction of action selection in an economic context, modeled as a game In order to initiate the research on this question, we introduce the study of an individual’s action prediction in a one-shot game based on free text he/she provides, while being unaware of the game to be played. We approach the problem by attributing commonsensical personality attributes via crowd-sourcing to free texts written by individuals, and employing transductive learning to predict actions taken by these individuals in one-shot games based on these attributes. Our approach allows us to train a single classifier that can make predictions with respect to actions taken in multiple games. In experiments with three well-studied games, our algorithm compares favorably with strong alternative approaches. In ablation analysis, we demonstrate the importance of our modeling choices—the representation of the text with the commonsensical personality attributes and our classifier—to the predictive power of our model.},
  archive      = {J_JAIR},
  author       = {Omer Ben-Porat and Sharon Hirsch and Lital Kuchy and Guy Elad and Roi Reichart and Moshe Tennenholtz},
  doi          = {10.1613/jair.1.11849},
  journal      = {Journal of Artificial Intelligence Research},
  pages        = {413–445},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {Predicting strategic behavior from free text},
  volume       = {68},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Conservative extensions in horn description logics with
inverse roles. <em>JAIR</em>, <em>68</em>, 365–411. (<a
href="https://doi.org/10.1613/jair.1.12182">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We investigate the decidability and computational complexity of conservative extensions and the related notions of inseparability and entailment in Horn description logics (DLs) with inverse roles. We consider both query conservative extensions, defined by requiring that the answers to all conjunctive queries are left unchanged, and deductive conservative extensions, which require that the entailed concept inclusions, role inclusions, and functionality assertions do not change. Upper bounds for query conservative extensions are particularly challenging because characterizations in terms of unbounded homomorphisms between universal models, which are the foundation of the standard approach to establishing decidability, fail in the presence of inverse roles. We resort to a characterization that carefully mixes unbounded and bounded homomorphisms and enables a decision procedure that combines tree automata and a mosaic technique. Our main results are that query conservative extensions are 2ExpTime-complete in all DLs between ELI and Horn-ALCHIF and between Horn-ALC and Horn-ALCHIF, and that deductive conservative extensions are 2ExpTime-complete in all DLs between ELI and ELHIF_bot. The same results hold for inseparability and entailment.},
  archive      = {J_JAIR},
  author       = {Jean Christoph Jung and Carsten Lutz and Mauricio Martel and Thomas Schneider},
  doi          = {10.1613/jair.1.12182},
  journal      = {Journal of Artificial Intelligence Research},
  pages        = {365-411},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {Conservative extensions in horn description logics with inverse roles},
  volume       = {68},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Sliding-window thompson sampling for non-stationary
settings. <em>JAIR</em>, <em>68</em>, 311–364. (<a
href="https://doi.org/10.1613/jair.1.11407">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-Armed Bandit (MAB) techniques have been successfully applied to many classes of sequential decision problems in the past decades. However, non-stationary settings -- very common in real-world applications -- received little attention so far, and theoretical guarantees on the regret are known only for some frequentist algorithms. In this paper, we propose an algorithm, namely Sliding-Window Thompson Sampling (SW-TS), for nonstationary stochastic MAB settings. Our algorithm is based on Thompson Sampling and exploits a sliding-window approach to tackle, in a unified fashion, two different forms of non-stationarity studied separately so far: abruptly changing and smoothly changing. In the former, the reward distributions are constant during sequences of rounds, and their change may be arbitrary and happen at unknown rounds, while, in the latter, the reward distributions smoothly evolve over rounds according to unknown dynamics. Under mild assumptions, we provide regret upper bounds on the dynamic pseudo-regret of SW-TS for the abruptly changing environment, for the smoothly changing one, and for the setting in which both the non-stationarity forms are present. Furthermore, we empirically show that SW-TS dramatically outperforms state-of-the-art algorithms even when the forms of non-stationarity are taken separately, as previously studied in the literature.},
  archive      = {J_JAIR},
  author       = {Francesco Trovo and Stefano Paladino and Marcello Restelli and Nicola Gatti},
  doi          = {10.1613/jair.1.11407},
  journal      = {Journal of Artificial Intelligence Research},
  pages        = {311-364},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {Sliding-window thompson sampling for non-stationary settings},
  volume       = {68},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Bridging the gap between probabilistic model checking and
probabilistic planning: Survey, compilations, and empirical comparison.
<em>JAIR</em>, <em>68</em>, 247–310. (<a
href="https://doi.org/10.1613/jair.1.11595">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Markov decision processes are of major interest in the planning community as well as in the model checking community. But in spite of the similarity in the considered formal models, the development of new techniques and methods happened largely independently in both communities. This work is intended as a beginning to unite the two research branches. We consider goal-reachability analysis as a common basis between both communities. The core of this paper is the translation from Jani, an overarching input language for quantitative model checkers, into the probabilistic planning domain definition language (PPDDL), and vice versa from PPDDL into Jani. These translations allow the creation of an overarching benchmark collection, including existing case studies from the model checking community, as well as benchmarks from the international probabilistic planning competitions (IPPC). We use this benchmark set as a basis for an extensive empirical comparison of various approaches from the model checking community, variants of value iteration, and MDP heuristic search algorithms developed by the AI planning community. On a per benchmark domain basis, techniques from one community can achieve state-ofthe-art performance in benchmarks of the other community. Across all benchmark domains of one community, the performance comparison is however in favor of the solvers and algorithms of that particular community. Reasons are the design of the benchmarks, as well as tool-related limitations. Our translation methods and benchmark collection foster crossfertilization between both communities, pointing out specific opportunities for widening the scope of solvers to different kinds of models, as well as for exchanging and adopting algorithms across communities.},
  archive      = {J_JAIR},
  author       = {Michaela Klauck and Marcel Steinmetz and Jörg Hoffmann and Holger Hermanns},
  doi          = {10.1613/jair.1.11595},
  journal      = {Journal of Artificial Intelligence Research},
  pages        = {247-310},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {Bridging the gap between probabilistic model checking and probabilistic planning: Survey, compilations, and empirical comparison},
  volume       = {68},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Improving nash social welfare approximations. <em>JAIR</em>,
<em>68</em>, 225–245. (<a
href="https://doi.org/10.1613/jair.1.11618">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider the problem of fairly allocating a set of indivisible goods among n agents. Various fairness notions have been proposed within the rapidly growing field of fair division, but the Nash social welfare (NSW) serves as a focal point. In part, this follows from the ‘unreasonable’ fairness guarantees provided, in the sense that a max NSW allocation meets multiple other fairness metrics simultaneously, all while satisfying a standard economic concept of efficiency, Pareto optimality. However, existing approximation algorithms fail to satisfy all of the remarkable fairness guarantees offered by a max NSW allocation, instead targeting only the specific NSW objective. We address this issue by presenting a 2 max NSW, Prop-1, 1/(2n) MMS, and Pareto optimal allocation in strongly polynomial time. Our techniques are based on a market interpretation of a fractional max NSW allocation. We present novel definitions of fairness concepts in terms of market prices, and design a new scheme to round a market equilibrium into an integral allocation in a way that provides most of the fairness properties of an integral max NSW allocation.},
  archive      = {J_JAIR},
  author       = {Peter McGlaughlin and Jugal Garg},
  doi          = {10.1613/jair.1.11618},
  journal      = {Journal of Artificial Intelligence Research},
  pages        = {225-245},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {Improving nash social welfare approximations},
  volume       = {68},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Towards knowledgeable supervised lifelong learning systems.
<em>JAIR</em>, <em>68</em>, 159–224. (<a
href="https://doi.org/10.1613/jair.1.11432">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Learning a sequence of tasks is a long-standing challenge in machine learning. This setting applies to learning systems that observe examples of a range of tasks at different points in time. A learning system should become more knowledgeable as more related tasks are learned. Although the problem of learning sequentially was acknowledged for the first time decades ago, the research in this area has been rather limited. Research in transfer learning, multitask learning, metalearning and deep learning has studied some challenges of these kinds of systems. Recent research in lifelong machine learning and continual learning has revived interest in this problem. We propose Proficiente, a full framework for long-term learning systems. Proficiente relies on knowledge transferred between hypotheses learned with Support Vector Machines. The first component of the framework is focused on transferring forward selectively from a set of existing hypotheses or functions representing knowledge acquired during previous tasks to a new target task. A second component of Proficiente is focused on transferring backward, a novel ability of long-term learning systems that aim to exploit knowledge derived from recent tasks to encourage refinement of existing knowledge. We propose a method that transfers selectively from a task learned recently to existing hypotheses representing previous tasks. The method encourages retention of existing knowledge whilst refining. We analyse the theoretical properties of the proposed framework. Proficiente is accompanied by an agnostic metric that can be used to determine if a long-term learning system is becoming more knowledgeable. We evaluate Proficiente in both synthetic and real-world datasets, and demonstrate scenarios where knowledgeable supervised learning systems can be achieved by means of transfer.},
  archive      = {J_JAIR},
  author       = {Diana Benavides-Prado and Yun Sing Koh and Patricia Riddle},
  doi          = {10.1613/jair.1.11432},
  journal      = {Journal of Artificial Intelligence Research},
  pages        = {159-224},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {Towards knowledgeable supervised lifelong learning systems},
  volume       = {68},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Variational bayes in private settings (VIPS). <em>JAIR</em>,
<em>68</em>, 109–157. (<a
href="https://doi.org/10.1613/jair.1.11763">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many applications of Bayesian data analysis involve sensitive information such as personal documents or medical records, motivating methods which ensure that privacy is protected. We introduce a general privacy-preserving framework for Variational Bayes (VB), a widely used optimization-based Bayesian inference method. Our framework respects differential privacy, the gold-standard privacy criterion, and encompasses a large class of probabilistic models, called the Conjugate Exponential (CE) family. We observe that we can straightforwardly privatise VB’s approximate posterior distributions for models in the CE family, by perturbing the expected sufficient statistics of the complete-data likelihood. For a broadly-used class of non-CE models, those with binomial likelihoods, we show how to bring such models into the CE family, such that inferences in the modified model resemble the private variational Bayes algorithm as closely as possible, using the Pólya-Gamma data augmentation scheme. The iterative nature of variational Bayes presents a further challenge since iterations increase the amount of noise needed. We overcome this by combining: (1) an improved composition method for differential privacy, called the moments accountant, which provides a tight bound on the privacy cost of multiple VB iterations and thus significantly decreases the amount of additive noise; and (2) the privacy amplification effect of subsampling mini-batches from large-scale data in stochastic learning. We empirically demonstrate the effectiveness of our method in CE and non-CE models including latent Dirichlet allocation, Bayesian logistic regression, and sigmoid belief networks, evaluated on real-world datasets.},
  archive      = {J_JAIR},
  author       = {Mijung Park and James Foulds and Kamalika Chaudhuri and Max Welling},
  doi          = {10.1613/jair.1.11763},
  journal      = {Journal of Artificial Intelligence Research},
  pages        = {109-157},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {Variational bayes in private settings (VIPS)},
  volume       = {68},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Vocabulary alignment in openly specified interactions.
<em>JAIR</em>, <em>68</em>, 69–107. (<a
href="https://doi.org/10.1613/jair.1.11497">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The problem of achieving common understanding between agents that use different vocabularies has been mainly addressed by techniques that assume the existence of shared external elements, such as a meta-language or a physical environment. In this article, we consider agents that use different vocabularies and only share knowledge of how to perform a task, given by the specification of an interaction protocol. We present a framework that lets agents learn a vocabulary alignment from the experience of interacting. Unlike previous work in this direction, we use open protocols that constrain possible actions instead of defining procedures, making our approach more general. We present two techniques that can be used either to learn an alignment from scratch or to repair an existent one, and we evaluate their performance experimentally.},
  archive      = {J_JAIR},
  author       = {Paula Daniela Chocron and Marco Schorlemmer},
  doi          = {10.1613/jair.1.11497},
  journal      = {Journal of Artificial Intelligence Research},
  pages        = {69-107},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {Vocabulary alignment in openly specified interactions},
  volume       = {68},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). ASNets: Deep learning for generalised planning.
<em>JAIR</em>, <em>68</em>, 1–68. (<a
href="https://doi.org/10.1613/jair.1.11633">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we discuss the learning of generalised policies for probabilistic and classical planning problems using Action Schema Networks (ASNets). The ASNet is a neural network architecture that exploits the relational structure of (P)PDDL planning problems to learn a common set of weights that can be applied to any problem in a domain. By mimicking the actions chosen by a traditional, non-learning planner on a handful of small problems in a domain, ASNets are able to learn a generalised reactive policy that can quickly solve much larger instances from the domain. This work extends the ASNet architecture to make it more expressive, while still remaining invariant to a range of symmetries that exist in PPDDL problems. We also present a thorough experimental evaluation of ASNets, including a comparison with heuristic search planners on seven probabilistic and deterministic domains, an extended evaluation on over 18,000 Blocksworld instances, and an ablation study. Finally, we show that sparsity-inducing regularisation can produce ASNets that are compact enough for humans to understand, yielding insights into how the structure of ASNets allows them to generalise across a domain.},
  archive      = {J_JAIR},
  author       = {Sam Toyer and Sylvie Thiébaux and Felipe Trevizan and Lexing Xie},
  doi          = {10.1613/jair.1.11633},
  journal      = {Journal of Artificial Intelligence Research},
  pages        = {1-68},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {ASNets: Deep learning for generalised planning},
  volume       = {68},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Learning the language of software errors. <em>JAIR</em>,
<em>67</em>, 881–903. (<a
href="https://doi.org/10.1613/jair.1.11798">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose to use algorithms for learning deterministic finite automata (DFA), such as Angluin’s L* algorithm, for learning a DFA that describes the possible scenarios under which a given program error occurs. The alphabet of this automaton is given by the user (for instance, a subset of the function call sites or branches), and hence the automaton describes a user-defined abstraction of those scenarios. More generally, the same technique can be used for visualising the behavior of a program or parts thereof. It can also be used for visually comparing different versions of a program (by presenting an automaton for the behavior in the symmetric difference between them), and for assisting in merging several development branches. We present experiments that demonstrate the power of an abstract visual representation of errors and of program segments, accessible via the project’s web page. In addition, our experiments in this paper demonstrate that such automata can be learned efficiently over real-world programs. We also present lazy learning, which is a method for reducing the number of membership queries while using L*, and demonstrate its effectiveness on standard benchmarks.},
  archive      = {J_JAIR},
  author       = {Hana Chockler and Pascal Kesseli and Daniel Kroening and Ofer Strichman},
  doi          = {10.1613/jair.1.11798},
  journal      = {Journal of Artificial Intelligence Research},
  pages        = {881-903},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {Learning the language of software errors},
  volume       = {67},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). HTN planning as heuristic progression search. <em>JAIR</em>,
<em>67</em>, 835–880. (<a
href="https://doi.org/10.1613/jair.1.11282">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The majority of search-based HTN planning systems can be divided into those searching a space of partial plans (a plan space) and those performing progression search, i.e., that build the solution in a forward manner. So far, all HTN planners that guide the search by using heuristic functions are based on plan space search. Those systems represent the set of search nodes more effectively by maintaining a partial ordering between tasks, but they have only limited information about the current state during search. In this article, we propose the use of progression search as basis for heuristic HTN planning systems. Such systems can calculate their heuristics incorporating the current state, because it is tracked during search. Our contribution is the following: We introduce two novel progression algorithms that avoid unnecessary branching when the problem at hand is partially ordered and show that both are sound and complete. We show that defining systematicity is problematic for search in HTN planning, propose a definition, and show that it is fulfilled by one of our algorithms. Then, we introduce a method to apply arbitrary classical planning heuristics to guide the search in HTN planning. It relaxes the HTN planning model to a classical model that is only used for calculating heuristics. It is updated during search and used to create heuristic values that are used to guide the HTN search. We show that it can be used to create HTN heuristics with interesting theoretical properties like safety, goal-awareness, and admissibility. Our empirical evaluation shows that the resulting system outperforms the state of the art in search-based HTN planning.},
  archive      = {J_JAIR},
  author       = {Daniel Höller and Pascal Bercher and Gregor Behnke and Susanne Biundo},
  doi          = {10.1613/jair.1.11282},
  journal      = {Journal of Artificial Intelligence Research},
  pages        = {835-880},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {HTN planning as heuristic progression search},
  volume       = {67},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Incomplete preferences in single-peaked electorates.
<em>JAIR</em>, <em>67</em>, 797–833. (<a
href="https://doi.org/10.1613/jair.1.11577">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Incomplete preferences are likely to arise in real-world preference aggregation scenarios. This paper deals with determining whether an incomplete preference profile is single-peaked. This is valuable information since many intractable voting problems become tractable given singlepeaked preferences. We prove that the problem of recognizing single-peakedness is NP-complete for incomplete profiles consisting of partial orders. Despite this intractability result, we find several polynomial-time algorithms for reasonably restricted settings. In particular, we give polynomial-time recognition algorithms for weak orders, which can be viewed as preferences with indifference.},
  archive      = {J_JAIR},
  author       = {Zack Fitzsimmons and Martin Lackner},
  doi          = {10.1613/jair.1.11577},
  journal      = {Journal of Artificial Intelligence Research},
  pages        = {797-833},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {Incomplete preferences in single-peaked electorates},
  volume       = {67},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Compositionality decomposed: How do neural networks
generalise? <em>JAIR</em>, <em>67</em>, 757–795. (<a
href="https://doi.org/10.1613/jair.1.11674">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Despite a multitude of empirical studies, little consensus exists on whether neural networks are able to generalise compositionally, a controversy that, in part, stems from a lack of agreement about what it means for a neural model to be compositional. As a response to this controversy, we present a set of tests that provide a bridge between, on the one hand, the vast amount of linguistic and philosophical theory about compositionality of language and, on the other, the successful neural models of language. We collect different interpretations of compositionality and translate them into five theoretically grounded tests for models that are formulated on a task-independent level. In particular, we provide tests to investigate (i) if models systematically recombine known parts and rules (ii) if models can extend their predictions beyond the length they have seen in the training data (iii) if models’ composition operations are local or global (iv) if models’ predictions are robust to synonym substitutions and (v) if models favour rules or exceptions during training. To demonstrate the usefulness of this evaluation paradigm, we instantiate these five tests on a highly compositional data set which we dub PCFG SET and apply the resulting tests to three popular sequence-to-sequence models: a recurrent, a convolution-based and a transformer model. We provide an in-depth analysis of the results, which uncover the strengths and weaknesses of these three architectures and point to potential areas of improvement.},
  archive      = {J_JAIR},
  author       = {Dieuwke Hupkes and Verna Dankers and Mathijs Mul and Elia Bruni},
  doi          = {10.1613/jair.1.11674},
  journal      = {Journal of Artificial Intelligence Research},
  pages        = {757-795},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {Compositionality decomposed: How do neural networks generalise?},
  volume       = {67},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Hedonic games with ordinal preferences and thresholds.
<em>JAIR</em>, <em>67</em>, 705–756. (<a
href="https://doi.org/10.1613/jair.1.11531">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a new representation setting for hedonic games, where each agent partitions the set of other agents into friends, enemies, and neutral agents, with friends and enemies being ranked. Under the assumption that preferences are monotonic (respectively, antimonotonic) with respect to the addition of friends (respectively, enemies), we propose a bipolar extension of the responsive extension principle, and use this principle to derive the (partial) preferences of agents over coalitions. Then, for a number of solution concepts, we characterize partitions that necessarily or possibly satisfy them, and we study the related problems in terms of their complexity.},
  archive      = {J_JAIR},
  author       = {Anna Maria Kerkmann and Jérôme Lang and Anja Rey and Jörg Rothe and Hilmar Schadrack and Lena Schend},
  doi          = {10.1613/jair.1.11531},
  journal      = {Journal of Artificial Intelligence Research},
  pages        = {705–756},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {Hedonic games with ordinal preferences and thresholds},
  volume       = {67},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Using task descriptions in lifelong machine learning for
improved performance and zero-shot transfer. <em>JAIR</em>, <em>67</em>,
673–704. (<a href="https://doi.org/10.1613/jair.1.11304">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Knowledge transfer between tasks can improve the performance of learned models, but requires an accurate estimate of inter-task relationships to identify the relevant knowledge to transfer. These inter-task relationships are typically estimated based on training data for each task, which is inefficient in lifelong learning settings where the goal is to learn each consecutive task rapidly from as little data as possible. To reduce this burden, we develop a lifelong learning method based on coupled dictionary learning that utilizes high-level task descriptions to model inter-task relationships. We show that using task descriptors improves the performance of the learned task policies, providing both theoretical justification for the benefit and empirical demonstration of the improvement across a variety of learning problems. Given only the descriptor for a new task, the lifelong learner is also able to accurately predict a model for the new task through zero-shot learning using the coupled dictionary, eliminating the need to gather training data before addressing the task.},
  archive      = {J_JAIR},
  author       = {Mohammad Rostami and David Isele and Eric Eaton},
  doi          = {10.1613/jair.1.11304},
  journal      = {Journal of Artificial Intelligence Research},
  pages        = {673-704},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {Using task descriptions in lifelong machine learning for improved performance and zero-shot transfer},
  volume       = {67},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A set of recommendations for assessing human–machine parity
in language translation. <em>JAIR</em>, <em>67</em>, 653–672. (<a
href="https://doi.org/10.1613/jair.1.11371">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The quality of machine translation has increased remarkably over the past years, to the degree that it was found to be indistinguishable from professional human translation in a&amp;nbsp;number of empirical investigations. We reassess Hassan et al.&#39;s 2018 investigation into Chinese to English news translation, showing that the finding of human–machine parity&amp;nbsp;was owed to weaknesses in the evaluation design—which is currently considered best practice in the field. We show that the professional human translations contained&amp;nbsp;significantly fewer errors, and that perceived quality in human evaluation depends on the choice of raters, the availability of linguistic context, and the creation of reference&amp;nbsp;translations. Our results call for revisiting current best practices to assess strong machine translation systems in general and human–machine parity in particular, for which we&amp;nbsp;offer a set of recommendations based on our empirical findings. &amp;nbsp;},
  archive      = {J_JAIR},
  author       = {Samuel Läubli and Sheila Castilho and Graham Neubig and Rico Sennrich and Qinlan Shen and Antonio Toral},
  doi          = {10.1613/jair.1.11371},
  journal      = {Journal of Artificial Intelligence Research},
  pages        = {653–672},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {A set of recommendations for assessing Human–Machine parity in language translation},
  volume       = {67},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Solving delete free planning with relaxed decision diagram
based heuristics. <em>JAIR</em>, <em>67</em>, 607–651. (<a
href="https://doi.org/10.1613/jair.1.11659">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We investigate the use of relaxed decision diagrams (DDs) for computing admissible heuristics for the cost-optimal delete-free planning (DFP) problem. Our main contributions are the introduction of two novel DD encodings for a DFP task: a multivalued decision diagram that includes the sequencing aspect of the problem and a binary decision diagram representation of its sequential relaxation. We present construction algorithms for each DD that leverage these different perspectives of the DFP task and provide theoretical and empirical analyses of the associated heuristics. We further show that relaxed DDs can be used beyond heuristic computation to extract delete-free plans, find action landmarks, and identify redundant actions. Our empirical analysis shows that while DD-based heuristics trail the state of the art, even small relaxed DDs are competitive with the linear programming heuristic for the DFP task, thus, revealing novel ways of designing admissible heuristics.},
  archive      = {J_JAIR},
  author       = {Margarita Paz Castro and Chiara Piacentini and Andre Augusto Cire and J. Christopher Beck},
  doi          = {10.1613/jair.1.11659},
  journal      = {Journal of Artificial Intelligence Research},
  pages        = {607-651},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {Solving delete free planning with relaxed decision diagram based heuristics},
  volume       = {67},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Agreement on target-bidirectional recurrent neural networks
for sequence-to-sequence learning. <em>JAIR</em>, <em>67</em>, 581–606.
(<a href="https://doi.org/10.1613/jair.1.12008">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recurrent neural networks are extremely appealing for sequence-to-sequence learning tasks. Despite their great success, they typically suffer from a shortcoming: they are prone to generate unbalanced targets with good prefixes but bad suffixes, and thus performance suffers when dealing with long sequences. We propose a simple yet effective approach to overcome this shortcoming. Our approach relies on the agreement between a pair of target-directional RNNs, which generates more balanced targets. In addition, we develop two efficient approximate search methods for agreement that are empirically shown to be almost optimal in terms of either sequence level or non-sequence level metrics. Extensive experiments were performed on three standard sequence-to-sequence transduction tasks: machine transliteration, grapheme-to-phoneme transformation and machine translation. The results show that the proposed approach achieves consistent and substantial improvements, compared to many state-of-the-art systems.},
  archive      = {J_JAIR},
  author       = {Lemao Liu and Andrew Finch and Masao Utiyama and Eiichiro Sumita},
  doi          = {10.1613/jair.1.12008},
  journal      = {Journal of Artificial Intelligence Research},
  pages        = {581-606},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {Agreement on target-bidirectional recurrent neural networks for sequence-to-sequence learning},
  volume       = {67},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Robust multi-agent path finding and executing.
<em>JAIR</em>, <em>67</em>, 549–579. (<a
href="https://doi.org/10.1613/jair.1.11734">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-agent path-finding (MAPF) is the problem of finding a plan for moving a set of agents from their initial locations to their goals without collisions. Following this plan, however, may not be possible due to unexpected events that delay some of the agents. In this work, we propose a holistic solution for MAPF that is robust to such unexpected delays. First, we introduce the notion of a k-robust MAPF plan, which is a plan that can be executed even if a limited number (k) of delays occur. We propose sufficient and required conditions for finding a k-robust plan, and show how to convert several MAPF solvers to find such plans. Then, we propose several robust execution policies. An execution policy is a policy for agents executing a MAPF plan. An execution policy is robust if following it guarantees that the agents reach their goals even if they encounter unexpected delays. Several classes of such robust execution policies are proposed and evaluated experimentally. Finally, we present robust execution policies for cases where communication between the agents may also be delayed. We performed an extensive experimental evaluation in which we compared different algorithms for finding robust MAPF plans, compared different ro- bust execution policies, and studied the interplay between having a robust plan and the performance when using a robust execution policy.},
  archive      = {J_JAIR},
  author       = {Dor Atzmon and Roni Stern and Ariel Felner and Glenn Wagner and Roman Barták and Neng-Fa Zhou},
  doi          = {10.1613/jair.1.11734},
  journal      = {Journal of Artificial Intelligence Research},
  pages        = {549-579},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {Robust multi-agent path finding and executing},
  volume       = {67},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A global constraint for the exact cover problem: Application
to conceptual clustering. <em>JAIR</em>, <em>67</em>, 509–547. (<a
href="https://doi.org/10.1613/jair.1.11870">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce the exactCover global constraint dedicated to the exact cover problem, the goal of which is to select subsets such that each element of a given set belongs to exactly one selected subset. This NP-complete problem occurs in many applications, and we more particularly focus on a conceptual clustering application. We introduce three propagation algorithms for exactCover, called Basic, DL, and DL+: Basic ensures the same level of consistency as arc consistency on a classical decomposition of exactCover into binary constraints, without using any specific data structure; DL ensures the same level of consistency as Basic but uses Dancing Links to efficiently maintain the relation between elements and subsets; and DL+ is a stronger propagator which exploits an extra property to filter more values than DL. We also consider the case where the number of selected subsets is constrained to be equal to a given integer variable k, and we show that this may be achieved either by combining exactCover with existing constraints, or by designing a specific propagator that integrates algorithms designed for the NValues constraint. These different propagators are experimentally evaluated on conceptual clustering problems, and they are compared with state-of-the-art declarative approaches. In particular, we show that our global constraint is competitive with recent ILP and CP models for mono-criterion problems, and it has better scale-up properties for multi-criteria problems.},
  archive      = {J_JAIR},
  author       = {Maxime Chabert and Christine Solnon},
  doi          = {10.1613/jair.1.11870},
  journal      = {Journal of Artificial Intelligence Research},
  pages        = {509-547},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {A global constraint for the exact cover problem: Application to conceptual clustering},
  volume       = {67},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Fair allocation with diminishing differences. <em>JAIR</em>,
<em>67</em>, 471–507. (<a
href="https://doi.org/10.1613/jair.1.11994">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ranking alternatives is a natural way for humans to explain their preferences. It is used in many settings, such as school choice, course allocations and residency matches. Without having any information on the underlying cardinal utilities, arguing about the fairness of allocations requires extending the ordinal item ranking to ordinal bundle ranking. The most commonly used such extension is stochastic dominance (SD), where a bundle X is preferred over a bundle Y if its score is better according to all additive score functions. SD is a very conservative extension, by which few allocations are necessarily fair while many allocations are possibly fair. We propose to make a natural assumption on the underlying cardinal utilities of the players, namely that the difference between two items at the top is larger than the difference between two items at the bottom. This assumption implies a preference extension which we call diminishing differences (DD), where X is preferred over Y if its score is better according to all additive score functions satisfying the DD assumption. We give a full characterization of allocations that are necessarily-proportional or possibly-proportional according to this assumption. Based on this characterization, we present a polynomial-time algorithm for finding a necessarily-DD-proportional allocation whenever it exists. Using simulations, we compare the various fairness criteria in terms of their probability of existence, and their probability of being fair by the underlying cardinal valuations. We find that necessary-DD-proportionality fares well in both measures. We also consider envy-freeness and Pareto optimality under diminishing-differences, as well as chore allocation under the analogous condition --- increasing-differences.},
  archive      = {J_JAIR},
  author       = {Erel Segal-Halevi and Avinatan Hassidim and Haris Aziz},
  doi          = {10.1613/jair.1.11994},
  journal      = {Journal of Artificial Intelligence Research},
  pages        = {471–507},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {Fair allocation with diminishing differences},
  volume       = {67},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Catching cheats: Detecting strategic manipulation in
distributed optimisation of electric vehicle aggregators. <em>JAIR</em>,
<em>67</em>, 437–470. (<a
href="https://doi.org/10.1613/jair.1.11573">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Given the rapid rise of electric vehicles (EVs) worldwide, and the ambitious targets set for the near future, the management of large EV fleets must be seen as a priority. Specifically, we study a scenario where EV charging is managed through self-interested EV aggregators who compete in the day-ahead market in order to purchase the electricity needed to meet their clients&#39; requirements. With the aim of reducing electricity costs and lowering the impact on electricity markets, a centralised bidding coordination framework has been proposed in the literature employing a coordinator. In order to improve privacy and limit the need for the coordinator, we propose a reformulation of the coordination framework as a decentralised algorithm, employing the Alternating Direction Method of Multipliers (ADMM). However, given the self-interested nature of the aggregators, they can deviate from the algorithm in order to reduce their energy costs. Hence, we study the strategic manipulation of the ADMM algorithm and, in doing so, describe and analyse different possible attack vectors and propose a mathematical framework to quantify and detect manipulation. Importantly, this detection framework is not limited to the considered EV scenario and can be applied to general ADMM algorithms. Finally, we test the proposed decentralised coordination and manipulation detection algorithms in realistic scenarios using real market and driver data from Spain. Our empirical results show that the decentralised algorithm&#39;s convergence to the optimal solution can be effectively disrupted by manipulative attacks achieving convergence to a different non-optimal solution which benefits the attacker. With respect to the detection algorithm, results indicate that it achieves very high accuracies and significantly outperforms a naive benchmark.},
  archive      = {J_JAIR},
  author       = {Alvaro Perez-Diaz and Enrico Harm Gerding and Frank McGroarty},
  doi          = {10.1613/jair.1.11573},
  journal      = {Journal of Artificial Intelligence Research},
  pages        = {437-470},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {Catching cheats: Detecting strategic manipulation in distributed optimisation of electric vehicle aggregators},
  volume       = {67},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Graph width measures for CNF-encodings with auxiliary
variables. <em>JAIR</em>, <em>67</em>, 409–436. (<a
href="https://doi.org/10.1613/jair.1.11750">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider bounded width CNF-formulas where the width is measured by popular graph width measures on graphs associated to CNF-formulas. Such restricted graph classes, in particular those of bounded treewidth, have been extensively studied for their uses in the design of algorithms for various computational problems on CNF-formulas. Here we consider the expressivity of these formulas in the model of clausal encodings with auxiliary variables. We first show that bounding the width for many of the measures from the literature leads to a dramatic loss of expressivity, restricting the formulas to such of low communication complexity. We then show that the width of optimal encodings with respect to different measures is strongly linked: there are two classes of width measures, one containing primal treewidth and the other incidence cliquewidth, such that in each class the width of optimal encodings only differs by constant factors. Moreover, between the two classes the width differs at most by a factor logarithmic in the number of variables. Both these results are in stark contrast to the setting without auxiliary variables where all width measures we consider here differ by more than constant factors and in many cases even by linear factors.},
  archive      = {J_JAIR},
  author       = {Stefan Mengel and Romain Wallon},
  doi          = {10.1613/jair.1.11750},
  journal      = {Journal of Artificial Intelligence Research},
  pages        = {409-436},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {Graph width measures for CNF-encodings with auxiliary variables},
  volume       = {67},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Adversarial attacks on crowdsourcing quality control.
<em>JAIR</em>, <em>67</em>, 375–408. (<a
href="https://doi.org/10.1613/jair.1.11332">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Crowdsourcing is a popular methodology to collect manual labels at scale. Such labels are often used to train AI models and, thus, quality control is a key aspect in the process. One of the most popular quality assurance mechanisms in paid micro-task crowdsourcing is based on gold questions: the use of a small set of tasks of which the requester knows the correct answer and, thus, is able to directly assess crowd work quality. In this paper, we show that such mechanism is prone to an attack carried out by a group of colluding crowd workers that is easy to implement and deploy: the inherent size limit of the gold set can be exploited by building an inferential system to detect which parts of the job are more likely to be gold questions. The described attack is robust to various forms of randomisation and programmatic generation of gold questions. We present the architecture of the proposed system, composed of a browser plug-in and an external server used to share information, and briefly introduce its potential evolution to a decentralised implementation. We implement and experimentally validate the gold detection system, using real-world data from a popular crowdsourcing platform.&amp;nbsp; Our experimental results show that crowdworkers using the proposed system spend more time on signalled gold questions but do not neglect the others thus achieving an increased overall work quality. Finally, we discuss the economic and sociological implications of this kind of attack.},
  archive      = {J_JAIR},
  author       = {Alessandro Checco and Jo Bates and Gianluca Demartini},
  doi          = {10.1613/jair.1.11332},
  journal      = {Journal of Artificial Intelligence Research},
  pages        = {375-408},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {Adversarial attacks on crowdsourcing quality control},
  volume       = {67},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Jointly improving parsing and perception for natural
language commands through human-robot dialog. <em>JAIR</em>,
<em>67</em>, 327–374. (<a
href="https://doi.org/10.1613/jair.1.11485">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this work, we present methods for using human-robot dialog to improve language understanding for a mobile robot agent. The agent parses natural language to underlying semantic meanings and uses robotic sensors to create multi-modal models of perceptual concepts like red and heavy. The agent can be used for showing navigation routes, delivering objects to people, and relocating objects from one location to another. We use dialog clari_cation questions both to understand commands and to generate additional parsing training data. The agent employs opportunistic active learning to select questions about how words relate to objects, improving its understanding of perceptual concepts. We evaluated this agent on Amazon Mechanical Turk. After training on data induced from conversations, the agent reduced the number of dialog questions it asked while receiving higher usability ratings. Additionally, we demonstrated the agent on a robotic platform, where it learned new perceptual concepts on the y while completing a real-world task.},
  archive      = {J_JAIR},
  author       = {Jesse Thomason and Aishwarya Padmakumar and Jivko Sinapov and Nick Walker and Yuqian Jiang and Harel Yedidsion and Justin Hart and Peter Stone and Raymond Mooney},
  doi          = {10.1613/jair.1.11485},
  journal      = {Journal of Artificial Intelligence Research},
  pages        = {327-374},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {Jointly improving parsing and perception for natural language commands through human-robot dialog},
  volume       = {67},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). TensorLog: A probabilistic database implemented using
deep-learning infrastructure. <em>JAIR</em>, <em>67</em>, 285–325. (<a
href="https://doi.org/10.1613/jair.1.11944">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present an implementation of a probabilistic first-order logic called TensorLog, in which classes of logical queries are compiled into differentiable functions in a neural-network infrastructure such as Tensorflow or Theano. This leads to a close integration of probabilistic logical reasoning with deep-learning infrastructure: in particular, it enables high-performance deep learning frameworks to be used for tuning the parameters of a probabilistic logic. The integration with these frameworks enables use of GPU-based parallel processors for inference and learning, making TensorLog the first highly parallellizable probabilistic logic. Experimental results show that TensorLog scales to problems involving hundreds of thousands of knowledge-base triples and tens of thousands of examples.},
  archive      = {J_JAIR},
  author       = {William Cohen and Fan Yang and Kathryn Rivard Mazaitis},
  doi          = {10.1613/jair.1.11944},
  journal      = {Journal of Artificial Intelligence Research},
  pages        = {285-325},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {TensorLog: A probabilistic database implemented using deep-learning infrastructure},
  volume       = {67},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Planning for hybrid systems via satisfiability modulo
theories. <em>JAIR</em>, <em>67</em>, 235–283. (<a
href="https://doi.org/10.1613/jair.1.11751">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Planning for hybrid systems is important for dealing with real-world applications, and PDDL+ supports this representation of domains with mixed discrete and continuous dynamics. In this paper we present a new approach for planning for hybrid systems, based on encoding the planning problem as a Satisfiability Modulo Theories (SMT) formula. This is the first SMT encoding that can handle the whole set of PDDL+ features (including processes and events), and is implemented in the planner SMTPlan. SMTPlan not only covers the full semantics of PDDL+, but can also deal with non-linear polynomial continuous change without discretization. This allows it to generate plans with non-linear dynamics that are correct-by-construction. The encoding is based on the notion of happenings, and can be applied on domains with nonlinear continuous change. We describe the encoding in detail and provide in-depth examples. We apply this encoding in an iterative deepening planning algorithm. Experimental results show that the approach dramatically outperforms existing work in finding plans for PDDL+ problems. We also present experiments which explore the performance of the proposed approach on temporal planning problems, showing that the scalability of the approach is limited by the size of the discrete search space. We further extend the encoding to include planning with control parameters. The extended encoding allows the definition of actions to include infinite domain parameters, called control parameters. We present experiments on a set of problems with control parameters to demonstrate the positive effect they provide to the approach of planning via SMT.},
  archive      = {J_JAIR},
  author       = {Michael Cashmore and Daniele Magazzeni and Parisa Zehtabi},
  doi          = {10.1613/jair.1.11751},
  journal      = {Journal of Artificial Intelligence Research},
  pages        = {235-283},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {Planning for hybrid systems via satisfiability modulo theories},
  volume       = {67},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Blind spot detection for safe sim-to-real transfer.
<em>JAIR</em>, <em>67</em>, 191–234. (<a
href="https://doi.org/10.1613/jair.1.11436">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Agents trained in simulation may make errors when performing actions in the real world due to mismatches between training and execution environments. These mistakes can be dangerous and difficult for the agent to discover because the agent is unable to predict them a priori. In this work, we propose the use of oracle feedback to learn a predictive model of these blind spots in order to reduce costly errors in real-world applications. We focus on blind spots in reinforcement learning (RL) that occur due to incomplete state representation: when the agent lacks necessary features to represent the true state of the world, and thus cannot distinguish between numerous states. We formalize the problem of discovering blind spots in RL as a noisy supervised learning problem with class imbalance. Our system learns models for predicting blind spots within unseen regions of the state space by combining techniques for label aggregation, calibration, and supervised learning. These models take into consideration noise emerging from different forms of oracle feedback, including demonstrations and corrections. We evaluate our approach across two domains and demonstrate that it achieves higher predictive performance than baseline methods, and also that the learned model can be used to selectively query an oracle at execution time to prevent errors. We also empirically analyze the biases of various feedback types and how these biases influence the discovery of blind spots. Further, we include analyses of our approach that incorporate relaxed initial optimality assumptions. (Interestingly, relaxing the assumptions of an optimal oracle and an optimal simulator policy helped our models to perform better.) We also propose extensions to our method that are intended to improve performance when using corrections and demonstrations data.},
  archive      = {J_JAIR},
  author       = {Ramya Ramakrishnan and Ece Kamar and Debadeepta Dey and Eric Horvitz and Julie Shah},
  doi          = {10.1613/jair.1.11436},
  journal      = {Journal of Artificial Intelligence Research},
  pages        = {191-234},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {Blind spot detection for safe sim-to-real transfer},
  volume       = {67},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). The force awakens: Artificial intelligence for consumer law.
<em>JAIR</em>, <em>67</em>, 169–190. (<a
href="https://doi.org/10.1613/jair.1.11519">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent years have been tainted by market practices that continuously expose us, as consumers, to new risks and threats. We have become accustomed, and sometimes even resigned, to businesses monitoring our activities, examining our data, and even meddling with our choices. Artificial Intelligence (AI) is often depicted as a weapon in the hands of businesses and blamed for allowing this to happen. In this paper, we envision a paradigm shift, where AI technologies are brought to the side of consumers and their organizations, with the aim of building an efficient and effective counter-power. AI-powered tools can support a massive-scale automated analysis of textual and audiovisual data, as well as code, for the benefit of consumers and their organizations. This in turn can lead to a better oversight of business activities, help consumers exercise their rights, and enable the civil society to mitigate information overload. We discuss the societal, political, and technological challenges that stand before that vision.&amp;nbsp; This article is part of the special track on AI and Society.},
  archive      = {J_JAIR},
  author       = {Marco Lippi and Giuseppe Contissa and Agnieszka Jablonowska and Francesca Lagioia and Hans-Wolfgang Micklitz and Przemyslaw Palka and Giovanni Sartor and Paolo Torroni},
  doi          = {10.1613/jair.1.11519},
  journal      = {Journal of Artificial Intelligence Research},
  pages        = {169-190},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {The force awakens: Artificial intelligence for consumer law},
  volume       = {67},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Saturated cost partitioning for optimal classical planning.
<em>JAIR</em>, <em>67</em>, 129–167. (<a
href="https://doi.org/10.1613/jair.1.11673">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cost partitioning is a method for admissibly combining a set of admissible heuristic estimators by distributing operator costs among the heuristics. Computing an optimal cost partitioning, i.e., the operator cost distribution that maximizes the heuristic value, is often prohibitively expensive to compute. Saturated cost partitioning is an alternative that is much faster to compute and has been shown to yield high-quality heuristics. However, its greedy nature makes it highly susceptible to the order in which the heuristics are considered. We propose a greedy algorithm to generate orders and show how to use hill-climbing search to optimize a given order. Combining both techniques leads to significantly better heuristic estimates than using the best random order that is generated in the same time. Since there is often no single order that gives good guidance on the whole state space, we use the maximum of multiple orders as a heuristic that is significantly better informed than any single-order heuristic, especially when we actively search for a set of diverse orders.},
  archive      = {J_JAIR},
  author       = {Jendrik Seipp and Thomas Keller and Malte Helmert},
  doi          = {10.1613/jair.1.11673},
  journal      = {Journal of Artificial Intelligence Research},
  pages        = {129–167},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {Saturated cost partitioning for optimal classical planning},
  volume       = {67},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Regret bounds for reinforcement learning via markov chain
concentration. <em>JAIR</em>, <em>67</em>, 115–128. (<a
href="https://doi.org/10.1613/jair.1.11316">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We give a simple optimistic algorithm for which it is easy to derive regret bounds of O(sqrt{t-mix SAT}) steps in uniformly ergodic Markov decision processes with S states, A actions, and mixing time parameter t-mix. These bounds are the first regret bounds in the general, non-episodic setting with an optimal dependence on all given parameters. They could only be improved by using an alternative mixing time parameter.},
  archive      = {J_JAIR},
  author       = {Ronald Ortner},
  doi          = {10.1613/jair.1.11316},
  journal      = {Journal of Artificial Intelligence Research},
  pages        = {115-128},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {Regret bounds for reinforcement learning via markov chain concentration},
  volume       = {67},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). The 2^k neighborhoods for grid path planning. <em>JAIR</em>,
<em>67</em>, 81–113. (<a
href="https://doi.org/10.1613/jair.1.11383">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Grid path planning is an important problem in AI. Its understanding has been key for the development of autonomous navigation systems. An interesting and rather surprising fact about the vast literature on this problem is that only a few neighborhoods have been used when evaluating these algorithms. Indeed, only the 4- and 8-neighborhoods are usually considered, and rarely the 16-neighborhood. This paper describes three contributions that enable the construction of effective grid path planners for extended 2k-neighborhoods; that is, neighborhoods that admit 2k neighbors per state, where k is a parameter. First, we provide a simple recursive definition of the 2k-neighborhood in terms of the 2k-1-neighborhood. Second, we derive distance functions, for any k ≥ 2, which allow us to propose admissible heuristics that are perfect for obstacle-free grids, which generalize the well-known Manhattan and Octile distances. Third, we define the notion of canonical path for the 2k-neighborhood; this allows us to incorporate our neighborhoods into two versions of A*, namely Canonical A* and Jump Point Search (JPS), whose performance, we show, scales well when increasing k. Our empirical evaluation shows that, when increasing k, the cost of the solution found improves substantially.&amp;nbsp; Used with the 2k-neighborhood, Canonical A* and JPS, in many configurations, are also superior to the any-angle path planner Theta* both in terms of solution quality and runtime. Our planner is competitive with one implementation of the any-angle path planner, ANYA in some configurations. Our main practical conclusion is that standard, well-understood grid path planning technology may provide an effective approach to any-angle grid path planning.},
  archive      = {J_JAIR},
  author       = {Nicolás Rivera and Carlos Hernández and Nicolás Hormazábal and Jorge A Baier},
  doi          = {10.1613/jair.1.11383},
  journal      = {Journal of Artificial Intelligence Research},
  pages        = {81-113},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {The 2^k neighborhoods for grid path planning},
  volume       = {67},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). The impact of treewidth on grounding and solving of answer
set programs. <em>JAIR</em>, <em>67</em>, 35–80. (<a
href="https://doi.org/10.1613/jair.1.11515">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we aim to study how the performance of modern answer set programming (ASP) solvers is influenced by the treewidth of the input program and to investigate the consequences of this relationship. We first perform an experimental evaluation that shows that the solving performance is heavily influenced by treewidth, given ground input programs that are otherwise uniform, both in size and construction. This observation leads to an important question for ASP, namely, how to design encodings such that the treewidth of the resulting ground program remains small. To this end, we study two classes of disjunctive programs, namely guarded and connection-guarded programs. In order to investigate these classes, we formalize the grounding process using MSO transductions. Our main results show that both classes guarantee that the treewidth of the program after grounding only depends on the treewidth (and the maximum degree, in case of connection-guarded programs) of the input instance. In terms of parameterized complexity, our findings yield corresponding FPT results for answer-set existence for bounded treewidth (and also degree, for connection-guarded programs) of the input instance. We further show that bounding treewidth alone leads to NP-hardness in the data complexity for connection-guarded programs,&amp;nbsp;which indicates that the two classes are fundamentally different. Finally, we show that for both classes, the data complexity remains&amp;nbsp;as hard as in the general case of ASP.},
  archive      = {J_JAIR},
  author       = {Bernhard Bliem and Michael Morak and Marius Moldovan and Stefan Woltran},
  doi          = {10.1613/jair.1.11515},
  journal      = {Journal of Artificial Intelligence Research},
  pages        = {35-80},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {The impact of treewidth on grounding and solving of answer set programs},
  volume       = {67},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Best-first enumeration based on bounding conflicts, and its
application to large-scale hybrid estimation. <em>JAIR</em>,
<em>67</em>, 1–34. (<a
href="https://doi.org/10.1613/jair.1.11892">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {There is an increasing desire for autonomous systems to have high levels of robustness and safety, attained through continuously planning and self-repairing online. Underlying this is the need to accurately estimate the system state and diagnose subtle failures. Estimation methods based on hybrid discrete and continuous state models have emerged as a method of precisely computing these estimates. However, existing methods have difficulty scaling to systems with more than a handful of components. Discrete, consistency based state estimation capabilities can scale to this level by combining best-first enumeration and conflict-directed search. While best-first methods have been developed for hybrid estimation, conflict-directed methods have thus far been elusive as conflicts learn inconsistencies from constraint violation, but probabilistic hybrid estimation is relatively unconstrained. In this paper we present an approach to hybrid estimation that unifies best-first enumeration and conflict-directed search through the concept of &quot;bounding&quot; conflicts, an extension of conflicts that represent tighter bounds on the cost of regions of the search space. This paper presents a general best-first enumeration algorithm based on bounding conflicts (A*BC) and a hybrid estimation method using this enumeration algorithm. Experiments show that an A*BC powered state estimator produces estimates up to an order of magnitude faster than the current state of the art, particularly on large systems.},
  archive      = {J_JAIR},
  author       = {Eric Michael Timmons and Brian Charles Williams},
  doi          = {10.1613/jair.1.11892},
  journal      = {Journal of Artificial Intelligence Research},
  pages        = {1-34},
  shortjournal = {J. Artif. Intell. Res.},
  title        = {Best-first enumeration based on bounding conflicts, and its application to large-scale hybrid estimation},
  volume       = {67},
  year         = {2020},
}
</textarea>
</details></li>
</ul>

</body>
</html>
