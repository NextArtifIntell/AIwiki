<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>TAP_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="tap---21">TAP - 21</h2>
<ul>
<li><details>
<summary>
(2020). Quantification of displacement for tactile sensation in a
contact-type low intensity focused ultrasound haptic device.
<em>TAP</em>, <em>18</em>(1), 1–8. (<a
href="https://doi.org/10.1145/3422820">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Tactile threshold of low-intensity focused ultrasound (LIFU) haptic devices has been defined as the minimum pressure required for tactile sensation. However, in contact-type LIFU haptic devices using an elastomer as a conductive medium, the tactile threshold is affected by the mechanical properties of the elastomer. Therefore, the tactile threshold needs to be redefined as a parameter that does not change with the mechanical properties of the elastomer. In this study, we used the LIFU haptic device to investigate the displacement of the elastomer surface at the tactile threshold while controlling the pulse duration, pulse repetition frequency, and pressure. We analyzed the displacement magnitude and rate to determine their relationship to the pressure. The displacement magnitude is the spatiotemporal peak of the displacement, and the displacement rate is the initial slope of the displacement at the starting point of LIFU pulse. The tactile threshold measured by the applied pressure showed the U-shaped graph, and the minimum pressure of 475 kPa at 2 ms and 407 kPa at 300 Hz was measured. The tactile threshold measured by the displacement show that the tactile sensation can be evoked at the small displacement magnitude (&lt;3 μm) when the high displacement rate is present (&gt;1.56 mm/s). Furthermore, the large displacement magnitude is required to induce the tactile sensation when the displacement rate is low. This result shows that the tactile threshold of a contact-type LIFU haptic device is affected by both the displacement magnitude and rate of the conductive medium. Our findings can be used as a guideline for developing a contact-type LIFU haptic device regardless of the elastomer used.},
  archive      = {J_TAP},
  doi          = {10.1145/3422820},
  journal      = {ACM Transactions on Applied Perception},
  month        = {12},
  number       = {1},
  pages        = {1-8},
  shortjournal = {ACM Trans. Appl. Perc.},
  title        = {Quantification of displacement for tactile sensation in a contact-type low intensity focused ultrasound haptic device},
  volume       = {18},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Evaluating automated face identity-masking methods with
human perception and a deep convolutional neural network. <em>TAP</em>,
<em>18</em>(1), 1–20. (<a
href="https://doi.org/10.1145/3422988">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Face de-identification (or “masking”) algorithms have been developed in response to the prevalent use of video recordings in public places. We evaluated the success of face identity masking for human perceivers and a deep convolutional neural network (DCNN). Eight de-identification algorithms were applied to videos of drivers’ faces, while they actively operated a motor vehicle. These masks were pre-selected to be applicable to low-quality video and to maintain coarse information about facial actions. Humans studied high-resolution images to learn driver identities and were tested on their recognition of active drivers in low-resolution videos. Faces in the videos were either unmasked or were masked by one of the eight algorithms. When participants were tested immediately after learning (Experiment 1), all masks reduced identification, with six of eight masks reducing identification to extremely poor performance. In a second experiment, two of the most effective masks were tested after a delay of 7 or 28 days. The delay did not further reduce identification of the masked faces. In all masked conditions, participants maintained stringent decision criteria, with low confidence in recognition, further indicating the effectiveness of the masks. Next, the DCNN performed an identity-matching task between high-resolution images and masked videos—a task analogous to that done by humans. The pattern of accuracy for the DCNN mirrored some, but not all, aspects of human performance, highlighting the need to test the effectiveness of identity masking for both humans and machines. The DCNN was also tested on its ability to match identity between masked and unmasked versions of the same video, based only on the face. DCNN performance for the eight masks offers insight into the nature of the information in faces that is coded in these networks.},
  archive      = {J_TAP},
  doi          = {10.1145/3422988},
  journal      = {ACM Transactions on Applied Perception},
  month        = {12},
  number       = {1},
  pages        = {1-20},
  shortjournal = {ACM Trans. Appl. Perc.},
  title        = {Evaluating automated face identity-masking methods with human perception and a deep convolutional neural network},
  volume       = {18},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Comparison of subjective methods for quality assessment of
3D graphics in virtual reality. <em>TAP</em>, <em>18</em>(1), 1–23. (<a
href="https://doi.org/10.1145/3427931">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Numerous methodologies for subjective quality assessment exist in the field of image processing. In particular, the Absolute Category Rating with Hidden Reference (ACR-HR), the Double Stimulus Impairment Scale (DSIS), and the Subjective Assessment Methodology for Video Quality (SAMVIQ) are considered three of the most prominent methods for assessing the visual quality of 2D images and videos. Are these methods valid/accurate to evaluate the perceived quality of 3D graphics data? Is the presence of an explicit reference necessary, due to the lack of human prior knowledge on 3D graphics data compared to natural images/videos? To answer these questions, we compare these three subjective methods (ACR-HR, DSIS, and SAMVIQ) on a dataset of high-quality colored 3D models, impaired with various distortions. These subjective experiments were conducted in a virtual reality environment. Our results show differences in the performance of the methods depending on the 3D contents and the types of distortions. We show that DSIS and SAMVIQ outperform ACR-HR in terms of accuracy and point out a stable performance. In regard to the time-effort, DSIS achieves the highest accuracy in the shortest assessment time. Results also yield interesting conclusions on the importance of a reference for judging the quality of 3D graphics. We finally provide recommendations regarding the influence of the number of observers on the accuracy.},
  archive      = {J_TAP},
  doi          = {10.1145/3427931},
  journal      = {ACM Transactions on Applied Perception},
  month        = {12},
  number       = {1},
  pages        = {1-23},
  shortjournal = {ACM Trans. Appl. Perc.},
  title        = {Comparison of subjective methods for quality assessment of 3D graphics in virtual reality},
  volume       = {18},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Crossing roads with a computer-generated agent: Persistent
effects on perception–action tuning. <em>TAP</em>, <em>18</em>(1), 1–16.
(<a href="https://doi.org/10.1145/3431923">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study investigated how people coordinate their decisions and actions with a risky or safe computer-generated agent in a humanoid or non-humanoid form and how this experience influences later behavior when acting alone. In Experiment 1, participants first repeatedly crossed continuous traffic in a virtual environment with a humanoid computer-generated agent (Figure 1). Participants were specifically instructed to cross with an agent that was programmed to be either safe (taking only large gaps) or risky (also taking relatively small gaps). Participants then repeatedly crossed the same roadway alone. We found that participants’ experiences with crossing safe vs. risky gaps with an agent persisted in later trials when the participants crossed alone, such that participants accepted tighter gaps if they were previously paired with a risky than a safe agent.  In Experiment 2 (Figure 2), we tested whether experience crossing with a risky or safe non-humanoid object (a floating box) also influenced later behavior when crossing alone. We again found that participants who crossed with the risky object partner took tighter gaps when later crossing alone than those who crossed with the safe object partner. The Discussion focuses on the impact of experiences with virtual agents on perception–action tuning and the potential of using virtual agents for training safe road-crossing behavior.},
  archive      = {J_TAP},
  doi          = {10.1145/3431923},
  journal      = {ACM Transactions on Applied Perception},
  month        = {12},
  number       = {1},
  pages        = {1-16},
  shortjournal = {ACM Trans. Appl. Perc.},
  title        = {Crossing roads with a computer-generated agent: Persistent effects on Perception–Action tuning},
  volume       = {18},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A fitts’ law evaluation of visuo-haptic fidelity and sensory
mismatch on user performance in a near-field disc transfer task in
virtual reality. <em>TAP</em>, <em>17</em>(4), 1–20. (<a
href="https://doi.org/10.1145/3419986">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The trade-off between speed and accuracy in precision tasks is important to evaluate during user interaction with input devices. When different sensory cues are added or altered in such interactions, those cues have an effect on this trade-off, and thus, they affect overall user performance. For instance, adding cues like haptic feedback and stereoscopic viewing will result in more realistic user interaction, thus improving performance in these tasks. Also, adding a noticeable disparity between physical and virtual movements creates a mismatch between visual and proprioceptive systems, which generally has a negative effect on performance. In this study, we investigate the effects of haptic feedback, stereoscopic viewing, and visuo-proprioceptive mismatch on how quickly and accurately users complete a virtual pick-and-place task using the PHANToM OMNI. Through this experiment, we find that in the movement phase of a ring transfer, movement time and user performance are affected by haptic feedback and visuo-proprioceptive mismatch, and the main effects of stereoscopic viewing appears to be limited to the more precise step when the ring is around the target peg.},
  archive      = {J_TAP},
  doi          = {10.1145/3419986},
  journal      = {ACM Transactions on Applied Perception},
  month        = {12},
  number       = {4},
  pages        = {1-20},
  shortjournal = {ACM Trans. Appl. Perc.},
  title        = {A fitts’ law evaluation of visuo-haptic fidelity and sensory mismatch on user performance in a near-field disc transfer task in virtual reality},
  volume       = {17},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Introduction to the special issue on SAP 2020. <em>TAP</em>,
<em>17</em>(4), 1–2. (<a href="https://doi.org/10.1145/3428144">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {No abstract available.},
  archive      = {J_TAP},
  doi          = {10.1145/3428144},
  journal      = {ACM Transactions on Applied Perception},
  month        = {12},
  number       = {4},
  pages        = {1-2},
  shortjournal = {ACM Trans. Appl. Perc.},
  title        = {Introduction to the special issue on SAP 2020},
  volume       = {17},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Toward quantifying ambiguities in artistic images.
<em>TAP</em>, <em>17</em>(4), 1–10. (<a
href="https://doi.org/10.1145/3418054">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {It has long been hypothesized that perceptual ambiguities play an important role in aesthetic experience: A work with some ambiguity engages a viewer more than one that does not. However, current frameworks for testing this theory are limited by the availability of stimuli and data collection methods. This article presents an approach to measuring the perceptual ambiguity of a collection of images. Crowdworkers are asked to describe image content, after different viewing durations. Experiments are performed using images created with Generative Adversarial Networks, using the Artbreeder website. We show that text processing of viewer responses can provide a fine-grained way to measure and describe image ambiguities.},
  archive      = {J_TAP},
  doi          = {10.1145/3418054},
  journal      = {ACM Transactions on Applied Perception},
  month        = {11},
  number       = {4},
  pages        = {1-10},
  shortjournal = {ACM Trans. Appl. Perc.},
  title        = {Toward quantifying ambiguities in artistic images},
  volume       = {17},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). How the presence and size of static peripheral blur affects
cybersickness in virtual reality. <em>TAP</em>, <em>17</em>(4), 1–18.
(<a href="https://doi.org/10.1145/3419984">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cybersickness (CS) is one of the challenges that has hindered the widespread adoption of Virtual Reality and its applications. Consequently, a number of studies have focused on extensively understanding and reducing CS. Inspired by previous work that has sought to reduce CS using foveated rendering and Field of View (FOV) restrictions, we investigated how the presence and size of a static central window in peripheral FOV blurring affects CS. To facilitate this peripheral FOV blur, we applied a Gaussian blur effect in the display peripheral region, provisioning a full-resolution central window. Thirty participants took part in a three-session, within-subjects experiment, performing search and spatial updating tasks in a first-person, slow-walking, maze-traveling scenario. Two different central window sizes (small and large) were tested against a baseline condition that didn’t feature display peripheral blurring. Results revealed that the baseline condition produced higher levels of CS than both conditions with a central window. While there were no significant differences between the small and large windows, we observed interaction effects suggesting an influence of window size on “adaptation to CS.” When the central window is small, adaptation to CS seems to take more time but is more pronounced. The interventions had no effect on spatial updating and presence, but were detectable when the blurred area was larger (small central window). Lower sickness levels observed in both window conditions supports the use of peripheral FOV blurring to reduce CS, reducing our dependence on eye tracking. This being said, researchers must strive to find the right balance between window size and detectability to ensure seamless virtual experiences.},
  archive      = {J_TAP},
  doi          = {10.1145/3419984},
  journal      = {ACM Transactions on Applied Perception},
  month        = {11},
  number       = {4},
  pages        = {1-18},
  shortjournal = {ACM Trans. Appl. Perc.},
  title        = {How the presence and size of static peripheral blur affects cybersickness in virtual reality},
  volume       = {17},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). The effect of gender and attractiveness of motion on
proximity in virtual reality. <em>TAP</em>, <em>17</em>(4), 1–15. (<a
href="https://doi.org/10.1145/3419985">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In human interaction, people will keep different distances from each other depending on their gender. For example, males will stand further away from males and closer to females. Previous studies in virtual reality (VR), where people were interacting with virtual humans, showed a similar result. However, many other variables influence proximity, such as appearance characteristics of the virtual character (e.g., attractiveness). Our study focuses on proximity to virtual walkers, where gender could be recognised from motion only, since previous studies using point-light displays found walking motion is rich in gender cues. In our experiment, a walking wooden mannequin approached the participant embodied in a virtual avatar using the HTC Vive Pro HMD and controllers. The mannequin animation was motion captured from several male and female actors and each motion was displayed individually on the character. Participants used the controller to stop the approaching mannequin when they felt it was uncomfortably close to them. Based on previous work, we hypothesised that proximity will be affected by the gender of the character, but unlike previous research, the gender in our experiment could only be determined from character’s motion. We also expected differences in proximity according to the gender of the participant. We additionally expected some motions to be rated more attractive than others and that attractive motions would reduce the proximity measure. Our results show support for the last two assumptions, but no difference in proximity was found according to the gender of the character’s motion. Our findings have implications for the design of virtual characters in interactive virtual environments.},
  archive      = {J_TAP},
  doi          = {10.1145/3419985},
  journal      = {ACM Transactions on Applied Perception},
  month        = {11},
  number       = {4},
  pages        = {1-15},
  shortjournal = {ACM Trans. Appl. Perc.},
  title        = {The effect of gender and attractiveness of motion on proximity in virtual reality},
  volume       = {17},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Providing semi-private feedback on a shared public screen by
controlling presentation onset. <em>TAP</em>, <em>17</em>(3), 1–32. (<a
href="https://doi.org/10.1145/3419983">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We describe a novel technique to provide semi-private feedback on a shared public screen. The technique uses a no-onset presentation that takes advantage of perceptual limitations in human vision to avoid alerting other users to feedback directed at one individual user by suppressing the sudden onset of the feedback. Three experiments evaluated the effectiveness of the technique and appropriate timing parameters and alternatives for presentation onset. Our experiments indicated that an 80 ms no-onset presentation allows participants to interpret information directed to them with over 90% accuracy, but their ability to interpret simultaneously presented information intended for others will be close to random chance. The technique initially camouflages the information being presented by overlaying additional visual elements and then removes those elements to reveal only the elements encoding the information being presented. We discuss applications for the technique, including classroom clicker usage, which was our original motivation for the study.},
  archive      = {J_TAP},
  doi          = {10.1145/3419983},
  journal      = {ACM Transactions on Applied Perception},
  month        = {11},
  number       = {3},
  pages        = {1-32},
  shortjournal = {ACM Trans. Appl. Perc.},
  title        = {Providing semi-private feedback on a shared public screen by controlling presentation onset},
  volume       = {17},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A systematic review of empirical measures of workload
capacity. <em>TAP</em>, <em>17</em>(3), 1–26. (<a
href="https://doi.org/10.1145/3422869">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The usability of the human-machine interface is dependent on the quality of its design and testing. Defining clear criteria that the interface must meet can assist the implementation and evaluation process. These criteria may be based on performance, the quality of users’ experience, error prevention, or the broad utility of the interface. In this article, we motivate the use for workload capacity as an empirical measure of usability. We first describe generic and specific uses for workload measures in terms of adaptive interfaces. We then carry out a systematic review of how workload capacity has been empirically measured, based on 172 relevant literature sources from psychology, neuroscience, engineering, and computer science. We then analyse and report on how workload capacity and related constructs, such as perceptual load, attention, and working memory have been defined and measured in these sources. We discuss similarities and differences between constructs and identify opportunities for integrating real-time workload capacity measures into dynamic interfaces.},
  archive      = {J_TAP},
  doi          = {10.1145/3422869},
  journal      = {ACM Transactions on Applied Perception},
  month        = {10},
  number       = {3},
  pages        = {1-26},
  shortjournal = {ACM Trans. Appl. Perc.},
  title        = {A systematic review of empirical measures of workload capacity},
  volume       = {17},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Learning the vibrotactile morse code alphabet. <em>TAP</em>,
<em>17</em>(3), 1–10. (<a
href="https://doi.org/10.1145/3402935">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Vibrotactile Morse code provides a way to convey words using the sense of touch with vibrations. This can be useful in applications for users with a visual and/or auditory impairment. The advantage of using vibrotactile Morse code is that it is technically easy to accomplish. The usefulness of tactile Morse code also depends on how easy it is to learn to use without providing a visual representation of the code. Here we investigated learning of the vibrotactile the Morse code alphabet without any visual representation of the code and whether the learned letters can immediately be used to recognize words. Two vibration motors were used: one was attached to the left arm (dots) and the other to the right arm (dashes). We gave the participants a learning session of 30 minutes and determined how many letters they had learned. All participants managed to learn at least 15 letters in this time. Directly afterward, they were presented with 2-, 3-, 4-, or 5-letter words consisting of only the letters they had learned. Participants were able to identify words, but correct rates decreased rapidly with word length. We can conclude that it is possible to learn vibrotactile Morse code using only a vibrotactile representation (15 to 24 letters in 30 minutes). After the learning session, it was possible to recognise words, but to increase the recognition rates extra training would be beneficial.},
  archive      = {J_TAP},
  doi          = {10.1145/3402935},
  journal      = {ACM Transactions on Applied Perception},
  month        = {8},
  number       = {3},
  pages        = {1-10},
  shortjournal = {ACM Trans. Appl. Perc.},
  title        = {Learning the vibrotactile morse code alphabet},
  volume       = {17},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). On the perception analysis of user feedback for interactive
face retrieval. <em>TAP</em>, <em>17</em>(3), 1–20. (<a
href="https://doi.org/10.1145/3403964">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we explore the coherence of face perception between human and machine in the scenario of interactive face retrieval. In the part of human perception, we collect user feedback to the stimuli of a target face and groups of displayed candidate face images in a face database with a large number of subjects. In the part of machine vision, we compare the benchmark features and general metrics to measure face similarity. We propose a series of coherence measurements to evaluate the statistic characteristic of human and machine face perception. We discover that despite the unfamiliarity of users to most faces in the database, the coherence between human and machine remains in a stable level across multiple variations in metrics, features, size of databases, and demographics. The simulation experiments with the coherence distributions demonstrate that the embedded information is valuable to speed up interactive retrieval. The comparisons over multiple parameter settings provide feasible instructions in designing the interactive face retrieval system with more consideration of human factors.},
  archive      = {J_TAP},
  doi          = {10.1145/3403964},
  journal      = {ACM Transactions on Applied Perception},
  month        = {8},
  number       = {3},
  pages        = {1-20},
  shortjournal = {ACM Trans. Appl. Perc.},
  title        = {On the perception analysis of user feedback for interactive face retrieval},
  volume       = {17},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Three perceptual dimensions for specular and diffuse
reflection. <em>TAP</em>, <em>17</em>(2), 1–26. (<a
href="https://doi.org/10.1145/3380741">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Previous research investigated the perceptual dimensionality of achromatic reflection of opaque surfaces, by using either simple analytic models of reflection or measured reflection properties of a limited sample of materials. Here, we aim to extend this work to a broader range of simulated materials. In a first experiment, we used sparse multidimensional scaling techniques to represent a set of rendered stimuli in a perceptual space that is consistent with participants’ similarity judgments. Participants were presented with one reference object and four comparisons, rendered with different material properties. They were asked to rank the comparisons according to their similarity to the reference, resulting in an efficient collection of a large number of similarity judgments. To interpret the space individuated by multidimensional scaling, we ran a second experiment in which observers were asked to rate our experimental stimuli according to a list of 30 adjectives referring to their surface reflectance properties. Our results suggest that perception of achromatic reflection is based on at least three dimensions, which we labelled “Lightness,” “Gloss,” and “Metallicity,” in accordance with the rating results. These dimensions are characterized by a relatively simple relationship with the parameters of the physically based rendering model used to generate our stimuli, indicating that they correspond to different physical properties of the rendered materials. Specifically, “Lightness” relates to diffuse reflections, “Gloss” to the presence of high contrast sharp specular highlights, and “Metallicity” to spread out specular reflections.},
  archive      = {J_TAP},
  doi          = {10.1145/3380741},
  journal      = {ACM Transactions on Applied Perception},
  month        = {5},
  number       = {2},
  pages        = {1-26},
  shortjournal = {ACM Trans. Appl. Perc.},
  title        = {Three perceptual dimensions for specular and diffuse reflection},
  volume       = {17},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Feature weighted linguistics classifier for predicting
learning difficulty using eye tracking. <em>TAP</em>, <em>17</em>(2),
1–25. (<a href="https://doi.org/10.1145/3380877">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article presents a new approach to predict learning difficulty in applications such as e-learning using eye movement and pupil response. We have developed 12 eye response features based on psycholinguistics, contextual information processing, anticipatory behavior analysis, recurrence fixation analysis, and pupillary response. A key aspect of the proposed approach is the temporal analysis of the feature response to the same concept. Results show that variations in eye response to the same concept over time are indicative of learning difficulty. A Feature Weighted Linguistics Classifier (FWLC) was developed to predict learning difficulty in real time. The proposed approach predicts learning difficulty with an accuracy of 90%.},
  archive      = {J_TAP},
  doi          = {10.1145/3380877},
  journal      = {ACM Transactions on Applied Perception},
  month        = {5},
  number       = {2},
  pages        = {1-25},
  shortjournal = {ACM Trans. Appl. Perc.},
  title        = {Feature weighted linguistics classifier for predicting learning difficulty using eye tracking},
  volume       = {17},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Establishing vibration-based tactile line profiles for use
in multimodal graphics. <em>TAP</em>, <em>17</em>(2), 1–14. (<a
href="https://doi.org/10.1145/3383457">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Vibration plays a significant role in the way users interact with touchscreens. For many users, vibration affords tactile alerts and other enhancements. For eyes-free users and users with visual impairments, vibration can also serve a more primary role in the user interface, such as indicating streets on maps, conveying information about graphs, or even specifying basic graphics. However, vibration is rarely used in current user interfaces beyond basic cuing. Furthermore, designers and developers who do actually use vibration more extensively are often unable to determine the exact properties of the vibration signals they are implementing, due to out-of-the-box software and hardware limitations. We make two contributions in this work. First, we investigate the contextual properties of touchscreen vibrations and how vibrations can be used to effectively convey traditional, embossed elements, such as dashes and dots. To do so, we developed an open source, Android-based library to generate vibrations that are perceptually salient and intuitive, improving upon existing vibration libraries. Second, we conducted a user study with 26 blind or visually impaired users to evaluate and categorize the effects with respect to traditional tactile line profiles. We have established a range of vibration effects that can be reliably generated by our haptic library and are perceptible and distinguishable by users.},
  archive      = {J_TAP},
  doi          = {10.1145/3383457},
  journal      = {ACM Transactions on Applied Perception},
  month        = {5},
  number       = {2},
  pages        = {1-14},
  shortjournal = {ACM Trans. Appl. Perc.},
  title        = {Establishing vibration-based tactile line profiles for use in multimodal graphics},
  volume       = {17},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Does it ping or pong? Auditory and tactile classification of
materials by bouncing events. <em>TAP</em>, <em>17</em>(2), 1–17. (<a
href="https://doi.org/10.1145/3393898">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Two experiments studied the role of impact sounds and vibrations in classification of materials. The task consisted of feeling on an actuated surface and listening through headphones to the recorded feedback of a ping-pong ball hitting three flat objects respectively made of wood, plastic, and metal, and then identifying their material. In Experiment 1, sounds and vibrations were recorded by keeping the objects in mechanical isolation. In Experiment 2, recordings were taken while the same objects stood on a table, causing their resonances to fade faster due to mechanical coupling with the support. A control experiment, where participants listened to and touched the real objects in mechanical isolation, showed high accuracy of classification from either sounds (90% correct) or vibrations (67% correct). Classification of reproduced bounces in Experiments 1 and 2 was less precise. In both experiments, the main effect of material was statistically significant; conversely, the main effect of modality (auditory or tactile) was significant only in the control. Identification of plastic and especially metal was less accurate in Experiment 2, suggesting that participants, when possible, classified materials by longer resonance tails. Audio-tactile summation of classification accuracy was found, suggesting that multisensory integration influences the perception of materials. Such results have prospective application to the nonvisual design of virtual buttons, which is the object of our current research.},
  archive      = {J_TAP},
  doi          = {10.1145/3393898},
  journal      = {ACM Transactions on Applied Perception},
  month        = {5},
  number       = {2},
  pages        = {1-17},
  shortjournal = {ACM Trans. Appl. Perc.},
  title        = {Does it ping or pong? auditory and tactile classification of materials by bouncing events},
  volume       = {17},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Visual information requirements for dismounted soldier
target acquisition. <em>TAP</em>, <em>17</em>(1), 1–20. (<a
href="https://doi.org/10.1145/3375000">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We conducted an empirical investigation of the visual information requirements for target detection and threat identification decisions in the dismounted soldier context. Forty soldiers viewed digital photographs of a person standing against a forested background. The soldiers made two-alternative detection decisions requiring them to determine whether the target was present in the scene, and two-alternative threat identification decisions that required discrimination of the objects held by the target, the clothing worn by the target, and target postures. The images were presented to subjects on a computer display, and variation in the apparent target distance was simulated through digital image magnification and by varying the viewing distance to the display. Image resolution was degraded progressively by spatial frequency filtering and we estimated the resolution threshold in each task. These threshold values were compared with the historical Johnson criteria for predicting imaging device performance. Our data are broadly consistent with the previously reported values, though our threat identification decisions required subjects to perceive information with a larger spatial scale than the Johnson criterion for identification of standing human targets. In a second experiment, we employed a four-alternative identification decision and found results that were consistent with those from Experiment 1. We also confirmed that the spatial scale of visual information used for target acquisition is highly task-specific, and provided a novel demonstration of changes in visual information requirements as a function of target range. These findings pose challenges for models of target acquisition with imaging devices.},
  archive      = {J_TAP},
  doi          = {10.1145/3375000},
  journal      = {ACM Transactions on Applied Perception},
  month        = {2},
  number       = {1},
  pages        = {1-20},
  shortjournal = {ACM Trans. Appl. Perc.},
  title        = {Visual information requirements for dismounted soldier target acquisition},
  volume       = {17},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Translational and rotational arrow cues (TRAC) navigation
method for manual alignment tasks. <em>TAP</em>, <em>17</em>(1), 1–19.
(<a href="https://doi.org/10.1145/3375001">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many tasks in image-guided surgery require a clinician to manually position an instrument in space, with respect to a patient, with five or six degrees of freedom (DOF). Displaying the current and desired pose of the object on a 2D display such as a computer monitor is straightforward. However, providing guidance to accurately and rapidly navigate the object in 5-DOF or 6-DOF is challenging. Guidance is typically accomplished by showing distinct orthogonal viewpoints of the workspace, requiring simultaneous alignment in all views. Although such methods are commonly used, they can be quite unintuitive, and it can take a long time to perform an accurate 5-DOF or 6-DOF alignment task. In this article, we describe a method of visually communicating navigation instructions using translational and rotational arrow cues (TRAC) defined in an object-centric frame, while displaying a single principal view that approximates the human’s egocentric view of the physical object. The target pose of the object is provided but typically is used only for the initial gross alignment. During the accurate-alignment stage, the user follows the unambiguous arrow commands. In a series of human-subject studies, we show that the TRAC method outperforms two common orthogonal-view methods—the triplanar display, and a sight-alignment method that closely approximates the Acrobot Navigation System—in terms of time to complete 5-DOF and 6-DOF navigation tasks. We also find that subjects can achieve 1 mm and 1° accuracy using the TRAC method with a median completion time of less than 20 seconds.},
  archive      = {J_TAP},
  doi          = {10.1145/3375001},
  journal      = {ACM Transactions on Applied Perception},
  month        = {2},
  number       = {1},
  pages        = {1-19},
  shortjournal = {ACM Trans. Appl. Perc.},
  title        = {Translational and rotational arrow cues (TRAC) navigation method for manual alignment tasks},
  volume       = {17},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). The impact of the complexity of harmony on the acceptability
of music. <em>TAP</em>, <em>17</em>(1), 1–27. (<a
href="https://doi.org/10.1145/3375014">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we contribute to the longstanding challenge of how to explain the listener’s acceptability for a particular piece of music, using harmony as one of the crucial dimensions in music, one of the least examined in this context. We propose three measures for the complexity of harmony: (i) the complexity based on usage of the basic tonal functions and parallels in the harmonic progression, (ii) the entropies of unigrams and bigrams in the sequence of chords, and (iii) the regularity of the harmonic progression. Additionally, we propose four measures for the acceptability of musical pieces (perceptual variables): difficulty, pleasantness, recognition, and repeatability. These measures have been evaluated in each musical example within our dataset, consisting of 160 carefully selected musical excerpts from different musical styles. The first and the third complexity measures and the musical style of excerpts were determined by the first author using criteria described in the article, while the entropies were computed by computer using Shannon’s formula, after the harmonic progression was determined. The four perceptual variables were obtained by a group of 21 participants, taking their mean values as the final score. A statistical analysis of this dataset shows that all the measures of complexity are consistent and are together with the musical style important features in explaining the musical acceptability. These relations were further elaborated by regression tree analysis for difficulty and pleasantness after unigram entropy was eliminated due to high correlation with bigram entropy. Results offer reasonable interpretations and also illuminate the relative importance of the predictor variables. In particular, the regularity of the harmonic progression is in both cases the most important predictor.},
  archive      = {J_TAP},
  doi          = {10.1145/3375014},
  journal      = {ACM Transactions on Applied Perception},
  month        = {2},
  number       = {1},
  pages        = {1-27},
  shortjournal = {ACM Trans. Appl. Perc.},
  title        = {The impact of the complexity of harmony on the acceptability of music},
  volume       = {17},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). The impact of olfactory and wind stimuli on 360 videos using
head-mounted displays. <em>TAP</em>, <em>17</em>(1), 1–13. (<a
href="https://doi.org/10.1145/3380903">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Consuming 360 audiovisual content using a Head-Mounted Display (HMD) has become a standard feature for Immersive Virtual Reality (IVR). However, most applications rely only on visual and auditory feedback whereas other senses are often disregarded. The main goal of this work was to study the effect of tactile and olfactory stimuli on participants’ sense of presence and cybersickness while watching a 360 video using an HMD-based IVR setup. An experiment with 48 participants and three experimental conditions (360 video, 360 video with olfactory stimulus, and 360 video with tactile stimulus) was performed. Presence and cybersickness were reported via post-test questionnaires. Statistical analysis showed a significant difference in presence between the control and the olfactory conditions. From the control to the tactile condition, mean values were higher but failed to show statistical significance. Thus, results suggest that adding an olfactory stimulus increases presence significantly while the addition of a tactile stimulus only shows a positive effect. Regarding cybersickness, no significant differences were found across conditions. We conclude that an olfactory stimulus contributes to higher presence and that a tactile stimulus, delivered in the form of cutaneous perception of wind, has no influence in presence. We further conclude that multisensory cues do not affect cybersickness.},
  archive      = {J_TAP},
  doi          = {10.1145/3380903},
  journal      = {ACM Transactions on Applied Perception},
  month        = {2},
  number       = {1},
  pages        = {1-13},
  shortjournal = {ACM Trans. Appl. Perc.},
  title        = {The impact of olfactory and wind stimuli on 360 videos using head-mounted displays},
  volume       = {17},
  year         = {2020},
}
</textarea>
</details></li>
</ul>

</body>
</html>
