<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>TOG_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="tog---272">TOG - 272</h2>
<ul>
<li><details>
<summary>
(2020). A safe and fast repulsion method for GPU-based cloth self
collisions. <em>TOG</em>, <em>40</em>(1), 5:1–18. (<a
href="https://doi.org/10.1145/3430025">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cloth dynamics and collision handling are the two most challenging topics in cloth simulation. While researchers have substantially improved the performances of cloth dynamics solvers recently, their success in fast collision detection and handling is rather limited. In this article, we focus our research on the safety, efficiency, and realism of the repulsion-based collision handling approach, which has demonstrated its potential in existing GPU-based simulators. Our first discovery is the necessary vertex distance conditions for cloth to enter self intersections, the negations of which can be viewed as vertex distance constraints continuous in time for sufficiently avoiding self collisions. Continuous constraints, however, cannot be enforced with ease. Our solution is to convert continuous constraints into three types of constraints: discrete edge length constraints, discrete vertex distance constraints, and vertex displacement constraints. Based on this solution, we develop a fast and safe collision handling process for enforcing constraints, a novel splitting method for integrating collision handling with dynamics solvers, and static and adaptive remeshing schemes to further improve the runtime performance. In summary, our cloth simulator is efficient, safe, robust, and parallelizable on a GPU. The experiment shows that it runs at least one order of magnitude faster than existing simulators.},
  archive      = {J_TOG},
  author       = {Longhua Wu and Botao Wu and Yin Yang and Huamin Wang},
  doi          = {10.1145/3430025},
  journal      = {ACM Transactions on Graphics},
  month        = {12},
  number       = {1},
  pages        = {5:1–18},
  shortjournal = {ACM Trans. Graph.},
  title        = {A safe and fast repulsion method for GPU-based cloth self collisions},
  volume       = {40},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Dynamic upsampling of smoke through dictionary-based
learning. <em>TOG</em>, <em>40</em>(1), 4:1–19. (<a
href="https://doi.org/10.1145/3412360">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Simulating turbulent smoke flows with fine details is computationally intensive. For iterative editing or simply faster generation, efficiently upsampling a low-resolution numerical simulation is an attractive alternative. We propose a novel learning approach to the dynamic upsampling of smoke flows based on a training set of flows at coarse and fine resolutions. Our multiscale neural network turns an input coarse animation into a sparse linear combination of small velocity patches present in a precomputed over-complete dictionary. These sparse coefficients are then used to generate a high-resolution smoke animation sequence by blending the fine counterparts of the coarse patches. Our network is initially trained from a sequence of example simulations to both construct the dictionary of corresponding coarse and fine patches and allow for the fast evaluation of a sparse patch encoding of any coarse input. The resulting network provides an accurate upsampling when the coarse input simulation is well approximated by patches present in the training set (e.g., for re-simulation), or simply visually plausible upsampling when input and training sets differ significantly. We show a variety of examples to ascertain the strengths and limitations of our approach and offer comparisons to existing approaches to demonstrate its quality and effectiveness.},
  archive      = {J_TOG},
  author       = {Kai Bai and Wei Li and Mathieu Desbrun and Xiaopei Liu},
  doi          = {10.1145/3412360},
  journal      = {ACM Transactions on Graphics},
  month        = {9},
  number       = {1},
  pages        = {4:1–19},
  shortjournal = {ACM Trans. Graph.},
  title        = {Dynamic upsampling of smoke through dictionary-based learning},
  volume       = {40},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). DeepMag: Source-specific change magnification using gradient
ascent. <em>TOG</em>, <em>40</em>(1), 2:1–14. (<a
href="https://doi.org/10.1145/3408865">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many important physical phenomena involve subtle signals that are difficult to observe with the unaided eye, yet visualizing them can be very informative. Current motion magnification techniques can reveal these small temporal variations in video, but require precise prior knowledge about the target signal, and cannot deal with interference motions at a similar frequency. We present DeepMag, an end-to-end deep neural video-processing framework based on gradient ascent that enables automated magnification of subtle color and motion signals from a specific source, even in the presence of large motions of various velocities. The advantages of DeepMag are highlighted via the task of video-based physiological visualization. Through systematic quantitative and qualitative evaluation of the approach on videos with different levels of head motion, we compare the magnification of pulse and respiration to existing state-of-the-art methods. Our method produces magnified videos with substantially fewer artifacts and blurring whilst magnifying the physiological changes by a similar degree.},
  archive      = {J_TOG},
  author       = {Weixuan Chen and Daniel McDuff},
  doi          = {10.1145/3408865},
  journal      = {ACM Transactions on Graphics},
  month        = {9},
  number       = {1},
  pages        = {2:1–14},
  shortjournal = {ACM Trans. Graph.},
  title        = {DeepMag: Source-specific change magnification using gradient ascent},
  volume       = {40},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). MotioNet: 3D human motion reconstruction from monocular
video with skeleton consistency. <em>TOG</em>, <em>40</em>(1), 1:1–15.
(<a href="https://doi.org/10.1145/3407659">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce MotioNet , a deep neural network that directly reconstructs the motion of a 3D human skeleton from a monocular video. While previous methods rely on either rigging or inverse kinematics (IK) to associate a consistent skeleton with temporally coherent joint rotations, our method is the first data-driven approach that directly outputs a kinematic skeleton, which is a complete, commonly used motion representation. At the crux of our approach lies a deep neural network with embedded kinematic priors, which decomposes sequences of 2D joint positions into two separate attributes: a single, symmetric skeleton encoded by bone lengths, and a sequence of 3D joint rotations associated with global root positions and foot contact labels. These attributes are fed into an integrated forward kinematics (FK) layer that outputs 3D positions, which are compared to a ground truth. In addition, an adversarial loss is applied to the velocities of the recovered rotations to ensure that they lie on the manifold of natural joint rotations. The key advantage of our approach is that it learns to infer natural joint rotations directly from the training data rather than assuming an underlying model, or inferring them from joint positions using a data-agnostic IK solver. We show that enforcing a single consistent skeleton along with temporally coherent joint rotations constrains the solution space, leading to a more robust handling of self-occlusions and depth ambiguities.},
  archive      = {J_TOG},
  author       = {Mingyi Shi and Kfir Aberman and Andreas Aristidou and Taku Komura and Dani Lischinski and Daniel Cohen-Or and Baoquan Chen},
  doi          = {10.1145/3407659},
  journal      = {ACM Transactions on Graphics},
  month        = {9},
  number       = {1},
  pages        = {1:1–15},
  shortjournal = {ACM Trans. Graph.},
  title        = {MotioNet: 3D human motion reconstruction from monocular video with skeleton consistency},
  volume       = {40},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). SIERE: A hybrid semi-implicit exponential integrator for
efficiently simulating stiff deformable objects. <em>TOG</em>,
<em>40</em>(1), 3:1–12. (<a
href="https://doi.org/10.1145/3410527">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Physics-based simulation methods for deformable objects suffer limitations due to the conflicting requirements that are placed on them. The work horse semi-implicit (SI) backward Euler method is very stable and inexpensive, but it is also a blunt instrument. It applies heavy damping, which depends on the timestep, to all solution modes and not just to high-frequency ones. As such, it makes simulations less lively, potentially missing important animation details. At the other end of the scale, exponential methods (exponential Rosenbrock Euler (ERE)) are known to deliver good approximations to all modes, but they get prohibitively expensive and less stable for very stiff material. In this article, we devise a hybrid, semi-implicit method called SIERE that allows the previous methods SI and ERE to each perform what they are good at. To do this, we employ at each timestep a partial spectral decomposition, which picks the lower, leading modes, applying ERE in the corresponding subspace. The rest is handled (i.e., effectively damped out) by SI. No solution of nonlinear algebraic equations is required throughout the algorithm. We show that the resulting method produces simulations that are visually as good as those of the exponential method at a computational price that does not increase with stiffness, while displaying stability and damping with respect to the high-frequency modes. Furthermore, the phenomenon of occasional divergence of SI is avoided.},
  archive      = {J_TOG},
  author       = {Yu Ju (Edwin) Chen and Seung Heon Sheen and Uri M. Ascher and Dinesh K. Pai},
  doi          = {10.1145/3410527},
  journal      = {ACM Transactions on Graphics},
  month        = {8},
  number       = {1},
  pages        = {3:1–12},
  shortjournal = {ACM Trans. Graph.},
  title        = {SIERE: A hybrid semi-implicit exponential integrator for efficiently simulating stiff deformable objects},
  volume       = {40},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Real-time rendering of decorative sound textures for
soundscapes. <em>TOG</em>, <em>39</em>(6), 271:1–12. (<a
href="https://doi.org/10.1145/3414685.3417875">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Audio recordings contain rich information about sound sources and their properties such as the location, loudness, and frequency of events. One prevalent component in sound recordings is the sound texture, which contains a massive number of events. In such a texture, there can be some distinct and repeated sounds that we term as a foreground sound. Birds chirping in the wind is one such decorative sound texture with the chirping as a foreground sound and the wind as a background texture. To render these decorative sound textures in real-time and with high quality, we create two-layer Markov Models to enable smooth transitions from sound grain to sound grain and propose a hierarchical scheme to generate Head-Related Transfer Function filters for localization cues of sounds represented as area/volume sources. Moreover, during the synthesis stage, we provide control over the frequency and intensity of sounds for customization. Lastly, foreground sounds are often blended into background textures such as the sound of rain splats on car surfaces becoming submerged in the background rain. We develop an extraction component that outperforms existing learning-based methods to facilitate our synthesis with perceptible foreground sounds and well-defined textures.},
  archive      = {J_TOG},
  author       = {Jinta Zheng and Shih-Hsuan Hung and Kyle Hiebel and Yue Zhang},
  doi          = {10.1145/3414685.3417875},
  journal      = {ACM Transactions on Graphics},
  month        = {11},
  number       = {6},
  pages        = {271:1–12},
  shortjournal = {ACM Trans. Graph.},
  title        = {Real-time rendering of decorative sound textures for soundscapes},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). QuickETC2: Fast ETC2 texture compression using luma
differences. <em>TOG</em>, <em>39</em>(6), 270:1–10. (<a
href="https://doi.org/10.1145/3414685.3417787">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Compressed textures are indispensable in most 3D graphics applications to reduce memory traffic and increase performance. For higher-quality graphics, the number and size of textures in an application have continuously increased. Additionally, the ETC2 texture format, which is mandatory in OpenGL ES 3.0, OpenGL 4.3, and Android 4.3 (and later versions), requires more complex texture compression than the traditional ETC1 format. As a result, texture compression becomes more and more time-consuming. To accelerate ETC2 compression, we introduce two new compression techniques, named QuickETC2. The first technique is an early compression-mode decision scheme. Instead of testing all ETC1/2 modes to compress a texel block, we select proper modes for each block by exploiting the luma difference of the block to reduce unnecessary compression overhead. The second technique is a fast luma-based T- and H-mode compression method. When clustering each texel into two groups, we replace the 3D RGB space with the 1D luma space and quickly find the two groups that have the minimum luma differences. We also selectively perform the T- or H-mode and reduce its distance candidates, according to the luma differences of each group. We have implemented both techniques with AVX2 intrinsics to exploit SIMD parallelism. According to our experiments, QuickETC2 can compress more than 2000 1K×1K-sized images per second on an octa-core CPU.},
  archive      = {J_TOG},
  author       = {Jae-Ho Nah},
  doi          = {10.1145/3414685.3417787},
  journal      = {ACM Transactions on Graphics},
  month        = {11},
  number       = {6},
  pages        = {270:1–10},
  shortjournal = {ACM Trans. Graph.},
  title        = {QuickETC2: Fast ETC2 texture compression using luma differences},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Optimizing depth perception in virtual and augmented reality
through gaze-contingent stereo rendering. <em>TOG</em>, <em>39</em>(6),
269:1–10. (<a href="https://doi.org/10.1145/3414685.3417820">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Virtual and augmented reality (VR/AR) displays crucially rely on stereoscopic rendering to enable perceptually realistic user experiences. Yet, existing near-eye display systems ignore the gaze-dependent shift of the no-parallax point in the human eye. Here, we introduce a gaze-contingent stereo rendering technique that models this effect and conduct several user studies to validate its effectiveness. Our findings include experimental validation of the location of the no-parallax point, which we then use to demonstrate significant improvements of disparity and shape distortion in a VR setting, and consistent alignment of physical and digitally rendered objects across depths in optical see-through AR. Our work shows that gaze-contingent stereo rendering improves perceptual realism and depth perception of emerging wearable computing systems.},
  archive      = {J_TOG},
  author       = {Brooke Krajancich and Petr Kellnhofer and Gordon Wetzstein},
  doi          = {10.1145/3414685.3417820},
  journal      = {ACM Transactions on Graphics},
  month        = {11},
  number       = {6},
  pages        = {269:1–10},
  shortjournal = {ACM Trans. Graph.},
  title        = {Optimizing depth perception in virtual and augmented reality through gaze-contingent stereo rendering},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Egocentric videoconferencing. <em>TOG</em>, <em>39</em>(6),
268:1–16. (<a href="https://doi.org/10.1145/3414685.3417808">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce a method for egocentric videoconferencing that enables hands-free video calls, for instance by people wearing smart glasses or other mixed-reality devices. Videoconferencing portrays valuable non-verbal communication and face expression cues, but usually requires a front-facing camera. Using a frontal camera in a hands-free setting when a person is on the move is impractical. Even holding a mobile phone camera in the front of the face while sitting for a long duration is not convenient. To overcome these issues, we propose a low-cost wearable egocentric camera setup that can be integrated into smart glasses. Our goal is to mimic a classical video call, and therefore, we transform the egocentric perspective of this camera into a front facing video. To this end, we employ a conditional generative adversarial neural network that learns a transition from the highly distorted egocentric views to frontal views common in videoconferencing. Our approach learns to transfer expression details directly from the egocentric view without using a complex intermediate parametric expressions model, as it is used by related face reenactment methods. We successfully handle subtle expressions, not easily captured by parametric blendshape-based solutions, e.g., tongue movement, eye movements, eye blinking, strong expressions and depth varying movements. To get control over the rigid head movements in the target view, we condition the generator on synthetic renderings of a moving neutral face. This allows us to synthesis results at different head poses. Our technique produces temporally smooth video-realistic renderings in real-time using a video-to-video translation network in conjunction with a temporal discriminator. We demonstrate the improved capabilities of our technique by comparing against related state-of-the art approaches.},
  archive      = {J_TOG},
  author       = {Mohamed Elgharib and Mohit Mendiratta and Justus Thies and Matthias Niessner and Hans-Peter Seidel and Ayush Tewari and Vladislav Golyanik and Christian Theobalt},
  doi          = {10.1145/3414685.3417808},
  journal      = {ACM Transactions on Graphics},
  month        = {11},
  number       = {6},
  pages        = {268:1–16},
  shortjournal = {ACM Trans. Graph.},
  title        = {Egocentric videoconferencing},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Imperceptible manipulation of lateral camera motion for
improved virtual reality applications. <em>TOG</em>, <em>39</em>(6),
267:1–14. (<a href="https://doi.org/10.1145/3414685.3417773">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Virtual Reality (VR) systems increase immersion by reproducing users&#39; movements in the real world. However, several works have shown that this real-to-virtual mapping does not need to be precise in order to convey a realistic experience. Being able to alter this mapping has many potential applications, since achieving an accurate real-to-virtual mapping is not always possible due to limitations in the capture or display hardware, or in the physical space available. In this work, we measure detection thresholds for lateral translation gains of virtual camera motion in response to the corresponding head motion under natural viewing, and in the absence of locomotion, so that virtual camera movement can be either compressed or expanded while these manipulations remain undetected. Finally, we propose three applications for our method, addressing three key problems in VR: improving 6-DoF viewing for captured 360° footage, overcoming physical constraints, and reducing simulator sickness. We have further validated our thresholds and evaluated our applications by means of additional user studies confirming that our manipulations remain imperceptible, and showing that (i) compressing virtual camera motion reduces visible artifacts in 6-DoF, hence improving perceived quality, (ii) virtual expansion allows for completion of virtual tasks within a reduced physical space, and (iii) simulator sickness may be alleviated in simple scenarios when our compression method is applied.},
  archive      = {J_TOG},
  author       = {Ana Serrano and Daniel Martin and Diego Gutierrez and Karol Myszkowski and Belen Masia},
  doi          = {10.1145/3414685.3417773},
  journal      = {ACM Transactions on Graphics},
  month        = {11},
  number       = {6},
  pages        = {267:1–14},
  shortjournal = {ACM Trans. Graph.},
  title        = {Imperceptible manipulation of lateral camera motion for improved virtual reality applications},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). OmniPhotos: Casual 360° VR photography. <em>TOG</em>,
<em>39</em>(6), 266:1–12. (<a
href="https://doi.org/10.1145/3414685.3417770">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Virtual reality headsets are becoming increasingly popular, yet it remains difficult for casual users to capture immersive 360° VR panoramas. State-of-the-art approaches require capture times of usually far more than a minute and are often limited in their supported range of head motion. We introduce OmniPhotos, a novel approach for quickly and casually capturing high-quality 360° panoramas with motion parallax. Our approach requires a single sweep with a consumer 360° video camera as input, which takes less than 3 seconds to capture with a rotating selfie stick or 10 seconds handheld. This is the fastest capture time for any VR photography approach supporting motion parallax by an order of magnitude. We improve the visual rendering quality of our OmniPhotos by alleviating vertical distortion using a novel deformable proxy geometry, which we fit to a sparse 3D reconstruction of captured scenes. In addition, the 360° input views significantly expand the available viewing area, and thus the range of motion, compared to previous approaches. We have captured more than 50 OmniPhotos and show video results for a large variety of scenes. We will make our code available.},
  archive      = {J_TOG},
  author       = {Tobias Bertel and Mingze Yuan and Reuben Lindroos and Christian Richardt},
  doi          = {10.1145/3414685.3417770},
  journal      = {ACM Transactions on Graphics},
  month        = {11},
  number       = {6},
  pages        = {266:1–12},
  shortjournal = {ACM Trans. Graph.},
  title        = {OmniPhotos: Casual 360° VR photography},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Chordal decomposition for spectral coarsening. <em>TOG</em>,
<em>39</em>(6), 265:1–16. (<a
href="https://doi.org/10.1145/3414685.3417789">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce a novel solver to significantly reduce the size of a geometric operator while preserving its spectral properties at the lowest frequencies. We use chordal decomposition to formulate a convex optimization problem which allows the user to control the operator sparsity pattern. This allows for a trade-off between the spectral accuracy of the operator and the cost of its application. We efficiently minimize the energy with a change of variables and achieve state-of-the-art results on spectral coarsening. Our solver further enables novel applications including volume-to-surface approximation and detaching the operator from the mesh, i.e., one can produce a mesh tailor-made for visualization and optimize an operator separately for computation.},
  archive      = {J_TOG},
  author       = {Honglin Chen and Hsueh-TI Derek Liu and Alec Jacobson and David I. W. Levin},
  doi          = {10.1145/3414685.3417789},
  journal      = {ACM Transactions on Graphics},
  month        = {11},
  number       = {6},
  pages        = {265:1–16},
  shortjournal = {ACM Trans. Graph.},
  title        = {Chordal decomposition for spectral coarsening},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). MapTree: Recovering multiple solutions in the space of maps.
<em>TOG</em>, <em>39</em>(6), 264:1–17. (<a
href="https://doi.org/10.1145/3414685.3417800">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper we propose an approach for computing multiple high-quality near-isometric dense correspondences between a pair of 3D shapes. Our method is fully automatic and does not rely on user-provided landmarks or descriptors. This allows us to analyze the full space of maps and extract multiple diverse and accurate solutions, rather than optimizing for a single optimal correspondence as done in most previous approaches. To achieve this, we propose a compact tree structure based on the spectral map representation for encoding and enumerating possible rough initializations, and a novel efficient approach for refining them to dense pointwise maps. This leads to a new method capable of both producing multiple high-quality correspondences across shapes and revealing the symmetry structure of a shape without a priori information. In addition, we demonstrate through extensive experiments that our method is robust and results in more accurate correspondences than state-of-the-art for shape matching and symmetry detection.},
  archive      = {J_TOG},
  author       = {Jing Ren and Simone Melzi and Maks Ovsjanikov and Peter Wonka},
  doi          = {10.1145/3414685.3417800},
  journal      = {ACM Transactions on Graphics},
  month        = {11},
  number       = {6},
  pages        = {264:1–17},
  shortjournal = {ACM Trans. Graph.},
  title        = {MapTree: Recovering multiple solutions in the space of maps},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). MeshWalker: Deep mesh understanding by random walks.
<em>TOG</em>, <em>39</em>(6), 263:1–13. (<a
href="https://doi.org/10.1145/3414685.3417806">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most attempts to represent 3D shapes for deep learning have focused on volumetric grids, multi-view images and point clouds. In this paper we look at the most popular representation of 3D shapes in computer graphics---a triangular mesh---and ask how it can be utilized within deep learning. The few attempts to answer this question propose to adapt convolutions &amp; pooling to suit Convolutional Neural Networks (CNNs). This paper proposes a very different approach, termed MeshWalker to learn the shape directly from a given mesh. The key idea is to represent the mesh by random walks along the surface, which &quot;explore&quot; the mesh&#39;s geometry and topology. Each walk is organized as a list of vertices, which in some manner imposes regularity on the mesh. The walk is fed into a Recurrent Neural Network (RNN) that &quot;remembers&quot; the history of the walk. We show that our approach achieves state-of-the-art results for two fundamental shape analysis tasks: shape classification and semantic segmentation. Furthermore, even a very small number of examples suffices for learning. This is highly important, since large datasets of meshes are difficult to acquire.},
  archive      = {J_TOG},
  author       = {Alon Lahav and Ayellet Tal},
  doi          = {10.1145/3414685.3417806},
  journal      = {ACM Transactions on Graphics},
  month        = {11},
  number       = {6},
  pages        = {263:1–13},
  shortjournal = {ACM Trans. Graph.},
  title        = {MeshWalker: Deep mesh understanding by random walks},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Discovering pattern structure using differentiable
compositing. <em>TOG</em>, <em>39</em>(6), 262:1–15. (<a
href="https://doi.org/10.1145/3414685.3417830">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Patterns, which are collections of elements arranged in regular or near-regular arrangements, are an important graphic art form and widely used due to their elegant simplicity and aesthetic appeal. When a pattern is encoded as a flat image without the underlying structure, manually editing the pattern is tedious and challenging as one has to both preserve the individual element shapes and their original relative arrangements. State-of-the-art deep learning frameworks that operate at the pixel level are unsuitable for manipulating such patterns. Specifically, these methods can easily disturb the shapes of the individual elements or their arrangement, and thus fail to preserve the latent structures of the input patterns. We present a novel differentiable compositing operator using pattern elements and use it to discover structures, in the form of a layered representation of graphical objects, directly from raw pattern images. This operator allows us to adapt current deep learning based image methods to effectively handle patterns. We evaluate our method on a range of patterns and demonstrate superiority in the context of pattern manipulations when compared against state-of-the-art pixel- or point-based alternatives.},
  archive      = {J_TOG},
  author       = {Pradyumna Reddy and Paul Guerrero and Matt Fisher and Wilmot Li and Niloy J. Mitra},
  doi          = {10.1145/3414685.3417830},
  journal      = {ACM Transactions on Graphics},
  month        = {11},
  number       = {6},
  pages        = {262:1–15},
  shortjournal = {ACM Trans. Graph.},
  title        = {Discovering pattern structure using differentiable compositing},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). DeformSyncNet: Deformation transfer via synchronized shape
deformation spaces. <em>TOG</em>, <em>39</em>(6), 261:1–16. (<a
href="https://doi.org/10.1145/3414685.3417783">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Shape deformation is an important component in any geometry processing toolbox. The goal is to enable intuitive deformations of single or multiple shapes or to transfer example deformations to new shapes while preserving the plausibility of the deformed shape(s). Existing approaches assume access to point-level or part-level correspondence or establish them in a preprocessing phase, thus limiting the scope and generality of such approaches. We propose DeformSyncNet, a new approach that allows consistent and synchronized shape deformations without requiring explicit correspondence information. Technically, we achieve this by encoding deformations into a class-specific idealized latent space while decoding them into an individual, model-specific linear deformation action space, operating directly in 3D. The underlying encoding and decoding are performed by specialized (jointly trained) neural networks. By design, the inductive bias of our networks results in a deformation space with several desirable properties, such as path invariance across different deformation pathways, which are then also approximately preserved in real space. We qualitatively and quantitatively evaluate our framework against multiple alternative approaches and demonstrate improved performance.},
  archive      = {J_TOG},
  author       = {Minhyuk Sung and Zhenyu Jiang and Panos Achlioptas and Niloy J. Mitra and Leonidas J. Guibas},
  doi          = {10.1145/3414685.3417783},
  journal      = {ACM Transactions on Graphics},
  month        = {11},
  number       = {6},
  pages        = {261:1–16},
  shortjournal = {ACM Trans. Graph.},
  title        = {DeformSyncNet: Deformation transfer via synchronized shape deformation spaces},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Light stage super-resolution: Continuous high-frequency
relighting. <em>TOG</em>, <em>39</em>(6), 260:1–12. (<a
href="https://doi.org/10.1145/3414685.3417821">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The light stage has been widely used in computer graphics for the past two decades, primarily to enable the relighting of human faces. By capturing the appearance of the human subject under different light sources, one obtains the light transport matrix of that subject, which enables image-based relighting in novel environments. However, due to the finite number of lights in the stage, the light transport matrix only represents a sparse sampling on the entire sphere. As a consequence, relighting the subject with a point light or a directional source that does not coincide exactly with one of the lights in the stage requires interpolation and resampling the images corresponding to nearby lights, and this leads to ghosting shadows, aliased specularities, and other artifacts. To ameliorate these artifacts and produce better results under arbitrary high-frequency lighting, this paper proposes a learning-based solution for the &quot;super-resolution&quot; of scans of human faces taken from a light stage. Given an arbitrary &quot;query&quot; light direction, our method aggregates the captured images corresponding to neighboring lights in the stage, and uses a neural network to synthesize a rendering of the face that appears to be illuminated by a &quot;virtual&quot; light source at the query location. This neural network must circumvent the inherent aliasing and regularity of the light stage data that was used for training, which we accomplish through the use of regularized traditional interpolation methods within our network. Our learned model is able to produce renderings for arbitrary light directions that exhibit realistic shadows and specular highlights, and is able to generalize across a wide variety of subjects. Our super-resolution approach enables more accurate renderings of human subjects under detailed environment maps, or the construction of simpler light stages that contain fewer light sources while still yielding comparable quality renderings as light stages with more densely sampled lights.},
  archive      = {J_TOG},
  author       = {Tiancheng Sun and Zexiang Xu and Xiuming Zhang and Sean Fanello and Christoph Rhemann and Paul Debevec and Yun-Ta Tsai and Jonathan T. Barron and Ravi Ramamoorthi},
  doi          = {10.1145/3414685.3417821},
  journal      = {ACM Transactions on Graphics},
  month        = {11},
  number       = {6},
  pages        = {260:1–12},
  shortjournal = {ACM Trans. Graph.},
  title        = {Light stage super-resolution: Continuous high-frequency relighting},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Deep relightable textures: Volumetric performance capture
with neural rendering. <em>TOG</em>, <em>39</em>(6), 259:1–21. (<a
href="https://doi.org/10.1145/3414685.3417814">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The increasing demand for 3D content in augmented and virtual reality has motivated the development of volumetric performance capture systemsnsuch as the Light Stage. Recent advances are pushing free viewpoint relightable videos of dynamic human performances closer to photorealistic quality. However, despite significant efforts, these sophisticated systems are limited by reconstruction and rendering algorithms which do not fully model complex 3D structures and higher order light transport effects such as global illumination and sub-surface scattering. In this paper, we propose a system that combines traditional geometric pipelines with a neural rendering scheme to generate photorealistic renderings of dynamic performances under desired viewpoint and lighting. Our system leverages deep neural networks that model the classical rendering process to learn implicit features that represent the view-dependent appearance of the subject independent of the geometry layout, allowing for generalization to unseen subject poses and even novel subject identity. Detailed experiments and comparisons demonstrate the efficacy and versatility of our method to generate high-quality results, significantly outperforming the existing state-of-the-art solutions.},
  archive      = {J_TOG},
  author       = {Abhimitra Meka and Rohit Pandey and Christian Häne and Sergio Orts-Escolano and Peter Barnum and Philip David-Son and Daniel Erickson and Yinda Zhang and Jonathan Taylor and Sofien Bouaziz and Chloe Legendre and Wan-Chun Ma and Ryan Overbeck and Thabo Beeler and Paul Debevec and Shahram Izadi and Christian Theobalt and Christoph Rhemann and Sean Fanello},
  doi          = {10.1145/3414685.3417814},
  journal      = {ACM Transactions on Graphics},
  month        = {11},
  number       = {6},
  pages        = {259:1–21},
  shortjournal = {ACM Trans. Graph.},
  title        = {Deep relightable textures: Volumetric performance capture with neural rendering},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Deferred neural lighting: Free-viewpoint relighting from
unstructured photographs. <em>TOG</em>, <em>39</em>(6), 258:1–15. (<a
href="https://doi.org/10.1145/3414685.3417767">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present deferred neural lighting, a novel method for free-viewpoint relighting from unstructured photographs of a scene captured with handheld devices. Our method leverages a scene-dependent neural rendering network for relighting a rough geometric proxy with learnable neural textures. Key to making the rendering network lighting aware are radiance cues: global illumination renderings of a rough proxy geometry of the scene for a small set of basis materials and lit by the target lighting. As such, the light transport through the scene is never explicitely modeled, but resolved at rendering time by a neural rendering network. We demonstrate that the neural textures and neural renderer can be trained end-to-end from unstructured photographs captured with a double hand-held camera setup that concurrently captures the scene while being lit by only one of the cameras&#39; flash lights. In addition, we propose a novel augmentation refinement strategy that exploits the linearity of light transport to extend the relighting capabilities of the neural rendering network to support other lighting types (e.g., environment lighting) beyond the lighting used during acquisition (i.e., flash lighting). We demonstrate our deferred neural lighting solution on a variety of real-world and synthetic scenes exhibiting a wide range of material properties, light transport effects, and geometrical complexity.},
  archive      = {J_TOG},
  author       = {Duan Gao and Guojun Chen and Yue Dong and Pieter Peers and Kun Xu and Xin Tong},
  doi          = {10.1145/3414685.3417767},
  journal      = {ACM Transactions on Graphics},
  month        = {11},
  number       = {6},
  pages        = {258:1–15},
  shortjournal = {ACM Trans. Graph.},
  title        = {Deferred neural lighting: Free-viewpoint relighting from unstructured photographs},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). X-fields: Implicit neural view-, light- and time-image
interpolation. <em>TOG</em>, <em>39</em>(6), 257:1–15. (<a
href="https://doi.org/10.1145/3414685.3417827">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We suggest to represent an X-Field ---a set of 2D images taken across different view, time or illumination conditions, i.e., video, lightfield, reflectance fields or combinations thereof---by learning a neural network (NN) to map their view, time or light coordinates to 2D images. Executing this NN at new coordinates results in joint view, time or light interpolation. The key idea to make this workable is a NN that already knows the &quot;basic tricks&quot; of graphics (lighting, 3D projection, occlusion) in a hard-coded and differentiable form. The NN represents the input to that rendering as an implicit map, that for any view, time, or light coordinate and for any pixel can quantify how it will move if view, time or light coordinates change (Jacobian of pixel position with respect to view, time, illumination, etc.). Our X-Field representation is trained for one scene within minutes, leading to a compact set of trainable parameters and hence real-time navigation in view, time and illumination.},
  archive      = {J_TOG},
  author       = {Mojtaba Bemana and Karol Myszkowski and Hans-Peter Seidel and Tobias Ritschel},
  doi          = {10.1145/3414685.3417827},
  journal      = {ACM Transactions on Graphics},
  month        = {11},
  number       = {6},
  pages        = {257:1–15},
  shortjournal = {ACM Trans. Graph.},
  title        = {X-fields: Implicit neural view-, light- and time-image interpolation},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Layered neural rendering for retiming people in video.
<em>TOG</em>, <em>39</em>(6), 256:1–14. (<a
href="https://doi.org/10.1145/3414685.3417760">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a method for retiming people in an ordinary, natural video --- manipulating and editing the time in which different motions of individuals in the video occur. We can temporally align different motions, change the speed of certain actions (speeding up/slowing down, or entirely &quot;freezing&quot; people), or &quot;erase&quot; selected people from the video altogether. We achieve these effects computationally via a dedicated learning-based layered video representation, where each frame in the video is decomposed into separate RGBA layers, representing the appearance of different people in the video. A key property of our model is that it not only disentangles the direct motions of each person in the input video, but also correlates each person automatically with the scene changes they generate---e.g., shadows, reflections, and motion of loose clothing. The layers can be individually retimed and recombined into a new video, allowing us to achieve realistic, high-quality renderings of retiming effects for real-world videos depicting complex actions and involving multiple individuals, including dancing, trampoline jumping, or group running.},
  archive      = {J_TOG},
  author       = {Erika Lu and Forrester Cole and Tali Dekel and Weidi Xie and Andrew Zisserman and David Salesin and William T. Freeman and Michael Rubinstein},
  doi          = {10.1145/3414685.3417760},
  journal      = {ACM Transactions on Graphics},
  month        = {11},
  number       = {6},
  pages        = {256:1–14},
  shortjournal = {ACM Trans. Graph.},
  title        = {Layered neural rendering for retiming people in video},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Mixed integer ink selection for spectral reproduction.
<em>TOG</em>, <em>39</em>(6), 255:1–16. (<a
href="https://doi.org/10.1145/3414685.3417761">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce a novel ink selection method for spectral printing. The ink selection algorithm takes a spectral image and a set of inks as input, and selects a subset of those inks that results in optimal spectral reproduction. We put forward an optimization formulation that searches a huge combinatorial space based on mixed integer programming. We show that solving this optimization in the conventional reflectance space is intractable. The main insight of this work is to solve our problem in the spectral absorbance space with a linearized formulation. The proposed ink selection copes with large-size problems for which previous methods are hopeless. We demonstrate the effectiveness of our method in a concrete setting by lifelike reproduction of handmade paintings. For a successful spectral reproduction of high-resolution paintings, we explore their spectral absorbance estimation, efficient coreset representation, and accurate data-driven reproduction.},
  archive      = {J_TOG},
  author       = {Navid Ansari and Omid Alizadeh-Mousavi and Hans-Peter Seidel and Vahid Babaei},
  doi          = {10.1145/3414685.3417761},
  journal      = {ACM Transactions on Graphics},
  month        = {11},
  number       = {6},
  pages        = {255:1–16},
  shortjournal = {ACM Trans. Graph.},
  title        = {Mixed integer ink selection for spectral reproduction},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). MaterialGAN: Reflectance capture using a generative SVBRDF
model. <em>TOG</em>, <em>39</em>(6), 254:1–13. (<a
href="https://doi.org/10.1145/3414685.3417779">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We address the problem of reconstructing spatially-varying BRDFs from a small set of image measurements. This is a fundamentally under-constrained problem, and previous work has relied on using various regularization priors or on capturing many images to produce plausible results. In this work, we present MaterialGAN , a deep generative convolutional network based on StyleGAN2, trained to synthesize realistic SVBRDF parameter maps. We show that MaterialGAN can be used as a powerful material prior in an inverse rendering framework: we optimize in its latent representation to generate material maps that match the appearance of the captured images when rendered. We demonstrate this framework on the task of reconstructing SVBRDFs from images captured under flash illumination using a hand-held mobile phone. Our method succeeds in producing plausible material maps that accurately reproduce the target images, and outperforms previous state-of-the-art material capture methods in evaluations on both synthetic and real data. Furthermore, our GAN-based latent space allows for high-level semantic material editing operations such as generating material variations and material morphing.},
  archive      = {J_TOG},
  author       = {Yu Guo and Cameron Smith and Miloš Hašan and Kalyan Sunkavalli and Shuang Zhao},
  doi          = {10.1145/3414685.3417779},
  journal      = {ACM Transactions on Graphics},
  month        = {11},
  number       = {6},
  pages        = {254:1–13},
  shortjournal = {ACM Trans. Graph.},
  title        = {MaterialGAN: Reflectance capture using a generative SVBRDF model},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A general framework for pearlescent materials. <em>TOG</em>,
<em>39</em>(6), 253:1–15. (<a
href="https://doi.org/10.1145/3414685.3417782">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The unique and visually mesmerizing appearance of pearlescent materials has made them an indispensable ingredient in a diverse array of applications including packaging, ceramics, printing, and cosmetics. In contrast to their natural counterparts, such synthetic examples of pearlescence are created by dispersing microscopic interference pigments within a dielectric resin. The resulting space of materials comprises an enormous range of different phenomena ranging from smooth lustrous appearance reminiscent of pearl to highly directional metallic gloss, along with a gradual change in color that depends on the angle of observation and illumination. All of these properties arise due to a complex optical process involving multiple scattering from platelets characterized by wave-optical interference. This article introduces a flexible model for simulating the optics of such pearlescent 3D microstructures. Following a thorough review of the properties of currently used pigments and manufacturing-related effects that influence pearlescence, we propose a new model which expands the range of appearance that can be represented, and closely reproduces the behavior of measured materials, as we show in our comparisons. Using our model, we conduct a systematic study of the parameter space and its relationship to different aspects of pearlescent appearance. We observe that several previously ignored parameters have a substantial impact on the material&#39;s optical behavior, including the multi-layered nature of modern interference pigments, correlations in the orientation of pigment particles, and variability in their properties (e.g. thickness). The utility of a general model for pearlescence extends far beyond computer graphics: inverse and differentiable approaches to rendering are increasingly used to disentangle the physics of scattering from real-world observations. Our approach could inform such reconstructions to enable the predictive design of tailored pearlescent materials.},
  archive      = {J_TOG},
  author       = {Ibón Guillén and Julio Marco and Diego Gutierrez and Wenzel Jakob and Adrian Jarabo},
  doi          = {10.1145/3414685.3417782},
  journal      = {ACM Transactions on Graphics},
  month        = {11},
  number       = {6},
  pages        = {253:1–15},
  shortjournal = {ACM Trans. Graph.},
  title        = {A general framework for pearlescent materials},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A wave optics based fiber scattering model. <em>TOG</em>,
<em>39</em>(6), 252:1–16. (<a
href="https://doi.org/10.1145/3414685.3417841">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing fiber scattering models in rendering are all based on tracing rays through fiber geometry, but for small fibers diffraction and interference are non-negligible, so relying on ray optics can result in appearance errors. This paper presents the first wave optics based fiber scattering model, introducing an azimuthal scattering function that comes from a full wave simulation. Solving Maxwell&#39;s equations for a straight fiber of constant cross section illuminated by a plane wave reduces to solving for a 3D electromagnetic field in a 2D domain, and our fiber scattering simulator solves this 2.5D problem efficiently using the boundary element method (BEM). From the resulting fields we compute extinction, absorption, and far-field scattering distributions, which we use to simulate shadowing and scattering by fibers in a path tracer. We validate our path tracer against the wave simulation and the simulation against a measurement of diffraction from a single textile fiber. Our results show that our approach can reproduce a wide range of fibers with different sizes, cross sections, and material properties, including textile fibers, animal fur, and human hair. The renderings include color effects, softening of sharp features, and strong forward scattering that are not predicted by traditional ray-based models, though the two approaches produce similar appearance for complex fiber assemblies under many conditions.},
  archive      = {J_TOG},
  author       = {Mengqi (Mandy) Xia and Bruce Walter and Eric Michielssen and David Bindel and Steve Marschner},
  doi          = {10.1145/3414685.3417841},
  journal      = {ACM Transactions on Graphics},
  month        = {11},
  number       = {6},
  pages        = {252:1–16},
  shortjournal = {ACM Trans. Graph.},
  title        = {A wave optics based fiber scattering model},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A practical ply-based appearance model of woven fabrics.
<em>TOG</em>, <em>39</em>(6), 251:1–13. (<a
href="https://doi.org/10.1145/3414685.3417777">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Simulating the appearance of woven fabrics is challenging due to the complex interplay of lighting between the constituent yarns and fibers. Conventional surface-based models lack the fidelity and details for producing realistic close-up renderings. Micro-appearance models, on the other hand, can produce highly detailed renderings by depicting fabrics fiber-by-fiber, but become expensive when handling large pieces of clothing. Further, neither surface-based nor micro-appearance model has not been shown in practice to match measurements of complex anisotropic reflection and transmission simultaneously. In this paper, we introduce a practical appearance model for woven fabrics. We model the structure of a fabric at the ply level and simulate the local appearance of fibers making up each ply. Our model accounts for both reflection and transmission of light and is capable of matching physical measurements better than prior methods including fiber based techniques. Compared to existing micro-appearance models, our model is light-weight and scales to large pieces of clothing.},
  archive      = {J_TOG},
  author       = {Zahra Montazeri and Søren B. Gammelmark and Shuang Zhao and Henrik Wann Jensen},
  doi          = {10.1145/3414685.3417777},
  journal      = {ACM Transactions on Graphics},
  month        = {11},
  number       = {6},
  pages        = {251:1–13},
  shortjournal = {ACM Trans. Graph.},
  title        = {A practical ply-based appearance model of woven fabrics},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Fast and robust mesh arrangements using floating-point
arithmetic. <em>TOG</em>, <em>39</em>(6), 250:1–16. (<a
href="https://doi.org/10.1145/3414685.3417818">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce a novel algorithm to transform any generic set of triangles in 3D space into a well-formed simplicial complex. Intersecting elements in the input are correctly identified, subdivided, and connected to arrange a valid configuration, leading to a topologically sound partition of the space into piece-wise linear cells. Our approach does not require the exact coordinates of intersection points to calculate the resulting complex. We represent any intersection point as an unevaluated combination of input vertices. We then extend the recently introduced concept of indirect predicates [Attene 2020] to define all the necessary geometric tests that, by construction, are both exact and efficient since they fully exploit the floating-point hardware. This design makes our method robust and guaranteed correct, while being virtually as fast as non-robust floating-point based implementations. Compared with existing robust methods, our algorithm offers a number of advantages: it is much faster, has a better memory layout, scales well on extremely challenging models, and allows fully exploiting modern multi-core hardware with a parallel implementation. We thoroughly tested our method on thousands of meshes, concluding that it consistently outperforms prior art. We also demonstrate its usefulness in various applications, such as computing efficient mesh booleans, Minkowski sums, and volume meshes.},
  archive      = {J_TOG},
  author       = {Gianmarco Cherchi and Marco Livesu and Riccardo Scateni and Marco Attene},
  doi          = {10.1145/3414685.3417818},
  journal      = {ACM Transactions on Graphics},
  month        = {11},
  number       = {6},
  pages        = {250:1–16},
  shortjournal = {ACM Trans. Graph.},
  title        = {Fast and robust mesh arrangements using floating-point arithmetic},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). You can find geodesic paths in triangle meshes by just
flipping edges. <em>TOG</em>, <em>39</em>(6), 249:1–15. (<a
href="https://doi.org/10.1145/3414685.3417839">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper introduces a new approach to computing geodesics on polyhedral surfaces---the basic idea is to iteratively perform edge flips , in the same spirit as the classic Delaunay flip algorithm. This process also produces a triangulation conforming to the output geodesics, which is immediately useful for tasks in geometry processing and numerical simulation. More precisely, our FlipOut algorithm transforms a given sequence of edges into a locally shortest geodesic while avoiding self-crossings (formally: it finds a geodesic in the same isotopy class ). The algorithm is guaranteed to terminate in a finite number of operations; practical runtimes are on the order of a few milliseconds, even for meshes with millions of triangles. The same approach is easily applied to curves beyond simple paths, including closed loops, curve networks, and multiply-covered curves. We explore how the method facilitates tasks such as straightening cuts and segmentation boundaries, computing geodesic Bézier curves, extending the notion of constrained Delaunay triangulations (CDT) to curved surfaces, and providing accurate boundary conditions for partial differential equations (PDEs). Evaluation on challenging datasets such as Thingi10k indicates that the method is both robust and efficient, even for low-quality triangulations.},
  archive      = {J_TOG},
  author       = {Nicholas Sharp and Keenan Crane},
  doi          = {10.1145/3414685.3417839},
  journal      = {ACM Transactions on Graphics},
  month        = {11},
  number       = {6},
  pages        = {249:1–15},
  shortjournal = {ACM Trans. Graph.},
  title        = {You can find geodesic paths in triangle meshes by just flipping edges},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Conforming weighted delaunay triangulations. <em>TOG</em>,
<em>39</em>(6), 248:1–16. (<a
href="https://doi.org/10.1145/3414685.3417776">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Given a set of points together with a set of simplices we show how to compute weights associated with the points such that the weighted Delaunay triangulation of the point set contains the simplices, if possible. For a given triangulated surface, this process provides a tetrahedral mesh conforming to the triangulation, i.e. solves the problem of meshing the triangulated surface without inserting additional vertices. The restriction to weighted Delaunay triangulations ensures that the orthogonal dual mesh is embedded, facilitating common geometry processing tasks. We show that the existence of a single simplex in a weighted Delaunay triangulation for given vertices amounts to a set of linear inequalities, one for each vertex. This means that the number of inequalities for a given triangle mesh is quadratic in the number of mesh elements, making the naive approach impractical. We devise an algorithm that incrementally selects a small subset of inequalities, repeatedly updating the weights, until the weighted Delaunay triangulation contains all constrained simplices or the problem becomes infeasible. Applying this algorithm to a range of triangle meshes commonly used graphics demonstrates that many of them admit a conforming weighted Delaunay triangulation, in contrast to conforming or constrained Delaunay that require additional vertices to split the input primitives.},
  archive      = {J_TOG},
  author       = {Marc Alexa},
  doi          = {10.1145/3414685.3417776},
  journal      = {ACM Transactions on Graphics},
  month        = {11},
  number       = {6},
  pages        = {248:1–16},
  shortjournal = {ACM Trans. Graph.},
  title        = {Conforming weighted delaunay triangulations},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Bijective projection in a shell. <em>TOG</em>,
<em>39</em>(6), 247:1–18. (<a
href="https://doi.org/10.1145/3414685.3417769">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce an algorithm to convert a self-intersection free, orientable, and manifold triangle mesh T into a generalized prismatic shell equipped with a bijective projection operator to map T to a class of discrete surfaces contained within the shell whose normals satisfy a simple local condition. Properties can be robustly and efficiently transferred between these surfaces using the prismatic layer as a common parametrization domain. The combination of the prismatic shell construction and corresponding projection operator is a robust building block readily usable in many downstream applications, including the solution of PDEs, displacement maps synthesis, Boolean operations, tetrahedral meshing, geometric textures, and nested cages.},
  archive      = {J_TOG},
  author       = {Zhongshi Jiang and Teseo Schneider and Denis Zorin and Daniele Panozzo},
  doi          = {10.1145/3414685.3417769},
  journal      = {ACM Transactions on Graphics},
  month        = {11},
  number       = {6},
  pages        = {247:1–18},
  shortjournal = {ACM Trans. Graph.},
  title        = {Bijective projection in a shell},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Path differential-informed stratified MCMC and adaptive
forward path sampling. <em>TOG</em>, <em>39</em>(6), 246:1–19. (<a
href="https://doi.org/10.1145/3414685.3417856">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Markov Chain Monte Carlo (MCMC) rendering is extensively studied, yet it remains largely unused in practice. We propose solutions to several practicability issues, opening up path space MCMC to become an adaptive sampling framework around established Monte Carlo (MC) techniques. We address non-uniform image quality by deriving an analytic target function for imagespace sample stratification. The function is based on a novel connection between variance and path differentials, allowing analytic variance estimates for MC samples, with potential uses in other adaptive algorithms outside MCMC. We simplify these estimates down to simple expressions using only quantities known in any MC renderer. We also address the issue that most existing MCMC renderers rely on bi-directional path tracing and reciprocal transport, which can be too costly and/or too complex in practice. Instead, we apply our theoretical framework to optimize an adaptive MCMC algorithm that only uses forward path construction. Notably, we construct our algorithm by adapting (with minimal changes) a full-featured path tracer into a single-path state space Markov Chain, bridging another gap between MCMC and existing MC techniques.},
  archive      = {J_TOG},
  author       = {Tobias Zirr and Carsten Dachsbacher},
  doi          = {10.1145/3414685.3417856},
  journal      = {ACM Transactions on Graphics},
  month        = {11},
  number       = {6},
  pages        = {246:1–19},
  shortjournal = {ACM Trans. Graph.},
  title        = {Path differential-informed stratified MCMC and adaptive forward path sampling},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Unbiased warped-area sampling for differentiable rendering.
<em>TOG</em>, <em>39</em>(6), 245:1–18. (<a
href="https://doi.org/10.1145/3414685.3417833">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Differentiable rendering computes derivatives of the light transport equation with respect to arbitrary 3D scene parameters, and enables various applications in inverse rendering and machine learning. We present an unbiased and efficient differentiable rendering algorithm that does not require explicit boundary sampling. We apply the divergence theorem to the derivative of the rendering integral to convert the boundary integral into an area integral. We rewrite the converted area integral to a form that is suitable for Monte Carlo rendering. We then develop an efficient Monte Carlo sampling algorithm for solving the area integral. Our method can be easily plugged into a traditional path tracer and does not require dedicated data structures for sampling boundaries. We analyze the convergence properties through bias-variance metrics, and demonstrate our estimator&#39;s advantages over existing methods for some synthetic inverse rendering examples.},
  archive      = {J_TOG},
  author       = {Sai Praveen Bangaru and Tzu-Mao Li and Frédo Durand},
  doi          = {10.1145/3414685.3417833},
  journal      = {ACM Transactions on Graphics},
  month        = {11},
  number       = {6},
  pages        = {245:1–18},
  shortjournal = {ACM Trans. Graph.},
  title        = {Unbiased warped-area sampling for differentiable rendering},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Screen-space blue-noise diffusion of monte carlo sampling
error via hierarchical ordering of pixels. <em>TOG</em>, <em>39</em>(6),
244:1–15. (<a href="https://doi.org/10.1145/3414685.3417881">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a novel technique for diffusing Monte Carlo sampling error as a blue noise in screen space. We show that automatic diffusion of sampling error can be achieved by ordering the pixels in a way that preserves locality, such as Morton&#39;s Z-ordering, and assigning the samples to the pixels from successive sub-sequences of a single low-discrepancy sequence, thus securing well-distributed samples for each pixel, local neighborhoods, and the whole image. We further show that a blue-noise distribution of the error is attainable by scrambling the Z-ordering to induce isotropy. We present an efficient technique to implement this hierarchical scrambling by defining a context-free grammar that describes infinite self-similar lookup trees. Our concept is scalable to arbitrary image resolutions, sample dimensions, and sample count, and supports progressive and adaptive sampling.},
  archive      = {J_TOG},
  author       = {Abdalla G. M. Ahmed and Peter Wonka},
  doi          = {10.1145/3414685.3417881},
  journal      = {ACM Transactions on Graphics},
  month        = {11},
  number       = {6},
  pages        = {244:1–15},
  shortjournal = {ACM Trans. Graph.},
  title        = {Screen-space blue-noise diffusion of monte carlo sampling error via hierarchical ordering of pixels},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Neural control variates. <em>TOG</em>, <em>39</em>(6),
243:1–19. (<a href="https://doi.org/10.1145/3414685.3417804">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose neural control variates (NCV) for unbiased variance reduction in parametric Monte Carlo integration. So far, the core challenge of applying the method of control variates has been finding a good approximation of the integrand that is cheap to integrate. We show that a set of neural networks can face that challenge: a normalizing flow that approximates the shape of the integrand and another neural network that infers the solution of the integral equation. We also propose to leverage a neural importance sampler to estimate the difference between the original integrand and the learned control variate. To optimize the resulting parametric estimator, we derive a theoretically optimal, variance-minimizing loss function, and propose an alternative, composite loss for stable online training in practice. When applied to light transport simulation, neural control variates are capable of matching the state-of-the-art performance of other unbiased approaches, while providing means to develop more performant, practical solutions. Specifically, we show that the learned light-field approximation is of sufficient quality for high-order bounces, allowing us to omit the error correction and thereby dramatically reduce the noise at the cost of negligible visible bias.},
  archive      = {J_TOG},
  author       = {Thomas Müller and Fabrice Rousselle and Alexander Keller and Jan Novák},
  doi          = {10.1145/3414685.3417804},
  journal      = {ACM Transactions on Graphics},
  month        = {11},
  number       = {6},
  pages        = {243:1–19},
  shortjournal = {ACM Trans. Graph.},
  title        = {Neural control variates},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Deep combiner for independent and correlated pixel
estimates. <em>TOG</em>, <em>39</em>(6), 242:1–12. (<a
href="https://doi.org/10.1145/3414685.3417847">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Monte Carlo integration is an efficient method to solve a high-dimensional integral in light transport simulation, but it typically produces noisy images due to its stochastic nature. Many existing methods, such as image denoising and gradient-domain reconstruction, aim to mitigate this noise by introducing some form of correlation among pixels. While those existing methods reduce noise, they are known to still suffer from method-specific residual noise or systematic errors. We propose a unified framework that reduces such remaining errors. Our framework takes a pair of images, one with independent estimates, and the other with the corresponding correlated estimates. Correlated pixel estimates are generated by various existing methods such as denoising and gradient-domain rendering. Our framework then combines the two images via a novel combination kernel. We model our combination kernel as a weighting function with a deep neural network that exploits the correlation among pixel estimates. To improve the robustness of our framework for outliers, we additionally propose an extension to handle multiple image buffers. The results demonstrate that our unified framework can successfully reduce the error of existing methods while treating them as black-boxes.},
  archive      = {J_TOG},
  author       = {Jonghee Back and Binh-Son Hua and Toshiya Hachisuka and Bochang Moon},
  doi          = {10.1145/3414685.3417847},
  journal      = {ACM Transactions on Graphics},
  month        = {11},
  number       = {6},
  pages        = {242:1–12},
  shortjournal = {ACM Trans. Graph.},
  title        = {Deep combiner for independent and correlated pixel estimates},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Path tracing estimators for refractive radiative transfer.
<em>TOG</em>, <em>39</em>(6), 241:1–15. (<a
href="https://doi.org/10.1145/3414685.3417793">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Rendering radiative transfer through media with a heterogeneous refractive index is challenging because the continuous refractive index variations result in light traveling along curved paths. Existing algorithms are based on photon mapping techniques, and thus are biased and result in strong artifacts. On the other hand, existing unbiased methods such as path tracing and bidirectional path tracing cannot be used in their current form to simulate media with a heterogeneous refractive index. We change this state of affairs by deriving unbiased path tracing estimators for this problem. Starting from the refractive radiative transfer equation (RRTE), we derive a path-integral formulation, which we use to generalize path tracing with next-event estimation and bidirectional path tracing to the heterogeneous refractive index setting. We then develop an optimization approach based on fast analytic derivative computations to produce the point-to-point connections required by these path tracing algorithms. We propose several acceleration techniques to handle complex scenes (surfaces and volumes) that include participating media with heterogeneous refractive fields. We use our algorithms to simulate a variety of scenes combining heterogeneous refraction and scattering, as well as tissue imaging techniques based on ultrasonic virtual waveguides and lenses. Our algorithms and publicly-available implementation can be used to characterize imaging systems such as refractive index microscopy, schlieren imaging, and acousto-optic imaging, and can facilitate the development of inverse rendering techniques for related applications.},
  archive      = {J_TOG},
  author       = {Adithya Pediredla and Yasin Karimi Chalmiani and Matteo Giuseppe Scopelliti and Maysamreza Chamanzar and Srinivasa Narasimhan and Ioannis Gkioulekas},
  doi          = {10.1145/3414685.3417793},
  journal      = {ACM Transactions on Graphics},
  month        = {11},
  number       = {6},
  pages        = {241:1–15},
  shortjournal = {ACM Trans. Graph.},
  title        = {Path tracing estimators for refractive radiative transfer},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). CPPM: Chi-squared progressive photon mapping. <em>TOG</em>,
<em>39</em>(6), 240:1–12. (<a
href="https://doi.org/10.1145/3414685.3417822">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a novel chi-squared progressive photon mapping algorithm (CPPM) that constructs an estimator by controlling the bandwidth to obtain superior image quality. Our estimator has parametric statistical advantages over prior nonparametric methods. First, we show that when a probability density function of the photon distribution is subject to uniform distribution, the radiance estimation is unbiased under certain assumptions. Next, the local photon distribution is evaluated via a chi-squared test to determine whether the photons follow the hypothesized distribution (uniform distribution) or not. If the statistical test deems that the photons inside the bandwidth are uniformly distributed, bandwidth reduction should be suspended. Finally, we present a pipeline with a bandwidth retention and conditional reduction scheme according to the test results. This pipeline not only accumulates sufficient photons for a reliable chi-squared test, but also guarantees that the estimate converges to the correct solution under our assumptions. We evaluate our method on various benchmarks and observe significant improvement in the running time and rendering quality in terms of mean squared error over prior progressive photon mapping methods.},
  archive      = {J_TOG},
  author       = {Zehui Lin and Sheng Li and Xinlu Zeng and Congyi Zhang and Jinzhu Jia and Guoping Wang and Dinesh Manocha},
  doi          = {10.1145/3414685.3417822},
  journal      = {ACM Transactions on Graphics},
  month        = {11},
  number       = {6},
  pages        = {240:1–12},
  shortjournal = {ACM Trans. Graph.},
  title        = {CPPM: Chi-squared progressive photon mapping},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Slope-space integrals for specular next event estimation.
<em>TOG</em>, <em>39</em>(6), 239:1–13. (<a
href="https://doi.org/10.1145/3414685.3417811">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Monte Carlo light transport simulations often lack robustness in scenes containing specular or near-specular materials. Widely used uni- and bidirectional sampling strategies tend to find light paths involving such materials with insufficient probability, producing unusable images that are contaminated by significant variance. This article addresses the problem of sampling a light path connecting two given scene points via a single specular reflection or refraction, extending the range of scenes that can be robustly handled by unbiased path sampling techniques. Our technique enables efficient rendering of challenging transport phenomena caused by such paths, such as underwater caustics or caustics involving glossy metallic objects. We derive analytic expressions that predict the total radiance due to a single reflective or refractive triangle with a microfacet BSDF and we show that this reduces to the well known Lambert boundary integral for irradiance. We subsequently show how this can be leveraged to efficiently sample connections on meshes comprised of vast numbers of triangles. Our derivation builds on the theory of off-center microfacets and involves integrals in the space of surface slopes. Our approach straightforwardly applies to the related problem of rendering glints with high-resolution normal maps describing specular microstructure. Our formulation alleviates problems raised by singularities in filtering integrals and enables a generalization of previous work to perfectly specular materials. We also extend previous work to the case of GGX distributions and introduce new techniques to improve accuracy and performance.},
  archive      = {J_TOG},
  author       = {Guillaume Loubet and Tizian Zeltner and Nicolas Holzschuch and Wenzel Jakob},
  doi          = {10.1145/3414685.3417811},
  journal      = {ACM Transactions on Graphics},
  month        = {11},
  number       = {6},
  pages        = {239:1–13},
  shortjournal = {ACM Trans. Graph.},
  title        = {Slope-space integrals for specular next event estimation},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Path cuts: Efficient rendering of pure specular light
transport. <em>TOG</em>, <em>39</em>(6), 238:1–12. (<a
href="https://doi.org/10.1145/3414685.3417792">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In scenes lit with sharp point-like light sources, light can bounce several times on specular materials before getting into our eyes, forming purely specular light paths. However, to our knowledge, rendering such multi-bounce pure specular paths has not been handled in previous work: while many light transport methods have been devised to sample various kinds of light paths, none of them are able to find multi-bounce pure specular light paths from a point light to a pinhole camera. In this paper, we present path cuts to efficiently render such light paths. We use a path space hierarchy combined with interval arithmetic bounds to prune non-contributing regions of path space, and to slice the path space into regions small enough to empirically contain at most one solution. Next, we use an automatic differentiation tool and a Newton-based solver to find an admissible specular path within a given path space region. We demonstrate results on several complex specular configurations, including RR, TT, TRT and TTTT paths.},
  archive      = {J_TOG},
  author       = {Beibei Wang and Miloš Hašan and Ling-Qi Yan},
  doi          = {10.1145/3414685.3417792},
  journal      = {ACM Transactions on Graphics},
  month        = {11},
  number       = {6},
  pages        = {238:1–12},
  shortjournal = {ACM Trans. Graph.},
  title        = {Path cuts: Efficient rendering of pure specular light transport},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Glossy probe reprojection for interactive global
illumination. <em>TOG</em>, <em>39</em>(6), 237:1–16. (<a
href="https://doi.org/10.1145/3414685.3417823">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent rendering advances dramatically reduce the cost of global illumination. But even with hardware acceleration, complex light paths with multiple glossy interactions are still expensive; our new algorithm stores these paths in precomputed light probes and reprojects them at runtime to provide interactivity. Combined with traditional light maps for diffuse lighting our approach interactively renders all light paths in static scenes with opaque objects. Naively reprojecting probes with glossy lighting is memory-intensive, requires efficient access to the correctly reflected radiance, and exhibits problems at occlusion boundaries in glossy reflections. Our solution addresses all these issues. To minimize memory, we introduce an adaptive light probe parameterization that allocates increased resolution for shinier surfaces and regions of higher geometric complexity. To efficiently sample glossy paths, our novel gathering algorithm reprojects probe texels in a view-dependent manner using efficient reflection estimation and a fast rasterization-based search. Naive probe reprojection often sharpens glossy reflections at occlusion boundaries, due to changes in parallax. To avoid this, we split the convolution induced by the BRDF into two steps: we precompute probes using a lower material roughness and apply an adaptive bilateral filter at runtime to reproduce the original surface roughness. Combining these elements, our algorithm interactively renders complex scenes while fitting in the memory, bandwidth, and computation constraints of current hardware.},
  archive      = {J_TOG},
  author       = {Simon Rodriguez and Thomas Leimkühler and Siddhant Prakash and Chris Wyman and Peter Shirley and George Drettakis},
  doi          = {10.1145/3414685.3417823},
  journal      = {ACM Transactions on Graphics},
  month        = {11},
  number       = {6},
  pages        = {237:1–16},
  shortjournal = {ACM Trans. Graph.},
  title        = {Glossy probe reprojection for interactive global illumination},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). MoGlow: Probabilistic and controllable motion synthesis
using normalising flows. <em>TOG</em>, <em>39</em>(6), 236:1–14. (<a
href="https://doi.org/10.1145/3414685.3417836">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data-driven modelling and synthesis of motion is an active research area with applications that include animation, games, and social robotics. This paper introduces a new class of probabilistic, generative, and controllable motion-data models based on normalising flows. Models of this kind can describe highly complex distributions, yet can be trained efficiently using exact maximum likelihood, unlike GANs or VAEs. Our proposed model is autoregressive and uses LSTMs to enable arbitrarily long time-dependencies. Importantly, is is also causal, meaning that each pose in the output sequence is generated without access to poses or control inputs from future time steps; this absence of algorithmic latency is important for interactive applications with real-time motion control. The approach can in principle be applied to any type of motion since it does not make restrictive, task-specific assumptions regarding the motion or the character morphology. We evaluate the models on motion-capture datasets of human and quadruped locomotion. Objective and subjective results show that randomly-sampled motion from the proposed method outperforms task-agnostic baselines and attains a motion quality close to recorded motion capture.},
  archive      = {J_TOG},
  author       = {Gustav Eje Henter and Simon Alexanderson and Jonas Beskow},
  doi          = {10.1145/3414685.3417836},
  journal      = {ACM Transactions on Graphics},
  month        = {11},
  number       = {6},
  pages        = {236:1–14},
  shortjournal = {ACM Trans. Graph.},
  title        = {MoGlow: Probabilistic and controllable motion synthesis using normalising flows},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). PhysCap: Physically plausible monocular 3D motion capture in
real time. <em>TOG</em>, <em>39</em>(6), 235:1–16. (<a
href="https://doi.org/10.1145/3414685.3417877">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Marker-less 3D human motion capture from a single colour camera has seen significant progress. However, it is a very challenging and severely ill-posed problem. In consequence, even the most accurate state-of-the-art approaches have significant limitations. Purely kinematic formulations on the basis of individual joints or skeletons, and the frequent frame-wise reconstruction in state-of-the-art methods greatly limit 3D accuracy and temporal stability compared to multi-view or marker-based motion capture. Further, captured 3D poses are often physically incorrect and biomechanically implausible, or exhibit implausible environment interactions (floor penetration, foot skating, unnatural body leaning and strong shifting in depth), which is problematic for any use case in computer graphics. We, therefore, present PhysCap , the first algorithm for physically plausible, real-time and marker-less human 3D motion capture with a single colour camera at 25 fps. Our algorithm first captures 3D human poses purely kinematically. To this end, a CNN infers 2D and 3D joint positions, and subsequently, an inverse kinematics step finds space-time coherent joint angles and global 3D pose. Next, these kinematic reconstructions are used as constraints in a real-time physics-based pose optimiser that accounts for environment constraints ( e.g. , collision handling and floor placement), gravity, and biophysical plausibility of human postures. Our approach employs a combination of ground reaction force and residual force for plausible root control, and uses a trained neural network to detect foot contact events in images. Our method captures physically plausible and temporally stable global 3D human motion, without physically implausible postures, floor penetrations or foot skating, from video in real time and in general scenes. PhysCap achieves state-of-the-art accuracy on established pose benchmarks, and we propose new metrics to demonstrate the improved physical plausibility and temporal stability.},
  archive      = {J_TOG},
  author       = {Soshi Shimada and Vladislav Golyanik and Weipeng Xu and Christian Theobalt},
  doi          = {10.1145/3414685.3417877},
  journal      = {ACM Transactions on Graphics},
  month        = {11},
  number       = {6},
  pages        = {235:1–16},
  shortjournal = {ACM Trans. Graph.},
  title        = {PhysCap: Physically plausible monocular 3D motion capture in real time},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). ShapeAssembly: Learning to generate programs for 3D shape
structure synthesis. <em>TOG</em>, <em>39</em>(6), 234:1–20. (<a
href="https://doi.org/10.1145/3414685.3417812">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Manually authoring 3D shapes is difficult and time consuming; generative models of 3D shapes offer compelling alternatives. Procedural representations are one such possibility: they offer high-quality and editable results but are difficult to author and often produce outputs with limited diversity. On the other extreme are deep generative models: given enough data, they can learn to generate any class of shape but their outputs have artifacts and the representation is not editable. In this paper, we take a step towards achieving the best of both worlds for novel 3D shape synthesis. First, we propose ShapeAssembly, a domain-specific &quot;assembly-language&quot; for 3D shape structures. ShapeAssembly programs construct shape structures by declaring cuboid part proxies and attaching them to one another, in a hierarchical and symmetrical fashion. ShapeAssembly functions are parameterized with continuous free variables, so that one program structure is able to capture a family of related shapes. We show how to extract ShapeAssembly programs from existing shape structures in the PartNet dataset. Then, we train a deep generative model, a hierarchical sequence VAE, that learns to write novel ShapeAssembly programs. Our approach leverages the strengths of each representation: the program captures the subset of shape variability that is interpretable and editable, and the deep generative model captures variability and correlations across shape collections that is hard to express procedurally. We evaluate our approach by comparing the shapes output by our generated programs to those from other recent shape structure synthesis models. We find that our generated shapes are more plausible and physically-valid than those of other methods. Additionally, we assess the latent spaces of these models, and find that ours is better structured and produces smoother interpolations. As an application, we use our generative model and differentiable program interpreter to infer and fit shape programs to unstructured geometry, such as point clouds.},
  archive      = {J_TOG},
  author       = {R. Kenny Jones and Theresa Barton and Xianghao Xu and Kai Wang and Ellen Jiang and Paul Guerrero and Niloy J. Mitra and Daniel Ritchie},
  doi          = {10.1145/3414685.3417812},
  journal      = {ACM Transactions on Graphics},
  month        = {11},
  number       = {6},
  pages        = {234:1–20},
  shortjournal = {ACM Trans. Graph.},
  title        = {ShapeAssembly: Learning to generate programs for 3D shape structure synthesis},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Scene mover: Automatic move planning for scene arrangement
by deep reinforcement learning. <em>TOG</em>, <em>39</em>(6), 233:1–15.
(<a href="https://doi.org/10.1145/3414685.3417788">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a novel approach for automatically generating a move plan for scene arrangement. 1 Given a scene like an apartment with many furniture objects, to transform its layout into another layout, one would need to determine a collision-free move plan. It could be challenging to design this plan manually because the furniture objects may block the way of each other if not moved properly; and there is a large complex search space of move action sequences that grow exponentially with the number of objects. To tackle this challenge, we propose a learning-based approach to generate a move plan automatically. At the core of our approach is a Monte Carlo tree that encodes possible states of the layout, based on which a search is performed to move a furniture object appropriately in the current layout. We trained a policy neural network embedded with a LSTM module for estimating the best actions to take in the expansion step and simulation step of the Monte Carlo tree search process. Leveraging the power of deep reinforcement learning, the network learned how to make such estimations through millions of trials of moving objects. We demonstrated our approach for moving objects under different scenarios and constraints. We also evaluated our approach on synthetic and real-world layouts, comparing its performance with that of humans and other baseline approaches.},
  archive      = {J_TOG},
  author       = {Hanqing Wang and Wei Liang and Lap-Fai Yu},
  doi          = {10.1145/3414685.3417788},
  journal      = {ACM Transactions on Graphics},
  month        = {11},
  number       = {6},
  pages        = {233:1–15},
  shortjournal = {ACM Trans. Graph.},
  title        = {Scene mover: Automatic move planning for scene arrangement by deep reinforcement learning},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). TAP-net: Transport-and-pack using reinforcement learning.
<em>TOG</em>, <em>39</em>(6), 232:1–15. (<a
href="https://doi.org/10.1145/3414685.3417796">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce the transport-and-pack (TAP) problem, a frequently encountered instance of real-world packing, and develop a neural optimization solution based on reinforcement learning. Given an initial spatial configuration of boxes, we seek an efficient method to iteratively transport and pack the boxes compactly into a target container. Due to obstruction and accessibility constraints, our problem has to add a new search dimension, i.e., finding an optimal transport sequence , to the already immense search space for packing alone. Using a learning-based approach, a trained network can learn and encode solution patterns to guide the solution of new problem instances instead of executing an expensive online search. In our work, we represent the transport constraints using a precedence graph and train a neural network, coined TAP-Net, using reinforcement learning to reward efficient and stable packing. The network is built on an encoder-decoder architecture, where the encoder employs convolution layers to encode the box geometry and precedence graph and the decoder is a recurrent neural network (RNN) which inputs the current encoder output, as well as the current box packing state of the target container, and outputs the next box to pack, as well as its orientation. We train our network on randomly generated initial box configurations, without supervision , via policy gradients to learn optimal TAP policies to maximize packing efficiency and stability. We demonstrate the performance of TAP-Net on a variety of examples, evaluating the network through ablation studies and comparisons to baselines and alternative network designs. We also show that our network generalizes well to larger problem instances, when trained on small-sized inputs.},
  archive      = {J_TOG},
  author       = {Ruizhen Hu and Juzhan Xu and Bin Chen and Minglun Gong and Hao Zhang and Hui Huang},
  doi          = {10.1145/3414685.3417796},
  journal      = {ACM Transactions on Graphics},
  month        = {11},
  number       = {6},
  pages        = {232:1–15},
  shortjournal = {ACM Trans. Graph.},
  title        = {TAP-net: Transport-and-pack using reinforcement learning},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A reduced-precision network for image reconstruction.
<em>TOG</em>, <em>39</em>(6), 231:1–12. (<a
href="https://doi.org/10.1145/3414685.3417786">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Neural networks are often quantized to use reduced-precision arithmetic, as it greatly improves their storage and computational costs. This approach is commonly used in image classification and natural language processing applications. However, using a quantized network for the reconstruction of HDR images can lead to a significant loss in image quality. In this paper, we introduce QW-Net , a neural network for image reconstruction, in which close to 95\% of the computations can be implemented with 4-bit integers. This is achieved using a combination of two U-shaped networks that are specialized for different tasks, a feature extraction network based on the U-Net architecture, coupled to a filtering network that reconstructs the output image. The feature extraction network has more computational complexity but is more resilient to quantization errors. The filtering network, on the other hand, has significantly fewer computations but requires higher precision. Our network recurrently warps and accumulates previous frames using motion vectors, producing temporally stable results with significantly better quality than TAA, a widely used technique in current games.},
  archive      = {J_TOG},
  author       = {Manu Mathew Thomas and Karthik Vaidyanathan and Gabor Liktor and Angus G. Forbes},
  doi          = {10.1145/3414685.3417786},
  journal      = {ACM Transactions on Graphics},
  month        = {11},
  number       = {6},
  pages        = {231:1–12},
  shortjournal = {ACM Trans. Graph.},
  title        = {A reduced-precision network for image reconstruction},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Learned feature embeddings for non-line-of-sight imaging and
recognition. <em>TOG</em>, <em>39</em>(6), 230:1–18. (<a
href="https://doi.org/10.1145/3414685.3417825">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Objects obscured by occluders are considered lost in the images acquired by conventional camera systems, prohibiting both visualization and understanding of such hidden objects. Non-line-of-sight methods (NLOS) aim at recovering information about hidden scenes, which could help make medical imaging less invasive, improve the safety of autonomous vehicles, and potentially enable capturing unprecedented high-definition RGB-D data sets that include geometry beyond the directly visible parts. Recent NLOS methods have demonstrated scene recovery from time-resolved pulse-illuminated measurements encoding occluded objects as faint indirect reflections. Unfortunately, these systems are fundamentally limited by the quartic intensity fall-off for diffuse scenes. With laser illumination limited by eye-safety limits, recovery algorithms must tackle this challenge by incorporating scene priors. However, existing NLOS reconstruction algorithms do not facilitate learning scene priors. Even if they did, datasets that allow for such supervision do not exist, and successful encoder-decoder networks and generative adversarial networks fail for real-world NLOS data. In this work, we close this gap by learning hidden scene feature representations tailored to both reconstruction and recognition tasks such as classification or object detection, while still relying on physical models at the feature level. We overcome the lack of real training data with a generalizable architecture that can be trained in simulation. We learn the differentiable scene representation jointly with the reconstruction task using a differentiable transient renderer in the objective, and demonstrate that it generalizes to unseen classes and unseen real-world scenes , unlike existing encoder-decoder architectures and generative adversarial networks. The proposed method allows for end-to-end training for different NLOS tasks , such as image reconstruction, classification, and object detection, while being memory-efficient and running at real-time rates. We demonstrate hidden view synthesis, RGB-D reconstruction, classification, and object detection in the hidden scene in an end-to-end fashion.},
  archive      = {J_TOG},
  author       = {Wenzheng Chen and Fangyin Wei and Kiriakos N. Kutulakos and Szymon Rusinkiewicz and Felix Heide},
  doi          = {10.1145/3414685.3417825},
  journal      = {ACM Transactions on Graphics},
  month        = {11},
  number       = {6},
  pages        = {230:1–18},
  shortjournal = {ACM Trans. Graph.},
  title        = {Learned feature embeddings for non-line-of-sight imaging and recognition},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Synthesizing light field from a single image with variable
MPI and two network fusion. <em>TOG</em>, <em>39</em>(6), 229:1–10. (<a
href="https://doi.org/10.1145/3414685.3417785">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a learning-based approach to synthesize a light field with a small baseline from a single image. We synthesize the novel view images by first using a convolutional neural network (CNN) to promote the input image into a layered representation of the scene. We extend the multiplane image (MPI) representation by allowing the disparity of the layers to be inferred from the input image. We show that, compared to the original MPI representation, our representation models the scenes more accurately. Moreover, we propose to handle the visible and occluded regions separately through two parallel networks. The synthesized images using these two networks are then combined through a soft visibility mask to generate the final results. To effectively train the networks, we introduce a large-scale light field dataset of over 2,000 unique scenes containing a wide range of objects. We demonstrate that our approach synthesizes high-quality light fields on a variety of scenes, better than the state-of-the-art methods.},
  archive      = {J_TOG},
  author       = {Qinbo Li and Nima Khademi Kalantari},
  doi          = {10.1145/3414685.3417785},
  journal      = {ACM Transactions on Graphics},
  month        = {11},
  number       = {6},
  pages        = {229:1–10},
  shortjournal = {ACM Trans. Graph.},
  title        = {Synthesizing light field from a single image with variable MPI and two network fusion},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Mononizing binocular videos. <em>TOG</em>, <em>39</em>(6),
228:1–16. (<a href="https://doi.org/10.1145/3414685.3417764">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents the idea of mono-nizing binocular videos and a framework to effectively realize it. Mono-nize means we purposely convert a binocular video into a regular monocular video with the stereo information implicitly encoded in a visual but nearly-imperceptible form. Hence, we can impartially distribute and show the mononized video as an ordinary monocular video. Unlike ordinary monocular videos, we can restore from it the original binocular video and show it on a stereoscopic display. To start, we formulate an encoding-and-decoding framework with the pyramidal deformable fusion module to exploit long-range correspondences between the left and right views, a quantization layer to suppress the restoring artifacts, and the compression noise simulation module to resist the compression noise introduced by modern video codecs. Our framework is self-supervised, as we articulate our objective function with loss terms defined on the input: a monocular term for creating the mononized video, an invertibility term for restoring the original video, and a temporal term for frame-to-frame coherence. Further, we conducted extensive experiments to evaluate our generated mononized videos and restored binocular videos for diverse types of images and 3D movies. Quantitative results on both standard metrics and user perception studies show the effectiveness of our method.},
  archive      = {J_TOG},
  author       = {Wenbo Hu and Menghan Xia and Chi-Wing Fu and Tien-Tsin Wong},
  doi          = {10.1145/3414685.3417764},
  journal      = {ACM Transactions on Graphics},
  month        = {11},
  number       = {6},
  pages        = {228:1–16},
  shortjournal = {ACM Trans. Graph.},
  title        = {Mononizing binocular videos},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). SketchPatch: Sketch stylization via seamless patch-level
synthesis. <em>TOG</em>, <em>39</em>(6), 227:1–14. (<a
href="https://doi.org/10.1145/3414685.3417816">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The paradigm of image-to-image translation is leveraged for the benefit of sketch stylization via transfer of geometric textural details. Lacking the necessary volumes of data for standard training of translation systems, we advocate for operation at the patch level, where a handful of stylized sketches provide ample mining potential for patches featuring basic geometric primitives. Operating at the patch level necessitates special consideration of full sketch translation, as individual translation of patches with no regard to neighbors is likely to produce visible seams and artifacts at patch borders. Aligned pairs of styled and plain primitives are combined to form input hybrids containing styled elements around the border and plain elements within, and given as input to a seamless translation (ST) generator, whose output patches are expected to reconstruct the fully styled patch. An adversarial addition promotes generalization and robustness to diverse geometries at inference time, forming a simple and effective system for arbitrary sketch stylization, as demonstrated upon a variety of styles and sketches.},
  archive      = {J_TOG},
  author       = {Noa Fish and Lilach Perry and Amit Bermano and Daniel Cohen-Or},
  doi          = {10.1145/3414685.3417816},
  journal      = {ACM Transactions on Graphics},
  month        = {11},
  number       = {6},
  pages        = {227:1–14},
  shortjournal = {ACM Trans. Graph.},
  title        = {SketchPatch: Sketch stylization via seamless patch-level synthesis},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Manga filling style conversion with screentone variational
autoencoder. <em>TOG</em>, <em>39</em>(6), 226:1–15. (<a
href="https://doi.org/10.1145/3414685.3417873">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Western color comics and Japanese-style screened manga are two popular comic styles. They mainly differ in the style of region-filling. However, the conversion between the two region-filling styles is very challenging, and manually done currently. In this paper, we identify that the major obstacle in the conversion between the two filling styles stems from the difference between the fundamental properties of screened region-filling and colored region-filling. To resolve this obstacle, we propose a screentone variational autoencoder, ScreenVAE, to map the screened manga to an intermediate domain. This intermediate domain can summarize local texture characteristics and is interpolative. With this domain, we effectively unify the properties of screening and color-filling, and ease the learning for bidirectional translation between screened manga and color comics. To carry out the bidirectional translation, we further propose a network to learn the translation between the intermediate domain and color comics. Our model can generate quality screened manga given a color comic, and generate color comic that retains the original screening intention by the bitonal manga artist. Several results are shown to demonstrate the effectiveness and convenience of the proposed method. We also demonstrate how the intermediate domain can assist other applications such as manga inpainting and photo-to-comic conversion.},
  archive      = {J_TOG},
  author       = {Minshan Xie and Chengze Li and Xueting Liu and Tien-Tsin Wong},
  doi          = {10.1145/3414685.3417873},
  journal      = {ACM Transactions on Graphics},
  month        = {11},
  number       = {6},
  pages        = {226:1–15},
  shortjournal = {ACM Trans. Graph.},
  title        = {Manga filling style conversion with screentone variational autoencoder},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Face identity disentanglement via latent space mapping.
<em>TOG</em>, <em>39</em>(6), 225:1–14. (<a
href="https://doi.org/10.1145/3414685.3417826">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Learning disentangled representations of data is a fundamental problem in artificial intelligence. Specifically, disentangled latent representations allow generative models to control and compose the disentangled factors in the synthesis process. Current methods, however, require extensive supervision and training, or instead, noticeably compromise quality. In this paper, we present a method that learns how to represent data in a disentangled way, with minimal supervision, manifested solely using available pre-trained networks. Our key insight is to decouple the processes of disentanglement and synthesis, by employing a leading pre-trained unconditional image generator, such as StyleGAN. By learning to map into its latent space, we leverage both its state-of-the-art quality, and its rich and expressive latent space, without the burden of training it. We demonstrate our approach on the complex and high dimensional domain of human heads. We evaluate our method qualitatively and quantitatively, and exhibit its success with de-identification operations and with temporal identity coherency in image sequences. Through extensive experimentation, we show that our method successfully disentangles identity from other facial attributes, surpassing existing methods, even though they require more training and supervision.},
  archive      = {J_TOG},
  author       = {Yotam Nitzan and Amit Bermano and Yangyan Li and Daniel Cohen-Or},
  doi          = {10.1145/3414685.3417826},
  journal      = {ACM Transactions on Graphics},
  month        = {11},
  number       = {6},
  pages        = {225:1–14},
  shortjournal = {ACM Trans. Graph.},
  title        = {Face identity disentanglement via latent space mapping},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Neural crossbreed: Neural based image metamorphosis.
<em>TOG</em>, <em>39</em>(6), 224:1–15. (<a
href="https://doi.org/10.1145/3414685.3417797">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose Neural Crossbreed, a feed-forward neural network that can learn a semantic change of input images in a latent space to create the morphing effect. Because the network learns a semantic change, a sequence of meaningful intermediate images can be generated without requiring the user to specify explicit correspondences. In addition, the semantic change learning makes it possible to perform the morphing between the images that contain objects with significantly different poses or camera views. Furthermore, just as in conventional morphing techniques, our morphing network can handle shape and appearance transitions separately by disentangling the content and the style transfer for rich usability. We prepare a training dataset for morphing using a pre-trained BigGAN, which generates an intermediate image by interpolating two latent vectors at an intended morphing value. This is the first attempt to address image morphing using a pre-trained generative model in order to learn semantic transformation. The experiments show that Neural Crossbreed produces high quality morphed images, overcoming various limitations associated with conventional approaches. In addition, Neural Crossbreed can be further extended for diverse applications such as multi-image morphing, appearance transfer, and video frame interpolation.},
  archive      = {J_TOG},
  author       = {Sanghun Park and Kwanggyoon Seo and Junyong Noh},
  doi          = {10.1145/3414685.3417797},
  journal      = {ACM Transactions on Graphics},
  month        = {11},
  number       = {6},
  pages        = {224:1–15},
  shortjournal = {ACM Trans. Graph.},
  title        = {Neural crossbreed: Neural based image metamorphosis},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). PIE: Portrait image embedding for semantic control.
<em>TOG</em>, <em>39</em>(6), 223:1–14. (<a
href="https://doi.org/10.1145/3414685.3417803">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Editing of portrait images is a very popular and important research topic with a large variety of applications. For ease of use, control should be provided via a semantically meaningful parameterization that is akin to computer animation controls. The vast majority of existing techniques do not provide such intuitive and fine-grained control, or only enable coarse editing of a single isolated control parameter. Very recently, high-quality semantically controlled editing has been demonstrated, however only on synthetically created StyleGAN images. We present the first approach for embedding real portrait images in the latent space of StyleGAN, which allows for intuitive editing of the head pose, facial expression, and scene illumination in the image. Semantic editing in parameter space is achieved based on StyleRig, a pretrained neural network that maps the control space of a 3D morphable face model to the latent space of the GAN. We design a novel hierarchical non-linear optimization problem to obtain the embedding. An identity preservation energy term allows spatially coherent edits while maintaining facial integrity. Our approach runs at interactive frame rates and thus allows the user to explore the space of possible edits. We evaluate our approach on a wide set of portrait photos, compare it to the current state of the art, and validate the effectiveness of its components in an ablation study.},
  archive      = {J_TOG},
  author       = {Ayush Tewari and Mohamed Elgharib and Mallikarjun B R and Florian Bernard and Hans-Peter Seidel and Patrick Pérez and Michael Zollhöfer and Christian Theobalt},
  doi          = {10.1145/3414685.3417803},
  journal      = {ACM Transactions on Graphics},
  month        = {11},
  number       = {6},
  pages        = {223:1–14},
  shortjournal = {ACM Trans. Graph.},
  title        = {PIE: Portrait image embedding for semantic control},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Speech gesture generation from the trimodal context of text,
audio, and speaker identity. <em>TOG</em>, <em>39</em>(6), 222:1–16. (<a
href="https://doi.org/10.1145/3414685.3417838">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For human-like agents, including virtual avatars and social robots, making proper gestures while speaking is crucial in human-agent interaction. Co-speech gestures enhance interaction experiences and make the agents look alive. However, it is difficult to generate human-like gestures due to the lack of understanding of how people gesture. Data-driven approaches attempt to learn gesticulation skills from human demonstrations, but the ambiguous and individual nature of gestures hinders learning. In this paper, we present an automatic gesture generation model that uses the multimodal context of speech text, audio, and speaker identity to reliably generate gestures. By incorporating a multimodal context and an adversarial training scheme, the proposed model outputs gestures that are human-like and that match with speech content and rhythm. We also introduce a new quantitative evaluation metric for gesture generation models. Experiments with the introduced metric and subjective human evaluation showed that the proposed gesture generation model is better than existing end-to-end generation models. We further confirm that our model is able to work with synthesized audio in a scenario where contexts are constrained, and show that different gesture styles can be generated for the same speech by specifying different speaker identities in the style embedding space that is learned from videos of various speakers. All the code and data is available at https://github.com/ai4r/Gesture-Generation-from-Trimodal-Context.},
  archive      = {J_TOG},
  author       = {Youngwoo Yoon and Bok Cha and Joo-Haeng Lee and Minsu Jang and Jaeyeon Lee and Jaehong Kim and Geehyuk Lee},
  doi          = {10.1145/3414685.3417838},
  journal      = {ACM Transactions on Graphics},
  month        = {11},
  number       = {6},
  pages        = {222:1–16},
  shortjournal = {ACM Trans. Graph.},
  title        = {Speech gesture generation from the trimodal context of text, audio, and speaker identity},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). MakeltTalk: Speaker-aware talking-head animation.
<em>TOG</em>, <em>39</em>(6), 221:1–15. (<a
href="https://doi.org/10.1145/3414685.3417774">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a method that generates expressive talking-head videos from a single facial image with audio as the only input. In contrast to previous attempts to learn direct mappings from audio to raw pixels for creating talking faces, our method first disentangles the content and speaker information in the input audio signal. The audio content robustly controls the motion of lips and nearby facial regions, while the speaker information determines the specifics of facial expressions and the rest of the talking-head dynamics. Another key component of our method is the prediction of facial landmarks reflecting the speaker-aware dynamics. Based on this intermediate representation, our method works with many portrait images in a single unified framework, including artistic paintings, sketches, 2D cartoon characters, Japanese mangas, and stylized caricatures. In addition, our method generalizes well for faces and characters that were not observed during training. We present extensive quantitative and qualitative evaluation of our method, in addition to user studies, demonstrating generated talking-heads of significantly higher quality compared to prior state-of-the-art methods.},
  archive      = {J_TOG},
  author       = {Yang Zhou and Xintong Han and Eli Shechtman and Jose Echevarria and Evangelos Kalogerakis and Dingzeyu Li},
  doi          = {10.1145/3414685.3417774},
  journal      = {ACM Transactions on Graphics},
  month        = {11},
  number       = {6},
  pages        = {221:1–15},
  shortjournal = {ACM Trans. Graph.},
  title        = {MakeltTalk: Speaker-aware talking-head animation},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Single image portrait relighting via explicit multiple
reflectance channel modeling. <em>TOG</em>, <em>39</em>(6), 220:1–13.
(<a href="https://doi.org/10.1145/3414685.3417824">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Portrait relighting aims to render a face image under different lighting conditions. Existing methods do not explicitly consider some challenging lighting effects such as specular and shadow, and thus may fail in handling extreme lighting conditions. In this paper, we propose a novel framework that explicitly models multiple reflectance channels for single image portrait relighting, including the facial albedo, geometry as well as two lighting effects, i.e. , specular and shadow. These channels are finally composed to generate the relit results via deep neural networks. Current datasets do not support learning such multiple reflectance channel modeling. Therefore, we present a large-scale dataset with the ground-truths of the channels, enabling us to train the deep neural networks in a supervised manner. Furthermore, we develop a novel module named Lighting guided Feature Modulation (LFM). In contrast to existing methods which simply incorporate the given lighting in the bottleneck of a network, LFM fuses the lighting by layer-wise feature modulation to deliver more convincing results. Extensive experiments demonstrate that our proposed method achieves better results and is able to generate challenging lighting effects.},
  archive      = {J_TOG},
  author       = {Zhibo Wang and Xin Yu and Ming Lu and Quan Wang and Chen Qian and Feng Xu},
  doi          = {10.1145/3414685.3417824},
  journal      = {ACM Transactions on Graphics},
  month        = {11},
  number       = {6},
  pages        = {220:1–13},
  shortjournal = {ACM Trans. Graph.},
  title        = {Single image portrait relighting via explicit multiple reflectance channel modeling},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Constraining dense hand surface tracking with elasticity.
<em>TOG</em>, <em>39</em>(6), 219:1–14. (<a
href="https://doi.org/10.1145/3414685.3417768">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many of the actions that we take with our hands involve self-contact and occlusion: shaking hands, making a fist, or interlacing our fingers while thinking. This use of of our hands illustrates the importance of tracking hands through self-contact and occlusion for many applications in computer vision and graphics, but existing methods for tracking hands and faces are not designed to treat the extreme amounts of self-contact and self-occlusion exhibited by common hand gestures. By extending recent advances in vision-based tracking and physically based animation, we present the first algorithm capable of tracking high-fidelity hand deformations through highly self-contacting and self-occluding hand gestures, for both single hands and two hands. By constraining a vision-based tracking algorithm with a physically based deformable model, we obtain an algorithm that is robust to the ubiquitous self-interactions and massive self-occlusions exhibited by common hand gestures, allowing us to track two hand interactions and some of the most difficult possible configurations of a human hand.},
  archive      = {J_TOG},
  author       = {Breannan Smith and Chenglei Wu and He Wen and Patrick Peluse and Yaser Sheikh and Jessica K. Hodgins and Takaaki Shiratori},
  doi          = {10.1145/3414685.3417768},
  journal      = {ACM Transactions on Graphics},
  month        = {11},
  number       = {6},
  pages        = {219:1–14},
  shortjournal = {ACM Trans. Graph.},
  title        = {Constraining dense hand surface tracking with elasticity},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). RGB2Hands: Real-time tracking of 3D hand interactions from
monocular RGB video. <em>TOG</em>, <em>39</em>(6), 218:1–16. (<a
href="https://doi.org/10.1145/3414685.3417852">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Tracking and reconstructing the 3D pose and geometry of two hands in interaction is a challenging problem that has a high relevance for several human-computer interaction applications, including AR/VR, robotics, or sign language recognition. Existing works are either limited to simpler tracking settings ( e.g. , considering only a single hand or two spatially separated hands), or rely on less ubiquitous sensors, such as depth cameras. In contrast, in this work we present the first real-time method for motion capture of skeletal pose and 3D surface geometry of hands from a single RGB camera that explicitly considers close interactions. In order to address the inherent depth ambiguities in RGB data, we propose a novel multi-task CNN that regresses multiple complementary pieces of information, including segmentation, dense matchings to a 3D hand model, and 2D keypoint positions, together with newly proposed intra-hand relative depth and inter-hand distance maps. These predictions are subsequently used in a generative model fitting framework in order to estimate pose and shape parameters of a 3D hand model for both hands. We experimentally verify the individual components of our RGB two-hand tracking and 3D reconstruction pipeline through an extensive ablation study. Moreover, we demonstrate that our approach offers previously unseen two-hand tracking performance from RGB, and quantitatively and qualitatively outperforms existing RGB-based methods that were not explicitly designed for two-hand interactions. Moreover, our method even performs on-par with depth-based real-time methods.},
  archive      = {J_TOG},
  author       = {Jiayi Wang and Franziska Mueller and Florian Bernard and Suzanne Sorli and Oleksandr Sotnychenko and Neng Qian and Miguel A. Otaduy and Dan Casas and Christian Theobalt},
  doi          = {10.1145/3414685.3417852},
  journal      = {ACM Transactions on Graphics},
  month        = {11},
  number       = {6},
  pages        = {218:1–16},
  shortjournal = {ACM Trans. Graph.},
  title        = {RGB2Hands: Real-time tracking of 3D hand interactions from monocular RGB video},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Data-driven authoring of large-scale ecosystems.
<em>TOG</em>, <em>39</em>(6), 217:1–14. (<a
href="https://doi.org/10.1145/3414685.3417848">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In computer graphics populating a large-scale natural scene with plants in a fashion that both reflects the complex interrelationships and diversity present in real ecosystems and is computationally efficient enough to support iterative authoring remains an open problem. Ecosystem simulations embody many of the botanical influences, such as sunlight, temperature, and moisture, but require hours to complete, while synthesis from statistical distributions tends not to capture fine-scale variety and complexity. Instead, we leverage real-world data and machine learning to derive a canopy height model (CHM) for unseen terrain provided by the user. Trees in the canopy layer are then fitted to the resulting CHM through a constrained iterative process that optimizes for a given distribution of species, and, finally, an understorey layer is synthesised using distributions derived from biome-specific undergrowth simulations. Such a hybrid data-driven approach has the advantage that it incorporates subtle biotic, abiotic, and disturbance factors implicitly encoded in the source data and evidences accepted biological behaviour, such as self-thinning, climatic adaptation, and gap dynamics.},
  archive      = {J_TOG},
  author       = {Konrad Kapp and James Gain and Eric Guérin and Eric Galin and Adrien Peytavie},
  doi          = {10.1145/3414685.3417848},
  journal      = {ACM Transactions on Graphics},
  month        = {11},
  number       = {6},
  pages        = {217:1–14},
  shortjournal = {ACM Trans. Graph.},
  title        = {Data-driven authoring of large-scale ecosystems},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). IOrthoPredictor: Model-guided deep prediction of teeth
alignment. <em>TOG</em>, <em>39</em>(6), 216:1–15. (<a
href="https://doi.org/10.1145/3414685.3417771">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we present iOrthoPredictor, a novel system to visually predict teeth alignment in photographs. Our system takes a frontal face image of a patient with visible malpositioned teeth along with a corresponding 3D teeth model as input, and generates a facial image with aligned teeth, simulating a real orthodontic treatment effect. The key enabler of our method is an effective disentanglement of an explicit representation of the teeth geometry from the in-mouth appearance, where the accuracy of teeth geometry transformation is ensured by the 3D teeth model while the in-mouth appearance is modeled as a latent variable. The disentanglement enables us to achieve fine-scale geometry control over the alignment while retaining the original teeth appearance attributes and lighting conditions. The whole pipeline consists of three deep neural networks: a U-Net architecture to explicitly extract the 2D teeth silhouette maps representing the teeth geometry in the input photo, a novel multilayer perceptron (MLP) based network to predict the aligned 3D teeth model, and an encoder-decoder based generative model to synthesize the in-mouth appearance conditional on the original teeth appearance and the aligned teeth geometry. Extensive experimental results and a user study demonstrate that iOrthoPredictor is effective in qualitatively predicting teeth alignment, and applicable to the orthodontic industry.},
  archive      = {J_TOG},
  author       = {Lingchen Yang and Zefeng Shi and Yiqian Wu and Xiang Li and Kun Zhou and Hongbo Fu and Youyi Zheng},
  doi          = {10.1145/3414685.3417771},
  journal      = {ACM Transactions on Graphics},
  month        = {11},
  number       = {6},
  pages        = {216:1–15},
  shortjournal = {ACM Trans. Graph.},
  title        = {IOrthoPredictor: Model-guided deep prediction of teeth alignment},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Dynamic facial asset and rig generation from a single scan.
<em>TOG</em>, <em>39</em>(6), 215:1–18. (<a
href="https://doi.org/10.1145/3414685.3417817">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The creation of high-fidelity computer-generated (CG) characters for films and games is tied with intensive manual labor, which involves the creation of comprehensive facial assets that are often captured using complex hardware. To simplify and accelerate this digitization process, we propose a framework for the automatic generation of high-quality dynamic facial models, including rigs which can be readily deployed for artists to polish. Our framework takes a single scan as input to generate a set of personalized blendshapes, dynamic textures, as well as secondary facial components ( e.g. , teeth and eyeballs). Based on a facial database with over 4, 000 scans with pore-level details, varying expressions and identities, we adopt a self-supervised neural network to learn personalized blendshapes from a set of template expressions. We also model the joint distribution between identities and expressions, enabling the inference of a full set of personalized blendshapes with dynamic appearances from a single neutral input scan. Our generated personalized face rig assets are seamlessly compatible with professional production pipelines for facial animation and rendering. We demonstrate a highly robust and effective framework on a wide range of subjects, and showcase high-fidelity facial animations with automatically generated personalized dynamic textures.},
  archive      = {J_TOG},
  author       = {Jiaman Li and Zhengfei Kuang and Yajie Zhao and Mingming He and Karl Bladin and Hao Li},
  doi          = {10.1145/3414685.3417817},
  journal      = {ACM Transactions on Graphics},
  month        = {11},
  number       = {6},
  pages        = {215:1–18},
  shortjournal = {ACM Trans. Graph.},
  title        = {Dynamic facial asset and rig generation from a single scan},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Monster mash: A single-view approach to casual 3D modeling
and animation. <em>TOG</em>, <em>39</em>(6), 214:1–12. (<a
href="https://doi.org/10.1145/3414685.3417805">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a new framework for sketch-based modeling and animation of 3D organic shapes that can work entirely in an intuitive 2D domain, enabling a playful, casual experience. Unlike previous sketch-based tools, our approach does not require a tedious part-based multi-view workflow with the explicit specification of an animation rig. Instead, we combine 3D inflation with a novel rigidity-preserving, layered deformation model, ARAP-L, to produce a smooth 3D mesh that is immediately ready for animation. Moreover, the resulting model can be animated from a single viewpoint --- and without the need to handle unwanted inter-penetrations, as required by previous approaches. We demonstrate the benefit of our approach on a variety of examples produced by inexperienced users as well as professional animators. For less experienced users, our single-view approach offers a simpler modeling and animating experience than working in a 3D environment, while for professionals, it offers a quick and casual workspace for ideation.},
  archive      = {J_TOG},
  author       = {Marek Dvorožňák and Daniel Sýkora and Cassidy Curtis and Brian Curless and Olga Sorkine-Hornung and David Salesin},
  doi          = {10.1145/3414685.3417805},
  journal      = {ACM Transactions on Graphics},
  month        = {11},
  number       = {6},
  pages        = {214:1–12},
  shortjournal = {ACM Trans. Graph.},
  title        = {Monster mash: A single-view approach to casual 3D modeling and animation},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). SymmetryNet: Learning to predict reflectional and rotational
symmetries of 3D shapes from single-view RGB-d images. <em>TOG</em>,
<em>39</em>(6), 213:1–14. (<a
href="https://doi.org/10.1145/3414685.3417775">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study the problem of symmetry detection of 3D shapes from single-view RGB-D images, where severely missing data renders geometric detection approach infeasible. We propose an end-to-end deep neural network which is able to predict both reflectional and rotational symmetries of 3D objects present in the input RGB-D image. Directly training a deep model for symmetry prediction, however, can quickly run into the issue of overfitting. We adopt a multi-task learning approach. Aside from symmetry axis prediction, our network is also trained to predict symmetry correspondences. In particular, given the 3D points present in the RGB-D image, our network outputs for each 3D point its symmetric counterpart corresponding to a specific predicted symmetry. In addition, our network is able to detect for a given shape multiple symmetries of different types. We also contribute a benchmark of 3D symmetry detection based on single-view RGB-D images. Extensive evaluation on the benchmark demonstrates the strong generalization ability of our method, in terms of high accuracy of both symmetry axis prediction and counterpart estimation. In particular, our method is robust in handling unseen object instances with large variation in shape, multi-symmetry composition, as well as novel object categories.},
  archive      = {J_TOG},
  author       = {Yifei Shi and Junwen Huang and Hongjia Zhang and Xin Xu and Szymon Rusinkiewicz and Kai Xu},
  doi          = {10.1145/3414685.3417775},
  journal      = {ACM Transactions on Graphics},
  month        = {11},
  number       = {6},
  pages        = {213:1–14},
  shortjournal = {ACM Trans. Graph.},
  title        = {SymmetryNet: Learning to predict reflectional and rotational symmetries of 3D shapes from single-view RGB-D images},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Appearance-preserving tactile optimization. <em>TOG</em>,
<em>39</em>(6), 212:1–16. (<a
href="https://doi.org/10.1145/3414685.3417857">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Textures are encountered often on various common objects and surfaces. Many textures combine visual and tactile aspects, each serving important purposes; most obviously, a texture alters the object&#39;s appearance or tactile feeling as well as serving for visual or tactile identification and improving usability. The tactile feel and visual appearance of objects are often linked, but they may interact in unpredictable ways. Advances in high-resolution 3D printing enable highly flexible control of geometry to permit manipulation of both visual appearance and tactile properties. In this paper, we propose an optimization method to independently control the tactile properties and visual appearance of a texture. Our optimization is enabled by neural network-based models, and allows the creation of textures with a desired tactile feeling while preserving a desired visual appearance at a relatively low computational cost, for use in a variety of applications.},
  archive      = {J_TOG},
  author       = {Chelsea Tymms and Siqi Wang and Denis Zorin},
  doi          = {10.1145/3414685.3417857},
  journal      = {ACM Transactions on Graphics},
  month        = {11},
  number       = {6},
  pages        = {212:1–16},
  shortjournal = {ACM Trans. Graph.},
  title        = {Appearance-preserving tactile optimization},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Freely orientable microstructures for designing deformable
3D prints. <em>TOG</em>, <em>39</em>(6), 211:1–16. (<a
href="https://doi.org/10.1145/3414685.3417790">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nature offers a marvel of astonishing and rich deformation behaviors. Yet, most of the objects we fabricate are comparatively rather inexpressive, either rigid or exhibiting simple homogeneous deformations when interacted with. We explore the synthesis and fabrication of novel microstructures that mimic the effects of having oriented rigid fibers in an otherwise flexible material: the result is extremely rigid along a transverse direction while being comparatively very flexible in the locally orthogonal plane. By allowing free gradation of the rigidity direction orientation within the object, the microstructures can be designed such that, under deformation, distances along fibers in the volume are preserved while others freely change. Through a simple painting tool, this allows a designer to influence the way the volume reshapes when deformed, and results in a wide range of novel possibilities. Many gradations are possible: local free orientation of the fibers; local control of the overall material rigidity (structure density); local canceling of the effect of the fibers, obtaining a more isotropic material. Our algorithm to synthesize the structures builds upon procedural texturing. It produces a cellular geometry that can be fabricated reliably despite 3D printing walls at a minimal thickness, allowing prints to be very flexible. The synthesis algorithm is efficient and scales to large volumes.},
  archive      = {J_TOG},
  author       = {Thibault Tricard and Vincent Tavernier and Cédric Zanni and Jonàs Martínez and Pierre-Alexandre Hugron and Fabrice Neyret and Sylvain Lefebvre},
  doi          = {10.1145/3414685.3417790},
  journal      = {ACM Transactions on Graphics},
  month        = {11},
  number       = {6},
  pages        = {211:1–16},
  shortjournal = {ACM Trans. Graph.},
  title        = {Freely orientable microstructures for designing deformable 3D prints},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Weavecraft: An interactive design and simulation tool for 3D
weaving. <em>TOG</em>, <em>39</em>(6), 210:1–16. (<a
href="https://doi.org/10.1145/3414685.3417865">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {3D weaving is an emerging technology for manufacturing multilayer woven textiles. In this work, we present Weavecraft: an interactive, simulation-based design tool for 3D weaving. Unlike existing textile software that uses 2D representations for design patterns, we propose a novel weave block representation that helps the user to understand 3D woven structures and to create complex multi-layered patterns. With Weavecraft, users can create blocks either from scratch or by loading traditional weaves, compose the blocks into large structures, and edit the pattern at various scales. Furthermore, users can verify the design with a physically based simulator, which predicts and visualizes the geometric structure of the woven material and reveals potential defects at an interactive rate. We demonstrate a range of results created with our tool, from simple two-layer cloth and well known 3D structures to a more sophisticated design of a 3D woven shoe, and we evaluate the effectiveness of our system via a formative user study.},
  archive      = {J_TOG},
  author       = {Rundong Wu and Joy Xiaoji Zhang and Jonathan Leaf and Xinru Hua and Ante Qu and Claire Harvey and Emily Holtzman and Joy Ko and Brooks Hagan and Doug James and François Guimbretière and Steve Marschner},
  doi          = {10.1145/3414685.3417865},
  journal      = {ACM Transactions on Graphics},
  month        = {11},
  number       = {6},
  pages        = {210:1–16},
  shortjournal = {ACM Trans. Graph.},
  title        = {Weavecraft: An interactive design and simulation tool for 3D weaving},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Freeform quad-based kirigami. <em>TOG</em>, <em>39</em>(6),
209:1–11. (<a href="https://doi.org/10.1145/3414685.3417844">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Kirigami, the traditional Japanese art of paper cutting and folding generalizes origami and has initiated new research in material science as well as graphics. In this paper we use its capabilities to perform geometric modeling with corrugated surface representations possessing an isometric unfolding into a planar domain after appropriate cuts are made. We initialize our box-based kirigami structures from orthogonal networks of curves, compute a first approximation of their unfolding via mappings between meshes, and complete the process by global optimization. Besides the modeling capabilities we also study the interesting geometry of special kirigami structures from the theoretical side. This experimental paper strives to relate unfoldable checkerboard arrangements of boxes to principal meshes, to the transformation theory of discrete differential geometry, and to a version of the Gauss theorema egregium.},
  archive      = {J_TOG},
  author       = {Caigui Jiang and Florian Rist and Helmut Pottmann and Johannes Wallner},
  doi          = {10.1145/3414685.3417844},
  journal      = {ACM Transactions on Graphics},
  month        = {11},
  number       = {6},
  pages        = {209:1–11},
  shortjournal = {ACM Trans. Graph.},
  title        = {Freeform quad-based kirigami},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Computational design of cold bent glass façades.
<em>TOG</em>, <em>39</em>(6), 208:1–16. (<a
href="https://doi.org/10.1145/3414685.3417843">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cold bent glass is a promising and cost-efficient method for realizing doubly curved glass façades. They are produced by attaching planar glass sheets to curved frames and must keep the occurring stress within safe limits. However, it is very challenging to navigate the design space of cold bent glass panels because of the fragility of the material, which impedes the form finding for practically feasible and aesthetically pleasing cold bent glass façades. We propose an interactive, data-driven approach for designing cold bent glass façades that can be seamlessly integrated into a typical architectural design pipeline. Our method allows non-expert users to interactively edit a parametric surface while providing real-time feedback on the deformed shape and maximum stress of cold bent glass panels. The designs are automatically refined to minimize several fairness criteria, while maximal stresses are kept within glass limits. We achieve interactive frame rates by using a differentiable Mixture Density Network trained from more than a million simulations. Given a curved boundary, our regression model is capable of handling multistable configurations and accurately predicting the equilibrium shape of the panel and its corresponding maximal stress. We show that the predictions are highly accurate and validate our results with a physical realization of a cold bent glass surface.},
  archive      = {J_TOG},
  author       = {Konstantinos Gavriil and Ruslan Guseinov and Jesús Pérez and Davide Pellis and Paul Henderson and Florian Rist and Helmut Pottmann and Bernd Bickel},
  doi          = {10.1145/3414685.3417843},
  journal      = {ACM Transactions on Graphics},
  month        = {11},
  number       = {6},
  pages        = {208:1–16},
  shortjournal = {ACM Trans. Graph.},
  title        = {Computational design of cold bent glass façades},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Neural light field 3D printing. <em>TOG</em>,
<em>39</em>(6), 207:1–12. (<a
href="https://doi.org/10.1145/3414685.3417879">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Modern 3D printers are capable of printing large-size light-field displays at high-resolutions. However, optimizing such displays in full 3D volume for a given light-field imagery is still a challenging task. Existing light field displays optimize over relatively small resolutions using a few co-planar layers in a 2.5D fashion to keep the problem tractable. In this paper, we propose a novel end-to-end optimization approach that encodes input light field imagery as a continuous-space implicit representation in a neural network. This allows fabricating high-resolution, attenuation-based volumetric displays that exhibit the target light fields. In addition, we incorporate the physical constraints of the material to the optimization such that the result can be printed in practice. Our simulation experiments demonstrate that our approach brings significant visual quality improvement compared to the multilayer and uniform grid-based approaches. We validate our simulations with fabricated prototypes and demonstrate that our pipeline is flexible enough to allow fabrications of both planar and non-planar displays.},
  archive      = {J_TOG},
  author       = {Quan Zheng and Vahid Babaei and Gordon Wetzstein and Hans-Peter Seidel and Matthias Zwicker and Gurprit Singh},
  doi          = {10.1145/3414685.3417879},
  journal      = {ACM Transactions on Graphics},
  month        = {11},
  number       = {6},
  pages        = {207:1–12},
  shortjournal = {ACM Trans. Graph.},
  title        = {Neural light field 3D printing},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Towards spatially varying gloss reproduction for 3D
printing. <em>TOG</em>, <em>39</em>(6), 206:1–13. (<a
href="https://doi.org/10.1145/3414685.3417850">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {3D printing technology is a powerful tool for manufacturing complex shapes with high-quality textures. Gloss, next to color and shape, is one of the most salient visual aspects of an object. Unfortunately, printing a wide range of spatially-varying gloss properties using state-of-the-art 3D printers is challenging as it relies on geometrical modifications to achieve the desired appearance. A common post-processing step is to apply off-the-shelf varnishes that modify the final gloss. The main difficulty in automating this process lies in the physical properties of the varnishes which owe their appearance to a high concentration of large particles and as such, they cannot be easily deposited with current 3D color printers. As a result, fine-grained control of gloss properties using today&#39;s 3D printing technologies is limited in terms of both spatial resolution and the range of achievable gloss. We address the above limitations and propose new printing hardware based on piezo-actuated needle valves capable of jetting highly viscous varnishes. Based on the new hardware setup, we present the complete pipeline for controlling the gloss of a given 2.5 D object, from printer calibration, through material selection, to the manufacturing of models with spatially-varying reflectance. Furthermore, we discuss the potential integration with current 3D printing technology. Apart from being a viable solution for 3D printing, our method offers an additional and essential benefit of separating color and gloss fabrication which makes the process more flexible and enables high-quality color and gloss reproduction.},
  archive      = {J_TOG},
  author       = {Michal Piovarči and Michael Foshey and Vahid Babaei and Szymon Rusinkiewicz and Wojciech Matusik and Piotr Didyk},
  doi          = {10.1145/3414685.3417850},
  journal      = {ACM Transactions on Graphics},
  month        = {11},
  number       = {6},
  pages        = {206:1–13},
  shortjournal = {ACM Trans. Graph.},
  title        = {Towards spatially varying gloss reproduction for 3D printing},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). DHFSlicer: Double height-field slicing for milling
fixed-height materials. <em>TOG</em>, <em>39</em>(6), 205:1–17. (<a
href="https://doi.org/10.1145/3414685.3417810">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {3-axis milling enables cheap and precise fabrication of target objects from precut slabs of materials such as wood or stone. However, the space of directly millable shapes is limited since a 3-axis mill can only carve a height-field (HF) surface during each milling and their size is bounded by the slab dimensions, one of which, the height , is typically significantly smaller than the other two for many typical materials. Extending 3-axis milling of precut slabs to general arbitrarily-sized shapes requires decomposing them into bounded-height 3-axis millable parts, or slices , which can be individually milled and then assembled to form the target object. We present DHFSlicer , a novel decomposition method that satisfies the above constraints and significantly reduces both milling time and material waste compared to alternative approaches. We satisfy the fabrication constraints by partitioning target objects into double height-field (DHF) slices, which can be fabricated using two milling passes: the HF surface accessible from one side is milled first, the slice is then flipped using appropriate fixtures, and then the second, remaining, HF surface is milled. DHFSlicer uses an efficient coarse-to-fine decomposition process: It first partitions the inputs into maximally coarse blocks that satisfy a local DHF criterion with respect to per-block milling axes, and then cuts each block into well-sized DHF slices. It minimizes milling time and material waste by keeping the slice count small, and maximizing slice height. We validate our method by embedding it within an end-to-end DHF milling pipeline and fabricating objects from slabs of foam, wood, and MDF; demonstrate that using the obtained slices reduces milling time and material waste by 42\% on average compared to existing automatic alternatives; and highlight the benefits of DHFSlicer via extensive ablation studies.},
  archive      = {J_TOG},
  author       = {Jinfan Yang and Chrystiano Araujo and Nicholas Vining and Zachary Ferguson and Enrique Rosales and Daniele Panozzo and Sylvain Lefevbre and Paolo Cignoni and Alla Sheffer},
  doi          = {10.1145/3414685.3417810},
  journal      = {ACM Transactions on Graphics},
  month        = {11},
  number       = {6},
  pages        = {205:1–17},
  shortjournal = {ACM Trans. Graph.},
  title        = {DHFSlicer: Double height-field slicing for milling fixed-height materials},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Reinforced FDM: Multi-axis filament alignment with
controlled anisotropic strength. <em>TOG</em>, <em>39</em>(6), 204:1–15.
(<a href="https://doi.org/10.1145/3414685.3417834">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The anisotropy of mechanical strength on a 3D printed model can be controlled in a multi-axis 3D printing system as materials can be accumulated along dynamically varied directions. In this paper, we present a new computational framework to generate specially designed layers and toolpaths of multi-axis 3D printing for strengthening a model by aligning filaments along the directions with large stresses. The major challenge comes from how to effectively decompose a solid into a sequence of strength-aware and collision-free working surfaces. We formulate it as a problem to compute an optimized governing field together with a selected orientation of fabrication setup. Iso-surfaces of the governing field are extracted as working surface layers for filament alignment. Supporting structures in curved layers are constructed by extrapolating the governing field to enable the fabrication of overhangs. Compared with planar-layer based Fused Deposition Modeling (FDM) technology, models fabricated by our method can withstand up to 6 . 35× loads in experimental tests.},
  archive      = {J_TOG},
  author       = {Guoxin Fang and Tianyu Zhang and Sikai Zhong and Xiangjia Chen and Zichun Zhong and Charlie C. L. Wang},
  doi          = {10.1145/3414685.3417834},
  journal      = {ACM Transactions on Graphics},
  month        = {11},
  number       = {6},
  pages        = {204:1–15},
  shortjournal = {ACM Trans. Graph.},
  title        = {Reinforced FDM: Multi-axis filament alignment with controlled anisotropic strength},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). VDAC: Volume decompose-and-carve for subtractive
manufacturing. <em>TOG</em>, <em>39</em>(6), 203:1–15. (<a
href="https://doi.org/10.1145/3414685.3417772">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce carvable volume decomposition for efficient 3-axis CNC machining of 3D freeform objects, where our goal is to develop a fully automatic method to jointly optimize setup and path planning. We formulate our joint optimization as a volume decomposition problem which prioritizes minimizing the number of setup directions while striving for a minimum number of continuously carvable volumes, where a 3D volume is continuously carvable, or simply carvable, if it can be carved with the machine cutter traversing a single continuous path. Geometrically, carvability combines visibility and monotonicity and presents a new shape property which had not been studied before. Given a target 3D shape and the initial material block, our algorithm first finds the minimum number of carving directions by solving a set cover problem. Specifically, we analyze cutter accessibility and select the carving directions based on an assessment of how likely they would lead to a small carvable volume decomposition. Next, to obtain a minimum decomposition based on the selected carving directions efficiently, we narrow down the solution search by focusing on a special kind of points in the residual volume, single access or SA points, which are points that can be accessed from one and only one of the selected carving directions. Candidate carvable volumes are grown starting from the SA points. Finally, we devise an energy term to evaluate the carvable volumes and their combinations, leading to the final decomposition. We demonstrate the performance of our decomposition algorithm on a variety of 2D and 3D examples and evaluate it against the ground truth, where possible, and solutions provided by human experts. Physically machined models are produced where each carvable volume is continuously carved following a connected Fermat spiral toolpath.},
  archive      = {J_TOG},
  author       = {Ali Mahdavi-Amiri and Fenggen Yu and Haisen Zhao and Adriana Schulz and Hao Zhang},
  doi          = {10.1145/3414685.3417772},
  journal      = {ACM Transactions on Graphics},
  month        = {11},
  number       = {6},
  pages        = {203:1–15},
  shortjournal = {ACM Trans. Graph.},
  title        = {VDAC: Volume decompose-and-carve for subtractive manufacturing},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Sparse cholesky updates for interactive mesh
parameterization. <em>TOG</em>, <em>39</em>(6), 202:1–14. (<a
href="https://doi.org/10.1145/3414685.3417828">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a novel linear solver for interactive parameterization tasks. Our method is based on the observation that quasi-conformal parameterizations of a triangle mesh are largely determined by boundary conditions. These boundary conditions are typically constructed interactively by users, who have to take several artistic and geometric constraints into account while introducing cuts on the geometry. Commonly, the main computational burden in these methods is solving a linear system every time new boundary conditions are imposed. The core of our solver is a novel approach to efficiently update the Cholesky factorization of the linear system to reflect new boundary conditions, thereby enabling a seamless and interactive workflow even for large meshes consisting of several millions of vertices.},
  archive      = {J_TOG},
  author       = {Philipp Herholz and Olga Sorkine-Hornung},
  doi          = {10.1145/3414685.3417828},
  journal      = {ACM Transactions on Graphics},
  month        = {11},
  number       = {6},
  pages        = {202:1–14},
  shortjournal = {ACM Trans. Graph.},
  title        = {Sparse cholesky updates for interactive mesh parameterization},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). To cut or to fill: A global optimization approach to
topological simplification. <em>TOG</em>, <em>39</em>(6), 201:1–18. (<a
href="https://doi.org/10.1145/3414685.3417854">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a novel algorithm for simplifying the topology of a 3D shape, which is characterized by the number of connected components, handles, and cavities. Existing methods either limit their modifications to be only cutting or only filling, or take a heuristic approach to decide where to cut or fill. We consider the problem of finding a globally optimal set of cuts and fills that achieve the simplest topology while minimizing geometric changes. We show that the problem can be formulated as graph labelling, and we solve it by a transformation to the Node-Weighted Steiner Tree problem. When tested on examples with varying levels of topological complexity, the algorithm shows notable improvement over existing simplification methods in both topological simplicity and geometric distortions.},
  archive      = {J_TOG},
  author       = {Dan Zeng and Erin Chambers and David Letscher and Tao Ju},
  doi          = {10.1145/3414685.3417854},
  journal      = {ACM Transactions on Graphics},
  month        = {11},
  number       = {6},
  pages        = {201:1–18},
  shortjournal = {ACM Trans. Graph.},
  title        = {To cut or to fill: A global optimization approach to topological simplification},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Shape approximation by developable wrapping. <em>TOG</em>,
<em>39</em>(6), 200:1–12. (<a
href="https://doi.org/10.1145/3414685.3417835">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present an automatic tool to approximate curved geometries with piece-wise developable surfaces. At the center of our work is an algorithm that wraps a given 3D input surface with multiple developable patches, each modeled as a discrete orthogonal geodesic net. Our algorithm features a global optimization routine for effectively finding the placement of the developable patches. After wrapping the mesh, we use these patches and a non-linear projection step to generate a surface that approximates the original input, but is also amendable to simple and efficient fabrication techniques thanks to being piecewise developable. Our algorithm allows users to steer the trade-off between approximation power and the number of developable patches used. We demonstrate the effectiveness of our approach on a range of 3D shapes. Compared to previous approaches, our results exhibit a smaller or comparable error with fewer patches to fabricate.},
  archive      = {J_TOG},
  author       = {Alexandra Ion and Michael Rabinovich and Philipp Herholz and Olga Sorkine-Hornung},
  doi          = {10.1145/3414685.3417835},
  journal      = {ACM Transactions on Graphics},
  month        = {11},
  number       = {6},
  pages        = {200:1–12},
  shortjournal = {ACM Trans. Graph.},
  title        = {Shape approximation by developable wrapping},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Nonlinear spectral geometry processing via the TV transform.
<em>TOG</em>, <em>39</em>(6), 199:1–16. (<a
href="https://doi.org/10.1145/3414685.3417849">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce a novel computational framework for digital geometry processing, based upon the derivation of a nonlinear operator associated to the total variation functional. Such an operator admits a generalized notion of spectral decomposition, yielding a convenient multiscale representation akin to Laplacian-based methods, while at the same time avoiding undesirable over-smoothing effects typical of such techniques. Our approach entails accurate, detail-preserving decomposition and manipulation of 3D shape geometry while taking an especially intuitive form: non-local semantic details are well separated into different bands, which can then be filtered and re-synthesized with a straightforward linear step. Our computational framework is flexible, can be applied to a variety of signals, and is easily adapted to different geometry representations, including triangle meshes and point clouds. We showcase our method through multiple applications in graphics, ranging from surface and signal denoising to enhancement, detail transfer, and cubic stylization.},
  archive      = {J_TOG},
  author       = {Marco Fumero and Michael Möller and Emanuele Rodolà},
  doi          = {10.1145/3414685.3417849},
  journal      = {ACM Transactions on Graphics},
  month        = {11},
  number       = {6},
  pages        = {199:1–16},
  shortjournal = {ACM Trans. Graph.},
  title        = {Nonlinear spectral geometry processing via the TV transform},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Opening and closing surfaces. <em>TOG</em>, <em>39</em>(6),
198:1–13. (<a href="https://doi.org/10.1145/3414685.3417778">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a new type of curvature flow for curves in 2D and surfaces in 3D. The flow is inspired by the mathematical morphology opening and closing operations. These operations are classically defined by composition of dilation and erosion operations. In practice, existing methods implemented this way will result in re-discretizing the entire shape, even if some parts of the surface do not change. Instead, our surface-only curvature-based flow moves the surface selectively in areas that should be repositioned. In our triangle mesh discretization, vertices in regions unaffected by the opening or closing will remain exactly in place and do not affect our method&#39;s complexity, which is output-sensitive.},
  archive      = {J_TOG},
  author       = {Silvia Sellán and Jacob Kesten and Ang Yan Sheng and Alec Jacobson},
  doi          = {10.1145/3414685.3417778},
  journal      = {ACM Transactions on Graphics},
  month        = {11},
  number       = {6},
  pages        = {198:1–13},
  shortjournal = {ACM Trans. Graph.},
  title        = {Opening and closing surfaces},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Functional optimization of fluidic devices with
differentiable stokes flow. <em>TOG</em>, <em>39</em>(6), 197:1–15. (<a
href="https://doi.org/10.1145/3414685.3417795">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a method for performance-driven optimization of fluidic devices. In our approach, engineers provide a high-level specification of a device using parametric surfaces for the fluid-solid boundaries. They also specify desired flow properties for inlets and outlets of the device. Our computational approach optimizes the boundary of the fluidic device such that its steady-state flow matches desired flow at outlets. In order to deal with computational challenges of this task, we propose an efficient, differentiable Stokes flow solver. Our solver provides explicit access to gradients of performance metrics with respect to the parametric boundary representation. This key feature allows us to couple the solver with efficient gradient-based optimization methods. We demonstrate the efficacy of this approach on designs of five complex 3D fluidic systems. Our approach makes an important step towards practical computational design tools for high-performance fluidic devices.},
  archive      = {J_TOG},
  author       = {Tao Du and Kui Wu and Andrew Spielberg and Wojciech Matusik and Bo Zhu and Eftychios Sifakis},
  doi          = {10.1145/3414685.3417795},
  journal      = {ACM Transactions on Graphics},
  month        = {11},
  number       = {6},
  pages        = {197:1–15},
  shortjournal = {ACM Trans. Graph.},
  title        = {Functional optimization of fluidic devices with differentiable stokes flow},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Match: Differentiable material graphs for procedural
material capture. <em>TOG</em>, <em>39</em>(6), 196:1–15. (<a
href="https://doi.org/10.1145/3414685.3417781">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present MATch , a method to automatically convert photographs of material samples into production-grade procedural material models. At the core of MATch is a new library DiffMat that provides differentiable building blocks for constructing procedural materials, and automatic translation of large-scale procedural models, with hundreds to thousands of node parameters, into differentiable node graphs. Combining these translated node graphs with a rendering layer yields an end-to-end differentiable pipeline that maps node graph parameters to rendered images. This facilitates the use of gradient-based optimization to estimate the parameters such that the resulting material, when rendered, matches the target image appearance, as quantified by a style transfer loss. In addition, we propose a deep neural feature-based graph selection and parameter initialization method that efficiently scales to a large number of procedural graphs. We evaluate our method on both rendered synthetic materials and real materials captured as flash photographs. We demonstrate that MATch can reconstruct more accurate, general, and complex procedural materials compared to the state-of-the-art. Moreover, by producing a procedural output, we unlock capabilities such as constructing arbitrary-resolution material maps and parametrically editing the material appearance.},
  archive      = {J_TOG},
  author       = {Liang Shi and Beichen Li and Miloš Hašan and Kalyan Sunkavalli and Tamy Boubekeur and Radomir Mech and Wojciech Matusik},
  doi          = {10.1145/3414685.3417781},
  journal      = {ACM Transactions on Graphics},
  month        = {11},
  number       = {6},
  pages        = {196:1–15},
  shortjournal = {ACM Trans. Graph.},
  title        = {Match: Differentiable material graphs for procedural material capture},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Differentiable refraction-tracing for mesh reconstruction of
transparent objects. <em>TOG</em>, <em>39</em>(6), 195:1–13. (<a
href="https://doi.org/10.1145/3414685.3417815">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Capturing the 3D geometry of transparent objects is a challenging task, ill-suited for general-purpose scanning and reconstruction techniques, since these cannot handle specular light transport phenomena. Existing state-of-the-art methods, designed specifically for this task, either involve a complex setup to reconstruct complete refractive ray paths, or leverage a data-driven approach based on synthetic training data. In either case, the reconstructed 3D models suffer from over-smoothing and loss of fine detail. This paper introduces a novel, high precision, 3D acquisition and reconstruction method for solid transparent objects. Using a static background with a coded pattern, we establish a mapping between the camera view rays and locations on the background. Differentiable tracing of refractive ray paths is then used to directly optimize a 3D mesh approximation of the object, while simultaneously ensuring silhouette consistency and smoothness. Extensive experiments and comparisons demonstrate the superior accuracy of our method.},
  archive      = {J_TOG},
  author       = {Jiahui Lyu and Bojian Wu and Dani Lischinski and Daniel Cohen-Or and Hui Huang},
  doi          = {10.1145/3414685.3417815},
  journal      = {ACM Transactions on Graphics},
  month        = {11},
  number       = {6},
  pages        = {195:1–13},
  shortjournal = {ACM Trans. Graph.},
  title        = {Differentiable refraction-tracing for mesh reconstruction of transparent objects},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Modular primitives for high-performance differentiable
rendering. <em>TOG</em>, <em>39</em>(6), 194:1–14. (<a
href="https://doi.org/10.1145/3414685.3417861">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a modular differentiable renderer design that yields performance superior to previous methods by leveraging existing, highly optimized hardware graphics pipelines. Our design supports all crucial operations in a modern graphics pipeline: rasterizing large numbers of triangles, attribute interpolation, filtered texture lookups, as well as user-programmable shading and geometry processing, all in high resolutions. Our modular primitives allow custom, high-performance graphics pipelines to be built directly within automatic differentiation frameworks such as PyTorch or TensorFlow. As a motivating application, we formulate facial performance capture as an inverse rendering problem and show that it can be solved efficiently using our tools. Our results indicate that this simple and straightforward approach achieves excellent geometric correspondence between rendered results and reference imagery.},
  archive      = {J_TOG},
  author       = {Samuli Laine and Janne Hellsten and Tero Karras and Yeongho Seol and Jaakko Lehtinen and Timo Aila},
  doi          = {10.1145/3414685.3417861},
  journal      = {ACM Transactions on Graphics},
  month        = {11},
  number       = {6},
  pages        = {194:1–14},
  shortjournal = {ACM Trans. Graph.},
  title        = {Modular primitives for high-performance differentiable rendering},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Differentiable vector graphics rasterization for editing and
learning. <em>TOG</em>, <em>39</em>(6), 193:1–15. (<a
href="https://doi.org/10.1145/3414685.3417871">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce a differentiable rasterizer that bridges the vector graphics and raster image domains, enabling powerful raster-based loss functions, optimization procedures, and machine learning techniques to edit and generate vector content. We observe that vector graphics rasterization is differentiable after pixel prefiltering. Our differentiable rasterizer offers two prefiltering options: an analytical prefiltering technique and a multisampling anti-aliasing technique. The analytical variant is faster but can suffer from artifacts such as conflation. The multisampling variant is still efficient, and can render high-quality images while computing unbiased gradients for each pixel with respect to curve parameters. We demonstrate that our rasterizer enables new applications, including a vector graphics editor guided by image metrics, a painterly rendering algorithm that fits vector primitives to an image by minimizing a deep perceptual loss function, new vector graphics editing algorithms that exploit well-known image processing methods such as seam carving, and deep generative models that generate vector content from raster-only supervision under a VAE or GAN training objective.},
  archive      = {J_TOG},
  author       = {Tzu-Mao Li and Michal Lukáč and Michaël Gharbi and Jonathan Ragan-Kelley},
  doi          = {10.1145/3414685.3417871},
  journal      = {ACM Transactions on Graphics},
  month        = {11},
  number       = {6},
  pages        = {193:1–15},
  shortjournal = {ACM Trans. Graph.},
  title        = {Differentiable vector graphics rasterization for editing and learning},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Offsite aerial path planning for efficient urban scene
reconstruction. <em>TOG</em>, <em>39</em>(6), 192:1–16. (<a
href="https://doi.org/10.1145/3414685.3417791">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With rapid development in UAV technologies, it is now possible to reconstruct large-scale outdoor scenes using only images captured by low-cost drones. The problem, however, becomes how to plan the aerial path for a drone to capture images so that two conflicting goals are optimized: maximizing the reconstruction quality and minimizing mid-air image acquisition effort. Existing approaches either resort to pre-defined dense and thus inefficient view sampling strategy, or plan the path adaptively but require two onsite flight passes and intensive computation in-between. Hence, using these methods to capture and reconstruct large-scale scenes can be tedious. In this paper, we present an adaptive aerial path planning algorithm that can be done before the site visit. Using only a 2D map and a satellite image of the to-be-reconstructed area, we first compute a coarse 2.5D model for the scene based on the relationship between buildings and their shadows. A novel Max-Min optimization is then proposed to select a minimal set of viewpoints that maximizes the reconstructability under the the same number of viewpoints. Experimental results on benchmark show that our planning approach can effectively reduce the number of viewpoints needed than the previous state-of-the-art method, while maintaining comparable reconstruction quality. Since no field computation or a second visit is needed, and the view number is also minimized, our approach significantly reduces the time required in the field as well as the off-line computation cost for multi-view stereo reconstruction, making it possible to reconstruct a large-scale urban scene in a short time with moderate effort.},
  archive      = {J_TOG},
  author       = {Xiaohui Zhou and Ke Xie and Kai Huang and Yilin Liu and Yang Zhou and Minglun Gong and Hui Huang},
  doi          = {10.1145/3414685.3417791},
  journal      = {ACM Transactions on Graphics},
  month        = {11},
  number       = {6},
  pages        = {192:1–16},
  shortjournal = {ACM Trans. Graph.},
  title        = {Offsite aerial path planning for efficient urban scene reconstruction},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A harmonic balance approach for designing compliant
mechanical systems with nonlinear periodic motions. <em>TOG</em>,
<em>39</em>(6), 191:1–14. (<a
href="https://doi.org/10.1145/3414685.3417765">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a computational method for designing compliant mechanical systems that exhibit large-amplitude oscillations. The technical core of our approach is an optimization-driven design tool that combines sensitivity analysis for optimization with the Harmonic Balance Method for simulation. By establishing dynamic force equilibrium in the frequency domain, our formulation avoids the major limitations of existing alternatives: it handles nonlinear forces, side-steps any transient process, and automatically produces periodic solutions. We introduce design objectives for amplitude optimization and trajectory matching that enable intuitive high-level authoring of large-amplitude motions. Our method can be applied to many types of mechanical systems, which we demonstrate through a set of examples involving compliant mechanisms, flexible rod networks, elastic thin shell models, and multi-material solids. We further validate our approach by manufacturing and evaluating several physical prototypes.},
  archive      = {J_TOG},
  author       = {Pengbin Tang and Jonas Zehnder and Stelian Coros and Bernhard Thomaszewski},
  doi          = {10.1145/3414685.3417765},
  journal      = {ACM Transactions on Graphics},
  month        = {11},
  number       = {6},
  pages        = {191:1–14},
  shortjournal = {ACM Trans. Graph.},
  title        = {A harmonic balance approach for designing compliant mechanical systems with nonlinear periodic motions},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). ADD: Analytically differentiable dynamics for multi-body
systems with frictional contact. <em>TOG</em>, <em>39</em>(6), 190:1–15.
(<a href="https://doi.org/10.1145/3414685.3417766">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a differentiable dynamics solver that is able to handle frictional contact for rigid and deformable objects within a unified framework. Through a principled mollification of normal and tangential contact forces, our method circumvents the main difficulties inherent to the non-smooth nature of frictional contact. We combine this new contact model with fully-implicit time integration to obtain a robust and efficient dynamics solver that is analytically differentiable. In conjunction with adjoint sensitivity analysis, our formulation enables gradient-based optimization with adaptive trade-offs between simulation accuracy and smoothness of objective function landscapes. We thoroughly analyse our approach on a set of simulation examples involving rigid bodies, visco-elastic materials, and coupled multi-body systems. We furthermore showcase applications of our differentiable simulator to parameter estimation for deformable objects, motion planning for robotic manipulation, trajectory optimization for compliant walking robots, as well as efficient self-supervised learning of control policies.},
  archive      = {J_TOG},
  author       = {Moritz Geilinger and David Hahn and Jonas Zehnder and Moritz Bächer and Bernhard Thomaszewski and Stelian Coros},
  doi          = {10.1145/3414685.3417766},
  journal      = {ACM Transactions on Graphics},
  month        = {11},
  number       = {6},
  pages        = {190:1–15},
  shortjournal = {ACM Trans. Graph.},
  title        = {ADD: Analytically differentiable dynamics for multi-body systems with frictional contact},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Learning to manipulate amorphous materials. <em>TOG</em>,
<em>39</em>(6), 189:1–11. (<a
href="https://doi.org/10.1145/3414685.3417868">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a method of training character manipulation of amorphous materials such as those often used in cooking. Common examples of amorphous materials include granular materials (salt, uncooked rice), fluids (honey), and visco-plastic materials (sticky rice, softened butter). A typical task is to spread a given material out across a flat surface using a tool such as a scraper or knife. We use reinforcement learning to train our controllers to manipulate materials in various ways. The training is performed in a physics simulator that uses position-based dynamics of particles to simulate the materials to be manipulated. The neural network control policy is given observations of the material (e.g. a low-resolution density map), and the policy outputs actions such as rotating and translating the knife. We demonstrate policies that have been successfully trained to carry out the following tasks: spreading, gathering, and flipping. We produce a final animation by using inverse kinematics to guide a character&#39;s arm and hand to match the motion of the manipulation tool such as a knife or a frying pan.},
  archive      = {J_TOG},
  author       = {Yunbo Zhang and Wenhao Yu and C. Karen Liu and Charlie Kemp and Greg Turk},
  doi          = {10.1145/3414685.3417868},
  journal      = {ACM Transactions on Graphics},
  month        = {11},
  number       = {6},
  pages        = {189:1–11},
  shortjournal = {ACM Trans. Graph.},
  title        = {Learning to manipulate amorphous materials},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). RoboGrammar: Graph grammar for terrain-optimized robot
design. <em>TOG</em>, <em>39</em>(6), 188:1–16. (<a
href="https://doi.org/10.1145/3414685.3417831">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present RoboGrammar , a fully automated approach for generating optimized robot structures to traverse given terrains. In this framework, we represent each robot design as a graph, and use a graph grammar to express possible arrangements of physical robot assemblies. Each robot design can then be expressed as a sequence of grammar rules. Using only a small set of rules our grammar can describe hundreds of thousands of possible robot designs. The construction of the grammar limits the design space to designs that can be fabricated. For a given input terrain, the design space is searched to find the top performing robots and their corresponding controllers. We introduce Graph Heuristic Search - a novel method for efficient search of combinatorial design spaces. In Graph Heuristic Search, we explore the design space while simultaneously learning a function that maps incomplete designs (e.g., nodes in the combinatorial search tree) to the best performance values that can be achieved by expanding these incomplete designs. Graph Heuristic Search prioritizes exploration of the most promising branches of the design space. To test our method we optimize robots for a number of challenging and varied terrains. We demonstrate that RoboGrammar can successfully generate nontrivial robots that are optimized for a single terrain or a combination of terrains.},
  archive      = {J_TOG},
  author       = {Allan Zhao and Jie Xu and Mina Konaković-Luković and Josephine Hughes and Andrew Spielberg and Daniela Rus and Wojciech Matusik},
  doi          = {10.1145/3414685.3417831},
  journal      = {ACM Transactions on Graphics},
  month        = {11},
  number       = {6},
  pages        = {188:1–16},
  shortjournal = {ACM Trans. Graph.},
  title        = {RoboGrammar: Graph grammar for terrain-optimized robot design},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Rendering near-field speckle statistics in scattering media.
<em>TOG</em>, <em>39</em>(6), 187:1–18. (<a
href="https://doi.org/10.1145/3414685.3417813">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce rendering algorithms for the simulation of speckle statistics observed in scattering media under coherent near-field imaging conditions. Our work is motivated by the recent proliferation of techniques that use speckle correlations for tissue imaging applications: The ability to simulate the image measurements used by these speckle imaging techniques in a physically-accurate and computationally-efficient way can facilitate the widespread adoption and improvement of these techniques. To this end, we draw inspiration from recently-introduced Monte Carlo algorithms for rendering speckle statistics under far-field conditions (collimated sensor and illumination). We derive variants of these algorithms that are better suited to the near-field conditions (focused sensor and illumination) required by tissue imaging applications. Our approach is based on using Gaussian apodization to approximate the sensor and illumination aperture, as well as von Mises-Fisher functions to approximate the phase function of the scattering material. We show that these approximations allow us to derive closed-form expressions for the focusing operations involved in simulating near-field speckle patterns. As we demonstrate in our experiments, these approximations accelerate speckle rendering simulations by a few orders of magnitude compared to previous techniques, at the cost of negligible bias. We validate the accuracy of our algorithms by reproducing ground truth speckle statistics simulated using wave-optics solvers, and real-material measurements available in the literature. Finally, we use our algorithms to simulate biomedical imaging techniques for focusing through tissue.},
  archive      = {J_TOG},
  author       = {Chen Bar and Ioannis Gkioulekas and Anat Levin},
  doi          = {10.1145/3414685.3417813},
  journal      = {ACM Transactions on Graphics},
  month        = {11},
  number       = {6},
  pages        = {187:1–18},
  shortjournal = {ACM Trans. Graph.},
  title        = {Rendering near-field speckle statistics in scattering media},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Learned hardware-in-the-loop phase retrieval for holographic
near-eye displays. <em>TOG</em>, <em>39</em>(6), 186:1–18. (<a
href="https://doi.org/10.1145/3414685.3417846">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Holography is arguably the most promising technology to provide wide field-of-view compact eyeglasses-style near-eye displays for augmented and virtual reality. However, the image quality of existing holographic displays is far from that of current generation conventional displays, effectively making today&#39;s holographic display systems impractical. This gap stems predominantly from the severe deviations in the idealized approximations of the &quot;unknown&quot; light transport model in a real holographic display, used for computing holograms. In this work, we depart from such approximate &quot;ideal&quot; coherent light transport models for computing holograms. Instead, we learn the deviations of the real display from the ideal light transport from the images measured using a display-camera hardware system. After this unknown light propagation is learned, we use it to compensate for severe aberrations in real holographic imagery. The proposed hardware-in-the-loop approach is robust to spatial, temporal and hardware deviations, and improves the image quality of existing methods qualitatively and quantitatively in SNR and perceptual quality. We validate our approach on a holographic display prototype and show that the method can fully compensate unknown aberrations and erroneous and non-linear SLM phase delays, without explicitly modeling them. As a result, the proposed method significantly outperforms existing state-of-the-art methods in simulation and experimentation - just by observing captured holographic images.},
  archive      = {J_TOG},
  author       = {Praneeth Chakravarthula and Ethan Tseng and Tarun Srivastava and Henry Fuchs and Felix Heide},
  doi          = {10.1145/3414685.3417846},
  journal      = {ACM Transactions on Graphics},
  month        = {11},
  number       = {6},
  pages        = {186:1–18},
  shortjournal = {ACM Trans. Graph.},
  title        = {Learned hardware-in-the-loop phase retrieval for holographic near-eye displays},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Neural holography with camera-in-the-loop training.
<em>TOG</em>, <em>39</em>(6), 185:1–14. (<a
href="https://doi.org/10.1145/3414685.3417802">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Holographic displays promise unprecedented capabilities for direct-view displays as well as virtual and augmented reality applications. However, one of the biggest challenges for computer-generated holography (CGH) is the fundamental tradeoff between algorithm runtime and achieved image quality, which has prevented high-quality holographic image synthesis at fast speeds. Moreover, the image quality achieved by most holographic displays is low, due to the mismatch between the optical wave propagation of the display and its simulated model. Here, we develop an algorithmic CGH framework that achieves unprecedented image fidelity and real-time framerates. Our framework comprises several parts, including a novel camera-in-the-loop optimization strategy that allows us to either optimize a hologram directly or train an interpretable model of the optical wave propagation and a neural network architecture that represents the first CGH algorithm capable of generating full-color high-quality holographic images at 1080p resolution in real time.},
  archive      = {J_TOG},
  author       = {Yifan Peng and Suyeon Choi and Nitish Padmanaban and Gordon Wetzstein},
  doi          = {10.1145/3414685.3417802},
  journal      = {ACM Transactions on Graphics},
  month        = {11},
  number       = {6},
  pages        = {185:1–14},
  shortjournal = {ACM Trans. Graph.},
  title        = {Neural holography with camera-in-the-loop training},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Design and fabrication of freeform holographic optical
elements. <em>TOG</em>, <em>39</em>(6), 184:1–15. (<a
href="https://doi.org/10.1145/3414685.3417762">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Holographic optical elements (HOEs) have a wide range of applications, including their emerging use in virtual and augmented reality displays, but their design and fabrication have remained largely limited to configurations using simple wavefronts. In this paper, we present a pipeline for the design, optimization, and fabrication of complex, customized HOEs that enhances their imaging performance and enables new applications. In particular, we propose an optimization method for grating vector fields that accounts for the unique selectivity properties of HOEs. We further show how our pipeline can be applied to two distinct HOE fabrication methods. The first uses a pair of freeform refractive elements to manufacture HOEs with high optical quality and precision. The second uses a holographic printer with two wavefront-modulating arms, enabling rapid prototyping. We propose a unified wavefront decomposition framework suitable for both fabrication approaches. To demonstrate the versatility of these methods, we fabricate and characterize a series of specialized HOEs, including an aspheric lens, a head-up display lens, a lens array, and, for the first time, a full-color caustic projection element.},
  archive      = {J_TOG},
  author       = {Changwon Jang and Olivier Mercier and Kiseung Bang and Gang Li and Yang Zhao and Douglas Lanman},
  doi          = {10.1145/3414685.3417762},
  journal      = {ACM Transactions on Graphics},
  month        = {11},
  number       = {6},
  pages        = {184:1–15},
  shortjournal = {ACM Trans. Graph.},
  title        = {Design and fabrication of freeform holographic optical elements},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). An implicit updated lagrangian formulation for liquids with
large surface energy. <em>TOG</em>, <em>39</em>(6), 183:1–13. (<a
href="https://doi.org/10.1145/3414685.3417845">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present an updated Lagrangian discretization of surface tension forces for the simulation of liquids with moderate to extreme surface tension effects. The potential energy associated with surface tension is proportional to the surface area of the liquid. We design discrete forces as gradients of this energy with respect to the motion of the fluid over a time step. We show that this naturally allows for inversion of the Hessian of the potential energy required with the use of Newton&#39;s method to solve the systems of nonlinear equations associated with implicit time stepping. The rotational invariance of the surface tension energy makes it non-convex and we define a definiteness fix procedure as in [Teran et al. 2005]. We design a novel level-set-based boundary quadrature technique to discretize the surface area calculation in our energy based formulation. Our approach works most naturally with Particle-In-Cell [Harlow 1964] techniques and we demonstrate our approach with a weakly incompressible model for liquid discretized with the Material Point Method [Sulsky et al. 1994]. We show that our approach is essential for allowing efficient implicit numerical integration in the limit of high surface tension materials like liquid metals.},
  archive      = {J_TOG},
  author       = {David A. B. Hyde and Steven W. Gagniere and Alan Marquez-Razon and Joseph Teran},
  doi          = {10.1145/3414685.3417845},
  journal      = {ACM Transactions on Graphics},
  month        = {11},
  number       = {6},
  pages        = {183:1–13},
  shortjournal = {ACM Trans. Graph.},
  title        = {An implicit updated lagrangian formulation for liquids with large surface energy},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Monolith: A monolithic pressure-viscosity-contact solver for
strong two-way rigid-rigid rigid-fluid coupling. <em>TOG</em>,
<em>39</em>(6), 182:1–16. (<a
href="https://doi.org/10.1145/3414685.3417798">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose Monolith , a monolithic pressure-viscosity-contact solver for more accurately, robustly, and efficiently simulating non-trivial two-way interactions of rigid bodies with inviscid, viscous, or non-Newtonian liquids. Our solver simultaneously handles incompressibility and (optionally) implicit viscosity integration for liquids, contact resolution for rigid bodies, and mutual interactions between liquids and rigid bodies by carefully formulating these as a single unified minimization problem. This monolithic approach reduces or eliminates an array of problematic artifacts, including liquid volume loss, solid interpenetrations, simulation instabilities, artificial &quot;melting&quot; of viscous liquid, and incorrect slip at liquid-solid interfaces. In the absence of solid-solid friction, our minimization problem is a Quadratic Program (QP) with a symmetric positive definite (SPD) matrix and can be treated with a single Linear Complementarity Problem (LCP) solve. When friction is present, we decouple the unified minimization problem into two subproblems so that it can be effectively handled via staggered projections with alternating LCP solves. We also propose a complementary approach for non-Newtonian fluids which can be seamlessly integrated and addressed during the staggered projections. We demonstrate the critical importance of a contact-aware , unified treatment of fluid-solid coupling and the effectiveness of our proposed Monolith solver in a wide range of practical scenarios.},
  archive      = {J_TOG},
  author       = {Tetsuya Takahashi and Christopher Batty},
  doi          = {10.1145/3414685.3417798},
  journal      = {ACM Transactions on Graphics},
  month        = {11},
  number       = {6},
  pages        = {182:1–16},
  shortjournal = {ACM Trans. Graph.},
  title        = {Monolith: A monolithic pressure-viscosity-contact solver for strong two-way rigid-rigid rigid-fluid coupling},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Higher-order finite elements for embedded simulation.
<em>TOG</em>, <em>39</em>(6), 181:1–14. (<a
href="https://doi.org/10.1145/3414685.3417853">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As demands for high-fidelity physics-based animations increase, the need for accurate methods for simulating deformable solids grows. While higherorder finite elements are commonplace in engineering due to their superior approximation properties for many problems, they have gained little traction in the computer graphics community. This may partially be explained by the need for finite element meshes to approximate the highly complex geometry of models used in graphics applications. Due to the additional perelement computational expense of higher-order elements, larger elements are needed, and the error incurred due to the geometry mismatch eradicates the benefits of higher-order discretizations. One solution to this problem is the embedding of the geometry into a coarser finite element mesh. However, to date there is no adequate, practical computational framework that permits the accurate embedding into higher-order elements. We develop a novel, robust quadrature generation method that generates theoretically guaranteed high-quality sub-cell integration rules of arbitrary polynomial accuracy. The number of quadrature points generated is bounded only by the desired degree of the polynomial, independent of the embedded geometry. Additionally, we build on recent work in the Finite Cell Method (FCM) community so as to tackle the severe ill-conditioning caused by partially filled elements by adapting an Additive-Schwarz-based preconditioner so that it is suitable for use with state-of-the-art non-linear material models from the graphics literature. Together these two contributions constitute a general-purpose framework for embedded simulation with higher-order finite elements. We finally demonstrate the benefits of our framework in several scenarios, in which second-order hexahedra and tetrahedra clearly outperform their first-order counterparts.},
  archive      = {J_TOG},
  author       = {Andreas Longva and Fabian Löschner and Tassilo Kugelstadt and José Antonio Fernández-Fernández and Jan Bender},
  doi          = {10.1145/3414685.3417853},
  journal      = {ACM Transactions on Graphics},
  month        = {11},
  number       = {6},
  pages        = {181:1–14},
  shortjournal = {ACM Trans. Graph.},
  title        = {Higher-order finite elements for embedded simulation},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). P-cloth: Interactive complex cloth simulation on multi-GPU
systems using dynamic matrix assembly and pipelined implicit
integrators. <em>TOG</em>, <em>39</em>(6), 180:1–15. (<a
href="https://doi.org/10.1145/3414685.3417763">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a novel parallel algorithm for cloth simulation that exploits multiple GPUs for fast computation and the handling of very high resolution meshes. To accelerate implicit integration, we describe new parallel algorithms for sparse matrix-vector multiplication (SpMV) and for dynamic matrix assembly on a multi-GPU workstation. Our algorithms use a novel work queue generation scheme for a fat-tree GPU interconnect topology. Furthermore, we present a novel collision handling scheme that uses spatial hashing for discrete and continuous collision detection along with a non-linear impact zone solver. Our parallel schemes can distribute the computation and storage overhead among multiple GPUs and enable us to perform almost interactive simulation on complex cloth meshes, which can hardly be handled on a single GPU due to memory limitations. We have evaluated the performance with two multi-GPU workstations (with 4 and 8 GPUs, respectively) on cloth meshes with 0.5 -- 1.65 M triangles. Our approach can reliably handle the collisions and generate vivid wrinkles and folds at 2 -- 5 fps, which is significantly faster than prior cloth simulation systems. We observe almost linear speedups with respect to the number of GPUs.},
  archive      = {J_TOG},
  author       = {Cheng Li and Min Tang and Ruofeng Tong and Ming Cai and Jieyi Zhao and Dinesh Manocha},
  doi          = {10.1145/3414685.3417763},
  journal      = {ACM Transactions on Graphics},
  month        = {11},
  number       = {6},
  pages        = {180:1–15},
  shortjournal = {ACM Trans. Graph.},
  title        = {P-cloth: Interactive complex cloth simulation on multi-GPU systems using dynamic matrix assembly and pipelined implicit integrators},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Complementary dynamics. <em>TOG</em>, <em>39</em>(6),
179:1–11. (<a href="https://doi.org/10.1145/3414685.3417819">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a novel approach to enrich arbitrary rig animations with elastodynamic secondary effects. Unlike previous methods which pit rig displacements and physical forces as adversaries against each other, we advocate that physics should complement artists&#39; intentions. We propose optimizing for elastodynamic displacements in the subspace orthogonal to displacements that can be created by the rig. This ensures that the additional dynamic motions do not undo the rig animation. The complementary space is high-dimensional, algebraically constructed without manual oversight, and capable of rich high-frequency dynamics. Unlike prior tracking methods, we do not require extra painted weights, segmentation into fixed and free regions or tracking clusters. Our method is agnostic to the physical model and plugs into non-linear FEM simulations, geometric as-rigid-as-possible energies, or mass-spring models. Our method does not require a particular type of rig and adds secondary effects to skeletal animations, cage-based deformations, wire deformers, motion capture data, and rigid-body simulations.},
  archive      = {J_TOG},
  author       = {Jiayi Eris Zhang and Seungbae Bang and David I. W. Levin and Alec Jacobson},
  doi          = {10.1145/3414685.3417819},
  journal      = {ACM Transactions on Graphics},
  month        = {11},
  number       = {6},
  pages        = {179:1–11},
  shortjournal = {ACM Trans. Graph.},
  title        = {Complementary dynamics},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A novel discretization and numerical solver for non-fourier
diffusion. <em>TOG</em>, <em>39</em>(6), 178:1–14. (<a
href="https://doi.org/10.1145/3414685.3417863">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce the C-F diffusion model [Anderson and Tamma 2006; Xue et al. 2018] to computer graphics for diffusion-driven problems that has several attractive properties: (a) it fundamentally explains diffusion from the perspective of the non-equilibrium statistical mechanical Boltzmann Transport Equation, (b) it allows for a finite propagation speed for diffusion, in contrast to the widely employed Fick&#39;s/Fourier&#39;s law, and (c) it can capture some of the most characteristic visual aspects of diffusion-driven physics, such as hydrogel swelling, limited diffusive domain for smoke flow, snowflake and dendrite formation, that span from Fourier-type to non-Fourier-type diffusive phenomena. We propose a unified convection-diffusion formulation using this model that treats both the diffusive quantity and its associated flux as the primary unknowns, and that recovers the traditional Fourier-type diffusion as a limiting case. We design a novel semi-implicit discretization for this formulation on staggered MAC grids and a geometric Multigrid-preconditioned Conjugate Gradients solver for efficient numerical solution. To highlight the efficacy of our method, we demonstrate end-to-end examples of elastic porous media simulated with the Material Point Method (MPM), and diffusion-driven Eulerian incompressible fluids.},
  archive      = {J_TOG},
  author       = {Tao Xue and Haozhe Su and Chengguizi Han and Chenfanfu Jiang and Mridul Aanjaneya},
  doi          = {10.1145/3414685.3417863},
  journal      = {ACM Transactions on Graphics},
  month        = {11},
  number       = {6},
  pages        = {178:1–14},
  shortjournal = {ACM Trans. Graph.},
  title        = {A novel discretization and numerical solver for non-fourier diffusion},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Simulation, modeling and authoring of glaciers.
<em>TOG</em>, <em>39</em>(6), 177:1–14. (<a
href="https://doi.org/10.1145/3414685.3417855">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Glaciers are some of the most visually arresting and scenic elements of cold regions and high mountain landscapes. Although snow-covered terrains have previously received attention in computer graphics, simulating the temporal evolution of glaciers as well as modeling their wide range of features has never been addressed. In this paper, we combine a Shallow Ice Approximation simulation with a procedural amplification process to author high-resolution realistic glaciers. Our multiresolution method allows the interactive simulation of the formation and the evolution of glaciers over hundreds of years. The user can easily modify the environment variables, such as the average temperature or precipitation rate, to control the glacier growth, or directly use brushes to sculpt the ice or bedrock with interactive feedback. Mesoscale and smallscale landforms that are not captured by the glacier simulation, such as crevasses, moraines, seracs, ogives, or icefalls are synthesized using procedural rules inspired by observations in glaciology and according to the physical parameters derived from the simulation. Our method lends itself to seamless integration into production pipelines to decorate reliefs with glaciers and realistic ice features.},
  archive      = {J_TOG},
  author       = {Oscar Argudo and Eric Galin and Adrien Peytavie and Axel Paris and Eric Guérin},
  doi          = {10.1145/3414685.3417855},
  journal      = {ACM Transactions on Graphics},
  month        = {11},
  number       = {6},
  pages        = {177:1–14},
  shortjournal = {ACM Trans. Graph.},
  title        = {Simulation, modeling and authoring of glaciers},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A moving least square reproducing kernel particle method for
unified multiphase continuum simulation. <em>TOG</em>, <em>39</em>(6),
176:1–15. (<a href="https://doi.org/10.1145/3414685.3417809">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In physically based-based animation, pure particle methods are popular due to their simple data structure, easy implementation, and convenient parallelization. As a pure particle-based method and using Galerkin discretization, the Moving Least Square Reproducing Kernel Method (MLSRK) was developed in engineering computation as a general numerical tool for solving PDEs. The basic idea of Moving Least Square (MLS) has also been used in computer graphics to estimate deformation gradient for deformable solids. Based on these previous studies, we propose a multiphase MLSRK framework that animates complex and coupled fluids and solids in a unified manner. Specifically, we use the Cauchy momentum equation and phase field model to uniformly capture the momentum balance and phase evolution/interaction in a multiphase system, and systematically formulate the MLSRK discretization to support general multiphase constitutive models. A series of animation examples are presented to demonstrate the performance of our new multiphase MLSRK framework, including hyperelastic, elastoplastic, viscous, fracturing and multiphase coupling behaviours etc.},
  archive      = {J_TOG},
  author       = {Xiao-Song Chen and Chen-Feng Li and Geng-Chen Cao and Yun-Tao Jiang and Shi-Min Hu},
  doi          = {10.1145/3414685.3417809},
  journal      = {ACM Transactions on Graphics},
  month        = {11},
  number       = {6},
  pages        = {176:1–15},
  shortjournal = {ACM Trans. Graph.},
  title        = {A moving least square reproducing kernel particle method for unified multiphase continuum simulation},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Stormscapes: Simulating cloud dynamics in the now.
<em>TOG</em>, <em>39</em>(6), 175:1–16. (<a
href="https://doi.org/10.1145/3414685.3417801">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The complex interplay of a number of physical and meteorological phenomena makes simulating clouds a challenging and open research problem. We explore a physically accurate model for simulating clouds and the dynamics of their transitions. We propose first-principle formulations for computing buoyancy and air pressure that allow us to simulate the variations of atmospheric density and varying temperature gradients. Our simulation allows us to model various cloud types, such as cumulus, stratus, and stratoscumulus, and their realistic formations caused by changes in the atmosphere. Moreover, we are able to simulate large-scale cloud super cells - clusters of cumulonimbus formations - that are commonly present during thunderstorms. To enable the efficient exploration of these stormscapes, we propose a lightweight set of high-level parameters that allow us to intuitively explore cloud formations and dynamics. Our method allows us to simulate cloud formations of up to about 20 km × 20 km extents at interactive rates. We explore the capabilities of physically accurate and yet interactive cloud simulations by showing numerous examples and by coupling our model with atmosphere measurements of real-time weather services to simulate cloud formations in the now. Finally, we quantitatively assess our model with cloud fraction profiles, a common measure for comparing cloud types.},
  archive      = {J_TOG},
  author       = {Torsten Hädrich and Miłosz Makowski and Wojtek Pałubicki and Daniel T. Banuti and Sören Pirk and Dominik L. Michels},
  doi          = {10.1145/3414685.3417801},
  journal      = {ACM Transactions on Graphics},
  month        = {11},
  number       = {6},
  pages        = {175:1–16},
  shortjournal = {ACM Trans. Graph.},
  title        = {Stormscapes: Simulating cloud dynamics in the now},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Surface-only ferrofluids. <em>TOG</em>, <em>39</em>(6),
174:1–17. (<a href="https://doi.org/10.1145/3414685.3417799">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We devise a novel surface-only approach for simulating the three dimensional free-surface flow of incompressible, inviscid, and linearly magnetizable ferrofluids. A Lagrangian velocity field is stored on a triangle mesh capturing the fluid&#39;s surface. The two key problems associated with the dynamic simulation of the fluid&#39;s interesting geometry are the magnetization process transitioning the fluid from a non-magnetic into a magnetic material, and the evaluation of magnetic forces. In this regard, our key observation is that for linearly incompressible ferrofluids, their magnetization and application of magnetic forces only require knowledge about the position of the fluids&#39; boundary. Consequently, our approach employs a boundary element method solving the magnetization problem and evaluating the so-called magnetic pressure required for the force evaluation. The magnetic pressure is added to the Dirichlet boundary condition of a surface-only liquids solver carrying out the dynamical simulation. By only considering the fluid&#39;s surface in contrast to its whole volume, we end up with an efficient approach enabling more complex and realistic ferrofluids to be explored in the digital domain without compromising efficiency. Our approach allows for the use of physical parameters leading to accurate simulations as demonstrated in qualitative and quantitative evaluations.},
  archive      = {J_TOG},
  author       = {Libo Huang and Dominik L. Michels},
  doi          = {10.1145/3414685.3417799},
  journal      = {ACM Transactions on Graphics},
  month        = {11},
  number       = {6},
  pages        = {174:1–17},
  shortjournal = {ACM Trans. Graph.},
  title        = {Surface-only ferrofluids},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Semi-analytic boundary handling below particle resolution
for smoothed particle hydrodynamics. <em>TOG</em>, <em>39</em>(6),
173:1–17. (<a href="https://doi.org/10.1145/3414685.3417829">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we present a novel semi-analytical boundary handling method for spatially adaptive and divergence-free smoothed particle hydrodynamics (SPH) simulations, including two-way coupling. Our method is consistent under varying particle resolutions and allows for the treatment of boundary features below the particle resolution. We achieve this by first introducing an analytic solution to the interaction of SPH particles with planar boundaries, in 2D and 3D, which we extend to arbitrary boundary geometries using signed distance fields (SDF) to construct locally planar boundaries. Using this boundary-integral-based approach, we can directly evaluate boundary contributions, for any quantity, allowing an easy integration into state of the art simulation methods. Overall, our method improves interactions with small boundary features, readily handles spatially adaptive fluids, preserves particle-boundary interactions across varying resolutions, can directly be implemented in existing SPH methods, and, for non-adaptive simulations, provides a reduction in memory consumption as well as an up to 2× speedup relative to current particle-based boundary handling approaches.},
  archive      = {J_TOG},
  author       = {Rene Winchenbach and Rustam Akhunov and Andreas Kolb},
  doi          = {10.1145/3414685.3417829},
  journal      = {ACM Transactions on Graphics},
  month        = {11},
  number       = {6},
  pages        = {173:1–17},
  shortjournal = {ACM Trans. Graph.},
  title        = {Semi-analytic boundary handling below particle resolution for smoothed particle hydrodynamics},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Frequency-domain smoke guiding. <em>TOG</em>,
<em>39</em>(6), 172:1–10. (<a
href="https://doi.org/10.1145/3414685.3417842">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a simple and efficient method for guiding an Eulerian smoke simulation to match the behavior of a specified velocity field, such as a low-resolution animation of the same scene, while preserving the rich, turbulent details arising in the simulated fluid. Our method works by simply combining the high-frequency component of the simulated fluid velocity with the low-frequency component of the input guiding field. We show how to eliminate the grid-aligned artifacts that appear in naive guiding approaches, and provide a frequency-domain analysis that motivates the use of ideal low-pass and high-pass filters to prevent artificial dissipation of small-scale details. We demonstrate our method on many scenes including those with static and moving obstacles, and show that it produces high-quality results with very little computational overhead.},
  archive      = {J_TOG},
  author       = {Zahra Forootaninia and Rahul Narain},
  doi          = {10.1145/3414685.3417842},
  journal      = {ACM Transactions on Graphics},
  month        = {11},
  number       = {6},
  pages        = {172:1–10},
  shortjournal = {ACM Trans. Graph.},
  title        = {Frequency-domain smoke guiding},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). An adaptive staggered-tilted grid for incompressible flow
simulation. <em>TOG</em>, <em>39</em>(6), 171:1–15. (<a
href="https://doi.org/10.1145/3414685.3417837">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Enabling adaptivity on a uniform Cartesian grid is challenging due to its highly structured grid cells and axis-aligned grid lines. In this paper, we propose a new grid structure - the adaptive staggered-tilted (AST) grid - to conduct adaptive fluid simulations on a regular discretization. The key mechanics underpinning our new grid structure is to allow the emergence of a new set of tilted grid cells from the nodal positions on a background uniform grid. The original axis-aligned cells, in conjunction with the populated axis-tilted cells, jointly function as the geometric primitives to enable adaptivity on a regular spatial discretization. By controlling the states of the tilted cells both temporally and spatially, we can dynamically evolve the adaptive discretizations on an Eulerian domain. Our grid structure preserves almost all the computational merits of a uniform Cartesian grid, including the cache-coherent data layout, the easiness for parallelization, and the existence of high-performance numerical solvers. Further, our grid structure can be integrated into other adaptive grid structures, such as an Octree or a sparsely populated grid, to accommodate the T-junction-free hierarchy. We demonstrate the efficacy of our AST grid by showing examples of large-scale incompressible flow simulation in domains with irregular boundaries.},
  archive      = {J_TOG},
  author       = {Yuwei Xiao and Szeyu Chan and Siqi Wang and Bo Zhu and Xubo Yang},
  doi          = {10.1145/3414685.3417837},
  journal      = {ACM Transactions on Graphics},
  month        = {11},
  number       = {6},
  pages        = {171:1–15},
  shortjournal = {ACM Trans. Graph.},
  title        = {An adaptive staggered-tilted grid for incompressible flow simulation},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). RBF liquids: An adaptive PIC solver using RBF-FD.
<em>TOG</em>, <em>39</em>(6), 170:1–13. (<a
href="https://doi.org/10.1145/3414685.3417794">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce a novel liquid simulation approach that combines a spatially adaptive pressure projection solver with the Particle-in-Cell (PIC) method. The solver relies on a generalized version of the Finite Difference (FD) method to approximate the pressure field and its gradients in tree-based grid discretizations, possibly non-graded. In our approach, FD stencils are computed by using meshfree interpolations provided by a variant of Radial Basis Function (RBF), known as RBF-Finite-Difference (RBF-FD). This meshfree version of the FD produces differentiation weights on scattered nodes with high-order accuracy. Our method adapts a quadtree/octree dynamically in a narrow-band around the liquid interface, providing an adaptive particle sampling for the PIC advection step. Furthermore, RBF affords an accurate scheme for velocity transfer between the grid and particles, keeping the system&#39;s stability and avoiding numerical dissipation. We also present a data structure that connects the spatial subdivision of a quadtree/octree with the topology of its corresponding dual-graph. Our data structure makes the setup of stencils straightforward, allowing its updating without the need to rebuild it from scratch at each time-step. We show the effectiveness and accuracy of our solver by simulating incompressible inviscid fluids and comparing results with regular PIC-based solvers available in the literature.},
  archive      = {J_TOG},
  author       = {Rafael Nakanishi and Filipe Nascimento and Rafael Campos and Paulo Pagliosa and Afonso Paiva},
  doi          = {10.1145/3414685.3417794},
  journal      = {ACM Transactions on Graphics},
  month        = {11},
  number       = {6},
  pages        = {170:1–13},
  shortjournal = {ACM Trans. Graph.},
  title        = {RBF liquids: An adaptive PIC solver using RBF-FD},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). An extended cut-cell method for sub-grid liquids tracking
with surface tension. <em>TOG</em>, <em>39</em>(6), 169:1–13. (<a
href="https://doi.org/10.1145/3414685.3417859">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Simulating liquid phenomena utilizing Eulerian frameworks is challenging, since highly energetic flows often induce severe topological changes, creating thin and complex liquid surfaces. Thus, capturing structures that are small relative to the grid size become intractable, since continually increasing the resolution will scale sub-optimally due to the pressure projection step. Previous methods successfully relied on using higher resolution grids for tracking the liquid surface implicitly; however this technique comes with drawbacks. The mismatch of pressure samples and surface degrees of freedom will cause artifacts such as hanging blobs and permanent kinks at the liquid-air interface. In this paper, we propose an extended cut-cell method for handling liquid structures that are smaller than a grid cell. At the core of our method is a novel iso-surface Poisson Solver, which converges with second-order accuracy for pressure values while maintaining attractive discretization properties such as symmetric positive definiteness. Additionally, we extend the iso-surface assumption to be also compatible with surface tension forces. Our results show that the proposed method provides a novel framework for handling arbitrarily small splashes that can also correctly interact with objects embodied by complex geometries.},
  archive      = {J_TOG},
  author       = {Yi-Lu Chen and Jonathan Meier and Barbara Solenthaler and Vinicius C. Azevedo},
  doi          = {10.1145/3414685.3417859},
  journal      = {ACM Transactions on Graphics},
  month        = {11},
  number       = {6},
  pages        = {169:1–13},
  shortjournal = {ACM Trans. Graph.},
  title        = {An extended cut-cell method for sub-grid liquids tracking with surface tension},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Continuous curve textures. <em>TOG</em>, <em>39</em>(6),
168:1–16. (<a href="https://doi.org/10.1145/3414685.3417780">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Repetitive patterns are ubiquitous in natural and human-made objects, and can be created with a variety of tools and methods. Manual authoring provides unmatched degree of freedom and control, but can require significant artistic expertise and manual labor. Computational methods can automate parts of the manual creation process, but are mainly tailored for discrete pixels or elements instead of more general continuous structures. We propose an example-based method to synthesize continuous curve patterns from exemplars. Our main idea is to extend prior sample-based discrete element synthesis methods to consider not only sample positions (geometry) but also their connections (topology). Since continuous structures can exhibit higher complexity than discrete elements, we also propose robust, hierarchical synthesis to enhance output quality. Our algorithm can generate a variety of continuous curve patterns fully automatically. For further quality improvement and customization, we also present an autocomplete user interface to facilitate interactive creation and iterative editing. We evaluate our methods and interface via different patterns, ablation studies, and comparisons with alternative methods.},
  archive      = {J_TOG},
  author       = {Peihan Tu and Li-Yi Wei and Koji Yatani and Takeo Igarashi and Matthias Zwicker},
  doi          = {10.1145/3414685.3417780},
  journal      = {ACM Transactions on Graphics},
  month        = {11},
  number       = {6},
  pages        = {168:1–16},
  shortjournal = {ACM Trans. Graph.},
  title        = {Continuous curve textures},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Lifting freehand concept sketches into 3D. <em>TOG</em>,
<em>39</em>(6), 167:1–16. (<a
href="https://doi.org/10.1145/3414685.3417851">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present the first algorithm capable of automatically lifting real-world, vector-format, industrial design sketches into 3D. Targeting real-world sketches raises numerous challenges due to inaccuracies, use of overdrawn strokes, and construction lines. In particular, while construction lines convey important 3D information, they add significant clutter and introduce multiple accidental 2D intersections. Our algorithm exploits the geometric cues provided by the construction lines and lifts them to 3D by computing their intended 3D intersections and depths. Once lifted to 3D, these lines provide valuable geometric constraints that we leverage to infer the 3D shape of other artist drawn strokes. The core challenge we address is inferring the 3D connectivity of construction and other lines from their 2D projections by separating 2D intersections into 3D intersections and accidental occlusions. We efficiently address this complex combinatorial problem using a dedicated search algorithm that leverages observations about designer drawing pREFERENCES, and uses those to explore only the most likely solutions of the 3D intersection detection problem. We demonstrate that our separator outputs are of comparable quality to human annotations, and that the 3D structures we recover enable a range of design editing and visualization applications, including novel view synthesis and 3D-aware scaling of the depicted shape.},
  archive      = {J_TOG},
  author       = {Yulia Gryaditskaya and Felix Hähnlein and Chenxi Liu and Alla Sheffer and Adrien Bousseau},
  doi          = {10.1145/3414685.3417851},
  journal      = {ACM Transactions on Graphics},
  month        = {11},
  number       = {6},
  pages        = {167:1–16},
  shortjournal = {ACM Trans. Graph.},
  title        = {Lifting freehand concept sketches into 3D},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Pixelor: A competitive sketching AI agent. So you think you
can sketch? <em>TOG</em>, <em>39</em>(6), 166:1–15. (<a
href="https://doi.org/10.1145/3414685.3417840">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present the first competitive drawing agent Pixelor that exhibits humanlevel performance at a Pictionary-like sketching game, where the participant whose sketch is recognized first is a winner. Our AI agent can autonomously sketch a given visual concept, and achieve a recognizable rendition as quickly or faster than a human competitor. The key to victory for the agent&#39;s goal is to learn the optimal stroke sequencing strategies that generate the most recognizable and distinguishable strokes first. Training Pixelor is done in two steps. First, we infer the stroke order that maximizes early recognizability of human training sketches. Second, this order is used to supervise the training of a sequence-to-sequence stroke generator. Our key technical contributions are a tractable search of the exponential space of orderings using neural sorting; and an improved Seq2Seq Wasserstein (S2S-WAE) generator that uses an optimal-transport loss to accommodate the multi-modal nature of the optimal stroke distribution. Our analysis shows that Pixelor is better than the human players of the Quick, Draw! game, under both AI and human judging of early recognition. To analyze the impact of human competitors&#39; strategies, we conducted a further human study with participants being given unlimited thinking time and training in early recognizability by feedback from an AI judge. The study shows that humans do gradually improve their strategies with training, but overall Pixelor still matches human performance. The code and the dataset are available at http://sketchx.ai/pixelor.},
  archive      = {J_TOG},
  author       = {Ayan Kumar Bhunia and Ayan Das and Umar Riaz Muhammad and Yongxin Yang and Timothy M. Hospedales and Tao Xiang and Yulia Gryaditskaya and Yi-Zhe Song},
  doi          = {10.1145/3414685.3417840},
  journal      = {ACM Transactions on Graphics},
  month        = {11},
  number       = {6},
  pages        = {166:1–15},
  shortjournal = {ACM Trans. Graph.},
  title        = {Pixelor: A competitive sketching AI agent. so you think you can sketch?},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Interactive liquid splash modeling by user sketches.
<em>TOG</em>, <em>39</em>(6), 165:1–13. (<a
href="https://doi.org/10.1145/3414685.3417832">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Splashing is one of the most fascinating liquid phenomena in the real world and it is favored by artists to create stunning visual effects, both statically and dynamically. Unfortunately, the generation of complex and specialized liquid splashes is a challenging task and often requires considerable time and effort. In this paper, we present a novel system that synthesizes realistic liquid splashes from simple user sketch input. Our system adopts a conditional generative adversarial network (cGAN) trained with physics-based simulation data to produce raw liquid splash models from input sketches, and then applies model refinement processes to further improve their small-scale details. The system considers not only the trajectory of every user stroke, but also its speed, which makes the splash model simulation-ready with its underlying 3D flow. Compared with simulation-based modeling techniques through trials and errors, our system offers flexibility, convenience and intuition in liquid splash design and editing. We evaluate the usability and the efficiency of our system in an immersive virtual reality environment. Thanks to this system, an amateur user can now generate a variety of realistic liquid splashes in just a few minutes.},
  archive      = {J_TOG},
  author       = {Guowei Yan and Zhili Chen and Jimei Yang and Huamin Wang},
  doi          = {10.1145/3414685.3417832},
  journal      = {ACM Transactions on Graphics},
  month        = {11},
  number       = {6},
  pages        = {165:1–13},
  shortjournal = {ACM Trans. Graph.},
  title        = {Interactive liquid splash modeling by user sketches},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Sketch2CAD: Sequential CAD modeling by sketching in context.
<em>TOG</em>, <em>39</em>(6), 164:1–14. (<a
href="https://doi.org/10.1145/3414685.3417807">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a sketch-based CAD modeling system, where users create objects incrementally by sketching the desired shape edits, which our system automatically translates to CAD operations. Our approach is motivated by the close similarities between the steps industrial designers follow to draw 3D shapes, and the operations CAD modeling systems offer to create similar shapes. To overcome the strong ambiguity with parsing 2D sketches, we observe that in a sketching sequence, each step makes sense and can be interpreted in the context of what has been drawn before. In our system, this context corresponds to a partial CAD model, inferred in the previous steps, which we feed along with the input sketch to a deep neural network in charge of interpreting how the model should be modified by that sketch. Our deep network architecture then recognizes the intended CAD operation and segments the sketch accordingly, such that a subsequent optimization estimates the parameters of the operation that best fit the segmented sketch strokes. Since there exists no datasets of paired sketching and CAD modeling sequences, we train our system by generating synthetic sequences of CAD operations that we render as line drawings. We present a proof of concept realization of our algorithm supporting four frequently used CAD operations. Using our system, participants are able to quickly model a large and diverse set of objects, demonstrating Sketch2CAD to be an alternate way of interacting with current CAD modeling systems.},
  archive      = {J_TOG},
  author       = {Changjian Li and Hao Pan and Adrien Bousseau and Niloy J. Mitra},
  doi          = {10.1145/3414685.3417807},
  journal      = {ACM Transactions on Graphics},
  month        = {11},
  number       = {6},
  pages        = {164:1–14},
  shortjournal = {ACM Trans. Graph.},
  title        = {Sketch2CAD: Sequential CAD modeling by sketching in context},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A benchmark for rough sketch cleanup. <em>TOG</em>,
<em>39</em>(6), 163:1–14. (<a
href="https://doi.org/10.1145/3414685.3417784">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sketching is a foundational step in the design process. Decades of sketch processing research have produced algorithms for 3D shape interpretation, beautification, animation generation, colorization, etc. However, there is a mismatch between sketches created in the wild and the clean, sketch-like input required by these algorithms, preventing their adoption in practice. The recent flurry of sketch vectorization, simplification, and cleanup algorithms could be used to bridge this gap. However, they differ wildly in the assumptions they make on the input and output sketches. We present the first benchmark to evaluate and focus sketch cleanup research. Our dataset consists of 281 sketches obtained in the wild and a curated subset of 101 sketches. For this curated subset along with 40 sketches from previous work, we commissioned manual vectorizations and multiple ground truth cleaned versions by professional artists. The sketches span artistic and technical categories and were created by a variety of artists with different styles. Most sketches have Creative Commons licenses; the rest permit academic use. Our benchmark&#39;s metrics measure the similarity of automatically cleaned rough sketches to artist-created ground truth; the ambiguity and messiness of rough sketches; and low-level properties of the output parameterized curves. Our evaluation identifies shortcomings among state-of-the-art cleanup algorithms and discusses open problems for future research.},
  archive      = {J_TOG},
  author       = {Chuan Yan and David Vanderhaeghe and Yotam Gingold},
  doi          = {10.1145/3414685.3417784},
  journal      = {ACM Transactions on Graphics},
  month        = {11},
  number       = {6},
  pages        = {163:1–14},
  shortjournal = {ACM Trans. Graph.},
  title        = {A benchmark for rough sketch cleanup},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Example-based microstructure rendering with constant
storage. <em>TOG</em>, <em>39</em>(5), 162:1–12. (<a
href="https://doi.org/10.1145/3406836">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Rendering glinty details from specular microstructure enhances the level of realism, but previous methods require heavy storage for the high-resolution height field or normal map and associated acceleration structures. In this article, we aim at dynamically generating theoretically infinite microstructure, preventing obvious tiling artifacts, while achieving constant storage cost. Unlike traditional texture synthesis, our method supports arbitrary point and range queries, and is essentially generating the microstructure implicitly. Our method fits the widely used microfacet rendering framework with multiple importance sampling (MIS), replacing the commonly used microfacet normal distribution functions (NDFs) like ground glass distribution (GGX) by a detailed local solution, with a small amount of runtime performance overhead.},
  archive      = {J_TOG},
  author       = {Beibei Wang and Miloš Hašan and Nicolas Holzschuch and Ling-Qi Yan},
  doi          = {10.1145/3406836},
  journal      = {ACM Transactions on Graphics},
  month        = {8},
  number       = {5},
  pages        = {162:1–12},
  shortjournal = {ACM Trans. Graph.},
  title        = {Example-based microstructure rendering with constant storage},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Computational p-willmore flow with conformal penalty.
<em>TOG</em>, <em>39</em>(5), 161:1–16. (<a
href="https://doi.org/10.1145/3369387">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The unsigned p-Willmore functional introduced in the work of Mondino [2011] generalizes important geometric functionals, which measure the area and Willmore energy of immersed surfaces. Presently, techniques from the work of Dziuk [2008] are adapted to compute the first variation of this functional as a weak-form system of equations, which are subsequently used to develop a model for the p-Willmore flow of closed surfaces in R 3 . This model is amenable to constraints on surface area and enclosed volume and is shown to decrease the p-Willmore energy monotonically. In addition, a penalty-based regularization procedure is formulated to prevent artificial mesh degeneration along the flow; inspired by a conformality condition derived in the work of Kamberov et al. [1996], this procedure encourages angle-preservation in a closed and oriented surface immersion as it evolves. Following this, a finite-element discretization of both procedures is discussed, an algorithm for running the flow is given, and an application to mesh editing is presented.},
  archive      = {J_TOG},
  author       = {Anthony Gruber and Eugenio Aulisa},
  doi          = {10.1145/3369387},
  journal      = {ACM Transactions on Graphics},
  month        = {8},
  number       = {5},
  pages        = {161:1–16},
  shortjournal = {ACM Trans. Graph.},
  title        = {Computational p-willmore flow with conformal penalty},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A class of c2 interpolating splines. <em>TOG</em>,
<em>39</em>(5), 160:1–14. (<a
href="https://doi.org/10.1145/3400301">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a class of non-polynomial parametric splines that interpolate the given control points and show that some curve types in this class have a set of highly desirable properties that were not previously demonstrated for interpolating curves before. In particular, the formulation of this class guarantees that the resulting curves have C 2 continuity everywhere and local support, such that only four control points define each curve segment between consecutive control points. These properties are achieved directly due to the mathematical formulation used for defining this class, without the need for a global numerical optimization step. We also provide four example spline types within this class. These examples show how guaranteed self-intersection-free curve segments can be achieved, regardless of the placement of control points, which has been a limitation of prior interpolating curve formulations. In addition, they present how perfect circular arcs and linear segments can be formed by splines within this class, which also have been challenging for prior methods of interpolating curves.},
  archive      = {J_TOG},
  author       = {Cem Yuksel},
  doi          = {10.1145/3400301},
  journal      = {ACM Transactions on Graphics},
  month        = {8},
  number       = {5},
  pages        = {160:1–14},
  shortjournal = {ACM Trans. Graph.},
  title        = {A class of c2 interpolating splines},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Capturing subjective first-person view shots with drones for
automated cinematography. <em>TOG</em>, <em>39</em>(5), 159:1–14. (<a
href="https://doi.org/10.1145/3378673">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose an approach to capture subjective first-person view (FPV) videos by drones for automated cinematography. FPV shots are intentionally not smooth to increase the level of immersion for the audience, and are usually captured by a walking camera operator holding traditional camera equipment. Our goal is to automatically control a drone in such a way that it imitates the motion dynamics of a walking camera operator, and, in turn, capture FPV videos. For this, given a user-defined camera path, orientation, and velocity, we first present a method to automatically generate the operator’s motion pattern and the associated motion of the camera, considering the damping mechanism of the camera equipment. Second, we propose a general computational approach that generates the drone commands to imitate the desired motion pattern. We express this task as a constrained optimization problem, where we aim to fulfill high-level user-defined goals, while imitating the dynamics of the walking camera operator and taking the drone’s physical constraints into account. Our approach is fully automatic, runs in real time, and is interactive, which provides artistic freedom in designing shots. It does not require a motion capture system, and works both indoors and outdoors. The validity of our approach has been confirmed via quantitative and qualitative evaluations.},
  archive      = {J_TOG},
  author       = {Amirsaman Ashtari and Stefan Stevšić and Tobias Nägeli and Jean-Charles Bazin and Otmar Hilliges},
  doi          = {10.1145/3378673},
  journal      = {ACM Transactions on Graphics},
  month        = {8},
  number       = {5},
  pages        = {159:1–14},
  shortjournal = {ACM Trans. Graph.},
  title        = {Capturing subjective first-person view shots with drones for automated cinematography},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Noise-resilient reconstruction of panoramas and 3D scenes
using robot-mounted unsynchronized commodity RGB-d cameras.
<em>TOG</em>, <em>39</em>(5), 152:1–15. (<a
href="https://doi.org/10.1145/3389412">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a two-stage approach to first constructing 3D panoramas and then stitching them for noise-resilient reconstruction of large-scale indoor scenes. Our approach requires multiple unsynchronized RGB-D cameras, mounted on a robot platform, which can perform in-place rotations at different locations in a scene. Such cameras rotate on a common (but unknown) axis, which provides a novel perspective for coping with unsynchronized cameras, without requiring sufficient overlap of their Field-of-View (FoV). Based on this key observation, we propose novel algorithms to track these cameras simultaneously. Furthermore, during the integration of raw frames onto an equirectangular panorama, we derive uncertainty estimates from multiple measurements assigned to the same pixels. This enables us to appropriately model the sensing noise and consider its influence, so as to achieve better noise resilience, and improve the geometric quality of each panorama and the accuracy of global inter-panorama registration. We evaluate and demonstrate the performance of our proposed method for enhancing the geometric quality of scene reconstruction from both real-world and synthetic scans.},
  archive      = {J_TOG},
  author       = {Sheng Yang and Beichen Li and Yan-Pei Cao and Hongbo Fu and Yu-Kun Lai and Leif Kobbelt and Shi-Min Hu},
  doi          = {10.1145/3389412},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {5},
  pages        = {152:1–15},
  shortjournal = {ACM Trans. Graph.},
  title        = {Noise-resilient reconstruction of panoramas and 3D scenes using robot-mounted unsynchronized commodity RGB-D cameras},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). 3D morphable face models—past, present, and future.
<em>TOG</em>, <em>39</em>(5), 157:1–38. (<a
href="https://doi.org/10.1145/3395208">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we provide a detailed survey of 3D Morphable Face Models over the 20 years since they were first proposed. The challenges in building and applying these models, namely, capture, modeling, image formation, and image analysis, are still active research topics, and we review the state-of-the-art in each of these areas. We also look ahead, identifying unsolved challenges, proposing directions for future research, and highlighting the broad range of current and future applications.},
  archive      = {J_TOG},
  author       = {Bernhard Egger and William A. P. Smith and Ayush Tewari and Stefanie Wuhrer and Michael Zollhoefer and Thabo Beeler and Florian Bernard and Timo Bolkart and Adam Kortylewski and Sami Romdhani and Christian Theobalt and Volker Blanz and Thomas Vetter},
  doi          = {10.1145/3395208},
  journal      = {ACM Transactions on Graphics},
  month        = {6},
  number       = {5},
  pages        = {157:1–38},
  shortjournal = {ACM Trans. Graph.},
  title        = {3D morphable face Models—Past, present, and future},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Kinetic shape reconstruction. <em>TOG</em>, <em>39</em>(5),
156:1–14. (<a href="https://doi.org/10.1145/3376918">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Converting point clouds into concise polygonal meshes in an automated manner is an enduring problem in computer graphics. Prior works, which typically operate by assembling planar shapes detected from input points, largely overlooked the scalability issue of processing a large number of shapes. As a result, they tend to produce overly simplified meshes with assembling approaches that can hardly digest more than 100 shapes in practice. We propose a shape assembling mechanism that is at least one order of magnitude more efficient, both in time and in number of processed shapes. Our key idea relies upon the design of a kinetic data structure for partitioning the space into convex polyhedra. Instead of slicing all the planar shapes exhaustively as prior methods, we create a partition where shapes grow at constant speed until colliding and forming polyhedra. This simple idea produces a lighter yet meaningful partition with a lower algorithmic complexity than an exhaustive partition. A watertight polygonal mesh is then extracted from the partition with a min-cut formulation. We demonstrate the robustness and efficacy of our algorithm on a variety of objects and scenes in terms of complexity, size, and acquisition characteristics. In particular, we show the method can both faithfully represent piecewise planar structures and approximate freeform objects while offering high resilience to occlusions and missing data.},
  archive      = {J_TOG},
  author       = {Jean-Philippe Bauchet and Florent Lafarge},
  doi          = {10.1145/3376918},
  journal      = {ACM Transactions on Graphics},
  month        = {6},
  number       = {5},
  pages        = {156:1–14},
  shortjournal = {ACM Trans. Graph.},
  title        = {Kinetic shape reconstruction},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Inverse procedural modeling of branching structures by
inferring l-systems. <em>TOG</em>, <em>39</em>(5), 155:1–13. (<a
href="https://doi.org/10.1145/3394105">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce an inverse procedural modeling approach that learns L-system representations of pixel images with branching structures. Our fully automatic model generates a compact set of textual rewriting rules that describe the input. We use deep learning to discover atomic structures such as line segments or branchings. Orientation and scaling of these structures are determined and the detected structures are combined into a tree. The initial representation is analyzed, and repeating parts are encoded into a small grammar by using greedy optimization while the user can control the size of the detected rules. The output is an L-system that represents the input image as a simple text and a set of terminal symbols. We apply our approach to a variety of examples, demonstrate its robustness against noise and blur, and we show that it can detect user sketches and complex input structures.},
  archive      = {J_TOG},
  author       = {Jianwei Guo and Haiyong Jiang and Bedrich Benes and Oliver Deussen and Xiaopeng Zhang and Dani Lischinski and Hui Huang},
  doi          = {10.1145/3394105},
  journal      = {ACM Transactions on Graphics},
  month        = {6},
  number       = {5},
  pages        = {155:1–13},
  shortjournal = {ACM Trans. Graph.},
  title        = {Inverse procedural modeling of branching structures by inferring L-systems},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Reinforcement of general shell structures. <em>TOG</em>,
<em>39</em>(5), 153:1–19. (<a
href="https://doi.org/10.1145/3375677">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce an efficient method for designing shell reinforcements of minimal weight. Inspired by classical Michell trusses, we create a reinforcement layout whose members are aligned with optimal stress directions, then optimize their shape minimizing the volume while keeping stresses bounded. We exploit two predominant techniques for reinforcing shells: adding ribs aligned with stress directions and using thicker walls on regions of high stress. Most previous work can generate either only ribs or only variable-thickness walls. However, in the general case, neither approach by itself will provide optimal solutions. By using a more precise volume model, our method is capable of producing optimized structures with the full range of qualitative behaviors: from ribs to walls and smoothly transitioning in between. Our method includes new algorithms for determining the layout of reinforcement structure elements, and an efficient algorithm to optimize their shape, minimizing a non-linear non-convex functional at a fraction of the cost and with better optimality compared to standard solvers. We demonstrate the optimization results for a variety of shapes and the improvements it yields in the strength of 3D-printed objects.},
  archive      = {J_TOG},
  author       = {Francisca Gil-Ureta and Nico Pietroni and Denis Zorin},
  doi          = {10.1145/3375677},
  journal      = {ACM Transactions on Graphics},
  month        = {6},
  number       = {5},
  pages        = {153:1–19},
  shortjournal = {ACM Trans. Graph.},
  title        = {Reinforcement of general shell structures},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Unsupervised detection of distinctive regions on 3D shapes.
<em>TOG</em>, <em>39</em>(5), 158:1–14. (<a
href="https://doi.org/10.1145/3366785">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article presents a novel approach to learn and detect distinctive regions on 3D shapes. Unlike previous works, which require labeled data, our method is unsupervised. We conduct the analysis on point sets sampled from 3D shapes, then formulate and train a deep neural network for an unsupervised shape clustering task to learn local and global features for distinguishing shapes with respect to a given shape set. To drive the network to learn in an unsupervised manner, we design a clustering-based nonparametric softmax classifier with an iterative re-clustering of shapes, and an adapted contrastive loss for enhancing the feature embedding quality and stabilizing the learning process. By then, we encourage the network to learn the point distinctiveness on the input shapes. We extensively evaluate various aspects of our approach and present its applications for distinctiveness-guided shape retrieval, sampling, and view selection in 3D scenes.},
  archive      = {J_TOG},
  author       = {Xianzhi Li and Lequan Yu and Chi-Wing Fu and Daniel Cohen-Or and Pheng-Ann Heng},
  doi          = {10.1145/3366785},
  journal      = {ACM Transactions on Graphics},
  month        = {5},
  number       = {5},
  pages        = {158:1–14},
  shortjournal = {ACM Trans. Graph.},
  title        = {Unsupervised detection of distinctive regions on 3D shapes},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Enhanced interactive 360° viewing via automatic guidance.
<em>TOG</em>, <em>39</em>(5), 154:1–15. (<a
href="https://doi.org/10.1145/3183794">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a new interactive playback method to enhance 360° viewing experiences. Our method automatically rotates the virtual camera of a 360°  panoramic video (360° video) player during interactive viewing to guide the viewer through the most important regions of the video. With this method, the viewer can watch a 360° video with minimum efforts to find important events in a scene both in interactive (e.g., HMD) and less-interactive (e.g., PC and TV) viewing environments. To estimate the importance of each viewing direction, we combine spatial and temporal saliency with cluster-based weighting. A maximum backward cumulative importance volume (MBCIV) is then constructed by accumulating this importance in the video space. During playback, which uses a forward tracing scheme through the MBCIV, the initial optimal path is found based on the viewer’s viewing direction. A smooth path is then derived using penalized curve fitting. Finally, the virtual camera is rotated to follow the path. The experiments and user studies demonstrate that our method allows the viewer to effectively enjoy 360° videos with minimum interaction efforts, or even through a non-interactive display.},
  archive      = {J_TOG},
  author       = {Seunghoon Cha and Jungjin Lee and Seunghwa Jeong and Younghui Kim and Junyong Noh},
  doi          = {10.1145/3183794},
  journal      = {ACM Transactions on Graphics},
  month        = {5},
  number       = {5},
  pages        = {154:1–15},
  shortjournal = {ACM Trans. Graph.},
  title        = {Enhanced interactive 360° viewing via automatic guidance},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Variance-aware path guiding. <em>TOG</em>, <em>39</em>(4),
151:151:1–151:12. (<a
href="https://doi.org/10.1145/3386569.3392441">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Path guiding is a promising tool to improve the performance of path tracing algorithms. However, not much research has investigated what target densities a guiding method should strive to learn for optimal performance. Instead, most previous work pursues the zero-variance goal: The local decisions are guided under the assumption that all other decisions along the random walk will be sampled perfectly. In practice, however, many decisions are poorly guided, or not guided at all. Furthermore, learned distributions are often marginalized, e.g., by neglecting the BSDF. We present a generic procedure to derive theoretically optimal target densities for local path guiding. These densities account for variance in nested estimators, and marginalize provably well over, e.g., the BSDF. We apply our theory in two state-of-the-art rendering applications: a path guiding solution for unidirectional path tracing [Müller et al. 2017] and a guiding method for light source selection for the many lights problem [Vévoda et al. 2018]. In both cases, we observe significant improvements, especially on glossy surfaces. The implementations for both applications consist of trivial modifications to the original code base, without introducing any additional overhead.},
  archive      = {J_TOG},
  author       = {Alexander Rath and Pascal Grittmann and Sebastian Herholz and Petr Vévoda and Philipp Slusallek and Jaroslav Křivánek},
  doi          = {10.1145/3386569.3392441},
  journal      = {ACM Transactions on Graphics},
  month        = {8},
  number       = {4},
  pages        = {151:151:1–151:12},
  shortjournal = {ACM Trans. Graph.},
  title        = {Variance-aware path guiding},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). The design and evolution of the UberBake light baking
system. <em>TOG</em>, <em>39</em>(4), 150:150:1–150:13. (<a
href="https://doi.org/10.1145/3386569.3392394">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We describe the design and evolution of UberBake, a global illumination system developed by Activision, which supports limited lighting changes in response to certain player interactions. Instead of relying on a fully dynamic solution, we use a traditional static light baking pipeline and extend it with a small set of features that allow us to dynamically update the precomputed lighting at run-time with minimal performance and memory overhead. This means that our system works on the complete set of target hardware, ranging from high-end PCs to previous generation gaming consoles, allowing the use of lighting changes for gameplay purposes. In particular, we show how to efficiently precompute lighting changes due to individual lights being enabled and disabled and doors opening and closing. Finally, we provide a detailed performance evaluation of our system using a set of production levels and discuss how to extend its dynamic capabilities in the future.},
  archive      = {J_TOG},
  author       = {Dario Seyb and Peter-Pike Sloan and Ari Silvennoinen and Michał Iwanicki and Wojciech Jarosz},
  doi          = {10.1145/3386569.3392394},
  journal      = {ACM Transactions on Graphics},
  month        = {8},
  number       = {4},
  pages        = {150:150:1–150:13},
  shortjournal = {ACM Trans. Graph.},
  title        = {The design and evolution of the UberBake light baking system},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Specular manifold sampling for rendering high-frequency
caustics and glints. <em>TOG</em>, <em>39</em>(4), 149:149:1–149:15. (<a
href="https://doi.org/10.1145/3386569.3392408">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Scattering from specular surfaces produces complex optical effects that are frequently encountered in realistic scenes: intricate caustics due to focused reflection, multiple refraction, and high-frequency glints from specular microstructure. Yet, despite their importance and considerable research to this end, sampling of light paths that cause these effects remains a formidable challenge. In this article, we propose a surprisingly simple and general sampling strategy for specular light paths including the above examples, unifying the previously disjoint areas of caustic and glint rendering into a single framework. Given two path vertices, our algorithm stochastically finds a specular subpath connecting the endpoints. In contrast to prior work, our method supports high-frequency normal- or displacement-mapped geometry, samples specular-diffuse-specular (&quot;SDS&quot;) paths, and is compatible with standard Monte Carlo methods including unidirectional path tracing. Both unbiased and biased variants of our approach can be constructed, the latter often significantly reducing variance, which may be appealing in applied settings (e.g. visual effects). We demonstrate our method on a range of challenging scenes and evaluate it against state-of-the-art methods for rendering caustics and glints.},
  archive      = {J_TOG},
  author       = {Tizian Zeltner and Iliyan Georgiev and Wenzel Jakob},
  doi          = {10.1145/3386569.3392408},
  journal      = {ACM Transactions on Graphics},
  month        = {8},
  number       = {4},
  pages        = {149:149:1–149:15},
  shortjournal = {ACM Trans. Graph.},
  title        = {Specular manifold sampling for rendering high-frequency caustics and glints},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Spatiotemporal reservoir resampling for real-time ray
tracing with dynamic direct lighting. <em>TOG</em>, <em>39</em>(4),
148:148:1–148:17. (<a
href="https://doi.org/10.1145/3386569.3392481">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Efficiently rendering direct lighting from millions of dynamic light sources using Monte Carlo integration remains a challenging problem, even for off-line rendering systems. We introduce a new algorithm---ReSTIR---that renders such lighting interactively, at high quality, and without needing to maintain complex data structures. We repeatedly resample a set of candidate light samples and apply further spatial and temporal resampling to leverage information from relevant nearby samples. We derive an unbiased Monte Carlo estimator for this approach, and show that it achieves equal-error 6×-60× faster than state-of-the-art methods. A biased estimator reduces noise further and is 35×-65× faster, at the cost of some energy loss. We implemented our approach on the GPU, rendering complex scenes containing up to 3.4 million dynamic, emissive triangles in under 50 ms per frame while tracing at most 8 rays per pixel.},
  archive      = {J_TOG},
  author       = {Benedikt Bitterli and Chris Wyman and Matt Pharr and Peter Shirley and Aaron Lefohn and Wojciech Jarosz},
  doi          = {10.1145/3386569.3392481},
  journal      = {ACM Transactions on Graphics},
  month        = {8},
  number       = {4},
  pages        = {148:148:1–148:17},
  shortjournal = {ACM Trans. Graph.},
  title        = {Spatiotemporal reservoir resampling for real-time ray tracing with dynamic direct lighting},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Robust fitting of parallax-aware mixtures for path guiding.
<em>TOG</em>, <em>39</em>(4), 147:147:1–147:15. (<a
href="https://doi.org/10.1145/3386569.3392421">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Effective local light transport guiding demands for high quality guiding information, i.e., a precise representation of the directional incident radiance distribution at every point inside the scene. We introduce a parallax-aware distribution model based on parametric mixtures. By parallax-aware warping of the distribution, the local approximation of the 5D radiance field remains valid and precise across large spatial regions, even for close-by contributors. Our robust optimization scheme fits parametric mixtures to radiance samples collected in previous rendering passes. Robustness is achieved by splitting and merging of components refining the mixture. These splitting and merging decisions minimize and bound the expected variance of the local radiance estimator. In addition, we extend the fitting scheme to a robust, iterative update method, which allows for incremental training of our model using smaller sample batches. This results in more frequent training updates and, at the same time, significantly reduces the required sample memory footprint. The parametric representation of our model allows for the application of advanced importance sampling methods such as radiance-based, cosine-aware, and even product importance sampling. Our method further smoothly integrates next-event estimation (NEE) into path guiding, avoiding importance sampling of contributions better covered by NEE. The proposed robust fitting and update scheme, in combination with the parallax-aware representation, results in faster learning and lower variance compared to state-of-the-art path guiding approaches.},
  archive      = {J_TOG},
  author       = {Lukas Ruppert and Sebastian Herholz and Hendrik P. A. Lensch},
  doi          = {10.1145/3386569.3392421},
  journal      = {ACM Transactions on Graphics},
  month        = {8},
  number       = {4},
  pages        = {147:147:1–147:15},
  shortjournal = {ACM Trans. Graph.},
  title        = {Robust fitting of parallax-aware mixtures for path guiding},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Radiative backpropagation: An adjoint method for
lightning-fast differentiable rendering. <em>TOG</em>, <em>39</em>(4),
146:146:1–146:15. (<a
href="https://doi.org/10.1145/3386569.3392406">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Physically based differentiable rendering has recently evolved into a powerful tool for solving inverse problems involving light. Methods in this area perform a differentiable simulation of the physical process of light transport and scattering to estimate partial derivatives relating scene parameters to pixels in the rendered image. Together with gradient-based optimization, such algorithms have interesting applications in diverse disciplines, e.g., to improve the reconstruction of 3D scenes, while accounting for interreflection and transparency, or to design meta-materials with specified optical properties. The most versatile differentiable rendering algorithms rely on reverse-mode differentiation to compute all requested derivatives at once, enabling optimization of scene descriptions with millions of free parameters. However, a severe limitation of the reverse-mode approach is that it requires a detailed transcript of the computation that is subsequently replayed to back-propagate derivatives to the scene parameters. The transcript of typical renderings is extremely large, exceeding the available system memory by many orders of magnitude, hence current methods are limited to simple scenes rendered at low resolutions and sample counts. We introduce radiative backpropagation , a fundamentally different approach to differentiable rendering that does not require a transcript, greatly improving its scalability and efficiency. Our main insight is that reverse-mode propagation through a rendering algorithm can be interpreted as the solution of a continuous transport problem involving the partial derivative of radiance with respect to the optimization objective. This quantity is &quot;emitted&quot; by sensors, &quot;scattered&quot; by the scene, and eventually &quot;received&quot; by objects with differentiable parameters. Differentiable rendering then decomposes into two separate primal and adjoint simulation steps that scale to complex scenes rendered at high resolutions. We also investigated biased variants of this algorithm and find that they considerably improve both runtime and convergence speed. We showcase an efficient GPU implementation of radiative backpropagation and compare its performance and the quality of its gradients to prior work.},
  archive      = {J_TOG},
  author       = {Merlin Nimier-David and Sébastien Speierer and Benoît Ruiz and Wenzel Jakob},
  doi          = {10.1145/3386569.3392406},
  journal      = {ACM Transactions on Graphics},
  month        = {8},
  number       = {4},
  pages        = {146:146:1–146:15},
  shortjournal = {ACM Trans. Graph.},
  title        = {Radiative backpropagation: An adjoint method for lightning-fast differentiable rendering},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Polar stroking: New theory and methods for stroking paths.
<em>TOG</em>, <em>39</em>(4), 145:145:1–145:15. (<a
href="https://doi.org/10.1145/3386569.3392458">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Stroking and filling are the two basic rendering operations on paths in vector graphics. The theory of filling a path is well-understood in terms of contour integrals and winding numbers, but when path rendering standards specify stroking, they resort to the analogy of painting pixels with a brush that traces the outline of the path. This means important standards such as PDF, SVG, and PostScript lack a rigorous way to say what samples are inside or outside a stroked path. Our work fills this gap with a principled theory of stroking. Guided by our theory, we develop a novel polar stroking method to render stroked paths robustly with an intuitive way to bound the tessellation error without needing recursion. Because polar stroking guarantees small uniform steps in tangent angle, it provides an efficient way to accumulate arc length along a path for texturing or dashing. While this paper focuses on developing the theory of our polar stroking method, we have successfully implemented our methods on modern programmable GPUs.},
  archive      = {J_TOG},
  author       = {Mark J. Kilgard},
  doi          = {10.1145/3386569.3392458},
  journal      = {ACM Transactions on Graphics},
  month        = {8},
  number       = {4},
  pages        = {145:145:1–145:15},
  shortjournal = {ACM Trans. Graph.},
  title        = {Polar stroking: New theory and methods for stroking paths},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Penrose: From mathematical notation to beautiful diagrams.
<em>TOG</em>, <em>39</em>(4), 144:144:1–144:16. (<a
href="https://doi.org/10.1145/3386569.3392375">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce a system called Penrose for creating mathematical diagrams. Its basic functionality is to translate abstract statements written in familiar math-like notation into one or more possible visual representations. Rather than rely on a fixed library of visualization tools, the visual representation is user-defined in a constraint-based specification language; diagrams are then generated automatically via constrained numerical optimization. The system is user-extensible to many domains of mathematics, and is fast enough for iterative design exploration. In contrast to tools that specify diagrams via direct manipulation or low-level graphics programming, Penrose enables rapid creation and exploration of diagrams that faithfully preserve the underlying mathematical meaning. We demonstrate the effectiveness and generality of the system by showing how it can be used to illustrate a diverse set of concepts from mathematics and computer graphics.},
  archive      = {J_TOG},
  author       = {Katherine Ye and Wode Ni and Max Krieger and Dor Ma&#39;ayan and Jenna Wise and Jonathan Aldrich and Joshua Sunshine and Keenan Crane},
  doi          = {10.1145/3386569.3392375},
  journal      = {ACM Transactions on Graphics},
  month        = {8},
  number       = {4},
  pages        = {144:144:1–144:16},
  shortjournal = {ACM Trans. Graph.},
  title        = {Penrose: From mathematical notation to beautiful diagrams},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Path-space differentiable rendering. <em>TOG</em>,
<em>39</em>(4), 143:143:1–143:19. (<a
href="https://doi.org/10.1145/3386569.3392383">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Physics-based differentiable rendering, the estimation of derivatives of radiometric measures with respect to arbitrary scene parameters, has a diverse array of applications from solving analysis-by-synthesis problems to training machine learning pipelines incorporating forward rendering processes. Unfortunately, general-purpose differentiable rendering remains challenging due to the lack of efficient estimators as well as the need to identify and handle complex discontinuities such as visibility boundaries. In this paper, we show how path integrals can be differentiated with respect to arbitrary differentiable changes of a scene. We provide a detailed theoretical analysis of this process and establish new differentiable rendering formulations based on the resulting differential path integrals. Our path-space differentiable rendering formulation allows the design of new Monte Carlo estimators that offer significantly better efficiency than state-of-the-art methods in handling complex geometric discontinuities and light transport phenomena such as caustics. We validate our method by comparing our derivative estimates to those generated using the finite-difference method. To demonstrate the effectiveness of our technique, we compare inverse-rendering performance with a few state-of-the-art differentiable rendering methods.},
  archive      = {J_TOG},
  author       = {Cheng Zhang and Bailey Miller and Kai Yan and Ioannis Gkioulekas and Shuang Zhao},
  doi          = {10.1145/3386569.3392383},
  journal      = {ACM Transactions on Graphics},
  month        = {8},
  number       = {4},
  pages        = {143:143:1–143:19},
  shortjournal = {ACM Trans. Graph.},
  title        = {Path-space differentiable rendering},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Neural supersampling for real-time rendering. <em>TOG</em>,
<em>39</em>(4), 142:142:1–142:12. (<a
href="https://doi.org/10.1145/3386569.3392376">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to higher resolutions and refresh rates, as well as more photorealistic effects, real-time rendering has become increasingly challenging for video games and emerging virtual reality headsets. To meet this demand, modern graphics hardware and game engines often reduce the computational cost by rendering at a lower resolution and then upsampling to the native resolution. Following the recent advances in image and video superresolution in computer vision, we propose a machine learning approach that is specifically tailored for high-quality upsampling of rendered content in real-time applications. The main insight of our work is that in rendered content, the image pixels are point-sampled, but precise temporal dynamics are available. Our method combines this specific information that is typically available in modern renderers (i.e., depth and dense motion vectors) with a novel temporal network design that takes into account such specifics and is aimed at maximizing video quality while delivering real-time performance. By training on a large synthetic dataset rendered from multiple 3D scenes with recorded camera motion, we demonstrate high fidelity and temporally stable results in real-time, even in the highly challenging 4 × 4 upsampling scenario, significantly outperforming existing superresolution and temporal antialiasing work.},
  archive      = {J_TOG},
  author       = {Lei Xiao and Salah Nouri and Matt Chapman and Alexander Fix and Douglas Lanman and Anton Kaplanyan},
  doi          = {10.1145/3386569.3392376},
  journal      = {ACM Transactions on Graphics},
  month        = {8},
  number       = {4},
  pages        = {142:142:1–142:12},
  shortjournal = {ACM Trans. Graph.},
  title        = {Neural supersampling for real-time rendering},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Massively parallel rendering of complex closed-form implicit
surfaces. <em>TOG</em>, <em>39</em>(4), 141:141:1–141:10. (<a
href="https://doi.org/10.1145/3386569.3392429">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a new method for directly rendering complex closed-form implicit surfaces on modern GPUs, taking advantage of their massive parallelism. Our model representation is unambiguously solid, can be sampled at arbitrary resolution, and supports both constructive solid geometry (CSG) and more unusual modeling operations (e.g. smooth blending of shapes). The rendering strategy scales to large-scale models with thousands of arithmetic operations in their underlying mathematical expressions. Our method only requires C 0 continuity, allowing for warping and blending operations which break Lipshitz continuity. To render a model, its underlying expression is evaluated in a shallow hierarchy of spatial regions, using a high branching factor for efficient parallelization. Interval arithmetic is used to both skip empty regions and construct reduced versions of the expression. The latter is the optimization that makes our algorithm practical: in one benchmark, expression complexity decreases by two orders of magnitude between the original and reduced expressions. Similar algorithms exist in the literature, but tend to be deeply recursive with heterogeneous workloads in each branch, which makes them GPU-unfriendly; our evaluation and expression reduction both run efficiently as massively parallel algorithms, entirely on the GPU. The resulting system renders complex implicit surfaces in high resolution and at interactive speeds. We examine how performance scales with computing power, presenting performance results on hardware ranging from older laptops to modern data-center GPUs, and showing significant improvements at each stage.},
  archive      = {J_TOG},
  author       = {Matthew J. Keeter},
  doi          = {10.1145/3386569.3392429},
  journal      = {ACM Transactions on Graphics},
  month        = {8},
  number       = {4},
  pages        = {141:141:1–141:10},
  shortjournal = {ACM Trans. Graph.},
  title        = {Massively parallel rendering of complex closed-form implicit surfaces},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Langevin monte carlo rendering with gradient-based
adaptation. <em>TOG</em>, <em>39</em>(4), 140:140:1–140:16. (<a
href="https://doi.org/10.1145/3386569.3392382">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce a suite of Langevin Monte Carlo algorithms for efficient photorealistic rendering of scenes with complex light transport effects, such as caustics, interreflections, and occlusions. Our algorithms operate in primary sample space, and use the Metropolis-adjusted Langevin algorithm (MALA) to generate new samples. Drawing inspiration from state-of-the-art stochastic gradient descent procedures, we combine MALA with adaptive preconditioning and momentum schemes that re-use previously-computed first-order gradients, either in an online or in a cache-driven fashion. This combination allows MALA to adapt to the local geometry of the primary sample space, without the computational overhead associated with previous Hessian-based adaptation algorithms. We use the theory of controlled Markov chain Monte Carlo to ensure that these combinations remain ergodic, and are therefore suitable for unbiased Monte Carlo rendering. Through extensive experiments, we show that our algorithms, MALA with online and cache-driven adaptation, can successfully handle complex light transport in a large variety of scenes, leading to improved performance (on average more than 3× variance reduction at equal time, and 7× for motion blur) compared to state-of-the-art Markov chain Monte Carlo rendering algorithms.},
  archive      = {J_TOG},
  author       = {Fujun Luan and Shuang Zhao and Kavita Bala and Ioannis Gkioulekas},
  doi          = {10.1145/3386569.3392382},
  journal      = {ACM Transactions on Graphics},
  month        = {8},
  number       = {4},
  pages        = {140:140:1–140:16},
  shortjournal = {ACM Trans. Graph.},
  title        = {Langevin monte carlo rendering with gradient-based adaptation},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Image-based acquisition and modeling of polarimetric
reflectance. <em>TOG</em>, <em>39</em>(4), 139:139:1–139:14. (<a
href="https://doi.org/10.1145/3386569.3392387">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Realistic modeling of the bidirectional reflectance distribution function (BRDF) of scene objects is a vital prerequisite for any type of physically based rendering. In the last decades, the availability of databases containing real-world material measurements has fueled considerable innovation in the development of such models. However, previous work in this area was mainly focused on increasing the visual realism of images, and hence ignored the effect of scattering on the polarization state of light, which is normally imperceptible to the human eye. Existing databases thus only capture scattered flux, or polarimetric BRDF datasets are too directionally sparse (e.g., in-plane) to be usable for simulation. While subtle to human observers, polarization is easily perceived by any optical sensor (e.g., using polarizing filters), providing a wealth of additional information about shape and material properties of the object under observation. Given the increasing application of rendering in the solution of inverse problems via analysis-by-synthesis and differentiation, the ability to realistically model polarized radiative transport is thus highly desirable. Polarization depends on the wavelength of the spectrum, and thus we provide the first polarimetric BRDF (pBRDF) dataset that captures the polarimetric properties of real-world materials over the full angular domain, and at multiple wavelengths. Acquisition of such reflectance data is challenging due to the extremely large space of angular, spectral, and polarimetric configurations that must be observed, and we propose a scheme combining image-based acquisition with spectroscopic ellipsometry to perform measurements in a realistic amount of time. This process yields raw Mueller matrices, which we subsequently transform into Rusinkiewicz-parameterized pBRDFs that can be used for rendering. Our dataset provides 25 isotropic pBRDFs spanning a wide range of appearances: diffuse/specular, metallic/dielectric, rough/smooth, and different color albedos, captured in five wavelength ranges covering the visible spectrum. We demonstrate usage of our data-driven pBRDF model in a physically based renderer that accounts for polarized interreflection, and we investigate the relationship of polarization and material appearance, providing insights into the behavior of characteristic real-world pBRDFs.},
  archive      = {J_TOG},
  author       = {Seung-Hwan Baek and Tizian Zeltner and Hyun Jin Ku and Inseung Hwang and Xin Tong and Wenzel Jakob and Min H. Kim},
  doi          = {10.1145/3386569.3392387},
  journal      = {ACM Transactions on Graphics},
  month        = {8},
  number       = {4},
  pages        = {139:139:1–139:14},
  shortjournal = {ACM Trans. Graph.},
  title        = {Image-based acquisition and modeling of polarimetric reflectance},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). GS-PAT: High-speed multi-point sound-fields for phased
arrays of transducers. <em>TOG</em>, <em>39</em>(4), 138:138:1–138:12.
(<a href="https://doi.org/10.1145/3386569.3392492">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Phased Arrays of Transducers (PATs) allow accurate control of ultrasound fields, with applications in haptics, levitation (i.e. displays) and parametric audio. However, algorithms for multi-point levitation or tactile feedback are usually limited to computing solutions in the order of hundreds of sound-fields per second, preventing the use of multiple high-speed points, a feature that can broaden the scope of applications of PATs. We present GS-PAT , a GPU multi-point phase retrieval algorithm, capable of computing 17K solutions per second for up to 32 simultaneous points in a mid-end consumer grade GPU (NVidia GTX 1660). We describe the algorithm and compare it to state of the art multi-point algorithms used for ultrasound haptics and levitation, showing similar quality of the generated sound-fields, and much higher computation rates. We then illustrate how the shift in paradigm enabled by GS-PAT (i.e. real-time control of several high-speed points) opens new applications for PAT technologies, such as in volumetric fully coloured displays, multi-point spatio-temporal tactile feedback, parametric audio and simultaneous combinations of these modalities.},
  archive      = {J_TOG},
  author       = {Diego Martinez Plasencia and Ryuji Hirayama and Roberto Montano-Murillo and Sriram Subramanian},
  doi          = {10.1145/3386569.3392492},
  journal      = {ACM Transactions on Graphics},
  month        = {8},
  number       = {4},
  pages        = {138:138:1–138:12},
  shortjournal = {ACM Trans. Graph.},
  title        = {GS-PAT: High-speed multi-point sound-fields for phased arrays of transducers},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Converting stroked primitives to filled primitives.
<em>TOG</em>, <em>39</em>(4), 137:137:1–137:17. (<a
href="https://doi.org/10.1145/3386569.3392392">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Vector graphics formats offer support for both filled and stroked primitives. Filled primitives paint all points in the region bounded by a set of outlines. Stroked primitives paint all points covered by a line drawn over the outlines. Editors allow users to convert stroked primitives to the outlines of equivalent filled primitives for further editing. Likewise, renderers typically convert stroked primitives to equivalent filled primitives prior to rendering. This conversion problem is deceivingly difficult to solve. Surprisingly, it has received little to no attention in the literature. Existing implementations output too many segments, do not satisfy accuracy requirements, or fail under a variety of conditions, often spectacularly. In this paper, we present a solution to the stroke-to-fill conversion problem that addresses these issues. One of our key insights is to take into account the evolutes of input outlines, in addition to their offsets, in regions of high curvature. Furthermore, our approach strives to maintain continuity between the input and the set of painted points. Our implementation is available in open source.},
  archive      = {J_TOG},
  author       = {Diego Nehab},
  doi          = {10.1145/3386569.3392392},
  journal      = {ACM Transactions on Graphics},
  month        = {8},
  number       = {4},
  pages        = {137:137:1–137:17},
  shortjournal = {ACM Trans. Graph.},
  title        = {Converting stroked primitives to filled primitives},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Continuous multiple importance sampling. <em>TOG</em>,
<em>39</em>(4), 136:136:1–136:12. (<a
href="https://doi.org/10.1145/3386569.3392436">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multiple importance sampling (MIS) is a provably good way to combine a finite set of sampling techniques to reduce variance in Monte Carlo integral estimation. However, there exist integration problems for which a continuum of sampling techniques is available. To handle such cases we establish a continuous MIS (CMIS) formulation as a generalization of MIS to uncountably infinite sets of techniques. Our formulation is equipped with a base estimator that is coupled with a provably optimal balance heuristic and a practical stochastic MIS (SMIS) estimator that makes CMIS accessible to a broad range of problems. To illustrate the effectiveness and utility of our framework, we apply it to three different light transport applications, showing improved performance over the prior state-of-the-art techniques.},
  archive      = {J_TOG},
  author       = {Rex West and Iliyan Georgiev and Adrien Gruson and Toshiya Hachisuka},
  doi          = {10.1145/3386569.3392436},
  journal      = {ACM Transactions on Graphics},
  month        = {8},
  number       = {4},
  pages        = {136:136:1–136:12},
  shortjournal = {ACM Trans. Graph.},
  title        = {Continuous multiple importance sampling},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Compositional neural scene representations for shading
inference. <em>TOG</em>, <em>39</em>(4), 135:135:1–135:13. (<a
href="https://doi.org/10.1145/3386569.3392475">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a technique for adaptively partitioning neural scene representations. Our method disentangles lighting, material, and geometric information yielding a scene representation that preserves the orthogonality of these components, improves interpretability of the model, and allows compositing new scenes by mixing components of existing ones. The proposed adaptive partitioning respects the uneven entropy of individual components and permits compressing the scene representation to lower its memory footprint and potentially reduce the evaluation cost of the model. Furthermore, the partitioned representation enables an in-depth analysis of existing image generators. We compare the flow of information through individual partitions, and by contrasting it to the impact of additional inputs (G-buffer), we are able to identify the roots of undesired visual artifacts, and propose one possible solution to remedy the poor performance. We also demonstrate the benefits of complementing traditional forward renderers by neural representations and synthesis, e.g. to infer expensive shading effects, and show how these could improve production rendering in the future if developed further.},
  archive      = {J_TOG},
  author       = {Jonathan Granskog and Fabrice Rousselle and Marios Papas and Jan Novák},
  doi          = {10.1145/3386569.3392475},
  journal      = {ACM Transactions on Graphics},
  month        = {8},
  number       = {4},
  pages        = {135:135:1–135:13},
  shortjournal = {ACM Trans. Graph.},
  title        = {Compositional neural scene representations for shading inference},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Analytic spherical harmonic gradients for real-time
rendering with many polygonal area lights. <em>TOG</em>, <em>39</em>(4),
134:134:1–134:14. (<a
href="https://doi.org/10.1145/3386569.3392373">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent work has developed analytic formulae for spherical harmonic (SH) coefficients from uniform polygonal lights, enabling near-field area lights to be included in Precomputed Radiance Transfer (PRT) systems, and in offline rendering. However, the method is inefficient since coefficients need to be recomputed at each vertex or shading point, for each light, even though the SH coefficients vary smoothly in space. The complexity scales linearly with the number of lights, making many-light rendering difficult. In this paper, we develop a novel analytic formula for the spatial gradients of the spherical harmonic coefficients for uniform polygonal area lights. The result is a significant generalization, involving the Reynolds transport theorem to reduce the problem to a boundary integral for which we derive a new analytic formula, showing how to reduce a key term to an earlier recurrence for SH coefficients. The implementation requires only minor additions to existing code for SH coefficients. The results also hold implications for recent efforts on differentiable rendering. We show that SH gradients enable very sparse spatial sampling, followed by accurate Hermite interpolation. This enables scaling PRT to hundreds of area lights with minimal overhead and real-time frame rates. Moreover, the SH gradient formula is a new mathematical result that potentially enables many other graphics applications.},
  archive      = {J_TOG},
  author       = {Lifan Wu and Guangyan Cai and Shuang Zhao and Ravi Ramamoorthi},
  doi          = {10.1145/3386569.3392373},
  journal      = {ACM Transactions on Graphics},
  month        = {8},
  number       = {4},
  pages        = {134:134:1–134:14},
  shortjournal = {ACM Trans. Graph.},
  title        = {Analytic spherical harmonic gradients for real-time rendering with many polygonal area lights},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A perceptual model of motion quality for rendering with
adaptive refresh-rate and resolution. <em>TOG</em>, <em>39</em>(4),
133:133:1–133:17. (<a
href="https://doi.org/10.1145/3386569.3392411">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Limited GPU performance budgets and transmission bandwidths mean that real-time rendering often has to compromise on the spatial resolution or temporal resolution (refresh rate). A common practice is to keep either the resolution or the refresh rate constant and dynamically control the other variable. But this strategy is non-optimal when the velocity of displayed content varies. To find the best trade-off between the spatial resolution and refresh rate, we propose a perceptual visual model that predicts the quality of motion given an object velocity and predictability of motion. The model considers two motion artifacts to establish an overall quality score: non-smooth (juddery) motion, and blur. Blur is modeled as a combined effect of eye motion, finite refresh rate and display resolution. To fit the free parameters of the proposed visual model, we measured eye movement for predictable and unpredictable motion, and conducted psychophysical experiments to measure the quality of motion from 50 Hz to 165 Hz. We demonstrate the utility of the model with our on-the-fly motion-adaptive rendering algorithm that adjusts the refresh rate of a G-Sync-capable monitor based on a given rendering budget and observed object motion. Our psychophysical validation experiments demonstrate that the proposed algorithm performs better than constant-refresh-rate solutions, showing that motion-adaptive rendering is an attractive technique for driving variable-refresh-rate displays.},
  archive      = {J_TOG},
  author       = {Gyorgy Denes and Akshay Jindal and Aliaksei Mikhailiuk and Rafał K. Mantiuk},
  doi          = {10.1145/3386569.3392411},
  journal      = {ACM Transactions on Graphics},
  month        = {8},
  number       = {4},
  pages        = {133:133:1–133:17},
  shortjournal = {ACM Trans. Graph.},
  title        = {A perceptual model of motion quality for rendering with adaptive refresh-rate and resolution},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Vid2Curve: Simultaneous camera motion estimation and thin
structure reconstruction from an RGB video. <em>TOG</em>,
<em>39</em>(4), 132:132:1–132:12. (<a
href="https://doi.org/10.1145/3386569.3392476">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Thin structures, such as wire-frame sculptures, fences, cables, power lines, and tree branches, are common in the real world. It is extremely challenging to acquire their 3D digital models using traditional image-based or depth-based reconstruction methods, because thin structures often lack distinct point features and have severe self-occlusion. We propose the first approach that simultaneously estimates camera motion and reconstructs the geometry of complex 3D thin structures in high quality from a color video captured by a handheld camera. Specifically, we present a new curve-based approach to estimate accurate camera poses by establishing correspondences between featureless thin objects in the foreground in consecutive video frames, without requiring visual texture in the background scene to lock on. Enabled by this effective curve-based camera pose estimation strategy, we develop an iterative optimization method with tailored measures on geometry, topology as well as self-occlusion handling for reconstructing 3D thin structures. Extensive validations on a variety of thin structures show that our method achieves accurate camera pose estimation and faithful reconstruction of 3D thin structures with complex shape and topology at a level that has not been attained by other existing reconstruction methods.},
  archive      = {J_TOG},
  author       = {Peng Wang and Lingjie Liu and Nenglun Chen and Hung-Kuo Chu and Christian Theobalt and Wenping Wang},
  doi          = {10.1145/3386569.3392476},
  journal      = {ACM Transactions on Graphics},
  month        = {8},
  number       = {4},
  pages        = {132:132:1–132:12},
  shortjournal = {ACM Trans. Graph.},
  title        = {Vid2Curve: Simultaneous camera motion estimation and thin structure reconstruction from an RGB video},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Variable-width contouring for additive manufacturing.
<em>TOG</em>, <em>39</em>(4), 131:131:1–131:17. (<a
href="https://doi.org/10.1145/3386569.3392448">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In most layered additive manufacturing processes, a tool solidifies or deposits material while following pre-planned trajectories to form solid beads. Many interesting problems arise in this context, among which one concerns the planning of trajectories for filling a planar shape as densely as possible. This is the problem we tackle in the present paper. Recent works have shown that allowing the bead width to vary along the trajectories helps increase the filling density. We present a novel technique that, given a deposition width range, constructs a set of closed beads whose width varies within the prescribed range and fill the input shape. The technique outperforms the state of the art in important metrics: filling density (while still guaranteeing the absence of bead overlap) and trajectories smoothness. We give a detailed geometric description of our algorithm, explore its behavior on example inputs and provide a statistical comparison with the state of the art. We show that it is possible to obtain high quality fabricated layers on commodity FDM printers.},
  archive      = {J_TOG},
  author       = {Samuel Hornus and Tim Kuipers and Olivier Devillers and Monique Teillaud and Jonàs Martínez and Marc Glisse and Sylvain Lazard and Sylvain Lefebvre},
  doi          = {10.1145/3386569.3392448},
  journal      = {ACM Transactions on Graphics},
  month        = {8},
  number       = {4},
  pages        = {131:131:1–131:17},
  shortjournal = {ACM Trans. Graph.},
  title        = {Variable-width contouring for additive manufacturing},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Uncertainty quantification for multi-scan registration.
<em>TOG</em>, <em>39</em>(4), 130:130:1–130:24. (<a
href="https://doi.org/10.1145/3386569.3392402">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A fundamental problem in scan-based 3D reconstruction is to align the depth scans under different camera poses into the same coordinate system. While there are abundant algorithms on aligning depth scans, few methods have focused on assessing the quality of a solution. This quality checking problem is vital, as we need to determine whether the current scans are sufficient or not and where to install additional scans to improve the reconstruction. On the other hand, this problem is fundamentally challenging because the underlying ground-truth is generally unavailable, and it is challenging to predict alignment errors such as global drifts manually. In this paper, we introduce a local uncertainty framework for geometric alignment algorithms. Our approach enjoys several appealing properties, such as it does not require re-sampling the input, no need for the underlying ground-truth, informative, and high computational efficiency. We apply this framework to two multi-scan alignment formulations, one minimizes geometric distances between pairs of scans, and another simultaneously aligns the input scans with a deforming model. The output of our approach can be seamlessly integrated with view selection, enabling uncertainty-aware view planning. Experimental results and user studies justify the effectiveness of our approach on both synthetic and real datasets.},
  archive      = {J_TOG},
  author       = {Xiangru Huang and Zhenxiao Liang and Qixing Huang},
  doi          = {10.1145/3386569.3392402},
  journal      = {ACM Transactions on Graphics},
  month        = {8},
  number       = {4},
  pages        = {130:130:1–130:24},
  shortjournal = {ACM Trans. Graph.},
  title        = {Uncertainty quantification for multi-scan registration},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). TilinGNN: Learning to tile with self-supervised graph neural
network. <em>TOG</em>, <em>39</em>(4), 129:129:1–129:16. (<a
href="https://doi.org/10.1145/3386569.3392380">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce the first neural optimization framework to solve a classical instance of the tiling problem. Namely, we seek a non-periodic tiling of an arbitrary 2D shape using one or more types of tiles---the tiles maximally fill the shape&#39;s interior without overlaps or holes. To start, we reformulate tiling as a graph problem by modeling candidate tile locations in the target shape as graph nodes and connectivity between tile locations as edges. Further, we build a graph convolutional neural network , coined TilinGNN, to progressively propagate and aggregate features over graph edges and predict tile placements. TilinGNN is trained by maximizing the tiling coverage on target shapes, while avoiding overlaps and holes between the tiles. Importantly, our network is self-supervised , as we articulate these criteria as loss terms defined on the network outputs, without the need of ground-truth tiling solutions. After training, the runtime of TilinGNN is roughly linear to the number of candidate tile locations, significantly outperforming traditional combinatorial search. We conducted various experiments on a variety of shapes to showcase the speed and versatility of TilinGNN. We also present comparisons to alternative methods and manual solutions, robustness analysis, and ablation studies to demonstrate the quality of our approach.},
  archive      = {J_TOG},
  author       = {Hao Xu and Ka-Hei Hui and Chi-Wing Fu and Hao Zhang},
  doi          = {10.1145/3386569.3392380},
  journal      = {ACM Transactions on Graphics},
  month        = {8},
  number       = {4},
  pages        = {129:129:1–129:16},
  shortjournal = {ACM Trans. Graph.},
  title        = {TilinGNN: Learning to tile with self-supervised graph neural network},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Quad-mesh based isometric mappings and developable surfaces.
<em>TOG</em>, <em>39</em>(4), 128:128:1–128:13. (<a
href="https://doi.org/10.1145/3386569.3392430">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We discretize isometric mappings between surfaces as correspondences between checkerboard patterns derived from quad meshes. This method captures the degrees of freedom inherent in smooth isometries and enables a natural definition of discrete developable surfaces. This definition, which is remarkably simple, leads to a class of discrete developables which is much more flexible in applications than previous concepts of discrete developables. In this paper, we employ optimization to efficiently compute isometric mappings, conformal mappings and isometric bending of surfaces. We perform geometric modeling of developables, including cutting, gluing and folding. The discrete mappings presented here have applications in both theory and practice: We propose a theory of curvatures derived from a discrete Gauss map as well as a construction of watertight CAD models consisting of developable spline surfaces.},
  archive      = {J_TOG},
  author       = {Caigui Jiang and Cheng Wang and Florian Rist and Johannes Wallner and Helmut Pottmann},
  doi          = {10.1145/3386569.3392430},
  journal      = {ACM Transactions on Graphics},
  month        = {8},
  number       = {4},
  pages        = {128:128:1–128:13},
  shortjournal = {ACM Trans. Graph.},
  title        = {Quad-mesh based isometric mappings and developable surfaces},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Principal symmetric meshes. <em>TOG</em>, <em>39</em>(4),
127:127:1–127:17. (<a
href="https://doi.org/10.1145/3386569.3392446">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The isolines of principal symmetric surface parametrizations run symmetrically to the principal directions. We describe two discrete versions of these special nets/quad meshes which are dual to each other and show their usefulness for various applications in the context of fabrication and architectural design. Our discretization of a principal symmetric mesh comes naturally with a family of spheres, the so-called Meusnier and Mannheim spheres. In our representation of principal symmetric meshes, we have direct control over the radii of theses spheres and the intersection angles of the parameter lines. This facilitates tasks such as generating Weingarten surfaces including constant mean curvature surfaces and minimal surfaces. We illustrate the potential of Weingarten surfaces for paneling doubly curved freeform facades by significantly reducing the number of necessary molds. Moreover, we have direct access to curvature adaptive tool paths for cylindrical CNC milling with circular edges as well as flank milling with rotational cones. Furthermore, the construction of curved support structures from congruent circular strips is easily managed by constant sphere radii. The underlying families of spheres are in a natural way discrete curvature spheres in analogy to smooth Möbius and Laguerre geometry which further leads to a novel discrete curvature theory for principal symmetric meshes.},
  archive      = {J_TOG},
  author       = {Davide Pellis and Hui Wang and Martin Kilian and Florian Rist and Helmut Pottmann and Christian Müller},
  doi          = {10.1145/3386569.3392446},
  journal      = {ACM Transactions on Graphics},
  month        = {8},
  number       = {4},
  pages        = {127:127:1–127:17},
  shortjournal = {ACM Trans. Graph.},
  title        = {Principal symmetric meshes},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Point2Mesh: A self-prior for deformable meshes.
<em>TOG</em>, <em>39</em>(4), 126:126:1–126:12. (<a
href="https://doi.org/10.1145/3386569.3392415">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we introduce Point2Mesh , a technique for reconstructing a surface mesh from an input point cloud. Instead of explicitly specifying a prior that encodes the expected shape properties, the prior is defined automatically using the input point cloud, which we refer to as a self-prior. The self-prior encapsulates reoccurring geometric repetitions from a single shape within the weights of a deep neural network. We optimize the network weights to deform an initial mesh to shrink-wrap a single input point cloud. This explicitly considers the entire reconstructed shape, since shared local kernels are calculated to fit the overall object. The convolutional kernels are optimized globally across the entire shape, which inherently encourages local-scale geometric self-similarity across the shape surface. We show that shrink-wrapping a point cloud with a self-prior converges to a desirable solution; compared to a prescribed smoothness prior, which often becomes trapped in undesirable local minima. While the performance of traditional reconstruction approaches degrades in non-ideal conditions that are often present in real world scanning, i.e. , unoriented normals, noise and missing (low density) parts, Point2Mesh is robust to non-ideal conditions. We demonstrate the performance of Point2Mesh on a large variety of shapes with varying complexity.},
  archive      = {J_TOG},
  author       = {Rana Hanocka and Gal Metzer and Raja Giryes and Daniel Cohen-Or},
  doi          = {10.1145/3386569.3392415},
  journal      = {ACM Transactions on Graphics},
  month        = {8},
  number       = {4},
  pages        = {126:126:1–126:12},
  shortjournal = {ACM Trans. Graph.},
  title        = {Point2Mesh: A self-prior for deformable meshes},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). On elastic geodesic grids and their planar to spatial
deployment. <em>TOG</em>, <em>39</em>(4), 125:125:1–125:12. (<a
href="https://doi.org/10.1145/3386569.3392490">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a novel type of planar-to-spatial deployable structures that we call elastic geodesic grids. Our approach aims at the approximation of freeform surfaces with spatial grids of bent lamellas which can be deployed from a planar configuration using a simple kinematic mechanism. Such elastic structures are easy-to-fabricate and easy-to-deploy and approximate shapes which combine physics and aesthetics. We propose a solution based on networks of geodesic curves on target surfaces and we introduce a set of conditions and assumptions which can be closely met in practice. Our formulation allows for a purely geometric approach which avoids the necessity of numerical shape optimization by building on top of theoretical insights from differential geometry. We propose a solution for the design, computation, and physical simulation of elastic geodesic grids, and present several fabricated small-scale examples with varying complexity. Moreover, we provide an empirical proof of our method by comparing the results to laser-scans of the fabricated models. Our method is intended as a form-finding tool for elastic gridshells in architecture and other creative disciplines and should give the designer an easy-to-handle way for the exploration of such structures.},
  archive      = {J_TOG},
  author       = {Stefan Pillwein and Kurt Leimer and Michael Birsak and Przemyslaw Musialski},
  doi          = {10.1145/3386569.3392490},
  journal      = {ACM Transactions on Graphics},
  month        = {8},
  number       = {4},
  pages        = {125:125:1–125:12},
  shortjournal = {ACM Trans. Graph.},
  title        = {On elastic geodesic grids and their planar to spatial deployment},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Neural subdivision. <em>TOG</em>, <em>39</em>(4),
124:124:1–124:16. (<a
href="https://doi.org/10.1145/3386569.3392418">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper introduces Neural Subdivision , a novel framework for data-driven coarse-to-fine geometry modeling. During inference, our method takes a coarse triangle mesh as input and recursively subdivides it to a finer geometry by applying the fixed topological updates of Loop Subdivision, but predicting vertex positions using a neural network conditioned on the local geometry of a patch. This approach enables us to learn complex non-linear subdivision schemes, beyond simple linear averaging used in classical techniques. One of our key contributions is a novel self-supervised training setup that only requires a set of high-resolution meshes for learning network weights. For any training shape, we stochastically generate diverse low-resolution discretizations of coarse counterparts, while maintaining a bijective mapping that prescribes the exact target position of every new vertex during the subdivision process. This leads to a very efficient and accurate loss function for conditional mesh generation, and enables us to train a method that generalizes across discretizations and favors preserving the manifold structure of the output. During training we optimize for the same set of network weights across all local mesh patches, thus providing an architecture that is not constrained to a specific input mesh, fixed genus, or category. Our network encodes patch geometry in a local frame in a rotation- and translation-invariant manner. Jointly, these design choices enable our method to generalize well, and we demonstrate that even when trained on a single high-resolution mesh our method generates reasonable subdivisions for novel shapes.},
  archive      = {J_TOG},
  author       = {Hsueh-Ti Derek Liu and Vladimir G. Kim and Siddhartha Chaudhuri and Noam Aigerman and Alec Jacobson},
  doi          = {10.1145/3386569.3392418},
  journal      = {ACM Transactions on Graphics},
  month        = {8},
  number       = {4},
  pages        = {124:124:1–124:16},
  shortjournal = {ACM Trans. Graph.},
  title        = {Neural subdivision},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Monte carlo geometry processing: A grid-free approach to
PDE-based methods on volumetric domains. <em>TOG</em>, <em>39</em>(4),
123:123:1–123:18. (<a
href="https://doi.org/10.1145/3386569.3392374">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper explores how core problems in PDE-based geometry processing can be efficiently and reliably solved via grid-free Monte Carlo methods. Modern geometric algorithms often need to solve Poisson-like equations on geometrically intricate domains. Conventional methods most often mesh the domain, which is both challenging and expensive for geometry with fine details or imperfections (holes, self-intersections, etc. ). In contrast, grid-free Monte Carlo methods avoid mesh generation entirely, and instead just evaluate closest point queries. They hence do not discretize space, time, nor even function spaces, and provide the exact solution (in expectation) even on extremely challenging models. More broadly, they share many benefits with Monte Carlo methods from photorealistic rendering: excellent scaling, trivial parallel implementation, view-dependent evaluation, and the ability to work with any kind of geometry (including implicit or procedural descriptions). We develop a complete &quot;black box&quot; solver that encompasses integration, variance reduction, and visualization, and explore how it can be used for various geometry processing tasks. In particular, we consider several fundamental linear elliptic PDEs with constant coefficients on solid regions of R n. Overall we find that Monte Carlo methods significantly broaden the horizons of geometry processing, since they easily handle problems of size and complexity that are essentially hopeless for conventional methods.},
  archive      = {J_TOG},
  author       = {Rohan Sawhney and Keenan Crane},
  doi          = {10.1145/3386569.3392374},
  journal      = {ACM Transactions on Graphics},
  month        = {8},
  number       = {4},
  pages        = {123:123:1–123:18},
  shortjournal = {ACM Trans. Graph.},
  title        = {Monte carlo geometry processing: A grid-free approach to PDE-based methods on volumetric domains},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). MGCN: Descriptor learning using multiscale GCNs.
<em>TOG</em>, <em>39</em>(4), 122:122:1–122:15. (<a
href="https://doi.org/10.1145/3386569.3392443">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a novel framework for computing descriptors for characterizing points on three-dimensional surfaces. First, we present a new non-learned feature that uses graph wavelets to decompose the Dirichlet energy on a surface. We call this new feature Wavelet Energy Decomposition Signature (WEDS). Second, we propose a new Multiscale Graph Convolutional Network (MGCN) to transform a non-learned feature to a more discriminative descriptor. Our results show that the new descriptor WEDS is more discriminative than the current state-of-the-art non-learned descriptors and that the combination of WEDS and MGCN is better than the state-of-the-art learned descriptors. An important design criterion for our descriptor is the robustness to different surface discretizations including triangulations with varying numbers of vertices. Our results demonstrate that previous graph convolutional networks significantly overfit to a particular resolution or even a particular triangulation, but MGCN generalizes well to different surface discretizations. In addition, MGCN is compatible with previous descriptors and it can also be used to improve the performance of other descriptors, such as the heat kernel signature, the wave kernel signature, or the local point signature.},
  archive      = {J_TOG},
  author       = {Yiqun Wang and Jing Ren and Dong-Ming Yan and Jianwei Guo and Xiaopeng Zhang and Peter Wonka},
  doi          = {10.1145/3386569.3392443},
  journal      = {ACM Transactions on Graphics},
  month        = {8},
  number       = {4},
  pages        = {122:122:1–122:15},
  shortjournal = {ACM Trans. Graph.},
  title        = {MGCN: Descriptor learning using multiscale GCNs},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). LoopyCuts: Practical feature-preserving block decomposition
for strongly hex-dominant meshing. <em>TOG</em>, <em>39</em>(4),
121:121:1–121:17. (<a
href="https://doi.org/10.1145/3386569.3392472">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a new fully automatic block-decomposition algorithm for feature-preserving, strongly hex-dominant meshing, that yields results with a drastically larger percentage of hex elements than prior art. Our method is guided by a surface field that conforms to both surface curvature and feature lines, and exploits an ordered set of cutting loops that evenly cover the input surface, defining an arrangement of loops suitable for hex-element generation. We decompose the solid into coarse blocks by iteratively cutting it with surfaces bounded by these loops. The vast majority of the obtained blocks can be turned into hexahedral cells via simple midpoint subdivision. Our method produces pure hexahedral meshes in approximately 80\% of the cases, and hex-dominant meshes with less than 2\% non-hexahedral cells in the remaining cases. We demonstrate the robustness of our method on 70+ models, including CAD objects with features of various complexity, organic and synthetic shapes, and provide extensive comparisons to prior art, demonstrating its superiority.},
  archive      = {J_TOG},
  author       = {Marco Livesu and Nico Pietroni and Enrico Puppo and Alla Sheffer and Paolo Cignoni},
  doi          = {10.1145/3386569.3392472},
  journal      = {ACM Transactions on Graphics},
  month        = {8},
  number       = {4},
  pages        = {121:121:1–121:17},
  shortjournal = {ACM Trans. Graph.},
  title        = {LoopyCuts: Practical feature-preserving block decomposition for strongly hex-dominant meshing},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Lifting simplices to find injectivity. <em>TOG</em>,
<em>39</em>(4), 120:120:1–120:17. (<a
href="https://doi.org/10.1145/3386569.3392484">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mapping a source mesh into a target domain while preserving local injectivity is an important but highly non-trivial task. Existing methods either require an already-injective starting configuration, which is often not available, or rely on sophisticated solving schemes. We propose a novel energy form, called Total Lifted Content (TLC), that is equipped with theoretical properties desirable for injectivity optimization. By lifting the simplices of the mesh into a higher dimension and measuring their contents (2D area or 3D volume) there, TLC is smooth over the entire embedding space and its global minima are always injective. The energy is simple to minimize using standard gradient-based solvers. Our method achieved 100\% success rate on an extensive benchmark of embedding problems for triangular and tetrahedral meshes, on which existing methods only have varied success.},
  archive      = {J_TOG},
  author       = {Xingyi Du and Noam Aigerman and Qingnan Zhou and Shahar Z. Kovalsky and Yajie Yan and Danny M. Kaufman and Tao Ju},
  doi          = {10.1145/3386569.3392484},
  journal      = {ACM Transactions on Graphics},
  month        = {8},
  number       = {4},
  pages        = {120:120:1–120:17},
  shortjournal = {ACM Trans. Graph.},
  title        = {Lifting simplices to find injectivity},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Inter-surface maps via constant-curvature metrics.
<em>TOG</em>, <em>39</em>(4), 119:119:1–119:15. (<a
href="https://doi.org/10.1145/3386569.3392399">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a novel approach to represent maps between two discrete surfaces of the same genus and to minimize intrinsic mapping distortion. Our maps are well-defined at every surface point and are guaranteed to be continuous bijections (surface homeomorphisms). As a key feature of our approach, only the images of vertices need to be represented explicitly, since the images of all other points (on edges or in faces) are properly defined implicitly. This definition is via unique geodesics in metrics of constant Gaussian curvature. Our method is built upon the fact that such metrics exist on surfaces of arbitrary topology, without the need for any cuts or cones (as asserted by the uniformization theorem). Depending on the surfaces&#39; genus, these metrics exhibit one of the three classical geometries: Euclidean, spherical or hyperbolic. Our formulation handles constructions in all three geometries in a unified way. In addition, by considering not only the vertex images but also the discrete metric as degrees of freedom, our formulation enables us to simultaneously optimize the images of these vertices and images of all other points.},
  archive      = {J_TOG},
  author       = {Patrick Schmidt and Marcel Campen and Janis Born and Leif Kobbelt},
  doi          = {10.1145/3386569.3392399},
  journal      = {ACM Transactions on Graphics},
  month        = {8},
  number       = {4},
  pages        = {119:119:1–119:15},
  shortjournal = {ACM Trans. Graph.},
  title        = {Inter-surface maps via constant-curvature metrics},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Graph2Plan: Learning floorplan generation from layout
graphs. <em>TOG</em>, <em>39</em>(4), 118:118:1–118:14. (<a
href="https://doi.org/10.1145/3386569.3392391">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce a learning framework for automated floorplan generation which combines generative modeling using deep neural networks and user-in-the-loop designs to enable human users to provide sparse design constraints. Such constraints are represented by a layout graph. The core component of our learning framework is a deep neural network, Graph2Plan, which converts a layout graph, along with a building boundary, into a floorplan that fulfills both the layout and boundary constraints. Given an input building boundary, we allow a user to specify room counts and other layout constraints, which are used to retrieve a set of floorplans, with their associated layout graphs, from a database. For each retrieved layout graph, along with the input boundary, Graph2Plan first generates a corresponding raster floorplan image, and then a refined set of boxes representing the rooms. Graph2Plan is trained on RPLAN, a large-scale dataset consisting of 80K annotated floorplans. The network is mainly based on convolutional processing over both the layout graph, via a graph neural network (GNN), and the input building boundary, as well as the raster floorplan images, via conventional image convolution. We demonstrate the quality and versatility of our floorplan generation framework in terms of its ability to cater to different user inputs. We conduct both qualitative and quantitative evaluations, ablation studies, and comparisons with state-of-the-art approaches.},
  archive      = {J_TOG},
  author       = {Ruizhen Hu and Zeyu Huang and Yuhan Tang and Oliver Van Kaick and Hao Zhang and Hui Huang},
  doi          = {10.1145/3386569.3392391},
  journal      = {ACM Transactions on Graphics},
  month        = {8},
  number       = {4},
  pages        = {118:118:1–118:14},
  shortjournal = {ACM Trans. Graph.},
  title        = {Graph2Plan: Learning floorplan generation from layout graphs},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Fast tetrahedral meshing in the wild. <em>TOG</em>,
<em>39</em>(4), 117:117:1–117:18. (<a
href="https://doi.org/10.1145/3386569.3392385">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a new tetrahedral meshing method, fTetWild, to convert triangle soups into high-quality tetrahedral meshes. Our method builds on the TetWild algorithm, replacing the rational triangle insertion with a new incremental approach to construct and optimize the output mesh, interleaving triangle insertion and mesh optimization. Our approach makes it possible to maintain a valid floating-point tetrahedral mesh at all algorithmic stages, eliminating the need for costly constructions with rational numbers used by TetWild, while maintaining full robustness and similar output quality. This allows us to improve on TetWild in two ways. First, our algorithm is significantly faster, with running time comparable to less robust Delaunay-based tetrahedralization algorithms. Second, our algorithm is guaranteed to produce a valid tetrahedral mesh with floating-point vertex coordinates, while TetWild produces a valid mesh with rational coordinates which is not guaranteed to be valid after floating-point conversion. As a trade-off, our algorithm no longer guarantees that all input triangles are present in the output mesh, but in practice, as confirmed by our tests on the Thingi10k dataset, the algorithm always succeeds in inserting all input triangles.},
  archive      = {J_TOG},
  author       = {Yixin Hu and Teseo Schneider and Bolun Wang and Denis Zorin and Daniele Panozzo},
  doi          = {10.1145/3386569.3392385},
  journal      = {ACM Transactions on Graphics},
  month        = {8},
  number       = {4},
  pages        = {117:117:1–117:18},
  shortjournal = {ACM Trans. Graph.},
  title        = {Fast tetrahedral meshing in the wild},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Fabrication-in-the-loop co-optimization of surfaces and
styli for drawing haptics. <em>TOG</em>, <em>39</em>(4),
116:116:1–116:16. (<a
href="https://doi.org/10.1145/3386569.3392467">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Digital drawing tools are now standard in art and design workflows. These tools offer comfort, portability, and precision as well as native integration with digital-art workflows, software, and tools. At the same time, artists continue to work with long-standing, traditional drawing tools. One feature of traditional tools, well-appreciated by many artists and lacking in digital tools, is the specific and diverse range of haptic responses provided by them. Haptic feedback in traditional drawing tools provides unique, per-tool responses that help determine the precision and character of individual strokes. In this work, we address the problem of fabricating digital drawing tools that closely match the haptic feedback of their traditional counterparts. This requires the formulation and solution of a complex, co-optimization of both digital styli and the drawing surfaces they move upon. Here, a potentially direct formulation of this optimization with numerical simulation-in-the-loop is not yet viable. As in many complex design tasks, state-of-the-art methods do not currently offer predictive modeling at rates and scales that can account for the numerous, coupled, physical behaviors governing the haptics of styli and surfaces, nor for the limitations and uncertainties inherent in their fabrication processes. To address these challenges, we propose fabrication-in-the-loop optimization. Critical to making this strategy practical we construct our objective via a Gaussian Process that does not require computing derivatives with respect to design parameters. Our Gaussian Process surrogate model then provides both function estimates and confidence intervals that guide the efficient sampling of our design space. In turn, this sampling critically reduces the numbers of fabricated examples during exploration and automatically handles exploration-exploitation trade-offs. We apply our method to fabricate drawing tools that provide a wide range of haptic feedback, and demonstrate that they are often hard for users to distinguish from their traditional drawing-tool analogs.},
  archive      = {J_TOG},
  author       = {Michal Piovarči and Danny M. Kaufman and David I. W. Levin and Piotr Didyk},
  doi          = {10.1145/3386569.3392467},
  journal      = {ACM Transactions on Graphics},
  month        = {8},
  number       = {4},
  pages        = {116:116:1–116:16},
  shortjournal = {ACM Trans. Graph.},
  title        = {Fabrication-in-the-loop co-optimization of surfaces and styli for drawing haptics},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Exertion-aware path generation. <em>TOG</em>,
<em>39</em>(4), 115:115:1–115:14. (<a
href="https://doi.org/10.1145/3386569.3392393">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a novel approach for generating paths with desired exertion properties, which can be used for delivering highly realistic and immersive virtual reality applications that help users achieve exertion goals. Given a terrain as input, our optimization-based approach automatically generates feasible paths on the terrain which users can bike to perform body training in virtual reality. The approach considers exertion properties such as the total work and the perceived level of path difficulty in generating the paths. To verify our approach, we applied it to generate paths on a variety of terrains with different exertion targets and constraints. To conduct our user studies, we built an exercise bike whose force feedback was controlled by the elevation angle of the generated path over the terrain. Our user study results showed that users found exercising with our generated paths in virtual reality more enjoyable compared to traditional exercising approaches. Their energy expenditure in biking the generated paths also matched with the specified targets, validating the efficacy of our approach.},
  archive      = {J_TOG},
  author       = {Wanwan Li and Biao Xie and Yongqi Zhang and Walter Meiss and Haikun Huang and Lap-Fai Yu},
  doi          = {10.1145/3386569.3392393},
  journal      = {ACM Transactions on Graphics},
  month        = {8},
  number       = {4},
  pages        = {115:115:1–115:14},
  shortjournal = {ACM Trans. Graph.},
  title        = {Exertion-aware path generation},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Exact and efficient polyhedral envelope containment check.
<em>TOG</em>, <em>39</em>(4), 114:114:1–114:14. (<a
href="https://doi.org/10.1145/3386569.3392426">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce a new technique to check containment of a triangle within an envelope built around a given triangle mesh. While existing methods conservatively check containment within a Euclidean envelope, our approach makes use of a non-Euclidean envelope where containment can be checked both exactly and efficiently. Exactness is crucial to address major robustness issues in existing geometry processing algorithms, which we demonstrate by integrating our technique in two surface triangle remeshing algorithms and a volumetric tetrahedral meshing algorithm. We provide a quantitative comparison of our method and alternative algorithms, showing that our solution, in addition to being exact, is also more efficient. Indeed, while containment within large envelopes can be checked in a comparable time, we show that our algorithm outperforms alternative methods when the envelope becomes thin.},
  archive      = {J_TOG},
  author       = {Bolun Wang and Teseo Schneider and Yixin Hu and Marco Attene and Daniele Panozzo},
  doi          = {10.1145/3386569.3392426},
  journal      = {ACM Transactions on Graphics},
  month        = {8},
  number       = {4},
  pages        = {114:114:1–114:14},
  shortjournal = {ACM Trans. Graph.},
  title        = {Exact and efficient polyhedral envelope containment check},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Error-bounded compatible remeshing. <em>TOG</em>,
<em>39</em>(4), 113:113:1–113:15. (<a
href="https://doi.org/10.1145/3386569.3392434">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a novel method to construct compatible surface meshes with bounded approximation errors. Given two oriented and topologically equivalent surfaces and a sparse set of corresponding landmarks, our method contains two steps: (1) generate compatible meshes with bounded approximation errors and (2) reduce mesh complexity while ensuring that approximation errors are always bounded. Central to the first step is a parameterization-based remeshing technique, which is capable of isotropically remeshing the input surfaces to be compatible and error-bounded. By iteratively performing a novel edge-based compatible remeshing and increasing the compatible target edge lengths, the second step effectively reduces mesh complexity while explicitly maintaining compatibility, regularity, and bounding approximation errors. Tests on various pairs of complex models demonstrate the efficacy and practicability of our method for constructing high-quality compatible meshes with bounded approximation errors.},
  archive      = {J_TOG},
  author       = {Yang Yang and Wen-Xiang Zhang and Yuan Liu and Ligang Liu and Xiao-Ming Fu},
  doi          = {10.1145/3386569.3392434},
  journal      = {ACM Transactions on Graphics},
  month        = {8},
  number       = {4},
  pages        = {113:113:1–113:15},
  shortjournal = {ACM Trans. Graph.},
  title        = {Error-bounded compatible remeshing},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). ENIGMA: Evolutionary non-isometric geometry MAtching.
<em>TOG</em>, <em>39</em>(4), 112:112:1–112:16. (<a
href="https://doi.org/10.1145/3386569.3392447">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper we propose a fully automatic method for shape correspondence that is widely applicable, and especially effective for non isometric shapes and shapes of different topology. We observe that fully-automatic shape correspondence can be decomposed as a hybrid discrete/continuous optimization problem, and we find the best sparse landmark correspondence, whose sparse-to-dense extension minimizes a local metric distortion. To tackle the combinatorial task of landmark correspondence we use an evolutionary genetic algorithm , where the local distortion of the sparse-to-dense extension is used as the objective function. We design novel geometrically guided genetic operators, which, when combined with our objective, are highly effective for non isometric shape matching. Our method outperforms state of the art methods for automatic shape correspondence both quantitatively and qualitatively on challenging datasets.},
  archive      = {J_TOG},
  author       = {Michal Edelstein and Danielle Ezuz and Mirela Ben-Chen},
  doi          = {10.1145/3386569.3392447},
  journal      = {ACM Transactions on Graphics},
  month        = {8},
  number       = {4},
  pages        = {112:112:1–112:16},
  shortjournal = {ACM Trans. Graph.},
  title        = {ENIGMA: Evolutionary non-isometric geometry MAtching},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Efficient bijective parameterizations. <em>TOG</em>,
<em>39</em>(4), 111:111:1–111:8. (<a
href="https://doi.org/10.1145/3386569.3392435">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a novel method to efficiently compute bijective parameterizations with low distortion on disk topology meshes. Our method relies on a second-order solver. To design an efficient solver, we develop two key techniques. First, we propose a coarse shell to substantially reduce the number of collision constraints that are used to guarantee overlap-free boundaries. During the optimization process, the shell ensures the Hessian matrix with a fixed nonzero structure and a low density, thereby significantly accelerating the optimization. The second is a triangle inequality-based barrier function that effectively ensures non-intersecting boundaries. Our barrier function is C ∞ inside the locally supported region and its convex second-order approximation is able to be analytically obtained. Compared to state-of-the-art methods for optimizing bijective parameterizations, our method exhibits better scalability and is about six times faster. The performance of our bijective parameterization algorithm is comparable to state-of-the-art methods of locally flip-free parameterizations. A large number of experimental results have shown the capability and feasibility of our method.},
  archive      = {J_TOG},
  author       = {Jian-Ping Su and Chunyang Ye and Ligang Liu and Xiao-Ming Fu},
  doi          = {10.1145/3386569.3392435},
  journal      = {ACM Transactions on Graphics},
  month        = {8},
  number       = {4},
  pages        = {111:111:1–111:8},
  shortjournal = {ACM Trans. Graph.},
  title        = {Efficient bijective parameterizations},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Discrete differential operators on polygonal meshes.
<em>TOG</em>, <em>39</em>(4), 110:110:1–110:14. (<a
href="https://doi.org/10.1145/3386569.3392389">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Geometry processing of surface meshes relies heavily on the discretization of differential operators such as gradient, Laplacian, and covariant derivative. While a variety of discrete operators over triangulated meshes have been developed and used for decades, a similar construction over polygonal meshes remains far less explored despite the prevalence of non-simplicial surfaces in geometric design and engineering applications. This paper introduces a principled construction of discrete differential operators on surface meshes formed by (possibly non-flat and non-convex) polygonal faces. Our approach is based on a novel mimetic discretization of the gradient operator that is linear-precise on arbitrary polygons. Equipped with this discrete gradient, we draw upon ideas from the Virtual Element Method in order to derive a series of discrete operators commonly used in graphics that are now valid over polygonal surfaces. We demonstrate the accuracy and robustness of our resulting operators through various numerical examples, before incorporating them into existing geometry processing algorithms.},
  archive      = {J_TOG},
  author       = {Fernando De Goes and Andrew Butts and Mathieu Desbrun},
  doi          = {10.1145/3386569.3392389},
  journal      = {ACM Transactions on Graphics},
  month        = {8},
  number       = {4},
  pages        = {110:110:1–110:14},
  shortjournal = {ACM Trans. Graph.},
  title        = {Discrete differential operators on polygonal meshes},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Developability of heightfields via rank minimization.
<em>TOG</em>, <em>39</em>(4), 109:109:1–109:15. (<a
href="https://doi.org/10.1145/3386569.3392419">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This work concerns the computation and approximation of developable surfaces --- surfaces that are locally isometric to the two-dimensional plane. These surfaces are heavily studied in differential geometry, and are also of great interest to fabrication, architecture and fashion. We focus specifically on developability of heightfields. Our main observation is that developability can be cast as a rank constraint, which can then be plugged into theoretically-grounded rank-minimization techniques from the field of compressed sensing. This leads to a convex semidefinite optimization problem, which receives an input heightfield and recovers a similar heightfield which is developable. Due to the sparsifying nature of compressed sensing, the recovered surface is piecewise developable, with creases emerging between connected developable pieces. The convex program includes one user-specified parameter, balancing adherence to the original surface with developability and number of patches. We moreover show, that in contrast to previous techniques, our discretization does not introduce a bias and the same results are achieved across resolutions and orientations, and with no limit on the number of creases and patches. We solve this convex semidefinite optimization problem efficiently, by devising a tailor-made ADMM solver which leverages matrix-projection observations unique to our problem. We employ our method on a plethora of experiments, from denoising 3D scans of developable geometry such as documents and buildings, through approximating general heightfields with developable ones, and up to interpolating sparse annotations with a developable heightfield.},
  archive      = {J_TOG},
  author       = {Silvia Sellán and Noam Aigerman and Alec Jacobson},
  doi          = {10.1145/3386569.3392419},
  journal      = {ACM Transactions on Graphics},
  month        = {8},
  number       = {4},
  pages        = {109:109:1–109:15},
  shortjournal = {ACM Trans. Graph.},
  title        = {Developability of heightfields via rank minimization},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Deep geometric texture synthesis. <em>TOG</em>,
<em>39</em>(4), 108:108:1–108:11. (<a
href="https://doi.org/10.1145/3386569.3392471">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, deep generative adversarial networks for image generation have advanced rapidly; yet, only a small amount of research has focused on generative models for irregular structures, particularly meshes. Nonetheless, mesh generation and synthesis remains a fundamental topic in computer graphics. In this work, we propose a novel framework for synthesizing geometric textures. It learns geometric texture statistics from local neighborhoods ( i.e. , local triangular patches) of a single reference 3D model. It learns deep features on the faces of the input triangulation, which is used to subdivide and generate offsets across multiple scales, without parameterization of the reference or target mesh. Our network displaces mesh vertices in any direction ( i.e. , in the normal and tangential direction), enabling synthesis of geometric textures, which cannot be expressed by a simple 2D displacement map. Learning and synthesizing on local geometric patches enables a genus-oblivious framework, facilitating texture transfer between shapes of different genus.},
  archive      = {J_TOG},
  author       = {Amir Hertz and Rana Hanocka and Raja Giryes and Daniel Cohen-Or},
  doi          = {10.1145/3386569.3392471},
  journal      = {ACM Transactions on Graphics},
  month        = {8},
  number       = {4},
  pages        = {108:108:1–108:11},
  shortjournal = {ACM Trans. Graph.},
  title        = {Deep geometric texture synthesis},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Data-driven extraction and composition of secondary dynamics
in facial performance capture. <em>TOG</em>, <em>39</em>(4),
107:107:1–107:10. (<a
href="https://doi.org/10.1145/3386569.3392463">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Performance capture of expressive subjects, particularly facial performances acquired with high spatial resolution, will inevitably incorporate some fraction of motion that is due to inertial effects and dynamic overshoot due to ballistic motion. This is true in most natural capture environments where the actor is able to move freely during their performance, rather than being tethered to a fixed position. Normally these secondary dynamic effects are unwanted, as the captured facial performance is often retargeted to different head motion, and sometimes to completely different characters, and in both cases the captured dynamic effects should be removed and new secondary effects should be added. This paper advances the hypothesis that for a highly constrained elastic medium such as the human face, these secondary inertial effects are predominantly due to the motion of the underlying bony structures (cranium and mandible). Our work aims to compute and characterize the difference between the captured dynamic facial performance, and a speculative quasistatic variant of the same motion should the inertial effects have been absent. This is used to either subtract parasitic secondary dynamics that resulted from unintentional motion during capture, or compose such effects on top of a quasistatic performance to simulate a new dynamic motion of the actor&#39;s body and skull, either artist-prescribed or acquired via motion capture. We propose a data-driven technique that comprises complementary removal and synthesis networks for secondary dynamics in facial performance capture. We show how such a system can be effectively trained from a collection of acquired dynamic deformations under varying expressions where the actor induces rigid head motion from walking and running, as well as forced oscillatory body motion in a controlled setting by external actuators.},
  archive      = {J_TOG},
  author       = {Gaspard Zoss and Eftychios Sifakis and Markus Gross and Thabo Beeler and Derek Bradley},
  doi          = {10.1145/3386569.3392463},
  journal      = {ACM Transactions on Graphics},
  month        = {8},
  number       = {4},
  pages        = {107:107:1–107:10},
  shortjournal = {ACM Trans. Graph.},
  title        = {Data-driven extraction and composition of secondary dynamics in facial performance capture},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Cut-enhanced PolyCube-maps for feature-aware all-hex
meshing. <em>TOG</em>, <em>39</em>(4), 106:106:1–106:14. (<a
href="https://doi.org/10.1145/3386569.3392378">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Volumetric PolyCube-Map-based methods offer automatic ways to construct all-hexahedral meshes for closed 3D polyhedral domains, but their meshing quality is limited by the lack of interior singularities and feature alignment. In the presented work, we propose cut-enhanced PolyCube-Maps , to introduce essential interior singularities and preserve most input features. Our main idea is simple and intuitive: by inserting proper parameterization seams into the initial PolyCube-Map via novel PolyCube cutting operations, the mapping distortion can be reduced significantly. The cut-enhanced PolyCube-Map computation includes feature-aware PolyCube-Map construction and cut-enhanced PolyCube deformation. The former aims to preserve input feature edges during the initial PolyCube-Map construction. The latter introduces seams into the volumetric PolyCube shape by cutting it through selective PolyCube edges and deforms the modified PolyCube under the seamless constraints to compute a low-distortion PolyCube-Map. The hexahedral mesh induced by the final PolyCube-Map can be further enhanced by our mesh improvement algorithm. We validate the efficacy of our method on a collection of more than one hundred CAD models and demonstrate its advantages over other automatic all-hex meshing methods and padding strategies. The limitations of cut-enhanced PolyCube-Maps are also discussed thoroughly.},
  archive      = {J_TOG},
  author       = {Hao-Xiang Guo and Xiaohan Liu and Dong-Ming Yan and Yang Liu},
  doi          = {10.1145/3386569.3392378},
  journal      = {ACM Transactions on Graphics},
  month        = {8},
  number       = {4},
  pages        = {106:106:1–106:14},
  shortjournal = {ACM Trans. Graph.},
  title        = {Cut-enhanced PolyCube-maps for feature-aware all-hex meshing},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Computational design of skintight clothing. <em>TOG</em>,
<em>39</em>(4), 105:105:1–105:12. (<a
href="https://doi.org/10.1145/3386569.3392477">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose an optimization-driven approach for automated, physics-based pattern design for tight-fitting clothing. Designing such clothing poses particular challenges since large nonlinear deformations, tight contact between cloth and body, and body deformations have to be accounted for. To address these challenges, we develop a computational model based on an embedding of the two-dimensional cloth mesh in the surface of the three-dimensional body mesh. Our Lagrangian-on-Lagrangian approach eliminates contact handling while coupling cloth and body. Building on this model, we develop a physics-driven optimization method based on sensitivity analysis that automatically computes optimal patterns according to design objectives encoding body shape, pressure distribution, seam traction, and other criteria. We demonstrate our approach by generating personalized patterns for various body shapes and a diverse set of garments with complex pattern layouts.},
  archive      = {J_TOG},
  author       = {Juan Montes and Bernhard Thomaszewski and Sudhir Mudur and Tiberiu Popa},
  doi          = {10.1145/3386569.3392477},
  journal      = {ACM Transactions on Graphics},
  month        = {8},
  number       = {4},
  pages        = {105:105:1–105:12},
  shortjournal = {ACM Trans. Graph.},
  title        = {Computational design of skintight clothing},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020b). C-space tunnel discovery for puzzle path planning.
<em>TOG</em>, <em>39</em>(4), 104:104:1–104:14. (<a
href="https://doi.org/10.1145/3386569.3392468">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Rigid body disentanglement puzzles are challenging for both humans and motion planning algorithms because their solutions involve tricky twisting and sliding moves that correspond to navigating through narrow tunnels in the puzzle&#39;s configuration space (C-space). We propose a tunnel-discovery and planning strategy for solving these puzzles. First, we locate important features on the pieces using geometric heuristics and machine learning, and then match pairs of these features to discover collision free states in the puzzle&#39;s C-space that lie within the narrow tunnels. Second, we propose a Rapidly-exploring Dense Tree (RDT) motion planner variant that builds tunnel escape roadmaps and then connects these roadmaps into a solution path connecting start and goal states. We evaluate our approach on a variety of challenging disentanglement puzzles and provide extensive baseline comparisons with other motion planning techniques.},
  archive      = {J_TOG},
  author       = {Xinya Zhang and Robert Belfer and Paul G. Kry and Etienne Vouga},
  doi          = {10.1145/3386569.3392468},
  journal      = {ACM Transactions on Graphics},
  month        = {8},
  number       = {4},
  pages        = {104:104:1–104:14},
  shortjournal = {ACM Trans. Graph.},
  title        = {C-space tunnel discovery for puzzle path planning},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Bézier guarding: Precise higher-order meshing of curved 2D
domains. <em>TOG</em>, <em>39</em>(4), 103:103:1–103:15. (<a
href="https://doi.org/10.1145/3386569.3392372">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a mesh generation algorithm for the curvilinear triangulation of planar domains with piecewise polynomial boundary. The resulting mesh consists of regular, injective higher-order triangular elements and precisely conforms with the domain&#39;s curved boundary. No smoothness requirements are imposed on the boundary. Prescribed piecewise polynomial curves in the interior, like material interfaces or feature curves, can be taken into account for precise interpolation by the resulting mesh&#39;s edges as well. In its core, the algorithm is based on a novel explicit construction of guaranteed injective Bézier triangles with certain edge curves and edge parametrizations prescribed. Due to the use of only rational arithmetic, the algorithm can optionally be performed using exact number types in practice, so as to provide robustness guarantees.},
  archive      = {J_TOG},
  author       = {Manish Mandad and Marcel Campen},
  doi          = {10.1145/3386569.3392372},
  journal      = {ACM Transactions on Graphics},
  month        = {8},
  number       = {4},
  pages        = {103:103:1–103:15},
  shortjournal = {ACM Trans. Graph.},
  title        = {Bézier guarding: Precise higher-order meshing of curved 2D domains},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Automatic structure synthesis for 3D woven relief.
<em>TOG</em>, <em>39</em>(4), 102:102:1–102:10. (<a
href="https://doi.org/10.1145/3386569.3392449">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {3D weaving is a manufacturing technique that creates multilayer textiles with substantial thickness. Currently, the primary use for these materials is in regularly structured carbon-polymer or glass-polymer composites, but in principle a wide range of complex shapes can be achieved, providing the opportunity to customize the fiber structure for individual parts and also making 3D weaving appealing in many soft-goods applications. The primary obstacle to broader use is the need to design intricate weave structures, involving tens to hundreds of thousands of yarn crossings, which are different for every shape to be produced. The goal of this research is to make 3D weaving as readily usable as CNC machining or 3D printing, by providing an algorithm to convert an arbitrary 3D solid model into machine instructions to weave the corresponding shape. We propose a method to generate 3D weaving patterns for height fields by slicing the shape along intersecting arrays of parallel planes and then computing the paths for all the warp and weft yarns, which travel in these planes. We demonstrate the method by generating weave structures for different shapes and fabricating a number of examples in polyester yarn using a Jacquard loom.},
  archive      = {J_TOG},
  author       = {Rundong Wu and Claire Harvey and Joy Xiaoji Zhang and Sean Kroszner and Brooks Hagan and Steve Marschner},
  doi          = {10.1145/3386569.3392449},
  journal      = {ACM Transactions on Graphics},
  month        = {8},
  number       = {4},
  pages        = {102:102:1–102:10},
  shortjournal = {ACM Trans. Graph.},
  title        = {Automatic structure synthesis for 3D woven relief},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A low-parametric rhombic microstructure family for irregular
lattices. <em>TOG</em>, <em>39</em>(4), 101:101:1–101:20. (<a
href="https://doi.org/10.1145/3386569.3392451">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {New fabrication technologies have significantly decreased the cost of fabrication of shapes with highly complex geometric structure. One important application of complex fine-scale geometric structures is to create variable effective elastic material properties in shapes manufactured from a single material. Modification of material properties has a variety of uses, from aerospace applications to soft robotics and prosthetic devices. Due to its scalability and effectiveness, an increasingly common approach to creating spatially varying materials is to partition a shape into cells and use a parametric family of small-scale geometric structures with known effective properties to fill the cells. We propose a new approach to solving this problem for extruded, planar microstructures. Differently from existing methods for two-scale optimization based on regular grids with square periodic cells, which cannot conform to an arbitrary boundary, we introduce cell decompositions consisting of (nearly) rhombic cells. These meshes have far greater flexibility than those with square cells in terms of approximating arbitrary shapes, and, at the same time, have a number of properties simplifying small-scale structure construction. Our main contributions include a new family of 2D cell geometry structures, explicitly parameterized by their effective Young&#39;s moduli E , Poisson&#39;s ratios v , and rhombic angle α with the geometry parameters expressed directly as smooth spline functions of E, v , and α. This family leads to smooth transitions between the tiles and can handle a broad range of rhombic cell shapes. We introduce a complete material design pipeline based on this microstructure family, composed of an algorithm to generate rhombic tessellation from quadrilateral meshes and an algorithm to synthesize the microstructure geometry. We fabricated a number of models and experimentally demonstrated how our method, in combination with material optimization, can be used to achieve the desired deformation behavior.},
  archive      = {J_TOG},
  author       = {Davi Colli Tozoni and Jérémie Dumas and Zhongshi Jiang and Julian Panetta and Daniele Panozzo and Denis Zorin},
  doi          = {10.1145/3386569.3392451},
  journal      = {ACM Transactions on Graphics},
  month        = {8},
  number       = {4},
  pages        = {101:101:1–101:20},
  shortjournal = {ACM Trans. Graph.},
  title        = {A low-parametric rhombic microstructure family for irregular lattices},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Unsupervised k-modal styled content generation.
<em>TOG</em>, <em>39</em>(4), 100:100:1–100:10. (<a
href="https://doi.org/10.1145/3386569.3392454">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The emergence of deep generative models has recently enabled the automatic generation of massive amounts of graphical content, both in 2D and in 3D. Generative Adversarial Networks (GANs) and style control mechanisms, such as Adaptive Instance Normalization (AdaIN), have proved particularly effective in this context, culminating in the state-of-the-art StyleGAN architecture. While such models are able to learn diverse distributions, provided a sufficiently large training set, they are not well-suited for scenarios where the distribution of the training data exhibits a multi-modal behavior. In such cases, reshaping a uniform or normal distribution over the latent space into a complex multi-modal distribution in the data domain is challenging, and the generator might fail to sample the target distribution well. Furthermore, existing unsupervised generative models are not able to control the mode of the generated samples independently of the other visual attributes, despite the fact that they are typically disentangled in the training data. In this paper, we introduce uMM-GAN, a novel architecture designed to better model multi-modal distributions, in an unsupervised fashion. Building upon the StyleGAN architecture, our network learns multiple modes, in a completely unsupervised manner , and combines them using a set of learned weights. We demonstrate that this approach is capable of effectively approximating a complex distribution as a superposition of multiple simple ones. We further show that uMM-GAN effectively disentangles between modes and style, thereby providing an independent degree of control over the generated content.},
  archive      = {J_TOG},
  author       = {Omry Sendik and Dani Lischinski and Daniel Cohen-Or},
  doi          = {10.1145/3386569.3392454},
  journal      = {ACM Transactions on Graphics},
  month        = {8},
  number       = {4},
  pages        = {100:100:1–100:10},
  shortjournal = {ACM Trans. Graph.},
  title        = {Unsupervised K-modal styled content generation},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Sliced optimal transport sampling. <em>TOG</em>,
<em>39</em>(4), 99:99:1–99:17. (<a
href="https://doi.org/10.1145/3386569.3392395">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we introduce a numerical technique to generate sample distributions in arbitrary dimension for improved accuracy of Monte Carlo integration. We point out that optimal transport offers theoretical bounds on Monte Carlo integration error, and that the recently-introduced numerical framework of sliced optimal transport (SOT) allows us to formulate a novel and efficient approach to generating well-distributed high-dimensional pointsets. The resulting sliced optimal transport sampling, solely involving repeated 1D solves, is particularly simple and efficient for the common case of a uniform density over a d -dimensional ball. We also construct a volume-preserving map from a d -ball to a d -cube (generalizing the Shirley-Chiu mapping to arbitrary dimensions) to offer fast SOT sampling over d -cubes. We provide ample numerical evidence of the improvement in Monte Carlo integration accuracy that SOT sampling brings compared to existing QMC techniques, and derive a projective variant for rendering which rivals, and at times outperforms, current sampling strategies using low-discrepancy sequences or optimized samples.},
  archive      = {J_TOG},
  author       = {Lois Paulin and Nicolas Bonneel and David Coeurjolly and Jean-Claude Iehl and Antoine Webanck and Mathieu Desbrun and Victor Ostromoukhov},
  doi          = {10.1145/3386569.3392395},
  journal      = {ACM Transactions on Graphics},
  month        = {8},
  number       = {4},
  pages        = {99:99:1–99:17},
  shortjournal = {ACM Trans. Graph.},
  title        = {Sliced optimal transport sampling},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). RoboCut: Hot-wire cutting with robot-controlled flexible
rods. <em>TOG</em>, <em>39</em>(4), 98:98:1–98:15. (<a
href="https://doi.org/10.1145/3386569.3392465">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hot-wire cutting is a subtractive fabrication technique used to carve foam and similar materials. Conventional machines rely on straight wires and are thus limited to creating piecewise ruled surfaces. In this work, we propose a method that exploits a dual-arm robot setup to actively control the shape of a flexible, heated rod as it cuts through the material. While this setting offers great freedom of shape, using it effectively requires concurrent reasoning about three tightly coupled sub-problems: 1) modeling the way in which the shape of the rod and the surface it sweeps are governed by the robot&#39;s motions; 2) approximating a target shape through a sequence of surfaces swept by the equilibrium shape of an elastic rod; and 3) generating collision-free motion trajectories that lead the robot to create desired sweeps with the deformable tool. We present a computational framework for robotic hot wire cutting that addresses all three sub-problems in a unified manner. We evaluate our approach on a set of simulated results and physical artefacts generated with our robotic fabrication system.},
  archive      = {J_TOG},
  author       = {Simon Duenser and Roi Poranne and Bernhard Thomaszewski and Stelian Coros},
  doi          = {10.1145/3386569.3392465},
  journal      = {ACM Transactions on Graphics},
  month        = {8},
  number       = {4},
  pages        = {98:98:1–98:15},
  shortjournal = {ACM Trans. Graph.},
  title        = {RoboCut: Hot-wire cutting with robot-controlled flexible rods},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Nonlinear color triads for approximation, learning and
direct manipulation of color distributions. <em>TOG</em>,
<em>39</em>(4), 97:97:1–97:13. (<a
href="https://doi.org/10.1145/3386569.3392461">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present nonlinear color triads, an extension of color gradients able to approximate a variety of natural color distributions that have no standard interactive representation. We derive a method to fit this compact parametric representation to existing images and show its power for tasks such as image editing and compression. Our color triad formulation can also be included in standard deep learning architectures, facilitating further research.},
  archive      = {J_TOG},
  author       = {Maria Shugrina and Amlan Kar and Sanja Fidler and Karan Singh},
  doi          = {10.1145/3386569.3392461},
  journal      = {ACM Transactions on Graphics},
  month        = {8},
  number       = {4},
  pages        = {97:97:1–97:13},
  shortjournal = {ACM Trans. Graph.},
  title        = {Nonlinear color triads for approximation, learning and direct manipulation of color distributions},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). NASOQ: Numerically accurate sparsity-oriented QP solver.
<em>TOG</em>, <em>39</em>(4), 96:96:1–96:17. (<a
href="https://doi.org/10.1145/3386569.3392486">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Quadratic programs (QP), minimizations of quadratic objectives subject to linear inequality and equality constraints, are at the heart of algorithms across scientific domains. Applications include fundamental tasks in geometry processing, simulation, engineering, animation and finance where the accurate, reliable, efficient, and scalable solution of QP problems is critical. However, available QP algorithms generally provide either accuracy or scalability - but not both. Some algorithms reliably solve QP problems to high accuracy but work only for smaller-scale QP problems due to their reliance on dense matrix methods. Alternately, many other QP solvers scale well via sparse, efficient algorithms but cannot reliably deliver solutions at requested accuracies. Towards addressing the need for accurate and efficient QP solvers at scale, we develop NASOQ, a new, full-space QP algorithm that provides accurate, efficient, and scalable solutions for QP problems. To enable NASOQ we construct a new row modification method and fast implementation of LDL factorization for indefinite systems. Together they enable efficient updates and accurate solutions of the iteratively modified KKT systems required for accurate QP solves. While QP methods have been previously tested on large synthetic benchmarks, to test and compare NASOQ&#39;s suitability for real-world applications we collect here a new benchmark set comprising a wide range of graphics-related QPs across physical simulation, animation, and geometry processing tasks. We combine these problems with numerous pre-existing stress-test QP benchmarks to form, to our knowledge, the largest-scale test set of application-based QP problems currently available. Building off of our base NASOQ solver we then develop and test two NASOQ variants against best, state-of-the-art available QP libraries - both commercial and open-source. Our two NASOQ-based methods each solve respectively 98.8\% and 99.5\% of problems across a range of requested accuracies from 10 -3 to 10 -9 with average speedups ranging from 1.7× to 24.8× over fastest competing methods.},
  archive      = {J_TOG},
  author       = {Kazem Cheshmi and Danny M. Kaufman and Shoaib Kamil and Maryam Mehri Dehnavi},
  doi          = {10.1145/3386569.3392486},
  journal      = {ACM Transactions on Graphics},
  month        = {8},
  number       = {4},
  pages        = {96:96:1–96:17},
  shortjournal = {ACM Trans. Graph.},
  title        = {NASOQ: Numerically accurate sparsity-oriented QP solver},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). MichiGAN: Multi-input-conditioned hair image generation for
portrait editing. <em>TOG</em>, <em>39</em>(4), 95:95:1–95:13. (<a
href="https://doi.org/10.1145/3386569.3392488">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Despite the recent success of face image generation with GANs, conditional hair editing remains challenging due to the under-explored complexity of its geometry and appearance. In this paper, we present MichiGAN (Multi-Input-Conditioned Hair Image GAN), a novel conditional image generation method for interactive portrait hair manipulation. To provide user control over every major hair visual factor, we explicitly disentangle hair into four orthogonal attributes, including shape, structure, appearance, and background. For each of them, we design a corresponding condition module to represent, process, and convert user inputs, and modulate the image generation pipeline in ways that respect the natures of different visual attributes. All these condition modules are integrated with the backbone generator to form the final end-to-end network, which allows fully-conditioned hair generation from multiple user inputs. Upon it, we also build an interactive portrait hair editing system that enables straightforward manipulation of hair by projecting intuitive and high-level user inputs such as painted masks, guiding strokes, or reference photos to well-defined condition representations. Through extensive experiments and evaluations, we demonstrate the superiority of our method regarding both result quality and user controllability.},
  archive      = {J_TOG},
  author       = {Zhentao Tan and Menglei Chai and Dongdong Chen and Jing Liao and Qi Chu and Lu Yuan and Sergey Tulyakov and Nenghai Yu},
  doi          = {10.1145/3386569.3392488},
  journal      = {ACM Transactions on Graphics},
  month        = {8},
  number       = {4},
  pages        = {95:95:1–95:13},
  shortjournal = {ACM Trans. Graph.},
  title        = {MichiGAN: Multi-input-conditioned hair image generation for portrait editing},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Fast and deep facial deformations. <em>TOG</em>,
<em>39</em>(4), 94:94:1–94:15. (<a
href="https://doi.org/10.1145/3386569.3392397">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Film-quality characters typically display highly complex and expressive facial deformation. The underlying rigs used to animate the deformations of a character&#39;s face are often computationally expensive, requiring high-end hardware to deform the mesh at interactive rates. In this paper, we present a method using convolutional neural networks for approximating the mesh deformations of characters&#39; faces. For the models we tested, our approximation runs up to 17 times faster than the original facial rig while still maintaining a high level of fidelity to the original rig. We also propose an extension to the approximation for handling high-frequency deformations such as fine skin wrinkles. While the implementation of the original animation rig depends on an extensive set of proprietary libraries making it difficult to install outside of an in-house development environment, our fast approximation relies on the widely available and easily deployed TensorFlow libraries. In addition to allowing high frame rate evaluation on modest hardware and in a wide range of computing environments, the large speed increase also enables interactive inverse kinematics on the animation rig. We demonstrate our approach and its applicability through interactive character posing and real-time facial performance capture.},
  archive      = {J_TOG},
  author       = {Stephen W. Bailey and Dalton Omens and Paul Dilorenzo and James F. O&#39;Brien},
  doi          = {10.1145/3386569.3392397},
  journal      = {ACM Transactions on Graphics},
  month        = {8},
  number       = {4},
  pages        = {94:94:1–94:15},
  shortjournal = {ACM Trans. Graph.},
  title        = {Fast and deep facial deformations},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Code replicability in computer graphics. <em>TOG</em>,
<em>39</em>(4), 93:93:1–93:8. (<a
href="https://doi.org/10.1145/3386569.3392413">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Being able to duplicate published research results is an important process of conducting research whether to build upon these findings or to compare with them. This process is called &quot;replicability&quot; when using the original authors&#39; artifacts (e.g., code), or &quot;reproducibility&quot; otherwise (e.g., re-implementing algorithms). Reproducibility and replicability of research results have gained a lot of interest recently with assessment studies being led in various fields, and they are often seen as a trigger for better result diffusion and transparency. In this work, we assess replicability in Computer Graphics, by evaluating whether the code is available and whether it works properly. As a proxy for this field we compiled, ran and analyzed 151 codes out of 374 papers from 2014, 2016 and 2018 SIGGRAPH conferences. This analysis shows a clear increase in the number of papers with available and operational research codes with a dependency on the subfields, and indicates a correlation between code replicability and citation count. We further provide an interactive tool to explore our results and evaluation data.},
  archive      = {J_TOG},
  author       = {Nicolas Bonneel and David Coeurjolly and Julie Digne and Nicolas Mellado},
  doi          = {10.1145/3386569.3392413},
  journal      = {ACM Transactions on Graphics},
  month        = {8},
  number       = {4},
  pages        = {93:93:1–93:8},
  shortjournal = {ACM Trans. Graph.},
  title        = {Code replicability in computer graphics},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). CNNs on surfaces using rotation-equivariant features.
<em>TOG</em>, <em>39</em>(4), 92:92:1–92:12. (<a
href="https://doi.org/10.1145/3386569.3392437">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper is concerned with a fundamental problem in geometric deep learning that arises in the construction of convolutional neural networks on surfaces. Due to curvature, the transport of filter kernels on surfaces results in a rotational ambiguity, which prevents a uniform alignment of these kernels on the surface. We propose a network architecture for surfaces that consists of vector-valued, rotation-equivariant features. The equivariance property makes it possible to locally align features, which were computed in arbitrary coordinate systems, when aggregating features in a convolution layer. The resulting network is agnostic to the choices of coordinate systems for the tangent spaces on the surface. We implement our approach for triangle meshes. Based on circular harmonic functions, we introduce convolution filters for meshes that are rotation-equivariant at the discrete level. We evaluate the resulting networks on shape correspondence and shape classifications tasks and compare their performance to other approaches.},
  archive      = {J_TOG},
  author       = {Ruben Wiersma and Elmar Eisemann and Klaus Hildebrandt},
  doi          = {10.1145/3386569.3392437},
  journal      = {ACM Transactions on Graphics},
  month        = {8},
  number       = {4},
  pages        = {92:92:1–92:12},
  shortjournal = {ACM Trans. Graph.},
  title        = {CNNs on surfaces using rotation-equivariant features},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). The eyes have it: An integrated eye and face model for
photorealistic facial animation. <em>TOG</em>, <em>39</em>(4),
91:91:1–91:15. (<a
href="https://doi.org/10.1145/3386569.3392493">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Interacting with people across large distances is important for remote work, interpersonal relationships, and entertainment. While such face-to-face interactions can be achieved using 2D video conferencing or, more recently, virtual reality (VR), telepresence systems currently distort the communication of eye contact and social gaze signals. Although methods have been proposed to redirect gaze in 2D teleconferencing situations to enable eye contact, 2D video conferencing lacks the 3D immersion of real life. To address these problems, we develop a system for face-to-face interaction in VR that focuses on reproducing photorealistic gaze and eye contact. To do this, we create a 3D virtual avatar model that can be animated by cameras mounted on a VR headset to accurately track and reproduce human gaze in VR. Our primary contributions in this work are a jointly-learnable 3D face and eyeball model that better represents gaze direction and upper facial expressions, a method for disentangling the gaze of the left and right eyes from each other and the rest of the face allowing the model to represent entirely unseen combinations of gaze and expression, and a gaze-aware model for precise animation from headset-mounted cameras. Our quantitative experiments show that our method results in higher reconstruction quality, and qualitative results show our method gives a greatly improved sense of presence for VR avatars.},
  archive      = {J_TOG},
  author       = {Gabriel Schwartz and Shih-En Wei and Te-Li Wang and Stephen Lombardi and Tomas Simon and Jason Saragih and Yaser Sheikh},
  doi          = {10.1145/3386569.3392493},
  journal      = {ACM Transactions on Graphics},
  month        = {8},
  number       = {4},
  pages        = {91:91:1–91:15},
  shortjournal = {ACM Trans. Graph.},
  title        = {The eyes have it: An integrated eye and face model for photorealistic facial animation},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Tactile rendering based on skin stress optimization.
<em>TOG</em>, <em>39</em>(4), 90:90:1–90:13. (<a
href="https://doi.org/10.1145/3386569.3392398">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a method to render virtual touch, such that the stimulus produced by a tactile device on a user&#39;s skin matches the stimulus computed in a virtual environment simulation. To achieve this, we solve the inverse mapping from skin stimulus to device configuration thanks to a novel optimization algorithm. Within this algorithm, we use a device-skin simulation model to estimate rendered stimuli, we account for trajectory-dependent effects efficiently by decoupling the computation of the friction state from the optimization of device configuration, and we accelerate computations using a neural-network approximation of the device-skin model. Altogether, we enable real-time tactile rendering of rich interactions including smooth rolling, but also contact with edges, or frictional stick-slip motion. We validate our algorithm both qualitatively through user experiments, and quantitatively on a BioTac biomimetic finger sensor.},
  archive      = {J_TOG},
  author       = {Mickeal Verschoor and Dan Casas and Miguel A. Otaduy},
  doi          = {10.1145/3386569.3392398},
  journal      = {ACM Transactions on Graphics},
  month        = {8},
  number       = {4},
  pages        = {90:90:1–90:13},
  shortjournal = {ACM Trans. Graph.},
  title        = {Tactile rendering based on skin stress optimization},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Tactile line drawings for improved shape understanding in
blind and visually impaired users. <em>TOG</em>, <em>39</em>(4),
89:89:1–89:13. (<a
href="https://doi.org/10.1145/3386569.3392388">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Members of the blind and visually impaired community rely heavily on tactile illustrations - raised line graphics on paper that are felt by hand - to understand geometric ideas in school textbooks, depict a story in children&#39;s books, or conceptualize exhibits in museums. However, these illustrations often fail to achieve their goals, in large part due to the lack of understanding in how 3D shapes can be represented in 2D projections. This paper describes a new technique to design tactile illustrations considering the needs of blind individuals. Successful illustration design of 3D objects presupposes identification and combination of important information in topology and geometry. We propose a twofold approach to improve shape understanding. First, we introduce a part-based multi-projection rendering strategy to display geometric information of 3D shapes, making use of canonical viewpoints and removing reliance on traditional perspective projections. Second, curvature information is extracted from cross sections and embedded as textures in our illustrations.},
  archive      = {J_TOG},
  author       = {Athina Panotopoulou and Xiaoting Zhang and Tammy Qiu and Xing-Dong Yang and Emily Whiting},
  doi          = {10.1145/3386569.3392388},
  journal      = {ACM Transactions on Graphics},
  month        = {8},
  number       = {4},
  pages        = {89:89:1–89:13},
  shortjournal = {ACM Trans. Graph.},
  title        = {Tactile line drawings for improved shape understanding in blind and visually impaired users},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Sequential gallery for interactive visual design
optimization. <em>TOG</em>, <em>39</em>(4), 88:88:1–88:12. (<a
href="https://doi.org/10.1145/3386569.3392444">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Visual design tasks often involve tuning many design parameters. For example, color grading of a photograph involves many parameters, some of which non-expert users might be unfamiliar with. We propose a novel user-in-the-loop optimization method that allows users to efficiently find an appropriate parameter set by exploring such a high-dimensional design space through much easier two-dimensional search subtasks. This method, called sequential plane search , is based on Bayesian optimization to keep necessary queries to users as few as possible. To help users respond to plane-search queries, we also propose using a gallery-based interface that provides options in the two-dimensional subspace arranged in an adaptive grid view. We call this interactive framework Sequential Gallery since users sequentially select the best option from the options provided by the interface. Our experiment with synthetic functions shows that our sequential plane search can find satisfactory solutions in fewer iterations than baselines. We also conducted a preliminary user study, results of which suggest that novices can effectively complete search tasks with Sequential Gallery in a photo-enhancement scenario.},
  archive      = {J_TOG},
  author       = {Yuki Koyama and Issei Sato and Masataka Goto},
  doi          = {10.1145/3386569.3392444},
  journal      = {ACM Transactions on Graphics},
  month        = {8},
  number       = {4},
  pages        = {88:88:1–88:12},
  shortjournal = {ACM Trans. Graph.},
  title        = {Sequential gallery for interactive visual design optimization},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). MEgATrack: Monochrome egocentric articulated hand-tracking
for virtual reality. <em>TOG</em>, <em>39</em>(4), 87:87:1–87:13. (<a
href="https://doi.org/10.1145/3386569.3392452">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a system for real-time hand-tracking to drive virtual and augmented reality (VR/AR) experiences. Using four fisheye monochrome cameras, our system generates accurate and low-jitter 3D hand motion across a large working volume for a diverse set of users. We achieve this by proposing neural network architectures for detecting hands and estimating hand keypoint locations. Our hand detection network robustly handles a variety of real world environments. The keypoint estimation network leverages tracking history to produce spatially and temporally consistent poses. We design scalable, semi-automated mechanisms to collect a large and diverse set of ground truth data using a combination of manual annotation and automated tracking. Additionally, we introduce a detection-by-tracking method that increases smoothness while reducing the computational cost; the optimized system runs at 60Hz on PC and 30Hz on a mobile processor. Together, these contributions yield a practical system for capturing a user&#39;s hands and is the default feature on the Oculus Quest VR headset powering input and social presence.},
  archive      = {J_TOG},
  author       = {Shangchen Han and Beibei Liu and Randi Cabezas and Christopher D. Twigg and Peizhao Zhang and Jeff Petkau and Tsz-Ho Yu and Chun-Jung Tai and Muzaffer Akbay and Zheng Wang and Asaf Nitzan and Gang Dong and Yuting Ye and Lingling Tao and Chengde Wan and Robert Wang},
  doi          = {10.1145/3386569.3392452},
  journal      = {ACM Transactions on Graphics},
  month        = {8},
  number       = {4},
  pages        = {87:87:1–87:13},
  shortjournal = {ACM Trans. Graph.},
  title        = {MEgATrack: Monochrome egocentric articulated hand-tracking for virtual reality},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Immersive light field video with a layered mesh
representation. <em>TOG</em>, <em>39</em>(4), 86:86:1–86:15. (<a
href="https://doi.org/10.1145/3386569.3392485">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a system for capturing, reconstructing, compressing, and rendering high quality immersive light field video. We accomplish this by leveraging the recently introduced DeepView view interpolation algorithm, replacing its underlying multi-plane image (MPI) scene representation with a collection of spherical shells that are better suited for representing panoramic light field content. We further process this data to reduce the large number of shell layers to a small, fixed number of RGBA+depth layers without significant loss in visual quality. The resulting RGB, alpha, and depth channels in these layers are then compressed using conventional texture atlasing and video compression techniques. The final compressed representation is lightweight and can be rendered on mobile VR/AR platforms or in a web browser. We demonstrate light field video results using data from the 16-camera rig of [Pozo et al. 2019] as well as a new low-cost hemispherical array made from 46 synchronized action sports cameras. From this data we produce 6 degree of freedom volumetric videos with a wide 70 cm viewing baseline, 10 pixels per degree angular resolution, and a wide field of view, at 30 frames per second video frame rates. Advancing over previous work, we show that our system is able to reproduce challenging content such as view-dependent reflections, semi-transparent surfaces, and near-field objects as close as 34 cm to the surface of the camera rig.},
  archive      = {J_TOG},
  author       = {Michael Broxton and John Flynn and Ryan Overbeck and Daniel Erickson and Peter Hedman and Matthew Duvall and Jason Dourgarian and Jay Busch and Matt Whalen and Paul Debevec},
  doi          = {10.1145/3386569.3392485},
  journal      = {ACM Transactions on Graphics},
  month        = {8},
  number       = {4},
  pages        = {86:86:1–86:15},
  shortjournal = {ACM Trans. Graph.},
  title        = {Immersive light field video with a layered mesh representation},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Human-in-the-loop differential subspace search in
high-dimensional latent space. <em>TOG</em>, <em>39</em>(4),
85:85:1–85:15. (<a
href="https://doi.org/10.1145/3386569.3392409">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Generative models based on deep neural networks often have a high-dimensional latent space, ranging sometimes to a few hundred dimensions or even higher, which typically makes them hard for a user to explore directly. We propose differential subspace search to allow efficient iterative user exploration in such a space, without relying on domain- or data-specific assumptions. We develop a general framework to extract low-dimensional subspaces based on a local differential analysis of the generative model, such that a small change in such a subspace would provide enough change in the resulting data. We do so by applying singular value decomposition to the Jacobian of the generative model and forming a subspace with the desired dimensionality spanned by a given number of singular vectors stochastically selected on the basis of their singular values, to maintain ergodicity. We use our framework to present 1D subspaces to the user via a 1D slider interface. Starting from an initial location, the user finds a new candidate in the presented 1D subspace, which is in turn updated at the new candidate location. This process is repeated until no further improvement can be made. Numerical simulations show that our method can better optimize synthetic black-box objective functions than the alternatives that we tested. Furthermore, we conducted a user study using complex generative models and the results show that our method enables more efficient exploration of high-dimensional latent spaces than the alternatives.},
  archive      = {J_TOG},
  author       = {Chia-Hsing Chiu and Yuki Koyama and Yu-Chi Lai and Takeo Igarashi and Yonghao Yue},
  doi          = {10.1145/3386569.3392409},
  journal      = {ACM Transactions on Graphics},
  month        = {8},
  number       = {4},
  pages        = {85:85:1–85:15},
  shortjournal = {ACM Trans. Graph.},
  title        = {Human-in-the-loop differential subspace search in high-dimensional latent space},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). HeadBlaster: A wearable approach to simulating motion
perception using head-mounted air propulsion jets. <em>TOG</em>,
<em>39</em>(4), 84:84:1–84:12. (<a
href="https://doi.org/10.1145/3386569.3392482">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present HeadBlaster, a novel wearable technology that creates motion perception by applying ungrounded force to the head to stimulate the vestibular and proprioception sensory systems. Compared to motion platforms that tilt the body, HeadBlaster more closely approximates how lateral inertial and centrifugal forces are felt during real motion to provide more persistent motion perception. In addition, because HeadBlaster only actuates the head rather than the entire body, it eliminates the mechanical motion platforms that users must be constrained to, which improves user mobility and enables room-scale VR experiences. We designed a wearable HeadBlaster system with 6 air nozzles integrated into a VR headset, using compressed air jets to provide persistent, lateral propulsion forces. By controlling multiple air jets, it is able to create the perception of lateral acceleration in 360 degrees. We conducted a series of perception and human-factor studies to quantify the head movement, the persistence of perceived acceleration, and the minimal level of detectable forces. We then explored the user experience of HeadBlaster through two VR applications: a custom surfing game, and a commercial driving simulator together with a commercial motion platform. Study results showed that HeadBlaster provided significantly longer perceived duration of acceleration than motion platforms. It also significantly improved realism and immersion, and was preferred by users compared to using VR alone. In addition, it can be used in conjunction with motion platforms to further augment the user experience.},
  archive      = {J_TOG},
  author       = {Shi-Hong Liu and Pai-Chien Yen and Yi-Hsuan Mao and Yu-Hsin Lin and Erick Chandra and Mike Y. Chen},
  doi          = {10.1145/3386569.3392482},
  journal      = {ACM Transactions on Graphics},
  month        = {8},
  number       = {4},
  pages        = {84:84:1–84:12},
  shortjournal = {ACM Trans. Graph.},
  title        = {HeadBlaster: A wearable approach to simulating motion perception using head-mounted air propulsion jets},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). ARAnimator: In-situ character animation in mobile AR with
user-defined motion gestures. <em>TOG</em>, <em>39</em>(4),
83:83:1–83:12. (<a
href="https://doi.org/10.1145/3386569.3392404">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Creating animated virtual AR characters closely interacting with real environments is interesting but difficult. Existing systems adopt video see-through approaches to indirectly control a virtual character in mobile AR, making close interaction with real environments not intuitive. In this work we use an AR-enabled mobile device to directly control the position and motion of a virtual character situated in a real environment. We conduct two guessability studies to elicit user-defined motions of a virtual character interacting with real environments, and a set of user-defined motion gestures describing specific character motions. We found that an SVM-based learning approach achieves reasonably high accuracy for gesture classification from the motion data of a mobile device. We present ARAnimator , which allows novice and casual animation users to directly represent a virtual character by an AR-enabled mobile phone and control its animation in AR scenes using motion gestures of the device, followed by animation preview and interactive editing through a video see-through interface. Our experimental results show that with ARAnimator , users are able to easily create in-situ character animations closely interacting with different real environments.},
  archive      = {J_TOG},
  author       = {Hui Ye and Kin Chung Kwan and Wanchao Su and Hongbo Fu},
  doi          = {10.1145/3386569.3392404},
  journal      = {ACM Transactions on Graphics},
  month        = {8},
  number       = {4},
  pages        = {83:83:1–83:12},
  shortjournal = {ACM Trans. Graph.},
  title        = {ARAnimator: In-situ character animation in mobile AR with user-defined motion gestures},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). XNect: Real-time multi-person 3D motion capture with a
single RGB camera. <em>TOG</em>, <em>39</em>(4), 82:82:1–82:17. (<a
href="https://doi.org/10.1145/3386569.3392410">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a real-time approach for multi-person 3D motion capture at over 30 fps using a single RGB camera. It operates successfully in generic scenes which may contain occlusions by objects and by other people. Our method operates in subsequent stages. The first stage is a convolutional neural network (CNN) that estimates 2D and 3D pose features along with identity assignments for all visible joints of all individuals. We contribute a new architecture for this CNN, called SelecSLS Net , that uses novel selective long and short range skip connections to improve the information flow allowing for a drastically faster network without compromising accuracy. In the second stage, a fullyconnected neural network turns the possibly partial (on account of occlusion) 2D pose and 3D pose features for each subject into a complete 3D pose estimate per individual. The third stage applies space-time skeletal model fitting to the predicted 2D and 3D pose per subject to further reconcile the 2D and 3D pose, and enforce temporal coherence. Our method returns the full skeletal pose in joint angles for each subject. This is a further key distinction from previous work that do not produce joint angle results of a coherent skeleton in real time for multi-person scenes. The proposed system runs on consumer hardware at a previously unseen speed of more than 30 fps given 512x320 images as input while achieving state-of-the-art accuracy, which we will demonstrate on a range of challenging real-world scenes.},
  archive      = {J_TOG},
  author       = {Dushyant Mehta and Oleksandr Sotnychenko and Franziska Mueller and Weipeng Xu and Mohamed Elgharib and Pascal Fua and Hans-Peter Seidel and Helge Rhodin and Gerard Pons-Moll and Christian Theobalt},
  doi          = {10.1145/3386569.3392410},
  journal      = {ACM Transactions on Graphics},
  month        = {8},
  number       = {4},
  pages        = {82:82:1–82:17},
  shortjournal = {ACM Trans. Graph.},
  title        = {XNect: Real-time multi-person 3D motion capture with a single RGB camera},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Single-shot high-quality facial geometry and skin appearance
capture. <em>TOG</em>, <em>39</em>(4), 81:81:1–81:12. (<a
href="https://doi.org/10.1145/3386569.3392464">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a new light-weight face capture system capable of reconstructing both high-quality geometry and detailed appearance maps from a single exposure. Unlike currently employed appearance acquisition systems, the proposed technology does not require active illumination and hence can readily be integrated with passive photogrammetry solutions. These solutions are in widespread use for 3D scanning humans as they can be assembled from off-the-shelf hardware components, but lack the capability of estimating appearance. This paper proposes a solution to overcome this limitation, by adding appearance capture to photogrammetry systems. The only additional hardware requirement to these solutions is that a subset of the cameras are cross-polarized with respect to the illumination, and the remaining cameras are parallel-polarized. The proposed algorithm leverages the images with the two different polarization states to reconstruct the geometry and to recover appearance properties. We do so by means of an inverse rendering framework, which solves per texel diffuse albedo, specular intensity, and high-resolution normals, as well as global specular roughness considering the subsurface scattering nature of skin. We show results for a variety of human subjects of different ages and skin typology, illustrating how the captured fine-detail skin surface and subsurface scattering effects lead to realistic renderings of their digital doubles, also in different illumination conditions.},
  archive      = {J_TOG},
  author       = {Jérémy Riviere and Paulo Gotardo and Derek Bradley and Abhijeet Ghosh and Thabo Beeler},
  doi          = {10.1145/3386569.3392464},
  journal      = {ACM Transactions on Graphics},
  month        = {8},
  number       = {4},
  pages        = {81:81:1–81:12},
  shortjournal = {ACM Trans. Graph.},
  title        = {Single-shot high-quality facial geometry and skin appearance capture},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Single image HDR reconstruction using a CNN with masked
features and perceptual loss. <em>TOG</em>, <em>39</em>(4),
80:80:1–80:10. (<a
href="https://doi.org/10.1145/3386569.3392403">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Digital cameras can only capture a limited range of real-world scenes&#39; luminance, producing images with saturated pixels. Existing single image high dynamic range (HDR) reconstruction methods attempt to expand the range of luminance, but are not able to hallucinate plausible textures, producing results with artifacts in the saturated areas. In this paper, we present a novel learning-based approach to reconstruct an HDR image by recovering the saturated pixels of an input LDR image in a visually pleasing way. Previous deep learning-based methods apply the same convolutional filters on wellexposed and saturated pixels, creating ambiguity during training and leading to checkerboard and halo artifacts. To overcome this problem, we propose a feature masking mechanism that reduces the contribution of the features from the saturated areas. Moreover, we adapt the VGG-based perceptual loss function to our application to be able to synthesize visually pleasing textures. Since the number of HDR images for training is limited, we propose to train our system in two stages. Specifically, we first train our system on a large number of images for image inpainting task and then fine-tune it on HDR reconstruction. Since most of the HDR examples contain smooth regions that are simple to reconstruct, we propose a sampling strategy to select challenging training patches during the HDR fine-tuning stage. We demonstrate through experimental results that our approach can reconstruct visually pleasing HDR results, better than the current state of the art on a wide range of scenes.},
  archive      = {J_TOG},
  author       = {Marcel Santana Santos and Tsang Ing Ren and Nima Khademi Kalantari},
  doi          = {10.1145/3386569.3392403},
  journal      = {ACM Transactions on Graphics},
  month        = {8},
  number       = {4},
  pages        = {80:80:1–80:10},
  shortjournal = {ACM Trans. Graph.},
  title        = {Single image HDR reconstruction using a CNN with masked features and perceptual loss},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Quanta burst photography. <em>TOG</em>, <em>39</em>(4),
79:79:1–79:16. (<a
href="https://doi.org/10.1145/3386569.3392470">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Single-photon avalanche diodes (SPADs) are an emerging sensor technology capable of detecting individual incident photons, and capturing their time-of-arrival with high timing precision. While these sensors were limited to singlepixel or low-resolution devices in the past, recently, large (up to 1 MPixel) SPAD arrays have been developed. These single-photon cameras (SPCs) are capable of capturing high-speed sequences of binary single-photon images with no read noise. We present quanta burst photography, a computational photography technique that leverages SPCs as passive imaging devices for photography in challenging conditions, including ultra low-light and fast motion. Inspired by recent success of conventional burst photography, we design algorithms that align and merge binary sequences captured by SPCs into intensity images with minimal motion blur and artifacts, high signal-to-noise ratio (SNR), and high dynamic range. We theoretically analyze the SNR and dynamic range of quanta burst photography, and identify the imaging regimes where it provides significant benefits. We demonstrate, via a recently developed SPAD array, that the proposed method is able to generate high-quality images for scenes with challenging lighting, complex geometries, high dynamic range and moving objects. With the ongoing development of SPAD arrays, we envision quanta burst photography finding applications in both consumer and scientific photography.},
  archive      = {J_TOG},
  author       = {Sizhuo Ma and Shantanu Gupta and Arin C. Ulku and Claudio Bruschini and Edoardo Charbon and Mohit Gupta},
  doi          = {10.1145/3386569.3392470},
  journal      = {ACM Transactions on Graphics},
  month        = {8},
  number       = {4},
  pages        = {79:79:1–79:16},
  shortjournal = {ACM Trans. Graph.},
  title        = {Quanta burst photography},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020a). Portrait shadow manipulation. <em>TOG</em>, <em>39</em>(4),
78:78:1–78:14. (<a
href="https://doi.org/10.1145/3386569.3392390">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Casually-taken portrait photographs often suffer from unflattering lighting and shadowing because of suboptimal conditions in the environment. Aesthetic qualities such as the position and softness of shadows and the lighting ratio between the bright and dark parts of the face are frequently determined by the constraints of the environment rather than by the photographer. Professionals address this issue by adding light shaping tools such as scrims, bounce cards, and flashes. In this paper, we present a computational approach that gives casual photographers some of this control, thereby allowing poorly-lit portraits to be relit post-capture in a realistic and easily-controllable way. Our approach relies on a pair of neural networks---one to remove foreign shadows cast by external objects, and another to soften facial shadows cast by the features of the subject and to add a synthetic fill light to improve the lighting ratio. To train our first network we construct a dataset of real-world portraits wherein synthetic foreign shadows are rendered onto the face, and we show that our network learns to remove those unwanted shadows. To train our second network we use a dataset of Light Stage scans of human subjects to construct input/output pairs of input images harshly lit by a small light source, and variably softened and fill-lit output images of each face. We propose a way to explicitly encode facial symmetry and show that our dataset and training procedure enable the model to generalize to images taken in the wild. Together, these networks enable the realistic and aesthetically pleasing enhancement of shadows and lights in real-world portrait images. 1},
  archive      = {J_TOG},
  author       = {Xuaner (Cecilia) Zhang and Jonathan T. Barron and Yun-Ta Tsai and Rohit Pandey and Xiuming Zhang and Ren Ng and David E. Jacobs},
  doi          = {10.1145/3386569.3392390},
  journal      = {ACM Transactions on Graphics},
  month        = {8},
  number       = {4},
  pages        = {78:78:1–78:14},
  shortjournal = {ACM Trans. Graph.},
  title        = {Portrait shadow manipulation},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). PolyFit: Perception-aligned vectorization of raster clip-art
via intermediate polygonal fitting. <em>TOG</em>, <em>39</em>(4),
77:77:1–77:16. (<a
href="https://doi.org/10.1145/3386569.3392401">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Raster clip-art images, which consist of distinctly colored regions separated by sharp boundaries typically allow for a clear mental vector interpretation. Converting these images into vector format can facilitate compact lossless storage and enable numerous processing operations. Despite recent progress, existing vectorization methods that target such data frequently produce vectorizations that fail to meet viewer expectations. We present PolyFit , a new clip-art vectorization method that produces vectorizations well aligned with human preferences. Since segmentation of such inputs into regions had been addressed successfully, we specifically focus on fitting piecewise smooth vector curves to the raster input region boundaries, a task prior methods are particularly prone to fail on. While perceptual studies suggest the criteria humans are likely to use during mental boundary vectorization, they provide no guidance as to the exact interaction between them; learning these interactions directly is problematic due to the large size of the solution space. To obtain the desired solution, we first approximate the raster region boundaries with coarse intermediate polygons leveraging a combination of perceptual cues with observations from studies of human preferences. We then use these intermediate polygons as auxiliary inputs for computing piecewise smooth vectorizations of raster inputs. We define a finite set of potential polygon to curve primitive maps, and learn the mapping from the polygons to their best fitting primitive configurations from human annotations, arriving at a compact set of local raster and polygon properties whose combinations reliably predict human-expected primitive choices. We use these primitives to obtain a final globally consistent spline vectorization. Extensive comparative user studies show that our method outperforms state-of-the-art approaches on a wide range of data, where our results are preferred three times as often as those of the closest competitor across multiple types of inputs with various resolutions.},
  archive      = {J_TOG},
  author       = {Edoardo Alberto Dominici and Nico Schertler and Jonathan Griffin and Shayan Hoshyari and Leonid Sigal and Alla Sheffer},
  doi          = {10.1145/3386569.3392401},
  journal      = {ACM Transactions on Graphics},
  month        = {8},
  number       = {4},
  pages        = {77:77:1–77:16},
  shortjournal = {ACM Trans. Graph.},
  title        = {PolyFit: Perception-aligned vectorization of raster clip-art via intermediate polygonal fitting},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). One shot 3D photography. <em>TOG</em>, <em>39</em>(4),
76:76:1–76:13. (<a
href="https://doi.org/10.1145/3386569.3392420">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {3D photography is a new medium that allows viewers to more fully experience a captured moment. In this work, we refer to a 3D photo as one that displays parallax induced by moving the viewpoint (as opposed to a stereo pair with a fixed viewpoint). 3D photos are static in time, like traditional photos, but are displayed with interactive parallax on mobile or desktop screens, as well as on Virtual Reality devices, where viewing it also includes stereo. We present an end-to-end system for creating and viewing 3D photos, and the algorithmic and design choices therein. Our 3D photos are captured in a single shot and processed directly on a mobile device. The method starts by estimating depth from the 2D input image using a new monocular depth estimation network that is optimized for mobile devices. It performs competitively to the state-of-the-art, but has lower latency and peak memory consumption and uses an order of magnitude fewer parameters. The resulting depth is lifted to a layered depth image, and new geometry is synthesized in parallax regions. We synthesize color texture and structures in the parallax regions as well, using an inpainting network, also optimized for mobile devices, on the LDI directly. Finally, we convert the result into a mesh-based representation that can be efficiently transmitted and rendered even on low-end devices and over poor network connections. Altogether, the processing takes just a few seconds on a mobile device, and the result can be instantly viewed and shared. We perform extensive quantitative evaluation to validate our system and compare its new components against the current state-of-the-art.},
  archive      = {J_TOG},
  author       = {Johannes Kopf and Kevin Matzen and Suhib Alsisan and Ocean Quigley and Francis Ge and Yangming Chong and Josh Patterson and Jan-Michael Frahm and Shu Wu and Matthew Yu and Peizhao Zhang and Zijian He and Peter Vajda and Ayush Saraf and Michael Cohen},
  doi          = {10.1145/3386569.3392420},
  journal      = {ACM Transactions on Graphics},
  month        = {8},
  number       = {4},
  pages        = {76:76:1–76:13},
  shortjournal = {ACM Trans. Graph.},
  title        = {One shot 3D photography},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Learning temporal coherence via self-supervision for
GAN-based video generation. <em>TOG</em>, <em>39</em>(4), 75:75:1–75:13.
(<a href="https://doi.org/10.1145/3386569.3392457">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Our work explores temporal self-supervision for GAN-based video generation tasks. While adversarial training successfully yields generative models for a variety of areas, temporal relationships in the generated data are much less explored. Natural temporal changes are crucial for sequential generation tasks, e.g. video super-resolution and unpaired video translation. For the former, state-of-the-art methods often favor simpler norm losses such as L 2 over adversarial training. However, their averaging nature easily leads to temporally smooth results with an undesirable lack of spatial detail. For unpaired video translation, existing approaches modify the generator networks to form spatio-temporal cycle consistencies. In contrast, we focus on improving learning objectives and propose a temporally self-supervised algorithm. For both tasks, we show that temporal adversarial learning is key to achieving temporally coherent solutions without sacrificing spatial detail. We also propose a novel Ping-Pong loss to improve the long-term temporal consistency. It effectively prevents recurrent networks from accumulating artifacts temporally without depressing detailed features. Additionally, we propose a first set of metrics to quantitatively evaluate the accuracy as well as the perceptual quality of the temporal evolution. A series of user studies confirm the rankings computed with these metrics. Code, data, models, and results are provided at https://github.com/thunil/TecoGAN.},
  archive      = {J_TOG},
  author       = {Mengyu Chu and You Xie and Jonas Mayer and Laura Leal-Taixé and Nils Thuerey},
  doi          = {10.1145/3386569.3392457},
  journal      = {ACM Transactions on Graphics},
  month        = {8},
  number       = {4},
  pages        = {75:75:1–75:13},
  shortjournal = {ACM Trans. Graph.},
  title        = {Learning temporal coherence via self-supervision for GAN-based video generation},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Interferometric transmission probing with coded mutual
intensity. <em>TOG</em>, <em>39</em>(4), 74:74:1–74:16. (<a
href="https://doi.org/10.1145/3386569.3392384">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce a new interferometric imaging methodology that we term interferometry with coded mutual intensity, which allows selectively imaging photon paths based on attributes such as their length and endpoints. At the core of our methodology is a new technical result that shows that manipulating the spatial coherence properties of the light source used in an interferometric system is equivalent, through a Fourier transform, to implementing light path probing patterns. These patterns can be applied to either the coherent transmission matrix, or the incoherent light transport matrix describing the propagation of light in a scene. We test our theory by building a prototype inspired by the Michelson interferometer, extended to allow for programmable phase and amplitude modulation of the illumination injected in the interferometer. We use our prototype to perform experiments such as visualizing complex fields, capturing direct and global transport components, acquiring light transport matrices, and performing anisotropic descattering, both in steady-state imaging and, by combining our technique with optical coherence tomography, in transient imaging.},
  archive      = {J_TOG},
  author       = {Alankar Kotwal and Anat Levin and Ioannis Gkioulekas},
  doi          = {10.1145/3386569.3392384},
  journal      = {ACM Transactions on Graphics},
  month        = {8},
  number       = {4},
  pages        = {74:74:1–74:16},
  shortjournal = {ACM Trans. Graph.},
  title        = {Interferometric transmission probing with coded mutual intensity},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Interactive video stylization using few-shot patch-based
training. <em>TOG</em>, <em>39</em>(4), 73:73:1–73:11. (<a
href="https://doi.org/10.1145/3386569.3392453">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we present a learning-based method to the keyframe-based video stylization that allows an artist to propagate the style from a few selected keyframes to the rest of the sequence. Its key advantage is that the resulting stylization is semantically meaningful, i.e., specific parts of moving objects are stylized according to the artist&#39;s intention. In contrast to previous style transfer techniques, our approach does not require any lengthy pre-training process nor a large training dataset. We demonstrate how to train an appearance translation network from scratch using only a few stylized exemplars while implicitly preserving temporal consistency. This leads to a video stylization framework that supports real-time inference, parallel processing, and random access to an arbitrary output frame. It can also merge the content from multiple keyframes without the need to perform an explicit blending operation. We demonstrate its practical utility in various interactive scenarios, where the user paints over a selected keyframe and sees her style transferred to an existing recorded sequence or a live video stream.},
  archive      = {J_TOG},
  author       = {Ondřej Texler and David Futschik and Michal kučera and Ondřej jamriška and Šárka Sochorová and Menclei Chai and Sergey Tulyakov and Daniel SÝkora},
  doi          = {10.1145/3386569.3392453},
  journal      = {ACM Transactions on Graphics},
  month        = {8},
  number       = {4},
  pages        = {73:73:1–73:11},
  shortjournal = {ACM Trans. Graph.},
  title        = {Interactive video stylization using few-shot patch-based training},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). DeepFaceDrawing: Deep generation of face images from
sketches. <em>TOG</em>, <em>39</em>(4), 72:72:1–72:16. (<a
href="https://doi.org/10.1145/3386569.3392386">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent deep image-to-image translation techniques allow fast generation of face images from freehand sketches. However, existing solutions tend to overfit to sketches, thus requiring professional sketches or even edge maps as input. To address this issue, our key idea is to implicitly model the shape space of plausible face images and synthesize a face image in this space to approximate an input sketch. We take a local-to-global approach. We first learn feature embeddings of key face components, and push corresponding parts of input sketches towards underlying component manifolds defined by the feature vectors of face component samples. We also propose another deep neural network to learn the mapping from the embedded component features to realistic images with multi-channel feature maps as intermediate results to improve the information flow. Our method essentially uses input sketches as soft constraints and is thus able to produce high-quality face images even from rough and/or incomplete sketches. Our tool is easy to use even for non-artists, while still supporting fine-grained control of shape details. Both qualitative and quantitative evaluations show the superior generation ability of our system to existing and alternative solutions. The usability and expressiveness of our system are confirmed by a user study.},
  archive      = {J_TOG},
  author       = {Shu-Yu Chen and Wanchao Su and Lin Gao and Shihong Xia and Hongbo Fu},
  doi          = {10.1145/3386569.3392386},
  journal      = {ACM Transactions on Graphics},
  month        = {8},
  number       = {4},
  pages        = {72:72:1–72:16},
  shortjournal = {ACM Trans. Graph.},
  title        = {DeepFaceDrawing: Deep generation of face images from sketches},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Consistent video depth estimation. <em>TOG</em>,
<em>39</em>(4), 71:71:1–71:13. (<a
href="https://doi.org/10.1145/3386569.3392377">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present an algorithm for reconstructing dense, geometrically consistent depth for all pixels in a monocular video. We leverage a conventional structure-from-motion reconstruction to establish geometric constraints on pixels in the video. Unlike the ad-hoc priors in classical reconstruction, we use a learning-based prior, i.e., a convolutional neural network trained for single-image depth estimation. At test time, we fine-tune this network to satisfy the geometric constraints of a particular input video, while retaining its ability to synthesize plausible depth details in parts of the video that are less constrained. We show through quantitative validation that our method achieves higher accuracy and a higher degree of geometric consistency than previous monocular reconstruction methods. Visually, our results appear more stable. Our algorithm is able to handle challenging hand-held captured input videos with a moderate degree of dynamic motion. The improved quality of the reconstruction enables several applications, such as scene reconstruction and advanced video-based visual effects.},
  archive      = {J_TOG},
  author       = {Xuan Luo and Jia-Bin Huang and Richard Szeliski and Kevin Matzen and Johannes Kopf},
  doi          = {10.1145/3386569.3392377},
  journal      = {ACM Transactions on Graphics},
  month        = {8},
  number       = {4},
  pages        = {71:71:1–71:13},
  shortjournal = {ACM Trans. Graph.},
  title        = {Consistent video depth estimation},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Computational image marking on metals via laser induced
heating. <em>TOG</em>, <em>39</em>(4), 70:70:1–70:11. (<a
href="https://doi.org/10.1145/3386569.3392423">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Laser irradiation induces colors on some industrially important materials, such as stainless steel and titanium. It is however challenging to find marking configurations that create colorful, high-resolution images. The brute-force solution to the gamut exploration problem does not scale with the high-dimensional design space of laser marking. Moreover, there exists no color reproduction workflow capable of reproducing color images with laser marking. Here, we propose a measurement-based, data-driven performance space exploration of the color laser marking process. We formulate this exploration as a search for the Pareto optimal solutions to a multi-objective optimization and solve it using an evolutionary algorithm. The explored set of diverse colors is then utilized to mark high-quality, full-color images.},
  archive      = {J_TOG},
  author       = {Sebastian Cucerca and Piotr Didyk and Hans-Peter Seidel and Vahid Babaei},
  doi          = {10.1145/3386569.3392423},
  journal      = {ACM Transactions on Graphics},
  month        = {8},
  number       = {4},
  pages        = {70:70:1–70:11},
  shortjournal = {ACM Trans. Graph.},
  title        = {Computational image marking on metals via laser induced heating},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Attribute2Font: Creating fonts you want from attributes.
<em>TOG</em>, <em>39</em>(4), 69:69:1–69:15. (<a
href="https://doi.org/10.1145/3386569.3392456">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Font design is now still considered as an exclusive privilege of professional designers, whose creativity is not possessed by existing software systems. Nevertheless, we also notice that most commercial font products are in fact manually designed by following specific requirements on some attributes of glyphs, such as italic, serif, cursive, width, angularity, etc. Inspired by this fact, we propose a novel model, Attribute2Font, to automatically create fonts by synthesizing visually pleasing glyph images according to user-specified attributes and their corresponding values. To the best of our knowledge, our model is the first one in the literature which is capable of generating glyph images in new font styles, instead of retrieving existing fonts, according to given values of specified font attributes. Specifically, Attribute2Font is trained to perform font style transfer between any two fonts conditioned on their attribute values. After training, our model can generate glyph images in accordance with an arbitrary set of font attribute values. Furthermore, a novel unit named Attribute Attention Module is designed to make those generated glyph images better embody the prominent font attributes. Considering that the annotations of font attribute values are extremely expensive to obtain, a semi-supervised learning scheme is also introduced to exploit a large number of unlabeled fonts. Experimental results demonstrate that our model achieves impressive performance on many tasks, such as creating glyph images in new font styles, editing existing fonts, interpolation among different fonts, etc.},
  archive      = {J_TOG},
  author       = {Yizhi Wang and Yue Gao and Zhouhui Lian},
  doi          = {10.1145/3386569.3392456},
  journal      = {ACM Transactions on Graphics},
  month        = {8},
  number       = {4},
  pages        = {69:69:1–69:15},
  shortjournal = {ACM Trans. Graph.},
  title        = {Attribute2Font: Creating fonts you want from attributes},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Towards occlusion-aware multifocal displays. <em>TOG</em>,
<em>39</em>(4), 68:68:1–68:15. (<a
href="https://doi.org/10.1145/3386569.3392424">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The human visual system uses numerous cues for depth perception, including disparity, accommodation, motion parallax and occlusion. It is incumbent upon virtual-reality displays to satisfy these cues to provide an immersive user experience. Multifocal displays, one of the classic approaches to satisfy the accommodation cue, place virtual content at multiple focal planes, each at a different depth. However, the content on focal planes close to the eye do not occlude those farther away; this deteriorates the occlusion cue as well as reduces contrast at depth discontinuities due to leakage of the defocus blur. This paper enables occlusion-aware multifocal displays using a novel ConeTilt operator that provides an additional degree of freedom --- tilting the light cone emitted at each pixel of the display panel. We show that, for scenes with relatively simple occlusion configurations, tilting the light cones provides the same effect as physical occlusion. We demonstrate that ConeTilt can be easily implemented by a phase-only spatial light modulator. Using a lab prototype, we show results that demonstrate the presence of occlusion cues and the increased contrast of the display at depth edges.},
  archive      = {J_TOG},
  author       = {Jen-Hao Rick Chang and Anat Levin and B. V. K. Vijaya Kumar and Aswin C. Sankaranarayanan},
  doi          = {10.1145/3386569.3392424},
  journal      = {ACM Transactions on Graphics},
  month        = {8},
  number       = {4},
  pages        = {68:68:1–68:15},
  shortjournal = {ACM Trans. Graph.},
  title        = {Towards occlusion-aware multifocal displays},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Holographic optics for thin and lightweight virtual reality.
<em>TOG</em>, <em>39</em>(4), 67:67:1–67:14. (<a
href="https://doi.org/10.1145/3386569.3392416">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a class of display designs combining holographic optics, directional backlighting, laser illumination, and polarization-based optical folding to achieve thin, lightweight, and high performance near-eye displays for virtual reality. Several design alternatives are proposed, compared, and experimentally validated as prototypes. Using only thin, flat films as optical components, we demonstrate VR displays with thicknesses of less than 9 mm, fields of view of over 90° horizontally, and form factors approaching sunglasses. In a benchtop form factor, we also demonstrate a full color display using wavelength-multiplexed holographic lenses that uses laser illumination to provide a large gamut and highly saturated color. We show experimentally that our designs support resolutions expected of modern VR headsets and can scale to human visual acuity limits. Current limitations are identified, and we discuss challenges to obtain full practicality.},
  archive      = {J_TOG},
  author       = {Andrew Maimone and Junren Wang},
  doi          = {10.1145/3386569.3392416},
  journal      = {ACM Transactions on Graphics},
  month        = {8},
  number       = {4},
  pages        = {67:67:1–67:14},
  shortjournal = {ACM Trans. Graph.},
  title        = {Holographic optics for thin and lightweight virtual reality},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). High resolution étendue expansion for holographic displays.
<em>TOG</em>, <em>39</em>(4), 66:66:1–66:14. (<a
href="https://doi.org/10.1145/3386569.3392414">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Holographic displays can create high quality 3D images while maintaining a small form factor suitable for head-mounted virtual and augmented reality systems. However, holographic displays have limited étendue based on the number of pixels in their spatial light modulators, creating a tradeoff between the eyebox size and the field-of-view. Scattering-based étendue expansion, in which coherent light is focused into an image after being scattered by a static mask, is a promising avenue to break this tradeoff. However, to date, this approach has been limited to very sparse content consisting of, for example, only tens of spots. In this work, we introduce new algorithms to scattering-based étendue expansion that support dense, photorealistic imagery at the native resolution of the spatial light modulator, offering up to a 20 dB improvement in peak signal to noise ratio over baseline methods. We propose spatial and frequency constraints to optimize performance for human perception, and performance is characterized both through simulation and a preliminary benchtop prototype. We further demonstrate the ability to generate content at multiple depths, and we provide a path for the miniaturization of our benchtop prototype into a sunglasses-like form factor.},
  archive      = {J_TOG},
  author       = {Grace Kuo and Laura Waller and Ren Ng and Andrew Maimone},
  doi          = {10.1145/3386569.3392414},
  journal      = {ACM Transactions on Graphics},
  month        = {8},
  number       = {4},
  pages        = {66:66:1–66:14},
  shortjournal = {ACM Trans. Graph.},
  title        = {High resolution étendue expansion for holographic displays},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Wave curves: Simulating lagrangian water waves on
dynamically deforming surfaces. <em>TOG</em>, <em>39</em>(4),
65:65:1–65:11. (<a
href="https://doi.org/10.1145/3386569.3392466">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a method to enhance the visual detail of a water surface simulation. Our method works as a post-processing step which takes a simulation as input and increases its apparent resolution by simulating many detailed Lagrangian water waves on top of it. We extend linear water wave theory to work in non-planar domains which deform over time, and we discretize the theory using Lagrangian wave packets attached to spline curves. The method is numerically stable and trivially parallelizable, and it produces high frequency ripples with dispersive wave-like behaviors customized to the underlying fluid simulation.},
  archive      = {J_TOG},
  author       = {Tomas Skrivan and Andreas Soderstrom and John Johansson and Christoph Sprenger and Ken Museth and Chris Wojtan},
  doi          = {10.1145/3386569.3392466},
  journal      = {ACM Transactions on Graphics},
  month        = {8},
  number       = {4},
  pages        = {65:65:1–65:11},
  shortjournal = {ACM Trans. Graph.},
  title        = {Wave curves: Simulating lagrangian water waves on dynamically deforming surfaces},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Unpaired motion style transfer from video to animation.
<em>TOG</em>, <em>39</em>(4), 64:64:1–64:12. (<a
href="https://doi.org/10.1145/3386569.3392469">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Transferring the motion style from one animation clip to another, while preserving the motion content of the latter, has been a long-standing problem in character animation. Most existing data-driven approaches are supervised and rely on paired data, where motions with the same content are performed in different styles. In addition, these approaches are limited to transfer of styles that were seen during training. In this paper, we present a novel data-driven framework for motion style transfer, which learns from an unpaired collection of motions with style labels, and enables transferring motion styles not observed during training. Furthermore, our framework is able to extract motion styles directly from videos, bypassing 3D reconstruction, and apply them to the 3D input motion. Our style transfer network encodes motions into two latent codes, for content and for style, each of which plays a different role in the decoding (synthesis) process. While the content code is decoded into the output motion by several temporal convolutional layers, the style code modifies deep features via temporally invariant adaptive instance normalization (AdaIN). Moreover, while the content code is encoded from 3D joint rotations, we learn a common embedding for style from either 3D or 2D joint positions, enabling style extraction from videos. Our results are comparable to the state-of-the-art, despite not requiring paired training data, and outperform other methods when transferring previously unseen styles. To our knowledge, we are the first to demonstrate style transfer directly from videos to 3D animations - an ability which enables one to extend the set of style examples far beyond motions captured by MoCap systems.},
  archive      = {J_TOG},
  author       = {Kfir Aberman and Yijia Weng and Dani Lischinski and Daniel Cohen-Or and Baoquan Chen},
  doi          = {10.1145/3386569.3392469},
  journal      = {ACM Transactions on Graphics},
  month        = {8},
  number       = {4},
  pages        = {64:64:1–64:12},
  shortjournal = {ACM Trans. Graph.},
  title        = {Unpaired motion style transfer from video to animation},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). The leopard never changes its spots: Realistic pigmentation
pattern formation by coupling tissue growth with reaction-diffusion.
<em>TOG</em>, <em>39</em>(4), 63:63:1–62:14. (<a
href="https://doi.org/10.1145/3386569.3392478">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Previous research in pattern formation using reaction-diffusion mostly focused on static domains, either for computational simplicity or mathematical tractability. In this work, we have explored the expressiveness of combining simple mechanisms as a possible explanation for pigmentation pattern formation, where tissue growth plays a crucial role. Our motivation is not only to realistically reproduce natural patterns but also to get insights into the underlying biological processes. Therefore, we present a novel approach to generate realistic animal skin patterns. First, we describe the approximation of tissue growth by a series of discrete matrix expansion operations. Then, we combine it with an adaptation of Turing&#39;s non-linear reaction-diffusion model, which enforces upper and lower bounds to the concentrations of the involved chemical reagents. We also propose the addition of a single-reagent continuous autocatalytic reaction, called reinforcement, to provide a mechanism to maintain an already established pattern during growth. By careful adjustment of the parameters and the sequencing of operations, we closely match the appearance of a few real species. In particular, we reproduce in detail the distinctive features of the leopard skin, also providing a hypothesis for the simultaneous productions of the most common melanin types, eumelanin and pheomelanin.},
  archive      = {J_TOG},
  author       = {Marcelo De Gomensoro Malheiros and Henrique Fensterseifer and Marcelo Walter},
  doi          = {10.1145/3386569.3392478},
  journal      = {ACM Transactions on Graphics},
  month        = {8},
  number       = {4},
  pages        = {63:63:1–62:14},
  shortjournal = {ACM Trans. Graph.},
  title        = {The leopard never changes its spots: Realistic pigmentation pattern formation by coupling tissue growth with reaction-diffusion},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Skeleton-aware networks for deep motion retargeting.
<em>TOG</em>, <em>39</em>(4), 62:62:1–62:14. (<a
href="https://doi.org/10.1145/3386569.3392462">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce a novel deep learning framework for data-driven motion retargeting between skeletons, which may have different structure, yet corresponding to homeomorphic graphs. Importantly, our approach learns how to retarget without requiring any explicit pairing between the motions in the training set. We leverage the fact that different homeomorphic skeletons may be reduced to a common primal skeleton by a sequence of edge merging operations, which we refer to as skeletal pooling. Thus, our main technical contribution is the introduction of novel differentiable convolution, pooling, and unpooling operators. These operators are skeleton-aware , meaning that they explicitly account for the skeleton&#39;s hierarchical structure and joint adjacency, and together they serve to transform the original motion into a collection of deep temporal features associated with the joints of the primal skeleton. In other words, our operators form the building blocks of a new deep motion processing framework that embeds the motion into a common latent space, shared by a collection of homeomorphic skeletons. Thus, retargeting can be achieved simply by encoding to, and decoding from this latent space. Our experiments show the effectiveness of our framework for motion retargeting, as well as motion processing in general, compared to existing approaches. Our approach is also quantitatively evaluated on a synthetic dataset that contains pairs of motions applied to different skeletons. To the best of our knowledge, our method is the first to perform retargeting between skeletons with differently sampled kinematic chains, without any paired examples.},
  archive      = {J_TOG},
  author       = {Kfir Aberman and Peizhuo Li and Dani Lischinski and Olga Sorkine-Hornung and Daniel Cohen-Or and Baoquan Chen},
  doi          = {10.1145/3386569.3392462},
  journal      = {ACM Transactions on Graphics},
  month        = {8},
  number       = {4},
  pages        = {62:62:1–62:14},
  shortjournal = {ACM Trans. Graph.},
  title        = {Skeleton-aware networks for deep motion retargeting},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Simple and scalable frictional contacts for thin nodal
objects. <em>TOG</em>, <em>39</em>(4), 61:61:1–61:16. (<a
href="https://doi.org/10.1145/3386569.3392439">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Frictional contacts are the primary way by which physical bodies interact, yet they pose many numerical challenges. Previous works have devised robust methods for handling collisions in elastic bodies, cloth, or fiber assemblies such as hair, but the performance of many of those algorithms degrades when applied to objects with different topologies or constitutive models, or simply cannot scale to high-enough numbers of contacting points. In this work we propose a unified approach, able to handle a large class of dynamical objects, that can solve for millions of contacts with unbiased Coulomb friction while keeping computation time and memory usage reasonable. Our method allows seamless coupling between the various simulated components that comprise virtual characters and their environment. Furthermore, our proposed approach is simple to implement and can be easily integrated in popular time integrators such as Projected Newton or ADMM.},
  archive      = {J_TOG},
  author       = {Gilles Daviet},
  doi          = {10.1145/3386569.3392439},
  journal      = {ACM Transactions on Graphics},
  month        = {8},
  number       = {4},
  pages        = {61:61:1–61:16},
  shortjournal = {ACM Trans. Graph.},
  title        = {Simple and scalable frictional contacts for thin nodal objects},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Robust motion in-betweening. <em>TOG</em>, <em>39</em>(4),
60:60:1–60:12. (<a
href="https://doi.org/10.1145/3386569.3392480">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this work we present a novel, robust transition generation technique that can serve as a new tool for 3D animators, based on adversarial recurrent neural networks. The system synthesises high-quality motions that use temporally-sparse keyframes as animation constraints. This is reminiscent of the job of in-betweening in traditional animation pipelines, in which an animator draws motion frames between provided keyframes. We first show that a state-of-the-art motion prediction model cannot be easily converted into a robust transition generator when only adding conditioning information about future keyframes. To solve this problem, we then propose two novel additive embedding modifiers that are applied at each timestep to latent representations encoded inside the network&#39;s architecture. One modifier is a time-to-arrival embedding that allows variations of the transition length with a single model. The other is a scheduled target noise vector that allows the system to be robust to target distortions and to sample different transitions given fixed keyframes. To qualitatively evaluate our method, we present a custom MotionBuilder plugin that uses our trained model to perform in-betweening in production scenarios. To quantitatively evaluate performance on transitions and generalizations to longer time horizons, we present well-defined in-betweening benchmarks on a subset of the widely used Human3.6M dataset and on LaFAN1, a novel high quality motion capture dataset that is more appropriate for transition generation. We are releasing this new dataset along with this work, with accompanying code for reproducing our baseline results.},
  archive      = {J_TOG},
  author       = {Félix G. Harvey and Mike Yurick and Derek Nowrouzezahrai and Christopher Pal},
  doi          = {10.1145/3386569.3392480},
  journal      = {ACM Transactions on Graphics},
  month        = {8},
  number       = {4},
  pages        = {60:60:1–60:12},
  shortjournal = {ACM Trans. Graph.},
  title        = {Robust motion in-betweening},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Robust eulerian-on-lagrangian rods. <em>TOG</em>,
<em>39</em>(4), 59:59:1–59:10. (<a
href="https://doi.org/10.1145/3386569.3392489">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper introduces a method to simulate complex rod assemblies and stacked layers with implicit contact handling, through Eulerian-on-Lagrangian (EoL) discretizations. Previous EoL methods fail to handle such complex situations, due to ubiquitous and intrinsic degeneracies in the contact geometry, which prevent the use of remeshing and make simulations unstable. We propose a novel mixed Eulerian-Lagrangian discretization that supports accurate and efficient contact as in EoL methods, but is transparent to internal rod forces, and hence insensitive to degeneracies. By combining the standard and novel EoL discretizations as appropriate, we derive mixed statics-dynamics equations of motion that can be solved in a unified manner with standard solvers. Our solution is simple and elegant in practice, and produces robust simulations on large-scale scenarios with complex rod arrangements and pervasive degeneracies. We demonstrate our method on multi-layer yarn-level cloth simulations, with implicit handling of both intra-and inter-layer contacts.},
  archive      = {J_TOG},
  author       = {Rosa M. Sánchez-Banderas and Alejandro Rodríguez and Héctor Barreiro and Miguel A. Otaduy},
  doi          = {10.1145/3386569.3392489},
  journal      = {ACM Transactions on Graphics},
  month        = {8},
  number       = {4},
  pages        = {59:59:1–59:10},
  shortjournal = {ACM Trans. Graph.},
  title        = {Robust eulerian-on-lagrangian rods},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). RigNet: Neural rigging for articulated characters.
<em>TOG</em>, <em>39</em>(4), 58:58:1–58:14. (<a
href="https://doi.org/10.1145/3386569.3392379">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present RigNet , an end-to-end automated method for producing animation rigs from input character models. Given an input 3D model representing an articulated character, RigNet predicts a skeleton that matches the animator expectations in joint placement and topology. It also estimates surface skin weights based on the predicted skeleton. Our method is based on a deep architecture that directly operates on the mesh representation without making assumptions on shape class and structure. The architecture is trained on a large and diverse collection of rigged models, including their mesh, skeletons and corresponding skin weights. Our evaluation is three-fold: we show better results than prior art when quantitatively compared to animator rigs; qualitatively we show that our rigs can be expressively posed and animated at multiple levels of detail; and finally, we evaluate the impact of various algorithm choices on our output rigs. 1},
  archive      = {J_TOG},
  author       = {Zhan Xu and Yang Zhou and Evangelos Kalogerakis and Chris Landreth and Karan Singh},
  doi          = {10.1145/3386569.3392379},
  journal      = {ACM Transactions on Graphics},
  month        = {8},
  number       = {4},
  pages        = {58:58:1–58:14},
  shortjournal = {ACM Trans. Graph.},
  title        = {RigNet: Neural rigging for articulated characters},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Projective dynamics with dry frictional contact.
<em>TOG</em>, <em>39</em>(4), 57:57:1–57:8. (<a
href="https://doi.org/10.1145/3386569.3392396">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Projective dynamics was introduced a few years ago as a fast method to yield an approximate yet stable solution to the dynamics of nodal systems subject to stiff internal forces. Previous attempts to include contact forces in that framework considered adding a quadratic penalty energy to the global system, which however broke the simple - constant matrix - structure of the global linear equation, while failing to treat contact in an implicit manner. In this paper we propose a simple yet effective method to integrate in a unified and semi-implicit way contact as well as dry frictional forces into the nested architecture of Projective dynamics. Assuming that contacts apply to nodes only, the key is to split the global matrix into a diagonal and a positive matrix, and use this splitting in the local step so as to make a good prediction of frictional contact forces at next iteration. Each frictional contact force is refined independently in the local step, while the original efficient structure of the global step is left unchanged. We apply our algorithm to cloth simulation and show that contact and dry friction can be captured at a reasonable precision within a few iterations only, hence one order of magnitude faster compared to global implicit contact solvers of the literature.},
  archive      = {J_TOG},
  author       = {Mickaël Ly and Jean Jouve and Laurence Boissieux and Florence Bertails-Descoubes},
  doi          = {10.1145/3386569.3392396},
  journal      = {ACM Transactions on Graphics},
  month        = {8},
  number       = {4},
  pages        = {57:57:1–57:8},
  shortjournal = {ACM Trans. Graph.},
  title        = {Projective dynamics with dry frictional contact},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Phong deformation: A better c0 interpolant for embedded
deformation. <em>TOG</em>, <em>39</em>(4), 56:56:1–56:9. (<a
href="https://doi.org/10.1145/3386569.3392371">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Physics-based simulations of deforming tetrahedral meshes are widely used to animate detailed embedded geometry. Unfortunately most practitioners still use linear interpolation (or other low-order schemes) on tetrahedra, which can produce undesirable visual artifacts, e.g., faceting and shading artifacts, that necessitate increasing the simulation&#39;s spatial resolution and, unfortunately, cost. In this paper, we propose Phong Deformation , a simple, robust and practical vertex-based quadratic interpolation scheme that, while still only C 0 continuous like linear interpolation, greatly reduces visual artifacts for embedded geometry. The method first averages element-based linear deformation models to vertices, then barycentrically interpolates the vertex models while also averaging with the traditional linear interpolation model. The method is a fast, robust, and easily implemented replacement for linear interpolation that produces visually better results for embedded deformation with irregular tetrahedral meshes.},
  archive      = {J_TOG},
  author       = {Doug L. James},
  doi          = {10.1145/3386569.3392371},
  journal      = {ACM Transactions on Graphics},
  month        = {8},
  number       = {4},
  pages        = {56:56:1–56:9},
  shortjournal = {ACM Trans. Graph.},
  title        = {Phong deformation: A better c0 interpolant for embedded deformation},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). N-dimensional rigid body dynamics. <em>TOG</em>,
<em>39</em>(4), 55:55:1–55:6. (<a
href="https://doi.org/10.1145/3386569.3392483">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {I present a formulation for Rigid Body Dynamics that is independent of the dimension of the space. I describe the state and equations of motion of rigid bodies using geometric algebra. Using collision detection algorithms extended to n D I resolve collisions and contact between bodies. My implementation is 4D, but the techniques described here apply to any number of dimensions. I display these four-dimensional rigid bodies by taking a three-dimensional slice through them. I allow the user to manipulate these bodies in real-time.},
  archive      = {J_TOG},
  author       = {Marc Ten Bosch},
  doi          = {10.1145/3386569.3392483},
  journal      = {ACM Transactions on Graphics},
  month        = {8},
  number       = {4},
  pages        = {55:55:1–55:6},
  shortjournal = {ACM Trans. Graph.},
  title        = {N-dimensional rigid body dynamics},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Local motion phases for learning multi-contact character
movements. <em>TOG</em>, <em>39</em>(4), 54:54:1–54:13. (<a
href="https://doi.org/10.1145/3386569.3392450">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Training a bipedal character to play basketball and interact with objects, or a quadruped character to move in various locomotion modes, are difficult tasks due to the fast and complex contacts happening during the motion. In this paper, we propose a novel framework to learn fast and dynamic character interactions that involve multiple contacts between the body and an object, another character and the environment, from a rich, unstructured motion capture database. We use one-on-one basketball play and character interactions with the environment as examples. To achieve this task, we propose a novel feature called local motion phase, that can help neural networks to learn asynchronous movements of each bone and its interaction with external objects such as a ball or an environment. We also propose a novel generative scheme to reproduce a wide variation of movements from abstract control signals given by a gamepad, which can be useful for changing the style of the motion under the same context. Our scheme is useful for animating contact-rich, complex interactions for real-time applications such as computer games.},
  archive      = {J_TOG},
  author       = {Sebastian Starke and Yiwei Zhao and Taku Komura and Kazi Zaman},
  doi          = {10.1145/3386569.3392450},
  journal      = {ACM Transactions on Graphics},
  month        = {8},
  number       = {4},
  pages        = {54:54:1–54:13},
  shortjournal = {ACM Trans. Graph.},
  title        = {Local motion phases for learning multi-contact character movements},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Learned motion matching. <em>TOG</em>, <em>39</em>(4),
53:53:1–53:12. (<a
href="https://doi.org/10.1145/3386569.3392440">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper we present a learned alternative to the Motion Matching algorithm which retains the positive properties of Motion Matching but additionally achieves the scalability of neural-network-based generative models. Although neural-network-based generative models for character animation are capable of learning expressive, compact controllers from vast amounts of animation data, methods such as Motion Matching still remain a popular choice in the games industry due to their flexibility, predictability, low preprocessing time, and visual quality - all properties which can sometimes be difficult to achieve with neural-network-based methods. Yet, unlike neural networks, the memory usage of such methods generally scales linearly with the amount of data used, resulting in a constant trade-off between the diversity of animation which can be produced and real world production budgets. In this work we combine the benefits of both approaches and, by breaking down the Motion Matching algorithm into its individual steps, show how learned, scalable alternatives can be used to replace each operation in turn. Our final model has no need to store animation data or additional matching meta-data in memory, meaning it scales as well as existing generative models. At the same time, we preserve the behavior of Motion Matching, retaining the quality, control, and quick iteration time which are so important in the industry.},
  archive      = {J_TOG},
  author       = {Daniel Holden and Oussama Kanoun and Maksym Perepichka and Tiberiu Popa},
  doi          = {10.1145/3386569.3392440},
  journal      = {ACM Transactions on Graphics},
  month        = {8},
  number       = {4},
  pages        = {53:53:1–53:12},
  shortjournal = {ACM Trans. Graph.},
  title        = {Learned motion matching},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Lagrangian neural style transfer for fluids. <em>TOG</em>,
<em>39</em>(4), 52:52:1–52:10. (<a
href="https://doi.org/10.1145/3386569.3392473">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Artistically controlling the shape, motion and appearance of fluid simulations pose major challenges in visual effects production. In this paper, we present a neural style transfer approach from images to 3D fluids formulated in a Lagrangian viewpoint. Using particles for style transfer has unique benefits compared to grid-based techniques. Attributes are stored on the particles and hence are trivially transported by the particle motion. This intrinsically ensures temporal consistency of the optimized stylized structure and notably improves the resulting quality. Simultaneously, the expensive, recursive alignment of stylization velocity fields of grid approaches is unnecessary, reducing the computation time to less than an hour and rendering neural flow stylization practical in production settings. Moreover, the Lagrangian representation improves artistic control as it allows for multi-fluid stylization and consistent color transfer from images, and the generality of the method enables stylization of smoke and liquids likewise.},
  archive      = {J_TOG},
  author       = {Byungsoo Kim and Vinicius C. Azevedo and Markus Gross and Barbara Solenthaler},
  doi          = {10.1145/3386569.3392473},
  journal      = {ACM Transactions on Graphics},
  month        = {8},
  number       = {4},
  pages        = {52:52:1–52:10},
  shortjournal = {ACM Trans. Graph.},
  title        = {Lagrangian neural style transfer for fluids},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). IQ-MPM: An interface quadrature material point method for
non-sticky strongly two-way coupled nonlinear solids and fluids.
<em>TOG</em>, <em>39</em>(4), 51:51:1–51:16. (<a
href="https://doi.org/10.1145/3386569.3392438">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a novel scheme for simulating two-way coupled interactions between nonlinear elastic solids and incompressible fluids. The key ingredient of this approach is a ghost matrix operator-splitting scheme for strongly coupled nonlinear elastica and incompressible fluids through the weak form of their governing equations. This leads to a stable and efficient method handling large time steps under the CFL limit while using a single monolithic solve for the coupled pressure fields, even in the case with highly nonlinear elastic solids. The use of the Material Point Method (MPM) is essential in the designing of the scheme, it not only preserves discretization consistency with the hybrid Lagrangian-Eulerian fluid solver, but also works naturally with our novel interface quadrature (IQ) discretization for free-slip boundary conditions. While traditional MPM suffers from sticky numerical artifacts, our framework naturally supports discontinuous tangential velocities at the solid-fluid interface. Our IQ discretization results in an easy-to-implement, fully particle-based treatment of the interfacial boundary, avoiding the additional complexities associated with intermediate level set or explicit mesh representations. The efficacy of the proposed scheme is verified by various challenging simulations with fluid-elastica interactions.},
  archive      = {J_TOG},
  author       = {Yu Fang and Ziyin Qu and Minchen Li and Xinxin Zhang and Yixin Zhu and Mridul Aanjaneya and Chenfanfu Jiang},
  doi          = {10.1145/3386569.3392438},
  journal      = {ACM Transactions on Graphics},
  month        = {8},
  number       = {4},
  pages        = {51:51:1–51:16},
  shortjournal = {ACM Trans. Graph.},
  title        = {IQ-MPM: An interface quadrature material point method for non-sticky strongly two-way coupled nonlinear solids and fluids},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Informative scene decomposition for crowd analysis,
comparison and simulation guidance. <em>TOG</em>, <em>39</em>(4),
50:50:1–50:13. (<a
href="https://doi.org/10.1145/3386569.3392407">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Crowd simulation is a central topic in several fields including graphics. To achieve high-fidelity simulations, data has been increasingly relied upon for analysis and simulation guidance. However, the information in real-world data is often noisy, mixed and unstructured, making it difficult for effective analysis, therefore has not been fully utilized. With the fast-growing volume of crowd data, such a bottleneck needs to be addressed. In this paper, we propose a new framework which comprehensively tackles this problem. It centers at an unsupervised method for analysis. The method takes as input raw and noisy data with highly mixed multi-dimensional (space, time and dynamics) information, and automatically structure it by learning the correlations among these dimensions. The dimensions together with their correlations fully describe the scene semantics which consists of recurring activity patterns in a scene, manifested as space flows with temporal and dynamics profiles. The effectiveness and robustness of the analysis have been tested on datasets with great variations in volume, duration, environment and crowd dynamics. Based on the analysis, new methods for data visualization, simulation evaluation and simulation guidance are also proposed. Together, our framework establishes a highly automated pipeline from raw data to crowd analysis, comparison and simulation guidance. Extensive experiments and evaluations have been conducted to show the flexibility, versatility and intuitiveness of our framework.},
  archive      = {J_TOG},
  author       = {Feixiang He and Yuanhang Xiang and Xi Zhao and He Wang},
  doi          = {10.1145/3386569.3392407},
  journal      = {ACM Transactions on Graphics},
  month        = {8},
  number       = {4},
  pages        = {50:50:1–50:13},
  shortjournal = {ACM Trans. Graph.},
  title        = {Informative scene decomposition for crowd analysis, comparison and simulation guidance},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Incremental potential contact: Intersection-and
inversion-free, large-deformation dynamics. <em>TOG</em>,
<em>39</em>(4), 49:49:1–49:20. (<a
href="https://doi.org/10.1145/3386569.3392425">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Contacts weave through every aspect of our physical world, from daily household chores to acts of nature. Modeling and predictive computation of these phenomena for solid mechanics is important to every discipline concerned with the motion of mechanical systems, including engineering and animation. Nevertheless, efficiently time-stepping accurate and consistent simulations of real-world contacting elastica remains an outstanding computational challenge. To model the complex interaction of deforming solids in contact we propose Incremental Potential Contact (IPC) - a new model and algorithm for variationally solving implicitly time-stepped nonlinear elastodynamics. IPC maintains an intersection- and inversion-free trajectory regardless of material parameters, time step sizes, impact velocities, severity of deformation, or boundary conditions enforced. Constructed with a custom nonlinear solver, IPC enables efficient resolution of time-stepping problems with separate, user-exposed accuracy tolerances that allow independent specification of the physical accuracy of the dynamics and the geometric accuracy of surface-to-surface conformation. This enables users to decouple, as needed per application, desired accuracies for a simulation&#39;s dynamics and geometry. The resulting time stepper solves contact problems that are intersection-free (and thus robust), inversion-free, efficient (at speeds comparable to or faster than available methods that lack both convergence and feasibility), and accurate (solved to user-specified accuracies). To our knowledge this is the first implicit time-stepping method, across both the engineering and graphics literature that can consistently enforce these guarantees as we vary simulation parameters. In an extensive comparison of available simulation methods, research libraries and commercial codes we confirm that available engineering and computer graphics methods, while each succeeding admirably in custom-tuned regimes, often fail with instabilities, egregious constraint violations and/or inaccurate and implausible solutions, as we vary input materials, contact numbers and time step. We also exercise IPC across a wide range of existing and new benchmark tests and demonstrate its accurate solution over a broad sweep of reasonable time-step sizes and beyond (up to h = 2s) across challenging large-deformation, large-contact stress-test scenarios with meshes composed of up to 2.3M tetrahedra and processing up to 498K contacts per time step. For applications requiring high-accuracy we demonstrate tight convergence on all measures. While, for applications requiring lower accuracies, e.g. animation, we confirm IPC can ensure feasibility and plausibility even when specified tolerances are lowered for efficiency.},
  archive      = {J_TOG},
  author       = {Minchen Li and Zachary Ferguson and Teseo Schneider and Timothy Langlois and Denis Zorin and Daniele Panozzo and Chenfanfu Jiang and Danny M. Kaufman},
  doi          = {10.1145/3386569.3392425},
  journal      = {ACM Transactions on Graphics},
  month        = {8},
  number       = {4},
  pages        = {49:49:1–49:20},
  shortjournal = {ACM Trans. Graph.},
  title        = {Incremental potential contact: Intersection-and inversion-free, large-deformation dynamics},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Homogenized yarn-level cloth. <em>TOG</em>, <em>39</em>(4),
48:48:1–48:15. (<a
href="https://doi.org/10.1145/3386569.3392412">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a method for animating yarn-level cloth effects using a thin-shell solver. We accomplish this through numerical homogenization: we first use a large number of yarn-level simulations to build a model of the potential energy density of the cloth, and then use this energy density function to compute forces in a thin shell simulator. We model several yarn-based materials, including both woven and knitted fabrics. Our model faithfully reproduces expected effects like the stiffness of woven fabrics, and the highly deformable nature and anisotropy of knitted fabrics. Our approach does not require any real-world experiments nor measurements; because the method is based entirely on simulations, it can generate entirely new material models quickly, without the need for testing apparatuses or human intervention. We provide data-driven models of several woven and knitted fabrics, which can be used for efficient simulation with an off-the-shelf cloth solver.},
  archive      = {J_TOG},
  author       = {Georg Sperl and Rahul Narain and Chris Wojtan},
  doi          = {10.1145/3386569.3392412},
  journal      = {ACM Transactions on Graphics},
  month        = {8},
  number       = {4},
  pages        = {48:48:1–48:15},
  shortjournal = {ACM Trans. Graph.},
  title        = {Homogenized yarn-level cloth},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Fast and scalable turbulent flow simulation with two-way
coupling. <em>TOG</em>, <em>39</em>(4), 47:47:1–47:20. (<a
href="https://doi.org/10.1145/3386569.3392400">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Despite their cinematic appeal, turbulent flows involving fluid-solid coupling remain a computational challenge in animation. At the root of this current limitation is the numerical dispersion from which most accurate Navier-Stokes solvers suffer: proper coupling between fluid and solid often generates artificial dispersion in the form of local, parasitic trains of velocity oscillations, eventually leading to numerical instability. While successive improvements over the years have led to conservative and detail-preserving fluid integrators, the dispersive nature of these solvers is rarely discussed despite its dramatic impact on fluid-structure interaction. In this paper, we introduce a novel low-dissipation and low-dispersion fluid solver that can simulate two-way coupling in an efficient and scalable manner, even for turbulent flows. In sharp contrast with most current CG approaches, we construct our solver from a kinetic formulation of the flow derived from statistical mechanics. Unlike existing lattice Boltzmann solvers, our approach leverages high-order moment relaxations as a key to controlling both dissipation and dispersion of the resulting scheme. Moreover, we combine our new fluid solver with the immersed boundary method to easily handle fluid-solid coupling through time adaptive simulations. Our kinetic solver is highly parallelizable by nature, making it ideally suited for implementation on single- or multi-GPU computing platforms. Extensive comparisons with existing solvers on synthetic tests and real-life experiments are used to highlight the multiple advantages of our work over traditional and more recent approaches, in terms of accuracy, scalability, and efficiency.},
  archive      = {J_TOG},
  author       = {Wei Li and Yixin Chen and Mathieu Desbrun and Changxi Zheng and Xiaopei Liu},
  doi          = {10.1145/3386569.3392400},
  journal      = {ACM Transactions on Graphics},
  month        = {8},
  number       = {4},
  pages        = {47:47:1–47:20},
  shortjournal = {ACM Trans. Graph.},
  title        = {Fast and scalable turbulent flow simulation with two-way coupling},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Fast and flexible multilegged locomotion using learned
centroidal dynamics. <em>TOG</em>, <em>39</em>(4), 46:46:1–46:17. (<a
href="https://doi.org/10.1145/3386569.3392432">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a flexible and efficient approach for generating multilegged locomotion. Our model-predictive control (MPC) system efficiently generates terrain-adaptive motions, as computed using a three-level planning approach. This leverages two commonly-used simplified dynamics models, an inverted pendulum on a cart model (IPC) and a centroidal dynamics model (CDM). Taken together, these ensure efficient computation and physical fidelity of the resulting motion. The final full-body motion is generated using a novel momentum-mapped inverse kinematics solver and is responsive to external pushes by using CDM forward dynamics. For additional efficiency and robustness, we then learn a predictive model that then replaces two of the intermediate steps. We demonstrate the rich capabilities of the method by applying it to monopeds, bipeds, and quadrupeds, and showing that it can generate a very broad range of motions at interactive rates, including banked variable-terrain walking and running, hurdles, jumps, leaps, stepping stones, monkey bars, implicit quadruped gait transitions, moon gravity, push-responses, and more.},
  archive      = {J_TOG},
  author       = {Taesoo Kwon and Yoonsang Lee and Michiel Van De Panne},
  doi          = {10.1145/3386569.3392432},
  journal      = {ACM Transactions on Graphics},
  month        = {8},
  number       = {4},
  pages        = {46:46:1–46:17},
  shortjournal = {ACM Trans. Graph.},
  title        = {Fast and flexible multilegged locomotion using learned centroidal dynamics},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Example-driven virtual cinematography by learning camera
behaviors. <em>TOG</em>, <em>39</em>(4), 45:45:1–45:14. (<a
href="https://doi.org/10.1145/3386569.3392427">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Designing a camera motion controller that has the capacity to move a virtual camera automatically in relation with contents of a 3D animation, in a cinematographic and principled way, is a complex and challenging task. Many cinematographic rules exist, yet practice shows there are significant stylistic variations in how these can be applied. In this paper, we propose an example-driven camera controller which can extract camera behaviors from an example film clip and re-apply the extracted behaviors to a 3D animation, through learning from a collection of camera motions. Our first technical contribution is the design of a low-dimensional cinematic feature space that captures the essence of a film&#39;s cinematic characteristics (camera angle and distance, screen composition and character configurations) and which is coupled with a neural network to automatically extract these cinematic characteristics from real film clips. Our second technical contribution is the design of a cascaded deep-learning architecture trained to (i) recognize a variety of camera motion behaviors from the extracted cinematic features, and (ii) predict the future motion of a virtual camera given a character 3D animation. We propose to rely on a Mixture of Experts (MoE) gating+prediction mechanism to ensure that distinct camera behaviors can be learned while ensuring generalization. We demonstrate the features of our approach through experiments that highlight (i) the quality of our cinematic feature extractor (ii) the capacity to learn a range of behaviors through the gating mechanism, and (iii) the ability to generate a variety of camera motions by applying different behaviors extracted from film clips. Such an example-driven approach offers a high level of controllability which opens new possibilities toward a deeper understanding of cinematographic style and enhanced possibilities in exploiting real film data in virtual environments.},
  archive      = {J_TOG},
  author       = {Hongda Jiang and Bin Wang and Xi Wang and Marc Christie and Baoquan Chen},
  doi          = {10.1145/3386569.3392427},
  journal      = {ACM Transactions on Graphics},
  month        = {8},
  number       = {4},
  pages        = {45:45:1–45:14},
  shortjournal = {ACM Trans. Graph.},
  title        = {Example-driven virtual cinematography by learning camera behaviors},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Directional sources and listeners in interactive sound
propagation using reciprocal wave field coding. <em>TOG</em>,
<em>39</em>(4), 44:44:1–44:14. (<a
href="https://doi.org/10.1145/3386569.3392459">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Common acoustic sources, like voices or musical instruments, exhibit strong frequency and directional dependence. When transported through complex environments, their anisotropic radiated field undergoes scattering, diffraction, and occlusion before reaching a directionally-sensitive listener. We present the first wave-based interactive auralization system that encodes and renders a complete reciprocal description of acoustic wave fields in general scenes. Our method renders directional effects at freely moving and rotating sources and listeners and supports any tabulated source directivity function and head-related transfer function. We represent a static scene&#39;s global acoustic transfer as an 11-dimensional bidirectional impulse response (BIR) field, which we extract from a set of wave simulations. We parametrically encode the BIR as a pair of radiating and arriving directions for the perceptually-salient initial ( direct ) response, and a compact 6 × 6 reflections transfer matrix capturing indirect energy transfer with scene-dependent anisotropy. We render our encoded data with an efficient and scalable algorithm - integrated in the Unreal Engine ™ - whose CPU performance is agnostic to scene complexity and angular source/listener resolutions. We demonstrate convincing effects that depend on detailed scene geometry, for a variety of environments and source types.},
  archive      = {J_TOG},
  author       = {Chakravarty R. Alla Chaitanya and Nikunj Raghuvanshi and Keith W. Godin and Zechen Zhang and Derek Nowrouzezahrai and John M. Snyder},
  doi          = {10.1145/3386569.3392459},
  journal      = {ACM Transactions on Graphics},
  month        = {8},
  number       = {4},
  pages        = {44:44:1–44:14},
  shortjournal = {ACM Trans. Graph.},
  title        = {Directional sources and listeners in interactive sound propagation using reciprocal wave field coding},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Constraint bubbles and affine regions: Reduced fluid models
for efficient immersed bubbles and flexible spatial coarsening.
<em>TOG</em>, <em>39</em>(4), 43:43:1–43:15. (<a
href="https://doi.org/10.1145/3386569.3392455">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose to enhance the capability of standard free-surface flow simulators with efficient support for immersed bubbles through two new models: constraint-based bubbles and affine fluid regions. Unlike its predecessors, our constraint-based model entirely dispenses with the need for advection or projection inside zero-density bubbles, with extremely modest additional computational overhead that is proportional to the surface area of all bubbles. This surface-only approach is easy to implement, realistically captures many familiar bubble behaviors, and even allows two or more distinct liquid bodies to correctly interact across completely unsimulated air. We augment this model with a per-bubble volume-tracking and correction framework to minimize the cumulative effects of gradual volume drift. To support bubbles with non-zero densities, we propose a novel reduced model for an irregular fluid region with a single pointwise incompressible affine vector field. This model requires only 11 interior velocity degrees of freedom per affine fluid region in 3D, and correctly reproduces buoyant, stationary, and sinking behaviors of a secondary fluid phase with non-zero density immersed in water. Since the pressure projection step in both the above schemes is a slightly modified Poisson-style system, we propose novel Multigrid-based preconditioners for Conjugate Gradients for fast numerical solutions of our new discretizations. Furthermore, we observe that by enforcing an incompressible affine vector field over a coalesced set of grid cells, our reduced model is effectively an irregular coarse super-cell. This offers a convenient and flexible adaptive coarsening strategy that integrates readily with the standard staggered grid approach for fluid simulation, yet supports coarsened regions that are arbitrary voxelized shapes, and provides an analytically divergence-free interior. We demonstrate its effectiveness with a new adaptive liquid simulator whose interior regions are coarsened into a mix of tiles with regular and irregular shapes.},
  archive      = {J_TOG},
  author       = {Ryan Goldade and Mridul Aanjaneya and Christopher Batty},
  doi          = {10.1145/3386569.3392455},
  journal      = {ACM Transactions on Graphics},
  month        = {8},
  number       = {4},
  pages        = {43:43:1–43:15},
  shortjournal = {ACM Trans. Graph.},
  title        = {Constraint bubbles and affine regions: Reduced fluid models for efficient immersed bubbles and flexible spatial coarsening},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Codimensional surface tension flow using
moving-least-squares particles. <em>TOG</em>, <em>39</em>(4),
42:42:1–42:14. (<a
href="https://doi.org/10.1145/3386569.3392487">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a new Eulerian-Lagrangian approach to simulate the various surface tension phenomena characterized by volume, thin sheets, thin filaments, and points using Moving-Least-Squares (MLS) particles. At the center of our approach is a meshless Lagrangian description of the different types of codimensional geometries and their transitions using an MLS approximation. In particular, we differentiate the codimension-1 and codimension-2 geometries on Lagrangian MLS particles to precisely describe the evolution of thin sheets and filaments, and we discretize the codimension-0 operators on a background Cartesian grid for efficient volumetric processing. Physical forces including surface tension and pressure across different codimensions are coupled in a monolithic manner by solving one single linear system to evolve the surface-tension driven Navier-Stokes system in a complex non-manifold space. The codimensional transitions are handled explicitly by tracking a codimension number stored on each particle, which replaces the tedious meshing operators in a conventional mesh-based approach. Using the proposed framework, we simulate a broad array of visually appealing surface tension phenomena, including the fluid chain, bell, polygon, catenoid, and dripping, to demonstrate the efficacy of our approach in capturing the complex fluid characteristics with mixed codimensions, in a robust, versatile, and connectivity-free manner.},
  archive      = {J_TOG},
  author       = {Hui Wang and Yongxu Jin and Anqi Luo and Xubo Yang and Bo Zhu},
  doi          = {10.1145/3386569.3392487},
  journal      = {ACM Transactions on Graphics},
  month        = {8},
  number       = {4},
  pages        = {42:42:1–42:14},
  shortjournal = {ACM Trans. Graph.},
  title        = {Codimensional surface tension flow using moving-least-squares particles},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Chemomechanical simulation of soap film flow on spherical
bubbles. <em>TOG</em>, <em>39</em>(4), 41:41:1–41:13. (<a
href="https://doi.org/10.1145/3386569.3392094">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Soap bubbles are widely appreciated for their fragile nature and their colorful appearance. The natural sciences and, in extension, computer graphics, have comprehensively studied the mechanical behavior of films and foams, as well as the optical properties of thin liquid layers. In this paper, we focus on the dynamics of material flow within the soap film, which results in fascinating, extremely detailed patterns. This flow is characterized by a complex coupling between surfactant concentration and Marangoni surface tension. We propose a novel chemomechanical simulation framework rooted in lubrication theory, which makes use of a custom semi-Lagrangian advection solver to enable the simulation of soap film dynamics on spherical bubbles both in free flow as well as under body forces such as gravity or external air flow. By comparing our simulated outcomes to videos of real-world soap bubbles recorded in a studio environment, we show that our framework, for the first time, closely recreates a wide range of dynamic effects that are also observed in experiment.},
  archive      = {J_TOG},
  author       = {Weizhen Huang and Julian Iseringhausen and Tom Kneiphof and Ziyin Qu and Chenfanfu Jiang and Matthias B. Hullin},
  doi          = {10.1145/3386569.3392094},
  journal      = {ACM Transactions on Graphics},
  month        = {8},
  number       = {4},
  pages        = {41:41:1–41:13},
  shortjournal = {ACM Trans. Graph.},
  title        = {Chemomechanical simulation of soap film flow on spherical bubbles},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Character controllers using motion VAEs. <em>TOG</em>,
<em>39</em>(4), 40:40:1–40:12. (<a
href="https://doi.org/10.1145/3386569.3392422">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A fundamental problem in computer animation is that of realizing purposeful and realistic human movement given a sufficiently-rich set of motion capture clips. We learn data-driven generative models of human movement using autoregressive conditional variational autoencoders, or Motion VAEs. The latent variables of the learned autoencoder define the action space for the movement and thereby govern its evolution over time. Planning or control algorithms can then use this action space to generate desired motions. In particular, we use deep reinforcement learning to learn controllers that achieve goal-directed movements. We demonstrate the effectiveness of the approach on multiple tasks. We further evaluate system-design choices and describe the current limitations of Motion VAEs.},
  archive      = {J_TOG},
  author       = {Hung Yu Ling and Fabio Zinno and George Cheng and Michiel Van De Panne},
  doi          = {10.1145/3386569.3392422},
  journal      = {ACM Transactions on Graphics},
  month        = {8},
  number       = {4},
  pages        = {40:40:1–40:12},
  shortjournal = {ACM Trans. Graph.},
  title        = {Character controllers using motion VAEs},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Catch &amp; carry: Reusable neural controllers for
vision-guided whole-body tasks. <em>TOG</em>, <em>39</em>(4),
39:39:1–39:12. (<a
href="https://doi.org/10.1145/3386569.3392474">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We address the longstanding challenge of producing flexible, realistic humanoid character controllers that can perform diverse whole-body tasks involving object interactions. This challenge is central to a variety of fields, from graphics and animation to robotics and motor neuroscience. Our physics-based environment uses realistic actuation and first-person perception - including touch sensors and egocentric vision - with a view to producing active-sensing behaviors (e.g. gaze direction), transferability to real robots, and comparisons to the biology. We develop an integrated neural-network based approach consisting of a motor primitive module, human demonstrations, and an instructed reinforcement learning regime with curricula and task variations. We demonstrate the utility of our approach for several tasks, including goal-conditioned box carrying and ball catching, and we characterize its behavioral robustness. The resulting controllers can be deployed in real-time on a standard PC. 1},
  archive      = {J_TOG},
  author       = {Josh Merel and Saran Tunyasuvunakool and Arun Ahuja and Yuval Tassa and Leonard Hasenclever and Vu Pham and Tom Erez and Greg Wayne and Nicolas Heess},
  doi          = {10.1145/3386569.3392474},
  journal      = {ACM Transactions on Graphics},
  month        = {8},
  number       = {4},
  pages        = {39:39:1–39:12},
  shortjournal = {ACM Trans. Graph.},
  title        = {Catch &amp; carry: Reusable neural controllers for vision-guided whole-body tasks},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). CARL: Controllable agent with reinforcement learning for
quadruped locomotion. <em>TOG</em>, <em>39</em>(4), 38:38:1–38:10. (<a
href="https://doi.org/10.1145/3386569.3392433">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Motion synthesis in a dynamic environment has been a long-standing problem for character animation. Methods using motion capture data tend to scale poorly in complex environments because of their larger capturing and labeling requirement. Physics-based controllers are effective in this regard, albeit less controllable. In this paper, we present CARL, a quadruped agent that can be controlled with high-level directives and react naturally to dynamic environments. Starting with an agent that can imitate individual animation clips, we use Generative Adversarial Networks to adapt high-level controls, such as speed and heading, to action distributions that correspond to the original animations. Further fine-tuning through the deep reinforcement learning enables the agent to recover from unseen external perturbations while producing smooth transitions. It then becomes straightforward to create autonomous agents in dynamic environments by adding navigation modules over the entire process. We evaluate our approach by measuring the agent&#39;s ability to follow user control and provide a visual analysis of the generated motion to show its effectiveness.},
  archive      = {J_TOG},
  author       = {Ying-Sheng Luo and Jonathan Hans Soeseno and Trista Pei-Chun Chen and Wei-Chao Chen},
  doi          = {10.1145/3386569.3392433},
  journal      = {ACM Transactions on Graphics},
  month        = {8},
  number       = {4},
  pages        = {38:38:1–38:10},
  shortjournal = {ACM Trans. Graph.},
  title        = {CARL: Controllable agent with reinforcement learning for quadruped locomotion},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). AnisoMPM: Animating anisotropic damage mechanics.
<em>TOG</em>, <em>39</em>(4), 37:37:1–37:16. (<a
href="https://doi.org/10.1145/3386569.3392428">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dynamic fracture surrounds us in our day-to-day lives, but animating this phenomenon is notoriously difficult and only further complicated by anisotropic materials---those with underlying structures that dictate preferred fracture directions. Thus, we present AnisoMPM: a robust and general approach for animating the dynamic fracture of isotropic, transversely isotropic, and orthotropic materials. AnisoMPM has three core components: a technique for anisotropic damage evolution, methods for anisotropic elastic response, and a coupling approach. For anisotropic damage, we adopt a non-local continuum damage mechanics (CDM) geometric approach to crack modeling and augment this with structural tensors to encode material anisotropy. Furthermore, we discretize our damage evolution with explicit and implicit integration, giving a high degree of computational efficiency and flexibility. We also utilize a QR-decomposition based anisotropic constitutive model that is inversion safe, more efficient than SVD models, easy to implement, robust to extreme deformations, and that captures all aforementioned modes of anisotropy. Our elasto-damage coupling is enforced through an additive decomposition of our hyperelasticity into a tensile and compressive component in which damage is used to degrade the tensile contribution to allow for material separation. For extremely stiff fibered materials, we further introduce a novel Galerkin weak form discretization that enables embedded directional inextensibility. We present this as a hard-constrained grid velocity solve that poses an alternative to our anisotropic elasticity that is locking-free and can model very stiff materials.},
  archive      = {J_TOG},
  author       = {Joshuah Wolper and Yunuo Chen and Minchen Li and Yu Fang and Ziyin Qu and Jiecong Lu and Meggie Cheng and Chenfanfu Jiang},
  doi          = {10.1145/3386569.3392428},
  journal      = {ACM Transactions on Graphics},
  month        = {8},
  number       = {4},
  pages        = {37:37:1–37:16},
  shortjournal = {ACM Trans. Graph.},
  title        = {AnisoMPM: Animating anisotropic damage mechanics},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). An implicit compressible SPH solver for snow simulation.
<em>TOG</em>, <em>39</em>(4), 36:36:1–36:16. (<a
href="https://doi.org/10.1145/3386569.3392431">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Snow is a complex material. It resists elastic normal and shear deformations, while some deformations are plastic. Snow can deform and break. It can be significantly compressed and gets harder under compression. Existing snow solvers produce impressive results. E.g., hybrid Lagrangian/Eulerian techniques have been used to capture all material properties of snow. The auxiliary grid, however, makes it challenging to handle small volumes. In particular, snow fall and accumulation on surfaces have not been demonstrated with these solvers yet. Existing particle-based snow solvers, on the other hand, can naturally handle small snow volumes. However, existing solutions consider simplified material properties. In particular, shear deformation and the hardening effect are typically omitted. We present a novel Lagrangian snow approach based on Smoothed Particle Hydrodynamics (SPH). Snow is modeled as an elastoplastic continuous material that captures all above-mentioned effects. The compression of snow is handled by a novel compressible pressure solver, where the typically employed state equation is replaced by an implicit formulation. Acceleration due to shear stress is computed using a second implicit formulation. The linear solvers of the two implicit formulations for accelerations due to shear and normal stress are realized with matrix-free implementations. Using implicit formulations and solving them with matrix-free solvers allows to couple the snow to other phases and is beneficial to the stability and the time step size, i.e., performance of the approach. Solid boundaries are represented with particles and a novel implicit formulation is used to handle friction at solid boundaries. We show that our approach can simulate accumulation, deformation, breaking, compression and hardening of snow. Furthermore, we demonstrate two-way coupling with rigid bodies, interaction with incompressible and highly viscous fluids and phase change from fluid to snow.},
  archive      = {J_TOG},
  author       = {Christoph Gissler and Andreas Henne and Stefan Band and Andreas Peer and Matthias Teschner},
  doi          = {10.1145/3386569.3392431},
  journal      = {ACM Transactions on Graphics},
  month        = {8},
  number       = {4},
  pages        = {36:36:1–36:16},
  shortjournal = {ACM Trans. Graph.},
  title        = {An implicit compressible SPH solver for snow simulation},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Adaptive merging for rigid body simulation. <em>TOG</em>,
<em>39</em>(4), 35:35:1–35:13. (<a
href="https://doi.org/10.1145/3386569.3392417">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We reduce computation time in rigid body simulations by merging collections of bodies when they share a common spatial velocity. Merging relies on monitoring the state of contacts, and a metric that compares the relative linear and angular motion of bodies based on their sizes. Unmerging relies on an inexpensive single iteration projected Gauss-Seidel sweep over contacts between merged bodies, which lets us update internal contact forces over time, and use the same metrics as merging to identify when bodies should unmerge. Furthermore we use a contact ordering for graph traversal refinement of the internal contact forces in collections, which helps to correctly identify all the bodies that must unmerge when there are impacts. The general concept of merging is similar to the common technique of sleeping and waking rigid bodies in the inertial frame, and we exploit this too, but our merging is in moving frames, and unmerging takes place at contacts between bodies rather than at the level of bodies themselves. We discuss the previous relative motion metrics in comparison to ours, and evaluate our method on a variety of scenarios.},
  archive      = {J_TOG},
  author       = {Eulalie Coevoet and Otman Benchekroun and Paul G. Kry},
  doi          = {10.1145/3386569.3392417},
  journal      = {ACM Transactions on Graphics},
  month        = {8},
  number       = {4},
  pages        = {35:35:1–35:13},
  shortjournal = {ACM Trans. Graph.},
  title        = {Adaptive merging for rigid body simulation},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Accurate face rig approximation with deep differential
subspace reconstruction. <em>TOG</em>, <em>39</em>(4), 34:34:1–34:12.
(<a href="https://doi.org/10.1145/3386569.3392491">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To be suitable for film-quality animation, rigs for character deformation must fulfill a broad set of requirements. They must be able to create highly stylized deformation, allow a wide variety of controls to permit artistic freedom, and accurately reflect the design intent. Facial deformation is especially challenging due to its nonlinearity with respect to the animation controls and its additional precision requirements, which often leads to highly complex face rigs that are not generalizable to other characters. This lack of generality creates a need for approximation methods that encode the deformation in simpler structures. We propose a rig approximation method that addresses these issues by learning localized shape information in differential coordinates and, separately, a subspace for mesh reconstruction. The use of differential coordinates produces a smooth distribution of errors in the resulting deformed surface, while the learned subspace provides constraints that reduce the low frequency error in the reconstruction. Our method can reconstruct both face and body deformations with high fidelity and does not require a set of well-posed animation examples, as we demonstrate with a variety of production characters.},
  archive      = {J_TOG},
  author       = {Steven L. Song and Weiqi Shi and Michael Reed},
  doi          = {10.1145/3386569.3392491},
  journal      = {ACM Transactions on Graphics},
  month        = {8},
  number       = {4},
  pages        = {34:34:1–34:12},
  shortjournal = {ACM Trans. Graph.},
  title        = {Accurate face rig approximation with deep differential subspace reconstruction},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A scalable approach to control diverse behaviors for
physically simulated characters. <em>TOG</em>, <em>39</em>(4),
33:33:1–33:12. (<a
href="https://doi.org/10.1145/3386569.3392381">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Human characters with a broad range of natural looking and physically realistic behaviors will enable the construction of compelling interactive experiences. In this paper, we develop a technique for learning controllers for a large set of heterogeneous behaviors. By dividing a reference library of motion into clusters of like motions, we are able to construct experts , learned controllers that can reproduce a simulated version of the motions in that cluster. These experts are then combined via a second learning phase, into a general controller with the capability to reproduce any motion in the reference library. We demonstrate the power of this approach by learning the motions produced by a motion graph constructed from eight hours of motion capture data and containing a diverse set of behaviors such as dancing (ballroom and breakdancing), Karate moves, gesturing, walking, and running.},
  archive      = {J_TOG},
  author       = {Jungdam Won and Deepak Gopinath and Jessica Hodgins},
  doi          = {10.1145/3386569.3392381},
  journal      = {ACM Transactions on Graphics},
  month        = {8},
  number       = {4},
  pages        = {33:33:1–33:12},
  shortjournal = {ACM Trans. Graph.},
  title        = {A scalable approach to control diverse behaviors for physically simulated characters},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A practical octree liquid simulator with adaptive surface
resolution. <em>TOG</em>, <em>39</em>(4), 32:32:1–32:17. (<a
href="https://doi.org/10.1145/3386569.3392460">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a new adaptive liquid simulation framework that achieves highly detailed behavior with reduced implementation complexity. Prior work has shown that spatially adaptive grids are efficient for simulating large-scale liquid scenarios, but in order to enable adaptivity along the liquid surface these methods require either expensive boundary-conforming (re-)meshing or elaborate treatments for second order accurate interface conditions. This complexity greatly increases the difficulty of implementation and maintainability, potentially making it infeasible for practitioners. We therefore present new algorithms for adaptive simulation that are comparatively easy to implement yet efficiently yield high quality results. First, we develop a novel staggered octree Poisson discretization for free surfaces that is second order in pressure and gives smooth surface motions even across octree T-junctions, without a power/Voronoi diagram construction. We augment this discretization with an adaptivity-compatible surface tension force that likewise supports T-junctions. Second, we propose a moving least squares strategy for level set and velocity interpolation that requires minimal knowledge of the local tree structure while blending near-seamlessly with standard trilinear interpolation in uniform regions. Finally, to maximally exploit the flexibility of our new surface-adaptive solver, we propose several novel extensions to sizing function design that enhance its effectiveness and flexibility. We perform a range of rigorous numerical experiments to evaluate the reliability and limitations of our method, as well as demonstrating it on several complex high-resolution liquid animation scenarios.},
  archive      = {J_TOG},
  author       = {Ryoichi Ando and Christopher Batty},
  doi          = {10.1145/3386569.3392460},
  journal      = {ACM Transactions on Graphics},
  month        = {8},
  number       = {4},
  pages        = {32:32:1–32:17},
  shortjournal = {ACM Trans. Graph.},
  title        = {A practical octree liquid simulator with adaptive surface resolution},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A model for soap film dynamics with evolving thickness.
<em>TOG</em>, <em>39</em>(4), 31:31:1–31:11. (<a
href="https://doi.org/10.1145/3386569.3392405">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Previous research on animations of soap bubbles, films, and foams largely focuses on the motion and geometric shape of the bubble surface. These works neglect the evolution of the bubble&#39;s thickness, which is normally responsible for visual phenomena like surface vortices, Newton&#39;s interference patterns, capillary waves, and deformation-dependent rupturing of films in a foam. In this paper, we model these natural phenomena by introducing the film thickness as a reduced degree of freedom in the Navier-Stokes equations and deriving their equations of motion. We discretize the equations on a non-manifold triangle mesh surface and couple it to an existing bubble solver. In doing so, we also introduce an incompressible fluid solver for 2.5D films and a novel advection algorithm for convecting fields across non-manifold surface junctions. Our simulations enhance state-of-the-art bubble solvers with additional effects caused by convection, rippling, draining, and evaporation of the thin film.},
  archive      = {J_TOG},
  author       = {Sadashige Ishida and Peter Synak and Fumiya Narita and Toshiya Hachisuka and Chris Wojtan},
  doi          = {10.1145/3386569.3392405},
  journal      = {ACM Transactions on Graphics},
  month        = {8},
  number       = {4},
  pages        = {31:31:1–31:11},
  shortjournal = {ACM Trans. Graph.},
  title        = {A model for soap film dynamics with evolving thickness},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A massively parallel and scalable multi-GPU material point
method. <em>TOG</em>, <em>39</em>(4), 30:30:1–30:15. (<a
href="https://doi.org/10.1145/3386569.3392442">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Harnessing the power of modern multi-GPU architectures, we present a massively parallel simulation system based on the Material Point Method (MPM) for simulating physical behaviors of materials undergoing complex topological changes, self-collision, and large deformations. Our system makes three critical contributions. First, we introduce a new particle data structure that promotes coalesced memory access patterns on the GPU and eliminates the need for complex atomic operations on the memory hierarchy when writing particle data to the grid. Second, we propose a kernel fusion approach using a new Grid-to-Particles-to-Grid ( G2P2G ) scheme, which efficiently reduces GPU kernel launches, improves latency, and significantly reduces the amount of global memory needed to store particle data. Finally, we introduce optimized algorithmic designs that allow for efficient sparse grids in a shared memory context, enabling us to best utilize modern multi-GPU computational platforms for hybrid Lagrangian-Eulerian computational patterns. We demonstrate the effectiveness of our method with extensive benchmarks, evaluations, and dynamic simulations with elastoplasticity, granular media, and fluid dynamics. In comparisons against an open-source and heavily optimized CPU-based MPM codebase [Fang et al. 2019] on an elastic sphere colliding scene with particle counts ranging from 5 to 40 million, our GPU MPM achieves over 100x per-time-step speedup on a workstation with an Intel 8086K CPU and a single Quadro P6000 GPU, exposing exciting possibilities for future MPM simulations in computer graphics and computational science. Moreover, compared to the state-of-the-art GPU MPM method [Hu et al. 2019a], we not only achieve 2x acceleration on a single GPU but our kernel fusion strategy and Array-of-Structs-of-Array ( AoSoA ) data structure design also generalizes to multi-GPU systems. Our multi-GPU MPM exhibits near-perfect weak and strong scaling with 4 GPUs, enabling performant and large-scale simulations on a 1024 3 grid with close to 100 million particles with less than 4 minutes per frame on a single 4-GPU workstation and 134 million particles with less than 1 minute per frame on an 8-GPU workstation.},
  archive      = {J_TOG},
  author       = {Xinlei Wang and Yuxing Qiu and Stuart R. Slattery and Yu Fang and Minchen Li and Song-Chun Zhu and Yixin Zhu and Min Tang and Dinesh Manocha and Chenfanfu Jiang},
  doi          = {10.1145/3386569.3392442},
  journal      = {ACM Transactions on Graphics},
  month        = {8},
  number       = {4},
  pages        = {30:30:1–30:15},
  shortjournal = {ACM Trans. Graph.},
  title        = {A massively parallel and scalable multi-GPU material point method},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A level-set method for magnetic substance simulation.
<em>TOG</em>, <em>39</em>(4), 29:29:1–29:13. (<a
href="https://doi.org/10.1145/3386569.3392445">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a versatile numerical approach to simulating various magnetic phenomena using a level-set method. At the heart of our method lies a novel two-way coupling mechanism between a magnetic field and a magnetizable mechanical system, which is based on the interfacial Helmholtz force drawn from the Minkowski form of the Maxwell stress tensor. We show that a magnetic-mechanical coupling system can be solved as an interfacial problem, both theoretically and computationally. In particular, we employ a Poisson equation with a jump condition across the interface to model the mechanical-to-magnetic interaction and a Helmholtz force on the free surface to model the magnetic-to-mechanical effects. Our computational framework can be easily integrated into a standard Euler fluid solver, enabling both simulation and visualization of a complex magnetic field and its interaction with immersed magnetizable objects in a large domain. We demonstrate the efficacy of our method through an array of magnetic substance simulations that exhibit rich geometric and dynamic characteristics, encompassing ferrofluid, rigid magnetic body, deformable magnetic body, and multi-phase couplings.},
  archive      = {J_TOG},
  author       = {Xingyu Ni and Bo Zhu and Bin Wang and Baoquan Chen},
  doi          = {10.1145/3386569.3392445},
  journal      = {ACM Transactions on Graphics},
  month        = {8},
  number       = {4},
  pages        = {29:29:1–29:13},
  shortjournal = {ACM Trans. Graph.},
  title        = {A level-set method for magnetic substance simulation},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Real-time image smoothing via iterative least squares.
<em>TOG</em>, <em>39</em>(3), 28:1–24. (<a
href="https://doi.org/10.1145/3388887">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Edge-preserving image smoothing is a fundamental procedure for many computer vision and graphic applications. There is a tradeoff between the smoothing quality and the processing speed: the high smoothing quality usually requires a high computational cost, which leads to the low processing speed. In this article, we propose a new global optimization based method, named iterative least squares (ILS), for efficient edge-preserving image smoothing. Our approach can produce high-quality results but at a much lower computational cost. Comprehensive experiments demonstrate that the proposed method can produce results with little visible artifacts. Moreover, the computation of ILS can be highly parallel, which can be easily accelerated through either multi-thread computing or the GPU hardware. With the acceleration of a GTX 1080 GPU, it is able to process images of 1080p resolution (1920 × 1080) at the rate of 20fps for color images and 47fps for gray images. In addition, the ILS is flexible and can be modified to handle more applications that require different smoothing properties. Experimental results of several applications show the effectiveness and efficiency of the proposed method. The code is available at https://github.com/wliusjtu/Real-time-Image-Smoothing-via-Iterative-Least-Squares.},
  archive      = {J_TOG},
  author       = {Wei Liu and Pingping Zhang and Xiaolin Huang and Jie Yang and Chunhua Shen and Ian Reid},
  doi          = {10.1145/3388887},
  journal      = {ACM Transactions on Graphics},
  month        = {6},
  number       = {3},
  pages        = {28:1–24},
  shortjournal = {ACM Trans. Graph.},
  title        = {Real-time image smoothing via iterative least squares},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Complete classification and efficient determination of
arrangements formed by two ellipsoids. <em>TOG</em>, <em>39</em>(3),
27:1–12. (<a href="https://doi.org/10.1145/3388540">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Arrangements of geometric objects refer to the spatial partitions formed by the objects, and they serve as an underlining structure of motion design, analysis, and planning in CAD/CAM, robotics, molecular modeling, manufacturing, and computer-assisted radio-surgery. Arrangements are especially useful to collision detection, which is a key task in various applications such as computer animation, virtual reality, computer games, robotics, CAD/CAM, and computational physics. Ellipsoids are commonly used as bounding volumes in approximating complex geometric objects in collision detection. In this article, we present an in-depth study on the arrangements formed by two ellipsoids. Specifically, we present a classification of these arrangements and propose an efficient algorithm for determining the arrangement formed by any particular pair of ellipsoids. A stratification diagram is also established to show the connections among all the arrangements formed by two ellipsoids. Our results, for the first time, elucidate all possible relative positions between two arbitrary ellipsoids and provide an efficient and robust algorithm for determining the relative position of any two given ellipsoids, therefore providing the necessary foundation for developing practical and trustworthy methods for processing ellipsoids for collision analysis or simulation in various applications.},
  archive      = {J_TOG},
  author       = {Xiaohong Jia and Changhe Tu and Bernard Mourrain and Wenping Wang},
  doi          = {10.1145/3388540},
  journal      = {ACM Transactions on Graphics},
  month        = {5},
  number       = {3},
  pages        = {27:1–12},
  shortjournal = {ACM Trans. Graph.},
  title        = {Complete classification and efficient determination of arrangements formed by two ellipsoids},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Delayed rejection metropolis light transport. <em>TOG</em>,
<em>39</em>(3), 26:1–14. (<a
href="https://doi.org/10.1145/3388538">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Designing robust mutation strategies for primary sample space Metropolis light transport is a challenging problem: poorly tuned mutations both hinder state space exploration and introduce structured image artifacts. Scenes with complex materials, lighting, and geometry make hand-designing strategies that remain optimal over the entire state space infeasible. Moreover, these difficult regions are often sparse in state space, and so relying exclusively on intricate—and often expensive—proposal mechanisms can be wasteful, whereas simpler inexpensive mechanisms are more sample efficient. We generalize Metropolis–Hastings light transport to employ a flexible two-stage mutation strategy based on delayed rejection Markov chain Monte Carlo. Our approach generates multiple proposals based on the failure of previous ones, all while preserving Markov chain ergodicity. This allows us to reduce error while maintaining fast global exploration and low correlation across chains. Direct application of delayed rejection to light transport leads to low acceptance probabilities, and so we also propose a novel transition kernel to alleviate this issue. We benchmark our approach on several applications including bold-then-timid and cheap-then-expensive proposals across different light transport algorithms. Our method is applicable to any primary sample space algorithm with minimal implementation effort, producing consistently better results on a variety of challenging scenes.},
  archive      = {J_TOG},
  author       = {Damien Rioux-Lavoie and Joey Litalien and Adrien Gruson and Toshiya Hachisuka and Derek Nowrouzezahrai},
  doi          = {10.1145/3388538},
  journal      = {ACM Transactions on Graphics},
  month        = {5},
  number       = {3},
  pages        = {26:1–14},
  shortjournal = {ACM Trans. Graph.},
  title        = {Delayed rejection metropolis light transport},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). VoroCrust: Voronoi meshing without clipping. <em>TOG</em>,
<em>39</em>(3), 23:1–16. (<a
href="https://doi.org/10.1145/3337680">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Polyhedral meshes are increasingly becoming an attractive option with particular advantages over traditional meshes for certain applications. What has been missing is a robust polyhedral meshing algorithm that can handle broad classes of domains exhibiting arbitrarily curved boundaries and sharp features. In addition, the power of primal-dual mesh pairs, exemplified by Voronoi-Delaunay meshes, has been recognized as an important ingredient in numerous formulations. The VoroCrust algorithm is the first provably correct algorithm for conforming polyhedral Voronoi meshing for non-convex and non-manifold domains with guarantees on the quality of both surface and volume elements. A robust refinement process estimates a suitable sizing field that enables the careful placement of Voronoi seeds across the surface, circumventing the need for clipping and avoiding its many drawbacks. The algorithm has the flexibility of filling the interior by either structured or random samples while preserving all sharp features in the output mesh. We demonstrate the capabilities of the algorithm on a variety of models and compare against state-of-the-art polyhedral meshing methods based on clipped Voronoi cells establishing the clear advantage of VoroCrust output.},
  archive      = {J_TOG},
  author       = {Ahmed Abdelkader and Chandrajit L. Bajaj and Mohamed S. Ebeida and Ahmed H. Mahmoud and Scott A. Mitchell and John D. Owens and Ahmad A. Rushdi},
  doi          = {10.1145/3337680},
  journal      = {ACM Transactions on Graphics},
  month        = {5},
  number       = {3},
  pages        = {23:1–16},
  shortjournal = {ACM Trans. Graph.},
  title        = {VoroCrust: Voronoi meshing without clipping},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Accurate rendering of liquid-crystals and inhomogeneous
optically anisotropic media. <em>TOG</em>, <em>39</em>(3), 22:1–23. (<a
href="https://doi.org/10.1145/3381748">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a novel method for devising a closed-form analytic expression to the light transport through the bulk of inhomogeneous optically anisotropic media. Those optically anisotropic materials, e.g., liquid-crystals and elastic fluids, arise in a plethora of established applications and exciting new research; however, current state-of-the-art methods of visually deducing their optical properties or rendering their appearance are either lacking or non-existent. We formulate our light transport problemunder the context of electromagnetism and derive, from first principles, a differential equation of the transmitted complex wave fields that fully account for the complicated interference phenomena that arise. At the core of our proposed rendering framework is a powerful mathematical representation, carefully crafted to enable us to produce highly accurate analytic approximative solutions for the light transport. This approach is previously unused in computer rendering, and our framework is capable of accurately rendering optically anisotropic materials with spatially varying optical properties at orders-of-magnitude better performance compared to existing methods. We demonstrate a few practical applications of our method, and we validate it against polarized photos of liquid-crystals as well as numerically against numerical solvers and qualitatively against brute-force renderings.},
  archive      = {J_TOG},
  author       = {Shlomi Steinberg},
  doi          = {10.1145/3381748},
  journal      = {ACM Transactions on Graphics},
  month        = {5},
  number       = {3},
  pages        = {22:1–23},
  shortjournal = {ACM Trans. Graph.},
  title        = {Accurate rendering of liquid-crystals and inhomogeneous optically anisotropic media},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Octahedral frames for feature-aligned cross fields.
<em>TOG</em>, <em>39</em>(3), 25:1–13. (<a
href="https://doi.org/10.1145/3374209">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a method for designing smooth cross fields on surfaces that automatically align to sharp features of an underlying geometry. Our approach introduces a novel class of energies based on a representation of cross fields in the spherical harmonic basis. We provide theoretical analysis of these energies in the smooth setting, showing that they penalize deviations from surface creases while otherwise promoting intrinsically smooth fields. We demonstrate the applicability of our method to quad meshing and include an extensive benchmark comparing our fields to other automatic approaches for generating feature-aligned cross fields on triangle meshes.},
  archive      = {J_TOG},
  author       = {Paul Zhang and Josh Vekhter and Edward Chien and David Bommes and Etienne Vouga and Justin Solomon},
  doi          = {10.1145/3374209},
  journal      = {ACM Transactions on Graphics},
  month        = {4},
  number       = {3},
  pages        = {25:1–13},
  shortjournal = {ACM Trans. Graph.},
  title        = {Octahedral frames for feature-aligned cross fields},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Local fourier slice photography. <em>TOG</em>,
<em>39</em>(3), 24:1–16. (<a
href="https://doi.org/10.1145/3339307">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Light field cameras provide intriguing possibilities, such as post-capture refocus or the ability to synthesize images from novel viewpoints. This comes, however, at the price of significant storage requirements. Compression techniques can be used to reduce these, but refocusing and reconstruction require so far, again, a dense pixel representation. To avoid this, we introduce local Fourier slice photography that allows for refocused image reconstruction directly from a sparse wavelet representation of a light field, either to obtain an image or a compressed representation of it. The result is made possible by wavelets that respect the “slicing’s” intrinsic structure and enable us to derive exact reconstruction filters for the refocused image in closed-form. Image reconstruction then amounts to applying these filters to the light field’s wavelet coefficients; hence, no reconstruction of a dense pixel representation is required. We demonstrate that this can reduce storage requirements and also computation times. We furthermore analyze the computational complexity of our algorithm and show that it scales linearly with the size of the reconstructed region and the non-negligible wavelet coefficients, i.e., with the visual complexity.},
  archive      = {J_TOG},
  author       = {Christian Lessig},
  doi          = {10.1145/3339307},
  journal      = {ACM Transactions on Graphics},
  month        = {4},
  number       = {3},
  pages        = {24:1–16},
  shortjournal = {ACM Trans. Graph.},
  title        = {Local fourier slice photography},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Hierarchical optimization time integration for CFL-rate MPM
stepping. <em>TOG</em>, <em>39</em>(3), 21:1–16. (<a
href="https://doi.org/10.1145/3386760">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose Hierarchical Optimization Time Integration (HOT) for efficient implicit timestepping of the material point method (MPM) irrespective of simulated materials and conditions. HOT is an MPM-specialized hierarchical optimization algorithm that solves nonlinear timestep problems for large-scale MPM systems near the CFL limit. HOT provides convergent simulations out of the box across widely varying materials and computational resolutions without parameter tuning. As an implicit MPM timestepper accelerated by a custom-designed Galerkin multigrid wrapped in a quasi-Newton solver, HOT is both highly parallelizable and robustly convergent. As we show in our analysis, HOT maintains consistent and efficient performance even as we grow stiffness, increase deformation, and vary materials over a wide range of finite strain, elastodynamic, and plastic examples. Through careful benchmark ablation studies, we compare the effectiveness of HOT against seemingly plausible alternative combinations of MPM with standard multigrid and other Newton-Krylov models. We show how these alternative designs result in severe issues and poor performance. In contrast, HOT outperforms existing state-of-the-art, heavily optimized implicit MPM codes with an up to 10× performance speedup across a wide range of challenging benchmark test simulations.},
  archive      = {J_TOG},
  author       = {Xinlei Wang and Minchen Li and Yu Fang and Xinxin Zhang and Ming Gao and Min Tang and Danny M. Kaufman and Chenfanfu Jiang},
  doi          = {10.1145/3386760},
  journal      = {ACM Transactions on Graphics},
  month        = {4},
  number       = {3},
  pages        = {21:1–16},
  shortjournal = {ACM Trans. Graph.},
  title        = {Hierarchical optimization time integration for CFL-rate MPM stepping},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Medial elastics: Efficient and collision-ready deformation
via medial axis transform. <em>TOG</em>, <em>39</em>(3), 20:1–17. (<a
href="https://doi.org/10.1145/3384515">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a framework for the interactive simulation of nonlinear deformable objects. The primary feature of our system is the seamless integration of deformable simulation and collision culling, which are often independently handled in existing animation systems. The bridge connecting them is the medial axis transform (MAT), a high-fidelity volumetric approximation of complex 3D shapes. From the physics simulation perspective, MAT leads to an expressive and compact reduced nonlinear model. We employ a semireduced projective dynamics formulation, which well captures high-frequency local deformations of high-resolution models while retaining a low computation cost. Our key observation is that the most compelling (nonlinear) deformable effects are enabled by the local constraints projection, which should not be aggressively reduced, and only apply model reduction at the global stage. From the collision detection (CD)/collision culling (CC) perspective, MAT is geometrically versatile using linear-interpolated spheres (i.e., the so-called medial primitives (MPs)) to approximate the boundary of the input model. The intersection test between two MPs is formulated as a quadratically constrained quadratic program problem. We give an algorithm to solve this problem exactly, which returns the deepest penetration between a pair of intersecting MPs. When coupled with spatial hashing, collision (including self-collision) can be efficiently identified on the GPU within a few milliseconds even for massive simulations. We have tested our system on a variety of geometrically complex and high-resolution deformable objects, and our system produces convincing animations with all of the collisions/self-collisions well handled at an interactive rate.},
  archive      = {J_TOG},
  author       = {Lei Lan and Ran Luo and Marco Fratarcangeli and Weiwei Xu and Huamin Wang and Xiaohu Guo and Junfeng Yao and Yin Yang},
  doi          = {10.1145/3384515},
  journal      = {ACM Transactions on Graphics},
  month        = {4},
  number       = {3},
  pages        = {20:1–17},
  shortjournal = {ACM Trans. Graph.},
  title        = {Medial elastics: Efficient and collision-ready deformation via medial axis transform},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Enlighten me: Importance of brightness and shadow for
character emotion and appeal. <em>TOG</em>, <em>39</em>(3), 19:1–12. (<a
href="https://doi.org/10.1145/3383195">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Lighting has been used to enhance emotion and appeal of characters for centuries, from paintings in the Renaissance to the modern-day digital arts. In VFX and animation studios, lighting is considered as important as modelling, shading, or rigging. Most existing work focuses on either empirical best-practice created by artists of the centuries or on lighting perception with basic shapes. In contrast, our work focuses on the effect of lighting on emotional characters. Our study presents an extensive set of novel perceptual experiments designed to investigate the effects of brightness levels (key light brightness) and the proportion of light intensity illuminating the two sides of a character’s face (key-to-fill ratio). We are particularly interested in the effect of lighting on the recognition of emotion, emotion intensity, and the overall appeal, as these are crucial factors for audience engagement. Our results have implications for artists and developers wishing to increase the appeal and emotional expression of their characters, ranging from cartoon to realistic styles. Our key finding is that lighting can be used to effectively alter the intensity of emotion of a character and that brighter conditions increased appeal across all of our experiments.},
  archive      = {J_TOG},
  author       = {Pisut Wisessing and Katja Zibrek and Douglas W. Cunningham and John Dingliana and Rachel McDonnell},
  doi          = {10.1145/3383195},
  journal      = {ACM Transactions on Graphics},
  month        = {4},
  number       = {3},
  pages        = {19:1–12},
  shortjournal = {ACM Trans. Graph.},
  title        = {Enlighten me: Importance of brightness and shadow for character emotion and appeal},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A smoothness energy without boundary distortion for curved
surfaces. <em>TOG</em>, <em>39</em>(3), 18:1–17. (<a
href="https://doi.org/10.1145/3377406">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Current quadratic smoothness energies for curved surfaces either exhibit distortions near the boundary due to zero Neumann boundary conditions or they do not correctly account for intrinsic curvature, which leads to unnatural-looking behavior away from the boundary. This leads to an unfortunate trade-off: One can either have natural behavior in the interior or a distortion-free result at the boundary, but not both. We introduce a generalized Hessian energy for curved surfaces, expressed in terms of the covariant one-form Dirichlet energy, the Gaussian curvature, and the exterior derivative. Energy minimizers solve the Laplace-Beltrami biharmonic equation, correctly accounting for intrinsic curvature, leading to natural-looking isolines. On the boundary, minimizers are as-linear-as-possible, which reduces the distortion of isolines at the boundary. We discretize the covariant one-form Dirichlet energy using Crouzeix-Raviart finite elements, arriving at a discrete formulation of the Hessian energy for applications on curved surfaces. We observe convergence of the discretization in our experiments.},
  archive      = {J_TOG},
  author       = {Oded Stein and Alec Jacobson and Max Wardetzky and Eitan Grinspun},
  doi          = {10.1145/3377406},
  journal      = {ACM Transactions on Graphics},
  month        = {3},
  number       = {3},
  pages        = {18:1–17},
  shortjournal = {ACM Trans. Graph.},
  title        = {A smoothness energy without boundary distortion for curved surfaces},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Deep generative modeling for scene synthesis via hybrid
representations. <em>TOG</em>, <em>39</em>(2), 17:1–21. (<a
href="https://doi.org/10.1145/3381866">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a deep generative scene modeling technique for indoor environments. Our goal is to train a generative model using a feed-forward neural network that maps a prior distribution (e.g., a normal distribution) to the distribution of primary objects in indoor scenes. We introduce a 3D object arrangement representation that models the locations and orientations of objects, based on their size and shape attributes. Moreover, our scene representation is applicable for 3D objects with different multiplicities (repetition counts), selected from a database. We show a principled way to train this model by combining discriminative losses for both a 3D object arrangement representation and a 2D image-based representation. We demonstrate the effectiveness of our scene representation and the network training method on benchmark datasets. We also show the applications of this generative model in scene interpolation and scene completion.},
  archive      = {J_TOG},
  author       = {Zaiwei Zhang and Zhenpei Yang and Chongyang Ma and Linjie Luo and Alexander Huth and Etienne Vouga and Qixing Huang},
  doi          = {10.1145/3381866},
  journal      = {ACM Transactions on Graphics},
  month        = {4},
  number       = {2},
  pages        = {17:1–21},
  shortjournal = {ACM Trans. Graph.},
  title        = {Deep generative modeling for scene synthesis via hybrid representations},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Algebraic representations for volumetric frame fields.
<em>TOG</em>, <em>39</em>(2), 16:1–17. (<a
href="https://doi.org/10.1145/3366786">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Field-guided parameterization methods have proven effective for quad meshing of surfaces; these methods compute smooth cross fields to guide the meshing process and then integrate the fields to construct a discrete mesh. A key challenge in extending these methods to three dimensions, however, is representation of field values. Whereas cross fields can be represented by tangent vector fields that form a linear space, the 3D analog—an octahedral frame field—takes values in a nonlinear manifold. In this work, we describe the space of octahedral frames in the language of differential and algebraic geometry. With this understanding, we develop geometry-aware tools for optimization of octahedral fields, namely geodesic stepping and exact projection via semidefinite relaxation. Our algebraic approach not only provides an elegant and mathematically sound description of the space of octahedral frames but also suggests a generalization to frames whose three axes scale independently, better capturing the singular behavior we expect to see in volumetric frame fields. These new odeco frames , so called as they are represented by orthogonally decomposable tensors, also admit a semidefinite program–based projection operator. Our description of the spaces of octahedral and odeco frames suggests computing frame fields via manifold-based optimization algorithms; we show that these algorithms efficiently produce high-quality fields while maintaining stability and smoothness.},
  archive      = {J_TOG},
  author       = {David Palmer and David Bommes and Justin Solomon},
  doi          = {10.1145/3366786},
  journal      = {ACM Transactions on Graphics},
  month        = {4},
  number       = {2},
  pages        = {16:1–17},
  shortjournal = {ACM Trans. Graph.},
  title        = {Algebraic representations for volumetric frame fields},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Resampling-aware weighting functions for bidirectional path
tracing using multiple light sub-paths. <em>TOG</em>, <em>39</em>(2),
15:1–11. (<a href="https://doi.org/10.1145/3338994">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Bidirectional path tracing (BPT) with multiple importance sampling (MIS) is a popular technique for rendering realistic images. Recently, it has been shown that BPT can be improved by preparing multiple light sub-paths and by resampling a small number of light sub-paths from them to generate full paths with large contribution. Traditionally, for MIS weights, the balance heuristic has widely been used to minimize the upper bound of variance, where each full path is weighted in proportion to the probability of the path. Although the probability of the path can change due to the resampling process, the weighting functions used in the previous methods remain unaffected by the change in probability, resulting in less efficiency. To address this problem, we propose new weighting functions for BPT with multiple light sub-paths. Our main contribution is a precise formulation of the variance and the derivation of the weighting functions that can appropriately treat the change in probability. We demonstrate that our weighting functions significantly improve the image quality. We will release a simple version of our implementation as open source to ensure reproducibility.},
  archive      = {J_TOG},
  author       = {Kosuke Nabata and Kei Iwasaki and Yoshinori Dobashi},
  doi          = {10.1145/3338994},
  journal      = {ACM Transactions on Graphics},
  month        = {3},
  number       = {2},
  pages        = {15:1–11},
  shortjournal = {ACM Trans. Graph.},
  title        = {Resampling-aware weighting functions for bidirectional path tracing using multiple light sub-paths},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Fast construction of discrete geodesic graphs. <em>TOG</em>,
<em>39</em>(2), 14:1–14. (<a
href="https://doi.org/10.1145/3144567">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper develops a new method for constructing Discrete Geodesic Graph (DGG)—an undirected, sparse graph for computing discrete geodesic distances and paths on triangle meshes. Based on a novel accuracy aware window propagation scheme, our method is able to compute the graph edges in a direct and efficient manner. Given a triangle mesh with n vertices and a user-specified accuracy parameter ɛ, our method produces a DGG with O ( n \√ɛ) edges in empirical O ( n \ɛ 0.75 log 1\ɛ) time, which greatly improves the time complexity O ( n \ɛ log 1\ɛ) of the existing method. Extensive evaluation on a large-scale 3D shape repository shows that our method is efficient and can produce high-quality geodesic distances with predictable accuracy and guaranteed true distance metric. In particular, our method has a great advantage over the existing approximate methods on meshes with high degree of anisotropy. The source code is available at https://github.com/GeodesicGraph.},
  archive      = {J_TOG},
  author       = {Yohanes Yudhi Adikusuma and Zheng Fang and Ying He},
  doi          = {10.1145/3144567},
  journal      = {ACM Transactions on Graphics},
  month        = {3},
  number       = {2},
  pages        = {14:1–14},
  shortjournal = {ACM Trans. Graph.},
  title        = {Fast construction of discrete geodesic graphs},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). End-to-end learned, optically coded super-resolution SPAD
camera. <em>TOG</em>, <em>39</em>(2), 9:1–14. (<a
href="https://doi.org/10.1145/3372261">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Single Photon Avalanche Photodiodes (SPADs) have recently received a lot of attention in imaging and vision applications due to their excellent performance in low-light conditions, as well as their ultra-high temporal resolution. Unfortunately, like many evolving sensor technologies, image sensors built around SPAD technology currently suffer from a low pixel count. In this work, we investigate a simple, low-cost, and compact optical coding camera design that supports high-resolution image reconstructions from raw measurements with low pixel counts. We demonstrate this approach for regular intensity imaging, depth imaging, as well transient imaging. Our method uses an end-to-end framework to simultaneously optimize the optical design and a reconstruction network for obtaining super-resolved images from raw measurements. The optical design space is that of an engineered point spread function (implemented with diffractive optics), which can be considered an optimized anti-aliasing filter to preserve as much high-resolution information as possible despite imaging with a low pixel count, low fill-factor SPAD array. We further investigate a deep network for reconstruction. The effectiveness of this joint design and reconstruction approach is demonstrated for a range of different applications, including high-speed imaging, and time of flight depth imaging, as well as transient imaging. While our work specifically focuses on low-resolution SPAD sensors, similar approaches should prove effective for other emerging image sensor technologies with low pixel counts and low fill-factors.},
  archive      = {J_TOG},
  author       = {Qilin Sun and Jian Zhang and Xiong Dun and Bernard Ghanem and Yifan Peng and Wolfgang Heidrich},
  doi          = {10.1145/3372261},
  journal      = {ACM Transactions on Graphics},
  month        = {3},
  number       = {2},
  pages        = {9:1–14},
  shortjournal = {ACM Trans. Graph.},
  title        = {End-to-end learned, optically coded super-resolution SPAD camera},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Generating digital painting lighting effects via RGB-space
geometry. <em>TOG</em>, <em>39</em>(2), 13:1–13. (<a
href="https://doi.org/10.1145/3372176">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present an algorithm to generate digital painting lighting effects from a single image. Our algorithm is based on a key observation: Artists use many overlapping strokes to paint lighting effects, i.e., pixels with dense stroke history tend to gather more illumination strokes. Based on this observation, we design an algorithm to both estimate the density of strokes in a digital painting using color geometry and then generate novel lighting effects by mimicking artists’ coarse-to-fine workflow. Coarse lighting effects are first generated using a wave transform and then retouched according to the stroke density of the original illustrations into usable lighting effects. Our algorithm is content-aware, with generated lighting effects naturally adapting to image structures, and can be used as an interactive tool to simplify current labor-intensive workflows for generating lighting effects for digital and matte paintings. In addition, our algorithm can also produce usable lighting effects for photographs or three-dimensional rendered images. We evaluate our approach with both an in-depth qualitative and a quantitative analysis that includes a perceptual user study. Results show that our proposed approach is not only able to produce favorable lighting effects with respect to existing approaches, but also that it is able to significantly reduce the needed interaction time.},
  archive      = {J_TOG},
  author       = {Lvmin Zhang and Edgar Simo-Serra and Yi Ji and Chunping Liu},
  doi          = {10.1145/3372176},
  journal      = {ACM Transactions on Graphics},
  month        = {2},
  number       = {2},
  pages        = {13:1–13},
  shortjournal = {ACM Trans. Graph.},
  title        = {Generating digital painting lighting effects via RGB-space geometry},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Computational parquetry: Fabricated style transfer with wood
pixels. <em>TOG</em>, <em>39</em>(2), 12:1–14. (<a
href="https://doi.org/10.1145/3378541">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Parquetry is the art and craft of decorating a surface with a pattern of differently colored veneers of wood, stone, or other materials. Traditionally, the process of designing and making parquetry has been driven by color, using the texture found in real wood only for stylization or as a decorative effect. Here, we introduce a computational pipeline that draws from the rich natural structure of strongly textured real-world veneers as a source of detail to approximate a target image as faithfully as possible using a manageable number of parts. This challenge is closely related to the established problems of patch-based image synthesis and stylization in some ways, but fundamentally different in others. Most importantly, the limited availability of resources (any piece of wood can only be used once) turns the relatively simple problem of finding the right piece for the target location into the combinatorial problem of finding optimal parts while avoiding resource collisions. We introduce an algorithm that efficiently solves an approximation to the problem. It further addresses challenges like gamut mapping, feature characterization, and the search for fabricable cuts. We demonstrate the effectiveness of the system by fabricating a selection of pieces of parquetry from different kinds of unstained wood veneer.},
  archive      = {J_TOG},
  author       = {Julian Iseringhausen and Michael Weinmann and Weizhen Huang and Matthias B. Hullin},
  doi          = {10.1145/3378541},
  journal      = {ACM Transactions on Graphics},
  month        = {2},
  number       = {2},
  pages        = {12:1–14},
  shortjournal = {ACM Trans. Graph.},
  title        = {Computational parquetry: Fabricated style transfer with wood pixels},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Subdivision directional fields. <em>TOG</em>,
<em>39</em>(2), 11:1–20. (<a
href="https://doi.org/10.1145/3375659">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a novel linear subdivision scheme for face-based tangent directional fields on triangle meshes. Our subdivision scheme is based on a novel coordinate-free representation of directional fields as halfedge-based scalar quantities, bridging the mixed finite-element representation with discrete exterior calculus. By commuting with differential operators, our subdivision is structure preserving: it reproduces curl-free fields precisely and reproduces divergence-free fields in the weak sense. Moreover, our subdivision scheme directly extends to directional fields with several vectors per face by working on the branched covering space. Finally, we demonstrate how our scheme can be applied to directional-field design, advection, and robust earth mover’s distance computation, for efficient and robust computation.},
  archive      = {J_TOG},
  author       = {Bram Custers and Amir Vaxman},
  doi          = {10.1145/3375659},
  journal      = {ACM Transactions on Graphics},
  month        = {2},
  number       = {2},
  pages        = {11:1–20},
  shortjournal = {ACM Trans. Graph.},
  title        = {Subdivision directional fields},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Gaze-contingent ocular parallax rendering for virtual
reality. <em>TOG</em>, <em>39</em>(2), 10:1–12. (<a
href="https://doi.org/10.1145/3361330">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Immersive computer graphics systems strive to generate perceptually realistic user experiences. Current-generation virtual reality (VR) displays are successful in accurately rendering many perceptually important effects, including perspective, disparity, motion parallax, and other depth cues. In this article, we introduce ocular parallax rendering, a technology that accurately renders small amounts of gaze-contingent parallax capable of improving depth perception and realism in VR. Ocular parallax describes the small amounts of depth-dependent image shifts on the retina that are created as the eye rotates. The effect occurs because the centers of rotation and projection of the eye are not the same. We study the perceptual implications of ocular parallax rendering by designing and conducting a series of user experiments. Specifically, we estimate perceptual detection and discrimination thresholds for this effect and demonstrate that it is clearly visible in most VR applications. Additionally, we show that ocular parallax rendering provides an effective ordinal depth cue and it improves the impression of realistic depth in VR.},
  archive      = {J_TOG},
  author       = {Robert Konrad and Anastasios Angelopoulos and Gordon Wetzstein},
  doi          = {10.1145/3361330},
  journal      = {ACM Transactions on Graphics},
  month        = {1},
  number       = {2},
  pages        = {10:1–12},
  shortjournal = {ACM Trans. Graph.},
  title        = {Gaze-contingent ocular parallax rendering for virtual reality},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Corner-operated tran-similar (COTS) maps, patterns, and
lattices. <em>TOG</em>, <em>39</em>(1), 5:1–14. (<a
href="https://doi.org/10.1145/3267346">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The planar COTS map proposed here takes the unit square to a region R bounded by four log-spiral edges. It is Corner-operated (controlled by the four corners of R ) and Tran-similar (it maps translations to similarities). The tiles of the COTS map of a regular pattern are similar to each other. It may facilitate intuitive design and algorithmic optimization of procedural models of complex, possibly multi-resolution, lattices, because it affords constant-cost algorithms for Point-in-Lattice testing and for Total-Area-Calculations. We provide simple, closed-form expressions for evaluating the COTS map and its inverse from the positions of its corners. We conjecture that the COTS map may be useful in a variety of applications in Engineering, Architecture, and Art, and we provide a few illustrative examples of its possibilities. We compare it to related, previously proposed, planar maps and discuss several variations and extensions.},
  archive      = {J_TOG},
  author       = {Jarek Rossignac},
  doi          = {10.1145/3267346},
  journal      = {ACM Transactions on Graphics},
  month        = {2},
  number       = {1},
  pages        = {5:1–14},
  shortjournal = {ACM Trans. Graph.},
  title        = {Corner-operated tran-similar (COTS) maps, patterns, and lattices},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Non-line-of-sight reconstruction using efficient transient
rendering. <em>TOG</em>, <em>39</em>(1), 8:1–14. (<a
href="https://doi.org/10.1145/3368314">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Being able to see beyond the direct line of sight is an intriguing prospect and could benefit a wide variety of important applications. Recent work has demonstrated that time-resolved measurements of indirect diffuse light contain valuable information for reconstructing shape and reflectance properties of objects located around a corner. In this article, we introduce a novel reconstruction scheme that, by design, produces solutions that are consistent with state-of-the-art physically based rendering. Our method combines an efficient forward model (a custom renderer for time-resolved three-bounce indirect light transport) with an optimization framework to reconstruct object geometry in an analysis-by-synthesis sense. We evaluate our algorithm on a variety of synthetic and experimental input data, and show that it gracefully handles uncooperative scenes with high levels of noise or non-diffuse material reflectance.},
  archive      = {J_TOG},
  author       = {Julian Iseringhausen and Matthias B. Hullin},
  doi          = {10.1145/3368314},
  journal      = {ACM Transactions on Graphics},
  month        = {1},
  number       = {1},
  pages        = {8:1–14},
  shortjournal = {ACM Trans. Graph.},
  title        = {Non-line-of-sight reconstruction using efficient transient rendering},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Adaptive incident radiance field sampling and reconstruction
using deep reinforcement learning. <em>TOG</em>, <em>39</em>(1), 6:1–17.
(<a href="https://doi.org/10.1145/3368313">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Serious noise affects the rendering of global illumination using Monte Carlo (MC) path tracing when insufficient samples are used. The two common solutions to this problem are filtering noisy inputs to generate smooth but biased results and sampling the MC integrand with a carefully crafted probability distribution function (PDF) to produce unbiased results. Both solutions benefit from an efficient incident radiance field sampling and reconstruction algorithm. This study proposes a method for training quality and reconstruction networks (Q- and R-networks, respectively) with a massive offline dataset for the adaptive sampling and reconstruction of first-bounce incident radiance fields. The convolutional neural network (CNN)-based R-network reconstructs the incident radiance field in a 4D space, whereas the deep reinforcement learning (DRL)-based Q-network predicts and guides the adaptive sampling process. The approach is verified by comparing it with state-of-the-art unbiased path guiding methods and filtering methods. Results demonstrate improvements for unbiased path guiding and competitive performance in biased applications, including filtering and irradiance caching.},
  archive      = {J_TOG},
  author       = {Yuchi Huo and Rui Wang and Ruzahng Zheng and Hualin Xu and Hujun Bao and Sung-Eui Yoon},
  doi          = {10.1145/3368313},
  journal      = {ACM Transactions on Graphics},
  month        = {1},
  number       = {1},
  pages        = {6:1–17},
  shortjournal = {ACM Trans. Graph.},
  title        = {Adaptive incident radiance field sampling and reconstruction using deep reinforcement learning},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Deep iterative frame interpolation for full-frame video
stabilization. <em>TOG</em>, <em>39</em>(1), 4:1–9. (<a
href="https://doi.org/10.1145/3363550">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Video stabilization is a fundamental and important technique for higher quality videos. Prior works have extensively explored video stabilization, but most of them involve cropping of the frame boundaries and introduce moderate levels of distortion. We present a novel deep approach to video stabilization that can generate video frames without cropping and low distortion. The proposed framework utilizes frame interpolation techniques to generate in between frames, leading to reduced inter-frame jitter. Once applied in an iterative fashion, the stabilization effect becomes stronger. A major advantage is that our framework is end-to-end trainable in an unsupervised manner. In addition, our method is able to run in near real-time (15 fps). To the best of our knowledge, this is the first work to propose an unsupervised deep approach to full-frame video stabilization. We show the advantages of our method through quantitative and qualitative evaluations comparing to the state-of-the-art methods.},
  archive      = {J_TOG},
  author       = {Jinsoo Choi and In So Kweon},
  doi          = {10.1145/3363550},
  journal      = {ACM Transactions on Graphics},
  month        = {1},
  number       = {1},
  pages        = {4:1–9},
  shortjournal = {ACM Trans. Graph.},
  title        = {Deep iterative frame interpolation for full-frame video stabilization},
  volume       = {39},
  year         = {2020},
}
</textarea>
</details></li>
</ul>

</body>
</html>
