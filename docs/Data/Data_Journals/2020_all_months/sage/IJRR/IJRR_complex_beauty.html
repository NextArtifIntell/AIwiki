<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>IJRR_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="ijrr---90">IJRR - 90</h2>
<ul>
<li><details>
<summary>
(2020). On the role of stiffness and synchronization in human–robot
handshaking. <em>The International Journal of Robotics Research</em>,
<em>39</em>(14), 1796–1811. (<a
href="https://doi.org/10.1177/0278364920903792">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {This article presents a system for soft human–robot handshaking, using a soft robot hand in conjunction with a lightweight and impedance-controlled robot arm. Using this system, we study how different factors influence the perceived naturalness, and give the robot different personality traits. Capitalizing on recent findings regarding handshake grasp force regulation, and on studies of the impedance control of the human arm, we investigate the role of arm stiffness as well as the kinesthetic synchronization of human and robot arm motions during the handshake. The system is implemented using a lightweight anthropomorphic arm, with a Pisa/IIT Softhand wearing a sensorized silicone glove as the end-effector. The robotic arm is impedance-controlled, and its stiffness changes according to different laws under investigation. An internal observer is employed to synchronize the human and robot arm motions. Thus, we simulate both active and passive behavior of the robotic arm during the interaction. Using the system, studies are conducted where 20 participants are asked to interact with the robot, and then rate the perceived quality of the interaction using Likert scales. Our results show that the control of the robotic arm kinesthetic behavior does have an effect on the interaction with the robot, in term of its perceived personality traits, responsiveness, and human-likeness. Our results pave the way towards robotic systems that are capable of performing human–robot interactions in a more human-like manner, and with personality.},
  archive  = {J},
  author   = {Domenico Mura and Espen Knoop and Manuel G Catalano and Giorgio Grioli and Moritz Bächer and Antonio Bicchi},
  doi      = {10.1177/0278364920903792},
  journal  = {The International Journal of Robotics Research},
  month    = {12},
  number   = {14},
  pages    = {1796-1811},
  title    = {On the role of stiffness and synchronization in human–robot handshaking},
  volume   = {39},
  year     = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Quasi-static analysis of planar sliding using friction
patches. <em>The International Journal of Robotics Research</em>,
<em>39</em>(14), 1775–1795. (<a
href="https://doi.org/10.1177/0278364920929082">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Flat objects lying on a surface are hard to grasp, but could be manipulated by sliding along the surface in a non-prehensile manner. This strategy is commonly employed by humans as pre-manipulation, for example to bring a cell phone to the edge of a table to pick it up. To endow robots with a similar capability, we introduce a mathematical model of planar sliding by means of a soft finger. The model reveals various aspects of interaction through frictional contacts, which can be used for planning and control. Specifically, using a quasi-static analysis we are able to derive a hybrid dynamical system to predict the motion of the object and the interaction forces. The conditions for which the object sticks to the friction patch, pivots, or completely slides against it are obtained. It is possible to find fixed points of the system and the path taken by the object to reach such configurations. Theoretical as well as comprehensive experimental results are presented.},
  archive  = {J},
  author   = {M. Mahdi Ghazaei Ardakani and Joao Bimbo and Domenico Prattichizzo},
  doi      = {10.1177/0278364920929082},
  journal  = {The International Journal of Robotics Research},
  month    = {12},
  number   = {14},
  pages    = {1775-1795},
  title    = {Quasi-static analysis of planar sliding using friction patches},
  volume   = {39},
  year     = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Hand–object configuration estimation using particle filters
for dexterous in-hand manipulation. <em>The International Journal of
Robotics Research</em>, <em>39</em>(14), 1760–1774. (<a
href="https://doi.org/10.1177/0278364919883343">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {We consider the problem of in-hand dexterous manipulation with a focus on unknown or uncertain hand–object parameters, such as hand configuration, object pose within hand, and contact positions. In particular, in this work we formulate a generic framework for hand–object configuration estimation using underactuated hands as an example. Owing to the passive reconfigurability and the lack of encoders in the hand’s joints, it is challenging to estimate, plan, and actively control underactuated manipulation. By modeling the grasp constraints, we present a particle filter-based framework to estimate the hand configuration. Specifically, given an arbitrary grasp, we start by sampling a set of hand configuration hypotheses and then randomly manipulate the object within the hand. While observing the object’s movements as evidence using an external camera, which is not necessarily calibrated with the hand frame, our estimator calculates the likelihood of each hypothesis to iteratively estimate the hand configuration. Once converged, the estimator is used to track the hand configuration in real time for future manipulations. Thereafter, we develop an algorithm to precisely plan and control the underactuated manipulation to move the grasped object to desired poses. In contrast to most other dexterous manipulation approaches, our framework does not require any tactile sensing or joint encoders, and can directly operate on any novel objects, without requiring a model of the object a priori . We implemented our framework on both the Yale Model O hand and the Yale T42 hand. The results show that the estimation is accurate for different objects, and that the framework can be easily adapted across different underactuated hand models. In the end, we evaluated our planning and control algorithm with handwriting tasks, and demonstrated the effectiveness of the proposed framework.},
  archive  = {J},
  author   = {Kaiyu Hang and Walter G. Bircher and Andrew S. Morgan and Aaron M. Dollar},
  doi      = {10.1177/0278364919883343},
  journal  = {The International Journal of Robotics Research},
  month    = {12},
  number   = {14},
  pages    = {1760-1774},
  title    = {Hand–object configuration estimation using particle filters for dexterous in-hand manipulation},
  volume   = {39},
  year     = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Model-free vision-based shaping of deformable plastic
materials. <em>The International Journal of Robotics Research</em>,
<em>39</em>(14), 1739–1759. (<a
href="https://doi.org/10.1177/0278364920907684">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {We address the problem of shaping deformable plastic materials using non-prehensile actions. Shaping plastic objects is challenging, because they are difficult to model and to track visually. We study this problem, by using kinetic sand, a plastic toy material that mimics the physical properties of wet sand. Inspired by a pilot study where humans shape kinetic sand, we define two types of actions: pushing the material from the sides and tapping from above. The chosen actions are executed with a robotic arm using image-based visual servoing. From the current and desired view of the material, we define states based on visual features such as the outer contour shape and the pixel luminosity values. These are mapped to actions, which are repeated iteratively to reduce the image error until convergence is reached. For pushing, we propose three methods for mapping the visual state to an action. These include heuristic methods and a neural network, trained from human actions. We show that it is possible to obtain simple shapes with the kinetic sand, without explicitly modeling the material. Our approach is limited in the types of shapes it can achieve. A richer set of action types and multi-step reasoning is needed to achieve more sophisticated shapes.},
  archive  = {J},
  author   = {Andrea Cherubini and Valerio Ortenzi and Akansel Cosgun and Robert Lee and Peter Corke},
  doi      = {10.1177/0278364920907684},
  journal  = {The International Journal of Robotics Research},
  month    = {12},
  number   = {14},
  pages    = {1739-1759},
  title    = {Model-free vision-based shaping of deformable plastic materials},
  volume   = {39},
  year     = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Robust navigation of a soft growing robot by exploiting
contact with the environment. <em>The International Journal of Robotics
Research</em>, <em>39</em>(14), 1724–1738. (<a
href="https://doi.org/10.1177/0278364920903774">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Navigation and motion control of a robot to a destination are tasks that have historically been performed with the assumption that contact with the environment is harmful. This makes sense for rigid-bodied robots, where obstacle collisions are fundamentally dangerous. However, because many soft robots have bodies that are low-inertia and compliant, obstacle contact is inherently safe. As a result, constraining paths of the robot to not interact with the environment is not necessary and may be limiting. In this article, we mathematically formalize interactions of a soft growing robot with a planar environment in an empirical kinematic model. Using this interaction model, we develop a method to plan paths for the robot to a destination. Rather than avoiding contact with the environment, the planner exploits obstacle contact when beneficial for navigation. We find that a planner that takes into account and capitalizes on environmental contact produces paths that are more robust to uncertainty than a planner that avoids all obstacle contact.},
  archive  = {J},
  author   = {Joseph D Greer and Laura H Blumenschein and Ron Alterovitz and Elliot W Hawkes and Allison M Okamura},
  doi      = {10.1177/0278364920903774},
  journal  = {The International Journal of Robotics Research},
  month    = {12},
  number   = {14},
  pages    = {1724-1738},
  title    = {Robust navigation of a soft growing robot by exploiting contact with the environment},
  volume   = {39},
  year     = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Hand closure model for planning top grasps with soft robotic
hands. <em>The International Journal of Robotics Research</em>,
<em>39</em>(14), 1706–1723. (<a
href="https://doi.org/10.1177/0278364920947469">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Automating the act of grasping is one of the most compelling challenges in robotics. In recent times, a major trend has gained the attention of the robotic grasping community: soft manipulation. Along with the design of intrinsically soft robotic hands, it is important to devise grasp planning strategies that can take into account the hand characteristics, but are general enough to be applied to different robotic systems. In this article, we investigate how to perform top grasps with soft hands according to a model-based approach, using both power and precision grasps. The so-called closure signature (CS) is used to model closure motions of soft hands by associating to them a preferred grasping direction. This direction can be aligned to a suitable direction over the object to achieve successful top grasps. The CS-alignment is here combined with a recently developed AI-driven grasp planner for rigid grippers that is adjusted and used to retrieve an estimate of the optimal grasp to be performed on the object. The resulting grasp planner is tested with multiple experimental trials with two different robotic hands. A wide set of objects with different shapes was grasped successfully.},
  archive  = {J},
  author   = {Maria Pozzi and Sara Marullo and Gionata Salvietti and Joao Bimbo and Monica Malvezzi and Domenico Prattichizzo},
  doi      = {10.1177/0278364920947469},
  journal  = {The International Journal of Robotics Research},
  month    = {12},
  number   = {14},
  pages    = {1706-1723},
  title    = {Hand closure model for planning top grasps with soft robotic hands},
  volume   = {39},
  year     = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Modeling and analysis of soft robotic fingers using the fin
ray effect. <em>The International Journal of Robotics Research</em>,
<em>39</em>(14), 1686–1705. (<a
href="https://doi.org/10.1177/0278364920913926">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Soft grasping of random objects in unstructured environments has been a research topic of predilection both in academia and in industry because of its complexity but great practical relevance. However, accurate modeling of soft hands and fingers has proven a difficult challenge to tackle. Focusing on this issue, this article presents a detailed mathematical modeling and performance analysis of parallel grippers equipped with soft fingers taking advantage of the fin ray effect (FRE). The FRE, based on biomimetic principles, is most commonly found in the design of grasping soft fingers, but despite their popularity, finding a convenient model to assess the grasp capabilities of these fingers is challenging. This article aims at solving this issue by providing an analytic tool to better understand and ultimately design this type of soft fingers. First, a kinetostatic model of a general multi-crossbeam finger is established. This model will allow for a fast yet accurate estimation of the contact forces generated when the fingers grasp an arbitrarily shaped object. The obtained mathematical model will be subsequently validated by numerically to ensure the estimations of the overall grasp strength and individual contact forces are indeed accurate. Physical experiments conducted with 3D-printed fingers of the most common architecture of FRE fingers will also be presented and shown to support the proposed model. Finally, the impact of the relative stiffness between different areas of the fingers will be evaluated to provide insight into further refinement and optimization of these fingers.},
  archive  = {J},
  author   = {Xiaowei Shan and Lionel Birglen},
  doi      = {10.1177/0278364920913926},
  journal  = {The International Journal of Robotics Research},
  month    = {12},
  number   = {14},
  pages    = {1686-1705},
  title    = {Modeling and analysis of soft robotic fingers using the fin ray effect},
  volume   = {39},
  year     = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Design and characterization of a hybrid soft gripper with
active palm pose control. <em>The International Journal of Robotics
Research</em>, <em>39</em>(14), 1668–1685. (<a
href="https://doi.org/10.1177/0278364920918918">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {The design and characterization of a soft gripper with an active palm to control grasp postures is presented herein. The gripper structure is a hybrid of soft and stiff components to facilitate integration with traditional arm manipulators. Three fingers and a palm constitute the gripper, all of which are vacuum actuated. Internal wedges are used to tailor the deformation of a soft outer reinforced skin as vacuum collapses the composite structure. A computational finite-element model is proposed to predict finger kinematics. Thanks to its active palm, the gripper is capable of grasping a wide range of part geometries and compliances while achieving a maximum payload of 30 N. The gripper natural softness enables robust open-loop grasping even when components are not properly aligned. Furthermore, the grasp pose of objects with various aspect ratios and compliances can be robustly maintained during manipulation at linear accelerations of up to 15 m/s 2 and angular accelerations of up to 5.23 rad/s 2 .},
  archive  = {J},
  author   = {Vignesh Subramaniam and Snehal Jain and Jai Agarwal and Pablo Valdivia y Alvarado},
  doi      = {10.1177/0278364920918918},
  journal  = {The International Journal of Robotics Research},
  month    = {12},
  number   = {14},
  pages    = {1668-1685},
  title    = {Design and characterization of a hybrid soft gripper with active palm pose control},
  volume   = {39},
  year     = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Multi-segment soft robotic fingers enable robust precision
grasping. <em>The International Journal of Robotics Research</em>,
<em>39</em>(14), 1647–1667. (<a
href="https://doi.org/10.1177/0278364920910465">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {In this work, we discuss the design of soft robotic fingers for robust precision grasping. Through a conceptual analysis of the finger shape and compliance during grasping, we confirm that antipodal grasps are more stable when contact with the object occurs on the side of the fingers (i.e., pinch grasps) instead of the fingertips. In addition, we show that achieving such pinch grasps with soft fingers for a wide variety of objects requires at least two independent bending segments each, but only requires actuation in the proximal segment. Using a physical prototype hand, we evaluate the improvement in pinch-grasping performance of this two-segment proximally actuated finger design compared to more typical, uniformly actuated fingers. Through an exploration of the relative lengths of the two finger segments, we show the tradeoff between power grasping strength and precision grasping capabilities for fingers with passive distal segments. We characterize grasping on the basis of the acquisition region, object sizes, rotational stability, and robustness to external forces. Based on these metrics, we confirm that higher-quality precision grasping is achieved through pinch grasping via fingers with the proximally actuated finger design compared to uniformly actuated fingers. However, power grasping is still best performed with uniformly actuated fingers. Accordingly, soft continuum fingers should be designed to have at least two independently actuated serial segments, since such fingers can maximize grasping performance during both power and precision grasps through controlled adaptation between uniform and proximally actuated finger structures.},
  archive  = {J},
  author   = {Clark B Teeple and Theodore N Koutros and Moritz A Graule and Robert J. Wood},
  doi      = {10.1177/0278364920910465},
  journal  = {The International Journal of Robotics Research},
  month    = {12},
  number   = {14},
  pages    = {1647-1667},
  title    = {Multi-segment soft robotic fingers enable robust precision grasping},
  volume   = {39},
  year     = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Design and prototyping soft–rigid tendon-driven modular
grippers using interpenetrating phase composites materials. <em>The
International Journal of Robotics Research</em>, <em>39</em>(14),
1635–1646. (<a href="https://doi.org/10.1177/0278364920907697">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Advances in soft robotics and material science have enabled rapid progress in soft grippers. The ability to 3D print materials with softer, more elastic materials properties is a recent development and a key enabling technology for the rapid development of soft robots. However, obtaining the desired mechanical properties (e.g., stiffness) of the soft joints and information about the parameters to select in 3D printers is often not straightforward. In this article, we propose the use of interpenetrating phase composites (IPCs) materials with mathematically generated topologies based on triply periodic minimal surfaces for the development of soft grippers with desired mechanical properties. The flexible joints of the gripper can be realized through two or more phases that are topologically interconnected such that each phase represents a standalone cellular structure. As a case study, we present the design and development of a two-finger soft gripper as an example to demonstrate the application scenario of our approach. The flexible parts with desired stiffness values are realized by using IPCs materials in which the reinforcement distribution can be regulated on the basis of mathematical models. We characterized the properties of the material through a set of quantitative experiments on IPCs material specimens, and then we realized qualitative grasping tests with the gripper and a set of objects with different shapes and sizes. We showed that by properly regulating the properties of IPCs material it is possible to design modular grippers with the same structure, but different closure motions. Grippers can be customized for different tasks by easily assembling and disassembling fingers.},
  archive  = {J},
  author   = {Irfan Hussain and Oraib Al-Ketan and Federico Renda and Monica Malvezzi and Domenico Prattichizzo and Lakmal Seneviratne and Rashid K Abu Al-Rub and Dongming Gan},
  doi      = {10.1177/0278364920907697},
  journal  = {The International Journal of Robotics Research},
  month    = {12},
  number   = {14},
  pages    = {1635-1646},
  title    = {Design and prototyping soft–rigid tendon-driven modular grippers using interpenetrating phase composites materials},
  volume   = {39},
  year     = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Geometric constraint-based modeling and analysis of a novel
continuum robot with shape memory alloy initiated variable stiffness.
<em>The International Journal of Robotics Research</em>,
<em>39</em>(14), 1620–1634. (<a
href="https://doi.org/10.1177/0278364920913929">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Continuum robots exhibit promising adaptability and dexterity for soft manipulation due to their intrinsic compliance. However, this compliance may lead to challenges in modeling as well as positioning and loading. In this paper, a virtual work-based static model is established to describe the deformation and mechanics of continuum robots with a generic rod-driven structure, taking the geometric constraint of the drive rods into account. Following this, this paper presents a novel variable stiffness mechanism powered by a set of embedded Shape Memory Alloy (SMA) springs, which can make the drive rods become ‘locked’ on the body structure with different configurations. The resulting effects of variable stiffness are then presented in the static model by introducing tensions of the SMA and friction on the rods. Compared with conventional models, there is no need to predefine the actuation forces of the drive rods; instead, actuation displacements are used in this new mechanism system with stiffness being regulated. As a result, the phenomenon that the continuum robot can exhibit an S-shaped curve when subject to single-directional forces is observed and analyzed. Simulations and experiments demonstrated that the presented mechanism has stiffness variation of over 287% and further demonstrated that the mechanism and its model are achievable with good accuracy, such that the ratio of positioning error is less than 2.23% at the robot end-effector to the robot length.},
  archive  = {J},
  author   = {Chenghao Yang and Shineng Geng and Ian Walker and David T Branson and Jinguo Liu and Jian S Dai and Rongjie Kang},
  doi      = {10.1177/0278364920913929},
  journal  = {The International Journal of Robotics Research},
  month    = {12},
  number   = {14},
  pages    = {1620-1634},
  title    = {Geometric constraint-based modeling and analysis of a novel continuum robot with shape memory alloy initiated variable stiffness},
  volume   = {39},
  year     = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Design, implementation, and control of a deformable
manipulator robot based on a compliant spine. <em>The International
Journal of Robotics Research</em>, <em>39</em>(14), 1604–1619. (<a
href="https://doi.org/10.1177/0278364920910487">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {This article presents the conception, the numerical modeling, and the control of a dexterous, deformable manipulator bio-inspired by the skeletal spine found in vertebrate animals. Through the implementation of this new manipulator, we show a methodology based on numerical models and simulations, that goes from design to control of continuum and soft robots. The manipulator is modeled using a finite element method (FEM), using a set of beam elements that reproduce the lattice structure of the robot. The model is computed and inverted in real-time using optimization methods. A closed-loop control strategy is implemented to account for the disparities between the model and the robot. This control strategy allows for accurate positioning, not only of the tip of the manipulator, but also the positioning of selected middle points along its backbone. In a scenario where the robot is piloted by a human operator, the command of the robot is enhanced by a haptic loop that renders the boundaries of its task space as well as the contact with its environment. The experimental validation of the model and control strategies is also presented in the form of an inspection task use case.},
  archive  = {J},
  author   = {Thor Morales Bieze and Alexandre Kruszewski and Bruno Carrez and Christian Duriez},
  doi      = {10.1177/0278364920910487},
  journal  = {The International Journal of Robotics Research},
  month    = {12},
  number   = {14},
  pages    = {1604-1619},
  title    = {Design, implementation, and control of a deformable manipulator robot based on a compliant spine},
  volume   = {39},
  year     = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Editorial. <em>The International Journal of Robotics
Research</em>, <em>39</em>(14), 1601–1603. (<a
href="https://doi.org/10.1177/0278364920962422">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive = {J},
  author  = {Antonio Bicchi and Oliver Brock},
  doi     = {10.1177/0278364920962422},
  journal = {The International Journal of Robotics Research},
  month   = {12},
  number  = {14},
  pages   = {1601-1603},
  title   = {Editorial},
  volume  = {39},
  year    = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Multimodal interaction-aware motion prediction for
autonomous street crossing. <em>The International Journal of Robotics
Research</em>, <em>39</em>(13), 1567–1598. (<a
href="https://doi.org/10.1177/0278364920961809">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {For mobile robots navigating on sidewalks, the ability to safely cross street intersections is essential. Most existing approaches rely on the recognition of the traffic light signal to make an informed crossing decision. Although these approaches have been crucial enablers for urban navigation, the capabilities of robots employing such approaches are still limited to navigating only on streets that contain signalized intersections. In this article, we address this challenge and propose a multimodal convolutional neural network framework to predict the safety of a street intersection for crossing. Our architecture consists of two subnetworks: an interaction-aware trajectory estimation stream ( interaction-aware temporal convolutional neural network (IA-TCNN)), that predicts the future states of all observed traffic participants in the scene; and a traffic light recognition stream AtteNet. Our IA-TCNN utilizes dilated causal convolutions to model the behavior of all the observable dynamic agents in the scene without explicitly assigning priorities to the interactions among them, whereas AtteNet utilizes squeeze-excitation blocks to learn a content-aware mechanism for selecting the relevant features from the data, thereby improving the noise robustness. Learned representations from the traffic light recognition stream are fused with the estimated trajectories from the motion prediction stream to learn the crossing decision. Incorporating the uncertainty information from both modules enables our architecture to learn a likelihood function that is robust to noise and mispredictions from either subnetworks. Simultaneously, by learning to estimate motion trajectories of the surrounding traffic participants and incorporating knowledge of the traffic light signal, our network learns a robust crossing procedure that is invariant to the type of street intersection. Furthermore, we extend our previously introduced Freiburg Street Crossing dataset with sequences captured at multiple intersections of varying types, demonstrating complex interactions among the traffic participants as well as various lighting and weather conditions. We perform comprehensive experimental evaluations on public datasets as well as our Freiburg Street Crossing dataset, which demonstrate that our network achieves state-of-the-art performance for each of the subtasks, as well as for the crossing safety prediction. Moreover, we deploy the proposed architectural framework on a robotic platform and conduct real-world experiments that demonstrate the suitability of the approach for real-time deployment and robustness to various environments.},
  archive  = {J},
  author   = {Noha Radwan and Wolfram Burgard and Abhinav Valada},
  doi      = {10.1177/0278364920961809},
  journal  = {The International Journal of Robotics Research},
  month    = {11},
  number   = {13},
  pages    = {1567-1598},
  title    = {Multimodal interaction-aware motion prediction for autonomous street crossing},
  volume   = {39},
  year     = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). An obstacle disturbance selection framework: Emergent robot
steady states under repeated collisions. <em>The International Journal
of Robotics Research</em>, <em>39</em>(13), 1549–1566. (<a
href="https://doi.org/10.1177/0278364920935514">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Natural environments are often filled with obstacles and disturbances. Traditional navigation and planning approaches normally depend on finding a traversable “free space” for robots to avoid unexpected contact or collision. We hypothesize that with a better understanding of the robot–obstacle interactions, these collisions and disturbances can be exploited as opportunities to improve robot locomotion in complex environments. In this article, we propose a novel obstacle disturbance selection (ODS) framework with the aim of allowing robots to actively select disturbances to achieve environment-aided locomotion. Using an empirically characterized relationship between leg–obstacle contact position and robot trajectory deviation, we simplify the representation of the obstacle-filled physical environment to a horizontal-plane disturbance force field. We then treat each robot leg as a “disturbance force selector” for prediction of obstacle-modulated robot dynamics. Combining the two representations provides analytical insights into the effects of gaits on legged traversal in cluttered environments. We illustrate the predictive power of the ODS framework by studying the horizontal-plane dynamics of a quadrupedal robot traversing an array of evenly-spaced cylindrical obstacles with both bounding and trotting gaits. Experiments corroborate numerical simulations that reveal the emergence of a stable equilibrium orientation in the face of repeated obstacle disturbances. The ODS reduction yields closed-form analytical predictions of the equilibrium position for different robot body aspect ratios, gait patterns, and obstacle spacings. We conclude with speculative remarks bearing on the prospects for novel ODS-based gait control schemes for shaping robot navigation in perturbation-rich environments.},
  archive  = {J},
  author   = {Feifei Qian and Daniel E Koditschek},
  doi      = {10.1177/0278364920935514},
  journal  = {The International Journal of Robotics Research},
  month    = {11},
  number   = {13},
  pages    = {1549-1566},
  title    = {An obstacle disturbance selection framework: Emergent robot steady states under repeated collisions},
  volume   = {39},
  year     = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Decentralized multi-agent information-theoretic control for
target estimation and localization: Finding gas leaks. <em>The
International Journal of Robotics Research</em>, <em>39</em>(13),
1525–1548. (<a href="https://doi.org/10.1177/0278364920957090">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {This article presents a new decentralized multi-agent information-theoretic (DeMAIT) control algorithm for mobile sensors (agents). The algorithm leverages Bayesian estimation and information-theoretic motion planning for efficient and effective estimation and localization of a target, such as a chemical gas leak. The algorithm consists of: (1) a non-parametric Bayesian estimator, (2) an information-theoretic trajectory planner that generates “informative trajectories” for agents to follow, and (3) a controller and collision avoidance algorithm to ensure that each agent follows its trajectory as closely as possible in a safe manner. Advances include the use of a new information-gain metric and its analytical gradient, which do not depend on an infinite series like prior information metrics. Dynamic programming and multi-threading techniques are applied to efficiently compute the mutual information to minimize measurement uncertainty. The estimation and motion planning processes also take into account the dynamics of the sensors and agents. Extensive simulations are conducted to compare the performance between the DeMAIT algorithm to a traditional raster-scanning method and a clustering method with coordination. The main hypothesis that the DeMAIT algorithm outperforms the other two methods is validated, specifically where the average localization success rate for the DeMAIT algorithm is (a) higher and (b) more robust to changes in the source location, robot team size, and search area size than the raster-scanning and clustering methods. Finally, outdoor field experiments are conducted using a team of custom-built aerial robots equipped with gas concentration sensors to demonstrate efficacy of the DeMAIT algorithm to estimate and find the source of a propane gas leak.},
  archive  = {J},
  author   = {Joseph R Bourne and Matthew N Goodell and Xiang He and Jake A Steiner and Kam K Leang},
  doi      = {10.1177/0278364920957090},
  journal  = {The International Journal of Robotics Research},
  month    = {11},
  number   = {13},
  pages    = {1525-1548},
  title    = {Decentralized multi-agent information-theoretic control for target estimation and localization: Finding gas leaks},
  volume   = {39},
  year     = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Integrity monitoring for kalman filter-based localization.
<em>The International Journal of Robotics Research</em>,
<em>39</em>(13), 1503–1524. (<a
href="https://doi.org/10.1177/0278364920960517">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {The problem of quantifying robot localization safety in the presence of undetected sensor faults is critical when preparing for future applications where robots may interact with humans in life-critical situations; however, the topic is only sparsely addressed in the robotics literature. In response, this work leverages prior work in aviation integrity monitoring to tackle the more challenging case of evaluating localization safety in Global Navigation Satellite System (GNSS)-denied environments. Localization integrity risk is the probability that a robot’s pose estimate lies outside pre-defined acceptable limits while no alarm is triggered. In this article, the integrity risk (i.e., localization safety) is rigorously upper bounded by accounting for both nominal sensor noise and other non-nominal sensor faults. An extended Kalman filter is employed to estimate the robot state, and a sequence of innovations is used for fault detection. The novelty of the work includes (1) the use of a time window to limit the number of monitored fault hypotheses while still guaranteeing safety with respect to previously occurring faults and (2) a new method to account for faults in the data association process.},
  archive  = {J},
  author   = {Guillermo Duenas Arana and Osama Abdul Hafez and Mathieu Joerger and Matthew Spenko},
  doi      = {10.1177/0278364920960517},
  journal  = {The International Journal of Robotics Research},
  month    = {11},
  number   = {13},
  pages    = {1503-1524},
  title    = {Integrity monitoring for kalman filter-based localization},
  volume   = {39},
  year     = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Exactly sparse gaussian variational inference with
application to derivative-free batch nonlinear state estimation. <em>The
International Journal of Robotics Research</em>, <em>39</em>(13),
1473–1502. (<a href="https://doi.org/10.1177/0278364920937608">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {We present a Gaussian variational inference (GVI) technique that can be applied to large-scale nonlinear batch state estimation problems. The main contribution is to show how to fit both the mean and (inverse) covariance of a Gaussian to the posterior efficiently, by exploiting factorization of the joint likelihood of the state and data, as is common in practical problems. This is different than maximum a posteriori (MAP) estimation, which seeks the point estimate for the state that maximizes the posterior (i.e., the mode). The proposed exactly sparse Gaussian variational inference (ESGVI) technique stores the inverse covariance matrix, which is typically very sparse (e.g., block-tridiagonal for classic state estimation). We show that the only blocks of the (dense) covariance matrix that are required during the calculations correspond to the non-zero blocks of the inverse covariance matrix, and further show how to calculate these blocks efficiently in the general GVI problem. ESGVI operates iteratively, and while we can use analytical derivatives at each iteration, Gaussian cubature can be substituted, thereby producing an efficient derivative-free batch formulation. ESGVI simplifies to precisely the Rauch–Tung–Striebel (RTS) smoother in the batch linear estimation case, but goes beyond the ‘extended’ RTS smoother in the nonlinear case because it finds the best-fit Gaussian (mean and covariance), not the MAP point estimate. We demonstrate the technique on controlled simulation problems and a batch nonlinear simultaneous localization and mapping problem with an experimental dataset.},
  archive  = {J},
  author   = {Timothy D Barfoot and James R Forbes and David J Yoon},
  doi      = {10.1177/0278364920937608},
  journal  = {The International Journal of Robotics Research},
  month    = {11},
  number   = {13},
  pages    = {1473-1502},
  title    = {Exactly sparse gaussian variational inference with application to derivative-free batch nonlinear state estimation},
  volume   = {39},
  year     = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Bridging the gap between safety and real-time performance in
receding-horizon trajectory design for mobile robots. <em>The
International Journal of Robotics Research</em>, <em>39</em>(12),
1419–1469. (<a href="https://doi.org/10.1177/0278364920943266">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {To operate with limited sensor horizons in unpredictable environments, autonomous robots use a receding-horizon strategy to plan trajectories, wherein they execute a short plan while creating the next plan. However, creating safe, dynamically feasible trajectories in real time is challenging, and planners must ensure persistent feasibility, meaning a new trajectory is always available before the previous one has finished executing. Existing approaches make a tradeoff between model complexity and planning speed, which can require sacrificing guarantees of safety and dynamic feasibility. This work presents the Reachability-based Trajectory Design (RTD) method for trajectory planning. RTD begins with an offline forward reachable set (FRS) computation of a robot’s motion when tracking parameterized trajectories; the FRS provably bounds tracking error. At runtime, the FRS is used to map obstacles to parameterized trajectories, allowing RTD to select a safe trajectory at every planning iteration. RTD prescribes an obstacle representation to ensure that obstacle constraints can be created and evaluated in real time while maintaining safety. Persistent feasibility is achieved by prescribing a minimum sensor horizon and a minimum duration for the planned trajectories. A system decomposition approach is used to improve the tractability of computing the FRS, allowing RTD to create more complex plans at runtime. RTD is compared in simulation with rapidly-exploring random trees and nonlinear model-predictive control. RTD is also demonstrated in randomly crafted environments on two hardware platforms: a differential-drive Segway and a car-like Rover. The proposed method is safe and persistently feasible across thousands of simulations and dozens of real-world hardware demos.},
  archive  = {J},
  author   = {Shreyas Kousik and Sean Vaskov and Fan Bu and Matthew Johnson-Roberson and Ram Vasudevan},
  doi      = {10.1177/0278364920943266},
  journal  = {The International Journal of Robotics Research},
  month    = {10},
  number   = {12},
  pages    = {1419-1469},
  title    = {Bridging the gap between safety and real-time performance in receding-horizon trajectory design for mobile robots},
  volume   = {39},
  year     = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Deep neural networks as add-on modules for enhancing robot
performance in impromptu trajectory tracking. <em>The International
Journal of Robotics Research</em>, <em>39</em>(12), 1397–1418. (<a
href="https://doi.org/10.1177/0278364920953902">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {High-accuracy trajectory tracking is critical to many robotic applications, including search and rescue, advanced manufacturing, and industrial inspection, to name a few. Yet the unmodeled dynamics and parametric uncertainties of operating in such complex environments make it difficult to design controllers that are capable of accurately tracking arbitrary, feasible trajectories from the first attempt (i.e., impromptu trajectory tracking). This article proposes a platform-independent, learning-based “add-on” module to enhance the tracking performance of black-box control systems in impromptu tracking tasks. Our approach is to pre-cascade a deep neural network (DNN) to a stabilized baseline control system, in order to establish an identity mapping from the desired output to the actual output. Previous research involving quadrotors showed that, for 30 arbitrary hand-drawn trajectories, the DNN-enhancement control architecture reduces tracking errors by 43% on average, as compared with the baseline controller. In this article, we provide a platform-independent formulation and practical design guidelines for the DNN-enhancement approach. In particular, we: (1) characterize the underlying function of the DNN module; (2) identify necessary conditions for the approach to be effective; (3) provide theoretical insights into the stability of the overall DNN-enhancement control architecture; (4) derive a condition that supports data-efficient training of the DNN module; and (5) compare the novel theory-driven DNN design with the prior trial-and-error design using detailed quadrotor experiments. We show that, as compared with the prior trial-and-error design, the novel theory-driven design allows us to reduce the input dimension of the DNN by two thirds while achieving similar tracking performance.},
  archive  = {J},
  author   = {Siqi Zhou and Mohamed K Helwa and Angela P Schoellig},
  doi      = {10.1177/0278364920953902},
  journal  = {The International Journal of Robotics Research},
  month    = {10},
  number   = {12},
  pages    = {1397-1418},
  title    = {Deep neural networks as add-on modules for enhancing robot performance in impromptu trajectory tracking},
  volume   = {39},
  year     = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Three-dimensional independent control of multiple magnetic
microrobots via inter-agent forces. <em>The International Journal of
Robotics Research</em>, <em>39</em>(12), 1377–1396. (<a
href="https://doi.org/10.1177/0278364920933655">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {This article presents a method to independently control the position of multiple microscale magnetic robots in three dimensions, operating in close proximity to each other. Having multiple magnetic microrobots work together in close proximity is difficult due to magnetic interactions between the robots, and here we aim to control those interactions for the creation of desired multi-agent formations in three dimensions. Based on the fact that all magnetic agents orient to the global input magnetic field, the local attraction–repulsion forces between nearby agents can be regulated. For the first time, 3D manipulation of two microgripping magnetic microrobots is demonstrated. We also mathematically and experimentally prove that the center-of-mass external magnetic pulling of the multi-agent system is possible in three dimensions with an underactuated magnetic field generator. Here we utilize the controlled interaction magnetic forces between two spherical agents to steer them along two prescribed paths. We apply our method to independently control the motion of a pair of magnetic microgrippers as functional microrobot candidates each equipped with a five-degree-of-freedom motion mechanism and a grasp–release mechanism for targeted cargo delivery. A proportional controller and an optimization-based controller are introduced and compared, with potential to control more than two magnetic agents in three dimensions. Average tracking errors of less than 141 and 165 micrometers are accomplished for the regulation of agents’ positions using optimization-based and proportional controllers, respectively, for spherical agents with approximate nominal radius of 500 micrometers operating within several body-lengths of each other.},
  archive  = {J},
  author   = {Mohammad Salehizadeh and Eric Diller},
  doi      = {10.1177/0278364920933655},
  journal  = {The International Journal of Robotics Research},
  month    = {10},
  number   = {12},
  pages    = {1377-1396},
  title    = {Three-dimensional independent control of multiple magnetic microrobots via inter-agent forces},
  volume   = {39},
  year     = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Ford multi-AV seasonal dataset. <em>The International
Journal of Robotics Research</em>, <em>39</em>(12), 1367–1376. (<a
href="https://doi.org/10.1177/0278364920961451">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {This article presents a challenging multi-agent seasonal dataset collected by a fleet of Ford autonomous vehicles (AVs) at different days and times during 2017–2018. The vehicles traversed an average route of 66 km in Michigan that included a mix of driving scenarios such as the Detroit airport, freeways, city centers, university campus, and suburban neighborhoods. Each vehicle used in this data collection was a Ford Fusion outfitted with an Applanix POS-LV GNSS/INS system, four HDL-32E Velodyne 3D-lidar scanners, six Point Grey 1.3 MP cameras arranged on the rooftop for 360° coverage, and one Point Grey 5 MP camera mounted behind the windshield for the forward field of view. We present the seasonal variation in weather, lighting, construction, and traffic conditions experienced in dynamic urban environments. We also include data from multiple AVs that were driven in close proximity. This dataset can help design robust algorithms for AVs and multi-agent systems. Each log in the dataset is time-stamped and contains raw data from all the sensors, calibration values, pose trajectory, ground-truth pose, and 3D maps. All data is available in rosbag format that can be visualized, modified, and applied using the open-source Robot Operating System (ROS). We also provide the output of reflectivity-based localization for bench-marking purposes. The dataset can be freely downloaded at avdata.ford.com .},
  archive  = {J},
  author   = {Siddharth Agarwal and Ankit Vora and Gaurav Pandey and Wayne Williams and Helen Kourous and James McBride},
  doi      = {10.1177/0278364920961451},
  journal  = {The International Journal of Robotics Research},
  month    = {10},
  number   = {12},
  pages    = {1367-1376},
  title    = {Ford multi-AV seasonal dataset},
  volume   = {39},
  year     = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). The blackbird UAV dataset. <em>The International Journal of
Robotics Research</em>, <em>39</em>(10-11), 1346–1364. (<a
href="https://doi.org/10.1177/0278364920908331">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {This article describes the Blackbird unmanned aerial vehicle (UAV) Dataset, a large-scale suite of sensor data and corresponding ground truth from a custom-built quadrotor platform equipped with an inertial measurement unit (IMU), rotor tachometers, and virtual color, grayscale, and depth cameras. Motivated by the increasing demand for agile, autonomous operation of aerial vehicles, this dataset is designed to facilitate the development and evaluation of high-performance UAV perception algorithms. The dataset contains over 10 hours of data from our quadrotor tracing 18 different trajectories at varying maximum speeds (0.5 to 13.8 ms -1 ) through 5 different visual environments for a total of 176 unique flights. For each flight, we provide 120 Hz grayscale, 60 Hz RGB-D, and 60 Hz semantically segmented images from forward stereo and downward-facing photorealistic virtual cameras in addition to 100 Hz IMU, ~190 Hz motor speed sensors, and 360 Hz millimeter-accurate motion capture ground truth. The Blackbird UAV dataset is therefore well suited to the development of algorithms for visual inertial navigation, 3D reconstruction, and depth estimation. As a benchmark for future algorithms, the performance of two state-of-the-art visual odometry algorithms are reported and scripts for comparing against the benchmarks are included with the dataset. The dataset is available for download at http://blackbird-dataset.mit.edu/ .},
  archive  = {J},
  author   = {Amado Antonini and Winter Guerra and Varun Murali and Thomas Sayre-McCord and Sertac Karaman},
  doi      = {10.1177/0278364920908331},
  journal  = {The International Journal of Robotics Research},
  month    = {9},
  number   = {10-11},
  pages    = {1346-1364},
  title    = {The blackbird UAV dataset},
  volume   = {39},
  year     = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). On infusing reachability-based safety assurance within
planning frameworks for human–robot vehicle interactions. <em>The
International Journal of Robotics Research</em>, <em>39</em>(10-11),
1326–1345. (<a href="https://doi.org/10.1177/0278364920950795">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Action anticipation, intent prediction, and proactive behavior are all desirable characteristics for autonomous driving policies in interactive scenarios. Paramount, however, is ensuring safety on the road: a key challenge in doing so is accounting for uncertainty in human driver actions without unduly impacting planner performance. This article introduces a minimally interventional safety controller operating within an autonomous vehicle control stack with the role of ensuring collision-free interaction with an externally controlled (e.g., human-driven) counterpart while respecting static obstacles such as a road boundary wall. We leverage reachability analysis to construct a real-time (100 Hz) controller that serves the dual role of (i) tracking an input trajectory from a higher-level planning algorithm using model predictive control, and (ii) assuring safety by maintaining the availability of a collision-free escape maneuver as a persistent constraint regardless of whatever future actions the other car takes. A full-scale steer-by-wire platform is used to conduct traffic-weaving experiments wherein two cars, initially side-by-side, must swap lanes in a limited amount of time and distance, emulating cars merging onto/off of a highway. We demonstrate that, with our control stack, the autonomous vehicle is able to avoid collision even when the other car defies the planner’s expectations and takes dangerous actions, either carelessly or with the intent to collide, and otherwise deviates minimally from the planned trajectory to the extent required to maintain safety.},
  archive  = {J},
  author   = {Karen Leung and Edward Schmerling and Mengxuan Zhang and Mo Chen and John Talbot and J Christian Gerdes and Marco Pavone},
  doi      = {10.1177/0278364920950795},
  journal  = {The International Journal of Robotics Research},
  month    = {9},
  number   = {10-11},
  pages    = {1326-1345},
  title    = {On infusing reachability-based safety assurance within planning frameworks for human–robot vehicle interactions},
  volume   = {39},
  year     = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Design and optimal control of a tiltrotor micro-aerial
vehicle for efficient omnidirectional flight. <em>The International
Journal of Robotics Research</em>, <em>39</em>(10-11), 1305–1325. (<a
href="https://doi.org/10.1177/0278364920943654">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Omnidirectional micro-aerial vehicles (MAVs) are a growing field of research, with demonstrated advantages for aerial interaction and uninhibited observation. While systems with complete pose omnidirectionality and high hover efficiency have been developed independently, a robust system that combines the two has not been demonstrated to date. This paper presents the design and optimal control of a novel omnidirectional vehicle that can exert a wrench in any orientation while maintaining efficient flight configurations. The system design is motivated by the result of a morphology design optimization. A six-degree-of-freedom optimal controller is derived, with an actuator allocation approach that implements task prioritization, and is robust to singularities. Flight experiments demonstrate and verify the system’s capabilities.},
  archive  = {J},
  author   = {Mike Allenspach and Karen Bodie and Maximilian Brunner and Luca Rinsoz and Zachary Taylor and Mina Kamel and Roland Siegwart and Juan Nieto},
  doi      = {10.1177/0278364920943654},
  journal  = {The International Journal of Robotics Research},
  month    = {9},
  number   = {10-11},
  pages    = {1305-1325},
  title    = {Design and optimal control of a tiltrotor micro-aerial vehicle for efficient omnidirectional flight},
  volume   = {39},
  year     = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Multimodal estimation and communication of latent semantic
knowledge for robust execution of robot instructions. <em>The
International Journal of Robotics Research</em>, <em>39</em>(10-11),
1279–1304. (<a href="https://doi.org/10.1177/0278364920917755">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {The goal of this article is to enable robots to perform robust task execution following human instructions in partially observable environments. A robot’s ability to interpret and execute commands is fundamentally tied to its semantic world knowledge. Commonly, robots use exteroceptive sensors, such as cameras or LiDAR, to detect entities in the workspace and infer their visual properties and spatial relationships. However, semantic world properties are often visually imperceptible. We posit the use of non-exteroceptive modalities including physical proprioception, factual descriptions, and domain knowledge as mechanisms for inferring semantic properties of objects. We introduce a probabilistic model that fuses linguistic knowledge with visual and haptic observations into a cumulative belief over latent world attributes to infer the meaning of instructions and execute the instructed tasks in a manner robust to erroneous, noisy, or contradictory evidence. In addition, we provide a method that allows the robot to communicate knowledge dissonance back to the human as a means of correcting errors in the operator’s world model. Finally, we propose an efficient framework that anticipates possible linguistic interactions and infers the associated groundings for the current world state, thereby bootstrapping both language understanding and generation. We present experiments on manipulators for tasks that require inference over partially observed semantic properties, and evaluate our framework’s ability to exploit expressed information and knowledge bases to facilitate convergence, and generate statements to correct declared facts that were observed to be inconsistent with the robot’s estimate of object properties.},
  archive  = {J},
  author   = {Jacob Arkin and Daehyung Park and Subhro Roy and Matthew R Walter and Nicholas Roy and Thomas M Howard and Rohan Paul},
  doi      = {10.1177/0278364920917755},
  journal  = {The International Journal of Robotics Research},
  month    = {9},
  number   = {10-11},
  pages    = {1279-1304},
  title    = {Multimodal estimation and communication of latent semantic knowledge for robust execution of robot instructions},
  volume   = {39},
  year     = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Scaling simulation-to-real transfer by learning a latent
space of robot skills. <em>The International Journal of Robotics
Research</em>, <em>39</em>(10-11), 1259–1278. (<a
href="https://doi.org/10.1177/0278364920944474">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {We present a strategy for simulation-to-real transfer, which builds on recent advances in robot skill decomposition. Rather than focusing on minimizing the simulation–reality gap, we propose a method for increasing the sample efficiency and robustness of existing simulation-to-real approaches which exploits hierarchy and online adaptation. Instead of learning a unique policy for each desired robotic task, we learn a diverse set of skills and their variations, and embed those skill variations in a continuously parameterized space. We then interpolate, search, and plan in this space to find a transferable policy which solves more complex, high-level tasks by combining low-level skills and their variations. In this work, we first characterize the behavior of this learned skill space, by experimenting with several techniques for composing pre-learned latent skills. We then discuss an algorithm which allows our method to perform long-horizon tasks never seen in simulation, by intelligently sequencing short-horizon latent skills. Our algorithm adapts to unseen tasks online by repeatedly choosing new skills from the latent space, using live sensor data and simulation to predict which latent skill will perform best next in the real world. Importantly, our method learns to control a real robot in joint-space to achieve these high-level tasks with little or no on-robot time, despite the fact that the low-level policies may not be perfectly transferable from simulation to real, and that the low-level skills were not trained on any examples of high-level tasks. In addition to our results indicating a lower sample complexity for families of tasks, we believe that our method provides a promising template for combining learning-based methods with proven classical robotics algorithms such as model-predictive control.},
  archive  = {J},
  author   = {Ryan C Julian and Eric Heiden and Zhanpeng He and Hejia Zhang and Stefan Schaal and Joseph J Lim and Gaurav S Sukhatme and Karol Hausman},
  doi      = {10.1177/0278364920944474},
  journal  = {The International Journal of Robotics Research},
  month    = {9},
  number   = {10-11},
  pages    = {1259-1278},
  title    = {Scaling simulation-to-real transfer by learning a latent space of robot skills},
  volume   = {39},
  year     = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Contact-space resolution model for a physically consistent
view of simultaneous collisions in articulated-body systems: Theory and
experimental results. <em>The International Journal of Robotics
Research</em>, <em>39</em>(10-11), 1239–1258. (<a
href="https://doi.org/10.1177/0278364920955242">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Multi-surface interactions occur frequently in articulated-rigid-body systems such as robotic manipulators. Real-time prediction of contact-interaction forces is challenging for systems with many degrees of freedom (DOFs) because joint and contact constraints must be enforced simultaneously. While several contact models exist for systems of free rigid bodies, fewer models are available for articulated-body systems. In this paper, we extend the method of Ruspini and Khatib and develop the contact-space resolution (CSR) model by applying the operational space theory of robot manipulation. Through a proper choice of contact-space coordinates, the projected dynamics of the system in the contact space is obtained. We show that the projection into the dynamically consistent null space preserves linear and angular momentum in a subspace of the system dynamics complementary to the joint and contact constraints. Furthermore, we illustrate that a simultaneous collision event between two articulated bodies can be resolved as an equivalent simultaneous collision between two non-articulated rigid bodies through the projected contact-space dynamics. Solving this reduced-dimensional problem is computationally efficient, but determining its accuracy requires physical experimentation. To gain further insights into the theoretical model predictions, we devised an apparatus consisting of colliding 1-, 2-, and 3-DOF articulated bodies where joint motion is recorded with high precision. Results validate that the CSR model accurately predicts the post-collision system state. Moreover, for the first time, we show that the projection of system dynamics into the mutually complementary contact space and null space is a physically verifiable phenomenon in articulated-rigid-body systems.},
  archive  = {J},
  author   = {Shameek Ganguly and Oussama Khatib},
  doi      = {10.1177/0278364920955242},
  journal  = {The International Journal of Robotics Research},
  month    = {9},
  number   = {10-11},
  pages    = {1239-1258},
  title    = {Contact-space resolution model for a physically consistent view of simultaneous collisions in articulated-body systems: Theory and experimental results},
  volume   = {39},
  year     = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Distributed and consistent multi-image feature matching via
QuickMatch. <em>The International Journal of Robotics Research</em>,
<em>39</em>(10-11), 1222–1238. (<a
href="https://doi.org/10.1177/0278364920917465">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {In this work, we consider the multi-image object matching problem in distributed networks of robots. Multi-image feature matching is a keystone of many applications, including Simultaneous Localization and Mapping, homography, object detection, and Structure from Motion. We first review the QuickMatch algorithm for multi-image feature matching. We then present NetMatch, an algorithm for distributing sets of features across computational units (agents) that largely preserves feature match quality and minimizes communication between agents (avoiding, in particular, the need to flood all data to all agents). Finally, we present an experimental application of both QuickMatch and NetMatch on an object matching test with low-quality images. The QuickMatch and NetMatch algorithms are compared with other standard matching algorithms in terms of preservation of match consistency. Our experiments show that QuickMatch and Netmatch can scale to larger numbers of images and features, and match more accurately than standard techniques.},
  archive  = {J},
  author   = {Zachary Serlin and Guang Yang and Brandon Sookraj and Calin Belta and Roberto Tron},
  doi      = {10.1177/0278364920917465},
  journal  = {The International Journal of Robotics Research},
  month    = {9},
  number   = {10-11},
  pages    = {1222-1238},
  title    = {Distributed and consistent multi-image feature matching via QuickMatch},
  volume   = {39},
  year     = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Search and rescue under the forest canopy using multiple
UAVs. <em>The International Journal of Robotics Research</em>,
<em>39</em>(10-11), 1201–1221. (<a
href="https://doi.org/10.1177/0278364920929398">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {We present a multi-robot system for GPS-denied search and rescue under the forest canopy. Forests are particularly challenging environments for collaborative exploration and mapping, in large part due to the existence of severe perceptual aliasing which hinders reliable loop closure detection for mutual localization and map fusion. Our proposed system features unmanned aerial vehicles (UAVs) that perform onboard sensing, estimation, and planning. When communication is available, each UAV transmits compressed tree-based submaps to a central ground station for collaborative simultaneous localization and mapping (CSLAM). To overcome high measurement noise and perceptual aliasing, we use the local configuration of a group of trees as a distinctive feature for robust loop closure detection. Furthermore, we propose a novel procedure based on cycle consistent multiway matching to recover from incorrect pairwise data associations. The returned global data association is guaranteed to be cycle consistent, and is shown to improve both precision and recall compared with the input pairwise associations. The proposed multi-UAV system is validated both in simulation and during real-world collaborative exploration missions at NASA Langley Research Center.},
  archive  = {J},
  author   = {Yulun Tian and Katherine Liu and Kyel Ok and Loc Tran and Danette Allen and Nicholas Roy and Jonathan P. How},
  doi      = {10.1177/0278364920929398},
  journal  = {The International Journal of Robotics Research},
  month    = {9},
  number   = {10-11},
  pages    = {1201-1221},
  title    = {Search and rescue under the forest canopy using multiple UAVs},
  volume   = {39},
  year     = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). ISER 2018 editorial. <em>The International Journal of
Robotics Research</em>, <em>39</em>(10-11), 1199–1200. (<a
href="https://doi.org/10.1177/0278364920960955">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive = {J},
  author  = {Jing Xiao and Torsten Kröger and Oussama Khatib},
  doi     = {10.1177/0278364920960955},
  journal = {The International Journal of Robotics Research},
  month   = {9},
  number  = {10-11},
  pages   = {1199-1200},
  title   = {ISER 2018 editorial},
  volume  = {39},
  year    = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Data-driven koopman operators for model-based shared control
of human–machine systems. <em>The International Journal of Robotics
Research</em>, <em>39</em>(9), 1178–1195. (<a
href="https://doi.org/10.1177/0278364920921935">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {We present a data-driven shared control algorithm that can be used to improve a human operator’s control of complex dynamic machines and achieve tasks that would otherwise be challenging, or impossible, for the user on their own. Our method assumes no a priori knowledge of the system dynamics. Instead, both the dynamics and information about the user’s interaction are learned from observation through the use of a Koopman operator. Using the learned model, we define an optimization problem to compute the autonomous partner’s control policy. Finally, we dynamically allocate control authority to each partner based on a comparison of the user input and the autonomously generated control. We refer to this idea as model-based shared control (MbSC). We evaluate the efficacy of our approach with two human subjects studies consisting of 32 total participants (16 subjects in each study). The first study imposes a linear constraint on the modeling and autonomous policy generation algorithms. The second study explores the more general, nonlinear variant. Overall, we find that MbSC significantly improves task and control metrics when compared with a natural learning, or user only, control paradigm. Our experiments suggest that models learned via the Koopman operator generalize across users, indicating that it is not necessary to collect data from each individual user before providing assistance with MbSC. We also demonstrate the data efficiency of MbSC and, consequently, its usefulness in online learning paradigms. Finally, we find that the nonlinear variant has a greater impact on a user’s ability to successfully achieve a defined task than the linear variant.},
  archive  = {J},
  author   = {Alexander Broad and Ian Abraham and Todd Murphey and Brenna Argall},
  doi      = {10.1177/0278364920921935},
  journal  = {The International Journal of Robotics Research},
  month    = {8},
  number   = {9},
  pages    = {1178-1195},
  title    = {Data-driven koopman operators for model-based shared control of human–machine systems},
  volume   = {39},
  year     = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). FSMI: Fast computation of shannon mutual information for
information-theoretic mapping. <em>The International Journal of Robotics
Research</em>, <em>39</em>(9), 1155–1177. (<a
href="https://doi.org/10.1177/0278364920921941">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Exploration tasks are embedded in many robotics applications, such as search and rescue and space exploration. Information-based exploration algorithms aim to find the most informative trajectories by maximizing an information-theoretic metric, such as the mutual information between the map and potential future measurements. Unfortunately, most existing information-based exploration algorithms are plagued by the computational difficulty of evaluating the Shannon mutual information metric. In this article, we consider the fundamental problem of evaluating Shannon mutual information between the map and a range measurement. First, we consider 2D environments. We propose a novel algorithm, called the fast Shannon mutual information (FSMI). The key insight behind the algorithm is that a certain integral can be computed analytically, leading to substantial computational savings. Second, we consider 3D environments, represented by efficient data structures, e.g., an OctoMap, such that the measurements are compressed by run-length encoding (RLE). We propose a novel algorithm, called FSMI-RLE, that efficiently evaluates the Shannon mutual information when the measurements are compressed using RLE. For both the FSMI and the FSMI-RLE, we also propose variants that make different assumptions on the sensor noise distribution for the purpose of further computational savings. We evaluate the proposed algorithms in extensive experiments. In particular, we show that the proposed algorithms outperform existing algorithms that compute Shannon mutual information as well as other algorithms that compute the Cauchy–Schwarz quadratic mutual information (CSQMI). In addition, we demonstrate the computation of Shannon mutual information on a 3D map for the first time.},
  archive  = {J},
  author   = {Zhengdong Zhang and Theia Henderson and Sertac Karaman and Vivienne Sze},
  doi      = {10.1177/0278364920921941},
  journal  = {The International Journal of Robotics Research},
  month    = {8},
  number   = {9},
  pages    = {1155-1177},
  title    = {FSMI: Fast computation of shannon mutual information for information-theoretic mapping},
  volume   = {39},
  year     = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Task-based hybrid shared control for training through
forceful interaction. <em>The International Journal of Robotics
Research</em>, <em>39</em>(9), 1138–1154. (<a
href="https://doi.org/10.1177/0278364920933654">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Despite the fact that robotic platforms can provide both consistent practice and objective assessments of users over the course of their training, there are relatively few instances where physical human–robot interaction has been significantly more effective than unassisted practice or human-mediated training. This article describes a hybrid shared control robot, which enhances task learning through kinesthetic feedback. The assistance assesses user actions using a task-specific evaluation criterion and selectively accepts or rejects them at each time instant. Through two human subject studies (total n = 68 ), we show that this hybrid approach of switching between full transparency and full rejection of user inputs leads to increased skill acquisition and short-term retention compared with unassisted practice. Moreover, we show that the shared control paradigm exhibits features previously shown to promote successful training. It avoids user passivity by only rejecting user actions and allowing failure at the task. It improves performance during assistance, providing meaningful task-specific feedback. It is sensitive to initial skill of the user and behaves as an “assist-as-needed” control scheme, adapting its engagement in real time based on the performance and needs of the user. Unlike other successful algorithms, it does not require explicit modulation of the level of impedance or error amplification during training and it is permissive to a range of strategies because of its evaluation criterion. We demonstrate that the proposed hybrid shared control paradigm with a task-based minimal intervention criterion significantly enhances task-specific training.},
  archive  = {J},
  author   = {Kathleen Fitzsimons and Aleksandra Kalinowska and Julius P Dewald and Todd D Murphey},
  doi      = {10.1177/0278364920933654},
  journal  = {The International Journal of Robotics Research},
  month    = {8},
  number   = {9},
  pages    = {1138-1154},
  title    = {Task-based hybrid shared control for training through forceful interaction},
  volume   = {39},
  year     = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Image-based estimation, planning, and control for high-speed
flying through multiple openings. <em>The International Journal of
Robotics Research</em>, <em>39</em>(9), 1122–1137. (<a
href="https://doi.org/10.1177/0278364920921943">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {This article focuses on enabling an aerial robot to fly through multiple openings at high speed using image-based estimation, planning, and control. State-of-the-art approaches assume that the robot’s global translational variables (e.g., position and velocity) can either be measured directly with external localization sensors or estimated onboard. Unfortunately, estimating the translational variables may be impractical because modeling errors and sensor noise can lead to poor performance. Furthermore, monocular-camera-based pose estimation techniques typically require a model of the gap (window) in order to handle the unknown scale. Herein, a new scheme for image-based estimation, aggressive-maneuvering trajectory generation, and motion control is developed for multi-rotor aerial robots. The approach described does not rely on measurement of the translational variables and does not require the model of the gap or window. First, the robot dynamics are expressed in terms of the image features that are invariant to rotation (invariant features). This step decouples the robot’s attitude and keeps the invariant features in the flat output space of the differentially flat system. Second, an optimal trajectory is efficiently generated in real time to obtain the dynamically-feasible trajectory for the invariant features. Finally, a controller is designed to enable real-time, image-based tracking of the trajectory. The performance of the estimation, planning, and control scheme is validated in simulations and through 80 successful experimental trials. Results show the ability to successfully fly through two narrow openings, where the estimation and planning computation and motion control from one opening to the next are performed in real time on the robot.},
  archive  = {J},
  author   = {Dejun Guo and Kam K Leang},
  doi      = {10.1177/0278364920921943},
  journal  = {The International Journal of Robotics Research},
  month    = {8},
  number   = {9},
  pages    = {1122-1137},
  title    = {Image-based estimation, planning, and control for high-speed flying through multiple openings},
  volume   = {39},
  year     = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Relative multiplicative extended kalman filter for
observable GPS-denied navigation. <em>The International Journal of
Robotics Research</em>, <em>39</em>(9), 1085–1121. (<a
href="https://doi.org/10.1177/0278364920903094">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {This work presents a multiplicative extended Kalman filter (MEKF) for estimating the relative state of a multirotor vehicle operating in a GPS-denied environment. The filter fuses data from an inertial measurement unit and altimeter with relative-pose updates from a keyframe-based visual odometry or laser scan-matching algorithm. Because the global position and heading states of the vehicle are unobservable in the absence of global measurements such as GPS, the filter in this article estimates the state with respect to a local frame that is colocated with the odometry keyframe. As a result, the odometry update provides nearly direct measurements of the relative vehicle pose, making those states observable. Recent publications have rigorously documented the theoretical advantages of such an observable parameterization, including improved consistency, accuracy, and system robustness, and have demonstrated the effectiveness of such an approach during prolonged multirotor flight tests. This article complements this prior work by providing a complete, self-contained, tutorial derivation of the relative MEKF, which has been thoroughly motivated but only briefly described to date. This article presents several improvements and extensions to the filter while clearly defining all quaternion conventions and properties used, including several new useful properties relating to error quaternions and their Euler-angle decomposition. Finally, this article derives the filter both for traditional dynamics defined with respect to an inertial frame, and for robocentric dynamics defined with respect to the vehicle’s body frame, and provides insights into the subtle differences that arise between the two formulations.},
  archive  = {J},
  author   = {Daniel P Koch and David O Wheeler and Randal W Beard and Timothy W McLain and Kevin M Brink},
  doi      = {10.1177/0278364920903094},
  journal  = {The International Journal of Robotics Research},
  month    = {8},
  number   = {9},
  pages    = {1085-1121},
  title    = {Relative multiplicative extended kalman filter for observable GPS-denied navigation},
  volume   = {39},
  year     = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Large-scale, real-time visual–inertial localization
revisited. <em>The International Journal of Robotics Research</em>,
<em>39</em>(9), 1061–1084. (<a
href="https://doi.org/10.1177/0278364920931151">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {The overarching goals in image-based localization are scale, robustness, and speed. In recent years, approaches based on local features and sparse 3D point-cloud models have both dominated the benchmarks and seen successful real-world deployment. They enable applications ranging from robot navigation, autonomous driving, virtual and augmented reality to device geo-localization. Recently, end-to-end learned localization approaches have been proposed which show promising results on small-scale datasets. However, the positioning accuracy, scalability, latency, and compute and storage requirements of these approaches remain open challenges. We aim to deploy localization at a global scale where one thus relies on methods using local features and sparse 3D models. Our approach spans from offline model building to real-time client-side pose fusion. The system compresses the appearance and geometry of the scene for efficient model storage and lookup leading to scalability beyond what has been demonstrated previously. It allows for low-latency localization queries and efficient fusion to be run in real-time on mobile platforms by combining server-side localization with real-time visual–inertial-based camera pose tracking. In order to further improve efficiency, we leverage a combination of priors, nearest-neighbor search, geometric match culling, and a cascaded pose candidate refinement step. This combination outperforms previous approaches when working with large-scale models and allows deployment at unprecedented scale. We demonstrate the effectiveness of our approach on a proof-of-concept system localizing 2.5 million images against models from four cities in different regions of the world achieving query latencies in the 200 ms range.},
  archive  = {J},
  author   = {Simon Lynen and Bernhard Zeisl and Dror Aiger and Michael Bosse and Joel Hesch and Marc Pollefeys and Roland Siegwart and Torsten Sattler},
  doi      = {10.1177/0278364920931151},
  journal  = {The International Journal of Robotics Research},
  month    = {8},
  number   = {9},
  pages    = {1061-1084},
  title    = {Large-scale, real-time visual–inertial localization revisited},
  volume   = {39},
  year     = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). The UMA-VI dataset: Visual–inertial odometry in low-textured
and dynamic illumination environments. <em>The International Journal of
Robotics Research</em>, <em>39</em>(9), 1052–1060. (<a
href="https://doi.org/10.1177/0278364920938439">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {This article presents a visual–inertial dataset gathered in indoor and outdoor scenarios with a handheld custom sensor rig, for over 80 min in total. The dataset contains hardware-synchronized data from a commercial stereo camera (Bumblebee®2), a custom stereo rig, and an inertial measurement unit. The most distinctive feature of this dataset is the strong presence of low-textured environments and scenes with dynamic illumination, which are recurrent corner cases of visual odometry and simultaneous localization and mapping (SLAM) methods. The dataset comprises 32 sequences and is provided with ground-truth poses at the beginning and the end of each of the sequences, thus allowing the accumulated drift to be measured in each case. We provide a trial evaluation of five existing state-of-the-art visual and visual–inertial methods on a subset of the dataset. We also make available open-source tools for evaluation purposes, as well as the intrinsic and extrinsic calibration parameters of all sensors in the rig. The dataset is available for download at http://mapir.uma.es/work/uma-visual-inertial-dataset},
  archive  = {J},
  author   = {David Zuñiga-Noël and Alberto Jaenal and Ruben Gomez-Ojeda and Javier Gonzalez-Jimenez},
  doi      = {10.1177/0278364920938439},
  journal  = {The International Journal of Robotics Research},
  month    = {8},
  number   = {9},
  pages    = {1052-1060},
  title    = {The UMA-VI dataset: Visual–inertial odometry in low-textured and dynamic illumination environments},
  volume   = {39},
  year     = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). PanoraMIS: An ultra-wide field of view image dataset for
vision-based robot-motion estimation. <em>The International Journal of
Robotics Research</em>, <em>39</em>(9), 1037–1051. (<a
href="https://doi.org/10.1177/0278364920915248">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {This article presents a new dataset of ultra-wide field of view images with accurate ground truth, called PanoraMIS . The dataset covers a large spectrum of panoramic cameras (catadioptric, twin-fisheye), robotic platforms (wheeled, aerial, and industrial robots), and testing environments (indoors and outdoors), and it is well suited to rigorously validate novel image-based robot-motion estimation algorithms, including visual odometry, visual SLAM, and deep learning-based methods. PanoraMIS and the accompanying documentation is publicly available on the Internet for the entire research community.},
  archive  = {J},
  author   = {Houssem-Eddine Benseddik and Fabio Morbidi and Guillaume Caron},
  doi      = {10.1177/0278364920915248},
  journal  = {The International Journal of Robotics Research},
  month    = {8},
  number   = {9},
  pages    = {1037-1051},
  title    = {PanoraMIS: An ultra-wide field of view image dataset for vision-based robot-motion estimation},
  volume   = {39},
  year     = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Video dataset of human demonstrations of folding clothing
for robotic folding. <em>The International Journal of Robotics
Research</em>, <em>39</em>(9), 1031–1036. (<a
href="https://doi.org/10.1177/0278364920940408">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {General-purpose clothes-folding robots do not yet exist owing to the deformable nature of textiles, making it hard to engineer manipulation pipelines or learn this task. In order to accelerate research for the learning of the robotic clothes-folding task, we introduce a video dataset of human folding demonstrations. In total, we provide 8.5 hours of demonstrations from multiple perspectives leading to 1,000 folding samples of different types of textiles. The demonstrations are recorded in multiple public places, in different conditions with a diverse set of people. Our dataset consists of anonymized RGB images, depth frames, skeleton keypoint trajectories, and object labels. In this article, we describe our recording setup, the data format, and utility scripts, which can be accessed at https://adverley.github.io/folding-demonstrations .},
  archive  = {J},
  author   = {Andreas Verleysen and Matthijs Biondina and Francis wyffels},
  doi      = {10.1177/0278364920940408},
  journal  = {The International Journal of Robotics Research},
  month    = {8},
  number   = {9},
  pages    = {1031-1036},
  title    = {Video dataset of human demonstrations of folding clothing for robotic folding},
  volume   = {39},
  year     = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Reactive sampling-based path planning with temporal logic
specifications. <em>The International Journal of Robotics Research</em>,
<em>39</em>(8), 1002–1028. (<a
href="https://doi.org/10.1177/0278364920918919">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {We develop a sampling-based motion planning algorithm that combines long-term temporal logic goals with short-term reactive requirements. The mission specification has two parts: (1) a global specification given as a linear temporal logic (LTL) formula over a set of static service requests that occur at the regions of a known environment, and (2) a local specification that requires servicing a set of dynamic requests that can be sensed locally during the execution. The proposed computational framework consists of two main ingredients: (a) an off-line sampling-based algorithm for the construction of a global transition system that contains a path satisfying the LTL formula; and (b) an on-line sampling-based algorithm to generate paths that service the local requests, while making sure that the satisfaction of the global specification is not affected. The off-line algorithm has four main features. First, it is incremental, in the sense that the procedure for finding a satisfying path at each iteration scales only with the number of new samples generated at that iteration. Second, the underlying graph is sparse, which implies low complexity for the overall method. Third, it is probabilistically complete. Fourth, under some mild assumptions, it has the best possible complexity bound. The on-line algorithm leverages ideas from LTL monitoring and potential functions to ensure progress towards the satisfaction of the global specification while servicing locally sensed requests. Examples and experimental trials illustrating the usefulness and the performance of the framework are included.},
  archive  = {J},
  author   = {Cristian Ioan Vasile and Xiao Li and Calin Belta},
  doi      = {10.1177/0278364920918919},
  journal  = {The International Journal of Robotics Research},
  month    = {7},
  number   = {8},
  pages    = {1002-1028},
  title    = {Reactive sampling-based path planning with temporal logic specifications},
  volume   = {39},
  year     = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Multimodal trajectory optimization for motion planning.
<em>The International Journal of Robotics Research</em>, <em>39</em>(8),
983–1001. (<a href="https://doi.org/10.1177/0278364920918296">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Existing motion planning methods often have two drawbacks: (1) goal configurations need to be specified by a user, and (2) only a single solution is generated under a given condition. In practice, multiple possible goal configurations exist to achieve a task. Although the choice of the goal configuration significantly affects the quality of the resulting trajectory, it is not trivial for a user to specify the optimal goal configuration. In addition, the objective function used in the trajectory optimization is often non-convex, and it can have multiple solutions that achieve comparable costs. In this study, we propose a framework that determines multiple trajectories that correspond to the different modes of the cost function. We reduce the problem of identifying the modes of the cost function to that of estimating the density induced by a distribution based on the cost function. The proposed framework enables users to select a preferable solution from multiple candidate trajectories, thereby making it easier to tune the cost function and obtain a satisfactory solution. We evaluated our proposed method with motion planning tasks in 2D and 3D space. Our experiments show that the proposed algorithm is capable of determining multiple solutions for those tasks.},
  archive  = {J},
  author   = {Takayuki Osa},
  doi      = {10.1177/0278364920918296},
  journal  = {The International Journal of Robotics Research},
  month    = {7},
  number   = {8},
  pages    = {983-1001},
  title    = {Multimodal trajectory optimization for motion planning},
  volume   = {39},
  year     = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Manipulating deformable objects by interleaving prediction,
planning, and control. <em>The International Journal of Robotics
Research</em>, <em>39</em>(8), 957–982. (<a
href="https://doi.org/10.1177/0278364920918299">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {We present a framework for deformable object manipulation that interleaves planning and control, enabling complex manipulation tasks without relying on high-fidelity modeling or simulation. The key question we address is when should we use planning and when should we use control to achieve the task? Planners are designed to find paths through complex configuration spaces, but for highly underactuated systems, such as deformable objects, achieving a specific configuration is very difficult even with high-fidelity models. Conversely, controllers can be designed to achieve specific configurations, but they can be trapped in undesirable local minima owing to obstacles. Our approach consists of three components: (1) a global motion planner to generate gross motion of the deformable object; (2) a local controller for refinement of the configuration of the deformable object; and (3) a novel deadlock prediction algorithm to determine when to use planning versus control. By separating planning from control we are able to use different representations of the deformable object, reducing overall complexity and enabling efficient computation of motion. We provide a detailed proof of probabilistic completeness for our planner, which is valid despite the fact that our system is underactuated and we do not have a steering function. We then demonstrate that our framework is able to successfully perform several manipulation tasks with rope and cloth in simulation, which cannot be performed using either our controller or planner alone. These experiments suggest that our planner can generate paths efficiently, taking under a second on average to find a feasible path in three out of four scenarios. We also show that our framework is effective on a 16-degree-of-freedom physical robot, where reachability and dual-arm constraints make the planning more difficult.},
  archive  = {J},
  author   = {Dale McConachie and Andrew Dobson and Mengyao Ruan and Dmitry Berenson},
  doi      = {10.1177/0278364920918299},
  journal  = {The International Journal of Robotics Research},
  month    = {7},
  number   = {8},
  pages    = {957-982},
  title    = {Manipulating deformable objects by interleaving prediction, planning, and control},
  volume   = {39},
  year     = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Dynamic locomotion for passive-ankle biped robots and
humanoids using whole-body locomotion control. <em>The International
Journal of Robotics Research</em>, <em>39</em>(8), 936–956. (<a
href="https://doi.org/10.1177/0278364920918014">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Whole-body control (WBC) is a generic task-oriented control method for feedback control of loco-manipulation behaviors in humanoid robots. The combination of WBC and model-based walking controllers has been widely utilized in various humanoid robots. However, to date, the WBC method has not been employed for unsupported passive-ankle dynamic locomotion. As such, in this article, we devise a new WBC, dubbed the whole-body locomotion controller (WBLC), that can achieve experimental dynamic walking on unsupported passive-ankle biped robots. A key aspect of WBLC is the relaxation of contact constraints such that the control commands produce reduced jerk when switching foot contacts. To achieve robust dynamic locomotion, we conduct an in-depth analysis of uncertainty for our dynamic walking algorithm called the time-to-velocity-reversal (TVR) planner. The uncertainty study is fundamental as it allows us to improve the control algorithms and mechanical structure of our robot to fulfill the tolerated uncertainty. In addition, we conduct extensive experimentation for: (1) unsupported dynamic balancing (i.e., in-place stepping) with a six-degree-of-freedom biped, Mercury; (2) unsupported directional walking with Mercury; (3) walking over an irregular and slippery terrain with Mercury; and 4) in-place walking with our newly designed ten-DoF viscoelastic liquid-cooled biped, DRACO. Overall, the main contributions of this work are on: (a) achieving various modalities of unsupported dynamic locomotion of passive-ankle bipeds using a WBLC controller and a TVR planner; (b) conducting an uncertainty analysis to improve the mechanical structure and the controllers of Mercury; and (c) devising a whole-body control strategy that reduces movement jerk during walking.},
  archive  = {J},
  author   = {Donghyun Kim and Steven Jens Jorgensen and Jaemin Lee and Junhyeok Ahn and Jianwen Luo and Luis Sentis},
  doi      = {10.1177/0278364920918014},
  journal  = {The International Journal of Robotics Research},
  month    = {7},
  number   = {8},
  pages    = {936-956},
  title    = {Dynamic locomotion for passive-ankle biped robots and humanoids using whole-body locomotion control},
  volume   = {39},
  year     = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Human motion trajectory prediction: A survey. <em>The
International Journal of Robotics Research</em>, <em>39</em>(8),
895–935. (<a href="https://doi.org/10.1177/0278364920917446">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {With growing numbers of intelligent autonomous systems in human environments, the ability of such systems to perceive, understand, and anticipate human behavior becomes increasingly important. Specifically, predicting future positions of dynamic agents and planning considering such predictions are key tasks for self-driving vehicles, service robots, and advanced surveillance systems. This article provides a survey of human motion trajectory prediction. We review, analyze, and structure a large selection of work from different communities and propose a taxonomy that categorizes existing methods based on the motion modeling approach and level of contextual information used. We provide an overview of the existing datasets and performance metrics. We discuss limitations of the state of the art and outline directions for further research.},
  archive  = {J},
  author   = {Andrey Rudenko and Luigi Palmieri and Michael Herman and Kris M Kitani and Dariu M Gavrila and Kai O Arras},
  doi      = {10.1177/0278364920917446},
  journal  = {The International Journal of Robotics Research},
  month    = {7},
  number   = {8},
  pages    = {895-935},
  title    = {Human motion trajectory prediction: A survey},
  volume   = {39},
  year     = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Distributed multi-robot collision avoidance via deep
reinforcement learning for navigation in complex scenarios. <em>The
International Journal of Robotics Research</em>, <em>39</em>(7),
856–892. (<a href="https://doi.org/10.1177/0278364920916531">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Developing a safe and efficient collision-avoidance policy for multiple robots is challenging in the decentralized scenarios where each robot generates its paths with limited observation of other robots’ states and intentions. Prior distributed multi-robot collision-avoidance systems often require frequent inter-robot communication or agent-level features to plan a local collision-free action, which is not robust and computationally prohibitive. In addition, the performance of these methods is not comparable with their centralized counterparts in practice. In this article, we present a decentralized sensor-level collision-avoidance policy for multi-robot systems, which shows promising results in practical applications. In particular, our policy directly maps raw sensor measurements to an agent’s steering commands in terms of the movement velocity. As a first step toward reducing the performance gap between decentralized and centralized methods, we present a multi-scenario multi-stage training framework to learn an optimal policy. The policy is trained over a large number of robots in rich, complex environments simultaneously using a policy-gradient-based reinforcement-learning algorithm. The learning algorithm is also integrated into a hybrid control framework to further improve the policy’s robustness and effectiveness. We validate the learned sensor-level collision-3avoidance policy in a variety of simulated and real-world scenarios with thorough performance evaluations for large-scale multi-robot systems. The generalization of the learned policy is verified in a set of unseen scenarios including the navigation of a group of heterogeneous robots and a large-scale scenario with 100 robots. Although the policy is trained using simulation data only, we have successfully deployed it on physical robots with shapes and dynamics characteristics that are different from the simulated agents, in order to demonstrate the controller’s robustness against the simulation-to-real modeling error. Finally, we show that the collision-avoidance policy learned from multi-robot navigation tasks provides an excellent solution for safe and effective autonomous navigation for a single robot working in a dense real human crowd. Our learned policy enables a robot to make effective progress in a crowd without getting stuck. More importantly, the policy has been successfully deployed on different types of physical robot platforms without tedious parameter tuning. Videos are available at https://sites.google.com/view/hybridmrca .},
  archive  = {J},
  author   = {Tingxiang Fan and Pinxin Long and Wenxi Liu and Jia Pan},
  doi      = {10.1177/0278364920916531},
  journal  = {The International Journal of Robotics Research},
  month    = {6},
  number   = {7},
  pages    = {856-892},
  title    = {Distributed multi-robot collision avoidance via deep reinforcement learning for navigation in complex scenarios},
  volume   = {39},
  year     = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Adaptive fovea for scanning depth sensors. <em>The
International Journal of Robotics Research</em>, <em>39</em>(7),
837–855. (<a href="https://doi.org/10.1177/0278364920920931">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Depth sensors have been used extensively for perception in robotics. Typically these sensors have a fixed angular resolution and field of view (FOV). This is in contrast to human perception, which involves foveating : scanning with the eyes’ highest angular resolution over regions of interest (ROIs). We build a scanning depth sensor that can control its angular resolution over the FOV. This opens up new directions for robotics research, because many algorithms in localization, mapping, exploration, and manipulation make implicit assumptions about the fixed resolution of a depth sensor, impacting latency, energy efficiency, and accuracy. Our algorithms increase resolution in ROIs either through deconvolutions or intelligent sample distribution across the FOV. The areas of high resolution in the sensor FOV act as artificial fovea and we adaptively vary the fovea locations to maximize a well-known information theoretic measure. We demonstrate novel applications such as adaptive time-of-flight (TOF) sensing, LiDAR zoom, gradient-based LiDAR sensing, and energy-efficient LiDAR scanning. As a proof of concept, we mount the sensor on a ground robot platform, showing how to reduce robot motion to obtain a desired scanning resolution. We also present a ROS wrapper for active simulation for our novel sensor in Gazebo. Finally, we provide extensive empirical analysis of all our algorithms, demonstrating trade-offs between time, resolution and stand-off distance.},
  archive  = {J},
  author   = {Zaid Tasneem and Charuvahan Adhivarahan and Dingkang Wang and Huikai Xie and Karthik Dantu and Sanjeev J Koppal},
  doi      = {10.1177/0278364920920931},
  journal  = {The International Journal of Robotics Research},
  month    = {6},
  number   = {7},
  pages    = {837-855},
  title    = {Adaptive fovea for scanning depth sensors},
  volume   = {39},
  year     = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). STyLuS*: A temporal logic optimal control synthesis
algorithm for large-scale multi-robot systems. <em>The International
Journal of Robotics Research</em>, <em>39</em>(7), 812–836. (<a
href="https://doi.org/10.1177/0278364920913922">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {This article proposes a new highly scalable and asymptotically optimal control synthesis algorithm from linear temporal logic specifications, called STyLu S * for large-Scale optimal Temporal Logic Synthesis, that is designed to solve complex temporal planning problems in large-scale multi-robot systems. Existing planning approaches with temporal logic specifications rely on graph search techniques applied to a product automaton constructed among the robots. In our previous work, we have proposed a more tractable sampling-based algorithm that builds incrementally trees that approximate the state space and transitions of the synchronous product automaton and does not require sophisticated graph search techniques. Here, we extend our previous work by introducing bias in the sampling process that is guided by transitions in the Büchi automaton that belong to the shortest path to the accepting states. This allows us to synthesize optimal motion plans from product automata with hundreds of orders of magnitude more states than those that existing optimal control synthesis methods or off-the-shelf model checkers can manipulate. We show that STyLu S * is probabilistically complete and asymptotically optimal and has exponential convergence rate. This is the first time that convergence rate results are provided for sampling-based optimal control synthesis methods. We provide simulation results that show that STyLu S * can synthesize optimal motion plans for very large multi-robot systems, which is impossible using state-of-the-art methods.},
  archive  = {J},
  author   = {Yiannis Kantaros and Michael M Zavlanos},
  doi      = {10.1177/0278364920913922},
  journal  = {The International Journal of Robotics Research},
  month    = {6},
  number   = {7},
  pages    = {812-836},
  title    = {STyLuS*: A temporal logic optimal control synthesis algorithm for large-scale multi-robot systems},
  volume   = {39},
  year     = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). The effects of reduced-gravity on planetary rover mobility.
<em>The International Journal of Robotics Research</em>, <em>39</em>(7),
797–811. (<a href="https://doi.org/10.1177/0278364920913945">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {One of the major challenges faced by planetary exploration rovers today is the negotiation of difficult terrain, such as fine granular regolith commonly found on the Moon and Mars. Current testing methods on Earth fail to account for the effect of reduced gravity on the soil itself. This work characterizes the effects of reduced gravity on wheel–soil interactions between an ExoMars rover wheel prototype and a martian soil simulant aboard parabolic flights producing effective martian and lunar gravitational accelerations. These experiments are the first to collect wheel–soil interaction imagery and force/torque sensor data alongside wheel sinkage data. Results from reduced-gravity flights are compared with on-ground experiments with all parameters equal, including wheel load, such that the only difference between the experiments is the effect of gravity on the soil itself. In lunar gravity, a statistically significant average reduction in traction of 20% is observed compared with 1 g , and in martian gravity an average traction reduction of 5–10% is observed. Subsurface soil imaging shows that soil mobilization increases as gravity decreases, suggesting a deterioration in soil strength, which could be the cause of the reduction in traction. Statistically significant increases in wheel sinkage in both martian and lunar gravity provide additional evidence for decreased soil strength. All of these observations (decreased traction, increased soil mobilization, and increased sinkage) hinder a rover’s ability to drive, and should be considered when interpreting results from reduced-load mobility tests conducted on Earth.},
  archive  = {J},
  author   = {Parna Niksirat and Adriana Daca and Krzysztof Skonieczny},
  doi      = {10.1177/0278364920913945},
  journal  = {The International Journal of Robotics Research},
  month    = {6},
  number   = {7},
  pages    = {797-811},
  title    = {The effects of reduced-gravity on planetary rover mobility},
  volume   = {39},
  year     = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Control of ATRIAS in three dimensions: Walking as a
forced-oscillation problem. <em>The International Journal of Robotics
Research</em>, <em>39</em>(7), 774–796. (<a
href="https://doi.org/10.1177/0278364920916777">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {In this article, we present a new controller for stable and robust walking control of ATRIAS, an underactuated bipedal robot designed based on the spring-loaded inverted pendulum (SLIP) model. We propose a forced-oscillation scheme for control of vertical motion, which we prove to be stable and contractive. Moreover, we prove that, through some mild assumptions, the dynamics of the system can be written in a hierarchical form that decouples the stability analyses of the horizontal and vertical directions. We leverage these properties to find a stabilizing class of functions for foot placement. The torso control is also proved to be decoupled using singular perturbation theory and is stabilized through a feedback linearization controller. We also take advantage of the proposed framework’s flexibility and extend it to include a new reflex-based uneven-terrain walking control scheme. We test the controller for various desired walking speeds (0 to 2.5 m/s), for stepping up and down unexpected obstacles (15 cm), and for high-speed walking on a random uneven terrain (up to 10 cm of step-ups and step-downs and up to 1.8 m/s). The results show successful performance of the controller and its stability and robustness against various perturbations.},
  archive  = {J},
  author   = {Siavash Rezazadeh and Jonathan W Hurst},
  doi      = {10.1177/0278364920916777},
  journal  = {The International Journal of Robotics Research},
  month    = {6},
  number   = {7},
  pages    = {774-796},
  title    = {Control of ATRIAS in three dimensions: Walking as a forced-oscillation problem},
  volume   = {39},
  year     = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Reactive planar non-prehensile manipulation with hybrid
model predictive control. <em>The International Journal of Robotics
Research</em>, <em>39</em>(7), 755–773. (<a
href="https://doi.org/10.1177/0278364920913938">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {This article presents an offline solution and online approximation to the hybrid control problem of planar non-prehensile manipulation. Hybrid dynamics and underactuation are key characteristics of this task that complicate the design of feedback controllers. We show that a model predictive control approach used in tandem with integer programming offers a powerful solution to capture the dynamic constraints associated with the friction cone as well as the hybrid nature of contact. We introduce the Model Predictive Controller with Learned Mode Scheduling (MPC-LMS), which leverages integer programming and machine learning techniques to effectively deal with the combinatorial complexity associated with determining sequences of contact modes. We validate the controller design through a numerical simulation study and with experiments on a planar manipulation setup using an industrial ABB IRB 120 robotic arm. Results show that the proposed algorithm achieves closed-loop tracking of a nominal trajectory by reasoning in real-time across multiple contact modalities.},
  archive  = {J},
  author   = {Francois R Hogan and Alberto Rodriguez},
  doi      = {10.1177/0278364920913938},
  journal  = {The International Journal of Robotics Research},
  month    = {6},
  number   = {7},
  pages    = {755-773},
  title    = {Reactive planar non-prehensile manipulation with hybrid model predictive control},
  volume   = {39},
  year     = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Underwater pre-touch based on artificial electric sense.
<em>The International Journal of Robotics Research</em>, <em>39</em>(6),
729–752. (<a href="https://doi.org/10.1177/0278364920903776">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {This article exploits a bio-inspired sensor technology named artificial electric-sense to emulate underwater pre-touch. The sensor is considered as an electric finger controlled remotely by an operator to follow the boundaries of objects. Using electric measurements only, the approach feeds back pre-touch forces and torques to the operator through an haptic interface. These forces and torques are generated by a set of virtual electric charges and dipoles arranged on the probe and reacting in the electric field reflected by the objects. This model of emulated forces is passive and guarantees the stability of a position–position haptic feedback loop. The whole approach is assessed through a set of experiments carried out on a Cartesian slave robot coupled to an haptic interface. The obtained results show the feasibility of the concept and its robustness to different configurations of objects. Such an electro-haptic feedback opens new perspectives in both electric field sensing and underwater robotics.},
  archive  = {J},
  author   = {Frédéric Boyer and Vincent Lebastard and Steven Bruce Ferrer and Franck Geffard},
  doi      = {10.1177/0278364920903776},
  journal  = {The International Journal of Robotics Research},
  month    = {5},
  number   = {6},
  pages    = {729-752},
  title    = {Underwater pre-touch based on artificial electric sense},
  volume   = {39},
  year     = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Simultaneous contact and aerodynamic force estimation
(s-CAFE) for aerial robots. <em>The International Journal of Robotics
Research</em>, <em>39</em>(6), 688–728. (<a
href="https://doi.org/10.1177/0278364920904788">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {In this article, we consider the problem of multirotor flying robots physically interacting with the environment under influence of wind. The results are the first algorithms for simultaneous online estimation of contact and aerodynamic wrenches acting on the robot based on real-world data, without the need for dedicated sensors. For this purpose, we investigated two model-based techniques for discriminating between aerodynamic and interaction forces. The first technique is based on aerodynamic and contact torque models, and uses the external force to estimate wind speed. Contacts are then detected based on the residual between estimated external torque and expected (modeled) aerodynamic torque. Upon detecting contact, wind speed is assumed to change very slowly. From the estimated interaction wrench, we are also able to determine the contact location. This is embedded into a particle filter framework to further improve contact location estimation. The second algorithm uses the propeller aerodynamic power and angular speed as measured by the speed controllers to obtain an estimate of the airspeed. An aerodynamics model is then used to determine the aerodynamic wrench. Both methods rely on accurate aerodynamics models. Therefore, we evaluate data-driven and physics-based models as well as offline system identification for flying robots. For obtaining ground-truth data, we performed autonomous flights in a 3D wind tunnel. Using this data, aerodynamic model selection, parameter identification, and discrimination between aerodynamic and contact forces could be performed. Finally, the developed methods could serve as useful estimators for interaction control schemes with simultaneous compensation of wind disturbances.},
  archive  = {J},
  author   = {Teodor Tomić and Philipp Lutz and Korbinian Schmid and Andrew Mathers and Sami Haddadin},
  doi      = {10.1177/0278364920904788},
  journal  = {The International Journal of Robotics Research},
  month    = {5},
  number   = {6},
  pages    = {688-728},
  title    = {Simultaneous contact and aerodynamic force estimation (s-CAFE) for aerial robots},
  volume   = {39},
  year     = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Pressure distribution classification and segmentation of
human hands in contact with the robot body. <em>The International
Journal of Robotics Research</em>, <em>39</em>(6), 668–687. (<a
href="https://doi.org/10.1177/0278364920907688">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {This article deals with the problem of the recognition of human hand touch by a robot equipped with large area tactile sensors covering its body. This problem is relevant in the domain of physical human–robot interaction for discriminating between human and non-human contacts and to trigger and to drive cooperative tasks or robot motions, or to ensure a safe interaction. The underlying assumption used in this article is that voluntary physical interaction tasks involve hand touch over the robot body, and therefore the capability to recognize hand contacts is a key element to discriminate a purposive human touch from other types of interaction. The proposed approach is based on a geometric transformation of the tactile data, formed by pressure measurements associated to a non-uniform cloud of 3D points ( taxels ) spread over a non-linear manifold corresponding to the robot body, into tactile images representing the contact pressure distribution in two dimensions. Tactile images can be processed using deep learning algorithms to recognize human hands and to compute the pressure distribution applied by the various hand segments: palm and single fingers. Experimental results, performed on a real robot covered with robot skin, show the effectiveness of the proposed methodology. Moreover, to evaluate its robustness, various types of failures have been simulated. A further analysis concerning the transferability of the system has been performed, considering contacts occurring on a different sensorized robot part.},
  archive  = {J},
  author   = {Alessandro Albini and Giorgio Cannata},
  doi      = {10.1177/0278364920907688},
  journal  = {The International Journal of Robotics Research},
  month    = {5},
  number   = {6},
  pages    = {668-687},
  title    = {Pressure distribution classification and segmentation of human hands in contact with the robot body},
  volume   = {39},
  year     = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Improving user specifications for robot behavior through
active preference learning: Framework and evaluation. <em>The
International Journal of Robotics Research</em>, <em>39</em>(6),
651–667. (<a href="https://doi.org/10.1177/0278364920910802">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {An important challenge in human–robot interaction (HRI) is enabling non-expert users to specify complex tasks for autonomous robots. Recently, active preference learning has been applied in HRI to interactively shape a robot’s behavior. We study a framework where users specify constraints on allowable robot movements on a graphical interface, yielding a robot task specification. However, users may not be able to accurately assess the impact of such constraints on the performance of a robot. Thus, we revise the specification by iteratively presenting users with alternative solutions where some constraints might be violated, and learn about the importance of the constraints from the users’ choices between these alternatives. We demonstrate our framework in a user study with a material transport task in an industrial facility. We show that nearly all users accept alternative solutions and thus obtain a revised specification through the learning process, and that the revision leads to a substantial improvement in robot performance. Further, the learning process reduces the variances between the specifications from different users and, thus, makes the specifications more similar. As a result, the users whose initial specifications had the largest impact on performance benefit the most from the interactive learning.},
  archive  = {J},
  author   = {Nils Wilde and Alexandru Blidaru and Stephen L Smith and Dana Kulić},
  doi      = {10.1177/0278364920910802},
  journal  = {The International Journal of Robotics Research},
  month    = {5},
  number   = {6},
  pages    = {651-667},
  title    = {Improving user specifications for robot behavior through active preference learning: Framework and evaluation},
  volume   = {39},
  year     = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). The canadian planetary emulation terrain energy-aware rover
navigation dataset. <em>The International Journal of Robotics
Research</em>, <em>39</em>(6), 641–650. (<a
href="https://doi.org/10.1177/0278364920908922">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Future exploratory missions to the Moon and to Mars will involve solar-powered rovers; careful vehicle energy management is critical to the success of such missions. This article describes a unique dataset gathered by a small, four-wheeled rover at a planetary analog test facility in Canada. The rover was equipped with a suite of sensors designed to enable the study of energy-aware navigation and path planning algorithms. The sensors included a colour omnidirectional stereo camera, a monocular camera, an inertial measurement unit, a pyranometer, drive power consumption monitors, wheel encoders, and a GPS receiver. In total, the rover drove more than 1.2 km over varied terrain at the analog test site. All data is presented in human-readable text files and as standard-format images; additional Robot Operating System (ROS) parsing tools and several georeferenced aerial maps of the test environment are also included. A series of potential research use cases is described.},
  archive  = {J},
  author   = {Olivier Lamarre and Oliver Limoyo and Filip Marić and Jonathan Kelly},
  doi      = {10.1177/0278364920908922},
  journal  = {The International Journal of Robotics Research},
  month    = {5},
  number   = {6},
  pages    = {641-650},
  title    = {The canadian planetary emulation terrain energy-aware rover navigation dataset},
  volume   = {39},
  year     = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Controlling two-dimensional collective formation and
cooperative behavior of magnetic microrobot swarms. <em>The
International Journal of Robotics Research</em>, <em>39</em>(5),
617–638. (<a href="https://doi.org/10.1177/0278364920903107">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Magnetically actuated mobile microrobots can access distant, enclosed, and small spaces, such as inside microfluidic channels and the human body, making them appealing for minimally invasive tasks. Despite their simplicity when scaling down, creating collective microrobots that can work closely and cooperatively, as well as reconfigure their formations for different tasks, would significantly enhance their capabilities such as manipulation of objects. However, a challenge of realizing such cooperative magnetic microrobots is to program and reconfigure their formations and collective motions with under-actuated control signals. This article presents a method of controlling 2D static and time-varying formations among collective self-repelling ferromagnetic microrobots (100 μ m to 350 μ m in diameter, up to 260 in number) by spatially and temporally programming an external magnetic potential energy distribution at the air–water interface or on solid surfaces. A general design method is introduced to program external magnetic potential energy using ferromagnets. A predictive model of the collective system is also presented to predict the formation and guide the design procedure. With the proposed method, versatile complex static formations are experimentally demonstrated and the programmability and scaling effects of formations are analyzed. We also demonstrate the collective mobility of these magnetic microrobots by controlling them to exhibit bio-inspired collective behaviors such as aggregation, directional motion with arbitrary swarm headings, and rotational swarming motion. Finally, the functions of the produced microrobotic swarm are demonstrated by controlling them to navigate through cluttered environments and complete reconfigurable cooperative manipulation tasks.},
  archive  = {J},
  author   = {Xiaoguang Dong and Metin Sitti},
  doi      = {10.1177/0278364920903107},
  journal  = {The International Journal of Robotics Research},
  month    = {4},
  number   = {5},
  pages    = {617-638},
  title    = {Controlling two-dimensional collective formation and cooperative behavior of magnetic microrobot swarms},
  volume   = {39},
  year     = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Locomotion of a multi-link non-holonomic snake robot with
passive joints. <em>The International Journal of Robotics Research</em>,
<em>39</em>(5), 598–616. (<a
href="https://doi.org/10.1177/0278364919898503">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Conventional approaches in prescribing controls for locomoting robots assume control over all input degrees of freedom (DOFs). Many robots, such as those with non-holonomic constraints, may not require or even allow for direct command over all DOFs. In particular, a snake robot with more than three links with non-holonomic constraints cannot achieve arbitrary configurations in all of its joints while simultaneously locomoting. For such a system, we assume partial command over a subset of the joints, and allow the rest to evolve according to kinematic chained and dynamic models. Different combinations of actuated and passive joints, as well as joints with dynamic elements such as torsional springs, can drastically change the coupling interactions and stable oscillations of joints. We use tools from nonlinear analysis to understand emergent oscillation modes of various robot configurations and connect them to overall locomotion using geometric mechanics and feedback control for robots that may not fully utilize all available inputs. We also experimentally verify observations and motion planning results on a physical non-holonomic snake robot.},
  archive  = {J},
  author   = {Tony Dear and Blake Buchanan and Rodrigo Abrajan-Guerrero and Scott David Kelly and Matthew Travers and Howie Choset},
  doi      = {10.1177/0278364919898503},
  journal  = {The International Journal of Robotics Research},
  month    = {4},
  number   = {5},
  pages    = {598-616},
  title    = {Locomotion of a multi-link non-holonomic snake robot with passive joints},
  volume   = {39},
  year     = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Automatically steering cardiac catheters in vivo with
respiratory motion compensation. <em>The International Journal of
Robotics Research</em>, <em>39</em>(5), 586–597. (<a
href="https://doi.org/10.1177/0278364920903785">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {A robotic system for automatically navigating ultrasound (US) imaging catheters can provide real-time intra-cardiac imaging for diagnosis and treatment while reducing the need for clinicians to perform manual catheter steering. Clinical deployment of such a system requires accurate navigation despite the presence of disturbances including cyclical physiological motions (e.g., respiration). In this work, we report results from in vivo trials of automatic target tracking using our system, which is the first to navigate cardiac catheters with respiratory motion compensation. The effects of respiratory disturbances on the US catheter are modeled and then applied to four-degree-of-freedom steering kinematics with predictive filtering. This enables the system to accurately steer the US catheter and aim the US imager at a target despite respiratory motion disturbance. In vivo animal respiratory motion compensation results demonstrate automatic US catheter steering to image a target ablation catheter with 1.05 mm and 1.33° mean absolute error. Robotic US catheter steering with motion compensation can improve cardiac catheterization techniques while reducing clinician effort and X-ray exposure.},
  archive  = {J},
  author   = {Paul M Loschak and Alperen Degirmenci and Cory M Tschabrunn and Elad Anter and Robert D Howe},
  doi      = {10.1177/0278364920903785},
  journal  = {The International Journal of Robotics Research},
  month    = {4},
  number   = {5},
  pages    = {586-597},
  title    = {Automatically steering cardiac catheters in vivo with respiratory motion compensation},
  volume   = {39},
  year     = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Human motion analysis in medical robotics via
high-dimensional inverse reinforcement learning. <em>The International
Journal of Robotics Research</em>, <em>39</em>(5), 568–585. (<a
href="https://doi.org/10.1177/0278364920903104">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {This work develops a novel high-dimensional inverse reinforcement learning (IRL) algorithm for human motion analysis in medical, clinical, and robotics applications. The method is based on the assumption that a surgical robot operators’ skill or a patient’s motor skill is encoded into the innate reward function during motion planning and recovered by an IRL algorithm from motion demonstrations. This class of applications is characterized by high-dimensional sensory data, which is computationally prohibitive for most existing IRL algorithms. We propose a novel function approximation framework and reformulate the Bellman optimality equation to handle high-dimensional state spaces efficiently. We compare different function approximators in simulated environments, and adopt a deep neural network as the function approximator. The technique is applied to evaluating human patients with spinal cord injuries under spinal stimulation, and the skill levels of surgical robot operators. The results demonstrate the efficiency and effectiveness of the proposed method.},
  archive  = {J},
  author   = {Kun Li and Joel W Burdick},
  doi      = {10.1177/0278364920903104},
  journal  = {The International Journal of Robotics Research},
  month    = {4},
  number   = {5},
  pages    = {568-585},
  title    = {Human motion analysis in medical robotics via high-dimensional inverse reinforcement learning},
  volume   = {39},
  year     = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Batch informed trees (BIT*): Informed asymptotically optimal
anytime search. <em>The International Journal of Robotics Research</em>,
<em>39</em>(5), 543–567. (<a
href="https://doi.org/10.1177/0278364919890396">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Path planning in robotics often requires finding high-quality solutions to continuously valued and/or high-dimensional problems. These problems are challenging and most planning algorithms instead solve simplified approximations. Popular approximations include graphs and random samples, as used by informed graph-based searches and anytime sampling-based planners, respectively. Informed graph-based searches, such as A * , traditionally use heuristics to search a priori graphs in order of potential solution quality. This makes their search efficient, but leaves their performance dependent on the chosen approximation. If the resolution of the chosen approximation is too low, then they may not find a (suitable) solution, but if it is too high, then they may take a prohibitively long time to do so. Anytime sampling-based planners, such as RRT * , traditionally use random sampling to approximate the problem domain incrementally. This allows them to increase resolution until a suitable solution is found, but makes their search dependent on the order of approximation. Arbitrary sequences of random samples approximate the problem domain in every direction simultaneously, but may be prohibitively inefficient at containing a solution. This article unifies and extends these two approaches to develop Batch Informed Trees (BIT*), an informed, anytime sampling-based planner. BIT * solves continuous path planning problems efficiently by using sampling and heuristics to alternately approximate and search the problem domain. Its search is ordered by potential solution quality, as in A * , and its approximation improves indefinitely with additional computational time, as in RRT * . It is shown analytically to be almost-surely asymptotically optimal and experimentally to outperform existing sampling-based planners, especially on high-dimensional planning problems.},
  archive  = {J},
  author   = {Jonathan D Gammell and Timothy D Barfoot and Siddhartha S Srinivasa},
  doi      = {10.1177/0278364919890396},
  journal  = {The International Journal of Robotics Research},
  month    = {4},
  number   = {5},
  pages    = {543-567},
  title    = {Batch informed trees (BIT*): Informed asymptotically optimal anytime search},
  volume   = {39},
  year     = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Unifying bilateral teleoperation and tele-impedance for
enhanced user experience. <em>The International Journal of Robotics
Research</em>, <em>39</em>(4), 514–539. (<a
href="https://doi.org/10.1177/0278364919891773">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Usability is one of the most important aspects of teleoperation. Ideally, the operator’s experience should be one of complete command over the remote environment, but also be as close as possible to what they would have if physically present at the remote end, i.e., transparency in terms of both action and perception. These two aspects may coincide in favorable conditions, where classic approaches such as the four-channel architecture ensures transparency of the control framework. In the presence of substantial delays between the user and the slave, however, the stability–performance trade-off inherent to bilateral teleoperation deteriorates not only transparency, but also command. An alternative, unilateral approach is given by tele-impedance, which controls the slave–environment interaction by measuring and remotely replicating the user’s limb endpoint position and impedance. Not including force feedback to the operator, tele-impedance is absolutely robust to delays, whereas it completely lacks transparency. This article introduces a novel control framework that integrates a new, fully transparent, two-channel bilateral architecture with the tele-impedance paradigm. The result is a unified solution that mitigates problems of classical approaches, and provides the user with additional tools to modulate the slave robot’s physical interaction behavior, resulting in a better operator experience in spite of time inconsistencies. The validity and effectiveness of the proposed solution is demonstrated in terms of performance in the interaction tasks, of user fatigue and overall experience.},
  archive  = {J},
  author   = {Marco Laghi and Arash Ajoudani and Manuel G. Catalano and Antonio Bicchi},
  doi      = {10.1177/0278364919891773},
  journal  = {The International Journal of Robotics Research},
  month    = {3},
  number   = {4},
  pages    = {514-539},
  title    = {Unifying bilateral teleoperation and tele-impedance for enhanced user experience},
  volume   = {39},
  year     = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Model-based dynamic feedback control of a planar soft robot:
Trajectory tracking and interaction with the environment. <em>The
International Journal of Robotics Research</em>, <em>39</em>(4),
490–513. (<a href="https://doi.org/10.1177/0278364919897292">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Leveraging the elastic bodies of soft robots promises to enable the execution of dynamic motions as well as compliant and safe interaction with an unstructured environment. However, the exploitation of these abilities is constrained by the lack of appropriate control strategies. This work tackles for the first time the development of closed-loop dynamic controllers for a continuous soft robot. We present two architectures designed for dynamic trajectory tracking and surface following, respectively. Both controllers are designed to preserve the natural softness of the robot and adapt to interactions with an unstructured environment. The validity of the controllers is proven analytically within the hypotheses of the model. The controllers are evaluated through an extensive series of simulations, and through experiments on a physical soft robot capable of planar motions.},
  archive  = {J},
  author   = {Cosimo Della Santina and Robert K Katzschmann and Antonio Bicchi and Daniela Rus},
  doi      = {10.1177/0278364919897292},
  journal  = {The International Journal of Robotics Research},
  month    = {3},
  number   = {4},
  pages    = {490-513},
  title    = {Model-based dynamic feedback control of a planar soft robot: Trajectory tracking and interaction with the environment},
  volume   = {39},
  year     = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Local flow sensing on helical microrobots for semi-automatic
motion adaptation. <em>The International Journal of Robotics
Research</em>, <em>39</em>(4), 476–489. (<a
href="https://doi.org/10.1177/0278364919894374">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Helical microrobots with dimensions below 100 µm could serve many applications for manipulation and sensing in small, closed environments such as blood vessels or inside microfluidic chips. However, environmental conditions such as surface stiction from the channel wall or local flow can quickly result in the loss of control of the microrobot, especially for untrained users. Therefore, to automatically adapt to changing conditions, we propose an algorithm that switches between a surface-based motion of the microrobot and a 3D swimming motion depending on the local flow value. Indeed swimming is better for avoiding obstacles and difficult surface stiction areas but it is more sensitive to the flow than surface motion such as rolling or spintop motion. First, we prove the flow sensing ability of helical microrobots based on the difference between the tracked and theoretical speed. For this, a 50 µm long and 5 µm diameter helical microrobot measures the flow profile shape in two different microchannels. These measurements are then compared with simulation results. Then, we demonstrate both swimming and surface-based motion using closed-loop control. Finally, we test our algorithm by following a 2D path using closed-loop control, and adapting the type of motion depending on the flow speed measured by the microrobot. Such results could enable simple high-level control that could expand the development of microrobots toward applications in complex microfluidic environments.},
  archive  = {J},
  author   = {Antoine Barbot and Dominique Decanini and Gilgueng Hwang},
  doi      = {10.1177/0278364919894374},
  journal  = {The International Journal of Robotics Research},
  month    = {3},
  number   = {4},
  pages    = {476-489},
  title    = {Local flow sensing on helical microrobots for semi-automatic motion adaptation},
  volume   = {39},
  year     = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). On the mechanical contribution of head stabilization to
passive dynamics of anthropometric walkers. <em>The International
Journal of Robotics Research</em>, <em>39</em>(4), 461–475. (<a
href="https://doi.org/10.1177/0278364919894387">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {During the steady gait, humans stabilize their head around the vertical orientation. Although there are sensori-cognitive explanations for this phenomenon, its mechanical effect on the body dynamics remains unexplored. In this study, we take profit from the similarities that human steady gait shares with the locomotion of passive-dynamics robots. We introduce a simplified anthropometric 2D model to reproduce a broad walking dynamics. In a previous study, we showed heuristically that the presence of a stabilized head–neck system has a significant influence on the dynamics of walking. This article gives new insights that lead to understanding this mechanical effect. In particular, we introduce an original cart upper-body model that allows to better understand the mechanical interest of head stabilization when walking, and we study how this effect is sensitive to the choice of control parameters.},
  archive  = {J},
  author   = {Mehdi Benallegue and Jean-Paul Laumond and Alain Berthoz},
  doi      = {10.1177/0278364919894387},
  journal  = {The International Journal of Robotics Research},
  month    = {3},
  number   = {4},
  pages    = {461-475},
  title    = {On the mechanical contribution of head stabilization to passive dynamics of anthropometric walkers},
  volume   = {39},
  year     = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Placing and scheduling many depth sensors for wide coverage
and efficient mapping in versatile legged robots. <em>The International
Journal of Robotics Research</em>, <em>39</em>(4), 431–460. (<a
href="https://doi.org/10.1177/0278364919891776">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {This article tackles the problem of designing 3D perception systems for robots with high visual requirements, such as versatile legged robots capable of different locomotion styles. In order to guarantee high visual coverage in varied conditions (e.g., biped walking, quadruped walking, ladder climbing), such robots need to be equipped with a large number of sensors, while at the same time managing the computational requirements that arise from such a system. We tackle this problem at both levels: sensor placement (how many sensors to install on the robot and where) and run-time acquisition scheduling under computational constraints (not all sensors can be acquired and processed at the same time). Our first contribution is a methodology for designing perception systems with a large number of depth sensors scattered throughout the links of a robot, using multi-objective optimization for optimal trade-offs between visual coverage and the number of sensors. We estimate the Pareto front of these objectives through evolutionary optimization, and implement a solution on a real legged robot. Our formulation includes constraints on task-specific coverage and design symmetry, which lead to reliable coverage and fast convergence of the optimization problem. Our second contribution is an algorithm for lowering the computational burden of mapping with such a high number of sensors, formulated as an information-maximization problem with several sampling techniques for speed. Our final system uses 20 depth sensors scattered throughout the robot, which can either be acquired simultaneously or optimally scheduled for low CPU usage while maximizing mapping quality. We show that, when compared with state-of-the-art robotic platforms, our system has higher coverage across a higher number of tasks, thus being suitable for challenging environments and versatile robots. We also demonstrate that our scheduling algorithm allows higher mapping performance to be obtained than with naïve and state-of-the-art methods by leveraging on measures of information gain and self-occlusion at low computational costs.},
  archive  = {J},
  author   = {Martim Brandão and Rui Figueiredo and Kazuki Takagi and Alexandre Bernardino and Kenji Hashimoto and Atsuo Takanishi},
  doi      = {10.1177/0278364919891776},
  journal  = {The International Journal of Robotics Research},
  month    = {3},
  number   = {4},
  pages    = {431-460},
  title    = {Placing and scheduling many depth sensors for wide coverage and efficient mapping in versatile legged robots},
  volume   = {39},
  year     = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Contact-aided invariant extended kalman filtering for robot
state estimation. <em>The International Journal of Robotics
Research</em>, <em>39</em>(4), 402–430. (<a
href="https://doi.org/10.1177/0278364919894385">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Legged robots require knowledge of pose and velocity in order to maintain stability and execute walking paths. Current solutions either rely on vision data, which is susceptible to environmental and lighting conditions, or fusion of kinematic and contact data with measurements from an inertial measurement unit (IMU). In this work, we develop a contact-aided invariant extended Kalman filter (InEKF) using the theory of Lie groups and invariant observer design. This filter combines contact-inertial dynamics with forward kinematic corrections to estimate pose and velocity along with all current contact points. We show that the error dynamics follows a log-linear autonomous differential equation with several important consequences: (a) the observable state variables can be rendered convergent with a domain of attraction that is independent of the system’s trajectory; (b) unlike the standard EKF, neither the linearized error dynamics nor the linearized observation model depend on the current state estimate, which (c) leads to improved convergence properties and (d) a local observability matrix that is consistent with the underlying nonlinear system. Furthermore, we demonstrate how to include IMU biases, add/remove contacts, and formulate both world-centric and robo-centric versions. We compare the convergence of the proposed InEKF with the commonly used quaternion-based extended Kalman filter (EKF) through both simulations and experiments on a Cassie-series bipedal robot. Filter accuracy is analyzed using motion capture, while a LiDAR mapping experiment provides a practical use case. Overall, the developed contact-aided InEKF provides better performance in comparison with the quaternion-based EKF as a result of exploiting symmetries present in system.},
  archive  = {J},
  author   = {Ross Hartley and Maani Ghaffari and Ryan M Eustice and Jessy W Grizzle},
  doi      = {10.1177/0278364919894385},
  journal  = {The International Journal of Robotics Research},
  month    = {3},
  number   = {4},
  pages    = {402-430},
  title    = {Contact-aided invariant extended kalman filtering for robot state estimation},
  volume   = {39},
  year     = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Approximation algorithms for tours of orientation-varying
view cones. <em>The International Journal of Robotics Research</em>,
<em>39</em>(4), 389–401. (<a
href="https://doi.org/10.1177/0278364919893455">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {This article considers the problem of finding a shortest tour to visit viewing sets of points on a plane. Each viewing set is represented as an inverted view cone with apex angle α and height h . The apex of each cone is restricted to lie on the ground plane. Its orientation angle (tilt) ϵ is the angle difference between the cone bisector and the ground plane normal. This is a novel variant of the 3D Traveling Salesman Problem with Neighborhoods (TSPN) called Cone-TSPN. One application of Cone-TSPN is to compute a trajectory to observe a given set of locations with a camera: for each location, we can generate a set of cones whose apex and orientation angles α and ϵ correspond to the camera’s field of view and tilt. The height of each cone h corresponds to the desired resolution. Recently, Plonski and Isler presented an approximation algorithm for Cone-TSPN for the case where all cones have a uniform orientation angle of ϵ = 0 . We study a new variant of Cone-TSPN where we relax this constraint and allow the cones to have non-uniform orientations. We call this problem Tilted Cone-TSPN and present a polynomial-time approximation algorithm with ratio O ( 1 + tan α 1 − tan ϵ tan α ( 1 + log max ( H ) min ( H ) ) ) , where H is the set of all cone heights. We demonstrate through simulations that our algorithm can be implemented in a practical way and that by exploiting the structure of the cones we can achieve shorter tours. Finally, we present experimental results from various agriculture applications that show the benefit of considering view angles for path planning.},
  archive  = {J},
  author   = {Nikolaos Stefas and Patrick A Plonski and Volkan Isler},
  doi      = {10.1177/0278364919893455},
  journal  = {The International Journal of Robotics Research},
  month    = {3},
  number   = {4},
  pages    = {389-401},
  title    = {Approximation algorithms for tours of orientation-varying view cones},
  volume   = {39},
  year     = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A scalable motion planner for high-dimensional kinematic
systems. <em>The International Journal of Robotics Research</em>,
<em>39</em>(4), 361–388. (<a
href="https://doi.org/10.1177/0278364919890408">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Sampling-based algorithms are known for their ability to effectively compute paths for high-dimensional robots in relatively short times. The same algorithms, however, are also notorious for poor-quality solution paths, particularly as the dimensionality of the system grows. This work proposes a new probabilistically complete sampling-based algorithm, XXL, specially designed to plan the motions of high-dimensional mobile manipulators and related platforms. Using a novel sampling and connection strategy that guides a set of points mapped on the robot through the workspace, XXL scales to realistic manipulator platforms with dozens of joints by focusing the search of the robot’s configuration space to specific degrees of freedom that affect motion in particular portions of the workspace. Simulated planning scenarios with the Robonaut2 platform and planar kinematic chains confirm that XXL exhibits competitive solution times relative to many existing works while obtaining execution-quality solution paths. Solutions from XXL are of comparable quality to cost-aware methods even though XXL does not explicitly optimize over any particular criteria, and are computed in an order of magnitude less time. Furthermore, observations about the performance of sampling-based algorithms on high-dimensional manipulator planning problems are presented that reveal a cautionary tale regarding two popular guiding heuristics used in these algorithms, indicating that a nearly random search may outperform the state-of-the-art when defining such heuristics is known to be difficult.},
  archive  = {J},
  author   = {Ryan Luna and Mark Moll and Julia Badger and Lydia E Kavraki},
  doi      = {10.1177/0278364919890408},
  journal  = {The International Journal of Robotics Research},
  month    = {3},
  number   = {4},
  pages    = {361-388},
  title    = {A scalable motion planner for high-dimensional kinematic systems},
  volume   = {39},
  year     = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Reviewer list. <em>The International Journal of Robotics
Research</em>, <em>39</em>(2-3), 356–358. (<a
href="https://doi.org/10.1177/0278364920905211">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive = {J},
  doi     = {10.1177/0278364920905211},
  journal = {The International Journal of Robotics Research},
  month   = {3},
  number  = {2-3},
  pages   = {356-358},
  title   = {Reviewer list},
  volume  = {39},
  year    = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). SegMap: Segment-based mapping and localization using
data-driven descriptors. <em>The International Journal of Robotics
Research</em>, <em>39</em>(2-3), 339–355. (<a
href="https://doi.org/10.1177/0278364919863090">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Precisely estimating a robot’s pose in a prior, global map is a fundamental capability for mobile robotics, e.g., autonomous driving or exploration in disaster zones. This task, however, remains challenging in unstructured, dynamic environments, where local features are not discriminative enough and global scene descriptors only provide coarse information. We therefore present SegMap : a map representation solution for localization and mapping based on the extraction of segments in 3D point clouds. Working at the level of segments offers increased invariance to view-point and local structural changes, and facilitates real-time processing of large-scale 3D data. SegMap exploits a single compact data-driven descriptor for performing multiple tasks: global localization, 3D dense map reconstruction, and semantic information extraction. The performance of SegMap is evaluated in multiple urban driving and search and rescue experiments. We show that the learned SegMap descriptor has superior segment retrieval capabilities, compared with state-of-the-art handcrafted descriptors. As a consequence, we achieve a higher localization accuracy and a 6% increase in recall over state-of-the-art handcrafted descriptors. These segment-based localizations allow us to reduce the open-loop odometry drift by up to 50%. SegMap is open-source available along with easy to run demonstrations.},
  archive  = {J},
  author   = {Renaud Dubé and Andrei Cramariuc and Daniel Dugas and Hannes Sommer and Marcin Dymczyk and Juan Nieto and Roland Siegwart and Cesar Cadena},
  doi      = {10.1177/0278364919863090},
  journal  = {The International Journal of Robotics Research},
  month    = {3},
  number   = {2-3},
  pages    = {339-355},
  title    = {SegMap: Segment-based mapping and localization using data-driven descriptors},
  volume   = {39},
  year     = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Adaptive bias and attitude observer on the special
orthogonal group for true-north gyrocompass systems: Theory and
preliminary results. <em>The International Journal of Robotics
Research</em>, <em>39</em>(2-3), 321–338. (<a
href="https://doi.org/10.1177/0278364919881689">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {This article reports an adaptive sensor bias observer and attitude observer operating directly on SO ( 3 ) for true-north gyrocompass systems that utilize six-degree-of-freedom inertial measurement units (IMUs) with three-axis accelerometers and three-axis angular rate gyroscopes (without magnetometers). Most present-day low-cost robotic vehicles employ attitude estimation systems that employ microelectromechanical system (MEMS) magnetometers, angular rate gyros, and accelerometers to estimate magnetic attitude (roll, pitch, and magnetic heading) with limited heading accuracy. Present-day MEMS gyros are not sensitive enough to dynamically detect the Earth’s rotation, and thus cannot be used to estimate true-north geodetic heading. Relying on magnetic compasses can be problematic for vehicles that operate in environments with magnetic anomalies and those requiring high-accuracy navigation as the limited accuracy ( &gt; 1 ° error) of magnetic compasses is typically the largest error source in underwater vehicle navigation systems. Moreover, magnetic compasses need to undergo time-consuming recalibration for hard-iron and soft-iron errors every time a vehicle is reconfigured with a new instrument or other payload, as very frequently occurs on oceanographic marine vehicles. In contrast, the gyrocompass system reported herein utilizes fiber optic gyroscope (FOG) IMU angular rate gyro and MEMS accelerometer measurements (without magnetometers) to dynamically estimate the instrument’s time-varying true-north attitude (roll, pitch, and geodetic heading) in real-time while the instrument is subject to a priori unknown rotations. This gyrocompass system is immune to magnetic anomalies and does not require recalibration every time a new payload is added to or removed from the vehicle. Stability proofs for the reported bias and attitude observers, preliminary simulations, and a full-scale vehicle trial are reported that suggest the viability of the true-north gyrocompass system to provide dynamic real-time true-north heading, pitch, and roll utilizing a comparatively low-cost FOG IMU.},
  archive  = {J},
  author   = {Andrew R Spielvogel and Louis L Whitcomb},
  doi      = {10.1177/0278364919881689},
  journal  = {The International Journal of Robotics Research},
  month    = {3},
  number   = {2-3},
  pages    = {321-338},
  title    = {Adaptive bias and attitude observer on the special orthogonal group for true-north gyrocompass systems: Theory and preliminary results},
  volume   = {39},
  year     = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Trajectory optimization on manifolds with applications to
quadrotor systems. <em>The International Journal of Robotics
Research</em>, <em>39</em>(2-3), 303–320. (<a
href="https://doi.org/10.1177/0278364919891775">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Manifolds are used in almost all robotics applications even if they are not modeled explicitly. We propose a differential geometric approach for optimizing trajectories on a Riemannian manifold with obstacles. The optimization problem depends on a metric and collision function specific to a manifold. We then propose our safe corridor on manifolds (SCM) method of computationally optimizing trajectories for robotics applications via a constrained optimization problem. Our method does not need equality constraints, which eliminates the need to project back to a feasible manifold during optimization. We then demonstrate how this algorithm works on an example problem on SO ( 3 ) and a perception-aware planning example for visual–inertially guided robots navigating in three dimensions. Formulating field of view constraints naturally results in modeling with the manifold R 3 × S 2 , which cannot be modeled as a Lie group. We also demonstrate the example of planning trajectories on SE ( 3 ) for a formation of quadrotors within an obstacle filled environment.},
  archive  = {J},
  author   = {Michael Watterson and Sikang Liu and Ke Sun and Trey Smith and Vijay Kumar},
  doi      = {10.1177/0278364919891775},
  journal  = {The International Journal of Robotics Research},
  month    = {3},
  number   = {2-3},
  pages    = {303-320},
  title    = {Trajectory optimization on manifolds with applications to quadrotor systems},
  volume   = {39},
  year     = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Imitation learning for agile autonomous driving. <em>The
International Journal of Robotics Research</em>, <em>39</em>(2-3),
286–302. (<a href="https://doi.org/10.1177/0278364919880273">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {We present an end-to-end imitation learning system for agile, off-road autonomous driving using only low-cost on-board sensors. By imitating a model predictive controller equipped with advanced sensors, we train a deep neural network control policy to map raw, high-dimensional observations to continuous steering and throttle commands. Compared with recent approaches to similar tasks, our method requires neither state estimation nor on-the-fly planning to navigate the vehicle. Our approach relies on, and experimentally validates, recent imitation learning theory. Empirically, we show that policies trained with online imitation learning overcome well-known challenges related to covariate shift and generalize better than policies trained with batch imitation learning. Built on these insights, our autonomous driving system demonstrates successful high-speed off-road driving, matching the state-of-the-art performance.},
  archive  = {J},
  author   = {Yunpeng Pan and Ching-An Cheng and Kamil Saigol and Keuntaek Lee and Xinyan Yan and Evangelos A Theodorou and Byron Boots},
  doi      = {10.1177/0278364919880273},
  journal  = {The International Journal of Robotics Research},
  month    = {3},
  number   = {2-3},
  pages    = {286-302},
  title    = {Imitation learning for agile autonomous driving},
  volume   = {39},
  year     = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). The critical radius in sampling-based motion planning.
<em>The International Journal of Robotics Research</em>,
<em>39</em>(2-3), 266–285. (<a
href="https://doi.org/10.1177/0278364919859627">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {We develop a new analysis of sampling-based motion planning in Euclidean space with uniform random sampling, which significantly improves upon the celebrated result of Karaman and Frazzoli and subsequent work. In particular, we prove the existence of a critical connection radius proportional to Θ ( n − 1 / d ) for n samples and d dimensions: below this value the planner is guaranteed to fail (similarly shown by Karaman and Frazzoli). More importantly, for larger radius values the planner is asymptotically (near-)optimal. Furthermore, our analysis yields an explicit lower bound of 1 − O ( n − 1 ) on the probability of success. A practical implication of our work is that asymptotic (near-)optimality is achieved when each sample is connected to only Θ ( 1 ) neighbors. This is in stark contrast to previous work that requires Θ ( log n ) connections, which are induced by a radius of order ( log n n ) 1 / d . Our analysis applies to the probabilistic roadmap method (PRM), as well as a variety of “PRM-based” planners, including RRG, FMT*, and BTT. Continuum percolation plays an important role in our proofs. Lastly, we develop similar theory for all the aforementioned planners when constructed with deterministic samples, which are then sparsified in a randomized fashion. We believe that this new model, and its analysis, is interesting in its own right.},
  archive  = {J},
  author   = {Kiril Solovey and Michal Kleinbort},
  doi      = {10.1177/0278364919859627},
  journal  = {The International Journal of Robotics Research},
  month    = {3},
  number   = {2-3},
  pages    = {266-285},
  title    = {The critical radius in sampling-based motion planning},
  volume   = {39},
  year     = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Confidence-aware motion prediction for real-time collision
avoidance1. <em>The International Journal of Robotics Research</em>,
<em>39</em>(2-3), 250–265. (<a
href="https://doi.org/10.1177/0278364919859436">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {One of the most difficult challenges in robot motion planning is to account for the behavior of other moving agents, such as humans. Commonly, practitioners employ predictive models to reason about where other agents are going to move . Though there has been much recent work in building predictive models, no model is ever perfect: an agent can always move unexpectedly, in a way that is not predicted or not assigned sufficient probability. In such cases, the robot may plan trajectories that appear safe but, in fact, lead to collision. Rather than trust a model’s predictions blindly, we propose that the robot should use the model’s current predictive accuracy to inform the degree of confidence in its future predictions. This model confidence inference allows us to generate probabilistic motion predictions that exploit modeled structure when the structure successfully explains human motion, and degrade gracefully whenever the human moves unexpectedly. We accomplish this by maintaining a Bayesian belief over a single parameter that governs the variance of our human motion model. We couple this prediction algorithm with a recently proposed robust motion planner and controller to guide the construction of robot trajectories that are, to a good approximation, collision-free with a high, user-specified probability. We provide extensive analysis of the combined approach and its overall safety properties by establishing a connection to reachability analysis, and conclude with a hardware demonstration in which a small quadcopter operates safely in the same space as a human pedestrian.},
  archive  = {J},
  author   = {David Fridovich-Keil and Andrea Bajcsy and Jaime F Fisac and Sylvia L Herbert and Steven Wang and Anca D Dragan and Claire J Tomlin},
  doi      = {10.1177/0278364919859436},
  journal  = {The International Journal of Robotics Research},
  month    = {3},
  number   = {2-3},
  pages    = {250-265},
  title    = {Confidence-aware motion prediction for real-time collision avoidance1},
  volume   = {39},
  year     = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Multi-task trust transfer for human–robot interaction.
<em>The International Journal of Robotics Research</em>,
<em>39</em>(2-3), 233–249. (<a
href="https://doi.org/10.1177/0278364919866905">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Trust is essential in shaping human interactions with one another and with robots. In this article we investigate how human trust in robot capabilities transfers across multiple tasks. We present a human-subject study of two distinct task domains: a Fetch robot performing household tasks and a virtual reality simulation of an autonomous vehicle performing driving and parking maneuvers. The findings expand our understanding of trust and provide new predictive models of trust evolution and transfer via latent task representations: a rational Bayes model, a data-driven neural network model, and a hybrid model that combines the two. Experiments show that the proposed models outperform prevailing models when predicting trust over unseen tasks and users. These results suggest that (i) task-dependent functional trust models capture human trust in robot capabilities more accurately and (ii) trust transfer across tasks can be inferred to a good degree. The latter enables trust-mediated robot decision-making for fluent human–robot interaction in multi-task settings.},
  archive  = {J},
  author   = {Harold Soh and Yaqi Xie and Min Chen and David Hsu},
  doi      = {10.1177/0278364919866905},
  journal  = {The International Journal of Robotics Research},
  month    = {3},
  number   = {2-3},
  pages    = {233-249},
  title    = {Multi-task trust transfer for human–robot interaction},
  volume   = {39},
  year     = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). INGRESS: Interactive visual grounding of referring
expressions. <em>The International Journal of Robotics Research</em>,
<em>39</em>(2-3), 217–232. (<a
href="https://doi.org/10.1177/0278364919897133">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {This article presents INGRESS, a robot system that follows human natural language instructions to pick and place everyday objects. The key question here is to ground referring expressions: understand expressions about objects and their relationships from image and natural language inputs. INGRESS allows unconstrained object categories and rich language expressions. Further, it asks questions to clarify ambiguous referring expressions interactively. To achieve these, we take the approach of grounding by generation and propose a two-stage neural-network model for grounding. The first stage uses a neural network to generate visual descriptions of objects, compares them with the input language expressions, and identifies a set of candidate objects. The second stage uses another neural network to examine all pairwise relations between the candidates and infers the most likely referred objects. The same neural networks are used for both grounding and question generation for disambiguation. Experiments show that INGRESS outperformed a state-of-the-art method on the RefCOCO dataset and in robot experiments with humans. The INGRESS source code is available at https://github.com/MohitShridhar/ingress .},
  archive  = {J},
  author   = {Mohit Shridhar and Dixant Mittal and David Hsu},
  doi      = {10.1177/0278364919897133},
  journal  = {The International Journal of Robotics Research},
  month    = {3},
  number   = {2-3},
  pages    = {217-232},
  title    = {INGRESS: Interactive visual grounding of referring expressions},
  volume   = {39},
  year     = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Learning task-oriented grasping for tool manipulation from
simulated self-supervision. <em>The International Journal of Robotics
Research</em>, <em>39</em>(2-3), 202–216. (<a
href="https://doi.org/10.1177/0278364919872545">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Tool manipulation is vital for facilitating robots to complete challenging task goals. It requires reasoning about the desired effect of the task and, thus, properly grasping and manipulating the tool to achieve the task. Most work in robotics has focused on task-agnostic grasping, which optimizes for only grasp robustness without considering the subsequent manipulation tasks. In this article, we propose the Task-Oriented Grasping Network (TOG-Net) to jointly optimize both task-oriented grasping of a tool and the manipulation policy for that tool. The training process of the model is based on large-scale simulated self-supervision with procedurally generated tool objects. We perform both simulated and real-world experiments on two tool-based manipulation tasks: sweeping and hammering. Our model achieves overall 71.1% task success rate for sweeping and 80.0% task success rate for hammering.},
  archive  = {J},
  author   = {Kuan Fang and Yuke Zhu and Animesh Garg and Andrey Kurenkov and Viraj Mehta and Li Fei-Fei and Silvio Savarese},
  doi      = {10.1177/0278364919872545},
  journal  = {The International Journal of Robotics Research},
  month    = {3},
  number   = {2-3},
  pages    = {202-216},
  title    = {Learning task-oriented grasping for tool manipulation from simulated self-supervision},
  volume   = {39},
  year     = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Learning robust, real-time, reactive robotic grasping.
<em>The International Journal of Robotics Research</em>,
<em>39</em>(2-3), 183–201. (<a
href="https://doi.org/10.1177/0278364919859066">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {We present a novel approach to perform object-independent grasp synthesis from depth images via deep neural networks. Our generative grasping convolutional neural network (GG-CNN) predicts a pixel-wise grasp quality that can be deployed in closed-loop grasping scenarios. GG-CNN overcomes shortcomings in existing techniques, namely discrete sampling of grasp candidates and long computation times. The network is orders of magnitude smaller than other state-of-the-art approaches while achieving better performance, particularly in clutter. We run a suite of real-world tests, during which we achieve an 84% grasp success rate on a set of previously unseen objects with adversarial geometry and 94% on household items. The lightweight nature enables closed-loop control of up to 50 Hz, with which we observed 88% grasp success on a set of household objects that are moved during the grasp attempt. We further propose a method combining our GG-CNN with a multi-view approach, which improves overall grasp success rate in clutter by 10%. Code is provided at https://github.com/dougsm/ggcnn},
  archive  = {J},
  author   = {Douglas Morrison and Peter Corke and Jürgen Leitner},
  doi      = {10.1177/0278364919859066},
  journal  = {The International Journal of Robotics Research},
  month    = {3},
  number   = {2-3},
  pages    = {183-201},
  title    = {Learning robust, real-time, reactive robotic grasping},
  volume   = {39},
  year     = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Planar in-hand manipulation via motion cones. <em>The
International Journal of Robotics Research</em>, <em>39</em>(2-3),
163–182. (<a href="https://doi.org/10.1177/0278364919880257">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {In this article, we present the mechanics and algorithms to compute the set of feasible motions of an object pushed in a plane. This set is known as the motion cone and was previously described for non-prehensile manipulation tasks in the horizontal plane. We generalize its construction to a broader set of planar tasks, such as those where external forces including gravity influence the dynamics of pushing, or prehensile tasks, where there are complex frictional interactions between the gripper, object, and pusher. We show that the motion cone is defined by a set of low-curvature surfaces and approximate it by a polyhedral cone. We verify its validity with thousands of pushing experiments recorded with a motion tracking system. Motion cones abstract the algebra involved in the dynamics of frictional pushing and can be used for simulation, planning, and control. In this article, we demonstrate their use for the dynamic propagation step in a sampling-based planning algorithm. By constraining the planner to explore only through the interior of motion cones, we obtain manipulation strategies that are robust against bounded uncertainties in the frictional parameters of the system. Our planner generates in-hand manipulation trajectories that involve sequences of continuous pushes, from different sides of the object when necessary, with 5–1,000 times speed improvements to equivalent algorithms.},
  archive  = {J},
  author   = {Nikhil Chavan-Dafle and Rachel Holladay and Alberto Rodriguez},
  doi      = {10.1177/0278364919880257},
  journal  = {The International Journal of Robotics Research},
  month    = {3},
  number   = {2-3},
  pages    = {163-182},
  title    = {Planar in-hand manipulation via motion cones},
  volume   = {39},
  year     = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Special issue on the 2018 robotics: Science and systems
conference. <em>The International Journal of Robotics Research</em>,
<em>39</em>(2-3), 161–162. (<a
href="https://doi.org/10.1177/0278364920902284">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive = {J},
  author  = {Nikolay Atanasov and Chris Amato},
  doi     = {10.1177/0278364920902284},
  journal = {The International Journal of Robotics Research},
  month   = {3},
  number  = {2-3},
  pages   = {161-162},
  title   = {Special issue on the 2018 robotics: Science and systems conference},
  volume  = {39},
  year    = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Globally stable velocity estimation using normalized
velocity measurement. <em>The International Journal of Robotics
Research</em>, <em>39</em>(1), 143–157. (<a
href="https://doi.org/10.1177/0278364919887436">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {The problem of estimating velocity from a monocular camera and calibrated inertial measurement unit (IMU) measurements is revisited. For the presented setup, it is assumed that normalized velocity measurements are available from the camera. By applying results from nonlinear observer theory, we present velocity estimators with proven global stability under defined conditions, and without the need to observe features from several camera frames. Several nonlinear methods are compared with each other, also against an extended Kalman filter (EKF), where the robustness of the nonlinear methods compared with the EKF are demonstrated in simulations and experiments.},
  archive  = {J},
  author   = {Elias Bjørne and Edmund F Brekke and Torleiv H Bryne and Jeff Delaune and Tor Arne Johansen},
  doi      = {10.1177/0278364919887436},
  journal  = {The International Journal of Robotics Research},
  month    = {1},
  number   = {1},
  pages    = {143-157},
  title    = {Globally stable velocity estimation using normalized velocity measurement},
  volume   = {39},
  year     = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Compact models for adaptive sampling in marine robotics.
<em>The International Journal of Robotics Research</em>, <em>39</em>(1),
127–142. (<a href="https://doi.org/10.1177/0278364919884141">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Finding high-value locations for in situ data collection is of substantial importance in ocean science, where diverse bio-physical processes interact to create dynamically evolving phenomena. These cover a variable spatial extent, and are sparse and difficult to predict. Autonomous robotic platforms can sustain themselves in harsh conditions with persistent presence, but require deployment at the correct place and time. To that end, we consider the use of remote sensing data for building compact models that can improve skill in predicting sub-mesoscale features and inform onboard sampling. The model enables prediction of regional patterns based on sparse in situ data, a capability that is essential in regions where use of satellite remote sensing in real time is often limited by cloud cover. Our model is based on classification of sea-surface temperature (SST) images, but the technique is general across any remotely sensed parameter. Images having similar magnitude and spatial patterns are grouped into a compact set of conditional means representing the dominant states. The classification is unsupervised and uses a combination of dictionary learning and hierarchical clustering. The method is demonstrated using SST images from Monterey Bay, California. The consistency of the classification result is verified and compared with oceanographic forcing using historical wind measurements. The established model is then shown to work in a real application using measurements from an autonomous surface vehicle (ASV), together with forecast and sampling strategies. Finally an analysis of the model prediction error is presented and compared across different paths and survey duration.},
  archive  = {J},
  author   = {Trygve Olav Fossum and John Ryan and Tapan Mukerji and Jo Eidsvik and Thom Maughan and Martin Ludvigsen and Kanna Rajan},
  doi      = {10.1177/0278364919884141},
  journal  = {The International Journal of Robotics Research},
  month    = {1},
  number   = {1},
  pages    = {127-142},
  title    = {Compact models for adaptive sampling in marine robotics},
  volume   = {39},
  year     = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Transforming multiple visual surveys of a natural
environment into time-lapses. <em>The International Journal of Robotics
Research</em>, <em>39</em>(1), 100–126. (<a
href="https://doi.org/10.1177/0278364919881205">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {This article presents a new framework to help transform visual surveys of a natural environment into time-lapses. As data association across year-long variation in appearance continues to represent a formidable challenge, we present success with a map-centric approach, which builds on 3D vision for visual data association. We use a foundation of map point priors and geometric constraints within a dense correspondence image alignment optimization to align images and acquire loop closures between surveys. This framework produces many loop closures between sessions. Outlier loop closures are filtered in the frontend and in the backend to improve robustness. From the result map, the Reprojection Flow algorithm is applied to create time-lapses. The evaluation of our framework on the Symphony Lake Dataset, which has considerable variation in appearance, led to year-long time-lapses of many different scenes. In comparison with another approach based on using iterative closest point (ICP) plus a homography, our framework produced more and better-quality alignments. With many scenes of the 1.3 km environment consistently aligning well in random image pairs, we next produced 100 time-lapses across 37 surveys captured in a year. Approximately one-third had at least 20 (out of usually 33) well-aligned images, which spanned all four seasons. With promising results, we evaluated the pose error of misaligned image pairs and found that improving map consistency could lead to even better results.},
  archive  = {J},
  author   = {Shane Griffith and Frank Dellaert and Cédric Pradalier},
  doi      = {10.1177/0278364919881205},
  journal  = {The International Journal of Robotics Research},
  month    = {1},
  number   = {1},
  pages    = {100-126},
  title    = {Transforming multiple visual surveys of a natural environment into time-lapses},
  volume   = {39},
  year     = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A corroborative approach to verification and validation of
human–robot teams. <em>The International Journal of Robotics
Research</em>, <em>39</em>(1), 73–99. (<a
href="https://doi.org/10.1177/0278364919883338">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {We present an approach for the verification and validation (V&amp;V) of robot assistants in the context of human–robot interactions, to demonstrate their trustworthiness through corroborative evidence of their safety and functional correctness. Key challenges include the complex and unpredictable nature of the real world in which assistant and service robots operate, the limitations on available V&amp;V techniques when used individually, and the consequent lack of confidence in the V&amp;V results. Our approach, called corroborative V&amp;V , addresses these challenges by combining several different V&amp;V techniques; in this paper we use formal verification (model checking), simulation-based testing, and user validation in experiments with a real robot. This combination of approaches allows V&amp;V of the human–robot interaction task at different levels of modeling detail and thoroughness of exploration, thus overcoming the individual limitations of each technique. We demonstrate our approach through a handover task, the most critical part of a complex cooperative manufacturing scenario, for which we propose safety and liveness requirements to verify and validate. Should the resulting V&amp;V evidence present discrepancies, an iterative process between the different V&amp;V techniques takes place until corroboration between the V&amp;V techniques is gained from refining and improving the assets (i.e., system and requirement models) to represent the human–robot interaction task in a more truthful manner. Therefore, corroborative V&amp;V affords a systematic approach to “meta-V&amp;V,” in which different V&amp;V techniques can be used to corroborate and check one another, increasing the level of certainty in the results of V&amp;V.},
  archive  = {J},
  author   = {Matt Webster and David Western and Dejanira Araiza-Illan and Clare Dixon and Kerstin Eder and Michael Fisher and Anthony G Pipe},
  doi      = {10.1177/0278364919883338},
  journal  = {The International Journal of Robotics Research},
  month    = {1},
  number   = {1},
  pages    = {73-99},
  title    = {A corroborative approach to verification and validation of human–robot teams},
  volume   = {39},
  year     = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Quantifying teaching behavior in robot learning from
demonstration. <em>The International Journal of Robotics Research</em>,
<em>39</em>(1), 54–72. (<a
href="https://doi.org/10.1177/0278364919884623">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Learning from demonstration allows for rapid deployment of robot manipulators to a great many tasks, by relying on a person showing the robot what to do rather than programming it. While this approach provides many opportunities, measuring, evaluating, and improving the person’s teaching ability has remained largely unexplored in robot manipulation research. To this end, a model for learning from demonstration is presented here that incorporates the teacher’s understanding of, and influence on, the learner. The proposed model is used to clarify the teacher’s objectives during learning from demonstration, providing new views on how teaching failures and efficiency can be defined. The benefit of this approach is shown in two experiments ( n = 30 and n = 36 , respectively), which highlight the difficulty teachers have in providing effective demonstrations, and show how ~ 169 –180% improvement in teaching efficiency can be achieved through evaluation and feedback shaped by the proposed framework, relative to unguided teaching.},
  archive  = {J},
  author   = {Aran Sena and Matthew Howard},
  doi      = {10.1177/0278364919884623},
  journal  = {The International Journal of Robotics Research},
  month    = {1},
  number   = {1},
  pages    = {54-72},
  title    = {Quantifying teaching behavior in robot learning from demonstration},
  volume   = {39},
  year     = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Leveraging depth data in remote robot teleoperation
interfaces for general object manipulation. <em>The International
Journal of Robotics Research</em>, <em>39</em>(1), 39–53. (<a
href="https://doi.org/10.1177/0278364919888565">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Robust remote teleoperation of high-degree-of-freedom manipulators is of critical importance across a wide range of robotics applications. Contemporary robot manipulation interfaces primarily utilize a free positioning pose specification approach to independently control each degree of freedom in free space. In this work, we present two novel interfaces, constrained positioning and point-and-click. Both novel approaches incorporate scene information from depth data into the grasp pose specification process, effectively reducing the number of 3D transformations the user must input. The novel interactions are designed for 2D image streams, rather than traditional 3D virtual scenes, further reducing mental transformations by eliminating the controllable camera viewpoint in favor of fixed physical camera viewpoints. We present interface implementations of our novel approaches, as well as free positioning, in both 2D and 3D visualization modes. In addition, we present results of a 90-participant user study evaluation comparing the effectiveness of each approach for a set of general object manipulation tasks, and the effects of implementing each approach in 2D image views versus 3D depth views. The results of our study show that point-and-click outperforms both free positioning and constrained positioning by significantly increasing the number of tasks completed and significantly reducing task failures and grasping errors, while significantly reducing the number of user interactions required to specify poses. In addition, we found that regardless of the interaction approach, the 2D visualization mode resulted in significantly better performance than the 3D visualization mode, with statistically significant reductions in task failures, grasping errors, task completion time, number of interactions, and user workload, all while reducing bandwidth requirements imposed by streaming depth data.},
  archive  = {J},
  author   = {David Kent and Carl Saldanha and Sonia Chernova},
  doi      = {10.1177/0278364919888565},
  journal  = {The International Journal of Robotics Research},
  month    = {1},
  number   = {1},
  pages    = {39-53},
  title    = {Leveraging depth data in remote robot teleoperation interfaces for general object manipulation},
  volume   = {39},
  year     = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Learning attribute grammars for movement primitive
sequencing. <em>The International Journal of Robotics Research</em>,
<em>39</em>(1), 21–38. (<a
href="https://doi.org/10.1177/0278364919868279">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Movement primitives are a well studied and widely applied concept in modern robotics. However, composing primitives out of an existing library has shown to be a challenging problem. We propose the use of probabilistic context-free grammars to sequence a series of primitives to generate complex robot policies from a given library of primitives. The rule-based nature of formal grammars allows an intuitive encoding of hierarchically structured tasks. This hierarchical concept strongly connects with the way robot policies can be learned, organized, and re-used. However, the induction of context-free grammars has proven to be a complicated and yet unsolved challenge. We exploit the physical nature of robot movement primitives to restrict and efficiently search the grammar space. The grammar is learned by applying a Markov chain Monte Carlo optimization over the posteriors of the grammars given the observations. The proposal distribution is defined as a mixture over the probabilities of the operators connecting the search space. Moreover, we present an approach for the categorization of probabilistic movement primitives and discuss how the connectibility of two primitives can be determined. These characteristics in combination with restrictions to the operators guarantee continuous sequences while reducing the grammar space. In addition, a set of attributes and conditions is introduced that augments probabilistic context-free grammars in order to solve primitive sequencing tasks with the capability to adapt single primitives within the sequence. The method was validated on tasks that require the generation of complex sequences consisting of simple movement primitives using a seven-degree-of-freedom lightweight robotic arm.},
  archive  = {J},
  author   = {Rudolf Lioutikov and Guilherme Maeda and Filipe Veiga and Kristian Kersting and Jan Peters},
  doi      = {10.1177/0278364919868279},
  journal  = {The International Journal of Robotics Research},
  month    = {1},
  number   = {1},
  pages    = {21-38},
  title    = {Learning attribute grammars for movement primitive sequencing},
  volume   = {39},
  year     = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Learning dexterous in-hand manipulation. <em>The
International Journal of Robotics Research</em>, <em>39</em>(1), 3–20.
(<a href="https://doi.org/10.1177/0278364919887447">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {We use reinforcement learning (RL) to learn dexterous in-hand manipulation policies that can perform vision-based object reorientation on a physical Shadow Dexterous Hand. The training is performed in a simulated environment in which we randomize many of the physical properties of the system such as friction coefficients and an object’s appearance. Our policies transfer to the physical robot despite being trained entirely in simulation. Our method does not rely on any human demonstrations, but many behaviors found in human manipulation emerge naturally, including finger gaiting, multi-finger coordination, and the controlled use of gravity. Our results were obtained using the same distributed RL system that was used to train OpenAI Five. We also include a video of our results: https://youtu.be/jwSbzNHGflM .},
  archive  = {J},
  author   = {OpenAI: Marcin Andrychowicz and Bowen Baker and Maciek Chociej and Rafal Józefowicz and Bob McGrew and Jakub Pachocki and Arthur Petron and Matthias Plappert and Glenn Powell and Alex Ray and Jonas Schneider and Szymon Sidor and Josh Tobin and Peter Welinder and Lilian Weng and Wojciech Zaremba},
  doi      = {10.1177/0278364919887447},
  journal  = {The International Journal of Robotics Research},
  month    = {1},
  number   = {1},
  pages    = {3-20},
  title    = {Learning dexterous in-hand manipulation},
  volume   = {39},
  year     = {2020},
}
</textarea>
</details></li>
</ul>

</body>
</html>
