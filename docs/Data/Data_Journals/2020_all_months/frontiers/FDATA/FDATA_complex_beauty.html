<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>FDATA_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="fdata---51">FDATA - 51</h2>
<ul>
<li><details>
<summary>
(2020). Heterogeneous reconstruction of tracks and primary vertices
with the CMS pixel tracker. <em>FDATA</em>, <em>3</em>, 601728. (<a
href="https://doi.org/10.3389/fdata.2020.601728">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The High-Luminosity upgrade of the Large Hadron Collider (LHC) will see the accelerator reach an instantaneous luminosity of 7 × 1034 cm−2 s−1 with an average pileup of 200 proton-proton collisions. These conditions will pose an unprecedented challenge to the online and offline reconstruction software developed by the experiments. The computational complexity will exceed by far the expected increase in processing power for conventional CPUs, demanding an alternative approach. Industry and High-Performance Computing (HPC) centers are successfully using heterogeneous computing platforms to achieve higher throughput and better energy efficiency by matching each job to the most appropriate architecture. In this paper we will describe the results of a heterogeneous implementation of pixel tracks and vertices reconstruction chain on Graphics Processing Units (GPUs). The framework has been designed and developed to be integrated in the CMS reconstruction software, CMSSW. The speed up achieved by leveraging GPUs allows for more complex algorithms to be executed, obtaining better physics output and a higher throughput.},
  archive      = {J_FDATA},
  author       = {Bocci, A. and Innocente, V. and Kortelainen, M. and Pantaleo, F. and Rovere, M.},
  doi          = {10.3389/fdata.2020.601728},
  journal      = {Frontiers in Big Data},
  month        = {12},
  pages        = {601728},
  shortjournal = {Front. Big Data},
  title        = {Heterogeneous reconstruction of tracks and primary vertices with the CMS pixel tracker},
  volume       = {3},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). The bayesian additive regression trees formula for safe
machine learning-based intraocular lens predictions. <em>FDATA</em>,
<em>3</em>, 572134. (<a
href="https://doi.org/10.3389/fdata.2020.572134">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Purpose: Our work introduces a highly accurate, safe, and sufficiently explicable machine-learning (artificial intelligence) model of intraocular lens power (IOL) translating into better post-surgical outcomes for patients with cataracts. We also demonstrate its improved predictive accuracy over previous formulas.Methods: We collected retrospective eye measurement data on 5,331 eyes from 3,276 patients across multiple centers who received a lens implantation during cataract surgery. The dependent measure is the post-operative manifest spherical equivalent error from intended and the independent variables are the patient- and eye-specific characteristics. This dataset was split so that one subset was for formula construction and the other for validating our new formula. Data excluded fellow eyes, so as not to confound the prediction with bilateral eyes.Results: Our formula is three times more precise than reported studies with a median absolute IOL error of 0.204 diopters (D). When converted to absolute predictive refraction errors on the cornea, the median error is 0.137 D which is close to the IOL manufacturer tolerance. These estimates are validated out-of-sample and thus are expected to reflect the future performance of our prediction formula, especially since our data were collected from a wide variety of patients, clinics, and manufacturers.Conclusion: The increased precision of IOL power calculations has the potential to optimize patient positive refractive outcomes. Our model also provides uncertainty plots that can be used in tandem with the clinician’s expertise and previous formula output, further enhancing the safety.Translational relavance: Our new machine learning process has the potential to significantly improve patient IOL refractive outcomes safely.},
  archive      = {J_FDATA},
  author       = {Clarke, Gerald P. and Kapelner, Adam},
  doi          = {10.3389/fdata.2020.572134},
  journal      = {Frontiers in Big Data},
  month        = {12},
  pages        = {572134},
  shortjournal = {Front. Big Data},
  title        = {The bayesian additive regression trees formula for safe machine learning-based intraocular lens predictions},
  volume       = {3},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Perspective: Acknowledging data work in the social media
research lifecycle. <em>FDATA</em>, <em>3</em>, 509954. (<a
href="https://doi.org/10.3389/fdata.2020.509954">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This perspective article suggests considering the everyday research data management work required to accomplish social media research along different phases in a data lifecycle to inform the ongoing discussion of social media research data’s quality and validity. Our perspective is informed by practical experience of archiving social media data, by results from a series of qualitative interviews with social media researchers, as well as by recent literature in the field. We emphasize how social media researchers are entangled in complexities between social media platform providers, social media users, other actors, as well as legal and ethical frameworks, that all affect their everyday research practices. Research design decisions are made iteratively at different stages, involving many decisions that may potentially impact the quality of research. We show that these decisions are often hidden, but that making them visible allows us to better understand what drives social media research into specific directions. Consequently, we argue that untangling and documenting choices during the research lifecycle, especially when researchers pursue specific approaches and may have actively decided against others (often due to external factors) is necessary and will help to spot and address structural challenges in the social media research ecosystem that go beyond critiques of individual opportunistic approaches to easily accessible data.},
  archive      = {J_FDATA},
  author       = {Kinder-Kurlanda, Katharina E. and Weller, Katrin},
  doi          = {10.3389/fdata.2020.509954},
  journal      = {Frontiers in Big Data},
  month        = {12},
  pages        = {509954},
  shortjournal = {Front. Big Data},
  title        = {Perspective: Acknowledging data work in the social media research lifecycle},
  volume       = {3},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). CLUE: A fast parallel clustering algorithm for high
granularity calorimeters in high-energy physics. <em>FDATA</em>,
<em>3</em>, 591315. (<a
href="https://doi.org/10.3389/fdata.2020.591315">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {One of the challenges of high granularity calorimeters, such as that to be built to cover the endcap region in the CMS Phase-2 Upgrade for HL-LHC, is that the large number of channels causes a surge in the computing load when clustering numerous digitized energy deposits (hits) in the reconstruction stage. In this article, we propose a fast and fully parallelizable density-based clustering algorithm, optimized for high-occupancy scenarios, where the number of clusters is much larger than the average number of hits in a cluster. The algorithm uses a grid spatial index for fast querying of neighbors and its timing scales linearly with the number of hits within the range considered. We also show a comparison of the performance on CPU and GPU implementations, demonstrating the power of algorithmic parallelization in the coming era of heterogeneous computing in high-energy physics.},
  archive      = {J_FDATA},
  author       = {Rovere, Marco and Chen, Ziheng and Di Pilato, Antonio and Pantaleo, Felice and Seez, Chris},
  doi          = {10.3389/fdata.2020.591315},
  journal      = {Frontiers in Big Data},
  month        = {11},
  pages        = {591315},
  shortjournal = {Front. Big Data},
  title        = {CLUE: A fast parallel clustering algorithm for high granularity calorimeters in high-energy physics},
  volume       = {3},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Securing machine learning in the cloud: A systematic review
of cloud machine learning security. <em>FDATA</em>, <em>3</em>, 587139.
(<a href="https://doi.org/10.3389/fdata.2020.587139">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the advances in machine learning (ML) and deep learning (DL) techniques, and the potency of cloud computing in offering services efficiently and cost-effectively, Machine Learning as a Service (MLaaS) cloud platforms have become popular. In addition, there is increasing adoption of third-party cloud services for outsourcing training of DL models, which requires substantial costly computational resources (e.g., high-performance graphics processing units (GPUs)). Such widespread usage of cloud-hosted ML/DL services opens a wide range of attack surfaces for adversaries to exploit the ML/DL system to achieve malicious goals. In this article, we conduct a systematic evaluation of literature of cloud-hosted ML/DL models along both the important dimensions—attacks and defenses—related to their security. Our systematic review identified a total of 31 related articles out of which 19 focused on attack, six focused on defense, and six focused on both attack and defense. Our evaluation reveals that there is an increasing interest from the research community on the perspective of attacking and defending different attacks on Machine Learning as a Service platforms. In addition, we identify the limitations and pitfalls of the analyzed articles and highlight open research issues that require further investigation.},
  archive      = {J_FDATA},
  author       = {Qayyum, Adnan and Ijaz, Aneeqa and Usama, Muhammad and Iqbal, Waleed and Qadir, Junaid and Elkhatib, Yehia and Al-Fuqaha, Ala},
  doi          = {10.3389/fdata.2020.587139},
  journal      = {Frontiers in Big Data},
  month        = {11},
  pages        = {587139},
  shortjournal = {Front. Big Data},
  title        = {Securing machine learning in the cloud: A systematic review of cloud machine learning security},
  volume       = {3},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Machine learning methods to predict acute respiratory
failure and acute respiratory distress syndrome. <em>FDATA</em>,
<em>3</em>, 579774. (<a
href="https://doi.org/10.3389/fdata.2020.579774">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Acute respiratory failure (ARF) is a common problem in medicine that utilizes significant healthcare resources and is associated with high morbidity and mortality. Classification of acute respiratory failure is complicated, and it is often determined by the level of mechanical support that is required, or the discrepancy between oxygen supply and uptake. These phenotypes make acute respiratory failure a continuum of syndromes, rather than one homogenous disease process. Early recognition of the risk factors for new or worsening acute respiratory failure may prevent that process from occurring. Predictive analytical methods using machine learning leverage clinical data to provide an early warning for impending acute respiratory failure or its sequelae. The aims of this review are to summarize the current literature on ARF prediction, to describe accepted procedures and common machine learning tools for predictive tasks through the lens of ARF prediction, and to demonstrate the challenges and potential solutions for ARF prediction that can improve patient outcomes.},
  archive      = {J_FDATA},
  author       = {Wong, An-Kwok Ian and Cheung, Patricia C. and Kamaleswaran, Rishikesan and Martin, Greg S. and Holder, Andre L.},
  doi          = {10.3389/fdata.2020.579774},
  journal      = {Frontiers in Big Data},
  month        = {11},
  pages        = {579774},
  shortjournal = {Front. Big Data},
  title        = {Machine learning methods to predict acute respiratory failure and acute respiratory distress syndrome},
  volume       = {3},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Interdisciplinary research in artificial intelligence:
Challenges and opportunities. <em>FDATA</em>, <em>3</em>, 577974. (<a
href="https://doi.org/10.3389/fdata.2020.577974">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The use of artificial intelligence (AI) in a variety of research fields is speeding up multiple digital revolutions, from shifting paradigms in healthcare, precision medicine and wearable sensing, to public services and education offered to the masses around the world, to future cities made optimally efficient by autonomous driving. When a revolution happens, the consequences are not obvious straight away, and to date, there is no uniformly adapted framework to guide AI research to ensure a sustainable societal transition. To answer this need, here we analyze three key challenges to interdisciplinary AI research, and deliver three broad conclusions: 1) future development of AI should not only impact other scientific domains but should also take inspiration and benefit from other fields of science, 2) AI research must be accompanied by decision explainability, dataset bias transparency as well as development of evaluation methodologies and creation of regulatory agencies to ensure responsibility, and 3) AI education should receive more attention, efforts and innovation from the educational and scientific communities. Our analysis is of interest not only to AI practitioners but also to other researchers and the general public as it offers ways to guide the emerging collaborations and interactions toward the most fruitful outcomes.},
  archive      = {J_FDATA},
  author       = {Kusters, Remy and Misevic, Dusan and Berry, Hugues and Cully, Antoine and Le Cunff, Yann and Dandoy, Loic and Díaz-Rodríguez, Natalia and Ficher, Marion and Grizou, Jonathan and Othmani, Alice and Palpanas, Themis and Komorowski, Matthieu and Loiseau, Patrick and Moulin Frier, Clément and Nanini, Santino and Quercia, Daniele and Sebag, Michele and Soulié Fogelman, Françoise and Taleb, Sofiane and Tupikina, Liubov and Sahu, Vaibhav and Vie, Jill-Jênn and Wehbi, Fatima},
  doi          = {10.3389/fdata.2020.577974},
  journal      = {Frontiers in Big Data},
  month        = {11},
  pages        = {577974},
  shortjournal = {Front. Big Data},
  title        = {Interdisciplinary research in artificial intelligence: Challenges and opportunities},
  volume       = {3},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). CADRE: A collaborative, cloud-based solution for big
bibliographic data research in academic libraries. <em>FDATA</em>,
<em>3</em>, 556282. (<a
href="https://doi.org/10.3389/fdata.2020.556282">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Big bibliographic datasets hold promise for revolutionizing the scientific enterprise when combined with state-of-the-science computational capabilities. Yet, hosting proprietary and open big bibliographic datasets poses significant difficulties for libraries, both large and small. Libraries face significant barriers to hosting such assets, including cost and expertise, which has limited their ability to provide stewardship for big datasets, and thus has hampered researchers&#39; access to them. What is needed is a solution to address the libraries&#39; and researchers’ joint needs. This article outlines the theoretical framework that underpins the Collaborative Archive and Data Research Environment project. We recommend a shared cloud-based infrastructure to address this need built on five pillars: 1) Community–a community of libraries and industry partners who support and maintain the platform and a community of researchers who use it; 2) Access–the sharing platform should be accessible and affordable to both proprietary data customers and the general public; 3) Data-Centric–the platform is optimized for efficient and high-quality bibliographic data services, satisfying diverse data needs; 4) Reproducibility–the platform should be designed to foster and encourage reproducible research; 5) Empowerment—the platform should empower researchers to perform big data analytics on the hosted datasets. In this article, we describe the many facets of the problem faced by American academic libraries and researchers wanting to work with big datasets. We propose a practical solution based on the five pillars: The Collaborative Archive and Data Research Environment. Finally, we address potential barriers to implementing this solution and strategies for overcoming them.},
  archive      = {J_FDATA},
  author       = {Mabry, Patricia L. and Yan, Xiaoran and Pentchev, Valentin and Van Rennes, Robert and McGavin, Stephanie Hernandez and Wittenberg, Jamie V.},
  doi          = {10.3389/fdata.2020.556282},
  journal      = {Frontiers in Big Data},
  month        = {11},
  pages        = {556282},
  shortjournal = {Front. Big Data},
  title        = {CADRE: A collaborative, cloud-based solution for big bibliographic data research in academic libraries},
  volume       = {3},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Dashboard of sentiment in austrian social media during
COVID-19. <em>FDATA</em>, <em>3</em>, 573654. (<a
href="https://doi.org/10.3389/fdata.2020.00032">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To track online emotional expressions on social media platforms close to real-time during the COVID-19 pandemic, we built a self-updating monitor of emotion dynamics using digital traces from three different data sources in Austria. This allows decision makers and the interested public to assess dynamics of sentiment online during the pandemic. We used web scraping and API access to retrieve data from the news platform derstandard.at, Twitter, and a chat platform for students. We documented the technical details of our workflow to provide materials for other researchers interested in building a similar tool for different contexts. Automated text analysis allowed us to highlight changes of language use during COVID-19 in comparison to a neutral baseline. We used special word clouds to visualize that overall difference. Longitudinally, our time series showed spikes in anxiety that can be linked to several events and media reporting. Additionally, we found a marked decrease in anger. The changes lasted for remarkably long periods of time (up to 12 weeks). We have also discussed these and more patterns and connect them to the emergence of collective emotions. The interactive dashboard showcasing our data is available online at http://www.mpellert.at/covid19_monitor_austria/. Our work is part of a web archive of resources on COVID-19 collected by the Austrian National Library.},
  archive      = {J_FDATA},
  author       = {Pellert, Max and Lasser, Jana and Metzler, Hannah and Garcia, David},
  doi          = {10.3389/fdata.2020.00032},
  journal      = {Frontiers in Big Data},
  month        = {10},
  pages        = {573654},
  shortjournal = {Front. Big Data},
  title        = {Dashboard of sentiment in austrian social media during COVID-19},
  volume       = {3},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). What do you think about your company’s leaks? A survey on
end-users perception toward data leakage mechanisms. <em>FDATA</em>,
<em>3</em>, 568257. (<a
href="https://doi.org/10.3389/fdata.2020.568257">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data leakage can lead to severe issues for a company, including financial loss, damage of goodwill, reputation, lawsuits and loss of future sales. To prevent these problems, a company can use other mechanisms on top of traditional Access Control. These mechanisms include for instance Data Leak Prevention or Information Rights Management and can be referred as Transmission Control. However, such solutions can lack usability and can be intrusive for end-users employees. To have a better understanding of the perception and usage of such mechanisms within business infrastructures, we have conducted in this article an online survey on 150 employees. These employees come from different companies of different sizes and sectors of activity. The results show that whatever the size of the company or its sector of activity, security mechanisms such as access control and transmission control can be considered as quite intrusive and blocking for employees. Moreover, our survey also shows interesting results regarding more acceptable and user-friendly anti-data leakage mechanisms that could be used within companies.},
  archive      = {J_FDATA},
  author       = {Bertrand, Yoann and Boudaoud, Karima and Riveill, Michel},
  doi          = {10.3389/fdata.2020.568257},
  journal      = {Frontiers in Big Data},
  month        = {10},
  pages        = {568257},
  shortjournal = {Front. Big Data},
  title        = {What do you think about your company’s leaks? a survey on end-users perception toward data leakage mechanisms},
  volume       = {3},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Inferring true COVID19 infection rates from deaths.
<em>FDATA</em>, <em>3</em>, 565589. (<a
href="https://doi.org/10.3389/fdata.2020.565589">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The novel coronavirus, SARS-CoV-2, commonly known as COVID19 has become a global pandemic in early 2020. The world has mounted a global social distancing intervention on a scale thought unimaginable prior to this outbreak; however, the economic impact and sustainability limits of this policy create significant challenges for government leaders around the world. Understanding the future spread and growth of COVID19 is further complicated by data quality issues due to high numbers of asymptomatic patients who may transmit the disease yet show no symptoms; lack of testing resources; failure of recovered patients to be counted; delays in reporting hospitalizations and deaths; and the co-morbidity of other life-threatening illnesses. We propose a Monte Carlo method for inferring true case counts from observed deaths using clinical estimates of Infection Fatality Ratios and Time to Death. Findings indicate that current COVID19 confirmed positive counts represent a small fraction of actual cases, and that even relatively effective surveillance regimes fail to identify all infectious individuals. We further demonstrate that the miscount also distorts officials&#39; ability to discern the peak of an epidemic, confounding efforts to assess the efficacy of various interventions.},
  archive      = {J_FDATA},
  author       = {McCulloh, Ian and Kiernan, Kevin and Kent, Trevor},
  doi          = {10.3389/fdata.2020.565589},
  journal      = {Frontiers in Big Data},
  month        = {10},
  pages        = {565589},
  shortjournal = {Front. Big Data},
  title        = {Inferring true COVID19 infection rates from deaths},
  volume       = {3},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). The human blood transcriptome in a large population cohort
and its relation to aging and health. <em>FDATA</em>, <em>3</em>,
548873. (<a href="https://doi.org/10.3389/fdata.2020.548873">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Background: The blood transcriptome is expected to provide a detailed picture of an organism&#39;s physiological state with potential outcomes for applications in medical diagnostics and molecular and epidemiological research. We here present the analysis of blood specimens of 3,388 adult individuals, together with phenotype characteristics such as disease history, medication status, lifestyle factors, and body mass index (BMI). The size and heterogeneity of this data challenges analytics in terms of dimension reduction, knowledge mining, feature extraction, and data integration.Methods: Self-organizing maps (SOM)-machine learning was applied to study transcriptional states on a population-wide scale. This method permits a detailed description and visualization of the molecular heterogeneity of transcriptomes and of their association with different phenotypic features.Results: The diversity of transcriptomes is described by personalized SOM-portraits, which specify the samples in terms of modules of co-expressed genes of different functional context. We identified two major blood transcriptome types where type 1 was found more in men, the elderly, and overweight people and it upregulated genes associated with inflammation and increased heme metabolism, while type 2 was predominantly found in women, younger, and normal weight participants and it was associated with activated immune responses, transcriptional, ribosomal, mitochondrial, and telomere-maintenance cell-functions. We find a striking overlap of signatures shared by multiple diseases, aging, and obesity driven by an underlying common pattern, which was associated with the immune response and the increase of inflammatory processes.Conclusions: Machine learning applications for large and heterogeneous omics data provide a holistic view on the diversity of the human blood transcriptome. It provides a tool for comparative analyses of transcriptional signatures and of associated phenotypes in population studies and medical applications.},
  archive      = {J_FDATA},
  author       = {Schmidt, Maria and Hopp, Lydia and Arakelyan, Arsen and Kirsten, Holger and Engel, Christoph and Wirkner, Kerstin and Krohn, Knut and Burkhardt, Ralph and Thiery, Joachim and Loeffler, Markus and Loeffler-Wirth, Henry and Binder, Hans},
  doi          = {10.3389/fdata.2020.548873},
  journal      = {Frontiers in Big Data},
  month        = {10},
  pages        = {548873},
  shortjournal = {Front. Big Data},
  title        = {The human blood transcriptome in a large population cohort and its relation to aging and health},
  volume       = {3},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Causal learning from predictive modeling for observational
data. <em>FDATA</em>, <em>3</em>, 535976. (<a
href="https://doi.org/10.3389/fdata.2020.535976">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider the problem of learning structured causal models from observational data. In this work, we use causal Bayesian networks to represent causal relationships among model variables. To this effect, we explore the use of two types of independencies—context-specific independence (CSI) and mutual independence (MI). We use CSI to identify the candidate set of causal relationships and then use MI to quantify their strengths and construct a causal model. We validate the learned models on benchmark networks and demonstrate the effectiveness when compared to some of the state-of-the-art Causal Bayesian Network Learning algorithms from observational Data.},
  archive      = {J_FDATA},
  author       = {Ramanan, Nandini and Natarajan, Sriraam},
  doi          = {10.3389/fdata.2020.535976},
  journal      = {Frontiers in Big Data},
  month        = {10},
  pages        = {535976},
  shortjournal = {Front. Big Data},
  title        = {Causal learning from predictive modeling for observational data},
  volume       = {3},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). LocationSpark: In-memory distributed spatial query
processing and optimization. <em>FDATA</em>, <em>3</em>, 534568. (<a
href="https://doi.org/10.3389/fdata.2020.00030">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to the ubiquity of spatial data applications and the large amounts of spatial data that these applications generate and process, there is a pressing need for scalable spatial query processing. In this paper, we present new techniques for spatial query processing and optimization in an in-memory and distributed setup to address scalability. More specifically, we introduce new techniques for handling query skew that commonly happens in practice, and minimizes communication costs accordingly. We propose a distributed query scheduler that uses a new cost model to minimize the cost of spatial query processing. The scheduler generates query execution plans that minimize the effect of query skew. The query scheduler utilizes new spatial indexing techniques based on bitmap filters to forward queries to the appropriate local nodes. Each local computation node is responsible for optimizing and selecting its best local query execution plan based on the indexes and the nature of the spatial queries in that node. All the proposed spatial query processing and optimization techniques are prototyped inside Spark, a distributed memory-based computation system. Our prototype system is termed LocationSpark. The experimental study is based on real datasets and demonstrates that LocationSpark can enhance distributed spatial query processing by up to an order of magnitude over existing in-memory and distributed spatial systems.},
  archive      = {J_FDATA},
  author       = {Tang, Mingjie and Yu, Yongyang and Mahmood, Ahmed R. and Malluhi, Qutaibah M. and Ouzzani, Mourad and Aref, Walid G.},
  doi          = {10.3389/fdata.2020.00030},
  journal      = {Frontiers in Big Data},
  month        = {10},
  pages        = {534568},
  shortjournal = {Front. Big Data},
  title        = {LocationSpark: In-memory distributed spatial query processing and optimization},
  volume       = {3},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Ensemble machine learning approach improves predicted
spatial variation of surface soil organic carbon stocks in data-limited
northern circumpolar region. <em>FDATA</em>, <em>3</em>, 528441. (<a
href="https://doi.org/10.3389/fdata.2020.528441">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Various approaches of differing mathematical complexities are being applied for spatial prediction of soil properties. Regression kriging is a widely used hybrid approach of spatial variation that combines correlation between soil properties and environmental factors with spatial autocorrelation between soil observations. In this study, we compared four machine learning approaches (gradient boosting machine, multinarrative adaptive regression spline, random forest, and support vector machine) with regression kriging to predict the spatial variation of surface (0–30 cm) soil organic carbon (SOC) stocks at 250-m spatial resolution across the northern circumpolar permafrost region. We combined 2,374 soil profile observations (calibration datasets) with georeferenced datasets of environmental factors (climate, topography, land cover, bedrock geology, and soil types) to predict the spatial variation of surface SOC stocks. We evaluated the prediction accuracy at randomly selected sites (validation datasets) across the study area. We found that different techniques inferred different numbers of environmental factors and their relative importance for prediction of SOC stocks. Regression kriging produced lower prediction errors in comparison to multinarrative adaptive regression spline and support vector machine, and comparable prediction accuracy to gradient boosting machine and random forest. However, the ensemble median prediction of SOC stocks obtained from all four machine learning techniques showed highest prediction accuracy. Although the use of different approaches in spatial prediction of soil properties will depend on the availability of soil and environmental datasets and computational resources, we conclude that the ensemble median prediction obtained from multiple machine learning approaches provides greater spatial details and produces the highest prediction accuracy. Thus an ensemble prediction approach can be a better choice than any single prediction technique for predicting the spatial variation of SOC stocks.},
  archive      = {J_FDATA},
  author       = {Mishra, Umakant and Gautam, Sagar and Riley, William J. and Hoffman, Forrest M.},
  doi          = {10.3389/fdata.2020.528441},
  journal      = {Frontiers in Big Data},
  month        = {10},
  pages        = {528441},
  shortjournal = {Front. Big Data},
  title        = {Ensemble machine learning approach improves predicted spatial variation of surface soil organic carbon stocks in data-limited northern circumpolar region},
  volume       = {3},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Decentralized but globally coordinated biodiversity data.
<em>FDATA</em>, <em>3</em>, 519133. (<a
href="https://doi.org/10.3389/fdata.2020.519133">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Centralized biodiversity data aggregation is too often failing societal needs due to pervasive and systemic data quality deficiencies. We argue for a novel approach that embodies the spirit of the Web (“small pieces loosely joined”) through the decentralized coordination of data across scientific languages and communities. The upfront cost of decentralization can be offset by the long-term benefit of achieving sustained expert engagement, higher-quality data products, and ultimately more societal impact for biodiversity data. Our decentralized approach encourages the emergence and evolution of multiple self-identifying communities of practice that are regionally, taxonomically, or institutionally localized. Each community is empowered to control the social and informational design and versioning of their local data infrastructures and signals. With no single aggregator to exert centralized control over biodiversity data, decentralization generates loosely connected networks of mid-level aggregators. Global coordination is nevertheless feasible through automatable data sharing agreements that enable efficient propagation and translation of biodiversity data across communities. The decentralized model also poses novel integration challenges, among which the explicit and continuous articulation of conflicting systematic classifications and phylogenies remain the most challenging. We discuss the development of available solutions, challenges, and outline next steps: the global effort of coordination should focus on developing shared languages for data signal translation, as opposed to homogenizing the data signal itself.},
  archive      = {J_FDATA},
  author       = {Sterner, Beckett W. and Gilbert, Edward E. and Franz, Nico M.},
  doi          = {10.3389/fdata.2020.519133},
  journal      = {Frontiers in Big Data},
  month        = {10},
  pages        = {519133},
  shortjournal = {Front. Big Data},
  title        = {Decentralized but globally coordinated biodiversity data},
  volume       = {3},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). The role of cue utilization and cognitive load in the
recognition of phishing emails. <em>FDATA</em>, <em>3</em>, 546860. (<a
href="https://doi.org/10.3389/fdata.2020.546860">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Phishing emails represent a major threat to online information security. While the prevailing research is focused on users&#39; susceptibility, few studies have considered the decision-making strategies that account for skilled detection. One relevant facet of decision-making is cue utilization, where users retrieve feature-event associations stored in long-term memory. High degrees of cue utilization help reduce the demands placed on working memory (i.e., cognitive load), and invariably improve decision performance (i.e., the information-reduction hypothesis in expert performance). The current study explored the effect of cue utilization and cognitive load when detecting phishing emails. A total of 50 undergraduate students completed: (1) a rail control task; (2) a phishing detection task; and (3) a survey of the cues used in detection. A cue utilization assessment battery (EXPERTise 2.0) then classified participants with either higher or lower cue utilization. As expected, higher cue utilization was associated with a greater likelihood of detecting phishing emails. However, variation in cognitive load had no effect on phishing detection, nor was there an interaction between cue utilization and cognitive load. Further, the findings revealed no significant difference in the types of cues used across cue utilization groups or performance levels. These findings have implications for our understanding of cognitive mechanisms that underpin the detection of phishing emails and the role of factors beyond the information-reduction hypothesis.},
  archive      = {J_FDATA},
  author       = {Nasser, George and Morrison, Ben W. and Bayl-Smith, Piers and Taib, Ronnie and Gayed, Michael and Wiggins, Mark W.},
  doi          = {10.3389/fdata.2020.546860},
  journal      = {Frontiers in Big Data},
  month        = {9},
  pages        = {546860},
  shortjournal = {Front. Big Data},
  title        = {The role of cue utilization and cognitive load in the recognition of phishing emails},
  volume       = {3},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Considerations for a more ethical approach to data in AI: On
data representation and infrastructure. <em>FDATA</em>, <em>3</em>,
527486. (<a href="https://doi.org/10.3389/fdata.2020.00025">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data shapes the development of Artificial Intelligence (AI) as we currently know it, and for many years centralized networking infrastructures have dominated both the sourcing and subsequent use of such data. Research suggests that centralized approaches result in poor representation, and as AI is now integrated more in daily life, there is a need for efforts to improve on this. The AI research community has begun to explore managing data infrastructures more democratically, finding that decentralized networking allows for more transparency which can alleviate core ethical concerns, such as selection-bias. With this in mind, herein, we present a mini-survey framed around data representation and data infrastructures in AI. We outline four key considerations (auditing, benchmarking, confidence and trust, explainability and interpretability) as they pertain to data-driven AI, and propose that reflection of them, along with improved interdisciplinary discussion may aid the mitigation of data-based AI ethical concerns, and ultimately improve individual wellbeing when interacting with AI.},
  archive      = {J_FDATA},
  author       = {Baird, Alice and Schuller, Björn},
  doi          = {10.3389/fdata.2020.00025},
  journal      = {Frontiers in Big Data},
  month        = {9},
  pages        = {527486},
  shortjournal = {Front. Big Data},
  title        = {Considerations for a more ethical approach to data in AI: On data representation and infrastructure},
  volume       = {3},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Next-generation digital ecosystem for climate data mining
and knowledge discovery: A review of digital data collection
technologies. <em>FDATA</em>, <em>3</em>, 525690. (<a
href="https://doi.org/10.3389/fdata.2020.00029">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Climate change has been called “the defining challenge of our age” and yet the global community lacks adequate information to understand whether actions to address it are succeeding or failing to mitigate it. The emergence of technologies such as earth observation (EO) and Internet-of-Things (IoT) promises to provide new advances in data collection for monitoring climate change mitigation, particularly where traditional means of data exploration and analysis, such as government-led statistical census efforts, are costly and time consuming. In this review article, we examine the extent to which digital data technologies, such as EO (e.g., remote sensing satellites, unmanned aerial vehicles or UAVs, generally from space) and IoT (e.g., smart meters, sensors, and actuators, generally from the ground) can address existing gaps that impede efforts to evaluate progress toward global climate change mitigation. We argue that there is underexplored potential for EO and IoT to advance large-scale data generation that can be translated to improve climate change data collection. Finally, we discuss how a system employing digital data collection technologies could leverage advances in distributed ledger technologies to address concerns of transparency, privacy, and data governance.},
  archive      = {J_FDATA},
  author       = {Hsu, Angel and Khoo, Willie and Goyal, Nihit and Wainstein, Martin},
  doi          = {10.3389/fdata.2020.00029},
  journal      = {Frontiers in Big Data},
  month        = {9},
  pages        = {525690},
  shortjournal = {Front. Big Data},
  title        = {Next-generation digital ecosystem for climate data mining and knowledge discovery: A review of digital data collection technologies},
  volume       = {3},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Big data and the little big bang: An epistemological
(r)evolution. <em>FDATA</em>, <em>3</em>, 514759. (<a
href="https://doi.org/10.3389/fdata.2020.00031">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Starting from an analysis of frequently employed definitions of big data, it will be argued that, to overcome the intrinsic weaknesses of big data, it is more appropriate to define the object in relational terms. The excessive emphasis on volume and technological aspects of big data, derived from their current definitions, combined with neglected epistemological issues gave birth to an objectivistic rhetoric surrounding big data as implicitly neutral, omni-comprehensive, and theory-free. This rhetoric contradicts the empirical reality that embraces big data: (1) data collection is not neutral nor objective; (2) exhaustivity is a mathematical limit; and (3) interpretation and knowledge production remain both theoretically informed and subjective. Addressing these issues, big data will be interpreted as a methodological revolution carried over by evolutionary processes in technology and epistemology. By distinguishing between forms of nominal and actual access, we claim that big data promoted a new digital divide changing stakeholders, gatekeepers, and the basic rules of knowledge discovery by radically shaping the power dynamics involved in the processes of production and analysis of data.},
  archive      = {J_FDATA},
  author       = {Balazka, Dominik and Rodighiero, Dario},
  doi          = {10.3389/fdata.2020.00031},
  journal      = {Frontiers in Big Data},
  month        = {9},
  pages        = {514759},
  shortjournal = {Front. Big Data},
  title        = {Big data and the little big bang: An epistemological (R)evolution},
  volume       = {3},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Statistical disease progression modeling in alzheimer
disease. <em>FDATA</em>, <em>3</em>, 553735. (<a
href="https://doi.org/10.3389/fdata.2020.00024">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Background: The characterizing symptom of Alzheimer disease (AD) is cognitive deterioration. While much recent work has focused on defining AD as a biological construct, most patients are still diagnosed, staged, and treated based on their cognitive symptoms. But the cognitive capability of a patient at any time throughout this deterioration reflects not only the disease state, but also the effect of the cognitive decline on the patient&#39;s pre-disease cognitive capability. Patients with high pre-disease cognitive capabilities tend to score better on cognitive tests that are sensitive early in disease relative to patients with low pre-disease cognitive capabilities at a similar disease stage. Thus, a single assessment with a cognitive test is often not adequate for determining the stage of an AD patient. Repeated evaluation of patients&#39; cognition over time may improve the ability to stage AD patients, and such longitudinal assessments in combinations with biomarker assessments can help elucidate the time dynamics of biomarkers. In turn, this can potentially lead to identification of markers that are predictive of disease stage and future cognitive decline, possibly before any cognitive deficit is measurable.Methods and Findings: This article presents a class of statistical disease progression models and applies them to longitudinal cognitive scores. These non-linear mixed-effects disease progression models explicitly model disease stage, baseline cognition, and the patients&#39; individual changes in cognitive ability as latent variables. Maximum-likelihood estimation in these models induces a data-driven criterion for separating disease progression and baseline cognition. Applied to data from the Alzheimer&#39;s Disease Neuroimaging Initiative, the model estimated a timeline of cognitive decline that spans ~15 years from the earliest subjective cognitive deficits to severe AD dementia. Subsequent analyses demonstrated how direct modeling of latent factors that modify the observed data patterns provides a scaffold for understanding disease progression, biomarkers, and treatment effects along the continuous time progression of disease.Conclusions: The presented framework enables direct interpretations of factors that modify cognitive decline. The results give new insights to the value of biomarkers for staging patients and suggest alternative explanations for previous findings related to accelerated cognitive decline among highly educated patients and patients on symptomatic treatments.},
  archive      = {J_FDATA},
  author       = {Raket, Lars Lau},
  doi          = {10.3389/fdata.2020.00024},
  journal      = {Frontiers in Big Data},
  month        = {8},
  pages        = {553735},
  shortjournal = {Front. Big Data},
  title        = {Statistical disease progression modeling in alzheimer disease},
  volume       = {3},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Embodied predictions, agency, and psychosis. <em>FDATA</em>,
<em>3</em>, 540130. (<a
href="https://doi.org/10.3389/fdata.2020.00027">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Psychotic symptoms, i.e., hallucinations and delusions, involve gross departures from conscious apprehension of consensual reality; respectively, perceiving and believing things that, according to same culture peers, do not obtain. In schizophrenia, those experiences are often related to abnormal sense of control over one&#39;s own actions, often expressed as a distorted sense of agency (i.e., passivity symptoms). Cognitive and computational neuroscience have furnished an account of these experiences and beliefs in terms of the brain&#39;s generative model of the world, which underwrites inferences to the best explanation of current and future states, in order to behave adaptively. Inference then involves a reliability-based trade off of predictions and prediction errors, and psychotic symptoms may arise as departures from this inference process, either an over- or under-weighting of priors relative to prediction errors. Surprisingly, there is empirical evidence in favor of both positions. Relatedly, there is evidence for both an enhanced and a diminished sense of agency in schizophrenia. How can this be? We argue that there is more than one generative model in the brain, and that ego- and allo-centric models operate in tandem. In brief, ego-centric models implement corollary discharge signals that cancel out the effects of self-generated actions while allo-centric models compare several hypothesis regarding the causes of sensory inputs (including the self among the potential causes). The two parallel hierarchies give rise to different levels of agency, with ego-centric models subserving “feelings of agency” and allo-centric predictions giving rise to “judgements of agency.” Those two components are weighted according to their reliability and combined, generating a higher-level “sense of agency.” We suggest that in schizophrenia a failure of corollary discharges to suppress self-generated inputs results in the absence of a “feeling of agency” and in a compensatory enhancement of allo-centric priors, which might underlie hallucinations, delusions of control but also, under certain circumstances, the enhancement of “judgments of agency.” We discuss the consequences of such a model, and potential courses of action that could lead to its falsification.},
  archive      = {J_FDATA},
  author       = {Leptourgos, Pantelis and Corlett, Philip R.},
  doi          = {10.3389/fdata.2020.00027},
  journal      = {Frontiers in Big Data},
  month        = {8},
  pages        = {540130},
  shortjournal = {Front. Big Data},
  title        = {Embodied predictions, agency, and psychosis},
  volume       = {3},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). R*-grove: Balanced spatial partitioning for large-scale
datasets. <em>FDATA</em>, <em>3</em>, 522968. (<a
href="https://doi.org/10.3389/fdata.2020.00028">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The rapid growth of big spatial data urged the research community to develop several big spatial data systems. Regardless of their architecture, one of the fundamental requirements of all these systems is to spatially partition the data efficiently across machines. The core challenges of big spatial partitioning are building high spatial quality partitions while simultaneously taking advantages of distributed processing models by providing load balanced partitions. Previous works on big spatial partitioning are to reuse existing index search trees as-is, e.g., the R-tree family, STR, Kd-tree, and Quad-tree, by building a temporary tree for a sample of the input and use its leaf nodes as partition boundaries. However, we show in this paper that none of those techniques has addressed the mentioned challenges completely. This paper proposes a novel partitioning method, termed R*-Grove, which can partition very large spatial datasets into high quality partitions with excellent load balance and block utilization. This appealing property allows R*-Grove to outperform existing techniques in spatial query processing. R*-Grove can be easily integrated into any big data platforms such as Apache Spark or Apache Hadoop. Our experiments show that R*-Grove outperforms the existing partitioning techniques for big spatial data systems. With all the proposed work publicly available as open source, we envision that R*-Grove will be adopted by the community to better serve big spatial data research.},
  archive      = {J_FDATA},
  author       = {Vu, Tin and Eldawy, Ahmed},
  doi          = {10.3389/fdata.2020.00028},
  journal      = {Frontiers in Big Data},
  month        = {8},
  pages        = {522968},
  shortjournal = {Front. Big Data},
  title        = {R*-grove: Balanced spatial partitioning for large-scale datasets},
  volume       = {3},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Novel criteria for when and how to exit a COVID-19 pandemic
lockdown. <em>FDATA</em>, <em>3</em>, 559517. (<a
href="https://doi.org/10.3389/fdata.2020.00026">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the first month of 2020, severe acute respiratory syndrome coronavirus-2 (SARS-CoV-2), a novel coronavirus spreading quickly via human-to-human transmission, caused the coronavirus disease 2019 (COVID-19) pandemic. Italy installed a successful nationwide lockdown to mitigate the exponential increase of case numbers, as the basic reproduction number R0 reached 1 within 4 weeks. But is R0 really the relevant criterion as to whether or not community spreading is under control? In most parts of the world, testing largely focused on symptomatic cases, and we thus hypothesized that the true number of infected cases and relative testing capacity are better determinants to guide lockdown exit strategies. We employed the SEIR model to estimate the numbers of undocumented cases. As expected, the estimated numbers of all cases largely exceeded the reported ones in all Italian regions. Next, we used the numbers of reported and estimated cases per million of population and compared it with the respective numbers of tests. In Lombardy, as the most affected region, testing capacity per reported new case seemed between two and eight most of the time, but testing capacity per estimated new cases never reached four up to April 30. In contrast, Veneto‘s testing capacity per reported and estimated new cases were much less discrepant and were between four and 16 most of the time. As per April 30 also Marche, Lazio and other Italian regions arrived close to 16 ratio of test capacity per new estimated infection. Thus, the criterion to exit a lockdown should be decided at the level of the regions, based on the local testing capacity that should reach 16 times the estimated true number of newly infected cases as predicted.},
  archive      = {J_FDATA},
  author       = {Li, Chenyu and Romagnani, Paola and Anders, Hans-Joachim},
  doi          = {10.3389/fdata.2020.00026},
  journal      = {Frontiers in Big Data},
  month        = {7},
  pages        = {559517},
  shortjournal = {Front. Big Data},
  title        = {Novel criteria for when and how to exit a COVID-19 pandemic lockdown},
  volume       = {3},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Vulnerabilities of connectionist AI applications: Evaluation
and defense. <em>FDATA</em>, <em>3</em>, 544373. (<a
href="https://doi.org/10.3389/fdata.2020.00023">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article deals with the IT security of connectionist artificial intelligence (AI) applications, focusing on threats to integrity, one of the three IT security goals. Such threats are for instance most relevant in prominent AI computer vision applications. In order to present a holistic view on the IT security goal integrity, many additional aspects, such as interpretability, robustness and documentation are taken into account. A comprehensive list of threats and possible mitigations is presented by reviewing the state-of-the-art literature. AI-specific vulnerabilities, such as adversarial attacks and poisoning attacks are discussed in detail, together with key factors underlying them. Additionally and in contrast to former reviews, the whole AI life cycle is analyzed with respect to vulnerabilities, including the planning, data acquisition, training, evaluation and operation phases. The discussion of mitigations is likewise not restricted to the level of the AI system itself but rather advocates viewing AI systems in the context of their life cycles and their embeddings in larger IT infrastructures and hardware devices. Based on this and the observation that adaptive attackers may circumvent any single published AI-specific defense to date, the article concludes that single protective measures are not sufficient but rather multiple measures on different levels have to be combined to achieve a minimum level of IT security for AI applications.},
  archive      = {J_FDATA},
  author       = {Berghoff, Christian and Neu, Matthias and von Twickel, Arndt},
  doi          = {10.3389/fdata.2020.00023},
  journal      = {Frontiers in Big Data},
  month        = {7},
  pages        = {544373},
  shortjournal = {Front. Big Data},
  title        = {Vulnerabilities of connectionist AI applications: Evaluation and defense},
  volume       = {3},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020a). Corrigendum: FoodKG: A tool to enrich knowledge graphs
using machine learning techniques. <em>FDATA</em>, <em>3</em>, 558254.
(<a href="https://doi.org/10.3389/fdata.2020.00021">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_FDATA},
  author       = {Gharibi, Mohamed and Zachariah, Arun and Rao, Praveen},
  doi          = {10.3389/fdata.2020.00021},
  journal      = {Frontiers in Big Data},
  month        = {6},
  pages        = {558254},
  shortjournal = {Front. Big Data},
  title        = {Corrigendum: FoodKG: a tool to enrich knowledge graphs using machine learning techniques},
  volume       = {3},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Conditional MaxRS query for evolving spatial data.
<em>FDATA</em>, <em>3</em>, 518947. (<a
href="https://doi.org/10.3389/fdata.2020.00020">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We address the problem of maintaining the correct answer-sets to a novel query—Conditional Maximizing Range-Sum (C-MaxRS)—for spatial data. Given a set of 2D point objects, possibly with associated weights, the traditional MaxRS problem determines an optimal placement for an axes-parallel rectangle r so that the number—or, the weighted sum—of the objects in its interior is maximized. The peculiarities of C-MaxRS is that in many practical settings, the objects from a particular set—e.g., restaurants—can be of different types—e.g., fast-food, Asian, etc. The C-MaxRS problem deals with maximizing the overall sum—however, it also incorporates class-based constraints, i.e., placement of r such that a lower bound on the count/weighted-sum of objects of interests from particular classes is ensured. We first propose an efficient algorithm to handle the static C-MaxRS query and then extend the solution to handle dynamic settings, where new data may be inserted or some of the existing data deleted. Subsequently we focus on the specific case of bulk-updates, which is common in many applications—i.e., multiple data points being simultaneously inserted or deleted. We show that dealing with events one by one is not efficient when processing bulk updates and present a novel technique to cater to such scenarios, by creating an index over the bursty data on-the-fly and processing the collection of events in an aggregate manner. Our experiments over datasets of up to 100,000 objects show that the proposed solutions provide significant efficiency benefits over the naïve approaches.},
  archive      = {J_FDATA},
  author       = {Mas-ud Hussain, Muhammed and Mostafiz, Mir Imtiaz and Mahmud, S. M. Farabi and Trajcevski, Goce and Eunus Ali, Mohammed},
  doi          = {10.3389/fdata.2020.00020},
  journal      = {Frontiers in Big Data},
  month        = {6},
  pages        = {518947},
  shortjournal = {Front. Big Data},
  title        = {Conditional MaxRS query for evolving spatial data},
  volume       = {3},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Reducing annotation burden through multimodal learning.
<em>FDATA</em>, <em>3</em>, 511516. (<a
href="https://doi.org/10.3389/fdata.2020.00019">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Choosing an optimal data fusion technique is essential when performing machine learning with multimodal data. In this study, we examined deep learning-based multimodal fusion techniques for the combined classification of radiological images and associated text reports. In our analysis, we (1) compared the classification performance of three prototypical multimodal fusion techniques: Early, Late, and Model fusion, (2) assessed the performance of multimodal compared to unimodal learning; and finally (3) investigated the amount of labeled data needed by multimodal vs. unimodal models to yield comparable classification performance. Our experiments demonstrate the potential of multimodal fusion methods to yield competitive results using less training data (labeled data) than their unimodal counterparts. This was more pronounced using the Early and less so using the Model and Late fusion approaches. With increasing amount of training data, unimodal models achieved comparable results to multimodal models. Overall, our results suggest the potential of multimodal learning to decrease the need for labeled training data resulting in a lower annotation burden for domain experts.},
  archive      = {J_FDATA},
  author       = {Lopez, Kevin and Fodeh, Samah J. and Allam, Ahmed and Brandt, Cynthia A. and Krauthammer, Michael},
  doi          = {10.3389/fdata.2020.00019},
  journal      = {Frontiers in Big Data},
  month        = {6},
  pages        = {511516},
  shortjournal = {Front. Big Data},
  title        = {Reducing annotation burden through multimodal learning},
  volume       = {3},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Deep learning optimizes data-driven representation of soil
organic carbon in earth system model over the conterminous united
states. <em>FDATA</em>, <em>3</em>, 509746. (<a
href="https://doi.org/10.3389/fdata.2020.00017">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Soil organic carbon (SOC) is a key component of the global carbon cycle, yet it is not well-represented in Earth system models to accurately predict global carbon dynamics in response to climate change. This novel study integrated deep learning, data assimilation, 25,444 vertical soil profiles, and the Community Land Model version 5 (CLM5) to optimize the model representation of SOC over the conterminous United States. We firstly constrained parameters in CLM5 using observations of vertical profiles of SOC in both a batch mode (using all individual soil layers in one batch) and at individual sites (site-by-site). The estimated parameter values from the site-by-site data assimilation were then either randomly sampled (random-sampling) to generate continentally homogeneous (constant) parameter values or maximally preserved for their spatially heterogeneous distributions (varying parameter values to match the spatial patterns from the site-by-site data assimilation) so as to optimize spatial representation of SOC in CLM5 through a deep learning technique (neural networking) over the conterminous United States. Comparing modeled spatial distributions of SOC by CLM5 to observations yielded increasing predictive accuracy from default CLM5 settings (R2 = 0.32) to randomly sampled (0.36), one-batch estimated (0.43), and deep learning optimized (0.62) parameter values. While CLM5 with parameter values derived from random-sampling and one-batch methods substantially corrected the overestimated SOC storage by that with default model parameters, there were still considerable geographical biases. CLM5 with the spatially heterogeneous parameter values optimized from the neural networking method had the least estimation error and less geographical biases across the conterminous United States. Our study indicated that deep learning in combination with data assimilation can significantly improve the representation of SOC by complex land biogeochemical models.},
  archive      = {J_FDATA},
  author       = {Tao, Feng and Zhou, Zhenghu and Huang, Yuanyuan and Li, Qianyu and Lu, Xingjie and Ma, Shuang and Huang, Xiaomeng and Liang, Yishuang and Hugelius, Gustaf and Jiang, Lifen and Doughty, Russell and Ren, Zhehao and Luo, Yiqi},
  doi          = {10.3389/fdata.2020.00017},
  journal      = {Frontiers in Big Data},
  month        = {6},
  pages        = {509746},
  shortjournal = {Front. Big Data},
  title        = {Deep learning optimizes data-driven representation of soil organic carbon in earth system model over the conterminous united states},
  volume       = {3},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). The ADC API: A web API for the programmatic query of the
AIRR data commons. <em>FDATA</em>, <em>3</em>, 505478. (<a
href="https://doi.org/10.3389/fdata.2020.00022">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Adaptive Immune Receptor Repertoire (AIRR) Community is a research-driven group that is establishing a clear set of community-accepted data and metadata standards; standards-based reference implementation tools; and policies and practices for infrastructure to support the deposit, curation, storage, and use of high-throughput sequencing data from B-cell and T-cell receptor repertoires (AIRR-seq data). The AIRR Data Commons is a distributed system of data repositories that utilizes a common data model, a common query language, and common interoperability formats for storage, query, and downloading of AIRR-seq data. Here is described the principal technical standards for the AIRR Data Commons consisting of the AIRR Data Model for repertoires and rearrangements, the AIRR Data Commons (ADC) API for programmatic query of data repositories, a reference implementation for ADC API services, and tools for querying and validating data repositories that support the ADC API. AIRR-seq data repositories can become part of the AIRR Data Commons by implementing the data model and API. The AIRR Data Commons allows AIRR-seq data to be reused for novel analyses and empowers researchers to discover new biological insights about the adaptive immune system.},
  archive      = {J_FDATA},
  author       = {Christley, Scott and Aguiar, Ademar and Blanck, George and Breden, Felix and Bukhari, Syed Ahmad Chan and Busse, Christian E. and Jaglale, Jerome and Harikrishnan, Srilakshmy L. and Laserson, Uri and Peters, Bjoern and Rocha, Artur and Schramm, Chaim A. and Taylor, Sarah and Vander Heiden, Jason Anthony and Zimonja, Bojan and Watson, Corey T. and Corrie, Brian and Cowell, Lindsay G.},
  doi          = {10.3389/fdata.2020.00022},
  journal      = {Frontiers in Big Data},
  month        = {6},
  pages        = {505478},
  shortjournal = {Front. Big Data},
  title        = {The ADC API: A web API for the programmatic query of the AIRR data commons},
  volume       = {3},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Theory in, theory out: The uses of social theory in machine
learning for social science. <em>FDATA</em>, <em>3</em>, 529453. (<a
href="https://doi.org/10.3389/fdata.2020.00018">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Research at the intersection of machine learning and the social sciences has provided critical new insights into social behavior. At the same time, a variety of issues have been identified with the machine learning models used to analyze social data. These issues range from technical problems with the data used and features constructed, to problematic modeling assumptions, to limited interpretability, to the models&#39; contributions to bias and inequality. Computational researchers have sought out technical solutions to these problems. The primary contribution of the present work is to argue that there is a limit to these technical solutions. At this limit, we must instead turn to social theory. We show how social theory can be used to answer basic methodological and interpretive questions that technical solutions cannot when building machine learning models, and when assessing, comparing, and using those models. In both cases, we draw on related existing critiques, provide examples of how social theory has already been used constructively in existing work, and discuss where other existing work may have benefited from the use of specific social theories. We believe this paper can act as a guide for computer and social scientists alike to navigate the substantive questions involved in applying the tools of machine learning to social data.},
  archive      = {J_FDATA},
  author       = {Radford, Jason and Joseph, Kenneth},
  doi          = {10.3389/fdata.2020.00018},
  journal      = {Frontiers in Big Data},
  month        = {5},
  pages        = {529453},
  shortjournal = {Front. Big Data},
  title        = {Theory in, theory out: The uses of social theory in machine learning for social science},
  volume       = {3},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Variational autoencoder modular bayesian networks for
simulation of heterogeneous clinical study data. <em>FDATA</em>,
<em>3</em>, 503996. (<a
href="https://doi.org/10.3389/fdata.2020.00016">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the area of Big Data, one of the major obstacles for the progress of biomedical research is the existence of data “silos” because legal and ethical constraints often do not allow for sharing sensitive patient data from clinical studies across institutions. While federated machine learning now allows for building models from scattered data of the same format, there is still the need to investigate, mine, and understand data of separate and very differently designed clinical studies that can only be accessed within each of the data-hosting organizations. Simulation of sufficiently realistic virtual patients based on the data within each individual organization could be a way to fill this gap. In this work, we propose a new machine learning approach [Variational Autoencoder Modular Bayesian Network (VAMBN)] to learn a generative model of longitudinal clinical study data. VAMBN considers typical key aspects of such data, namely limited sample size coupled with comparable many variables of different numerical scales and statistical properties, and many missing values. We show that with VAMBN, we can simulate virtual patients in a sufficiently realistic manner while making theoretical guarantees on data privacy. In addition, VAMBN allows for simulating counterfactual scenarios. Hence, VAMBN could facilitate data sharing as well as design of clinical trials.},
  archive      = {J_FDATA},
  author       = {Gootjes-Dreesbach, Luise and Sood, Meemansa and Sahay, Akrishta and Hofmann-Apitius, Martin and Fröhlich, Holger},
  doi          = {10.3389/fdata.2020.00016},
  journal      = {Frontiers in Big Data},
  month        = {5},
  pages        = {503996},
  shortjournal = {Front. Big Data},
  title        = {Variational autoencoder modular bayesian networks for simulation of heterogeneous clinical study data},
  volume       = {3},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Accelerating spatial cross-matching on CPU-GPU hybrid
platform with CUDA and OpenACC. <em>FDATA</em>, <em>3</em>, 472603. (<a
href="https://doi.org/10.3389/fdata.2020.00014">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Spatial cross-matching operation over geospatial polygonal datasets is a highly compute-intensive yet an essential task to a wide array of real-world applications. At the same time, modern computing systems are typically equipped with multiple processing units capable of task parallelization and optimization at various levels. This mandates for the exploration of novel strategies in the geospatial domain focusing on efficient utilization of computing resources, such as CPUs and GPUs. In this paper, we present a CPU-GPU hybrid platform to accelerate the cross-matching operation of geospatial datasets. We propose a pipeline of geospatial subtasks that are dynamically scheduled to be executed on either CPU or GPU. To accommodate geospatial datasets processing on GPU using pixelization approach, we convert the floating point-valued vertices into integer-valued vertices with an adaptive scaling factor as a function of the area of minimum bounding box. We present a comparative analysis of GPU enabled cross-matching algorithm implementation in CUDA and OpenACC accelerated C++. We test our implementations over Natural Earth Data and our results indicate that although CUDA based implementations provide better performance, OpenACC accelerated implementations are more portable and extendable while still providing considerable performance gain as compared to CPU. We also investigate the effects of input data size on the IO / computation ratio and note that a larger dataset compensates for IO overheads associated with GPU computations. Finally, we demonstrate that an efficient cross-matching comparison can be achieved with a cost-effective GPU.},
  archive      = {J_FDATA},
  author       = {Baig, Furqan and Gao, Chao and Teng, Dejun and Kong, Jun and Wang, Fusheng},
  doi          = {10.3389/fdata.2020.00014},
  journal      = {Frontiers in Big Data},
  month        = {5},
  pages        = {472603},
  shortjournal = {Front. Big Data},
  title        = {Accelerating spatial cross-matching on CPU-GPU hybrid platform with CUDA and OpenACC},
  volume       = {3},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Quantitatively measuring privacy in interactive query
settings within RDBMS framework. <em>FDATA</em>, <em>3</em>, 455587. (<a
href="https://doi.org/10.3389/fdata.2020.00011">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Little attention has been paid to the measurement of risk to privacy in Database Management Systems, despite their prevalence as a modality of data access. This paper proposes PriDe, a quantitative privacy metric that provides a measure (privacy score) of privacy risk when executing queries in relational database management systems. PriDe measures the degree to which attribute values, retrieved by a principal (user) engaging in an interactive query session, represent a reduction of privacy with respect to the attribute values previously retrieved by the principal. It can be deployed in interactive query settings where the user sends SQL queries to the database and gets results at run-time and provides privacy-conscious organizations with a way to monitor the usage of the application data made available to third parties in terms of privacy. The proposed approach, without loss of generality, is applicable to BigSQL-style technologies. Additionally, the paper proposes a privacy equivalence relation that facilitates the computation of the privacy score.},
  archive      = {J_FDATA},
  author       = {Khan, Muhammad Imran and Foley, Simon N. and O&#39;Sullivan, Barry},
  doi          = {10.3389/fdata.2020.00011},
  journal      = {Frontiers in Big Data},
  month        = {5},
  pages        = {455587},
  shortjournal = {Front. Big Data},
  title        = {Quantitatively measuring privacy in interactive query settings within RDBMS framework},
  volume       = {3},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Improving diagnosis of depression with XGBOOST machine
learning model and a large biomarkers dutch dataset (n = 11,081).
<em>FDATA</em>, <em>3</em>, 523466. (<a
href="https://doi.org/10.3389/fdata.2020.00015">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Machine Learning has been on the rise and healthcare is no exception to that. In healthcare, mental health is gaining more and more space. The diagnosis of mental disorders is based upon standardized patient interviews with defined set of questions and scales which is a time consuming and costly process. Our objective was to apply the machine learning model and to evaluate to see if there is predictive power of biomarkers data to enhance the diagnosis of depression cases. In this research paper, we aimed to explore the detection of depression cases among the sample of 11,081 Dutch citizen dataset. Most of the earlier studies have balanced datasets wherein the proportion of healthy cases and unhealthy cases are equal but in our study, the dataset contains only 570 cases of self-reported depression out of 11,081 cases hence it is a class imbalance classification problem. The machine learning model built on imbalance dataset gives predictions biased toward majority class hence the model will always predict the case as no depression case even if it is a case of depression. We used different resampling strategies to address the class imbalance problem. We created multiple samples by under sampling, over sampling, over-under sampling and ROSE sampling techniques to balance the dataset and then, we applied machine learning algorithm “Extreme Gradient Boosting” (XGBoost) on each sample to classify the mental illness cases from healthy cases. The balanced accuracy, precision, recall and F1 score obtained from over-sampling and over-under sampling were more than 0.90.},
  archive      = {J_FDATA},
  author       = {Sharma, Amita and Verbeke, Willem J. M. I.},
  doi          = {10.3389/fdata.2020.00015},
  journal      = {Frontiers in Big Data},
  month        = {4},
  pages        = {523466},
  shortjournal = {Front. Big Data},
  title        = {Improving diagnosis of depression with XGBOOST machine learning model and a large biomarkers dutch dataset (n = 11,081)},
  volume       = {3},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Agricultural drought monitoring via the assimilation of SMAP
soil moisture retrievals into a global soil water balance model.
<em>FDATA</em>, <em>3</em>, 477647. (<a
href="https://doi.org/10.3389/fdata.2020.00010">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {From an agricultural perspective, drought refers to an unusual deficiency of plant available water in the root-zone of the soil profile. This paper focuses on evaluating the benefit of assimilating soil moisture retrievals from the Soil Moisture Active Passive (SMAP) mission into the USDA-FAS Palmer model for agricultural drought monitoring. This will be done by examining the standardized soil moisture anomaly index. The skill of the SMAP-enhanced Palmer model is assessed over three agricultural regions that have experienced major drought since the launch of SMAP in early 2015: (1) the 2015 drought in California (CA), USA, (2) the 2017 drought in South Africa, and (3) the 2018 mid-winter drought in Australia. During these three events, the SMAP-enhanced Palmer soil moisture estimates (PM+SMAP) are compared against the Climate Hazards group Infrared Precipitation with Stations (CHIRPS) rainfall dataset and Normalized Difference Vegetation Index (NDVI) products. Results demonstrate the benefit of assimilating SMAP and confirm its potential for improving U.S. Department of Agriculture-Foreign Agricultural Service root-zone soil moisture information generated using the Palmer model. In particular, PM+SMAP soil moisture estimates are shown to enhance the spatial variability of Palmer model root-zone soil moisture estimates and adjust the Palmer model drought response to improve its consistency with ancillary CHIRPS precipitation and NDVI information.},
  archive      = {J_FDATA},
  author       = {Mladenova, Iliana E. and Bolten, John D. and Crow, Wade and Sazib, Nazmus and Reynolds, Curt},
  doi          = {10.3389/fdata.2020.00010},
  journal      = {Frontiers in Big Data},
  month        = {4},
  pages        = {477647},
  shortjournal = {Front. Big Data},
  title        = {Agricultural drought monitoring via the assimilation of SMAP soil moisture retrievals into a global soil water balance model},
  volume       = {3},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Leveraging big data and analytics to improve food, energy,
and water system sustainability. <em>FDATA</em>, <em>3</em>, 473673. (<a
href="https://doi.org/10.3389/fdata.2020.00013">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the world population projected to grow significantly over the next few decades, and in the presence of additional stress caused by climate change and urbanization, securing the essential resources of food, energy, and water is one of the most pressing challenges that the world faces today. There is an increasing priority placed by the United Nations (UN) and US federal agencies on efforts to ensure the security of these critical resources, understand their interactions, and address common underlying challenges. At the heart of the technological challenge is data science applied to environmental data. The aim of this special publication is the focus on big data science for food, energy, and water systems (FEWSs). We describe a research methodology to frame in the FEWS context, including decision tools to aid policy makers and non-governmental organizations (NGOs) to tackle specific UN Sustainable Development Goals (SDGs). Through this exercise, we aim to improve the “supply chain” of FEWS research, from gathering and analyzing data to decision tools supporting policy makers in addressing FEWS issues in specific contexts. We discuss prior research in each of the segments to highlight shortcomings as well as future research directions.},
  archive      = {J_FDATA},
  author       = {Pitts, Joshua and Gopal, Sucharita and Ma, Yaxiong and Koch, Magaly and Boumans, Roelof M. and Kaufman, Les},
  doi          = {10.3389/fdata.2020.00013},
  journal      = {Frontiers in Big Data},
  month        = {4},
  pages        = {473673},
  shortjournal = {Front. Big Data},
  title        = {Leveraging big data and analytics to improve food, energy, and water system sustainability},
  volume       = {3},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020b). FoodKG: A tool to enrich knowledge graphs using machine
learning techniques. <em>FDATA</em>, <em>3</em>, 462001. (<a
href="https://doi.org/10.3389/fdata.2020.00012">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {While there exist a plethora of datasets on the Internet related to Food, Energy, and Water (FEW), there is a real lack of reliable methods and tools that can consume these resources. This hinders the development of novel decision-making applications utilizing knowledge graphs. In this paper, we introduce a novel software tool, called FoodKG, that enriches FEW knowledge graphs using advanced machine learning techniques. Our overarching goal is to improve decision-making and knowledge discovery as well as to provide improved search results for data scientists in the FEW domains. Given an input knowledge graph (constructed on raw FEW datasets), FoodKG enriches it with semantically related triples, relations, and images based on the original dataset terms and classes. FoodKG employs an existing graph embedding technique trained on a controlled vocabulary called AGROVOC, which is published by the Food and Agriculture Organization of the United Nations. AGROVOC includes terms and classes in the agriculture and food domains. As a result, FoodKG can enhance knowledge graphs with semantic similarity scores and relations between different classes, classify the existing entities, and allow FEW experts and researchers to use scientific terms for describing FEW concepts. The resulting model obtained after training on AGROVOC was evaluated against the state-of-the-art word embedding and knowledge graph embedding models that were trained on the same dataset. We observed that this model outperformed its competitors based on the Spearman Correlation Coefficient score.},
  archive      = {J_FDATA},
  author       = {Gharibi, Mohamed and Zachariah, Arun and Rao, Praveen},
  doi          = {10.3389/fdata.2020.00012},
  journal      = {Frontiers in Big Data},
  month        = {4},
  pages        = {462001},
  shortjournal = {Front. Big Data},
  title        = {FoodKG: A tool to enrich knowledge graphs using machine learning techniques},
  volume       = {3},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Unsupervised word embedding learning by incorporating local
and global contexts. <em>FDATA</em>, <em>3</em>, 517899. (<a
href="https://doi.org/10.3389/fdata.2020.00009">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Word embedding has benefited a broad spectrum of text analysis tasks by learning distributed word representations to encode word semantics. Word representations are typically learned by modeling local contexts of words, assuming that words sharing similar surrounding words are semantically close. We argue that local contexts can only partially define word semantics in the unsupervised word embedding learning. Global contexts, referring to the broader semantic units, such as the document or paragraph where the word appears, can capture different aspects of word semantics and complement local contexts. We propose two simple yet effective unsupervised word embedding models that jointly model both local and global contexts to learn word representations. We provide theoretical interpretations of the proposed models to demonstrate how local and global contexts are jointly modeled, assuming a generative relationship between words and contexts. We conduct a thorough evaluation on a wide range of benchmark datasets. Our quantitative analysis and case study show that despite their simplicity, our two proposed models achieve superior performance on word similarity and text classification tasks.},
  archive      = {J_FDATA},
  author       = {Meng, Yu and Huang, Jiaxin and Wang, Guangyuan and Wang, Zihan and Zhang, Chao and Han, Jiawei},
  doi          = {10.3389/fdata.2020.00009},
  journal      = {Frontiers in Big Data},
  month        = {3},
  pages        = {517899},
  shortjournal = {Front. Big Data},
  title        = {Unsupervised word embedding learning by incorporating local and global contexts},
  volume       = {3},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Robust machine learning for colorectal cancer risk
prediction and stratification. <em>FDATA</em>, <em>3</em>, 504048. (<a
href="https://doi.org/10.3389/fdata.2020.00006">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {While colorectal cancer (CRC) is third in prevalence and mortality among cancers in the United States, there is no effective method to screen the general public for CRC risk. In this study, to identify an effective mass screening method for CRC risk, we evaluated seven supervised machine learning algorithms: linear discriminant analysis, support vector machine, naive Bayes, decision tree, random forest, logistic regression, and artificial neural network. Models were trained and cross-tested with the National Health Interview Survey (NHIS) and the Prostate, Lung, Colorectal, Ovarian Cancer Screening (PLCO) datasets. Six imputation methods were used to handle missing data: mean, Gaussian, Lorentzian, one-hot encoding, Gaussian expectation-maximization, and listwise deletion. Among all of the model configurations and imputation method combinations, the artificial neural network with expectation-maximization imputation emerged as the best, having a concordance of 0.70 ± 0.02, sensitivity of 0.63 ± 0.06, and specificity of 0.82 ± 0.04. In stratifying CRC risk in the NHIS and PLCO datasets, only 2% of negative cases were misclassified as high risk and 6% of positive cases were misclassified as low risk. In modeling the CRC-free probability with Kaplan-Meier estimators, low-, medium-, and high CRC-risk groups have statistically-significant separation. Our results indicated that the trained artificial neural network can be used as an effective screening tool for early intervention and prevention of CRC in large populations.},
  archive      = {J_FDATA},
  author       = {Nartowt, Bradley J. and Hart, Gregory R. and Muhammad, Wazir and Liang, Ying and Stark, Gigi F. and Deng, Jun},
  doi          = {10.3389/fdata.2020.00006},
  journal      = {Frontiers in Big Data},
  month        = {3},
  pages        = {504048},
  shortjournal = {Front. Big Data},
  title        = {Robust machine learning for colorectal cancer risk prediction and stratification},
  volume       = {3},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). AI in healthcare: Time-series forecasting using statistical,
neural, and ensemble architectures. <em>FDATA</em>, <em>3</em>, 475663.
(<a href="https://doi.org/10.3389/fdata.2020.00004">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Both statistical and neural methods have been proposed in the literature to predict healthcare expenditures. However, less attention has been given to comparing predictions from both these methods as well as ensemble approaches in the healthcare domain. The primary objective of this paper was to evaluate different statistical, neural, and ensemble techniques in their ability to predict patients&#39; weekly average expenditures on certain pain medications. Two statistical models, persistence (baseline) and autoregressive integrated moving average (ARIMA), a multilayer perceptron (MLP) model, a long short-term memory (LSTM) model, and an ensemble model combining predictions of the ARIMA, MLP, and LSTM models were calibrated to predict the expenditures on two different pain medications. In the MLP and LSTM models, we compared the influence of shuffling of training data and dropout of certain nodes in MLPs and nodes and recurrent connections in LSTMs in layers during training. Results revealed that the ensemble model outperformed the persistence, ARIMA, MLP, and LSTM models across both pain medications. In general, not shuffling the training data and adding the dropout helped the MLP models and shuffling the training data and not adding the dropout helped the LSTM models across both medications. We highlight the implications of using statistical, neural, and ensemble methods for time-series forecasting of outcomes in the healthcare domain.},
  archive      = {J_FDATA},
  author       = {Kaushik, Shruti and Choudhury, Abhinav and Sheron, Pankaj Kumar and Dasgupta, Nataraj and Natarajan, Sayee and Pickett, Larry A. and Dutt, Varun},
  doi          = {10.3389/fdata.2020.00004},
  journal      = {Frontiers in Big Data},
  month        = {3},
  pages        = {475663},
  shortjournal = {Front. Big Data},
  title        = {AI in healthcare: Time-series forecasting using statistical, neural, and ensemble architectures},
  volume       = {3},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). On robustness of neural architecture search under label
noise. <em>FDATA</em>, <em>3</em>, 516956. (<a
href="https://doi.org/10.3389/fdata.2020.00002">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Neural architecture search (NAS), which aims at automatically seeking proper neural architectures given a specific task, has attracted extensive attention recently in supervised learning applications. In most real-world situations, the class labels provided in the training data would be noisy due to many reasons, such as subjective judgments, inadequate information, and random human errors. Existing work has demonstrated the adverse effects of label noise on the learning of weights of neural networks. These effects could become more critical in NAS since the architectures are not only trained with noisy labels but are also compared based on their performances on noisy validation sets. In this paper, we systematically explore the robustness of NAS under label noise. We show that label noise in the training and/or validation data can lead to various degrees of performance variations. Through empirical experiments, using robust loss functions can mitigate the performance degradation under symmetric label noise as well as under a simple model of class conditional label noise. We also provide a theoretical justification for this. Both empirical and theoretical results provide a strong argument in favor of employing the robust loss function in NAS under high-level noise.},
  archive      = {J_FDATA},
  author       = {Chen, Yi-Wei and Song, Qingquan and Liu, Xi and Sastry, P. S. and Hu, Xia},
  doi          = {10.3389/fdata.2020.00002},
  journal      = {Frontiers in Big Data},
  month        = {2},
  pages        = {516956},
  shortjournal = {Front. Big Data},
  title        = {On robustness of neural architecture search under label noise},
  volume       = {3},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Commentary: A robust data-driven approach identifies four
personality types across four large data sets. <em>FDATA</em>,
<em>3</em>, 503189. (<a
href="https://doi.org/10.3389/fdata.2020.00008">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_FDATA},
  author       = {Katahira, Kentaro and Kunisato, Yoshihiko and Yamashita, Yuichi and Suzuki, Shinsuke},
  doi          = {10.3389/fdata.2020.00008},
  journal      = {Frontiers in Big Data},
  month        = {2},
  pages        = {503189},
  shortjournal = {Front. Big Data},
  title        = {Commentary: A robust data-driven approach identifies four personality types across four large data sets},
  volume       = {3},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Algorithmic profiling of job seekers in austria: How
austerity politics are made effective. <em>FDATA</em>, <em>3</em>,
502780. (<a href="https://doi.org/10.3389/fdata.2020.00005">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As of 2020, the Public Employment Service Austria (AMS) makes use of algorithmic profiling of job seekers to increase the efficiency of its counseling process and the effectiveness of active labor market programs. Based on a statistical model of job seekers&#39; prospects on the labor market, the system—that has become known as the AMS algorithm—is designed to classify clients of the AMS into three categories: those with high chances to find a job within half a year, those with mediocre prospects on the job market, and those clients with a bad outlook of employment in the next 2 years. Depending on the category a particular job seeker is classified under, they will be offered differing support in (re)entering the labor market. Based in science and technology studies, critical data studies and research on fairness, accountability and transparency of algorithmic systems, this paper examines the inherent politics of the AMS algorithm. An in-depth analysis of relevant technical documentation and policy documents investigates crucial conceptual, technical, and social implications of the system. The analysis shows how the design of the algorithm is influenced by technical affordances, but also by social values, norms, and goals. A discussion of the tensions, challenges and possible biases that the system entails calls into question the objectivity and neutrality of data claims and of high hopes pinned on evidence-based decision-making. In this way, the paper sheds light on the coproduction of (semi)automated managerial practices in employment agencies and the framing of unemployment under austerity politics.},
  archive      = {J_FDATA},
  author       = {Allhutter, Doris and Cech, Florian and Fischer, Fabian and Grill, Gabriel and Mager, Astrid},
  doi          = {10.3389/fdata.2020.00005},
  journal      = {Frontiers in Big Data},
  month        = {2},
  pages        = {502780},
  shortjournal = {Front. Big Data},
  title        = {Algorithmic profiling of job seekers in austria: How austerity politics are made effective},
  volume       = {3},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). The datafication of hate: Expectations and challenges in
automated hate speech monitoring. <em>FDATA</em>, <em>3</em>, 498515.
(<a href="https://doi.org/10.3389/fdata.2020.00003">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hate speech has been identified as a pressing problem in society and several automated approaches have been designed to detect and prevent it. This paper reports and reflects upon an action research setting consisting of multi-organizational collaboration conducted during Finnish municipal elections in 2017, wherein a technical infrastructure was designed to automatically monitor candidates&#39; social media updates for hate speech. The setting allowed us to engage in a 2-fold investigation. First, the collaboration offered a unique view for exploring how hate speech emerges as a technical problem. The project developed an adequately well-working algorithmic solution using supervised machine learning. We tested the performance of various feature extraction and machine learning methods and ended up using a combination of Bag-of-Words feature extraction with Support-Vector Machines. However, an automated approach required heavy simplification, such as using rudimentary scales for classifying hate speech and a reliance on word-based approaches, while in reality hate speech is a linguistic and social phenomenon with various tones and forms. Second, the action-research-oriented setting allowed us to observe affective responses, such as the hopes, dreams, and fears related to machine learning technology. Based on participatory observations, project artifacts and documents, interviews with project participants, and online reactions to the detection project, we identified participants&#39; aspirations for effective automation as well as the level of neutrality and objectivity introduced by an algorithmic system. However, the participants expressed more critical views toward the system after the monitoring process. Our findings highlight how the powerful expectations related to technology can easily end up dominating a project dealing with a contested, topical social issue. We conclude by discussing the problematic aspects of datafying hate and suggesting some practical implications for hate speech recognition.},
  archive      = {J_FDATA},
  author       = {Laaksonen, Salla-Maaria and Haapoja, Jesse and Kinnunen, Teemu and Nelimarkka, Matti and Pöyhtäri, Reeta},
  doi          = {10.3389/fdata.2020.00003},
  journal      = {Frontiers in Big Data},
  month        = {2},
  pages        = {498515},
  shortjournal = {Front. Big Data},
  title        = {The datafication of hate: Expectations and challenges in automated hate speech monitoring},
  volume       = {3},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Types of privacy expectations. <em>FDATA</em>, <em>3</em>,
496695. (<a href="https://doi.org/10.3389/fdata.2020.00007">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Understanding user privacy expectations is important and challenging. General Data Protection Regulation (GDPR) for instance requires companies to assess user privacy expectations. Existing privacy literature has largely considered privacy expectation as a single-level construct. We show that it is a multi-level construct and people have distinct types of privacy expectations. Furthermore, the types represent distinct levels of user privacy, and, hence, there can be an ordering among the types. Inspired by expectations-related theory in non-privacy literature, we propose a conceptual model of privacy expectation with four distinct types – Desired, Predicted, Deserved and Minimum. We validate our proposed model using an empirical within-subjects study that examines the effect of privacy expectation types on participant ratings of privacy expectation in a scenario involving collection of health-related browsing activity by a bank. Results from a stratified random sample (N = 1,249), representative of United States online population (±2.8%), confirm that people have distinct types of privacy expectations. About one third of the population rates the Predicted and Minimum expectation types differently, and differences are more pronounced between younger (18–29 years) and older (60+ years) population. Therefore, studies measuring privacy expectations must explicitly account for different types of privacy expectations.},
  archive      = {J_FDATA},
  author       = {Rao, Ashwini and Pfeffer, Juergen},
  doi          = {10.3389/fdata.2020.00007},
  journal      = {Frontiers in Big Data},
  month        = {2},
  pages        = {496695},
  shortjournal = {Front. Big Data},
  title        = {Types of privacy expectations},
  volume       = {3},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Tropical cyclone track forecasting using fused deep learning
from aligned reanalysis data. <em>FDATA</em>, <em>3</em>, 506897. (<a
href="https://doi.org/10.3389/fdata.2020.00001">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The forecast of tropical cyclone trajectories is crucial for the protection of people and property. Although forecast dynamical models can provide high-precision short-term forecasts, they are computationally demanding, and current statistical forecasting models have much room for improvement given that the database of past hurricanes is constantly growing. Machine learning methods, that can capture non-linearities and complex relations, have only been scarcely tested for this application. We propose a neural network model fusing past trajectory data and reanalysis atmospheric images (wind and pressure 3D fields). We use a moving frame of reference that follows the storm center for the 24 h tracking forecast. The network is trained to estimate the longitude and latitude displacement of tropical cyclones and depressions from a large database from both hemispheres (more than 3,000 storms since 1979, sampled at a 6 h frequency). The advantage of the fused network is demonstrated and a comparison with current forecast models shows that deep learning methods could provide a valuable and complementary prediction. Moreover, our method can give a forecast for a new storm in a few seconds, which is an important asset for real-time forecasts compared to traditional forecasts.},
  archive      = {J_FDATA},
  author       = {Giffard-Roisin, Sophie and Yang, Mo and Charpiat, Guillaume and Kumler Bonfanti, Christina and Kégl, Balázs and Monteleoni, Claire},
  doi          = {10.3389/fdata.2020.00001},
  journal      = {Frontiers in Big Data},
  month        = {1},
  pages        = {506897},
  shortjournal = {Front. Big Data},
  title        = {Tropical cyclone track forecasting using fused deep learning from aligned reanalysis data},
  volume       = {3},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Neural-symbolic argumentation mining: An argument in favor
of deep learning and reasoning. <em>FDATA</em>, <em>2</em>, 506333. (<a
href="https://doi.org/10.3389/fdata.2019.00052">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning is bringing remarkable contributions to the field of argumentation mining, but the existing approaches still need to fill the gap toward performing advanced reasoning tasks. In this position paper, we posit that neural-symbolic and statistical relational learning could play a crucial role in the integration of symbolic and sub-symbolic methods to achieve this goal.},
  archive      = {J_FDATA},
  author       = {Galassi, Andrea and Kersting, Kristian and Lippi, Marco and Shao, Xiaoting and Torroni, Paolo},
  doi          = {10.3389/fdata.2019.00052},
  journal      = {Frontiers in Big Data},
  month        = {1},
  pages        = {506333},
  shortjournal = {Front. Big Data},
  title        = {Neural-symbolic argumentation mining: An argument in favor of deep learning and reasoning},
  volume       = {2},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Application of a novel subject classification scheme for a
bibliographic database using a data-driven correspondence.
<em>FDATA</em>, <em>2</em>, 495044. (<a
href="https://doi.org/10.3389/fdata.2019.00048">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A novel subject classification scheme should often be applied to a preclassified bibliographic database for the research evaluation task. Generally, adopting a new subject classification scheme is labor intensive and time consuming, and an effective and efficient approach is necessary. Hence, we propose an approach to apply a new subject classification scheme for a subject-classified database using a data-driven correspondence between the new and present ones. In this paper, we define a subject classification model of the bibliographic database comprising a topological space. Then, we show our approach based on this model, wherein forming a compact topological space is required for a novel subject classification scheme. To form the space, a correspondence between two subject classification schemes using a research project database is utilized as data. As a case study, we applied our approach to a practical example. It is a tool used as world proprietary benchmarking for research evaluation based on a citation database. We tried to add a novel subject classification of a research project database.},
  archive      = {J_FDATA},
  author       = {Kurakawa, Kei and Sun, Yuan and Ando, Satoko},
  doi          = {10.3389/fdata.2019.00048},
  journal      = {Frontiers in Big Data},
  month        = {1},
  pages        = {495044},
  shortjournal = {Front. Big Data},
  title        = {Application of a novel subject classification scheme for a bibliographic database using a data-driven correspondence},
  volume       = {2},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Attribute-aware recommender system based on collaborative
filtering: Survey and classification. <em>FDATA</em>, <em>2</em>,
490677. (<a href="https://doi.org/10.3389/fdata.2019.00049">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Attribute-aware CF models aim at rating prediction given not only the historical rating given by users to items but also the information associated with users (e.g., age), items (e.g., price), and ratings (e.g., rating time). This paper surveys work in the past decade to develop attribute-aware CF systems and finds that they can be classified into four different categories mathematically. We provide readers not only with a high-level mathematical interpretation of the existing work in this area but also with mathematical insight into each category of models. Finally, we provide in-depth experiment results comparing the effectiveness of the major models in each category.},
  archive      = {J_FDATA},
  author       = {Chen, Wen-Hao and Hsu, Chin-Chi and Lai, Yi-An and Liu, Vincent and Yeh, Mi-Yen and Lin, Shou-De},
  doi          = {10.3389/fdata.2019.00049},
  journal      = {Frontiers in Big Data},
  month        = {1},
  pages        = {490677},
  shortjournal = {Front. Big Data},
  title        = {Attribute-aware recommender system based on collaborative filtering: Survey and classification},
  volume       = {2},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Spatial pattern and environmental drivers of acid
phosphatase activity in europe. <em>FDATA</em>, <em>2</em>, 475915. (<a
href="https://doi.org/10.3389/fdata.2019.00051">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Acid phosphatase produced by plants and microbes plays a fundamental role in the recycling of soil phosphorus (P). A quantification of the spatial variation in potential acid phosphatase activity (AP) on large spatial scales and its drivers can help to reduce the uncertainty in our understanding of bio-availability of soil P. We applied two machine-learning methods (Random forests and back-propagation artificial networks) to simulate the spatial patterns of AP across Europe by scaling up 126 site observations of potential AP activity from field samples measured in the laboratory, using 12 environmental drivers as predictors. The back-propagation artificial network (BPN) method explained 58% of AP variability, more than the regression tree model (49%). In addition, BPN was able to identify the gradients in AP along three transects in Europe. Partial correlation analysis revealed that soil nutrients (total nitrogen, total P, and labile organic P) and climatic controls (annual precipitation, mean annual temperature, and temperature amplitude) were the dominant factors influencing AP variations in space. Higher AP occurred in regions with higher mean annual temperature, precipitation and higher soil total nitrogen. Soil TP and Po were non-monotonically correlated with modeled AP for Europe, indicating diffident strategies of P utilization by biomes in arid and humid area. This study helps to separate the influences of each factor on AP production and to reduce the uncertainty in estimating soil P availability. The BPN model trained with European data, however, could not produce a robust global map of AP due to the lack of representative measurements of AP for tropical regions. Filling this data gap will help us to understand the physiological basis of P-use strategies in natural soils.},
  archive      = {J_FDATA},
  author       = {Sun, Yan and Goll, Daniel S. and Ciais, Philippe and Peng, Shushi and Margalef, Olga and Asensio, Dolores and Sardans, Jordi and Peñuelas, Josep},
  doi          = {10.3389/fdata.2019.00051},
  journal      = {Frontiers in Big Data},
  month        = {1},
  pages        = {475915},
  shortjournal = {Front. Big Data},
  title        = {Spatial pattern and environmental drivers of acid phosphatase activity in europe},
  volume       = {2},
  year         = {2020},
}
</textarea>
</details></li>
</ul>

</body>
</html>
