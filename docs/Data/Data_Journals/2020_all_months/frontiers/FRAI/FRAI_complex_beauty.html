<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>FRAI_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="frai---97">FRAI - 97</h2>
<ul>
<li><details>
<summary>
(2020). Q-finder: An algorithm for credible subgroup discovery in
clinical data analysis — an application to the international diabetes
management practice study. <em>FRAI</em>, <em>3</em>, 559927. (<a
href="https://doi.org/10.3389/frai.2020.559927">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Addressing the heterogeneity of both the outcome of a disease and the treatment response to an intervention is a mandatory pathway for regulatory approval of medicines. In randomized clinical trials (RCTs), confirmatory subgroup analyses focus on the assessment of drugs in predefined subgroups, while exploratory ones allow a posteriori the identification of subsets of patients who respond differently. Within the latter area, subgroup discovery (SD) data mining approach is widely used—particularly in precision medicine—to evaluate treatment effect across different groups of patients from various data sources (be it from clinical trials or real-world data). However, both the limited consideration by standard SD algorithms of recommended criteria to define credible subgroups and the lack of statistical power of the findings after correcting for multiple testing hinder the generation of hypothesis and their acceptance by healthcare authorities and practitioners. In this paper, we present the Q-Finder algorithm that aims to generate statistically credible subgroups to answer clinical questions, such as finding drivers of natural disease progression or treatment response. It combines an exhaustive search with a cascade of filters based on metrics assessing key credibility criteria, including relative risk reduction assessment, adjustment on confounding factors, individual feature’s contribution to the subgroup’s effect, interaction tests for assessing between-subgroup treatment effect interactions and tests adjustment (multiple testing). This allows Q-Finder to directly target and assess subgroups on recommended credibility criteria. The top-k credible subgroups are then selected, while accounting for subgroups’ diversity and, possibly, clinical relevance. Those subgroups are tested on independent data to assess their consistency across databases, while preserving statistical power by limiting the number of tests. To illustrate this algorithm, we applied it on the database of the International Diabetes Management Practice Study (IDMPS) to better understand the drivers of improved glycemic control and rate of episodes of hypoglycemia in type 2 diabetics patients. We compared Q-Finder with state-of-the-art approaches from both Subgroup Identification and Knowledge Discovery in Databases literature. The results demonstrate its ability to identify and support a short list of highly credible and diverse data-driven subgroups for both prognostic and predictive tasks.},
  archive      = {J_FRAI},
  author       = {Esnault, Cyril and Gadonna, May-Line and Queyrel, Maxence and Templier, Alexandre and Zucker, Jean-Daniel},
  doi          = {10.3389/frai.2020.559927},
  journal      = {Frontiers in Artificial Intelligence},
  month        = {12},
  pages        = {559927},
  shortjournal = {Front. Artif. Intell.},
  title        = {Q-finder: An algorithm for credible subgroup discovery in clinical data analysis — an application to the international diabetes management practice study},
  volume       = {3},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Artificial intelligence: A clarification of misconceptions,
myths and desired status. <em>FRAI</em>, <em>3</em>, 524339. (<a
href="https://doi.org/10.3389/frai.2020.524339">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The field artificial intelligence (AI) was founded over 65 years ago. Starting with great hopes and ambitious goals the field progressed through various stages of popularity and has recently undergone a revival through the introduction of deep neural networks. Some problems of AI are that, so far, neither the “intelligence” nor the goals of AI are formally defined causing confusion when comparing AI to other fields. In this paper, we present a perspective on the desired and current status of AI in relation to machine learning and statistics and clarify common misconceptions and myths. Our discussion is intended to lift the veil of vagueness surrounding AI to reveal its true countenance.},
  archive      = {J_FRAI},
  author       = {Emmert-Streib, Frank and Yli-Harja, Olli and Dehmer, Matthias},
  doi          = {10.3389/frai.2020.524339},
  journal      = {Frontiers in Artificial Intelligence},
  month        = {12},
  pages        = {524339},
  shortjournal = {Front. Artif. Intell.},
  title        = {Artificial intelligence: A clarification of misconceptions, myths and desired status},
  volume       = {3},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Editorial: Article collection on the human aspects in
adaptive and personalized interactive environments. <em>FRAI</em>,
<em>3</em>, 606068. (<a
href="https://doi.org/10.3389/frai.2020.606068">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_FRAI},
  author       = {Germanakos, Panagiotis and Dimitrova, Vania Gatseva and Kleanthous, Styliani},
  doi          = {10.3389/frai.2020.606068},
  journal      = {Frontiers in Artificial Intelligence},
  month        = {11},
  pages        = {606068},
  shortjournal = {Front. Artif. Intell.},
  title        = {Editorial: Article collection on the human aspects in adaptive and personalized interactive environments},
  volume       = {3},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Using word embeddings to learn a better food ontology.
<em>FRAI</em>, <em>3</em>, 584784. (<a
href="https://doi.org/10.3389/frai.2020.584784">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Food ontologies require significant effort to create and maintain as they involve manual and time-consuming tasks, often with limited alignment to the underlying food science knowledge. We propose a semi-supervised framework for the automated ontology population from an existing ontology scaffold by using word embeddings. Having applied this on the domain of food and subsequent evaluation against an expert-curated ontology, FoodOn, we observe that the food word embeddings capture the latent relationships and characteristics of foods. The resulting ontology, which utilizes word embeddings trained from the Wikipedia corpus, has an improvement of 89.7% in precision when compared to the expert-curated ontology FoodOn (0.34 vs. 0.18, respectively, p value = 2.6 × 10–138), and it has a 43.6% shorter path distance (hops) between predicted and actual food instances (2.91 vs. 5.16, respectively, p value = 4.7 × 10–84) when compared to other methods. This work demonstrates how high-dimensional representations of food can be used to populate ontologies and paves the way for learning ontologies that integrate contextual information from a variety of sources and types.},
  archive      = {J_FRAI},
  author       = {Youn, Jason and Naravane, Tarini and Tagkopoulos, Ilias},
  doi          = {10.3389/frai.2020.584784},
  journal      = {Frontiers in Artificial Intelligence},
  month        = {11},
  pages        = {584784},
  shortjournal = {Front. Artif. Intell.},
  title        = {Using word embeddings to learn a better food ontology},
  volume       = {3},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Prognostics and health management of industrial assets:
Current progress and road ahead. <em>FRAI</em>, <em>3</em>, 578613. (<a
href="https://doi.org/10.3389/frai.2020.578613">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Prognostic and Health Management (PHM) systems are some of the main protagonists of the Industry 4.0 revolution. Efficiently detecting whether an industrial component has deviated from its normal operating condition or predicting when a fault will occur are the main challenges these systems aim at addressing. Efficient PHM methods promise to decrease the probability of extreme failure events, thus improving the safety level of industrial machines. Furthermore, they could potentially drastically reduce the often conspicuous costs associated with scheduled maintenance operations. The increasing availability of data and the stunning progress of Machine Learning (ML) and Deep Learning (DL) techniques over the last decade represent two strong motivating factors for the development of data-driven PHM systems. On the other hand, the black-box nature of DL models significantly hinders their level of interpretability, de facto limiting their application to real-world scenarios. In this work, we explore the intersection of Artificial Intelligence (AI) methods and PHM applications. We present a thorough review of existing works both in the contexts of fault diagnosis and fault prognosis, highlighting the benefits and the drawbacks introduced by the adoption of AI techniques. Our goal is to highlight potentially fruitful research directions along with characterizing the main challenges that need to be addressed in order to realize the promises of AI-based PHM systems.},
  archive      = {J_FRAI},
  author       = {Biggio, Luca and Kastanis, Iason},
  doi          = {10.3389/frai.2020.578613},
  journal      = {Frontiers in Artificial Intelligence},
  month        = {11},
  pages        = {578613},
  shortjournal = {Front. Artif. Intell.},
  title        = {Prognostics and health management of industrial assets: Current progress and road ahead},
  volume       = {3},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Predictive maintenance for injection molding machines
enabled by cognitive analytics for industry 4.0. <em>FRAI</em>,
<em>3</em>, 578152. (<a
href="https://doi.org/10.3389/frai.2020.578152">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The exploitation of big volumes of data in Industry 4.0 and the increasing development of cognitive systems strongly facilitate the realm of predictive maintenance for real-time decisions and early fault detection in manufacturing and production. Cognitive factories of Industry 4.0 aim to be flexible, adaptive, and reliable, in order to derive an efficient production scheme, handle unforeseen conditions, predict failures, and aid the decision makers. The nature of the data streams available in industrial sites and the lack of annotated reference data or expert labels create the challenge to design augmented and combined data analytics solutions. This paper introduces a cognitive analytics, self- and autonomous-learned system bearing predictive maintenance solutions for Industry 4.0. A complete methodology for real-time anomaly detection on industrial data and its application on injection molding machines are presented in this study. Ensemble prediction models are implemented on the top of supervised and unsupervised learners and build a compound prediction model of historical data utilizing different algorithms’ outputs to a common consensus. The generated models are deployed on a real-time monitoring system, detecting faults in real-time incoming data streams. The key strength of the proposed system is the cognitive mechanism which encompasses a real-time self-retraining functionality based on a novel double-oriented evaluation objective, a data-driven and a model-based one. The presented application aims to support maintenance activities from injection molding machines’ operators and demonstrate the advances that can be offered by exploiting artificial intelligence capabilities in Industry 4.0.},
  archive      = {J_FRAI},
  author       = {Rousopoulou, Vaia and Nizamis, Alexandros and Vafeiadis, Thanasis and Ioannidis, Dimosthenis and Tzovaras, Dimitrios},
  doi          = {10.3389/frai.2020.578152},
  journal      = {Frontiers in Artificial Intelligence},
  month        = {11},
  pages        = {578152},
  shortjournal = {Front. Artif. Intell.},
  title        = {Predictive maintenance for injection molding machines enabled by cognitive analytics for industry 4.0},
  volume       = {3},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Automatic detection of flavescence dorée symptoms across
white grapevine varieties using deep learning. <em>FRAI</em>,
<em>3</em>, 564878. (<a
href="https://doi.org/10.3389/frai.2020.564878">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Flavescence dorée (FD) is a grapevine disease caused by phytoplasmas and transmitted by leafhoppers that has been spreading in European vineyards despite significant efforts to control it. In this study, we aim to develop a model for the automatic detection of FD-like symptoms (which encompass other grapevine yellows symptoms). The concept is to detect likely FD-affected grapevines so that samples can be removed for FD laboratory identification, followed by uprooting if they test positive, all to be conducted quickly and without omission, thus avoiding further contamination in the fields. Developing FD-like symptoms detection models is not simple, as it requires dealing with the complexity of field conditions and FD symptoms’ expression. To address these challenges, we use deep learning, which has already been proven effective in similar contexts. More specifically, we train a Convolutional Neural Network on image patches, and convert it into a Fully Convolutional Network to perform inference. As a result, we obtain a coarse segmentation of the likely FD-affected areas while having only trained a classifier, which is less demanding in terms of annotations. We evaluate the performance of our model trained on a white grape variety, Chardonnay, across five other grape varieties with varying FD symptoms expressions. Of the two largest test datasets, the true positive rate for Chardonnay reaches 98.48% whereas for Ugni-Blanc it drops to 8.3%, underlining the need for a multi-varietal training dataset to capture the diversity of FD symptoms. To obtain more transparent results and to better understand the model’s sensitivity, we investigate its behavior using two visualization techniques, Guided Gradient-weighted Class Activation Mapping and the Uniform Manifold Approximation and Projection. Such techniques lead to a more comprehensive analysis with greater reliability, which is essential for in-field applications, and more broadly, for all applications impacting humans and the environment.},
  archive      = {J_FRAI},
  author       = {Boulent, Justine and St-Charles, Pierre-Luc and Foucher, Samuel and Théau, Jérome},
  doi          = {10.3389/frai.2020.564878},
  journal      = {Frontiers in Artificial Intelligence},
  month        = {11},
  pages        = {564878},
  shortjournal = {Front. Artif. Intell.},
  title        = {Automatic detection of flavescence dorée symptoms across white grapevine varieties using deep learning},
  volume       = {3},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Machine learning approaches reveal that the number of tests
do not matter to the prediction of global confirmed COVID-19 cases.
<em>FRAI</em>, <em>3</em>, 561801. (<a
href="https://doi.org/10.3389/frai.2020.561801">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Coronavirus disease 2019 (COVID-19) has developed into a global pandemic, affecting every nation and territory in the world. Machine learning-based approaches are useful when trying to understand the complexity behind the spread of the disease and how to contain its spread effectively. The unsupervised learning method could be useful to evaluate the shortcomings of health facilities in areas of increased infection as well as what strategies are necessary to prevent disease spread within or outside of the country. To contribute toward the well-being of society, this paper focusses on the implementation of machine learning techniques for identifying common prevailing public health care facilities and concerns related to COVID-19 as well as attitudes to infection prevention strategies held by people from different countries concerning the current pandemic situation. Regression tree, random forest, cluster analysis and principal component machine learning techniques are used to analyze the global COVID-19 data of 133 countries obtained from the Worldometer website as of April 17, 2020. The analysis revealed that there are four major clusters among the countries. Eight countries having the highest cumulative infected cases and deaths, forming the first cluster. Seven countries, United States, Spain, Italy, France, Germany, United Kingdom, and Iran, play a vital role in explaining the 60% variation of the total variations by us of the first component characterized by all variables except for the rate variables. The remaining countries explain only 20% of the variation of the total variation by use of the second component characterized by only rate variables. Most strikingly, the analysis found that the variable number of tests by the country did not play a vital role in the prediction of the cumulative number of confirmed cases.},
  archive      = {J_FRAI},
  author       = {Khan, Md Hasinur Rahaman and Hossain, Ahmed},
  doi          = {10.3389/frai.2020.561801},
  journal      = {Frontiers in Artificial Intelligence},
  month        = {11},
  pages        = {561801},
  shortjournal = {Front. Artif. Intell.},
  title        = {Machine learning approaches reveal that the number of tests do not matter to the prediction of global confirmed COVID-19 cases},
  volume       = {3},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A comparison of artificial intelligence and human doctors
for the purpose of triage and diagnosis. <em>FRAI</em>, <em>3</em>,
543405. (<a href="https://doi.org/10.3389/frai.2020.543405">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {AI virtual assistants have significant potential to alleviate the pressure on overly burdened healthcare systems by enabling patients to self-assess their symptoms and to seek further care when appropriate. For these systems to make a meaningful contribution to healthcare globally, they must be trusted by patients and healthcare professionals alike, and service the needs of patients in diverse regions and segments of the population. We developed an AI virtual assistant which provides patients with triage and diagnostic information. Crucially, the system is based on a generative model, which allows for relatively straightforward re-parameterization to reflect local disease and risk factor burden in diverse regions and population segments. This is an appealing property, particularly when considering the potential of AI systems to improve the provision of healthcare on a global scale in many regions and for both developing and developed countries. We performed a prospective validation study of the accuracy and safety of the AI system and human doctors. Importantly, we assessed the accuracy and safety of both the AI and human doctors independently against identical clinical cases and, unlike previous studies, also accounted for the information gathering process of both agents. Overall, we found that the AI system is able to provide patients with triage and diagnostic information with a level of clinical accuracy and safety comparable to that of human doctors. Through this approach and study, we hope to start building trust in AI-powered systems by directly comparing their performance to human doctors, who do not always agree with each other on the cause of patients’ symptoms or the most appropriate triage recommendation.},
  archive      = {J_FRAI},
  author       = {Baker, Adam and Perov, Yura and Middleton, Katherine and Baxter, Janie and Mullarkey, Daniel and Sangar, Davinder and Butt, Mobasher and DoRosario, Arnold and Johri, Saurabh},
  doi          = {10.3389/frai.2020.543405},
  journal      = {Frontiers in Artificial Intelligence},
  month        = {11},
  pages        = {543405},
  shortjournal = {Front. Artif. Intell.},
  title        = {A comparison of artificial intelligence and human doctors for the purpose of triage and diagnosis},
  volume       = {3},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Population-based screening for endometrial cancer: Human
vs. Machine intelligence. <em>FRAI</em>, <em>3</em>, 539879. (<a
href="https://doi.org/10.3389/frai.2020.539879">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Incidence and mortality rates of endometrial cancer are increasing, leading to increased interest in endometrial cancer risk prediction and stratification to help in screening and prevention. Previous risk models have had moderate success with the area under the curve (AUC) ranging from 0.68 to 0.77. Here we demonstrate a population-based machine learning model for endometrial cancer screening that achieves a testing AUC of 0.96.We train seven machine learning algorithms based solely on personal health data, without any genomic, imaging, biomarkers, or invasive procedures. The data come from the Prostate, Lung, Colorectal, and Ovarian Cancer Screening Trial (PLCO). We further compare our machine learning model with 15 gynecologic oncologists and primary care physicians in the stratification of endometrial cancer risk for 100 women.We find a random forest model that achieves a testing AUC of 0.96 and a neural network model that achieves a testing AUC of 0.91. We test both models in risk stratification against 15 practicing physicians. Our random forest model is 2.5 times better at identifying above-average risk women with a 2-fold reduction in the false positive rate. Our neural network model is 2 times better at identifying above-average risk women with a 3-fold reduction in the false positive rate.Our machine learning models provide a non-invasive and cost-effective way to identify high-risk sub-populations who may benefit from early screening of endometrial cancer, prior to disease onset. Through statistical biopsy of personal health data, we have identified a new and effective approach for early cancer detection and prevention for individual patients.},
  archive      = {J_FRAI},
  author       = {Hart, Gregory R. and Yan, Vanessa and Huang, Gloria S. and Liang, Ying and Nartowt, Bradley J. and Muhammad, Wazir and Deng, Jun},
  doi          = {10.3389/frai.2020.539879},
  journal      = {Frontiers in Artificial Intelligence},
  month        = {11},
  pages        = {539879},
  shortjournal = {Front. Artif. Intell.},
  title        = {Population-based screening for endometrial cancer: Human vs. machine intelligence},
  volume       = {3},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Deep learning for understanding satellite imagery: An
experimental survey. <em>FRAI</em>, <em>3</em>, 534696. (<a
href="https://doi.org/10.3389/frai.2020.534696">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Translating satellite imagery into maps requires intensive effort and time, especially leading to inaccurate maps of the affected regions during disaster and conflict. The combination of availability of recent datasets and advances in computer vision made through deep learning paved the way toward automated satellite image translation. To facilitate research in this direction, we introduce the Satellite Imagery Competition using a modified SpaceNet dataset. Participants had to come up with different segmentation models to detect positions of buildings on satellite images. In this work, we present five approaches based on improvements of U-Net and Mask R-Convolutional Neuronal Networks models, coupled with unique training adaptations using boosting algorithms, morphological filter, Conditional Random Fields and custom losses. The good results—as high as &lt;mml:math id=&quot;m1&quot; xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;A&lt;/mml:mi&gt;&lt;mml:mi&gt;P&lt;/mml:mi&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mn&gt;0.937&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:math&gt; and &lt;mml:math id=&quot;m2&quot; xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;A&lt;/mml:mi&gt;&lt;mml:mi&gt;R&lt;/mml:mi&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mn&gt;0.959&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:math&gt;—from these models demonstrate the feasibility of Deep Learning in automated satellite image annotation.},
  archive      = {J_FRAI},
  author       = {Mohanty, Sharada Prasanna and Czakon, Jakub and Kaczmarek, Kamil A. and Pyskir, Andrzej and Tarasiewicz, Piotr and Kunwar, Saket and Rohrbach, Janick and Luo, Dave and Prasad, Manjunath and Fleer, Sascha and Göpfert, Jan Philip and Tandon, Akshat and Mollard, Guillaume and Rayaprolu, Nikhil and Salathe, Marcel and Schilling, Malte},
  doi          = {10.3389/frai.2020.534696},
  journal      = {Frontiers in Artificial Intelligence},
  month        = {11},
  pages        = {534696},
  shortjournal = {Front. Artif. Intell.},
  title        = {Deep learning for understanding satellite imagery: An experimental survey},
  volume       = {3},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). E-commerce shopping motivation and the influence of
persuasive strategies. <em>FRAI</em>, <em>3</em>, 506455. (<a
href="https://doi.org/10.3389/frai.2020.00067">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Persuasive strategies are used to influence the behavior or attitude of people without coercion and are commonly used in online systems such as e-commerce systems. However, in order to make persuasive strategies more effective, research suggests that they should be tailored to groups of similar individuals. Research in the traits that are effective in tailoring or personalizing persuasive strategies is an ongoing research area. In the present study, we propose the use of shoppers&#39; online shopping motivation in tailoring six commonly used influence strategies: scarcity, authority, consensus, liking, reciprocity, and commitment. We aim to identify how these influence strategies can be tailored or personalized to e-commerce shoppers based on the online consumers&#39; motivation when shopping. To achieve this, a research model was developed using Partial Least Squares-Structural Equation Modeling (PLS-SEM) and tested by conducting a study of 226 online shoppers. The result of our structural model suggests that persuasive strategies can influence e-commerce shoppers in various ways depending on the shopping motivation of the shopper. Balanced buyers—the shoppers who typically plan their shopping ahead and are influenced by the desire to search for information online—have the strongest influence on commitment strategy and have insignificant effects on the other strategies. Convenience shoppers—those motivated to shop online because of convenience—have the strongest influence on scarcity, while store-oriented shoppers—those who are motivated by the need for social interaction and immediate possession of goods—have the strongest influence on consensus. Variety seekers—consumers who are motivated to shop online because of the opportunity to search through a variety of products and brands, on the other hand, have the strongest influence on authority.},
  archive      = {J_FRAI},
  author       = {Adaji, Ifeoma and Oyibo, Kiemute and Vassileva, Julita},
  doi          = {10.3389/frai.2020.00067},
  journal      = {Frontiers in Artificial Intelligence},
  month        = {11},
  pages        = {506455},
  shortjournal = {Front. Artif. Intell.},
  title        = {E-commerce shopping motivation and the influence of persuasive strategies},
  volume       = {3},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). On the use of AI for generation of functional music to
improve mental health. <em>FRAI</em>, <em>3</em>, 497864. (<a
href="https://doi.org/10.3389/frai.2020.497864">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Increasingly music has been shown to have both physical and mental health benefits including improvements in cardiovascular health, a link to reduction of cases of dementia in elderly populations, and improvements in markers of general mental well-being such as stress reduction. Here, we describe short case studies addressing general mental well-being (anxiety, stress-reduction) through AI-driven music generation. Engaging in active listening and music-making activities (especially for at risk age groups) can be particularly beneficial, and the practice of music therapy has been shown to be helpful in a range of use cases across a wide age range. However, access to music-making can be prohibitive in terms of access to expertize, materials, and cost. Furthermore the use of existing music for functional outcomes (such as targeted improvement in physical and mental health markers suggested above) can be hindered by issues of repetition and subsequent over-familiarity with existing material. In this paper, we describe machine learning approaches which create functional music informed by biophysiological measurement across two case studies, with target emotional states at opposing ends of a Cartesian affective space (a dimensional emotion space with points ranging from descriptors from relaxation, to fear). Galvanic skin response is used as a marker of psychological arousal and as an estimate of emotional state to be used as a control signal in the training of the machine learning algorithm. This algorithm creates a non-linear time series of musical features for sound synthesis “on-the-fly”, using a perceptually informed musical feature similarity model. We find an interaction between familiarity and perceived emotional response. We also report on subsequent psychometric evaluation of the generated material, and consider how these - and similar techniques - might be useful for a range of functional music generation tasks, for example, in nonlinear sound-tracking such as that found in interactive media or video games.},
  archive      = {J_FRAI},
  author       = {Williams, Duncan and Hodge, Victoria J. and Wu, Chia-Yu},
  doi          = {10.3389/frai.2020.497864},
  journal      = {Frontiers in Artificial Intelligence},
  month        = {11},
  pages        = {497864},
  shortjournal = {Front. Artif. Intell.},
  title        = {On the use of AI for generation of functional music to improve mental health},
  volume       = {3},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Tuberculosis diagnostics and localization in chest x-rays
via deep learning models. <em>FRAI</em>, <em>3</em>, 583427. (<a
href="https://doi.org/10.3389/frai.2020.583427">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For decades, tuberculosis (TB), a potentially serious infectious lung disease, continues to be a leading cause of worldwide death. Proven to be conveniently efficient and cost-effective, chest X-ray (CXR) has become the preliminary medical imaging tool for detecting TB. Arguably, the quality of TB diagnosis will improve vastly with automated CXRs for TB detection and the localization of suspected areas, which may manifest TB. The current line of research aims to develop an efficient computer-aided detection system that will support doctors (and radiologists) to become well-informed when making TB diagnosis from patients&#39; CXRs. Here, an integrated process to improve TB diagnostics via convolutional neural networks (CNNs) and localization in CXRs via deep-learning models is proposed. Three key steps in the TB diagnostics process include (a) modifying CNN model structures, (b) model fine-tuning via artificial bee colony algorithm, and (c) the implementation of linear average–based ensemble method. Comparisons of the overall performance are made across all three steps among the experimented deep CNN models on two publicly available CXR datasets, namely, the Shenzhen Hospital CXR dataset and the National Institutes of Health CXR dataset. Validated performance includes detecting CXR abnormalities and differentiating among seven TB-related manifestations (consolidation, effusion, fibrosis, infiltration, mass, nodule, and pleural thickening). Importantly, class activation mapping is employed to inform a visual interpretation of the diagnostic result by localizing the detected lung abnormality manifestation on CXR. Compared to the state-of-the-art, the resulting approach showcases an outstanding performance both in the lung abnormality detection and the specific TB-related manifestation diagnosis vis-à-vis the localization in CXRs.},
  archive      = {J_FRAI},
  author       = {Guo, Ruihua and Passi, Kalpdrum and Jain, Chakresh Kumar},
  doi          = {10.3389/frai.2020.583427},
  journal      = {Frontiers in Artificial Intelligence},
  month        = {10},
  pages        = {583427},
  shortjournal = {Front. Artif. Intell.},
  title        = {Tuberculosis diagnostics and localization in chest X-rays via deep learning models},
  volume       = {3},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Perceptions of artificial intelligence among healthcare
staff: A qualitative survey study. <em>FRAI</em>, <em>3</em>, 578983.
(<a href="https://doi.org/10.3389/frai.2020.578983">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Objectives: The medical community is in agreement that artificial intelligence (AI) will have a radical impact on patient care in the near future. The purpose of this study is to assess the awareness of AI technologies among health professionals and to investigate their perceptions toward AI applications in medicine.Design: A web-based Google Forms survey was distributed via the Royal Free London NHS Foundation Trust e-newsletter.Setting: Only staff working at the NHS Foundation Trust received an invitation to complete the online questionnaire.Participants: 98 healthcare professionals out of 7,538 (response rate 1.3%; CI 95%; margin of error 9.64%) completed the survey, including medical doctors, nurses, therapists, managers, and others.Primary outcome: To investigate the prior knowledge of health professionals on the subject of AI as well as their attitudes and worries about its current and future applications.Results: 64% of respondents reported never coming across applications of AI in their work and 87% did not know the difference between machine learning and deep learning, although 50% knew at least one of the two terms. Furthermore, only 5% stated using speech recognition or transcription applications on a daily basis, while 63% never utilize them. 80% of participants believed there may be serious privacy issues associated with the use of AI and 40% considered AI to be potentially even more dangerous than nuclear weapons. However, 79% also believed AI could be useful or extremely useful in their field of work and only 10% were worried AI will replace them at their job.Conclusions: Despite agreeing on the usefulness of AI in the medical field, most health professionals lack a full understanding of the principles of AI and are worried about potential consequences of its widespread use in clinical practice. The cooperation of healthcare workers is crucial for the integration of AI into clinical practice and without it the NHS may miss out on an exceptionally rewarding opportunity. This highlights the need for better education and clear regulatory frameworks.},
  archive      = {J_FRAI},
  author       = {Castagno, Simone and Khalifa, Mohamed},
  doi          = {10.3389/frai.2020.578983},
  journal      = {Frontiers in Artificial Intelligence},
  month        = {10},
  pages        = {578983},
  shortjournal = {Front. Artif. Intell.},
  title        = {Perceptions of artificial intelligence among healthcare staff: A qualitative survey study},
  volume       = {3},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Adjusting for conditional bias in process model simulations
of hydrological extremes: An experiment using the north wyke farm
platform. <em>FRAI</em>, <em>3</em>, 565859. (<a
href="https://doi.org/10.3389/frai.2020.565859">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Peak flow events can lead to flooding which can have negative impacts on human life and ecosystem services. Therefore, accurate forecasting of such peak flows is important. Physically-based process models are commonly used to simulate water flow, but they often under-predict peak events (i.e., are conditionally biased), undermining their suitability for use in flood forecasting. In this research, we explored methods to increase the accuracy of peak flow simulations from a process-based model by combining the model’s output with: a) a semi-parametric conditional extreme model and b) an extreme learning machine model. The proposed 3-model hybrid approach was evaluated using fine temporal resolution water flow data from a sub-catchment of the North Wyke Farm Platform, a grassland research station in south-west England, United Kingdom. The hybrid model was assessed objectively against its simpler constituent models using a jackknife evaluation procedure with several error and agreement indices. The proposed hybrid approach was better able to capture the dynamics of the flow process and, thereby, increase prediction accuracy of the peak flow events.},
  archive      = {J_FRAI},
  author       = {Curceac, Stelian and Atkinson, Peter M. and Milne, Alice and Wu, Lianhai and Harris, Paul},
  doi          = {10.3389/frai.2020.565859},
  journal      = {Frontiers in Artificial Intelligence},
  month        = {10},
  pages        = {565859},
  shortjournal = {Front. Artif. Intell.},
  title        = {Adjusting for conditional bias in process model simulations of hydrological extremes: An experiment using the north wyke farm platform},
  volume       = {3},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). An interpretable predictive model of vaccine utilization for
tanzania. <em>FRAI</em>, <em>3</em>, 559617. (<a
href="https://doi.org/10.3389/frai.2020.559617">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Providing accurate utilization forecasts is key to maintaining optimal vaccine stocks in any health facility. Current approaches to vaccine utilization forecasting are based on often outdated population census data, and rely on weak, low-dimensional demand forecasting models. Further, these models provide very little insights into factors that influence vaccine utilization. Here, we built a state-of-the-art, machine learning model using novel, temporally and regionally relevant vaccine utilization data. This highly multidimensional machine learning approach accurately predicted bi-weekly vaccine utilization at the individual health facility level. Specifically, we achieved a forecasting fraction error of less than two for about 45% of regional health facilities in both the Tanzania regions analyzed. Our “random forest regressor” had an average forecasting fraction error that was almost 18 times less compared to the existing system. Importantly, using our model, we gleaned several key insights into factors underlying utilization forecasts. This work serves as an important starting point to reimagining predictive health systems in the developing world by leveraging the power of Artificial Intelligence and big data.},
  archive      = {J_FRAI},
  author       = {Hariharan, Ramkumar and Sundberg, Johnna and Gallino, Giacomo and Schmidt, Ashley and Arenth, Drew and Sra, Suvrit and Fels, Benjamin},
  doi          = {10.3389/frai.2020.559617},
  journal      = {Frontiers in Artificial Intelligence},
  month        = {10},
  pages        = {559617},
  shortjournal = {Front. Artif. Intell.},
  title        = {An interpretable predictive model of vaccine utilization for tanzania},
  volume       = {3},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Prognostic value of transfer learning based features in
resectable pancreatic ductal adenocarcinoma. <em>FRAI</em>, <em>3</em>,
550890. (<a href="https://doi.org/10.3389/frai.2020.550890">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Background: Pancreatic Ductal Adenocarcinoma (PDAC) is one of the most aggressive cancers with an extremely poor prognosis. Radiomics has shown prognostic ability in multiple types of cancer including PDAC. However, the prognostic value of traditional radiomics pipelines, which are based on hand-crafted radiomic features alone is limited.Methods: Convolutional neural networks (CNNs) have been shown to outperform radiomics models in computer vision tasks. However, training a CNN from scratch requires a large sample size which is not feasible in most medical imaging studies. As an alternative solution, CNN-based transfer learning models have shown the potential for achieving reasonable performance using small datasets. In this work, we developed and validated a CNN-based transfer learning model for prognostication of overall survival in PDAC patients using two independent resectable PDAC cohorts.Results: The proposed transfer learning-based prognostication model for overall survival achieved the area under the receiver operating characteristic curve of 0.81 on the test cohort, which was significantly higher than that of the traditional radiomics model (0.54). To further assess the prognostic value of the models, the predicted probabilities of death generated from the two models were used as risk scores in a univariate Cox Proportional Hazard model and while the risk score from the traditional radiomics model was not associated with overall survival, the proposed transfer learning-based risk score had significant prognostic value with hazard ratio of 1.86 (95% Confidence Interval: 1.15–3.53, p-value: 0.04).Conclusions: This result suggests that transfer learning-based models may significantly improve prognostic performance in typical small sample size medical imaging studies.},
  archive      = {J_FRAI},
  author       = {Zhang, Yucheng and Lobo-Mueller, Edrise M. and Karanicolas, Paul and Gallinger, Steven and Haider, Masoom A. and Khalvati, Farzad},
  doi          = {10.3389/frai.2020.550890},
  journal      = {Frontiers in Artificial Intelligence},
  month        = {10},
  pages        = {550890},
  shortjournal = {Front. Artif. Intell.},
  title        = {Prognostic value of transfer learning based features in resectable pancreatic ductal adenocarcinoma},
  volume       = {3},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Artificial intelligence, big data, and mHealth: The
frontiers of the prevention of violence against children. <em>FRAI</em>,
<em>3</em>, 543305. (<a
href="https://doi.org/10.3389/frai.2020.543305">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Violence against children is a global public health threat of considerable concern. At least half of all children worldwide experience violence every year; globally, the total number of children between the ages of 2 and 17 years who have experienced violence in any given year is one billion. Based on a review of the literature, we argue that there is substantial potential for AI (and associated machine learning and big data), and mHealth approaches to be utilized to prevent and address violence at a large scale. This potential is particularly marked in low- and middle-income countries (LMIC), although whether it could translate into effective solutions at scale remains unclear. We discuss possible entry points for Artificial Intelligence (AI), big data, and mHealth approaches to violence prevention, linking these to the World Health Organization&#39;s seven INSPIRE strategies. However, such work should be approached with caution. We highlight clear directions for future work in technology-based and technology-enabled violence prevention. We argue that there is a need for good agent-based models at the level of entire cities where and when violence can occur, where local response systems are. Yet, there is a need to develop common, reliable, and valid population- and individual/family-level data on predictors of violence. These indicators could be integrated into routine health or other information systems and become the basis of Al algorithms for violence prevention and response systems. Further, data on individual help-seeking behavior, risk factors for child maltreatment, and other information which could help us to identify the parameters required to understand what happens to cause, and in response to violence, are needed. To respond to ethical issues engendered by these kinds of interventions, there must be concerted, meaningful efforts to develop participatory and user-led work in the AI space, to ensure that the privacy and profiling concerns outlined above are addressed explicitly going forward. Finally, we make the case that developing AI and other technological infrastructure will require substantial investment, particularly in LMIC.},
  archive      = {J_FRAI},
  author       = {Hunt, Xanthe and Tomlinson, Mark and Sikander, Siham and Skeen, Sarah and Marlow, Marguerite and du Toit, Stefani and Eisner, Manuel},
  doi          = {10.3389/frai.2020.543305},
  journal      = {Frontiers in Artificial Intelligence},
  month        = {10},
  pages        = {543305},
  shortjournal = {Front. Artif. Intell.},
  title        = {Artificial intelligence, big data, and mHealth: The frontiers of the prevention of violence against children},
  volume       = {3},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Attitudes toward multilingualism in luxembourg. A
comparative analysis of online news comments and crowdsourced
questionnaire data. <em>FRAI</em>, <em>3</em>, 536086. (<a
href="https://doi.org/10.3389/frai.2020.536086">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Attitudes are a fundamental characteristic of human activity. Their main function is the situational assessment of phenomena in practice to maintain action ability and to provide orientation in social interaction. In sociolinguistics, research into attitudes toward varieties and their speakers is a central component of the analysis of linguistic and cultural dynamics. In recent years, computational linguistics has also shown an increased interest in the social conditionality of language. To date, such approaches have lacked a linguistically based theory of attitudes, which, for example, enables an exact terminological differentiation between publicly taken stances and the assumed underlying attitudes. Against this backdrop, the present study contributes to the connection of sociolinguistic and computational linguistic approaches to the analysis of language attitudes. We model a free text corpus of user comments from the RTL.lu news platform using representation learning (Word2Vec). In the aggregated data, we look for contextual similarities between vector representations of words that provide evidence of stances toward multilingualism in Luxembourg. We then contrast this data with the results of a quantitative attitudes study, which was carried out as part of the crowdsourcing project “Schnëssen.” The combination of the different datasets enables the reconstruction of socially pertinent attitudes represented in public discourse. The results demonstrate the central importance of attitudes toward the different languages in Luxembourg for the cultural self-understanding of the population. We also introduce a tool for the automatic orthographic correction of Luxembourgish texts (spellux). In view of the ongoing standardization of Luxembourgish and a lack of rule knowledge in the population, orthographic variation—among other factors like code-switching or regional dialects—poses a great challenge for the automatic processing of text data. The correction tool enables the orthographic normalization of Luxembourgish texts and with that a consolidation of the vocabulary for the training of word embedding models.},
  archive      = {J_FRAI},
  author       = {Purschke, Christoph},
  doi          = {10.3389/frai.2020.536086},
  journal      = {Frontiers in Artificial Intelligence},
  month        = {10},
  pages        = {536086},
  shortjournal = {Front. Artif. Intell.},
  title        = {Attitudes toward multilingualism in luxembourg. a comparative analysis of online news comments and crowdsourced questionnaire data},
  volume       = {3},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). FinTech: A new hedge for a financial re-intermediation.
Strategy and risk perspectives. <em>FRAI</em>, <em>3</em>, 524418. (<a
href="https://doi.org/10.3389/frai.2020.00063">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The emergence of new technologies and players, along with a favorable regulatory framework (PSD2 Directive), is changing the banking industry. FinTechs and TechFins have allowed the introduction of new services and changed the way customers interact to satisfy their financial needs. The FinTech landscape is constantly evolving in the market. Different business value propositions are entering the financial services industry, moving from increasing the user&#39;s experience to developing a time to market framework for banks to innovate products, processes, and channels, increasing the cost efficiency and looking for a “partnering on order” to lighten the regulatory burdens for banks. The many businesses of banks are changing their value chains, and banks&#39; business models should do the same accordingly. Strategists could no longer take their value chains as a given; choices have to be made on what needs to be protected and maintained, what abandoned and the new on coming to make banks evolve and become more resilient in doing their job. Banking is shifting significantly from a pipeline, vertical paradigm, to open banking business models where open innovation, modularity, and ecosystem-based bank&#39;s business model may become the ongoing mainstream and paradigm to follow and develop. Opportunities and threats for banks are many and new ones to re-gaining their role in the market throughout a re-intermediation process.},
  archive      = {J_FRAI},
  author       = {Omarini, Anna},
  doi          = {10.3389/frai.2020.00063},
  journal      = {Frontiers in Artificial Intelligence},
  month        = {10},
  pages        = {524418},
  shortjournal = {Front. Artif. Intell.},
  title        = {FinTech: A new hedge for a financial re-intermediation. strategy and risk perspectives},
  volume       = {3},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Deep active inference and scene construction. <em>FRAI</em>,
<em>3</em>, 509354. (<a
href="https://doi.org/10.3389/frai.2020.509354">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Adaptive agents must act in intrinsically uncertain environments with complex latent structure. Here, we elaborate a model of visual foraging—in a hierarchical context—wherein agents infer a higher-order visual pattern (a “scene”) by sequentially sampling ambiguous cues. Inspired by previous models of scene construction—that cast perception and action as consequences of approximate Bayesian inference—we use active inference to simulate decisions of agents categorizing a scene in a hierarchically-structured setting. Under active inference, agents develop probabilistic beliefs about their environment, while actively sampling it to maximize the evidence for their internal generative model. This approximate evidence maximization (i.e., self-evidencing) comprises drives to both maximize rewards and resolve uncertainty about hidden states. This is realized via minimization of a free energy functional of posterior beliefs about both the world as well as the actions used to sample or perturb it, corresponding to perception and action, respectively. We show that active inference, in the context of hierarchical scene construction, gives rise to many empirical evidence accumulation phenomena, such as noise-sensitive reaction times and epistemic saccades. We explain these behaviors in terms of the principled drives that constitute the expected free energy, the key quantity for evaluating policies under active inference. In addition, we report novel behaviors exhibited by these active inference agents that furnish new predictions for research on evidence accumulation and perceptual decision-making. We discuss the implications of this hierarchical active inference scheme for tasks that require planned sequences of information-gathering actions to infer compositional latent structure (such as visual scene construction and sentence comprehension). This work sets the stage for future experiments to investigate active inference in relation to other formulations of evidence accumulation (e.g., drift-diffusion models) in tasks that require planning in uncertain environments with higher-order structure.},
  archive      = {J_FRAI},
  author       = {Heins, R. Conor and Mirza, M. Berk and Parr, Thomas and Friston, Karl and Kagan, Igor and Pooresmaeili, Arezoo},
  doi          = {10.3389/frai.2020.509354},
  journal      = {Frontiers in Artificial Intelligence},
  month        = {10},
  pages        = {509354},
  shortjournal = {Front. Artif. Intell.},
  title        = {Deep active inference and scene construction},
  volume       = {3},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Creating music with fuzzy logic. <em>FRAI</em>, <em>3</em>,
508712. (<a href="https://doi.org/10.3389/frai.2020.00059">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fuzzy logic is an artificial intelligence technique that has applications in many areas, due to its importance in handling uncertain inputs. Despite the great recent success of other branches of AI, such as deep neural networks, fuzzy logic is still a very powerful machine learning technique, based on expert reasoning, that can be of help in many areas of musical creativity, such as composing music, synthesizing sounds, gestural mappings in electronic instruments, parametric control of sound synthesis, audiovisual content generation or sonification. We propose that fuzzy logic is a very suitable framework for thinking and operating not only with sound and acoustic signals but also with symbolic representations of music. In this article, we discuss the application of fuzzy logic ideas to music, introduce the Fuzzy Logic Control Toolkit, a set of tools to use fuzzy logic inside the MaxMSP real-time sound synthesis environment, and show how some fuzzy logic concepts can be used and incorporated into fields, such as algorithmic composition, sound synthesis and parametric control of computer music. Finally, we discuss the composition of Incerta, an acousmatic multichannel composition as a concrete example of the application of fuzzy concepts to musical creation.},
  archive      = {J_FRAI},
  author       = {Cádiz, Rodrigo F.},
  doi          = {10.3389/frai.2020.00059},
  journal      = {Frontiers in Artificial Intelligence},
  month        = {10},
  pages        = {508712},
  shortjournal = {Front. Artif. Intell.},
  title        = {Creating music with fuzzy logic},
  volume       = {3},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Integration of AI and machine learning in radiotherapy QA.
<em>FRAI</em>, <em>3</em>, 577620. (<a
href="https://doi.org/10.3389/frai.2020.577620">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The use of machine learning and other sophisticated models to aid in prediction and decision making has become widely popular across a breadth of disciplines. Within the greater diagnostic radiology, radiation oncology, and medical physics communities promising work is being performed in tissue classification and cancer staging, outcome prediction, automated segmentation, treatment planning, and quality assurance as well as other areas. In this article, machine learning approaches are explored, highlighting specific applications in machine and patient-specific quality assurance (QA). Machine learning can analyze multiple elements of a delivery system on its performance over time including the multileaf collimator (MLC), imaging system, mechanical and dosimetric parameters. Virtual Intensity-Modulated Radiation Therapy (IMRT) QA can predict passing rates using different measurement techniques, different treatment planning systems, and different treatment delivery machines across multiple institutions. Prediction of QA passing rates and other metrics can have profound implications on the current IMRT process. Here we cover general concepts of machine learning in dosimetry and various methods used in virtual IMRT QA, as well as their clinical applications.},
  archive      = {J_FRAI},
  author       = {Chan, Maria F. and Witztum, Alon and Valdes, Gilmer},
  doi          = {10.3389/frai.2020.577620},
  journal      = {Frontiers in Artificial Intelligence},
  month        = {9},
  pages        = {577620},
  shortjournal = {Front. Artif. Intell.},
  title        = {Integration of AI and machine learning in radiotherapy QA},
  volume       = {3},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Automated assessment of peristomal skin discoloration and
leakage area using artificial intelligence. <em>FRAI</em>, <em>3</em>,
572696. (<a href="https://doi.org/10.3389/frai.2020.00072">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For people living with an ostomy, development of peristomal skin complications (PSCs) is the most common post-operative challenge. A visual sign of PSCs is discoloration (redness) of the peristomal skin often resulting from leakage of ostomy output under the baseplate. If left unattended, a mild skin condition may progress into a severe disorder; consequently, it is important to monitor discoloration and leakage patterns closely. The Ostomy Skin Tool is current state-of-the-art for evaluation of peristomal skin, but it relies on patients visiting their healthcare professional regularly. To enable close monitoring of peristomal skin over time, an automated strategy not relying on scheduled consultations is required. Several medical fields have implemented automated image analysis based on artificial intelligence, and these deep learning algorithms have become increasingly recognized as a valuable tool in healthcare. Therefore, the main objective of this study was to develop deep learning algorithms which could provide automated, consistent, and objective assessments of changes in peristomal skin discoloration and leakage patterns. A total of 614 peristomal skin images were used for development of the discoloration model, which predicted the area of the discolored peristomal skin with an accuracy of 95% alongside precision and recall scores of 79.6 and 75.0%, respectively. The algorithm predicting leakage patterns was developed based on 954 product images, and leakage area was determined with 98.8% accuracy, 75.0% precision, and 71.5% recall. Combined, these data for the first time demonstrate implementation of artificial intelligence for automated assessment of changes in peristomal skin discoloration and leakage patterns.},
  archive      = {J_FRAI},
  author       = {Andersen, Niels K. and Trøjgaard, Pernille and Herschend, Nana O. and Størling, Zenia M.},
  doi          = {10.3389/frai.2020.00072},
  journal      = {Frontiers in Artificial Intelligence},
  month        = {9},
  pages        = {572696},
  shortjournal = {Front. Artif. Intell.},
  title        = {Automated assessment of peristomal skin discoloration and leakage area using artificial intelligence},
  volume       = {3},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Fluence map prediction using deep learning models – direct
plan generation for pancreas stereotactic body radiation therapy.
<em>FRAI</em>, <em>3</em>, 555388. (<a
href="https://doi.org/10.3389/frai.2020.00068">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Purpose: Treatment planning for pancreas stereotactic body radiation therapy (SBRT) is a difficult and time-consuming task. In this study, we aim to develop a novel deep learning framework to generate clinical-quality plans by direct prediction of fluence maps from patient anatomy using convolutional neural networks (CNNs).Materials and Methods: Our proposed framework utilizes two CNNs to predict intensity-modulated radiation therapy fluence maps and generate deliverable plans: (1) Field-dose CNN predicts field-dose distributions in the region of interest using planning images and structure contours; (2) a fluence map CNN predicts the final fluence map per beam using the predicted field dose projected onto the beam&#39;s eye view. The predicted fluence maps were subsequently imported into the treatment planning system for leaf sequencing and final dose calculation (model-predicted plans). One hundred patients previously treated with pancreas SBRT were included in this retrospective study, and they were split into 85 training cases and 15 test cases. For each network, 10% of training data were randomly selected for model validation. Nine-beam benchmark plans with standardized target prescription and organ-at-risk constraints were planned by experienced clinical physicists and used as the gold standard to train the model. Model-predicted plans were compared with benchmark plans in terms of dosimetric endpoints, fluence map deliverability, and total monitor units.Results: The average time for fluence-map prediction per patient was 7.1 s. Comparing model-predicted plans with benchmark plans, target mean dose, maximum dose (0.1 cc), and D95% absolute differences in percentages of prescription were 0.1, 3.9, and 2.1%, respectively; organ-at-risk mean dose and maximum dose (0.1 cc) absolute differences were 0.2 and 4.4%, respectively. The predicted plans had fluence map gamma indices (97.69 ± 0.96% vs. 98.14 ± 0.74%) and total monitor units (2,122 ± 281 vs. 2,265 ± 373) that were comparable to the benchmark plans.Conclusions: We develop a novel deep learning framework for pancreas SBRT planning, which predicts a fluence map for each beam and can, therefore, bypass the lengthy inverse optimization process. The proposed framework could potentially change the paradigm of treatment planning by harnessing the power of deep learning to generate clinically deliverable plans in seconds.},
  archive      = {J_FRAI},
  author       = {Wang, Wentao and Sheng, Yang and Wang, Chunhao and Zhang, Jiahan and Li, Xinyi and Palta, Manisha and Czito, Brian and Willett, Christopher G. and Wu, Qiuwen and Ge, Yaorong and Yin, Fang-Fang and Wu, Q. Jackie},
  doi          = {10.3389/frai.2020.00068},
  journal      = {Frontiers in Artificial Intelligence},
  month        = {9},
  pages        = {555388},
  shortjournal = {Front. Artif. Intell.},
  title        = {Fluence map prediction using deep learning models – direct plan generation for pancreas stereotactic body radiation therapy},
  volume       = {3},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). BRAVE-NET: Fully automated arterial brain vessel
segmentation in patients with cerebrovascular disease. <em>FRAI</em>,
<em>3</em>, 552258. (<a
href="https://doi.org/10.3389/frai.2020.552258">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Introduction: Arterial brain vessel assessment is crucial for the diagnostic process in patients with cerebrovascular disease. Non-invasive neuroimaging techniques, such as time-of-flight (TOF) magnetic resonance angiography (MRA) imaging are applied in the clinical routine to depict arteries. They are, however, only visually assessed. Fully automated vessel segmentation integrated into the clinical routine could facilitate the time-critical diagnosis of vessel abnormalities and might facilitate the identification of valuable biomarkers for cerebrovascular events. In the present work, we developed and validated a new deep learning model for vessel segmentation, coined BRAVE-NET, on a large aggregated dataset of patients with cerebrovascular diseases.Methods: BRAVE-NET is a multiscale 3-D convolutional neural network (CNN) model developed on a dataset of 264 patients from three different studies enrolling patients with cerebrovascular diseases. A context path, dually capturing high- and low-resolution volumes, and deep supervision were implemented. The BRAVE-NET model was compared to a baseline Unet model and variants with only context paths and deep supervision, respectively. The models were developed and validated using high-quality manual labels as ground truth. Next to precision and recall, the performance was assessed quantitatively by Dice coefficient (DSC); average Hausdorff distance (AVD); 95-percentile Hausdorff distance (95HD); and via visual qualitative rating.Results: The BRAVE-NET performance surpassed the other models for arterial brain vessel segmentation with a DSC = 0.931, AVD = 0.165, and 95HD = 29.153. The BRAVE-NET model was also the most resistant toward false labelings as revealed by the visual analysis. The performance improvement is primarily attributed to the integration of the multiscaling context path into the 3-D Unet and to a lesser extent to the deep supervision architectural component.Discussion: We present a new state-of-the-art of arterial brain vessel segmentation tailored to cerebrovascular pathology. We provide an extensive experimental validation of the model using a large aggregated dataset encompassing a large variability of cerebrovascular disease and an external set of healthy volunteers. The framework provides the technological foundation for improving the clinical workflow and can serve as a biomarker extraction tool in cerebrovascular diseases.},
  archive      = {J_FRAI},
  author       = {Hilbert, Adam and Madai, Vince I. and Akay, Ela M. and Aydin, Orhun U. and Behland, Jonas and Sobesky, Jan and Galinovic, Ivana and Khalil, Ahmed A. and Taha, Abdel A. and Wuerfel, Jens and Dusek, Petr and Niendorf, Thoralf and Fiebach, Jochen B. and Frey, Dietmar and Livne, Michelle},
  doi          = {10.3389/frai.2020.552258},
  journal      = {Frontiers in Artificial Intelligence},
  month        = {9},
  pages        = {552258},
  shortjournal = {Front. Artif. Intell.},
  title        = {BRAVE-NET: Fully automated arterial brain vessel segmentation in patients with cerebrovascular disease},
  volume       = {3},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). INALU: Improved neural arithmetic logic unit. <em>FRAI</em>,
<em>3</em>, 548098. (<a
href="https://doi.org/10.3389/frai.2020.00071">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Neural networks have to capture mathematical relationships in order to learn various tasks. They approximate these relations implicitly and therefore often do not generalize well. The recently proposed Neural Arithmetic Logic Unit (NALU) is a novel neural architecture which is able to explicitly represent the mathematical relationships by the units of the network to learn operations such as summation, subtraction or multiplication. Although NALUs have been shown to perform well on various downstream tasks, an in-depth analysis reveals practical shortcomings by design, such as the inability to multiply or divide negative input values or training stability issues for deeper networks. We address these issues and propose an improved model architecture. We evaluate our model empirically in various settings from learning basic arithmetic operations to more complex functions. Our experiments indicate that our model solves stability issues and outperforms the original NALU model in means of arithmetic precision and convergence.},
  archive      = {J_FRAI},
  author       = {Schlör, Daniel and Ring, Markus and Hotho, Andreas},
  doi          = {10.3389/frai.2020.00071},
  journal      = {Frontiers in Artificial Intelligence},
  month        = {9},
  pages        = {548098},
  shortjournal = {Front. Artif. Intell.},
  title        = {INALU: Improved neural arithmetic logic unit},
  volume       = {3},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). One hundred years of migration discourse in the times: A
discourse-historical word vector space approach to the construction of
meaning. <em>FRAI</em>, <em>3</em>, 540637. (<a
href="https://doi.org/10.3389/frai.2020.00064">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study proposes an experimental method to trace the historical evolution of media discourse as a means to investigate the construction of collective meaning. Based on distributional semantics theory (Harris, 1954; Firth, 1957) and critical discourse theory (Wodak and Fairclough, 1997), it explores the value of merging two techniques widely employed to investigate language and meaning in two separate fields: neural word embeddings (computational linguistics) and the discourse-historical approach (DHA; Reisigl and Wodak, 2001) (applied linguistics). As a use case, we investigate the historical changes in the semantic space of public discourse of migration in the United Kingdom, and we use the Times Digital Archive (TDA) from 1900 to 2000 as dataset. For the computational part, we use the publicly available TDA word2vec models1 (Kenter et al., 2015; Martinez-Ortiz et al., 2016); these models have been trained according to sliding time windows with the specific intention to map conceptual change. We then use DHA to triangulate the results generated by the word vector models with social and historical data to identify plausible explanations for the changes in the public debate. By bringing the focus of the analysis to the level of discourse, with this method, we aim to go beyond mapping different senses expressed by single words and to add the currently missing sociohistorical and sociolinguistic depth to the computational results. The study rests on the foundation that social changes will be reflected in changes in public discourse (Couldry, 2008). Although correlation does not prove direct causation, we argue that historical events, language, and meaning should be considered as a mutually reinforcing cycle in which the language used to describe events shapes explicit meanings, which in turn trigger other events, which again will be reflected in the public discourse.},
  archive      = {J_FRAI},
  author       = {Viola, Lorella and Verheul, Jaap},
  doi          = {10.3389/frai.2020.00064},
  journal      = {Frontiers in Artificial Intelligence},
  month        = {9},
  pages        = {540637},
  shortjournal = {Front. Artif. Intell.},
  title        = {One hundred years of migration discourse in the times: A discourse-historical word vector space approach to the construction of meaning},
  volume       = {3},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Linguistic variation and change in 250 years of english
scientific writing: A data-driven approach. <em>FRAI</em>, <em>3</em>,
532252. (<a href="https://doi.org/10.3389/frai.2020.00073">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We trace the evolution of Scientific English through the Late Modern period to modern time on the basis of a comprehensive corpus composed of the Transactions and Proceedings of the Royal Society of London, the first and longest-running English scientific journal established in 1665. Specifically, we explore the linguistic imprints of specialization and diversification in the science domain which accumulate in the formation of “scientific language” and field-specific sublanguages/registers (chemistry, biology etc.). We pursue an exploratory, data-driven approach using state-of-the-art computational language models and combine them with selected information-theoretic measures (entropy, relative entropy) for comparing models along relevant dimensions of variation (time, register). Focusing on selected linguistic variables (lexis, grammar), we show how we deploy computational language models for capturing linguistic variation and change and discuss benefits and limitations.},
  archive      = {J_FRAI},
  author       = {Bizzoni, Yuri and Degaetano-Ortlieb, Stefania and Fankhauser, Peter and Teich, Elke},
  doi          = {10.3389/frai.2020.00073},
  journal      = {Frontiers in Artificial Intelligence},
  month        = {9},
  pages        = {532252},
  shortjournal = {Front. Artif. Intell.},
  title        = {Linguistic variation and change in 250 years of english scientific writing: A data-driven approach},
  volume       = {3},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Clustering and recognition of spatiotemporal features
through interpretable embedding of sequence to sequence recurrent neural
networks. <em>FRAI</em>, <em>3</em>, 527594. (<a
href="https://doi.org/10.3389/frai.2020.00070">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Encoder-decoder recurrent neural network models (RNN Seq2Seq) have achieved success in ubiquitous areas of computation and applications. They were shown to be effective in modeling data with both temporal and spatial dependencies for translation or prediction tasks. In this study, we propose an embedding approach to visualize and interpret the representation of data by these models. Furthermore, we show that the embedding is an effective method for unsupervised learning and can be utilized to estimate the optimality of model training. In particular, we demonstrate that embedding space projections of the decoder states of RNN Seq2Seq model trained on sequences prediction are organized in clusters capturing similarities and differences in the dynamics of these sequences. Such performance corresponds to an unsupervised clustering of any spatio-temporal features and can be employed for time-dependent problems such as temporal segmentation, clustering of dynamic activity, self-supervised classification, action recognition, failure prediction, etc. We test and demonstrate the application of the embedding methodology to time-sequences of 3D human body poses. We show that the methodology provides a high-quality unsupervised categorization of movements. The source code with examples is available in a Github repository1.},
  archive      = {J_FRAI},
  author       = {Su, Kun and Shlizerman, Eli},
  doi          = {10.3389/frai.2020.00070},
  journal      = {Frontiers in Artificial Intelligence},
  month        = {9},
  pages        = {527594},
  shortjournal = {Front. Artif. Intell.},
  title        = {Clustering and recognition of spatiotemporal features through interpretable embedding of sequence to sequence recurrent neural networks},
  volume       = {3},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). How risk profiles of investors affect robo-advised
portfolios. <em>FRAI</em>, <em>3</em>, 523840. (<a
href="https://doi.org/10.3389/frai.2020.00060">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Automated financial advising (robo-advising) has become an established practice in wealth management, yet very few studies have looked at the cross-section of the robo-advisors and the factors explaining the persistent variability in their portfolio allocation recommendations. Using a sample of 53 advising platforms from the US and Germany, we show that the underlying algorithms manage to identify different risk profiles, although substantial variability is evident even within the same investor types&#39; groups. The robo-advisor expertise in a particular asset class seems to play a significant role, as does the geographical location, while the breadth of the offered investment choice (number of portfolios) across the robo-advisors under study does not seem to have an effect.},
  archive      = {J_FRAI},
  author       = {Boreiko, Dmitri and Massarotti, Francesca},
  doi          = {10.3389/frai.2020.00060},
  journal      = {Frontiers in Artificial Intelligence},
  month        = {9},
  pages        = {523840},
  shortjournal = {Front. Artif. Intell.},
  title        = {How risk profiles of investors affect robo-advised portfolios},
  volume       = {3},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A bayesian account of generalist and specialist formation
under the active inference framework. <em>FRAI</em>, <em>3</em>, 520268.
(<a href="https://doi.org/10.3389/frai.2020.00069">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper offers a formal account of policy learning, or habitual behavioral optimization, under the framework of Active Inference. In this setting, habit formation becomes an autodidactic, experience-dependent process, based upon what the agent sees itself doing. We focus on the effect of environmental volatility on habit formation by simulating artificial agents operating in a partially observable Markov decision process. Specifically, we used a “two-step” maze paradigm, in which the agent has to decide whether to go left or right to secure a reward. We observe that in volatile environments with numerous reward locations, the agents learn to adopt a generalist strategy, never forming a strong habitual behavior for any preferred maze direction. Conversely, in conservative or static environments, agents adopt a specialist strategy; forming strong preferences for policies that result in approach to a small number of previously-observed reward locations. The pros and cons of the two strategies are tested and discussed. In general, specialization offers greater benefits, but only when contingencies are conserved over time. We consider the implications of this formal (Active Inference) account of policy learning for understanding the relationship between specialization and habit formation.},
  archive      = {J_FRAI},
  author       = {Chen, Anthony G. and Benrimoh, David and Parr, Thomas and Friston, Karl J.},
  doi          = {10.3389/frai.2020.00069},
  journal      = {Frontiers in Artificial Intelligence},
  month        = {9},
  pages        = {520268},
  shortjournal = {Front. Artif. Intell.},
  title        = {A bayesian account of generalist and specialist formation under the active inference framework},
  volume       = {3},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). The next generation of medical decision support: A roadmap
toward transparent expert companions. <em>FRAI</em>, <em>3</em>, 507973.
(<a href="https://doi.org/10.3389/frai.2020.507973">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Increasing quality and performance of artificial intelligence (AI) in general and machine learning (ML) in particular is followed by a wider use of these approaches in everyday life. As part of this development, ML classifiers have also gained more importance for diagnosing diseases within biomedical engineering and medical sciences. However, many of those ubiquitous high-performing ML algorithms reveal a black-box-nature, leading to opaque and incomprehensible systems that complicate human interpretations of single predictions or the whole prediction process. This puts up a serious challenge on human decision makers to develop trust, which is much needed in life-changing decision tasks. This paper is designed to answer the question how expert companion systems for decision support can be designed to be interpretable and therefore transparent and comprehensible for humans. On the other hand, an approach for interactive ML as well as human-in-the-loop-learning is demonstrated in order to integrate human expert knowledge into ML models so that humans and machines act as companions within a critical decision task. We especially address the problem of Semantic Alignment between ML classifiers and its human users as a prerequisite for semantically relevant and useful explanations as well as interactions. Our roadmap paper presents and discusses an interdisciplinary yet integrated Comprehensible Artificial Intelligence (cAI)-transition-framework with regard to the task of medical diagnosis. We explain and integrate relevant concepts and research areas to provide the reader with a hands-on-cookbook for achieving the transition from opaque black-box models to interactive, transparent, comprehensible and trustworthy systems. To make our approach tangible, we present suitable state of the art methods with regard to the medical domain and include a realization concept of our framework. The emphasis is on the concept of Mutual Explanations (ME) that we introduce as a dialog-based, incremental process in order to provide human ML users with trust, but also with stronger participation within the learning process.},
  archive      = {J_FRAI},
  author       = {Bruckert, Sebastian and Finzel, Bettina and Schmid, Ute},
  doi          = {10.3389/frai.2020.507973},
  journal      = {Frontiers in Artificial Intelligence},
  month        = {9},
  pages        = {507973},
  shortjournal = {Front. Artif. Intell.},
  title        = {The next generation of medical decision support: A roadmap toward transparent expert companions},
  volume       = {3},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Comparing constraints on contraction using bayesian
regression modeling. <em>FRAI</em>, <em>3</em>, 563398. (<a
href="https://doi.org/10.3389/frai.2020.00058">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper has three goals: (1) to document the factors shaping is-contraction in Mainstream American English; (2) to assess the extent to which these factors also shape contraction of has; (3) to use shared patterns of contraction across the two verbs to draw conclusions about how the varying forms are represented grammatically. While is has two distinct phonological forms in variation, has has three. This necessitates regression modeling which can handle non-binary response variables; I use Bayesian Markov chain Monte Carlo modeling. Through this modeling, I (1) uncover a number of novel predictors shaping contraction of is, and (2) demonstrate that many of the patterns shown by is are also in evidence for has. I also (3) argue that modeling has-variation as the product of two stages of binary choices—a common treatment of three-way variation in variationist sociolinguistics—cannot adequately explain the quantitative patterns, which are only compatible with a grammatical model under which three distinct forms vary with each other. The findings have theoretical and methodological consequences for sociolinguistic work on ternary variables.},
  archive      = {J_FRAI},
  author       = {MacKenzie, Laurel},
  doi          = {10.3389/frai.2020.00058},
  journal      = {Frontiers in Artificial Intelligence},
  month        = {8},
  pages        = {563398},
  shortjournal = {Front. Artif. Intell.},
  title        = {Comparing constraints on contraction using bayesian regression modeling},
  volume       = {3},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Artificial intelligence for COVID-19 drug discovery and
vaccine development. <em>FRAI</em>, <em>3</em>, 560670. (<a
href="https://doi.org/10.3389/frai.2020.00065">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {SARS-COV-2 has roused the scientific community with a call to action to combat the growing pandemic. At the time of this writing, there are as yet no novel antiviral agents or approved vaccines available for deployment as a frontline defense. Understanding the pathobiology of COVID-19 could aid scientists in their discovery of potent antivirals by elucidating unexplored viral pathways. One method for accomplishing this is the leveraging of computational methods to discover new candidate drugs and vaccines in silico. In the last decade, machine learning-based models, trained on specific biomolecules, have offered inexpensive and rapid implementation methods for the discovery of effective viral therapies. Given a target biomolecule, these models are capable of predicting inhibitor candidates in a structural-based manner. If enough data are presented to a model, it can aid the search for a drug or vaccine candidate by identifying patterns within the data. In this review, we focus on the recent advances of COVID-19 drug and vaccine development using artificial intelligence and the potential of intelligent training for the discovery of COVID-19 therapeutics. To facilitate applications of deep learning for SARS-COV-2, we highlight multiple molecular targets of COVID-19, inhibition of which may increase patient survival. Moreover, we present CoronaDB-AI, a dataset of compounds, peptides, and epitopes discovered either in silico or in vitro that can be potentially used for training models in order to extract COVID-19 treatment. The information and datasets provided in this review can be used to train deep learning-based models and accelerate the discovery of effective viral therapies.},
  archive      = {J_FRAI},
  author       = {Keshavarzi Arshadi, Arash and Webb, Julia and Salem, Milad and Cruz, Emmanuel and Calad-Thomson, Stacie and Ghadirian, Niloofar and Collins, Jennifer and Diez-Cecilia, Elena and Kelly, Brendan and Goodarzi, Hani and Yuan, Jiann Shiun},
  doi          = {10.3389/frai.2020.00065},
  journal      = {Frontiers in Artificial Intelligence},
  month        = {8},
  pages        = {560670},
  shortjournal = {Front. Artif. Intell.},
  title        = {Artificial intelligence for COVID-19 drug discovery and vaccine development},
  volume       = {3},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Knowledge models as teaching aid for training intensity
modulated radiation therapy planning: A lung cancer case study.
<em>FRAI</em>, <em>3</em>, 555491. (<a
href="https://doi.org/10.3389/frai.2020.00066">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Purpose: Artificial intelligence (AI) employs knowledge models that often behave as a black-box to the majority of users and are not designed to improve the skill level of users. In this study, we aim to demonstrate the feasibility that AI can serve as an effective teaching aid to train individuals to develop optimal intensity modulated radiation therapy (IMRT) plans.Methods and Materials: The training program is composed of a host of training cases and a tutoring system that consists of a front-end visualization module powered by knowledge models and a scoring system. The current tutoring system includes a beam angle prediction model and a dose-volume histogram (DVH) prediction model. The scoring system consists of physician chosen criteria for clinical plan evaluation as well as specially designed criteria for learning guidance. The training program includes six lung/mediastinum IMRT patients: one benchmark case and five training cases. A plan for the benchmark case is completed by each trainee entirely independently pre- and post-training. Five training cases cover a wide spectrum of complexity from easy (2), intermediate (1) to hard (2). Five trainees completed the training program with the help of one trainer. Plans designed by the trainees were evaluated by both the scoring system and a radiation oncologist to quantify planning quality.Results: For the benchmark case, trainees scored an average of 21.6% of the total max points pre-training and improved to an average of 51.8% post-training. In comparison, the benchmark case&#39;s clinical plans score an average of 54.1% of the total max points. Two of the five trainees&#39; post-training plans on the benchmark case were rated as comparable to the clinically delivered plans by the physician and all five were noticeably improved by the physician&#39;s standards. The total training time for each trainee ranged between 9 and 12 h.Conclusion: This first attempt at a knowledge model based training program brought unexperienced planners to a level close to experienced planners in fewer than 2 days. The proposed tutoring system can serve as an important component in an AI ecosystem that will enable clinical practitioners to effectively and confidently use KBP.},
  archive      = {J_FRAI},
  author       = {Mistro, Matt and Sheng, Yang and Ge, Yaorong and Kelsey, Chris R. and Palta, Jatinder R. and Cai, Jing and Wu, Qiuwen and Yin, Fang-Fang and Wu, Q. Jackie},
  doi          = {10.3389/frai.2020.00066},
  journal      = {Frontiers in Artificial Intelligence},
  month        = {8},
  pages        = {555491},
  shortjournal = {Front. Artif. Intell.},
  title        = {Knowledge models as teaching aid for training intensity modulated radiation therapy planning: A lung cancer case study},
  volume       = {3},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). How we do things with words: Analyzing text as social and
cultural data. <em>FRAI</em>, <em>3</em>, 552471. (<a
href="https://doi.org/10.3389/frai.2020.00062">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article we describe our experiences with computational text analysis involving rich social and cultural concepts. We hope to achieve three primary goals. First, we aim to shed light on thorny issues not always at the forefront of discussions about computational text analysis methods. Second, we hope to provide a set of key questions that can guide work in this area. Our guidance is based on our own experiences and is therefore inherently imperfect. Still, given our diversity of disciplinary backgrounds and research practices, we hope to capture a range of ideas and identify commonalities that resonate for many. This leads to our final goal: to help promote interdisciplinary collaborations. Interdisciplinary insights and partnerships are essential for realizing the full potential of any computational text analysis involving social and cultural concepts, and the more we bridge these divides, the more fruitful we believe our work will be.},
  archive      = {J_FRAI},
  author       = {Nguyen, Dong and Liakata, Maria and DeDeo, Simon and Eisenstein, Jacob and Mimno, David and Tromble, Rebekah and Winters, Jane},
  doi          = {10.3389/frai.2020.00062},
  journal      = {Frontiers in Artificial Intelligence},
  month        = {8},
  pages        = {552471},
  shortjournal = {Front. Artif. Intell.},
  title        = {How we do things with words: Analyzing text as social and cultural data},
  volume       = {3},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Exploratory-phase-free estimation of GP hyperparameters in
sequential design methods—at the example of bayesian inverse problems.
<em>FRAI</em>, <em>3</em>, 547259. (<a
href="https://doi.org/10.3389/frai.2020.00052">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Methods for sequential design of computer experiments typically consist of two phases. In the first phase, the exploratory phase, a space-filling initial design is used to estimate hyperparameters of a Gaussian process emulator (GPE) and to provide some initial global exploration of the model function. In the second phase, more design points are added one by one to improve the GPE and to solve the actual problem at hand (e.g., Bayesian optimization, estimation of failure probabilities, solving Bayesian inverse problems). In this article, we investigate whether hyperparameters can be estimated without a separate exploratory phase. Such an approach will leave hyperparameters uncertain in the first iterations, so the acquisition function (which tells where to evaluate the model function next) and the GPE-based estimator need to be adapted to non-Gaussian random fields. Numerical experiments are performed exemplarily on a sequential method for solving Bayesian inverse problems. These experiments show that hyperparameters can indeed be estimated without an exploratory phase and the resulting method works almost as efficient as if the hyperparameters had been known beforehand. This means that the estimation of hyperparameters should not be the reason for including an exploratory phase. Furthermore, we show numerical examples, where these results allow us to eliminate the exploratory phase to make the sequential design method both faster (requiring fewer model evaluations) and easier to use (requiring fewer choices by the user).},
  archive      = {J_FRAI},
  author       = {Sinsbeck, Michael and Höge, Marvin and Nowak, Wolfgang},
  doi          = {10.3389/frai.2020.00052},
  journal      = {Frontiers in Artificial Intelligence},
  month        = {8},
  pages        = {547259},
  shortjournal = {Front. Artif. Intell.},
  title        = {Exploratory-phase-free estimation of GP hyperparameters in sequential design Methods—At the example of bayesian inverse problems},
  volume       = {3},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). The greatest challenge to using AI/ML for primary health
care: Mindset or datasets? <em>FRAI</em>, <em>3</em>, 543161. (<a
href="https://doi.org/10.3389/frai.2020.00053">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The global vision for primary health care (PHC) is defined by regular access to quality care for comprehensive services throughout the course of life. However, this is not what typically happens, especially in low- and middle-income countries, where many people access the formal health system only for emergent needs. Yet, even episodic care is nearly impossible to attain due to infrastructure barriers, critical shortages of health care providers, and low-quality care. Artificial intelligence and machine learning (AI/ML) can help us revolutionize the current reality of health care into the vision of continuous health care that promotes individuals to maintain a constant healthy state. AI/ML can deliver precise recommendations to the individual, transforming patients from a passive receiver of health services into an active participant of their own care. By accounting for each individual, AI/ML can also ensure equitable coverage for entire populations with an ongoing data exchange between personal health, genomic data, public health, and environmental factors. The greatest challenge to enlisting AI/ML in the quest toward the PHC vision will be instilling a sense of responsibility with global citizens to recognize health data for the global good while prioritizing protected, individually owned data sets. Only when individuals start taking a collective approach to health data, shifting the mindset toward the goal of prevention, will the potential of AI/ML for PHC be realized. Until we overcome this challenge, the paradigm shift of the global community away from our ad hoc, reactive health system culture will not be achieved.},
  archive      = {J_FRAI},
  author       = {Troncoso, Erica L.},
  doi          = {10.3389/frai.2020.00053},
  journal      = {Frontiers in Artificial Intelligence},
  month        = {8},
  pages        = {543161},
  shortjournal = {Front. Artif. Intell.},
  title        = {The greatest challenge to using AI/ML for primary health care: Mindset or datasets?},
  volume       = {3},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A framework for the computational linguistic analysis of
dehumanization. <em>FRAI</em>, <em>3</em>, 540127. (<a
href="https://doi.org/10.3389/frai.2020.00055">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dehumanization is a pernicious psychological process that often leads to extreme intergroup bias, hate speech, and violence aimed at targeted social groups. Despite these serious consequences and the wealth of available data, dehumanization has not yet been computationally studied on a large scale. Drawing upon social psychology research, we create a computational linguistic framework for analyzing dehumanizing language by identifying linguistic correlates of salient components of dehumanization. We then apply this framework to analyze discussions of LGBTQ people in the New York Times from 1986 to 2015. Overall, we find increasingly humanizing descriptions of LGBTQ people over time. However, we find that the label homosexual has emerged to be much more strongly associated with dehumanizing attitudes than other labels, such as gay. Our proposed techniques highlight processes of linguistic variation and change in discourses surrounding marginalized groups. Furthermore, the ability to analyze dehumanizing language at a large scale has implications for automatically detecting and understanding media bias as well as abusive language online.},
  archive      = {J_FRAI},
  author       = {Mendelsohn, Julia and Tsvetkov, Yulia and Jurafsky, Dan},
  doi          = {10.3389/frai.2020.00055},
  journal      = {Frontiers in Artificial Intelligence},
  month        = {8},
  pages        = {540127},
  shortjournal = {Front. Artif. Intell.},
  title        = {A framework for the computational linguistic analysis of dehumanization},
  volume       = {3},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). College football overtime outcomes: Implications for in-game
decision-making. <em>FRAI</em>, <em>3</em>, 536651. (<a
href="https://doi.org/10.3389/frai.2020.00061">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The use of AI and machine learning in sports is increasingly prevalent, including their use for in-game strategy and tactics. This paper reports on the use of machine learning techniques, applying it to analysis of U.S. Division I-A College Football overtime games. The present overtime rules for tie games in Division I-A college football was adopted in 1996. Previous research (Rosen and Wilson, 2007) found little to suggest that the predominantly used strategy of going on defense first was advantageous. Over the past decade, even with significant transformation of new offensive and defensive strategies, college football coaches still opt for the same conventional wisdom strategy. In revisiting this analysis of overtime games using both logistic regression and inductive learning/decision tree analysis, the study validates there remains no advantage to the defense first strategy in overtime. The study found evidence that point spread (as an indicator of team strength) and red zone offense performance of both teams were useful to predict game results. Additionally, by altering the decision-making “frame,” specific scenarios are illustrated where a coach can use these machine learning discovered relationships to influence end-of-regulation game decisions that may increase their likelihood of winning whether in regulation time or in overtime.},
  archive      = {J_FRAI},
  author       = {Wilson, Rick L.},
  doi          = {10.3389/frai.2020.00061},
  journal      = {Frontiers in Artificial Intelligence},
  month        = {8},
  pages        = {536651},
  shortjournal = {Front. Artif. Intell.},
  title        = {College football overtime outcomes: Implications for in-game decision-making},
  volume       = {3},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A proxy for detecting IUGR based on gestational age
estimation in a guatemalan rural population. <em>FRAI</em>, <em>3</em>,
513791. (<a href="https://doi.org/10.3389/frai.2020.00056">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In-utero progress of fetal development is normally assessed through manual measurements taken from ultrasound images, requiring relatively expensive equipment and well-trained personnel. Such monitoring is therefore unavailable in low- and middle-income countries (LMICs), where most of the perinatal mortality and morbidity exists. The work presented here attempts to identify a proxy for IUGR, which is a significant contributor to perinatal death in LMICs, by determining gestational age (GA) from data derived from simple-to-use, low-cost one-dimensional Doppler ultrasound (1D-DUS) and blood pressure devices. A total of 114 paired 1D-DUS recordings and maternal blood pressure recordings were selected, based on previously described signal quality measures. The average length of 1D-DUS recording was 10.43 ± 1.41 min. The min/median/max systolic and diastolic maternal blood pressures were 79/102/121 and 50.5/63.5/78.5 mmHg, respectively. GA was estimated using features derived from the 1D-DUS and maternal blood pressure using a support vector regression (SVR) approach and GA based on the last menstrual period as a reference target. A total of 50 trials of 5-fold cross-validation were performed for feature selection. The final SVR model was retrained on the training data and then tested on a held-out set comprising 28 normal weight and 25 low birth weight (LBW) newborns. The mean absolute GA error with respect to the last menstrual period was found to be 0.72 and 1.01 months for the normal and LBW newborns, respectively. The mean error in the GA estimate was shown to be negatively correlated with the birth weight. Thus, if the estimated GA is lower than the (remembered) GA calculated from last menstruation, then this could be interpreted as a potential sign of IUGR associated with LBW, and referral and intervention may be necessary. The assessment system may, therefore, have an immediate impact if coupled with suitable intervention, such as nutritional supplementation. However, a prospective clinical trial is required to show the efficacy of such a metric in the detection of IUGR and the impact of the intervention.},
  archive      = {J_FRAI},
  author       = {Valderrama, Camilo E. and Marzbanrad, Faezeh and Hall-Clifford, Rachel and Rohloff, Peter and Clifford, Gari D.},
  doi          = {10.3389/frai.2020.00056},
  journal      = {Frontiers in Artificial Intelligence},
  month        = {8},
  pages        = {513791},
  shortjournal = {Front. Artif. Intell.},
  title        = {A proxy for detecting IUGR based on gestational age estimation in a guatemalan rural population},
  volume       = {3},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Automated page turner for musicians. <em>FRAI</em>,
<em>3</em>, 509433. (<a
href="https://doi.org/10.3389/frai.2020.00057">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {An increasing number of musicians are opting to use tablet devices instead of traditional print media for their music sheets since the digital medium offers the benefit of storing a lot of music in a compact space. The limited screen size of the tablet devices makes the music difficult to read and musicians often opt to display part of the music page at a time. With fewer music lines on display, the musician will then have to resort to scrolling through the music to read the entire score. This scrolling is annoying since the musicians will need to remove their hands from the instrument to interact with the tablet, causing a break in the music if this is not done quickly enough, or if the tablet is not sufficiently responsive. In this paper, we describe an alternative page turning system which automates the page turning event of the musician. By actively monitoring the musician&#39;s on-screen point of regard, the system retains the musician in the loop and thus, the page turns are attuned to the musician&#39;s position on the score. By analysing the way the musician&#39;s gaze changes between attention to the score and the instrument as well as the way musicians fixate on different parts of the score, we note that musicians often look away from the score and toward their hands, or elsewhere, when playing the instrument. As a result, the eye regions fall outside the field-of-view of the eye-gaze tracker, giving rise to erratic page-turns. To counteract this problem, we create a gaze prediction model that uses Kalman filtering to predict where the musician would be looking on the score. We evaluate our hands-free page turning system using 15 different piano songs containing different levels of difficulty, various repeats, and which also required playing in different registers on the piano, thus, evaluating the applicability of the page-turner under different conditions. Performance of the page-turner was quantified through the number of correct page turns, the number of delayed page turns, and the number of mistaken page turns. Of the 289 page turns involved in the experiment, 98.3% were successfully executed, 1.7% were delayed, while no mistaken page turns were observed.},
  archive      = {J_FRAI},
  author       = {Tabone, André and Bonnici, Alexandra and Cristina, Stefania},
  doi          = {10.3389/frai.2020.00057},
  journal      = {Frontiers in Artificial Intelligence},
  month        = {8},
  pages        = {509433},
  shortjournal = {Front. Artif. Intell.},
  title        = {Automated page turner for musicians},
  volume       = {3},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Intelligent multirobot navigation and arrival-time control
using a scalable PSO-optimized hierarchical controller. <em>FRAI</em>,
<em>3</em>, 502843. (<a
href="https://doi.org/10.3389/frai.2020.00050">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a hierarchical fuzzy logic system for precision coordination of multiple mobile agents such that they achieve simultaneous arrival at their destination positions in a cluttered urban environment. We assume that each agent is equipped with a 2D scanning Lidar to make movement decisions based on local distance and bearing information. Two solution approaches are considered and compared. Both of them are structured around a hierarchical arrangement of control modules to enable synchronization of the agents&#39; arrival times while avoiding collision with obstacles. The proposed control module controls both moving speeds and directions of the robots to achieve the simultaneous target-reaching task. The control system consists of two levels: the lower-level individual navigation control for obstacle avoidance and the higher-level coordination control to ensure the same time of arrival for all robots at their target. The first approach is based on cascading fuzzy logic controllers, and the second approach considers the use of a Long Short-Term Memory recurrent neural network module alongside fuzzy logic controllers. The parameters of all the controllers are optimized using the particle swarm optimization algorithm. To increase the scalability of the proposed control modules, an interpolation method is introduced to determine the velocity scaling factors and the searching directions of the robots. A physics-based simulator, Webots, is used as a training and testing environment for the two learning models to facilitate the deployment of codes to hardware, which will be conducted in the next phase of our research.},
  archive      = {J_FRAI},
  author       = {Chang, Yu-Cheng and Dostovalova, Anna and Lin, Chin-Teng and Kim, Jijoong},
  doi          = {10.3389/frai.2020.00050},
  journal      = {Frontiers in Artificial Intelligence},
  month        = {8},
  pages        = {502843},
  shortjournal = {Front. Artif. Intell.},
  title        = {Intelligent multirobot navigation and arrival-time control using a scalable PSO-optimized hierarchical controller},
  volume       = {3},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). An interactive visualization for feature localization in
deep neural networks. <em>FRAI</em>, <em>3</em>, 558861. (<a
href="https://doi.org/10.3389/frai.2020.00049">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep artificial neural networks have become the go-to method for many machine learning tasks. In the field of computer vision, deep convolutional neural networks achieve state-of-the-art performance for tasks such as classification, object detection, or instance segmentation. As deep neural networks become more and more complex, their inner workings become more and more opaque, rendering them a “black box” whose decision making process is no longer comprehensible. In recent years, various methods have been presented that attempt to peek inside the black box and to visualize the inner workings of deep neural networks, with a focus on deep convolutional neural networks for computer vision. These methods can serve as a toolbox to facilitate the design and inspection of neural networks for computer vision and the interpretation of the decision making process of the network. Here, we present the new tool Interactive Feature Localization in Deep neural networks (IFeaLiD) which provides a novel visualization approach to convolutional neural network layers. The tool interprets neural network layers as multivariate feature maps and visualizes the similarity between the feature vectors of individual pixels of an input image in a heat map display. The similarity display can reveal how the input image is perceived by different layers of the network and how the perception of one particular image region compares to the perception of the remaining image. IFeaLiD runs interactively in a web browser and can process even high resolution feature maps in real time by using GPU acceleration with WebGL 2. We present examples from four computer vision datasets with feature maps from different layers of a pre-trained ResNet101. IFeaLiD is open source and available online at https://ifealid.cebitec.uni-bielefeld.de.},
  archive      = {J_FRAI},
  author       = {Zurowietz, Martin and Nattkemper, Tim W.},
  doi          = {10.3389/frai.2020.00049},
  journal      = {Frontiers in Artificial Intelligence},
  month        = {7},
  pages        = {558861},
  shortjournal = {Front. Artif. Intell.},
  title        = {An interactive visualization for feature localization in deep neural networks},
  volume       = {3},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). General northern english. Exploring regional variation in
the north of england with machine learning. <em>FRAI</em>, <em>3</em>,
545883. (<a href="https://doi.org/10.3389/frai.2020.00048">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we present a novel computational approach to the analysis of accent variation. The case study is dialect leveling in the North of England, manifested as reduction of accent variation across the North and emergence of General Northern English (GNE), a pan-regional standard accent associated with middle-class speakers. We investigated this instance of dialect leveling using random forest classification, with audio data from a crowd-sourced corpus of 105 urban, mostly highly-educated speakers from five northern UK cities: Leeds, Liverpool, Manchester, Newcastle upon Tyne, and Sheffield. We trained random forest models to identify individual northern cities from a sample of other northern accents, based on first two formant measurements of full vowel systems. We tested the models using unseen data. We relied on undersampling, bagging (bootstrap aggregation) and leave-one-out cross-validation to address some challenges associated with the data set, such as unbalanced data and relatively small sample size. The accuracy of classification provides us with a measure of relative similarity between different pairs of cities, while calculating conditional feature importance allows us to identify which input features (which vowels and which formants) have the largest influence in the prediction. We do find a considerable degree of leveling, especially between Manchester, Leeds and Sheffield, although some differences persist. The features that contribute to these differences most systematically are typically not the ones discussed in previous dialect descriptions. We propose that the most systematic regional features are also not salient, and as such, they serve as sociolinguistic regional indicators. We supplement the random forest results with a more traditional variationist description of by-city vowel systems, and we use both sources of evidence to inform a description of the vowels of General Northern English.},
  archive      = {J_FRAI},
  author       = {Strycharczuk, Patrycja and López-Ibáñez, Manuel and Brown, Georgina and Leemann, Adrian},
  doi          = {10.3389/frai.2020.00048},
  journal      = {Frontiers in Artificial Intelligence},
  month        = {7},
  pages        = {545883},
  shortjournal = {Front. Artif. Intell.},
  title        = {General northern english. exploring regional variation in the north of england with machine learning},
  volume       = {3},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Size matters: Digital social networks and language change.
<em>FRAI</em>, <em>3</em>, 541586. (<a
href="https://doi.org/10.3389/frai.2020.00046">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Social networks play a role in language variation and change, and the social network theory has offered a powerful tool in modeling innovation diffusion. Networks are characterized by ties of varying strength which influence how novel information is accessed. It is widely held that weak-ties promote change, whereas strong ties lead to norm-enforcing communities that resist change. However, the model is primarily suited to investigate small ego networks, and its predictive power remains to be tested in large digital networks of mobile individuals. This article revisits the social network model in sociolinguistics and investigates network size as a crucial component in the theory. We specifically concentrate on whether the distinction between weak and strong ties levels in large networks over 100 nodes. The article presents two computational methods that can handle large and messy social media data and render them usable for analyzing networks, thus expanding the empirical and methodological basis from small-scale ethnographic observations. The first method aims to uncover broad quantitative patterns in data and utilizes a cohort-based approach to network size. The second is an algorithm-based approach that uses mutual interaction parameters on Twitter. Our results gained from both methods suggest that network size plays a role, and that the distinction between weak ties and slightly stronger ties levels out once the network size grows beyond roughly 120 nodes. This finding is closely similar to the findings in other fields of the study of social networks and calls for new research avenues in computational sociolinguistics.},
  archive      = {J_FRAI},
  author       = {Laitinen, Mikko and Fatemi, Masoud and Lundberg, Jonas},
  doi          = {10.3389/frai.2020.00046},
  journal      = {Frontiers in Artificial Intelligence},
  month        = {7},
  pages        = {541586},
  shortjournal = {Front. Artif. Intell.},
  title        = {Size matters: Digital social networks and language change},
  volume       = {3},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Using topic modeling methods for short-text data: A
comparative analysis. <em>FRAI</em>, <em>3</em>, 539160. (<a
href="https://doi.org/10.3389/frai.2020.00042">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the growth of online social network platforms and applications, large amounts of textual user-generated content are created daily in the form of comments, reviews, and short-text messages. As a result, users often find it challenging to discover useful information or more on the topic being discussed from such content. Machine learning and natural language processing algorithms are used to analyze the massive amount of textual social media data available online, including topic modeling techniques that have gained popularity in recent years. This paper investigates the topic modeling subject and its common application areas, methods, and tools. Also, we examine and compare five frequently used topic modeling methods, as applied to short textual social data, to show their benefits practically in detecting important topics. These methods are latent semantic analysis, latent Dirichlet allocation, non-negative matrix factorization, random projection, and principal component analysis. Two textual datasets were selected to evaluate the performance of included topic modeling methods based on the topic quality and some standard statistical evaluation metrics, like recall, precision, F-score, and topic coherence. As a result, latent Dirichlet allocation and non-negative matrix factorization methods delivered more meaningful extracted topics and obtained good results. The paper sheds light on some common topic modeling methods in a short-text context and provides direction for researchers who seek to apply these methods.},
  archive      = {J_FRAI},
  author       = {Albalawi, Rania and Yeap, Tet Hin and Benyoucef, Morad},
  doi          = {10.3389/frai.2020.00042},
  journal      = {Frontiers in Artificial Intelligence},
  month        = {7},
  pages        = {539160},
  shortjournal = {Front. Artif. Intell.},
  title        = {Using topic modeling methods for short-text data: A comparative analysis},
  volume       = {3},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Unanswerable questions about images and texts.
<em>FRAI</em>, <em>3</em>, 532037. (<a
href="https://doi.org/10.3389/frai.2020.00051">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Questions about a text or an image that cannot be answered raise distinctive issues for an AI. This note discusses the problem of unanswerable questions in VQA (visual question answering), in QA (textual question answering), and in AI generally.},
  archive      = {J_FRAI},
  author       = {Davis, Ernest},
  doi          = {10.3389/frai.2020.00051},
  journal      = {Frontiers in Artificial Intelligence},
  month        = {7},
  pages        = {532037},
  shortjournal = {Front. Artif. Intell.},
  title        = {Unanswerable questions about images and texts},
  volume       = {3},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Generative adversarial phonology: Modeling unsupervised
phonetic and phonological learning with neural networks. <em>FRAI</em>,
<em>3</em>, 530080. (<a
href="https://doi.org/10.3389/frai.2020.00044">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Training deep neural networks on well-understood dependencies in speech data can provide new insights into how they learn internal representations. This paper argues that acquisition of speech can be modeled as a dependency between random space and generated speech data in the Generative Adversarial Network architecture and proposes a methodology to uncover the network&#39;s internal representations that correspond to phonetic and phonological properties. The Generative Adversarial architecture is uniquely appropriate for modeling phonetic and phonological learning because the network is trained on unannotated raw acoustic data and learning is unsupervised without any language-specific assumptions or pre-assumed levels of abstraction. A Generative Adversarial Network was trained on an allophonic distribution in English, in which voiceless stops surface as aspirated word-initially before stressed vowels, except if preceded by a sibilant [s]. The network successfully learns the allophonic alternation: the network&#39;s generated speech signal contains the conditional distribution of aspiration duration. The paper proposes a technique for establishing the network&#39;s internal representations that identifies latent variables that correspond to, for example, presence of [s] and its spectral properties. By manipulating these variables, we actively control the presence of [s] and its frication amplitude in the generated outputs. This suggests that the network learns to use latent variables as an approximation of phonetic and phonological representations. Crucially, we observe that the dependencies learned in training extend beyond the training interval, which allows for additional exploration of learning representations. The paper also discusses how the network&#39;s architecture and innovative outputs resemble and differ from linguistic behavior in language acquisition, speech disorders, and speech errors, and how well-understood dependencies in speech data can help us interpret how neural networks learn their representations.},
  archive      = {J_FRAI},
  author       = {Beguš, Gašper},
  doi          = {10.3389/frai.2020.00044},
  journal      = {Frontiers in Artificial Intelligence},
  month        = {7},
  pages        = {530080},
  shortjournal = {Front. Artif. Intell.},
  title        = {Generative adversarial phonology: Modeling unsupervised phonetic and phonological learning with neural networks},
  volume       = {3},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Process mining of football event data: A novel approach for
tactical insights into the game. <em>FRAI</em>, <em>3</em>, 519116. (<a
href="https://doi.org/10.3389/frai.2020.00047">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The paper explores process mining and its usefulness for analyzing football event data. We work with professional event data provided by OPTA Sports from the European Championship in 2016. We analyze one game of a favorite team (England) against an underdog team (Iceland). The success of the underdog teams in the Euro 2016 was remarkable, and it is what made the event special. For this reason, it is interesting to compare the performance of a favorite and an underdog team by applying process mining. The goal is to show the options that these types of algorithms and visual analytics offer for the interpretation of event data in football and discuss how the gained insights can support decision makers not only in pre- and post-match analysis but also during live games as well. We show process mining techniques which can be used to gain team or individual player insights by considering the types of actions, the sequence of actions, and the order of player involvement in each sequence. Finally, we also demonstrate the detection of typical or unusual behavior by trace and sequence clustering.},
  archive      = {J_FRAI},
  author       = {Kröckel, Pavlina and Bodendorf, Freimut},
  doi          = {10.3389/frai.2020.00047},
  journal      = {Frontiers in Artificial Intelligence},
  month        = {7},
  pages        = {519116},
  shortjournal = {Front. Artif. Intell.},
  title        = {Process mining of football event data: A novel approach for tactical insights into the game},
  volume       = {3},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Opinion formation on the internet: The influence of
personality, network structure, and content on sharing messages online.
<em>FRAI</em>, <em>3</em>, 509148. (<a
href="https://doi.org/10.3389/frai.2020.00045">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Today the majority of people uses online social networks not only to stay in contact with friends, but also to find information about relevant topics, or to spread information. While a lot of research has been conducted into opinion formation, only little is known about which factors influence whether a user of online social networks disseminates information or not. To answer this question, we created an agent-based model and simulated message spreading in social networks using a latent-process model. In our model, we varied four different content types, six different network types, and we varied between a model that includes a personality model for its agents and one that did not. We found that the network type has only a weak influence on the distribution of content, whereas the message type has a clear influence on how many users receive a message. Using a personality model helped achieved more realistic outcomes.},
  archive      = {J_FRAI},
  author       = {Burbach, Laura and Halbach, Patrick and Ziefle, Martina and Calero Valdez, André},
  doi          = {10.3389/frai.2020.00045},
  journal      = {Frontiers in Artificial Intelligence},
  month        = {7},
  pages        = {509148},
  shortjournal = {Front. Artif. Intell.},
  title        = {Opinion formation on the internet: The influence of personality, network structure, and content on sharing messages online},
  volume       = {3},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). There is hope after all: Quantifying opinion and
trustworthiness in neural networks. <em>FRAI</em>, <em>3</em>, 493181.
(<a href="https://doi.org/10.3389/frai.2020.00054">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Artificial Intelligence (AI) plays a fundamental role in the modern world, especially when used as an autonomous decision maker. One common concern nowadays is “how trustworthy the AIs are.” Human operators follow a strict educational curriculum and performance assessment that could be exploited to quantify how much we entrust them. To quantify the trust of AI decision makers, we must go beyond task accuracy especially when facing limited, incomplete, misleading, controversial or noisy datasets. Toward addressing these challenges, we describe DeepTrust, a Subjective Logic (SL) inspired framework that constructs a probabilistic logic description of an AI algorithm and takes into account the trustworthiness of both dataset and inner algorithmic workings. DeepTrust identifies proper multi-layered neural network (NN) topologies that have high projected trust probabilities, even when trained with untrusted data. We show that uncertain opinion of data is not always malicious while evaluating NN&#39;s opinion and trustworthiness, whereas the disbelief opinion hurts trust the most. Also trust probability does not necessarily correlate with accuracy. DeepTrust also provides a projected trust probability of NN&#39;s prediction, which is useful when the NN generates an over-confident output under problematic datasets. These findings open new analytical avenues for designing and improving the NN topology by optimizing opinion and trustworthiness, along with accuracy, in a multi-objective optimization formulation, subject to space and time constraints.},
  archive      = {J_FRAI},
  author       = {Cheng, Mingxi and Nazarian, Shahin and Bogdan, Paul},
  doi          = {10.3389/frai.2020.00054},
  journal      = {Frontiers in Artificial Intelligence},
  month        = {7},
  pages        = {493181},
  shortjournal = {Front. Artif. Intell.},
  title        = {There is hope after all: Quantifying opinion and trustworthiness in neural networks},
  volume       = {3},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Are GRU cells more specific and LSTM cells more sensitive in
motive classification of text? <em>FRAI</em>, <em>3</em>, 539404. (<a
href="https://doi.org/10.3389/frai.2020.00040">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the Thematic Apperception Test, a picture story exercise (TAT/PSE; Heckhausen, 1963), it is assumed that unconscious motives can be detected in the text someone is telling about pictures shown in the test. Therefore, this text is classified by trained experts regarding evaluation rules. We tried to automate this coding and used a recurrent neuronal network (RNN) because of the sequential input data. There are two different cell types to improve recurrent neural networks regarding long-term dependencies in sequential input data: long-short-term-memory cells (LSTMs) and gated-recurrent units (GRUs). Some results indicate that GRUs can outperform LSTMs; others show the opposite. So the question remains when to use GRU or LSTM cells. The results show (N = 18000 data, 10-fold cross-validated) that the GRUs outperform LSTMs (accuracy = .85 vs. .82) for overall motive coding. Further analysis showed that GRUs have higher specificity (true negative rate) and learn better less prevalent content. LSTMs have higher sensitivity (true positive rate) and learn better high prevalent content. A closer look at a picture x category matrix reveals that LSTMs outperform GRUs only where deep context understanding is important. As these both techniques do not clearly present a major advantage over one another in the domain investigated here, an interesting topic for future work is to develop a method that combines their strengths.},
  archive      = {J_FRAI},
  author       = {Gruber, Nicole and Jockisch, Alfred},
  doi          = {10.3389/frai.2020.00040},
  journal      = {Frontiers in Artificial Intelligence},
  month        = {6},
  pages        = {539404},
  shortjournal = {Front. Artif. Intell.},
  title        = {Are GRU cells more specific and LSTM cells more sensitive in motive classification of text?},
  volume       = {3},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Deep learnability: Using neural networks to quantify
language similarity and learnability. <em>FRAI</em>, <em>3</em>, 523684.
(<a href="https://doi.org/10.3389/frai.2020.00043">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Learning a second language (L2) usually progresses faster if a learner&#39;s L2 is similar to their first language (L1). Yet global similarity between languages is difficult to quantify, obscuring its precise effect on learnability. Further, the combinatorial explosion of possible L1 and L2 language pairs, combined with the difficulty of controlling for idiosyncratic differences across language pairs and language learners, limits the generalizability of the experimental approach. In this study, we present a different approach, employing artificial languages, and artificial learners. We built a set of five artificial languages whose underlying grammars and vocabulary were manipulated to ensure a known degree of similarity between each pair of languages. We next built a series of neural network models for each language, and sequentially trained them on pairs of languages. These models thus represented L1 speakers learning L2s. By observing the change in activity of the cells between the L1-speaker model and the L2-learner model, we estimated how much change was needed for the model to learn the new language. We then compared the change for each L1/L2 bilingual model to the underlying similarity across each language pair. The results showed that this approach can not only recover the facilitative effect of similarity on L2 acquisition, but can also offer new insights into the differential effects across different domains of similarity. These findings serve as a proof of concept for a generalizable approach that can be applied to natural languages.},
  archive      = {J_FRAI},
  author       = {Cohen, Clara and Higham, Catherine F. and Nabi, Syed Waqar},
  doi          = {10.3389/frai.2020.00043},
  journal      = {Frontiers in Artificial Intelligence},
  month        = {6},
  pages        = {523684},
  shortjournal = {Front. Artif. Intell.},
  title        = {Deep learnability: Using neural networks to quantify language similarity and learnability},
  volume       = {3},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). An integrated world modeling theory (IWMT) of consciousness:
Combining integrated information and global neuronal workspace theories
with the free energy principle and active inference framework; toward
solving the hard problem and characterizing agentic causation.
<em>FRAI</em>, <em>3</em>, 520574. (<a
href="https://doi.org/10.3389/frai.2020.00030">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Free Energy Principle and Active Inference Framework (FEP-AI) begins with the understanding that persisting systems must regulate environmental exchanges and prevent entropic accumulation. In FEP-AI, minds and brains are predictive controllers for autonomous systems, where action-driven perception is realized as probabilistic inference. Integrated Information Theory (IIT) begins with considering the preconditions for a system to intrinsically exist, as well as axioms regarding the nature of consciousness. IIT has produced controversy because of its surprising entailments: quasi-panpsychism; subjectivity without referents or dynamics; and the possibility of fully-intelligent-yet-unconscious brain simulations. Here, I describe how these controversies might be resolved by integrating IIT with FEP-AI, where integrated information only entails consciousness for systems with perspectival reference frames capable of generating models with spatial, temporal, and causal coherence for self and world. Without that connection with external reality, systems could have arbitrarily high amounts of integrated information, but nonetheless would not entail subjective experience. I further describe how an integration of these frameworks may contribute to their evolution as unified systems theories and models of emergent causation. Then, inspired by both Global Neuronal Workspace Theory (GNWT) and the Harmonic Brain Modes framework, I describe how streams of consciousness may emerge as an evolving generation of sensorimotor predictions, with the precise composition of experiences depending on the integration abilities of synchronous complexes as self-organizing harmonic modes (SOHMs). These integrating dynamics may be particularly likely to occur via richly connected subnetworks affording body-centric sources of phenomenal binding and executive control. Along these connectivity backbones, SOHMs are proposed to implement turbo coding via loopy message-passing over predictive (autoencoding) networks, thus generating maximum a posteriori estimates as coherent vectors governing neural evolution, with alpha frequencies generating basic awareness, and cross-frequency phase-coupling within theta frequencies for access consciousness and volitional control. These dynamic cores of integrated information also function as global workspaces, centered on posterior cortices, but capable of being entrained with frontal cortices and interoceptive hierarchies, thus affording agentic causation. Integrated World Modeling Theory (IWMT) represents a synthetic approach to understanding minds that reveals compatibility between leading theories of consciousness, thus enabling inferential synergy.},
  archive      = {J_FRAI},
  author       = {Safron, Adam},
  doi          = {10.3389/frai.2020.00030},
  journal      = {Frontiers in Artificial Intelligence},
  month        = {6},
  pages        = {520574},
  shortjournal = {Front. Artif. Intell.},
  title        = {An integrated world modeling theory (IWMT) of consciousness: Combining integrated information and global neuronal workspace theories with the free energy principle and active inference framework; toward solving the hard problem and characterizing agentic causation},
  volume       = {3},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Forecasting and evaluating multiple interventions for
COVID-19 worldwide. <em>FRAI</em>, <em>3</em>, 546366. (<a
href="https://doi.org/10.3389/frai.2020.00041">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As the Covid-19 pandemic surges around the world, questions arise about the number of global cases at the pandemic&#39;s peak, the length of the pandemic before receding, and the timing of intervention strategies to significantly stop the spread of Covid-19. We have developed artificial intelligence (AI)-inspired methods for modeling the transmission dynamics of the epidemics and evaluating interventions to curb the spread and impact of COVID-19. The developed methods were applied to the surveillance data of cumulative and new COVID-19 cases and deaths reported by WHO as of March 16th, 2020. Both the timing and the degree of intervention were evaluated. The average error of five-step ahead forecasting was 2.5%. The total peak number of cumulative cases, new cases, and the maximum number of cumulative cases in the world with complete intervention implemented 4 weeks later than the beginning date (March 16th, 2020) reached 75,249,909, 10,086,085, and 255,392,154, respectively. However, the total peak number of cumulative cases, new cases, and the maximum number of cumulative cases in the world with complete intervention after 1 week were reduced to 951,799, 108,853 and 1,530,276, respectively. Duration time of the COVID-19 spread was reduced from 356 days to 232 days between later and earlier interventions. We observed that delaying intervention for 1 month caused the maximum number of cumulative cases reduce by −166.89 times that of earlier complete intervention, and the number of deaths increased from 53,560 to 8,938,725. Earlier and complete intervention is necessary to stem the tide of COVID-19 infection.},
  archive      = {J_FRAI},
  author       = {Hu, Zixin and Ge, Qiyang and Li, Shudi and Boerwinkle, Eric and Jin, Li and Xiong, Momiao},
  doi          = {10.3389/frai.2020.00041},
  journal      = {Frontiers in Artificial Intelligence},
  month        = {5},
  pages        = {546366},
  shortjournal = {Front. Artif. Intell.},
  title        = {Forecasting and evaluating multiple interventions for COVID-19 worldwide},
  volume       = {3},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Tuning fairness by balancing target labels. <em>FRAI</em>,
<em>3</em>, 536141. (<a
href="https://doi.org/10.3389/frai.2020.00033">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The issue of fairness in machine learning models has recently attracted a lot of attention as ensuring it will ensure continued confidence of the general public in the deployment of machine learning systems. We focus on mitigating the harm incurred by a biased machine learning system that offers better outputs (e.g., loans, job interviews) for certain groups than for others. We show that bias in the output can naturally be controlled in probabilistic models by introducing a latent target output. This formulation has several advantages: first, it is a unified framework for several notions of group fairness such as Demographic Parity and Equality of Opportunity; second, it is expressed as a marginalization instead of a constrained problem; and third, it allows the encoding of our knowledge of what unbiased outputs should be. Practically, the second allows us to avoid unstable constrained optimization procedures and to reuse off-the-shelf toolboxes. The latter translates to the ability to control the level of fairness by directly varying fairness target rates. In contrast, existing approaches rely on intermediate, arguably unintuitive, control parameters such as covariance thresholds.},
  archive      = {J_FRAI},
  author       = {Kehrenberg, Thomas and Chen, Zexun and Quadrianto, Novi},
  doi          = {10.3389/frai.2020.00033},
  journal      = {Frontiers in Artificial Intelligence},
  month        = {5},
  pages        = {536141},
  shortjournal = {Front. Artif. Intell.},
  title        = {Tuning fairness by balancing target labels},
  volume       = {3},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Evaluating multimedia and language tasks. <em>FRAI</em>,
<em>3</em>, 533980. (<a
href="https://doi.org/10.3389/frai.2020.00032">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Evaluating information access tasks, including textual and multimedia search, question answering, and understanding has been the core mission of NIST&#39;s Retrieval Group since 1989. The TRECVID Evaluations of Multimedia Access began in 2001 with a goal of driving content-based search technology for multimedia just as its progenitor, the Text Retrieval Conference (TREC) did for text and web1.},
  archive      = {J_FRAI},
  author       = {Soboroff, Ian and Awad, George and Butt, Asad and Curtis, Keith},
  doi          = {10.3389/frai.2020.00032},
  journal      = {Frontiers in Artificial Intelligence},
  month        = {5},
  pages        = {533980},
  shortjournal = {Front. Artif. Intell.},
  title        = {Evaluating multimedia and language tasks},
  volume       = {3},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). On consequentialism and fairness. <em>FRAI</em>, <em>3</em>,
526255. (<a href="https://doi.org/10.3389/frai.2020.00034">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent work on fairness in machine learning has primarily emphasized how to define, quantify, and encourage “fair” outcomes. Less attention has been paid, however, to the ethical foundations which underlie such efforts. Among the ethical perspectives that should be taken into consideration is consequentialism, the position that, roughly speaking, outcomes are all that matter. Although consequentialism is not free from difficulties, and although it does not necessarily provide a tractable way of choosing actions (because of the combined problems of uncertainty, subjectivity, and aggregation), it nevertheless provides a powerful foundation from which to critique the existing literature on machine learning fairness. Moreover, it brings to the fore some of the tradeoffs involved, including the problem of who counts, the pros and cons of using a policy, and the relative value of the distant future. In this paper we provide a consequentialist critique of common definitions of fairness within machine learning, as well as a machine learning perspective on consequentialism. We conclude with a broader discussion of the issues of learning and randomization, which have important implications for the ethics of automated decision making systems.},
  archive      = {J_FRAI},
  author       = {Card, Dallas and Smith, Noah A.},
  doi          = {10.3389/frai.2020.00034},
  journal      = {Frontiers in Artificial Intelligence},
  month        = {5},
  pages        = {526255},
  shortjournal = {Front. Artif. Intell.},
  title        = {On consequentialism and fairness},
  volume       = {3},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A new acoustic-based pronunciation distance measure.
<em>FRAI</em>, <em>3</em>, 523433. (<a
href="https://doi.org/10.3389/frai.2020.00039">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present an acoustic distance measure for comparing pronunciations, and apply the measure to assess foreign accent strength in American-English by comparing speech of non-native American-English speakers to a collection of native American-English speakers. An acoustic-only measure is valuable as it does not require the time-consuming and error-prone process of phonetically transcribing speech samples which is necessary for current edit distance-based approaches. We minimize speaker variability in the data set by employing speaker-based cepstral mean and variance normalization, and compute word-based acoustic distances using the dynamic time warping algorithm. Our results indicate a strong correlation of r = −0.71 (p &amp;lt; 0.0001) between the acoustic distances and human judgments of native-likeness provided by more than 1,100 native American-English raters. Therefore, the convenient acoustic measure performs only slightly lower than the state-of-the-art transcription-based performance of r = −0.77. We also report the results of several small experiments which show that the acoustic measure is not only sensitive to segmental differences, but also to intonational differences and durational differences. However, it is not immune to unwanted differences caused by using a different recording device.},
  archive      = {J_FRAI},
  author       = {Bartelds, Martijn and Richter, Caitlin and Liberman, Mark and Wieling, Martijn},
  doi          = {10.3389/frai.2020.00039},
  journal      = {Frontiers in Artificial Intelligence},
  month        = {5},
  pages        = {523433},
  shortjournal = {Front. Artif. Intell.},
  title        = {A new acoustic-based pronunciation distance measure},
  volume       = {3},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Toward “english” phonetics: Variability in the
pre-consonantal voicing effect across english dialects and speakers.
<em>FRAI</em>, <em>3</em>, 522832. (<a
href="https://doi.org/10.3389/frai.2020.00038">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent advances in access to spoken-language corpora and development of speech processing tools have made possible the performance of “large-scale” phonetic and sociolinguistic research. This study illustrates the usefulness of such a large-scale approach—using data from multiple corpora across a range of English dialects, collected, and analyzed with the SPADE project—to examine how the pre-consonantal Voicing Effect (longer vowels before voiced than voiceless obstruents, in e.g., bead vs. beat) is realized in spontaneous speech, and varies across dialects and individual speakers. Compared with previous reports of controlled laboratory speech, the Voicing Effect was found to be substantially smaller in spontaneous speech, but still influenced by the expected range of phonetic factors. Dialects of English differed substantially from each other in the size of the Voicing Effect, whilst individual speakers varied little relative to their particular dialect. This study demonstrates the value of large-scale phonetic research as a means of developing our understanding of the structure of speech variability, and illustrates how large-scale studies, such as those carried out within SPADE, can be applied to other questions in phonetic and sociolinguistic research.},
  archive      = {J_FRAI},
  author       = {Tanner, James and Sonderegger, Morgan and Stuart-Smith, Jane and Fruehwald, Josef},
  doi          = {10.3389/frai.2020.00038},
  journal      = {Frontiers in Artificial Intelligence},
  month        = {5},
  pages        = {522832},
  shortjournal = {Front. Artif. Intell.},
  title        = {Toward “English” phonetics: Variability in the pre-consonantal voicing effect across english dialects and speakers},
  volume       = {3},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Predicting biomass and yield in a tomato phenotyping
experiment using UAV imagery and random forest. <em>FRAI</em>,
<em>3</em>, 520685. (<a
href="https://doi.org/10.3389/frai.2020.00028">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Biomass and yield are key variables for assessing the production and performance of agricultural systems. Modeling and predicting the biomass and yield of individual plants at the farm scale represents a major challenge in precision agriculture, particularly when salinity and other abiotic stresses may play a role. Here, we evaluate a diversity panel of the wild tomato species (Solanum pimpinellifolium) through both field and unmanned aerial vehicle (UAV)-based phenotyping of 600 control and 600 salt-treated plants. The study objective was to predict fresh shoot mass, tomato fruit numbers, and yield mass at harvest based on a range of variables derived from the UAV imagery. UAV-based red–green–blue (RGB) imageries collected 1, 2, 4, 6, 7, and 8 weeks before harvest were also used to determine if prediction accuracies varied between control and salt-treated plants. Multispectral UAV-based imagery was also collected 1 and 2 weeks prior to harvest to further explore predictive insights. In order to estimate the end of season biomass and yield, a random forest machine learning approach was implemented using UAV-imagery-derived predictors as input variables. Shape features derived from the UAV, such as plant area, border length, width, and length, were found to have the highest importance in the predictions, followed by vegetation indices and the entropy texture measure. The multispectral UAV imagery collected 2 weeks prior to harvest produced the highest explained variances for fresh shoot mass (87.95%), fruit numbers (63.88%), and yield mass per plant (66.51%). The RGB UAV imagery produced very similar results to those of the multispectral UAV dataset, with the explained variance reducing as a function of increasing time to harvest. The results showed that predicting the yield of salt-stressed plants produced higher accuracies when the models excluded control plants, whereas predicting the yield of control plants was not affected by the inclusion of salt-stressed plants within the models. This research demonstrates that it is possible to predict the average biomass and yield up to 8 weeks prior to harvest within 4.23% of field-based measurements and up to 4 weeks prior to harvest at the individual plant level. Results from this work may be useful in providing guidance for yield forecasting of healthy and salt-stressed tomato plants, which in turn may inform growing practices, logistical planning, and sales operations.},
  archive      = {J_FRAI},
  author       = {Johansen, Kasper and Morton, Mitchell J. L. and Malbeteau, Yoann and Aragon, Bruno and Al-Mashharawi, Samer and Ziliani, Matteo G. and Angel, Yoseline and Fiene, Gabriele and Negrão, Sónia and Mousa, Magdi A. A. and Tester, Mark A. and McCabe, Matthew F.},
  doi          = {10.3389/frai.2020.00028},
  journal      = {Frontiers in Artificial Intelligence},
  month        = {5},
  pages        = {520685},
  shortjournal = {Front. Artif. Intell.},
  title        = {Predicting biomass and yield in a tomato phenotyping experiment using UAV imagery and random forest},
  volume       = {3},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). The moral choice machine. <em>FRAI</em>, <em>3</em>, 516840.
(<a href="https://doi.org/10.3389/frai.2020.00036">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Allowing machines to choose whether to kill humans would be devastating for world peace and security. But how do we equip machines with the ability to learn ethical or even moral choices? In this study, we show that applying machine learning to human texts can extract deontological ethical reasoning about “right” and “wrong” conduct. We create a template list of prompts and responses, such as “Should I [action]?”, “Is it okay to [action]?”, etc. with corresponding answers of “Yes/no, I should (not).” and &quot;Yes/no, it is (not).&quot; The model&#39;s bias score is the difference between the model&#39;s score of the positive response (“Yes, I should”) and that of the negative response (“No, I should not”). For a given choice, the model&#39;s overall bias score is the mean of the bias scores of all question/answer templates paired with that choice. Specifically, the resulting model, called the Moral Choice Machine (MCM), calculates the bias score on a sentence level using embeddings of the Universal Sentence Encoder since the moral value of an action to be taken depends on its context. It is objectionable to kill living beings, but it is fine to kill time. It is essential to eat, yet one might not eat dirt. It is important to spread information, yet one should not spread misinformation. Our results indicate that text corpora contain recoverable and accurate imprints of our social, ethical and moral choices, even with context information. Actually, training the Moral Choice Machine on different temporal news and book corpora from the year 1510 to 2008/2009 demonstrate the evolution of moral and ethical choices over different time periods for both atomic actions and actions with context information. By training it on different cultural sources such as the Bible and the constitution of different countries, the dynamics of moral choices in culture, including technology are revealed. That is the fact that moral biases can be extracted, quantified, tracked, and compared across cultures and over time.},
  archive      = {J_FRAI},
  author       = {Schramowski, Patrick and Turan, Cigdem and Jentzsch, Sophie and Rothkopf, Constantin and Kersting, Kristian},
  doi          = {10.3389/frai.2020.00036},
  journal      = {Frontiers in Artificial Intelligence},
  month        = {5},
  pages        = {516840},
  shortjournal = {Front. Artif. Intell.},
  title        = {The moral choice machine},
  volume       = {3},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Implicit standardization in a minority language community:
Real-time syntactic change among hasidic yiddish writers. <em>FRAI</em>,
<em>3</em>, 514771. (<a
href="https://doi.org/10.3389/frai.2020.00035">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The recent turn to “big data” from social media corpora has enabled sociolinguists to investigate patterns of language variation and change at unprecedented scales. However, research in this paradigm has been slow to address variable phenomena in minority languages, where data scarcity and the absence of computational tools (e.g., taggers, parsers) often present significant barriers to entry. This article analyzes socio-syntactic variation in one minority language variety, Hasidic Yiddish, focusing on a variable for which tokens can be identified in raw text using purely morphological criteria. In non-finite particle verbs, the overt tense marker tsu (cf. English to, German zu) is variably realized either between the preverbal particle and verb (e.g., oyf-tsu-es-n up-to-eat-INF ‘to eat up’; the conservative variant) or before both elements (tsu oyf-es-n to up-eat-INF; the innovative variant). Nearly 38,000 tokens of non-finite particle verbs were extracted from the popular Hasidic Yiddish discussion forum Kave Shtiebel (the ‘coffee room’; kaveshtiebel.com). A mixed-effects regression analysis reveals that despite a forum-wide favoring effect for the innovative variant, users favor the conservative variant the longer their accounts remain open and active. This process of rapid implicit standardization is supported by ethnographic evidence highlighting the spread of language norms among Hasidic writers on the internet, most of whom did not have the opportunity to express themselves in written Yiddish prior to the advent of social media.},
  archive      = {J_FRAI},
  author       = {Bleaman, Isaac L.},
  doi          = {10.3389/frai.2020.00035},
  journal      = {Frontiers in Artificial Intelligence},
  month        = {5},
  pages        = {514771},
  shortjournal = {Front. Artif. Intell.},
  title        = {Implicit standardization in a minority language community: Real-time syntactic change among hasidic yiddish writers},
  volume       = {3},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A dynamic representation solution for machine learning-aided
performance technology. <em>FRAI</em>, <em>3</em>, 508440. (<a
href="https://doi.org/10.3389/frai.2020.00029">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper illuminates some root causes of confusion about dynamic representation in music technology and introduces a system that addresses this problem to provide context-dependent dynamics for machine learning-aided performance. While terms used for dynamic representations like forte and mezzo-forte have been extant for centuries, the canon gives us no straight answer on how these terms must be applied to literal decibel ranges. The common conception that dynamic terms should be understood as context-dependent is ubiquitous and reasonably simple for most human musicians to grasp. This logic breaks down when applied to digital music technologies. At a fundamental level, these technologies define all musical parameters using discrete numbers, rather than with continuous data, making it impossible for these technologies to make context-dependent decisions. The authors give examples in which this lack of contextual inputs in music technology often leads musicians, composers, and producers to ignore dynamics altogether as a concern in their given practice. The authors then present a system that uses an adaptive process to maximize its ability to hear relevant audio events, and which establishes its own definition for context-dependent dynamics for situations involving music technologies. The authors also describe a generative program that uses these context-dependent dynamic systems in conjunction with a Markov model culled from a living performer–composer as a choice engine for new music improvisations.},
  archive      = {J_FRAI},
  author       = {Palamara, Jason and Deal, W. Scott},
  doi          = {10.3389/frai.2020.00029},
  journal      = {Frontiers in Artificial Intelligence},
  month        = {5},
  pages        = {508440},
  shortjournal = {Front. Artif. Intell.},
  title        = {A dynamic representation solution for machine learning-aided performance technology},
  volume       = {3},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Bringing big data to bear in environmental public health:
Challenges and recommendations. <em>FRAI</em>, <em>3</em>, 478444. (<a
href="https://doi.org/10.3389/frai.2020.00031">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Understanding the role that the environment plays in influencing public health often involves collecting and studying large, complex data sets. There have been a number of private and public efforts to gather sufficient information and confront significant unknowns in the field of environmental public health, yet there is a persistent and largely unmet need for findable, accessible, interoperable, and reusable (FAIR) data. Even when data are readily available, the ability to create, analyze, and draw conclusions from these data using emerging computational tools, such as augmented and artificial intelligence (AI) and machine learning, requires technical skills not currently implemented on a programmatic level across research hubs and academic institutions. We argue that collaborative efforts in data curation and storage, scientific computing, and training are of paramount importance to empower researchers within environmental sciences and the broader public health community to apply AI approaches and fully realize their potential. Leaders in the field were asked to prioritize challenges in incorporating big data in environmental public health research: inconsistent implementation of FAIR principles in data collection and sharing, a lack of skilled data scientists and appropriate cyber-infrastructures, and limited understanding of possibilities and communication of benefits were among those identified. These issues are discussed, and actionable recommendations are provided.},
  archive      = {J_FRAI},
  author       = {Comess, Saskia and Akbay, Alexia and Vasiliou, Melpomene and Hines, Ronald N. and Joppa, Lucas and Vasiliou, Vasilis and Kleinstreuer, Nicole},
  doi          = {10.3389/frai.2020.00031},
  journal      = {Frontiers in Artificial Intelligence},
  month        = {5},
  pages        = {478444},
  shortjournal = {Front. Artif. Intell.},
  title        = {Bringing big data to bear in environmental public health: Challenges and recommendations},
  volume       = {3},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). PECLIDES neuro: A personalisable clinical decision support
system for neurological diseases. <em>FRAI</em>, <em>3</em>, 526217. (<a
href="https://doi.org/10.3389/frai.2020.00023">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Neurodegenerative diseases such as Alzheimer&#39;s and Parkinson&#39;s impact millions of people worldwide. Early diagnosis has proven to greatly increase the chances of slowing down the diseases&#39; progression. Correct diagnosis often relies on the analysis of large amounts of patient data, and thus lends itself well to support from machine learning algorithms, which are able to learn from past diagnosis and see clearly through the complex interactions of a patient&#39;s symptoms and data. Unfortunately, many contemporary machine learning techniques fail to reveal details about how they reach their conclusions, a property considered fundamental when providing a diagnosis. Here we introduce our Personalisable Clinical Decision Support System (PECLIDES), an algorithmic process formulated to address this specific fault in diagnosis detection. PECLIDES provides a clear insight into the decision-making process leading to a diagnosis, making it a gray box model. Our algorithm enriches the fundamental work of Masheyekhi and Gras in data integration, personal medicine, usability, visualization, and interactivity.Our decision support system is an operation of translational medicine. It is based on random forests, is personalisable and allows a clear insight into the decision-making process. A well-structured rule set is created and every rule of the decision-making process can be observed by the user (physician). Furthermore, the user has an impact on the creation of the final rule set and the algorithm allows the comparison of different diseases as well as regional differences in the same disease. The algorithm is applicable to various decision problems. In this paper we will evaluate it on diagnosing neurological diseases and therefore refer to the algorithm as PECLIDES Neuro1.},
  archive      = {J_FRAI},
  author       = {Müller, Tamara T. and Lio, Pietro},
  doi          = {10.3389/frai.2020.00023},
  journal      = {Frontiers in Artificial Intelligence},
  month        = {4},
  pages        = {526217},
  shortjournal = {Front. Artif. Intell.},
  title        = {PECLIDES neuro: A personalisable clinical decision support system for neurological diseases},
  volume       = {3},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). On the improvement of default forecast through textual
analysis. <em>FRAI</em>, <em>3</em>, 523672. (<a
href="https://doi.org/10.3389/frai.2020.00016">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Textual analysis is a widely used methodology in several research areas. In this paper we apply textual analysis to augment the conventional set of account defaults drivers with new text based variables. Through the employment of ad hoc dictionaries and distance measures we are able to classify each account transaction into qualitative macro-categories. The aim is to classify bank account users into different client profiles and verify whether they can act as effective predictors of default through supervised classification models.},
  archive      = {J_FRAI},
  author       = {Cerchiello, Paola and Scaramozzino, Roberta},
  doi          = {10.3389/frai.2020.00016},
  journal      = {Frontiers in Artificial Intelligence},
  month        = {4},
  pages        = {523672},
  shortjournal = {Front. Artif. Intell.},
  title        = {On the improvement of default forecast through textual analysis},
  volume       = {3},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Machine learning models for the classification of sleep
deprivation induced performance impairment during a psychomotor
vigilance task using indices of eye and face tracking. <em>FRAI</em>,
<em>3</em>, 522054. (<a
href="https://doi.org/10.3389/frai.2020.00017">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {High risk professions, such as pilots, police officers, and TSA agents, require sustained vigilance over long periods of time and/or under conditions of little sleep. This can lead to performance impairment in occupational tasks. Predicting impaired states before performance decrement manifests is critical to prevent costly and damaging mistakes. We hypothesize that machine learning models developed to analyze indices of eye and face tracking technologies can accurately predict impaired states. To test this we trained 12 types of machine learning algorithms using five methods of feature selection with indices of eye and face tracking to predict the performance of individual subjects during a psychomotor vigilance task completed at 2-h intervals during a 25-h sleep deprivation protocol. Our results show that (1) indices of eye and face tracking are sensitive to physiological and behavioral changes concomitant with impairment; (2) methods of feature selection heavily influence classification performance of machine learning algorithms; and (3) machine learning models using indices of eye and face tracking can correctly predict whether an individual&#39;s performance is “normal” or “impaired” with an accuracy up to 81.6%. These methods can be used to develop machine learning based systems intended to prevent operational mishaps due to sleep deprivation by predicting operator impairment, using indices of eye and face tracking.},
  archive      = {J_FRAI},
  author       = {Daley, Matthew S. and Gever, David and Posada-Quintero, Hugo F. and Kong, Youngsun and Chon, Ki and Bolkhovsky, Jeffrey B.},
  doi          = {10.3389/frai.2020.00017},
  journal      = {Frontiers in Artificial Intelligence},
  month        = {4},
  pages        = {522054},
  shortjournal = {Front. Artif. Intell.},
  title        = {Machine learning models for the classification of sleep deprivation induced performance impairment during a psychomotor vigilance task using indices of eye and face tracking},
  volume       = {3},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Hybrid hashtags: #YouKnowYoureAKiwiWhen your tweet contains
māori and english. <em>FRAI</em>, <em>3</em>, 521523. (<a
href="https://doi.org/10.3389/frai.2020.00015">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Twitter constitutes a rich resource for investigating language contact phenomena. In this paper, we report findings from the analysis of a large-scale diachronic corpus of over one million tweets, containing loanwords from te reo Māori, the indigenous language spoken in New Zealand, into (primarily, New Zealand) English. Our analysis focuses on hashtags comprising mixed-language resources (which we term hybrid hashtags), bringing together descriptive linguistic tools (investigating length, word class, and semantic domains of the hashtags) and quantitative methods (Random Forests and regression analysis). Our work has implications for language change and the study of loanwords (we argue that hybrid hashtags can be linked to loanword entrenchment), and for the study of language on social media (we challenge proposals of hashtags as “words,” and show that hashtags have a dual discourse role: a micro-function within the immediate linguistic context in which they occur and a macro-function within the tweet as a whole).},
  archive      = {J_FRAI},
  author       = {Trye, David and Calude, Andreea S. and Bravo-Marquez, Felipe and Keegan, Te Taka},
  doi          = {10.3389/frai.2020.00015},
  journal      = {Frontiers in Artificial Intelligence},
  month        = {4},
  pages        = {521523},
  shortjournal = {Front. Artif. Intell.},
  title        = {Hybrid hashtags: #YouKnowYoureAKiwiWhen your tweet contains māori and english},
  volume       = {3},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Explainable AI in fintech risk management. <em>FRAI</em>,
<em>3</em>, 521340. (<a
href="https://doi.org/10.3389/frai.2020.00026">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The paper proposes an explainable AI model that can be used in fintech risk management and, in particular, in measuring the risks that arise when credit is borrowed employing peer to peer lending platforms. The model employs Shapley values, so that AI predictions are interpreted according to the underlying explanatory variables. The empirical analysis of 15,000 small and medium companies asking for peer to peer lending credit reveals that both risky and not risky borrowers can be grouped according to a set of similar financial characteristics, which can be employed to explain and understand their credit score and, therefore, to predict their future behavior.},
  archive      = {J_FRAI},
  author       = {Bussmann, Niklas and Giudici, Paolo and Marinelli, Dimitri and Papenbrock, Jochen},
  doi          = {10.3389/frai.2020.00026},
  journal      = {Frontiers in Artificial Intelligence},
  month        = {4},
  pages        = {521340},
  shortjournal = {Front. Artif. Intell.},
  title        = {Explainable AI in fintech risk management},
  volume       = {3},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). The bitcoin as a virtual commodity: Empirical evidence and
implications. <em>FRAI</em>, <em>3</em>, 513109. (<a
href="https://doi.org/10.3389/frai.2020.00021">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The present work investigates the impact on financial intermediation of distributed ledger technology (DLT), which is usually associated with the blockchain technology and is at the base of the cryptocurrencies&#39; development. “Bitcoin” is the expression of its main application since it was the first new currency that gained popularity some years after its release date and it is still the major cryptocurrency in the market. For this reason, the present analysis is focused on studying its price determination, which seems to be still almost unpredictable. We carry out an empirical analysis based on a cost of production model, trying to detect whether the Bitcoin price could be justified by and connected to the profits and costs associated with the mining effort. We construct a sample model, composed of the hardware devices employed in the mining process. After collecting the technical information required and computing a cost and a profit function for each period, an implied price for the Bitcoin value is derived. The interconnection between this price and the historical one is analyzed, adopting a Vector Autoregression (VAR) model. Our main results put on evidence that there aren&#39;t ultimate drivers for Bitcoin price; probably many factors should be expressed and studied at the same time, taking into account their variability and different relevance over time. It seems that the historical price fluctuated around the model (or implied) price until 2017, when the Bitcoin price significantly increased. During the last months of 2018, the prices seem to converge again, following a common path. In detail, we focus on the time window in which Bitcoin experienced its higher price volatility; the results suggest that it is disconnected from the one predicted by the model. These findings may depend on the particular features of the new cryptocurrencies, which have not been completely understood yet. In our opinion, there is not enough knowledge on cryptocurrencies to assert that Bitcoin price is (or is not) based on the profit and cost derived by the mining process, but these intrinsic characteristics must be considered, including other possible Bitcoin price drivers.},
  archive      = {J_FRAI},
  author       = {Baldan, Cinzia and Zen, Francesco},
  doi          = {10.3389/frai.2020.00021},
  journal      = {Frontiers in Artificial Intelligence},
  month        = {4},
  pages        = {513109},
  shortjournal = {Front. Artif. Intell.},
  title        = {The bitcoin as a virtual commodity: Empirical evidence and implications},
  volume       = {3},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Network models to enhance automated cryptocurrency portfolio
management. <em>FRAI</em>, <em>3</em>, 510510. (<a
href="https://doi.org/10.3389/frai.2020.00022">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The usage of cryptocurrencies, together with that of financial automated consultancy, is widely spreading in the last few years. However, automated consultancy services are not yet exploiting the potentiality of this nascent market, which represents a class of innovative financial products that can be proposed by robo-advisors. For this reason, we propose a novel approach to build efficient portfolio allocation strategies involving volatile financial instruments, such as cryptocurrencies. In other words, we develop an extension of the traditional Markowitz model which combines Random Matrix Theory and network measures, in order to achieve portfolio weights enhancing portfolios&#39; risk-return profiles. The results show that overall our model overperforms several competing alternatives, maintaining a relatively low level of risk.},
  archive      = {J_FRAI},
  author       = {Giudici, Paolo and Pagnottoni, Paolo and Polinesi, Gloria},
  doi          = {10.3389/frai.2020.00022},
  journal      = {Frontiers in Artificial Intelligence},
  month        = {4},
  pages        = {510510},
  shortjournal = {Front. Artif. Intell.},
  title        = {Network models to enhance automated cryptocurrency portfolio management},
  volume       = {3},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Designing and evaluating the usability of a machine learning
API for rapid prototyping music technology. <em>FRAI</em>, <em>3</em>,
508707. (<a href="https://doi.org/10.3389/frai.2020.00013">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To better support creative software developers and music technologists&#39; needs, and to empower them as machine learning users and innovators, the usability of and developer experience with machine learning tools must be considered and better understood. We review background research on the design and evaluation of application programming interfaces (APIs), with a focus on the domain of machine learning for music technology software development. We present the design rationale for the RAPID-MIX API, an easy-to-use API for rapid prototyping with interactive machine learning, and a usability evaluation study with software developers of music technology. A cognitive dimensions questionnaire was designed and delivered to a group of 12 participants who used the RAPID-MIX API in their software projects, including people who developed systems for personal use and professionals developing software products for music and creative technology companies. The results from questionnaire indicate that participants found the RAPID-MIX API a machine learning API which is easy to learn and use, fun, and good for rapid prototyping with interactive machine learning. Based on these findings, we present an analysis and characterization of the RAPID-MIX API based on the cognitive dimensions framework, and discuss its design trade-offs and usability issues. We use these insights and our design experience to provide design recommendations for ML APIs for rapid prototyping of music technology. We conclude with a summary of the main insights, a discussion of the merits and challenges of the application of the CDs framework to the evaluation of machine learning APIs, and directions to future work which our research deems valuable.},
  archive      = {J_FRAI},
  author       = {Bernardo, Francisco and Zbyszyński, Michael and Grierson, Mick and Fiebrink, Rebecca},
  doi          = {10.3389/frai.2020.00013},
  journal      = {Frontiers in Artificial Intelligence},
  month        = {4},
  pages        = {508707},
  shortjournal = {Front. Artif. Intell.},
  title        = {Designing and evaluating the usability of a machine learning API for rapid prototyping music technology},
  volume       = {3},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Computational creativity and music generation systems: An
introduction to the state of the art. <em>FRAI</em>, <em>3</em>, 508631.
(<a href="https://doi.org/10.3389/frai.2020.00014">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Computational Creativity is a multidisciplinary field that tries to obtain creative behaviors from computers. One of its most prolific subfields is that of Music Generation (also called Algorithmic Composition or Musical Metacreation), that uses computational means to compose music. Due to the multidisciplinary nature of this research field, it is sometimes hard to define precise goals and to keep track of what problems can be considered solved by state-of-the-art systems and what instead needs further developments. With this survey, we try to give a complete introduction to those who wish to explore Computational Creativity and Music Generation. To do so, we first give a picture of the research on the definition and the evaluation of creativity, both human and computational, needed to understand how computational means can be used to obtain creative behaviors and its importance within Artificial Intelligence studies. We then review the state of the art of Music Generation Systems, by citing examples for all the main approaches to music generation, and by listing the open challenges that were identified by previous reviews on the subject. For each of these challenges, we cite works that have proposed solutions, describing what still needs to be done and some possible directions for further research.},
  archive      = {J_FRAI},
  author       = {Carnovalini, Filippo and Rodà, Antonio},
  doi          = {10.3389/frai.2020.00014},
  journal      = {Frontiers in Artificial Intelligence},
  month        = {4},
  pages        = {508631},
  shortjournal = {Front. Artif. Intell.},
  title        = {Computational creativity and music generation systems: An introduction to the state of the art},
  volume       = {3},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Evaluating personalization: The AB testing pitfalls
companies might not be aware of—a spotlight on the automotive sector
websites. <em>FRAI</em>, <em>3</em>, 504859. (<a
href="https://doi.org/10.3389/frai.2020.00020">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The importance of companies&#39; website as instrument for relationship marketing activities is well-known both in the academia and in the industry. In the last decades, there has been great interest in studying how technology can be used to influence people&#39;s attitudes and motivate behavior change. With this, web personalization has had increasing research and practitioner interest. However, the evaluation of user interaction with companies&#39; websites and personalization effects remains an elusive goal for organizations. Online controlled experiments (A/B tests) are one of the most commonly known and used techniques for this online evaluation. And, while there is clearly value in evaluating personalized features by means of online controlled experiments, there are some pitfalls to bear in mind while testing. In this paper we present five experimentation pitfalls, firstly identified in an automotive company&#39;s website and found to be present in other sectors, that are particularly important or likely to appear when evaluating personalization features. In order to obtain the listed pitfalls, different methods have been used, including literature review, direct, and indirect observation within organizations of the automotive sector and a set of interviews to organizations form other sectors. Finally, the list of five resulting pitfalls is presented and some suggestions are made on how to avoid or mitigate each of them.},
  archive      = {J_FRAI},
  author       = {Esteller-Cucala, Maria and Fernandez, Vicenc and Villuendas, Diego},
  doi          = {10.3389/frai.2020.00020},
  journal      = {Frontiers in Artificial Intelligence},
  month        = {4},
  pages        = {504859},
  shortjournal = {Front. Artif. Intell.},
  title        = {Evaluating personalization: The AB testing pitfalls companies might not be aware of—A spotlight on the automotive sector websites},
  volume       = {3},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Digital normativity: A challenge for human subjectivation.
<em>FRAI</em>, <em>3</em>, 504698. (<a
href="https://doi.org/10.3389/frai.2020.00027">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_FRAI},
  author       = {Fourneret, Eric and Yvert, Blaise},
  doi          = {10.3389/frai.2020.00027},
  journal      = {Frontiers in Artificial Intelligence},
  month        = {4},
  pages        = {504698},
  shortjournal = {Front. Artif. Intell.},
  title        = {Digital normativity: A challenge for human subjectivation},
  volume       = {3},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Trends in persuasive technologies for physical activity and
sedentary behavior: A systematic review. <em>FRAI</em>, <em>3</em>,
502810. (<a href="https://doi.org/10.3389/frai.2020.00007">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Persuasive technology (PT) is increasingly being used in the health and wellness domain to motivate and assist users with different lifestyles and behavioral health issues to change their attitudes and/or behaviors. There is growing evidence that PT can be effective at promoting behaviors in many health and wellness domains, including promoting physical activity (PA), healthy eating, and reducing sedentary behavior (SB). SB has been shown to pose a risk to overall health. Thus, reducing SB and increasing PA have been the focus of much PT work. This paper aims to provide a systematic review of PTs for promoting PA and reducing SB. Specifically, we answer some fundamental questions regarding its design and effectiveness based on an empirical review of the literature on PTs for promoting PA and discouraging SB, from 2003 to 2019 (170 papers). There are three main objectives: (1) to evaluate the effectiveness of PT in promoting PA and reducing SB; (2) to summarize and highlight trends in the outcomes such as system design, research methods, persuasive strategies employed and their implementaions, behavioral theories, and employed technological platforms; (3) to reveal the pitfalls and gaps in the present literature that can be leveraged and used to inform future research on designing PT for PA and SB.},
  archive      = {J_FRAI},
  author       = {Aldenaini, Noora and Alqahtani, Felwah and Orji, Rita and Sampalli, Srinivas},
  doi          = {10.3389/frai.2020.00007},
  journal      = {Frontiers in Artificial Intelligence},
  month        = {4},
  pages        = {502810},
  shortjournal = {Front. Artif. Intell.},
  title        = {Trends in persuasive technologies for physical activity and sedentary behavior: A systematic review},
  volume       = {3},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Self-net: Lifelong learning via continual self-modeling.
<em>FRAI</em>, <em>3</em>, 502426. (<a
href="https://doi.org/10.3389/frai.2020.00019">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Learning a set of tasks over time, also known as continual learning (CL), is one of the most challenging problems in artificial intelligence. While recent approaches achieve some degree of CL in deep neural networks, they either (1) store a new network (or an equivalent number of parameters) for each new task, (2) store training data from previous tasks, or (3) restrict the network&#39;s ability to learn new tasks. To address these issues, we propose a novel framework, Self-Net, that uses an autoencoder to learn a set of low-dimensional representations of the weights learned for different tasks. We demonstrate that these low-dimensional vectors can then be used to generate high-fidelity recollections of the original weights. Self-Net can incorporate new tasks over time with little retraining, minimal loss in performance for older tasks, and without storing prior training data. We show that our technique achieves over 10X storage compression in a continual fashion, and that it outperforms state-of-the-art approaches on numerous datasets, including continual versions of MNIST, CIFAR10, CIFAR100, Atari, and task-incremental CORe50. To the best of our knowledge, we are the first to use autoencoders to sequentially encode sets of network weights to enable continual learning.},
  archive      = {J_FRAI},
  author       = {Mandivarapu, Jaya Krishna and Camp, Blake and Estrada, Rolando},
  doi          = {10.3389/frai.2020.00019},
  journal      = {Frontiers in Artificial Intelligence},
  month        = {4},
  pages        = {502426},
  shortjournal = {Front. Artif. Intell.},
  title        = {Self-net: Lifelong learning via continual self-modeling},
  volume       = {3},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Initial coin offerings: Risk or opportunity? <em>FRAI</em>,
<em>3</em>, 499201. (<a
href="https://doi.org/10.3389/frai.2020.00018">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Initial coin offerings (ICOs) are one of the several by-products in the world of the cryptocurrencies. Start-ups and existing businesses are turning to alternative sources of capital as opposed to classical channels like banks or venture capitalists. They can offer the inner value of their business by selling “tokens,” i.e., units of the chosen cryptocurrency, like a regular firm would do by means of an IPO. The investors, of course, hope for an increase in the value of the token in the short term, provided a solid and valid business idea typically described by the ICO issuers in a white paper. However, fraudulent activities perpetrated by unscrupulous actors are frequent and it would be crucial to highlight in advance clear signs of illegal money raising. In this paper, we employ statistical approaches to detect what characteristics of ICOs are significantly related to fraudulent behavior. We leverage a number of different variables like: entrepreneurial skills, Telegram chats, and relative sentiment for each ICO, type of business, issuing country, team characteristics. Through logistic regression, multinomial logistic regression, and text analysis, we are able to shed light on the riskiest ICOs.},
  archive      = {J_FRAI},
  author       = {Toma, Anca Mirela and Cerchiello, Paola},
  doi          = {10.3389/frai.2020.00018},
  journal      = {Frontiers in Artificial Intelligence},
  month        = {4},
  pages        = {499201},
  shortjournal = {Front. Artif. Intell.},
  title        = {Initial coin offerings: Risk or opportunity?},
  volume       = {3},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Clearing the transcription hurdle in dialect corpus
building: The corpus of southern dutch dialects as case study.
<em>FRAI</em>, <em>3</em>, 498815. (<a
href="https://doi.org/10.3389/frai.2020.00010">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper discusses how the transcription hurdle in dialect corpus building can be cleared. While corpus analysis has strongly gained in popularity in linguistic research, dialect corpora are still relatively scarce. This scarcity can be attributed to several factors, one of which is the challenging nature of transcribing dialects, given a lack of both orthographic norms for many dialects and speech technological tools trained on dialect data. This paper addresses the questions (i) how dialects can be transcribed efficiently and (ii) whether speech technological tools can lighten the transcription work. These questions are tackled using the Southern Dutch dialects (SDDs) as case study, for which the usefulness of automatic speech recognition (ASR), respeaking, and forced alignment is considered. Tests with these tools indicate that dialects still constitute a major speech technological challenge. In the case of the SDDs, the decision was made to use speech technology only for the word-level segmentation of the audio files, as the transcription itself could not be sped up by ASR tools. The discussion does however indicate that the usefulness of ASR and other related tools for a dialect corpus project is strongly determined by the sound quality of the dialect recordings, the availability of statistical dialect-specific models, the degree of linguistic differentiation between the dialects and the standard language, and the goals the transcripts have to serve.},
  archive      = {J_FRAI},
  author       = {Ghyselen, Anne-Sophie and Breitbarth, Anne and Farasyn, Melissa and Van Keymeulen, Jacques and van Hessen, Arjan},
  doi          = {10.3389/frai.2020.00010},
  journal      = {Frontiers in Artificial Intelligence},
  month        = {4},
  pages        = {498815},
  shortjournal = {Front. Artif. Intell.},
  title        = {Clearing the transcription hurdle in dialect corpus building: The corpus of southern dutch dialects as case study},
  volume       = {3},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Learning to play the chess variant crazyhouse above world
champion level with deep neural networks and human data. <em>FRAI</em>,
<em>3</em>, 492595. (<a
href="https://doi.org/10.3389/frai.2020.00024">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep neural networks have been successfully applied in learning the board games Go, chess, and shogi without prior knowledge by making use of reinforcement learning. Although starting from zero knowledge has been shown to yield impressive results, it is associated with high computationally costs especially for complex games. With this paper, we present CrazyAra which is a neural network based engine solely trained in supervised manner for the chess variant crazyhouse. Crazyhouse is a game with a higher branching factor than chess and there is only limited data of lower quality available compared to AlphaGo. Therefore, we focus on improving efficiency in multiple aspects while relying on low computational resources. These improvements include modifications in the neural network design and training configuration, the introduction of a data normalization step and a more sample efficient Monte-Carlo tree search which has a lower chance to blunder. After training on 569537 human games for 1.5 days we achieve a move prediction accuracy of 60.4%. During development, versions of CrazyAra played professional human players. Most notably, CrazyAra achieved a four to one win over 2017 crazyhouse world champion Justin Tan (aka LM Jann Lee) who is more than 400 Elo higher rated compared to the average player in our training set. Furthermore, we test the playing strength of CrazyAra on CPU against all participants of the second Crazyhouse Computer Championships 2017, winning against twelve of the thirteen participants. Finally, for CrazyAraFish we continue training our model on generated engine games. In 10 long-time control matches playing Stockfish 10, CrazyAraFish wins three games and draws one out of 10 matches.},
  archive      = {J_FRAI},
  author       = {Czech, Johannes and Willig, Moritz and Beyer, Alena and Kersting, Kristian and Fürnkranz, Johannes},
  doi          = {10.3389/frai.2020.00024},
  journal      = {Frontiers in Artificial Intelligence},
  month        = {4},
  pages        = {492595},
  shortjournal = {Front. Artif. Intell.},
  title        = {Learning to play the chess variant crazyhouse above world champion level with deep neural networks and human data},
  volume       = {3},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Discovery of physics from data: Universal laws and
discrepancies. <em>FRAI</em>, <em>3</em>, 479363. (<a
href="https://doi.org/10.3389/frai.2020.00025">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Machine learning (ML) and artificial intelligence (AI) algorithms are now being used to automate the discovery of physics principles and governing equations from measurement data alone. However, positing a universal physical law from data is challenging without simultaneously proposing an accompanying discrepancy model to account for the inevitable mismatch between theory and measurements. By revisiting the classic problem of modeling falling objects of different size and mass, we highlight a number of nuanced issues that must be addressed by modern data-driven methods for automated physics discovery. Specifically, we show that measurement noise and complex secondary physical mechanisms, like unsteady fluid drag forces, can obscure the underlying law of gravitation, leading to an erroneous model. We use the sparse identification of non-linear dynamics (SINDy) method to identify governing equations for real-world measurement data and simulated trajectories. Incorporating into SINDy the assumption that each falling object is governed by a similar physical law is shown to improve the robustness of the learned models, but discrepancies between the predictions and observations persist due to subtleties in drag dynamics. This work highlights the fact that the naive application of ML/AI will generally be insufficient to infer universal physical laws without further modification.},
  archive      = {J_FRAI},
  author       = {de Silva, Brian M. and Higdon, David M. and Brunton, Steven L. and Kutz, J. Nathan},
  doi          = {10.3389/frai.2020.00025},
  journal      = {Frontiers in Artificial Intelligence},
  month        = {4},
  pages        = {479363},
  shortjournal = {Front. Artif. Intell.},
  title        = {Discovery of physics from data: Universal laws and discrepancies},
  volume       = {3},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Where did FinTechs come from, and where do they go? The
transformation of the financial industry in germany after
digitalization. <em>FRAI</em>, <em>3</em>, 511504. (<a
href="https://doi.org/10.3389/frai.2020.00008">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The digitalization of financial services opened a window for new players in the financial industry. These start-ups take on tasks and functions previously reserved for banks, such as financing, asset management, and payments. In this article, we trace the transformation of the industry after digitalization. By using data on FinTech formations in Germany, we provide first evidence that entrepreneurial dynamics in the FinTech sector are not so much driven by technology as by the educational and business background of the founders. Furthermore, we investigate the reactions of traditional banks to the emergence of these start-ups. In contrast with other emerging industries such as biotechnology, a network analysis shows that FinTechs have mostly engaged in strategic partnerships and only a few banks have acquired or obtained a financial interest in a FinTech. We explain the restraint of traditional banks to fully endorse the new possibilities of digitalized financial services with the characteristics of the technology itself and with the postponed fundamental decisions of banks to modernize their IT infrastructure.},
  archive      = {J_FRAI},
  author       = {Brandl, Barbara and Hornuf, Lars},
  doi          = {10.3389/frai.2020.00008},
  journal      = {Frontiers in Artificial Intelligence},
  month        = {3},
  pages        = {511504},
  shortjournal = {Front. Artif. Intell.},
  title        = {Where did FinTechs come from, and where do they go? the transformation of the financial industry in germany after digitalization},
  volume       = {3},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Understanding musical predictions with an embodied interface
for musical machine learning. <em>FRAI</em>, <em>3</em>, 508777. (<a
href="https://doi.org/10.3389/frai.2020.00006">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Machine-learning models of music often exist outside the worlds of musical performance practice and abstracted from the physical gestures of musicians. In this work, we consider how a recurrent neural network (RNN) model of simple music gestures may be integrated into a physical instrument so that predictions are sonically and physically entwined with the performer&#39;s actions. We introduce EMPI, an embodied musical prediction interface that simplifies musical interaction and prediction to just one dimension of continuous input and output. The predictive model is a mixture density RNN trained to estimate the performer&#39;s next physical input action and the time at which this will occur. Predictions are represented sonically through synthesized audio, and physically with a motorized output indicator. We use EMPI to investigate how performers understand and exploit different predictive models to make music through a controlled study of performances with different models and levels of physical feedback. We show that while performers often favor a model trained on human-sourced data, they find different musical affordances in models trained on synthetic, and even random, data. Physical representation of predictions seemed to affect the length of performances. This work contributes new understandings of how musicians use generative ML models in real-time performance backed up by experimental evidence. We argue that a constrained musical interface can expose the affordances of embodied predictive interactions.},
  archive      = {J_FRAI},
  author       = {Martin, Charles Patrick and Glette, Kyrre and Nygaard, Tønnes Frostad and Torresen, Jim},
  doi          = {10.3389/frai.2020.00006},
  journal      = {Frontiers in Artificial Intelligence},
  month        = {3},
  pages        = {508777},
  shortjournal = {Front. Artif. Intell.},
  title        = {Understanding musical predictions with an embodied interface for musical machine learning},
  volume       = {3},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Toward a taxonomy for adaptive data visualization in
analytics applications. <em>FRAI</em>, <em>3</em>, 504499. (<a
href="https://doi.org/10.3389/frai.2020.00009">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data analytics as a field is currently at a crucial point in its development, as a commoditization takes place in the context of increasing amounts of data, more user diversity, and automated analysis solutions, the latter potentially eliminating the need for expert analysts. A central hypothesis of the present paper is that data visualizations should be adapted to both the user and the context. This idea was initially addressed in Study 1, which demonstrated substantial interindividual variability among a group of experts when freely choosing an option to visualize data sets. To lay the theoretical groundwork for a systematic, taxonomic approach, a user model combining user traits, states, strategies, and actions was proposed and further evaluated empirically in Studies 2 and 3. The results implied that for adapting to user traits, statistical expertise is a relevant dimension that should be considered. Additionally, for adapting to user states different user intentions such as monitoring and analysis should be accounted for. These results were used to develop a taxonomy which adapts visualization recommendations to these (and other) factors. A preliminary attempt to validate the taxonomy in Study 4 tested its visualization recommendations with a group of experts. While the corresponding results were somewhat ambiguous overall, some aspects nevertheless supported the claim that a user-adaptive data visualization approach based on the principles outlined in the taxonomy can indeed be useful. While the present approach to user adaptivity is still in its infancy and should be extended (e.g., by testing more participants), the general approach appears to be very promising.},
  archive      = {J_FRAI},
  author       = {Poetzsch, Tristan and Germanakos, Panagiotis and Huestegge, Lynn},
  doi          = {10.3389/frai.2020.00009},
  journal      = {Frontiers in Artificial Intelligence},
  month        = {3},
  pages        = {504499},
  shortjournal = {Front. Artif. Intell.},
  title        = {Toward a taxonomy for adaptive data visualization in analytics applications},
  volume       = {3},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Adapting learning activity selection to emotional stability
and competence. <em>FRAI</em>, <em>3</em>, 501938. (<a
href="https://doi.org/10.3389/frai.2020.00011">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper investigates how humans adapt next learning activity selection (in particular the knowledge it assumes and the knowledge it teaches) to learner personality and competence to inspire an adaptive learning activity selection algorithm. First, the paper describes the investigation to produce validated materials for the main study, namely the creation and validation of learner competence statements. Next, through an empirical study, we investigate the impact on learning activity selection of learners&#39; emotional stability and competence. Participants considered a fictional learner with a certain competence, emotional stability, recent and prior learning activities engaged in, and selected the next learning activity in terms of the knowledge it used and the knowledge it taught. Three algorithms were created to adapt the selection of learning activities&#39; knowledge complexity to learners&#39; personality and competence. Finally, we evaluated the algorithms through a study with teachers, resulting in an algorithm that selects learning activities with varying assumed and taught knowledge adapted to learner characteristics.},
  archive      = {J_FRAI},
  author       = {Alhathli, Manal and Masthoff, Judith and Beacham, Nigel},
  doi          = {10.3389/frai.2020.00011},
  journal      = {Frontiers in Artificial Intelligence},
  month        = {3},
  pages        = {501938},
  shortjournal = {Front. Artif. Intell.},
  title        = {Adapting learning activity selection to emotional stability and competence},
  volume       = {3},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A frequency-domain machine learning method for
dual-calibrated fMRI mapping of oxygen extraction fraction (OEF) and
cerebral metabolic rate of oxygen consumption (CMRO2). <em>FRAI</em>,
<em>3</em>, 475718. (<a
href="https://doi.org/10.3389/frai.2020.00012">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Magnetic resonance imaging (MRI) offers the possibility to non-invasively map the brain&#39;s metabolic oxygen consumption (CMRO2), which is essential for understanding and monitoring neural function in both health and disease. However, in depth study of oxygen metabolism with MRI has so far been hindered by the lack of robust methods. One MRI method of mapping CMRO2 is based on the simultaneous acquisition of cerebral blood flow (CBF) and blood oxygen level dependent (BOLD) weighted images during respiratory modulation of both oxygen and carbon dioxide. Although this dual-calibrated methodology has shown promise in the research setting, current analysis methods are unstable in the presence of noise and/or are computationally demanding. In this paper, we present a machine learning implementation for the multi-parametric assessment of dual-calibrated fMRI data. The proposed method aims to address the issues of stability, accuracy, and computational overhead, removing significant barriers to the investigation of oxygen metabolism with MRI. The method utilizes a time-frequency transformation of the acquired perfusion and BOLD-weighted data, from which appropriate feature vectors are selected for training of machine learning regressors. The implemented machine learning methods are chosen for their robustness to noise and their ability to map complex non-linear relationships (such as those that exist between BOLD signal weighting and blood oxygenation). An extremely randomized trees (ET) regressor is used to estimate resting blood flow and a multi-layer perceptron (MLP) is used to estimate CMRO2 and the oxygen extraction fraction (OEF). Synthetic data with additive noise are used to train the regressors, with data simulated to cover a wide range of physiologically plausible parameters. The performance of the implemented analysis method is compared to published methods both in simulation and with in-vivo data (n = 30). The proposed method is demonstrated to significantly reduce computation time, error, and proportional bias in both CMRO2 and OEF estimates. The introduction of the proposed analysis pipeline has the potential to not only increase the detectability of metabolic difference between groups of subjects, but may also allow for single subject examinations within a clinical context.},
  archive      = {J_FRAI},
  author       = {Germuska, Michael and Chandler, Hannah Louise and Okell, Thomas and Fasano, Fabrizio and Tomassini, Valentina and Murphy, Kevin and Wise, Richard G.},
  doi          = {10.3389/frai.2020.00012},
  journal      = {Frontiers in Artificial Intelligence},
  month        = {3},
  pages        = {475718},
  shortjournal = {Front. Artif. Intell.},
  title        = {A frequency-domain machine learning method for dual-calibrated fMRI mapping of oxygen extraction fraction (OEF) and cerebral metabolic rate of oxygen consumption (CMRO2)},
  volume       = {3},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Information theoretic characterization of uncertainty
distinguishes surprise from accuracy signals in the brain.
<em>FRAI</em>, <em>3</em>, 508474. (<a
href="https://doi.org/10.3389/frai.2020.00005">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Uncertainty presents a problem for both human and machine decision-making. While utility maximization has traditionally been viewed as the motive force behind choice behavior, it has been theorized that uncertainty minimization may supersede reward motivation. Beyond reward, decisions are guided by belief, i.e., confidence-weighted expectations. Evidence challenging a belief evokes surprise, which signals a deviation from expectation (stimulus-bound surprise) but also provides an information gain. To support the theory that uncertainty minimization is an essential drive for the brain, we probe the neural trace of uncertainty-related decision variables, namely confidence, surprise, and information gain, in a discrete decision with a deterministic outcome. Confidence and surprise were elicited with a gambling task administered in a functional magnetic resonance imaging experiment, where agents start with a uniform probability distribution, transition to a non-uniform probabilistic state, and end in a fully certain state. After controlling for reward expectation, we find confidence, taken as the negative entropy of a trial, correlates with a response in the hippocampus and temporal lobe. Stimulus-bound surprise, taken as Shannon information, correlates with responses in the insula and striatum. In addition, we also find a neural response to a measure of information gain captured by a confidence error, a quantity we dub accuracy. BOLD responses to accuracy were found in the cerebellum and precuneus, after controlling for reward prediction errors and stimulus-bound surprise at the same time point. Our results suggest that, even absent an overt need for learning, the human brain expends energy on information gain and uncertainty minimization.},
  archive      = {J_FRAI},
  author       = {Loued-Khenissi, Leyla and Preuschoff, Kerstin},
  doi          = {10.3389/frai.2020.00005},
  journal      = {Frontiers in Artificial Intelligence},
  month        = {2},
  pages        = {508474},
  shortjournal = {Front. Artif. Intell.},
  title        = {Information theoretic characterization of uncertainty distinguishes surprise from accuracy signals in the brain},
  volume       = {3},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Interpretability with accurate small models. <em>FRAI</em>,
<em>3</em>, 507097. (<a
href="https://doi.org/10.3389/frai.2020.00003">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Models often need to be constrained to a certain size for them to be considered interpretable. For example, a decision tree of depth 5 is much easier to understand than one of depth 50. Limiting model size, however, often reduces accuracy. We suggest a practical technique that minimizes this trade-off between interpretability and classification accuracy. This enables an arbitrary learning algorithm to produce highly accurate small-sized models. Our technique identifies the training data distribution to learn from that leads to the highest accuracy for a model of a given size. We represent the training distribution as a combination of sampling schemes. Each scheme is defined by a parameterized probability mass function applied to the segmentation produced by a decision tree. An Infinite Mixture Model with Beta components is used to represent a combination of such schemes. The mixture model parameters are learned using Bayesian Optimization. Under simplistic assumptions, we would need to optimize for O(d) variables for a distribution over a d-dimensional input space, which is cumbersome for most real-world data. However, we show that our technique significantly reduces this number to a fixed set of eight variables at the cost of relatively cheap preprocessing. The proposed technique is flexible: it is model-agnostic, i.e., it may be applied to the learning algorithm for any model family, and it admits a general notion of model size. We demonstrate its effectiveness using multiple real-world datasets to construct decision trees, linear probability models and gradient boosted models with different sizes. We observe significant improvements in the F1-score in most instances, exceeding an improvement of 100% in some cases.},
  archive      = {J_FRAI},
  author       = {Ghose, Abhishek and Ravindran, Balaraman},
  doi          = {10.3389/frai.2020.00003},
  journal      = {Frontiers in Artificial Intelligence},
  month        = {2},
  pages        = {507097},
  shortjournal = {Front. Artif. Intell.},
  title        = {Interpretability with accurate small models},
  volume       = {3},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). An introductory review of deep learning for prediction
models with big data. <em>FRAI</em>, <em>3</em>, 507091. (<a
href="https://doi.org/10.3389/frai.2020.00004">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning models stand for a new learning paradigm in artificial intelligence (AI) and machine learning. Recent breakthrough results in image analysis and speech recognition have generated a massive interest in this field because also applications in many other domains providing big data seem possible. On a downside, the mathematical and computational methodology underlying deep learning models is very challenging, especially for interdisciplinary scientists. For this reason, we present in this paper an introductory review of deep learning approaches including Deep Feedforward Neural Networks (D-FFNN), Convolutional Neural Networks (CNNs), Deep Belief Networks (DBNs), Autoencoders (AEs), and Long Short-Term Memory (LSTM) networks. These models form the major core architectures of deep learning models currently used and should belong in any data scientist&#39;s toolbox. Importantly, those core architectural building blocks can be composed flexibly—in an almost Lego-like manner—to build new application-specific network architectures. Hence, a basic understanding of these network architectures is important to be prepared for future developments in AI.},
  archive      = {J_FRAI},
  author       = {Emmert-Streib, Frank and Yang, Zhen and Feng, Han and Tripathi, Shailesh and Dehmer, Matthias},
  doi          = {10.3389/frai.2020.00004},
  journal      = {Frontiers in Artificial Intelligence},
  month        = {2},
  pages        = {507091},
  shortjournal = {Front. Artif. Intell.},
  title        = {An introductory review of deep learning for prediction models with big data},
  volume       = {3},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Retrospective inference as a form of bounded rationality,
and its beneficial influence on learning. <em>FRAI</em>, <em>3</em>,
484533. (<a href="https://doi.org/10.3389/frai.2020.00002">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Probabilistic models of cognition typically assume that agents make inferences about current states by combining new sensory information with fixed beliefs about the past, an approach known as Bayesian filtering. This is computationally parsimonious, but, in general, leads to suboptimal beliefs about past states, since it ignores the fact that new observations typically contain information about the past as well as the present. This is disadvantageous both because knowledge of past states may be intrinsically valuable, and because it impairs learning about fixed or slowly changing parameters of the environment. For these reasons, in offline data analysis it is usual to infer on every set of states using the entire time series of observations, an approach known as (fixed-interval) Bayesian smoothing. Unfortunately, however, this is impractical for real agents, since it requires the maintenance and updating of beliefs about an ever-growing set of states. We propose an intermediate approach, finite retrospective inference (FRI), in which agents perform update beliefs about a limited number of past states (Formally, this represents online fixed-lag smoothing with a sliding window). This can be seen as a form of bounded rationality in which agents seek to optimize the accuracy of their beliefs subject to computational and other resource costs. We show through simulation that this approach has the capacity to significantly increase the accuracy of both inference and learning, using a simple variational scheme applied to both randomly generated Hidden Markov models (HMMs), and a specific application of the HMM, in the form of the widely used probabilistic reversal task. Our proposal thus constitutes a theoretical contribution to normative accounts of bounded rationality, which makes testable empirical predictions that can be explored in future work.},
  archive      = {J_FRAI},
  author       = {FitzGerald, Thomas H. B. and Penny, Will D. and Bonnici, Heidi M. and Adams, Rick A.},
  doi          = {10.3389/frai.2020.00002},
  journal      = {Frontiers in Artificial Intelligence},
  month        = {2},
  pages        = {484533},
  shortjournal = {Front. Artif. Intell.},
  title        = {Retrospective inference as a form of bounded rationality, and its beneficial influence on learning},
  volume       = {3},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Fuzzy leaky bucket system for intelligent management of
consumer electricity elastic load in smart grids. <em>FRAI</em>,
<em>3</em>, 507585. (<a
href="https://doi.org/10.3389/frai.2020.00001">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper frames itself in an informational rich smart electricity grid where consumers have access to various streams of information and make decisions over their daily consumption pattern. In particular, a new intelligent management system to accommodate possible optimal decisions for elastic load consumption is discussed. The energy management system implements a fuzzy driven leaky bucket that manages the elastic load of a consumer by controlling the token rate buffer via a set of four fuzzy variables (among them the electricity price). The goal of this innovative system is to allow loads that are identified as elastic to be scheduled only when it is potentially beneficial to the consumer. To that end, a fuzzy algorithm comprised of a set of rules is developed to manage the token rate of the leaky bucket and through that the decisions over the fate of elastic loads. The developed system is applied on a set of real-world electricity consumption data taken from a residential consumer, and benchmarked against a full scheduling method, where the elastic load is fully scheduled offline. Results exhibit that the proposed fuzzy logic method outperforms the full scheduling method in the vast majority of the cases, i.e., over 79% of the cases with respect to consumption cost. Furthermore, they validate its ability to conduct real time decision making with no human in the loop.},
  archive      = {J_FRAI},
  author       = {Alamaniotis, Miltiadis},
  doi          = {10.3389/frai.2020.00001},
  journal      = {Frontiers in Artificial Intelligence},
  month        = {1},
  pages        = {507585},
  shortjournal = {Front. Artif. Intell.},
  title        = {Fuzzy leaky bucket system for intelligent management of consumer electricity elastic load in smart grids},
  volume       = {3},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Analysis of features selected by a deep learning model for
differential treatment selection in depression. <em>FRAI</em>,
<em>2</em>, 508714. (<a
href="https://doi.org/10.3389/frai.2019.00031">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Background: Deep learning has utility in predicting differential antidepressant treatment response among patients with major depressive disorder, yet there remains a paucity of research describing how to interpret deep learning models in a clinically or etiologically meaningful way. In this paper, we describe methods for analyzing deep learning models of clinical and demographic psychiatric data, using our recent work on a deep learning model of STAR*D and CO-MED remission prediction.Methods: Our deep learning analysis with STAR*D and CO-MED yielded four models that predicted response to the four treatments used across the two datasets. Here, we use classical statistics and simple data representations to improve interpretability of the features output by our deep learning model and provide finer grained understanding of their clinical and etiological significance. Specifically, we use representations derived from our model to yield features predicting both treatment non-response and differential treatment response to four standard antidepressants, and use linear regression and t-tests to address questions about the contribution of trauma, education, and somatic symptoms to our models.Results: Traditional statistics were able to probe the input features of our deep learning models, reproducing results from previous research, while providing novel insights into depression causes and treatments. We found that specific features were predictive of treatment response, and were able to break these down by treatment and non-response categories; that specific trauma indices were differentially predictive of baseline depression severity; that somatic symptoms were significantly different between males and females, and that education and low income proved important psycho-social stressors associated with depression.Conclusion: Traditional statistics can augment interpretation of deep learning models. Such interpretation can lend us new hypotheses about depression and contribute to building causal models of etiology and prognosis. We discuss dataset-specific effects and ideal clinical samples for machine learning analysis aimed at improving tools to assist in optimizing treatment.},
  archive      = {J_FRAI},
  author       = {Mehltretter, Joseph and Rollins, Colleen and Benrimoh, David and Fratila, Robert and Perlman, Kelly and Israel, Sonia and Miresco, Marc and Wakid, Marina and Turecki, Gustavo},
  doi          = {10.3389/frai.2019.00031},
  journal      = {Frontiers in Artificial Intelligence},
  month        = {1},
  pages        = {508714},
  shortjournal = {Front. Artif. Intell.},
  title        = {Analysis of features selected by a deep learning model for differential treatment selection in depression},
  volume       = {2},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Mysteries, epistemological modesty, and artificial
intelligence in surgery. <em>FRAI</em>, <em>2</em>, 492473. (<a
href="https://doi.org/10.3389/frai.2019.00032">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_FRAI},
  author       = {Loftus, Tyler J. and Upchurch, Gilbert R. and Delitto, Daniel and Rashidi, Parisa and Bihorac, Azra},
  doi          = {10.3389/frai.2019.00032},
  journal      = {Frontiers in Artificial Intelligence},
  month        = {1},
  pages        = {492473},
  shortjournal = {Front. Artif. Intell.},
  title        = {Mysteries, epistemological modesty, and artificial intelligence in surgery},
  volume       = {2},
  year         = {2020},
}
</textarea>
</details></li>
</ul>

</body>
</html>
