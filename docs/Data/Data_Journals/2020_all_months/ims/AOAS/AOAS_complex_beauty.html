<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>AOAS_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="aoas---98">AOAS - 98</h2>
<ul>
<li><details>
<summary>
(2020b). Rejoinder: Fitting a folded normal distribution without EM.
<em>AOAS</em>, <em>14</em>(4), 2101. (<a
href="https://doi.org/10.1214/20-AOAS1412">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_AOAS},
  author       = {Iain L. MacDonald},
  doi          = {10.1214/20-AOAS1412},
  journal      = {The Annals of Applied Statistics},
  number       = {4},
  pages        = {2101},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Rejoinder: Fitting a folded normal distribution without EM},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Response to “fitting a folded normal distribution without
EM.” <em>AOAS</em>, <em>14</em>(4), 2099–2100. (<a
href="https://doi.org/10.1214/20-AOAS1411">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_AOAS},
  author       = {Sungkyu Jung and Mark Foskey and J. S. Marron},
  doi          = {10.1214/20-AOAS1411},
  journal      = {The Annals of Applied Statistics},
  number       = {4},
  pages        = {2099-2100},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Response to ‘Fitting a folded normal distribution without EM’},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020a). Letter to the editor: Fitting a folded normal distribution
without EM. <em>AOAS</em>, <em>14</em>(4), 2096–2098. (<a
href="https://doi.org/10.1214/20-AOAS1410">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The problem of fitting a folded normal distribution by maximum likelihood has been described as ‘not straightforward’, and alternatives such as EM proposed. We suggest here that it is in fact straightforward to fit such a distribution by direct numerical maximization of the likelihood. We demonstrate this in an example. The relevant R code is included.},
  archive      = {J_AOAS},
  author       = {Iain L. MacDonald},
  doi          = {10.1214/20-AOAS1410},
  journal      = {The Annals of Applied Statistics},
  number       = {4},
  pages        = {2096-2098},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Letter to the editor: Fitting a folded normal distribution without EM},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Mitigating unobserved spatial confounding when estimating
the effect of supermarket access on cardiovascular disease deaths.
<em>AOAS</em>, <em>14</em>(4), 2069–2095. (<a
href="https://doi.org/10.1214/20-AOAS1377">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Confounding by unmeasured spatial variables has received some attention in the spatial statistics and causal inference literatures, but concepts and approaches have remained largely separated. In this paper we aim to bridge these distinct strands of statistics by considering unmeasured spatial confounding within a causal inference framework and estimating effects using outcome regression tools popular within the spatial literature. First, we show how using spatially correlated random effects in the outcome model, an approach common among spatial statisticians, does not necessarily mitigate bias due to spatial confounding, a previously published but not universally known result. Motivated by the bias term of commonly-used estimators, we propose an affine estimator which addresses this deficiency. We discuss how unbiased estimation of causal parameters in the presence of unmeasured spatial confounding can only be achieved under an untestable set of assumptions which will often be application specific. We provide a set of assumptions which describe how the exposure and outcome of interest relate to the unmeasured variables, and we show that this set of assumptions is sufficient for identification of the causal effect based on the observed data when spatial dependencies can be represented by a ring graph. We implement our method using a fully Bayesian approach applicable to any type of outcome variable. This work is motivated by and used to estimate the effect of county-level limited access to supermarkets on the rate of cardiovascular disease deaths in the elderly across the whole continental United States. Even though standard approaches return null or protective effects, our approach uncovers evidence of unobserved spatial confounding and indicates that limited supermarket access has a harmful effect on cardiovascular mortality.},
  archive      = {J_AOAS},
  author       = {Patrick M. Schnell and Georgia Papadogeorgou},
  doi          = {10.1214/20-AOAS1377},
  journal      = {The Annals of Applied Statistics},
  number       = {4},
  pages        = {2069-2095},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Mitigating unobserved spatial confounding when estimating the effect of supermarket access on cardiovascular disease deaths},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Region-referenced spectral power dynamics of EEG signals: A
hierarchical modeling approach. <em>AOAS</em>, <em>14</em>(4),
2053–2068. (<a href="https://doi.org/10.1214/20-AOAS1374">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Functional brain imaging through electroencephalography (EEG) relies upon the analysis and interpretation of high-dimensional, spatially organized time series. We propose to represent time-localized frequency domain characterizations of EEG data as region-referenced functional data. This representation is coupled with a hierarchical regression modeling approach to multivariate functional observations. Within this familiar setting we discuss how several prior models relate to structural assumptions about multivariate covariance operators. An overarching modeling framework, based on infinite factorial decompositions, is finally proposed to balance flexibility and efficiency in estimation. The motivating application stems from a study of implicit auditory learning, in which typically developing (TD) children, and children with autism spectrum disorder (ASD) were exposed to a continuous speech stream. Using the proposed model, we examine differential band power dynamics as brain function is interrogated throughout the duration of a computer-controlled experiment. Our work offers a novel look at previous findings in psychiatry and provides further insights into the understanding of ASD. Our approach to inference is fully Bayesian and implemented in a highly optimized Rcpp package.},
  archive      = {J_AOAS},
  author       = {Qian Li and John Shamshoian and Damla Şentürk and Catherine Sugar and Shafali Jeste and Charlotte DiStefano and Donatello Telesca},
  doi          = {10.1214/20-AOAS1374},
  journal      = {The Annals of Applied Statistics},
  number       = {4},
  pages        = {2053-2068},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Region-referenced spectral power dynamics of EEG signals: A hierarchical modeling approach},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Modelling the sound production of narwhals using a point
process framework with memory effects. <em>AOAS</em>, <em>14</em>(4),
2037–2052. (<a href="https://doi.org/10.1214/20-AOAS1379">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Obtaining an adequate description of the behaviour of narwhals in a pristine environment is important to understand natural behaviour as well as providing the means to determine potential changes in behaviour directly or indirectly caused by human activity. Based on $\text{Acousonde}^{\text{TM}}$ data from five narwhals in Scoresby Sound, this paper aims at modelling buzzing and calling rates of East Greenland narwhals as functions of time, space and, possibly, autoregressive memory. Both buzzing and calling are sounds produced by narwhals. Buzzing is a way for the whale to navigate and locate prey using echolocation, while calling is associated with social communication between whales. Logistic regression models without and with autoregressive components are compared based on AIC and comparatively assessed using diagnostics from point process theory. Adding an autoregressive component appears to improve the models, and further improvements for the buzzing model are made with a non-GLM extension. Effects of extrinsic covariates and memory are presented and interpreted. Buzzing occurs at deeper depths, and initiations of buzzes are separated by refractory periods. A possible feeding area is identified. Calling occurs closer to the surface, and, while the probability of calling in general is lower than buzzing, it is more likely that calls are clustered together rather than spread randomly.},
  archive      = {J_AOAS},
  author       = {Aleksander Søltoft-Jensen and Mads Peter Heide-Jørgensen and Susanne Ditlevsen},
  doi          = {10.1214/20-AOAS1379},
  journal      = {The Annals of Applied Statistics},
  number       = {4},
  pages        = {2037-2052},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Modelling the sound production of narwhals using a point process framework with memory effects},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Structured discrepancy in bayesian model calibration for
ChemCam on the mars curiosity rover. <em>AOAS</em>, <em>14</em>(4),
2020–2036. (<a href="https://doi.org/10.1214/20-AOAS1373">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Mars rover Curiosity carries an instrument called ChemCam to determine the composition of the soil and rocks via laser-induced breakdown spectroscopy (LIBS). Los Alamos National Laboratory has developed a simulation capability that can predict spectra from ChemCam, but there are major-scale differences between the prediction and observation. This presents a challenge when using Bayesian model calibration to determine the unknown physical parameters that describe the LIBS observations. We present an analysis of LIBS data to support ChemCam based on including a structured discrepancy model in a Bayesian model-calibration scheme. This is both a novel application and an illustration of the importance of setting scientifically informed and constrained discrepancy models within Bayesian model calibration.},
  archive      = {J_AOAS},
  author       = {K. Sham Bhat and Kary Myers and Earl Lawrence and James Colgan and Elizabeth Judge},
  doi          = {10.1214/20-AOAS1373},
  journal      = {The Annals of Applied Statistics},
  number       = {4},
  pages        = {2020-2036},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Structured discrepancy in bayesian model calibration for ChemCam on the mars curiosity rover},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Nonparametric bayesian multiarmed bandits for single-cell
experiment design. <em>AOAS</em>, <em>14</em>(4), 2003–2019. (<a
href="https://doi.org/10.1214/20-AOAS1370">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The problem of maximizing cell type discovery under budget constraints is a fundamental challenge for the collection and analysis of single-cell RNA-sequencing (scRNA-seq) data. In this paper we introduce a simple, computationally efficient and scalable Bayesian nonparametric sequential approach to optimize the budget allocation when designing a large-scale experiment for the collection of scRNA-seq data for the purpose of, but not limited to, creating cell atlases. Our approach relies on the following tools: (i) a hierarchical Pitman–Yor prior that recapitulates biological assumptions regarding cellular differentiation, and (ii) a Thompson sampling multiarmed bandit strategy that balances exploitation and exploration to prioritize experiments across a sequence of trials. Posterior inference is performed by using a sequential Monte Carlo approach which allows us to fully exploit the sequential nature of our species sampling problem. We empirically show that our approach outperforms state-of-the-art methods and achieves near-Oracle performance on simulated and scRNA-seq data alike. HPY-TS code is available at https://github.com/fedfer/HPYsinglecell.},
  archive      = {J_AOAS},
  author       = {Federico Camerlenghi and Bianca Dumitrascu and Federico Ferrari and Barbara E. Engelhardt and Stefano Favaro},
  doi          = {10.1214/20-AOAS1370},
  journal      = {The Annals of Applied Statistics},
  number       = {4},
  pages        = {2003-2019},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Nonparametric bayesian multiarmed bandits for single-cell experiment design},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Hawkes binomial topic model with applications to coupled
conflict-twitter data. <em>AOAS</em>, <em>14</em>(4), 1984–2002. (<a
href="https://doi.org/10.1214/20-AOAS1352">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider the problem of modeling and clustering heterogeneous event data arising from coupled conflict event and social media data sets. In this setting conflict events trigger responses on social media, and, at the same time, signals of grievance detected in social media may serve as leading indicators for subsequent conflict events. For this purpose we introduce the Hawkes Binomial Topic Model (HBTM) where marks, Tweets and conflict event descriptions are represented as bags of words following a Binomial distribution. When viewed as a branching process, the daughter event bag of words is generated by randomly turning on/off parent words through independent Bernoulli random variables. We then use expectation–maximization to estimate the model parameters and branching structure of the process. The inferred branching structure is then used for topic cascade detection, short-term forecasting, and investigating the causal dependence of grievance on social media and conflict events in recent elections in Nigeria and Kenya.},
  archive      = {J_AOAS},
  author       = {George Mohler and Erin McGrath and Cody Buntain and Gary LaFree},
  doi          = {10.1214/20-AOAS1352},
  journal      = {The Annals of Applied Statistics},
  number       = {4},
  pages        = {1984-2002},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Hawkes binomial topic model with applications to coupled conflict-twitter data},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Incorporating compositional heterogeneity into lie markov
models for phylogenetic inference. <em>AOAS</em>, <em>14</em>(4),
1964–1983. (<a href="https://doi.org/10.1214/20-AOAS1369">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Phylogenetics uses alignments of molecular sequence data to learn about evolutionary trees. Substitutions in sequences are modelled through a continuous-time Markov process, characterised by an instantaneous rate matrix, which standard models assume is time-reversible and stationary. These assumptions are biologically questionable and induce a likelihood function which is invariant to a tree’s root position. This hampers inference because a tree’s biological interpretation depends critically on where it is rooted. Relaxing both assumptions, we introduce a model whose likelihood can distinguish between rooted trees. The model is nonstationary with step changes in the instantaneous rate matrix at each speciation event. Exploiting recent theoretical work, each rate matrix belongs to a nonreversible family of Lie Markov models. These models are closed under matrix multiplication, so our extension offers the conceptually appealing property that a tree and all its subtrees could have arisen from the same family of nonstationary models. We adopt a Bayesian approach, describe an MCMC algorithm for posterior inference and provide software. The biological insight that our model can provide is illustrated through an analysis in which nonreversible but stationary and nonstationary but reversible models cannot identify a plausible root.},
  archive      = {J_AOAS},
  author       = {Naomi E. Hannaford and Sarah E. Heaps and Tom M. W. Nye and Tom A. Williams and T. Martin Embley},
  doi          = {10.1214/20-AOAS1369},
  journal      = {The Annals of Applied Statistics},
  number       = {4},
  pages        = {1964-1983},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Incorporating compositional heterogeneity into lie markov models for phylogenetic inference},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Integrative statistical methods for exposure mixtures and
health. <em>AOAS</em>, <em>14</em>(4), 1945–1963. (<a
href="https://doi.org/10.1214/20-AOAS1364">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Humans are concurrently exposed to chemically, structurally and toxicologically diverse chemicals. A critical challenge for environmental epidemiology is to quantify the risk of adverse health outcomes resulting from exposures to such chemical mixtures and to identify which mixture constituents may be driving etiologic associations. A variety of statistical methods have been proposed to address these critical research questions. However, they generally rely solely on measured exposure and health data available within a specific study. Advancements in understanding of the role of mixtures on human health impacts may be better achieved through the utilization of external data and knowledge from multiple disciplines with innovative statistical tools. In this paper we develop new methods for health analyses that incorporate auxiliary information about the chemicals in a mixture, such as physicochemical, structural and/or toxicological data. We expect that the constituents identified using auxiliary information will be more biologically meaningful than those identified by methods that solely utilize observed correlations between measured exposure. We develop flexible Bayesian models by specifying prior distributions for the exposures and their effects that include auxiliary information and examine this idea over a spectrum of analyses from regression to factor analysis. The methods are applied to study the effects of volatile organic compounds on emergency room visits in Atlanta. We find that including cheminformatic information about the exposure variables improves prediction and provides a more interpretable model for emergency room visits for respiratory diseases.},
  archive      = {J_AOAS},
  author       = {Brian J. Reich and Yawen Guan and Denis Fourches and Joshua L. Warren and Stefanie E. Sarnat and Howard H. Chang},
  doi          = {10.1214/20-AOAS1364},
  journal      = {The Annals of Applied Statistics},
  number       = {4},
  pages        = {1945-1963},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Integrative statistical methods for exposure mixtures and health},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Bayesian inference for multistrain epidemics with
application to ESCHERICHIA COLI o157: H7 in feedlot cattle.
<em>AOAS</em>, <em>14</em>(4), 1925–1944. (<a
href="https://doi.org/10.1214/20-AOAS1366">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For most pathogens, testing procedures can be used to distinguish between different strains with which individuals are infected. Due to the growing availability of such data, multistrain models have increased in popularity over the past few years. Quantifying the interactions between different strains of a pathogen is crucial in order to obtain a more complete understanding of the transmission process, but statistical methods for this type of problem are still in the early stages of development. Motivated by this demand, we construct a stochastic epidemic model that incorporates additional strain information and propose a statistical algorithm for efficient inference. The model improves upon existing methods in the sense that it allows for both imperfect diagnostic test sensitivities and strain misclassification. Extensive simulation studies were conducted in order to assess the performance of our method, while the utility of the developed methodology is demonstrated on data obtained from a longitudinal study of Escherichia coli O157:H7 strains in feedlot cattle.},
  archive      = {J_AOAS},
  author       = {Panayiota Touloupou and Bärbel Finkenstädt and Thomas E. Besser and Nigel P. French and Simon E. F. Spencer},
  doi          = {10.1214/20-AOAS1366},
  journal      = {The Annals of Applied Statistics},
  number       = {4},
  pages        = {1925-1944},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Bayesian inference for multistrain epidemics with application to ESCHERICHIA COLI o157: H7 in feedlot cattle},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Bayesian profiling multiple imputation for missing
hemoglobin values in electronic health records. <em>AOAS</em>,
<em>14</em>(4), 1903–1924. (<a
href="https://doi.org/10.1214/20-AOAS1378">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Electronic health records (EHRs) are increasingly used for clinical and comparative effectiveness research but suffer from missing data. Motivated by health services research on diabetes care, we seek to increase the quality of EHRs by focusing on missing values of longitudinal glycosylated hemoglobin (A1c), a key risk factor for diabetes complications and adverse events. Under the framework of multiple imputation (MI), we propose an individualized Bayesian latent profiling approach to capture A1c measurement trajectories subject to missingness. The proposed method is applied to EHRs of adult patients with diabetes in a large academic Midwestern health system between 2003 and 2013 and had Medicare A and B coverage. We combine MI inferences to evaluate the association of A1c levels with the incidence of acute adverse health events and examine patient heterogeneity across identified patient profiles. We investigate different missingness mechanisms and perform imputation diagnostics. Our approach is computationally efficient and fits flexible models that provide useful clinical insights.},
  archive      = {J_AOAS},
  author       = {Yajuan Si and Mari Palta and Maureen Smith},
  doi          = {10.1214/20-AOAS1378},
  journal      = {The Annals of Applied Statistics},
  number       = {4},
  pages        = {1903-1924},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Bayesian profiling multiple imputation for missing hemoglobin values in electronic health records},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A bayesian time-varying effect model for behavioral mHealth
data. <em>AOAS</em>, <em>14</em>(4), 1878–1902. (<a
href="https://doi.org/10.1214/20-AOAS1402">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The integration of mobile health (mHealth) devices into behavioral health research has fundamentally changed the way researchers and interventionalists are able to collect data as well as deploy and evaluate intervention strategies. In these studies, researchers often collect intensive longitudinal data (ILD) using ecological momentary assessment methods which aim to capture psychological, emotional and environmental factors that may relate to a behavioral outcome in near real time. In order to investigate ILD collected in a novel, smartphone-based smoking cessation study, we propose a Bayesian variable selection approach for time-varying effect models, designed to identify dynamic relations between potential risk factors and smoking behaviors in the critical moments around a quit attempt. We use parameter-expansion and data-augmentation techniques to efficiently explore how the underlying structure of these relations varies over time and across subjects. We achieve deeper insights into these relations by introducing nonparametric priors for regression coefficients that cluster similar effects for risk factors while simultaneously determining their inclusion. Results indicate that our approach is well positioned to help researchers effectively evaluate, design and deliver tailored intervention strategies in the critical moments surrounding a quit attempt.},
  archive      = {J_AOAS},
  author       = {Matthew D. Koslovsky and Emily T. Hébert and Michael S. Businelle and Marina Vannucci},
  doi          = {10.1214/20-AOAS1402},
  journal      = {The Annals of Applied Statistics},
  number       = {4},
  pages        = {1878-1902},
  shortjournal = {Ann. Appl. Stat.},
  title        = {A bayesian time-varying effect model for behavioral mHealth data},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). RNDClone: Tumor subclone reconstruction based on integrating
DNA and RNA sequence data. <em>AOAS</em>, <em>14</em>(4), 1856–1877. (<a
href="https://doi.org/10.1214/20-AOAS1368">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Tumor cell population consists of genetically heterogeneous subpopulations, known as subclones. Bulk sequencing data using high-throughput sequencing technology provide total and variant DNA and RNA read counts for many nucleotide loci as a mixture of signals from different subclones. We present RNDClone as a tool to deconvolute the mixture and reconstruct the subclones with distinct DNA genotypes and RNA expression profiles. In particular, we infer the number and population frequencies of subclones as well as subclonal copy numbers, variant allele numbers and gene expression levels by jointly modeling DNA and RNA read counts from the same tumor samples based on generalized latent factor models. Incorporating data at the RNA level provides new insights into intra-tumor heterogeneity in addition to the existing DNA-based inference. Performance of RNDClone is assessed using simulated and real-world datasets, including an analysis of three samples from a lung cancer patient in The Cancer Genome Atlas (TCGA). A potential fatal subclone is identified from the primary tumor which could explain the rapid prognosis and sudden death of the patient despite a promising diagnosis by conventional standards. The R package $\mathtt{RNDClone}$ is available in the Supplementary Material (Zhou et al. (2020)) and online at https://github.com/tianjianzhou/RNDClone.},
  archive      = {J_AOAS},
  author       = {Tianjian Zhou and Subhajit Sengupta and Peter Müller and Yuan Ji},
  doi          = {10.1214/20-AOAS1368},
  journal      = {The Annals of Applied Statistics},
  number       = {4},
  pages        = {1856-1877},
  shortjournal = {Ann. Appl. Stat.},
  title        = {RNDClone: Tumor subclone reconstruction based on integrating DNA and RNA sequence data},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Mixture of hidden markov models for accelerometer data.
<em>AOAS</em>, <em>14</em>(4), 1834–1855. (<a
href="https://doi.org/10.1214/20-AOAS1375">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Motivated by the analysis of accelerometer data taken across a population of individuals, we introduce a specific finite mixture of hidden Markov models with particular characteristics that adapt well to the specific nature of this type of longitudinal data. Our model allows for the computation of statistics that characterize the physical activity of a subject (e.g., the mean time spent at different activity levels and the probability of the transition between two activity levels) without specifying the activity levels in advance but by estimating them from the data. In addition, this approach allows the heterogeneity of the population to be taken into account and subpopulations with homogeneous physical activity behavior to be defined. We prove that, under mild assumptions, this model implies that the probability of misclassifying a subject decreases at an exponential decay with the length of its measurement sequence. Model identifiability is also investigated. We also report a comprehensive suite of numerical simulations to support our theoretical findings. The method is motivated by and applied to the Physical Activity and Transit Survey.},
  archive      = {J_AOAS},
  author       = {Marie Du Roy de Chaumaray and Matthieu Marbac and Fabien Navarro},
  doi          = {10.1214/20-AOAS1375},
  journal      = {The Annals of Applied Statistics},
  number       = {4},
  pages        = {1834-1855},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Mixture of hidden markov models for accelerometer data},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). The statistical performance of matching-adjusted indirect
comparisons: Estimating treatment effects with aggregate external
control data. <em>AOAS</em>, <em>14</em>(4), 1806–1833. (<a
href="https://doi.org/10.1214/20-AOAS1359">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Indirect comparisons of treatment-specific outcomes across separate studies often inform decision making in the absence of head-to-head randomized comparisons. Differences in baseline characteristics between study populations may introduce confounding bias in such comparisons. Matching-adjusted indirect comparison (MAIC) (Pharmacoeconomics 28 (2010) 935–945) has been used to adjust for differences in observed baseline covariates when the individual patient-level data (IPD) are available for only one study and aggregate data (AGD) are available for the other study. The approach weights outcomes from the IPD using estimates of trial selection odds that balance baseline covariates between the IPD and AGD. With the increasing use of MAIC, there is a need for formal assessments of its statistical properties. In this paper we formulate identification assumptions for causal estimands that justify MAIC estimators. We then examine large sample properties and evaluate strategies for estimating standard errors without the full IPD from both studies. The finite-sample bias of MAIC and the performance of confidence intervals based on different standard error estimators are evaluated through simulations. The method is illustrated through an example comparing placebo arm and natural history outcomes in Duchenne muscular dystrophy.},
  archive      = {J_AOAS},
  author       = {David Cheng and Rajeev Ayyagari and James Signorovitch},
  doi          = {10.1214/20-AOAS1359},
  journal      = {The Annals of Applied Statistics},
  number       = {4},
  pages        = {1806-1833},
  shortjournal = {Ann. Appl. Stat.},
  title        = {The statistical performance of matching-adjusted indirect comparisons: Estimating treatment effects with aggregate external control data},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A nearest-neighbor based nonparametric test for viral
remodeling in heterogeneous single-cell proteomic data. <em>AOAS</em>,
<em>14</em>(4), 1777–1805. (<a
href="https://doi.org/10.1214/20-AOAS1362">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {An important problem in contemporary immunology studies based on single-cell protein expression data is to determine whether cellular expressions are remodeled postinfection by a pathogen. One natural approach for detecting such changes is to use nonparametric two-sample statistical tests. However, in single-cell studies direct application of these tests is often inadequate, because single-cell level expression data from processed uninfected populations often contain attributes of several latent subpopulations with highly heterogeneous characteristics. As a result, viruses often infect these different subpopulations at different rates, in which case the traditional nonparametric two-sample tests for checking similarity in distributions are no longer conservative. In this paper, we propose a new nonparametric method for Testing Remodeling under Heterogeneity (TRUH) that can accurately detect changes in the infected samples compared to possibly heterogeneous uninfected samples. Our testing framework is based on composite nulls and is designed to allow the null model to encompass the possibility that the infected samples, though unaltered by the virus, might be dominantly arising from underrepresented subpopulations in the baseline data. The TRUH statistic, which uses nearest neighbor projections of the infected samples into the baseline uninfected population, is calibrated using a novel bootstrap algorithm. We demonstrate the nonasymptotic performance of the test via simulation experiments and also derive the large sample limit of the test statistic which provides theoretical support toward consistent asymptotic calibration of the test. We use the TRUH statistic for studying remodeling in tonsillar T cells under different types of HIV infection and find that, unlike traditional tests which do not have any heterogeneity correction, TRUH based statistical inference conforms to the biologically validated immunological theories on HIV infection.},
  archive      = {J_AOAS},
  author       = {Trambak Banerjee and Bhaswar B. Bhattacharya and Gourab Mukherjee},
  doi          = {10.1214/20-AOAS1362},
  journal      = {The Annals of Applied Statistics},
  number       = {4},
  pages        = {1777-1805},
  shortjournal = {Ann. Appl. Stat.},
  title        = {A nearest-neighbor based nonparametric test for viral remodeling in heterogeneous single-cell proteomic data},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Effective model calibration via sensible variable
identification and adjustment with application to composite fuselage
simulation. <em>AOAS</em>, <em>14</em>(4), 1759–1776. (<a
href="https://doi.org/10.1214/20-AOAS1353">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Estimation of model parameters of computer simulators, also known as calibration, is an important topic in many engineering applications. In this paper we consider the calibration of computer model parameters with the help of engineering design knowledge. We introduce the concept of sensible (calibration) variables. Sensible variables are model parameters, which are sensitive in the engineering modeling, and whose optimal values differ from the engineering design values. We propose an effective calibration method to identify and to determine appropriate levels for the sensible variables with limited physical experimental data. The methodology is applied to a composite fuselage simulation problem.},
  archive      = {J_AOAS},
  author       = {Yan Wang and Xiaowei Yue and Rui Tuo and Jeffrey H. Hunt and Jianjun Shi},
  doi          = {10.1214/20-AOAS1353},
  journal      = {The Annals of Applied Statistics},
  number       = {4},
  pages        = {1759-1776},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Effective model calibration via sensible variable identification and adjustment with application to composite fuselage simulation},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Identifying main effects and interactions among exposures
using gaussian processes. <em>AOAS</em>, <em>14</em>(4), 1743–1758. (<a
href="https://doi.org/10.1214/20-AOAS1363">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article is motivated by the problem of studying the joint effect of different chemical exposures on human health outcomes. This is essentially a nonparametric regression problem, with interest being focused not on a black box for prediction but instead on selection of main effects and interactions. For interpretability we decompose the expected health outcome into a linear main effect, pairwise interactions and a nonlinear deviation. Our interest is in model selection for these different components, accounting for uncertainty and addressing nonidentifiability between the linear and nonparametric components of the semiparametric model. We propose a Bayesian approach to inference, placing variable selection priors on the different components, and developing a Markov chain Monte Carlo (MCMC) algorithm. A key component of our approach is the incorporation of a heredity constraint to only include interactions in the presence of main effects, effectively reducing dimensionality of the model search. We adapt a projection approach developed in the spatial statistics literature to enforce identifiability in modeling the nonparametric component using a Gaussian process. We also employ a dimension reduction strategy to sample the nonlinear random effects that aids the mixing of the MCMC algorithm. The proposed MixSelect framework is evaluated using a simulation study, and is illustrated using data from the National Health and Nutrition Examination Survey (NHANES). Code is available on GitHub.},
  archive      = {J_AOAS},
  author       = {Federico Ferrari and David B. Dunson},
  doi          = {10.1214/20-AOAS1363},
  journal      = {The Annals of Applied Statistics},
  number       = {4},
  pages        = {1743-1758},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Identifying main effects and interactions among exposures using gaussian processes},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Classification from only positive and unlabeled functional
data. <em>AOAS</em>, <em>14</em>(4), 1724–1742. (<a
href="https://doi.org/10.1214/20-AOAS1404">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In various fields, data recorded continuously during a time interval and curve data, such as spectral data, become common. These kinds of data can be interpreted as functional data. In this paper we have studied binary classification from only positive and unlabeled functional data (PU classification for functional data). Our first contribution is to present a simple classification algorithm for this problem. The key feature of the algorithm is that it is not required an estimation of the unknown class prior (or the constant probability that a positive object is labeled). It is worth noting that the idea of our method can be applied to kernel linear discriminant analysis for general data. Our second contribution is to prove that, under mild regularity conditions similar to those in a supervised context, the proposed algorithm can achieve perfect asymptotic classification in the context of PU classification. In fact, we show that the proposed algorithm works well not only in numerical experiments but also for real data examples. Moreover, as an important practical application, we have used the proposed algorithm to identify handball players at risk for anterior cruciate ligament (ACL) injury based on ground reaction force data.},
  archive      = {J_AOAS},
  author       = {Yoshikazu Terada and Issei Ogasawara and Ken Nakata},
  doi          = {10.1214/20-AOAS1404},
  journal      = {The Annals of Applied Statistics},
  number       = {4},
  pages        = {1724-1742},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Classification from only positive and unlabeled functional data},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Mining events with declassified diplomatic documents.
<em>AOAS</em>, <em>14</em>(4), 1699–1723. (<a
href="https://doi.org/10.1214/20-AOAS1344">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Since 1973, the U.S. State Department has been using electronic record systems to preserve classified communications. Recently, approximately 1.9 million of these records from 1973–77 have been made available by the U.S. National Archives. While some of these communication streams have periods witnessing an acceleration in the rate of transmission, others do not show any notable patterns in communication intensity. Given the sheer volume of these communications, far greater than what had been available until now, scholars need automated statistical techniques to identify the communications that warrant closer study. We develop a statistical framework that can identify from a large corpus of documents a handful that historians would consider more interesting. Our approach brings together techniques from nonparametric signal estimation, statistical hypothesis testing and modern optimization methods—leading to a set of tools that help us identify and analyze various geometrical aspects of the communication streams. Dominant periods of heightened activities, as identified through these methods, correspond well with historical events recognized by standard reference works on the 1970s.},
  archive      = {J_AOAS},
  author       = {Yuanjun Gao and Jack Goetz and Matthew Connelly and Rahul Mazumder},
  doi          = {10.1214/20-AOAS1344},
  journal      = {The Annals of Applied Statistics},
  number       = {4},
  pages        = {1699-1723},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Mining events with declassified diplomatic documents},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Feature selection for data integration with mixed multiview
data. <em>AOAS</em>, <em>14</em>(4), 1676–1698. (<a
href="https://doi.org/10.1214/20-AOAS1389">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data integration methods that analyze multiple sources of data simultaneously can often provide more holistic insights than can separate inquiries of each data source. Motivated by the advantages of data integration in the era of “big data,” we investigate feature selection for high-dimensional multiview data with mixed data types (e.g., continuous, binary, count-valued). This heterogeneity of multiview data poses numerous challenges for existing feature selection methods. However, after critically examining these issues through empirical and theoretically-guided lenses, we develop a practical solution, the Block Randomized Adaptive Iterative Lasso (B-RAIL) which combines the strengths of the randomized Lasso, adaptive weighting schemes and stability selection. B-RAIL serves as a versatile data integration method for sparse regression and graph selection, and we demonstrate the effectiveness of B-RAIL through extensive simulations and a case study to infer the ovarian cancer gene regulatory network. In this case study, B-RAIL successfully identifies well-known biomarkers associated with ovarian cancer and hints at novel candidates for future ovarian cancer research.},
  archive      = {J_AOAS},
  author       = {Yulia Baker and Tiffany M. Tang and Genevera I. Allen},
  doi          = {10.1214/20-AOAS1389},
  journal      = {The Annals of Applied Statistics},
  number       = {4},
  pages        = {1676-1698},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Feature selection for data integration with mixed multiview data},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Data fusion model for speciated nitrogen to identify
environmental drivers and improve estimation of nitrogen in lakes.
<em>AOAS</em>, <em>14</em>(4), 1651–1675. (<a
href="https://doi.org/10.1214/20-AOAS1371">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Concentrations of nitrogen provide a critical metric for understanding ecosystem function and water quality in lakes. However, varying approaches for quantifying nitrogen concentrations may bias the comparison of water quality across lakes and regions. Different measurements of total nitrogen exist based on its composition (e.g., organic versus inorganic, dissolved versus particulate), which we refer to as nitrogen species. Fortunately, measurements of multiple nitrogen species are often collected and can, therefore, be leveraged together to inform our understanding of the controls on total nitrogen in lakes. We develop a multivariate hierarchical statistical model that fuses speciated nitrogen measurements, obtained across multiple methods of reporting, in order to improve our estimates of total nitrogen. The model accounts for lower detection limits and measurement error that vary across lake, species and observation. By modeling speciated nitrogen, as opposed to previous efforts that mostly consider only total nitrogen, we obtain more resolved inference with regard to differences in sources of nitrogen and their relationship with complex environmental drivers. We illustrate the inferential benefits of our model using speciated nitrogen data from the LAke GeOSpatial and temporal database (LAGOS).},
  archive      = {J_AOAS},
  author       = {Erin M. Schliep and Sarah M. Collins and Shirley Rojas-Salazar and Noah R. Lottig and Emily H. Stanley},
  doi          = {10.1214/20-AOAS1371},
  journal      = {The Annals of Applied Statistics},
  number       = {4},
  pages        = {1651-1675},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Data fusion model for speciated nitrogen to identify environmental drivers and improve estimation of nitrogen in lakes},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Monotonic effects of characteristics on returns.
<em>AOAS</em>, <em>14</em>(4), 1622–1650. (<a
href="https://doi.org/10.1214/20-AOAS1351">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper considers the problem of modeling a firm’s expected return as a nonlinear function of its observable characteristics. We investigate whether theoretically-motivated monotonicity constraints on characteristics and nonstationarity of the conditional expectation function provide statistical and economic benefit. We present an interpretable model that has similar out-of-sample performance to black-box machine learning methods. With this model, the data provide support for monotonicity and time variability of the conditional expectation function. Additionally, we develop an approach for characteristic selection using loss functions to summarize the posterior distribution. Standard unexplained volume, short-term reversal, size, and variants of momentum are found to be significant characteristics, and there is evidence this set changes over time.},
  archive      = {J_AOAS},
  author       = {Jared D. Fisher and David W. Puelz and Carlos M. Carvalho},
  doi          = {10.1214/20-AOAS1351},
  journal      = {The Annals of Applied Statistics},
  number       = {4},
  pages        = {1622-1650},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Monotonic effects of characteristics on returns},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Measuring timeliness of annual reports filing by jump
additive models. <em>AOAS</em>, <em>14</em>(4), 1604–1621. (<a
href="https://doi.org/10.1214/20-AOAS1365">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Foreign public issuers (FPIs) are required by the Securities and Exchanges Commission (SEC) to file Form 20-F as comprehensive annual reports. In an effort to increase the usefulness of 20-Fs, the SEC recently enacted a regulation to accelerate the deadline of 20-F filing from six months to four months after the fiscal year-end. The rationale is that the shortened reporting lag would improve the informational relevance of 20-Fs. In this work we propose a jump additive model to evaluate the SEC’s rationale by investigating the relationship between the timeliness of 20-F filing and its decision usefulness using the market data. The proposed model extends the conventional additive models to allow possible discontinuities in the regression functions. We suggest a two-step jump-preserving estimation procedure and show that it is statistically consistent. By applying the procedure to the 20-F study, we find a moderate positive association between the magnitude of the market reaction and the filing timeliness when the acceleration is less than $17$ days. We also find that the market considers the filings significantly more informative when the acceleration is more than $18$ days and such reaction tapers off when the acceleration exceeds $40$ days.},
  archive      = {J_AOAS},
  author       = {Yicheng Kang},
  doi          = {10.1214/20-AOAS1365},
  journal      = {The Annals of Applied Statistics},
  number       = {4},
  pages        = {1604-1621},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Measuring timeliness of annual reports filing by jump additive models},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Hierarchical multidimensional scaling for the comparison of
musical performance styles. <em>AOAS</em>, <em>14</em>(4), 1581–1603.
(<a href="https://doi.org/10.1214/20-AOAS1391">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Quantification of stylistic differences between musical artists is of academic interest to the music community and is also useful for other applications, such as music information retrieval and recommendation systems. Information about stylistic differences can be obtained by comparing the performances of different artists across common musical pieces. In this article we develop a statistical methodology for identifying and quantifying systematic stylistic differences among artists that are consistent across audio recordings of a common set of pieces, in terms of several musical features. Our focus is on a comparison of 10 different orchestras, based on data from audio recordings of the nine Beethoven symphonies. As generative or fully parametric models of raw audio data can be highly complex and more complex than necessary for our goal of identifying differences between orchestras, we propose to reduce the data from a set of audio recordings down to pairwise distances between orchestras, based on different musical characteristics of the recordings, such as tempo, dynamics and timbre. For each of these characteristics, we obtain multiple pairwise distance matrices, one for each movement of each symphony. We develop a hierarchical multidimensional scaling (HMDS) model to identify and quantify systematic differences between orchestras in terms of these three musical characteristics and interpret the results in the context of known qualitative information about the orchestras. This methodology is able to recover several expected systematic similarities between orchestras as well as to identify some more novel results. For example, we find that modern recordings exhibit a high degree of similarity to each other, as compared to older recordings.},
  archive      = {J_AOAS},
  author       = {Anna K. Yanchenko and Peter D. Hoff},
  doi          = {10.1214/20-AOAS1391},
  journal      = {The Annals of Applied Statistics},
  number       = {4},
  pages        = {1581-1603},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Hierarchical multidimensional scaling for the comparison of musical performance styles},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Inferring a consensus problem list using penalized
multistage models for ordered data. <em>AOAS</em>, <em>14</em>(3),
1557–1580. (<a href="https://doi.org/10.1214/20-AOAS1361">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A patient’s medical problem list describes his or her current health status and aids in the coordination and transfer of care between providers. Because a problem list is generated once and then subsequently modified or updated, what is not usually observable is the provider-effect. That is, to what extent does a patient’s problem in the electronic medical record actually reflect a consensus communication of that patient’s current health status? To that end, we report on and analyze a unique interview-based design in which multiple medical providers independently generate problem lists for each of three patient case abstracts of varying clinical difficulty. Due to the uniqueness of both our data and the scientific objectives of our analysis, we apply and extend so-called multistage models for ordered lists and equip the models with variable selection penalties to induce sparsity. Each problem has a corresponding nonnegative parameter estimate, interpreted as a relative log-odds ratio, with larger values suggesting greater importance and zero values suggesting unimportant problems. We use these fitted penalized models to quantify and report the extent of consensus. We conduct a simulation study to evaluate the performance of our methodology and then analyze the motivating problem list data. For the three case abstracts, the proportions of problems with model-estimated nonzero log-odds ratios were $10/28$, $16/47$ and $13/30$. Physicians exhibited consensus on the highest ranked problems in the first and last case abstracts but agreement quickly deteriorated; in contrast, physicians broadly disagreed on the relevant problems for the middle—and most difficult—case abstract.},
  archive      = {J_AOAS},
  author       = {Philip S. Boonstra and John C. Krauss},
  doi          = {10.1214/20-AOAS1361},
  journal      = {The Annals of Applied Statistics},
  number       = {3},
  pages        = {1557-1580},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Inferring a consensus problem list using penalized multistage models for ordered data},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Log-contrast regression with functional compositional
predictors: Linking preterm infants’ gut microbiome trajectories to
neurobehavioral outcome. <em>AOAS</em>, <em>14</em>(3), 1535–1556. (<a
href="https://doi.org/10.1214/20-AOAS1357">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The neonatal intensive care unit (NICU) experience is known to be one of the most crucial factors that drive preterm infants’ neurodevelopmental and health outcome. It is hypothesized that stressful early life experience of very preterm neonate is imprinting gut microbiome by the regulation of the so-called brain-gut axis, and, consequently, certain microbiome markers are predictive of later infant neurodevelopment. To investigate, a preterm infant study was conducted; infant fecal samples were collected during the infants’ first month of postnatal age, resulting in functional compositional microbiome data, and neurobehavioral outcomes were measured when infants reached 36–38 weeks of postmenstrual age. To identify potential microbiome markers and estimate how the trajectories of gut microbiome compositions during early postnatal stage impact later neurobehavioral outcomes of the preterm infants, we innovate a sparse log-contrast regression with functional compositional predictors. The functional simplex structure is strictly preserved, and the functional compositional predictors are allowed to have sparse, smoothly varying and accumulating effects on the outcome through time. Through a pragmatic basis expansion step, the problem boils down to a linearly constrained sparse group regression, for which we develop an efficient algorithm and obtain theoretical performance guarantees. Our approach yields insightful results in the preterm infant study. The identified microbiome markers and the estimated time dynamics of their impact on the neurobehavioral outcome shed lights on the linkage between stress accumulation in early postnatal stage and neurodevelpomental process of infants.},
  archive      = {J_AOAS},
  author       = {Zhe Sun and Wanli Xu and Xiaomei Cong and Gen Li and Kun Chen},
  doi          = {10.1214/20-AOAS1357},
  journal      = {The Annals of Applied Statistics},
  number       = {3},
  pages        = {1535-1556},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Log-contrast regression with functional compositional predictors: Linking preterm infants’ gut microbiome trajectories to neurobehavioral outcome},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Identifying overlapping terrorist cells from the noordin top
actor–event network. <em>AOAS</em>, <em>14</em>(3), 1516–1534. (<a
href="https://doi.org/10.1214/20-AOAS1358">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Actor–event data are common in sociological settings, whereby one registers the pattern of attendance of a group of social actors to a number of events. We focus on 79 members of the Noordin Top terrorist network, who were monitored attending 45 events. The attendance or nonattendance of the terrorist to events defines the social fabric, such as group coherence and social communities. The aim of the analysis of such data is to learn about the affiliation structure. Actor–event data is often transformed to actor–actor data in order to be further analysed by network models, such as stochastic block models. This transformation and such analyses lead to a natural loss of information, particularly when one is interested in identifying, possibly overlapping, subgroups or communities of actors on the basis of their attendances to events. In this paper we propose an actor–event model for overlapping communities of terrorists which simplifies interpretation of the network. We propose a mixture model with overlapping clusters for the analysis of the binary actor–event network data, called $\mathtt{manet}$, and develop a Bayesian procedure for inference. After a simulation study, we show how this analysis of the terrorist network has clear interpretative advantages over the more traditional approaches of affiliation network analysis.},
  archive      = {J_AOAS},
  author       = {Saverio Ranciati and Veronica Vinciotti and Ernst C. Wit},
  doi          = {10.1214/20-AOAS1358},
  journal      = {The Annals of Applied Statistics},
  number       = {3},
  pages        = {1516-1534},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Identifying overlapping terrorist cells from the noordin top actor–event network},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Adaptive log-linear zero-inflated generalized poisson
autoregressive model with applications to crime counts. <em>AOAS</em>,
<em>14</em>(3), 1493–1515. (<a
href="https://doi.org/10.1214/20-AOAS1360">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This research proposes a comprehensive ALG model (Adaptive Log-linear zero-inflated Generalized Poisson integer-valued GARCH) to describe the dynamics of integer-valued time series of crime incidents with the features of autocorrelation, heteroscedasticity, overdispersion and excessive number of zero observations. The proposed ALG model captures time-varying nonlinear dependence and simultaneously incorporates the impact of multiple exogenous variables in a unified modeling framework. We use an adaptive approach to automatically detect subsamples of local homogeneity at each time point of interest and estimate the time-dependent parameters through an adaptive Bayesian Markov chain Monte Carlo (MCMC) sampling scheme. A simulation study shows stable and accurate finite sample performances of the ALG model under both homogeneous and heterogeneous scenarios. When implemented with data on crime incidents in Byron, Australia, the ALG model delivers a persuasive estimation of the stochastic intensity of criminal incidents and provides insightful interpretations on both the dynamics of intensity and the impacts of temperature and demographic factors for different crime categories.},
  archive      = {J_AOAS},
  author       = {Xiaofei Xu and Ying Chen and Cathy W. S. Chen and Xiancheng Lin},
  doi          = {10.1214/20-AOAS1360},
  journal      = {The Annals of Applied Statistics},
  number       = {3},
  pages        = {1493-1515},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Adaptive log-linear zero-inflated generalized poisson autoregressive model with applications to crime counts},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A bayesian model of microbiome data for simultaneous
identification of covariate associations and prediction of phenotypic
outcomes. <em>AOAS</em>, <em>14</em>(3), 1471–1492. (<a
href="https://doi.org/10.1214/20-AOAS1354">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {One of the major research questions regarding human microbiome studies is the feasibility of designing interventions that modulate the composition of the microbiome to promote health and to cure disease. This requires extensive understanding of the modulating factors of the microbiome, such as dietary intake, as well as the relation between microbial composition and phenotypic outcomes, such as body mass index (BMI). Previous efforts have modeled these data separately, employing two-step approaches that can produce biased interpretations of the results. Here, we propose a Bayesian joint model that simultaneously identifies clinical covariates associated with microbial composition data and predicts a phenotypic response using information contained in the compositional data. Using spike-and-slab priors, our approach can handle high-dimensional compositional as well as clinical data. Additionally, we accommodate the compositional structure of the data via balances and overdispersion typically found in microbial samples. We apply our model to understand the relations between dietary intake, microbial samples and BMI. In this analysis we find numerous associations between microbial taxa and dietary factors that may lead to a microbiome that is generally more hospitable to the development of chronic diseases, such as obesity. Additionally, we demonstrate on simulated data how our method outperforms two-step approaches and also present a sensitivity analysis.},
  archive      = {J_AOAS},
  author       = {Matthew D. Koslovsky and Kristi L. Hoffman and Carrie R. Daniel and Marina Vannucci},
  doi          = {10.1214/20-AOAS1354},
  journal      = {The Annals of Applied Statistics},
  number       = {3},
  pages        = {1471-1492},
  shortjournal = {Ann. Appl. Stat.},
  title        = {A bayesian model of microbiome data for simultaneous identification of covariate associations and prediction of phenotypic outcomes},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A bayesian hierarchical model for evaluating forensic
footwear evidence. <em>AOAS</em>, <em>14</em>(3), 1449–1470. (<a
href="https://doi.org/10.1214/20-AOAS1334">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {When a latent shoeprint is discovered at a crime scene, forensic analysts inspect it for distinctive patterns of wear such as scratches and holes (known as accidentals) on the source shoe’s sole. If its accidentals correspond to those of a suspect’s shoe, the print can be used as forensic evidence to place the suspect at the crime scene. The strength of this evidence depends on the random match probability—the chance that a shoe chosen at random would match the crime scene print’s accidentals. Evaluating random match probabilities requires an accurate model for the spatial distribution of accidentals on shoe soles. A recent report by the President’s Council of Advisors in Science and Technology criticized existing models in the literature, calling for new empirically validated techniques. We respond to this request with a new spatial point process model (code and synthetic data is available as Supplementary Material) for accidental locations, developed within a hierarchical Bayesian framework. We treat the tread pattern of each shoe as a covariate, allowing us to pool information across large heterogeneous databases of shoes. Existing models ignore this information; our results show that including it leads to significantly better model fit. We demonstrate this by fitting our model to one such database.},
  archive      = {J_AOAS},
  author       = {Neil A. Spencer and Jared S. Murray},
  doi          = {10.1214/20-AOAS1334},
  journal      = {The Annals of Applied Statistics},
  number       = {3},
  pages        = {1449-1470},
  shortjournal = {Ann. Appl. Stat.},
  title        = {A bayesian hierarchical model for evaluating forensic footwear evidence},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Causal inference from observational studies with clustered
interference, with application to a cholera vaccine study.
<em>AOAS</em>, <em>14</em>(3), 1432–1448. (<a
href="https://doi.org/10.1214/19-AOAS1314">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Understanding the population-level effects of vaccines has important public health policy implications. Inferring vaccine effects from an observational study is challenging because participants are not randomized to vaccine (i.e., treatment). Observational studies of infectious diseases present the additional challenge that vaccinating one participant may affect another participant’s outcome, that is, there may be interference. In this paper recent approaches to defining vaccine effects in the presence of interference are considered, and new causal estimands designed specifically for use with observational studies are proposed. Previously defined estimands target counterfactual scenarios in which individuals independently choose to be vaccinated with equal probability. However, in settings where there is interference between individuals within clusters, it may be unlikely that treatment selection is independent between individuals in the same cluster. The proposed causal estimands instead describe counterfactual scenarios which allow for within-cluster dependence in the individual treatment selections. These estimands may be more relevant for policy-makers or public health officials who desire to quantify the effect of increasing the proportion of vaccinated individuals in a population. Inverse probability-weighted estimators for these estimands are proposed. The large-sample properties of the estimators are derived, and a simulation study demonstrating the finite-sample performance of the estimators is presented. The proposed methods are illustrated by analyzing data from a study of cholera vaccination in over 100,000 individuals in Bangladesh.},
  archive      = {J_AOAS},
  author       = {Brian G. Barkley and Michael G. Hudgens and John D. Clemens and Mohammad Ali and Michael E. Emch},
  doi          = {10.1214/19-AOAS1314},
  journal      = {The Annals of Applied Statistics},
  number       = {3},
  pages        = {1432-1448},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Causal inference from observational studies with clustered interference, with application to a cholera vaccine study},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Doubly robust treatment effect estimation with missing
attributes. <em>AOAS</em>, <em>14</em>(3), 1409–1431. (<a
href="https://doi.org/10.1214/20-AOAS1356">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Missing attributes are ubiquitous in causal inference, as they are in most applied statistical work. In this paper we consider various sets of assumptions under which causal inference is possible despite missing attributes and discuss corresponding approaches to average treatment effect estimation, including generalized propensity score methods and multiple imputation. Across an extensive simulation study, we show that no single method systematically outperforms others. We find, however, that doubly robust modifications of standard methods for average treatment effect estimation with missing data repeatedly perform better than their nondoubly robust baselines; for example, doubly robust generalized propensity score methods beat inverse-weighting with the generalized propensity score. This finding is reinforced in an analysis of an observational study on the effect on mortality of tranexamic acid administration among patients with traumatic brain injury in the context of critical care management. Here, doubly robust estimators recover confidence intervals that are consistent with evidence from randomized trials, whereas nondoubly robust estimators do not.},
  archive      = {J_AOAS},
  author       = {Imke Mayer and Erik Sverdrup and Tobias Gauss and Jean-Denis Moyer and Stefan Wager and Julie Josse},
  doi          = {10.1214/20-AOAS1356},
  journal      = {The Annals of Applied Statistics},
  number       = {3},
  pages        = {1409-1431},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Doubly robust treatment effect estimation with missing attributes},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Quantifying time-varying sources in magnetoencephalography—a
discrete approach. <em>AOAS</em>, <em>14</em>(3), 1379–1408. (<a
href="https://doi.org/10.1214/19-AOAS1321">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study the distribution of brain source from the most advanced brain imaging technique, Magnetoencephalography (MEG) which measures the magnetic fields outside of the human head produced by the electrical activity inside the brain. Common time-varying source localization methods assume the source current with a time-varying structure and solve the MEG inverse problem by mainly estimating the source moment parameters. These methods use the fact that the magnetic fields linearly depend on the moment parameters of the source and work well under the linear dynamic system. However, magnetic fields are known to be nonlinearly related to the location parameters of the source. The existing work on estimating the time-varying unknown location parameters is limited. We are motivated to investigate the source distribution for the location parameters based on a dynamic framework, where the posterior distribution of the source is computed in a closed form discretely. The new framework allows us not only to directly approximate the posterior distribution of the source current, where sequential sampling methods may suffer from slow convergence due to the large volume of measurement, but also to quantify the source distribution at any time point from the entire set of measurements reflecting the distribution of the source, rather than using only the measurements up to the time point of interest. Both a dynamic procedure and a switch procedure are pro- posed for the new discrete approach, balancing estimation accuracy and computational efficiency when multiple sources are present. In both simulation and real data, we illustrate that the new method is able to provide comprehensive insight into the time evolution of the sources at different stages of the MEG and EEG experiment.},
  archive      = {J_AOAS},
  author       = {Zhigang Yao and Zengyan Fan and Masahito Hayashi and William F. Eddy},
  doi          = {10.1214/19-AOAS1321},
  journal      = {The Annals of Applied Statistics},
  number       = {3},
  pages        = {1379-1408},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Quantifying time-varying sources in magnetoencephalography—A discrete approach},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Spatiotemporal probabilistic wind vector forecasting over
saudi arabia. <em>AOAS</em>, <em>14</em>(3), 1359–1378. (<a
href="https://doi.org/10.1214/20-AOAS1347">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Saudi Arabia has recently begun promoting renewable energy as a potential alternative to fossil fuels for domestic power generation. In order to efficiently connect wind energy to the existing power grids, reliable wind forecasts and an accurate way of quantifying the uncertainties of these forecasts are required. Motivated by a data set of hourly wind speeds from 28 stations in Saudi Arabia, we build spatiotemporal models for short-term probabilistic forecasts of wind vectors. Traditionally, wind speed and wind direction have been considered independently, without taking dependencies into account. However, in many situations, for example, energy management, it is essential to have information on the bivariate nature of the wind. We compare a coregionalization model for the wind vector with a univariate spatiotemporal model for the transformed wind speed in terms of sharpness and calibration. In both cases the linear predictor is a function of covariates, a smooth function to capture the daily seasonality in the wind and a latent Gaussian field to model the spatial and temporal dependencies. Substantial improvements in reliability are observed when modelling the full bivariate structure instead of only considering speed. Furthermore, the bivariate model has the advantage of also producing forecasts for the wind direction. A Bayesian framework is used to obtain forecasts that are accurate and reliable, even at stations without observations, with relatively low computational cost. Simulated high-resolution data from a computer model are used to validate spatiotemporal forecasts. A detailed analysis on this case study shows how increasing the number of locations can improve the forecast performance.},
  archive      = {J_AOAS},
  author       = {Amanda Lenzi and Marc G. Genton},
  doi          = {10.1214/20-AOAS1347},
  journal      = {The Annals of Applied Statistics},
  number       = {3},
  pages        = {1359-1378},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Spatiotemporal probabilistic wind vector forecasting over saudi arabia},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Climate extreme event attribution using multivariate
peaks-over-thresholds modeling and counterfactual theory. <em>AOAS</em>,
<em>14</em>(3), 1342–1358. (<a
href="https://doi.org/10.1214/20-AOAS1355">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Numerical climate models are complex and combine a large number of physical processes. They are key tools in quantifying the relative contribution of potential anthropogenic causes (e.g., the current increase in greenhouse gases) on high-impact atmospheric variables like heavy rainfall. These so-called climate extreme event attribution problems are particularly challenging in a multivariate context, that is, when the atmospheric variables are measured on a possibly high-dimensional grid. In this paper we leverage two statistical theories to assess causality in the context of multivariate extreme event attribution. As we consider an event to be extreme when at least one of the components of the vector of interest is large, extreme-value theory justifies, in an asymptotical sense, a multivariate generalized Pareto distribution to model joint extremes. Under this class of distributions, we derive and study probabilities of necessary and sufficient causation as defined by the counterfactual theory of Pearl. To increase causal evidence, we propose a dimension reduction strategy based on the optimal linear projection that maximizes such causation probabilities. Our approach is tested on simulated examples and applied to weekly winter maxima precipitation outputs of the French CNRM from the recent CMIP6 experiment.},
  archive      = {J_AOAS},
  author       = {Anna Kiriliouk and Philippe Naveau},
  doi          = {10.1214/20-AOAS1355},
  journal      = {The Annals of Applied Statistics},
  number       = {3},
  pages        = {1342-1358},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Climate extreme event attribution using multivariate peaks-over-thresholds modeling and counterfactual theory},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). The jensen effect and functional single index models:
Estimating the ecological implications of nonlinear reaction norms.
<em>AOAS</em>, <em>14</em>(3), 1326–1341. (<a
href="https://doi.org/10.1214/20-AOAS1349">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper develops tools to characterize how species are affected by environmental variability, based on a functional single index model relating a response such as growth rate to environmental conditions. In ecology the curvature of such responses are used, via Jensen’s inequality, to determine whether environmental variability is harmful or beneficial, and differing nonlinear responses to environmental variability can contribute to the coexistence of competing species. Here, we address estimation and inference for these models with observational data on individual responses to environmental conditions. Because nonparametric estimation of the curvature (second derivative) in a nonparametric functional single index model requires unrealistic sample sizes, we instead focus on directly estimating the effect of the nonlinearity by comparing the average response to a variable environment with the response at the expected environment, which we call the Jensen Effect. We develop a test statistic to assess whether this effect is significantly different from zero. In doing so we reinterpret the SiZer method of Chaudhuri and Marron (J. Amer. Statist. Assoc. 94 (1999) 807–823) by maximizing a test statistic over smoothing parameters. We show that our proposed method works well both in simulations and on real ecological data from the long-term data set described in Drake (Proc. R. Soc. Lond., B Biol. Sci. 272 (2005) 1823–1827).},
  archive      = {J_AOAS},
  author       = {Zi Ye and Giles Hooker and Stephen P. Ellner},
  doi          = {10.1214/20-AOAS1349},
  journal      = {The Annals of Applied Statistics},
  number       = {3},
  pages        = {1326-1341},
  shortjournal = {Ann. Appl. Stat.},
  title        = {The jensen effect and functional single index models: Estimating the ecological implications of nonlinear reaction norms},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). PTEM: A popularity-based topical expertise model for
community question answering. <em>AOAS</em>, <em>14</em>(3), 1304–1325.
(<a href="https://doi.org/10.1214/20-AOAS1346">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Community Question Answering (CQA) websites are widely used in sharing knowledge, where users can ask questions, reply answers and evaluate answers. So far, the evaluation of answers has been explained by the contents of answers through the investigation of users’ topics of interest and expertise levels. In this paper we focus on modeling the user’s evaluation behavior, in that users can see the answerer’s profile as well as the answer content before evaluating the quality of the answer. We propose a model called Popularity-based Topical Expertise Model (PTEM), a generative model to analyze the rich-get-richer phenomenon that popular user’s answers are more recommended. We can simultaneously estimate the topical expertise of each user and the strength of the rich-get-richer effect through the EM algorithm combined with collapsed Gibbs sampling. Experiments are performed on the StackExchange data, and the results demonstrate a rich-get-richer phenomenon in the community. We further discuss the superiority and usefulness of the proposed model through analysis in the discipline of philosophy.},
  archive      = {J_AOAS},
  author       = {Hohyun Jung and Jae-Gil Lee and Namgil Lee and Sung-Ho Kim},
  doi          = {10.1214/20-AOAS1346},
  journal      = {The Annals of Applied Statistics},
  number       = {3},
  pages        = {1304-1325},
  shortjournal = {Ann. Appl. Stat.},
  title        = {PTEM: A popularity-based topical expertise model for community question answering},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Does terrorism trigger online hate speech? On the
association of events and time series. <em>AOAS</em>, <em>14</em>(3),
1285–1303. (<a href="https://doi.org/10.1214/20-AOAS1338">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hate speech is ubiquitous on the Web. Recently, the offline causes that contribute to online hate speech have received increasing attention. A recurring question is whether the occurrence of extreme events offline systematically triggers bursts of hate speech online, indicated by peaks in the volume of hateful social media posts. Formally, this question translates into measuring the association between a sparse event series and a time series. We propose a novel statistical methodology to measure, test and visualize the systematic association between rare events and peaks in a time series. In contrast to previous methods for causal inference or independence tests on time series, our approach focuses only on the timing of events and peaks and no other distributional characteristics. We follow the framework of event coincidence analysis (ECA) that was originally developed to correlate point processes. We formulate a discrete-time variant of ECA and derive all required distributions to enable analyses of peaks in time series with a special focus on serial dependencies and peaks over multiple thresholds. The analysis gives rise to a novel visualization of the association via quantile-trigger rate plots. We demonstrate the utility of our approach by analyzing whether Islamist terrorist attacks in Western Europe and North America systematically trigger bursts of hate speech and counter-hate speech on Twitter.},
  archive      = {J_AOAS},
  author       = {Erik Scharwächter and Emmanuel Müller},
  doi          = {10.1214/20-AOAS1338},
  journal      = {The Annals of Applied Statistics},
  number       = {3},
  pages        = {1285-1303},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Does terrorism trigger online hate speech? on the association of events and time series},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A novel change-point approach for the detection of gas
emission sources using remotely contained concentration data.
<em>AOAS</em>, <em>14</em>(3), 1258–1284. (<a
href="https://doi.org/10.1214/20-AOAS1345">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Motivated by an example from remote sensing of gas emission sources, we derive two novel change-point procedures for multivariate time series where, in contrast to classical change-point literature, the changes are not required to be aligned in the different components of the time series. Instead, the change points are described by a functional relationship where the precise shape depends on unknown parameters of interest such as the source of the gas emission in the above example. Two different types of tests and the corresponding estimators for the unknown parameters describing the change locations are proposed. We derive the null asymptotics for both tests under weak assumptions on the error time series and show asymptotic consistency under alternatives. Furthermore, we prove consistency for the corresponding estimators of the parameters of interest. The small-sample behavior of the methodology is assessed by means of a simulation study, and the above remote sensing example analyzed in detail.},
  archive      = {J_AOAS},
  author       = {Idris Eckley and Claudia Kirch and Silke Weber},
  doi          = {10.1214/20-AOAS1345},
  journal      = {The Annals of Applied Statistics},
  number       = {3},
  pages        = {1258-1284},
  shortjournal = {Ann. Appl. Stat.},
  title        = {A novel change-point approach for the detection of gas emission sources using remotely contained concentration data},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A semiparametric mixture method for local false discovery
rate estimation from multiple studies. <em>AOAS</em>, <em>14</em>(3),
1242–1257. (<a href="https://doi.org/10.1214/20-AOAS1341">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Antineutrophil cytoplasmic antibody associated vasculitis (AAV) is extremely heterogeneous in clinical presentation and involves multiple organ systems. While the clinical presentation of AAV is diverse, we hypothesized that all AAV share common pathways and tested the hypothesis based on three different microarray studies of peripheral leukocytes, sinus and orbital inflammation disease. For the hypothesis testing we developed a two-component semiparametric mixture model to estimate the local false discovery rates from the $p$-values of three studies. The two pillars of the proposed approach are Efron’s empirical null principle and log-concave density estimation for the alternative distribution. Our method outperforms other existing methods, in particular when the proportion of null is not that high. It is robust against the misspecification of alternative distribution. A unique feature of our method is that it can be extended to compute the local false discovery rates by combining multiple lists of $p$-values.},
  archive      = {J_AOAS},
  author       = {Seok-Oh Jeong and Dongseok Choi and Woncheol Jang},
  doi          = {10.1214/20-AOAS1341},
  journal      = {The Annals of Applied Statistics},
  number       = {3},
  pages        = {1242-1257},
  shortjournal = {Ann. Appl. Stat.},
  title        = {A semiparametric mixture method for local false discovery rate estimation from multiple studies},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Size estimation of key populations in the HIV epidemic in
eSwatini using incomplete and misaligned capture-recapture data.
<em>AOAS</em>, <em>14</em>(3), 1207–1241. (<a
href="https://doi.org/10.1214/20-AOAS1327">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In 2020, our understanding of the distributions of HIV risks in the most burdened settings, including eSwatini, remains limited. In part, this is driven by the limited availability of the size and burden of the populations at the greatest risk for HIV. Given pervasive social and healthcare stigmas, the size estimations of these populations often rely on the multiplier method—a variant of the capture-recapture approach where the first survey is replaced by an enumeration of population members who used some service or attended an event. To characterize the distributions of marginalized communities in eSwatini, multiple data sources are available at each region for the multiplier method. Current practices in such circumstances produce multiple population size estimates at each region ignoring the correlation among these estimates. We recast the multiple multiplier method as a special case of capture-recapture problem with incomplete data and propose a fully model based approach for size estimation using multiple capture-recapture data with arbitrary pattern of incompleteness. We use a data augmentation scheme that allows us to model the correlations in the data and produce a unified estimate of population size per region. A hierarchical model ties together the models for multiple regions, allowing us to borrow strength across the regions and enabling extrapolation to areas without data. In eSwatini we also encounter data misalignment where counts from some of the data sources are not available for each region but as an aggregate over few regions. We propose a solution to the general misalignment problem which considers data-source-specific patterns of misalignment. We use simulation studies to demonstrate the accurate inferential capabilities of our Bayesian multiplier method. This approach is then used to produce uncertainty-quantified population size estimates of key populations in eSwatini. Lastly, we propose a Bayesian nonparametric extension for incomplete capture-recapture that allows nonindependent data sources.},
  archive      = {J_AOAS},
  author       = {Abhirup Datta and Andrew Pita and Amrita Rao and Bhekie Sithole and Zandile Mnisi and Stefan Baral},
  doi          = {10.1214/20-AOAS1327},
  journal      = {The Annals of Applied Statistics},
  number       = {3},
  pages        = {1207-1241},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Size estimation of key populations in the HIV epidemic in eSwatini using incomplete and misaligned capture-recapture data},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Active matrix factorization for surveys. <em>AOAS</em>,
<em>14</em>(3), 1182–1206. (<a
href="https://doi.org/10.1214/20-AOAS1322">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Amid historically low response rates, survey researchers seek ways to reduce respondent burden while measuring desired concepts with precision. We propose to ask fewer questions of respondents and impute missing responses via probabilistic matrix factorization. A variance-minimizing active learning criterion chooses the most informative questions per respondent. In simulations of our matrix sampling procedure on real-world surveys as well as a Facebook survey experiment, we find active question selection achieves efficiency gains over baselines. The reduction in imputation error is heterogeneous across questions and depends on the latent concepts they capture. Modeling responses with the ordered logit likelihood improves imputations and yields an adaptive question order. We find for the Facebook survey that potential biases from order effects are likely to be small. With our method, survey researchers obtain principled suggestions of questions to retain and, if desired, can automate the design of shorter instruments.},
  archive      = {J_AOAS},
  author       = {Chelsea Zhang and Sean J. Taylor and Curtiss Cobb and Jasjeet Sekhon},
  doi          = {10.1214/20-AOAS1322},
  journal      = {The Annals of Applied Statistics},
  number       = {3},
  pages        = {1182-1206},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Active matrix factorization for surveys},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Optimal EMG placement for a robotic prosthesis controller
with sequential, adaptive functional estimation (SAFE). <em>AOAS</em>,
<em>14</em>(3), 1164–1181. (<a
href="https://doi.org/10.1214/20-AOAS1324">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Robotic hand prostheses require a controller to decode muscle contraction information, such as electromyogram (EMG) signals, into the user’s desired hand movement. State-of-the-art decoders demand extensive training, require data from a large number of EMG sensors and are prone to poor predictions. Biomechanical models of a single movement degree-of-freedom tell us that relatively few muscles, and, hence, fewer EMG sensors are needed to predict movement. We propose a novel decoder based on a dynamic, functional linear model with velocity or acceleration as its response and the recent past EMG signals as functional covariates. The effect of each EMG signal varies with the recent position to account for biomechanical features of hand movement, increasing the predictive capability of a single EMG signal compared to existing decoders. The effects are estimated with a multistage, adaptive estimation procedure that we call Sequential Adaptive Functional Estimation (SAFE). Starting with 16 potential EMG sensors, our method correctly identifies the few EMG signals that are known to be important for an able-bodied subject. Furthermore, the estimated effects are interpretable and can significantly improve understanding and development of robotic hand prostheses.},
  archive      = {J_AOAS},
  author       = {Jonathan Stallrich and Md Nazmul Islam and Ana-Maria Staicu and Dustin Crouch and Lizhi Pan and He Huang},
  doi          = {10.1214/20-AOAS1324},
  journal      = {The Annals of Applied Statistics},
  number       = {3},
  pages        = {1164-1181},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Optimal EMG placement for a robotic prosthesis controller with sequential, adaptive functional estimation (SAFE)},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Statistical methods for analysis of combined categorical
biomarker data from multiple studies. <em>AOAS</em>, <em>14</em>(3),
1146–1163. (<a href="https://doi.org/10.1214/20-AOAS1337">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the analysis of pooled data from multiple studies involving a biomarker exposure, the biomarker measurements can vary across laboratories and usually require calibration to a reference assay prior to pooling. Previous researches consider the measurements from a reference laboratory as the gold standard, even though measurements in the reference laboratory are not necessarily closer to the underlying truth in reality. In this paper we do not treat any laboratory measurements as the gold standard, and we develop two statistical methods, the exact calibration and cut-off calibration methods, for the analysis of aggregated categorical biomarker data. We compare the performance of both methods for estimating the biomarker-disease relationship under a random sample or controls-only calibration design. Our findings include: (1) the exact calibration method provides significantly less biased estimates and more accurate confidence intervals than the other method; (2) the cut-off calibration method could yield estimates with minimal bias and valid confidence intervals under small measurement errors and/or small exposure effects; (3) controls-only calibration design can result in additional bias, but the bias is minimal if the exposure effects and/or disease prevalences are small. Finally, we illustrate the methods in an application evaluating the relationship between circulating vitamin D levels and colorectal cancer risk in a pooling project.},
  archive      = {J_AOAS},
  author       = {Chao Cheng and Molin Wang},
  doi          = {10.1214/20-AOAS1337},
  journal      = {The Annals of Applied Statistics},
  number       = {3},
  pages        = {1146-1163},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Statistical methods for analysis of combined categorical biomarker data from multiple studies},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Markov decision processes with dynamic transition
probabilities: An analysis of shooting strategies in basketball.
<em>AOAS</em>, <em>14</em>(3), 1122–1145. (<a
href="https://doi.org/10.1214/20-AOAS1348">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper we model basketball plays as episodes from team-specific nonstationary Markov decision processes (MDPs) with shot clock dependent transition probabilities. Bayesian hierarchical models are employed in the modeling and parametrization of the transition probabilities to borrow strength across players and through time. To enable computational feasibility, we combine lineup-specific MDPs into team-average MDPs using a novel transition weighting scheme. Specifically, we derive the dynamics of the team-average process such that the expected transition count for an arbitrary state-pair is equal to the weighted sum of the expected counts of the separate lineup-specific MDPs. We then utilize these nonstationary MDPs in the creation of a basketball play simulator with uncertainty propagated via posterior samples of the model components. After calibration, we simulate seasons both on-policy and under altered policies and explore the net changes in efficiency and production under the alternate policies. Additionally, we discuss the game-theoretic ramifications of testing alternative decision policies.},
  archive      = {J_AOAS},
  author       = {Nathan Sandholtz and Luke Bornn},
  doi          = {10.1214/20-AOAS1348},
  journal      = {The Annals of Applied Statistics},
  number       = {3},
  pages        = {1122-1145},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Markov decision processes with dynamic transition probabilities: An analysis of shooting strategies in basketball},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Efficiency in lung transplant allocation strategies.
<em>AOAS</em>, <em>14</em>(3), 1088–1121. (<a
href="https://doi.org/10.1214/20-AOAS1350">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Currently in the United States, lung transplantations are allocated to candidates according to each candidate’s lung allocation score (LAS). The LAS is an ad hoc ranking system for patients’ priorities of transplantation. The goal of this study is to develop a framework for improving patients’ life expectancies over the LAS based on a comprehensive modeling of the lung transplantation waiting list. Patients and organs are modeled as arriving according to Poisson processes, patients’ health status evolving a waiting time inhomogeneous Markov process until death or transplantation, with organ recipient’s expected post-transplant residual life depending on waiting time and health status at transplantation. Under allocation rules satisfying minimal fairness requirements, the long-term average expected life converges, and its limit is a natural standard for comparing allocation strategies. Via the Hamilton–Jacobi–Bellman equations, upper bounds for the limiting average expected life are derived as a function of organ availability. Corresponding to each upper bound is an allocable set of (state, time) pairs at which patients would be optimally transplanted. The allocable set expands monotonically as organ availability increases which motivates the development of an allocation strategy that leads to long-term expected life close to the upper bound. Simulation studies are conducted with model parameters estimated from national lung transplantation data. Results suggest that, compared to the LAS, the proposed allocation strategy could provide a 7.7\% increase in average total life. We further extend the results to the allocation and matching of multiple organ types.},
  archive      = {J_AOAS},
  author       = {Jingjing Zou and David J. Lederer and Daniel Rabinowitz},
  doi          = {10.1214/20-AOAS1350},
  journal      = {The Annals of Applied Statistics},
  number       = {3},
  pages        = {1088-1121},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Efficiency in lung transplant allocation strategies},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Statistical methods for replicability assessment.
<em>AOAS</em>, <em>14</em>(3), 1063–1087. (<a
href="https://doi.org/10.1214/20-AOAS1336">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Large-scale replication studies like the Reproducibility Project: Psychology (RP:P) provide invaluable systematic data on scientific replicability, but most analyses and interpretations of the data fail to agree on the definition of “replicability” and disentangle the inexorable consequences of known selection bias from competing explanations. We discuss three concrete definitions of replicability based on: (1) whether published findings about the signs of effects are mostly correct, (2) how effective replication studies are in reproducing whatever true effect size was present in the original experiment and (3) whether true effect sizes tend to diminish in replication. We apply techniques from multiple testing and postselection inference to develop new methods that answer these questions while explicitly accounting for selection bias. Our analyses suggest that the RP:P dataset is largely consistent with publication bias due to selection of significant effects. The methods in this paper make no distributional assumptions about the true effect sizes.},
  archive      = {J_AOAS},
  author       = {Kenneth Hung and William Fithian},
  doi          = {10.1214/20-AOAS1336},
  journal      = {The Annals of Applied Statistics},
  number       = {3},
  pages        = {1063-1087},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Statistical methods for replicability assessment},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Accounting for dependent errors in predictors and
time-to-event outcomes using electronic health records, validation
samples and multiple imputation. <em>AOAS</em>, <em>14</em>(2),
1045–1061. (<a href="https://doi.org/10.1214/20-AOAS1343">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data from electronic health records (EHR) are prone to errors which are often correlated across multiple variables. The error structure is further complicated when analysis variables are derived as functions of two or more error-prone variables. Such errors can substantially impact estimates, yet we are unaware of methods that simultaneously account for errors in covariates and time-to-event outcomes. Using EHR data from 4217 patients, the hazard ratio for an AIDS-defining event associated with a 100 cell/mm$^{3}$ increase in CD4 count at ART initiation was 0.74 (95$\%$CI: 0.68–0.80) using unvalidated data and 0.60 (95$\%$CI: 0.53–0.68) using fully validated data. Our goal is to obtain unbiased and efficient estimates after validating a random subset of records. We propose fitting discrete failure time models to the validated subsample and then multiply imputing values for unvalidated records. We demonstrate how this approach simultaneously addresses dependent errors in predictors, time-to-event outcomes, and inclusion criteria. Using the fully validated dataset as a gold standard, we compare the mean squared error of our estimates with those from the unvalidated dataset and the corresponding subsample-only dataset for various subsample sizes. By incorporating reasonably sized validated subsamples and appropriate imputation models, our approach had improved estimation over both the naive analysis and the analysis using only the validation subsample.},
  archive      = {J_AOAS},
  author       = {Mark J. Giganti and Pamela A. Shaw and Guanhua Chen and Sally S. Bebawy and Megan M. Turner and Timothy R. Sterling and Bryan E. Shepherd},
  doi          = {10.1214/20-AOAS1343},
  journal      = {The Annals of Applied Statistics},
  number       = {2},
  pages        = {1045-1061},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Accounting for dependent errors in predictors and time-to-event outcomes using electronic health records, validation samples and multiple imputation},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Analyses of preventive care measures with incomplete
historical data in electronic medical records: An example from
colorectal cancer screening. <em>AOAS</em>, <em>14</em>(2), 1030–1044.
(<a href="https://doi.org/10.1214/20-AOAS1342">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The calculation of quality of care measures based on electronic medical records (EMRs) may be inaccurate because of incomplete capture of past services. We evaluate the influence of different statistical approaches for calculating the proportion of patients who are up-to-date for a preventive service, using the example of colorectal cancer (CRC) screening. We propose an extension of traditional mixture models to account for the uncertainty in compliance which is further complicated by the choice of various screening modalities with different recommended screening intervals. We conducted simulation studies to compare various statistical approaches and demonstrated that the proposed method can alleviate bias when individuals with complete prior medical history information were not representative of the targeted population. The method is motivated by and applied to data from the National Cancer Institute–funded consortium Population-Based Research Optimizing Screening through Personalized Regiments (PROSPR). Findings from the application are important for the evaluation of appropriate use of preventive care and provide a novel tool for dealing with similar analytical challenges with EMR data in broad settings.},
  archive      = {J_AOAS},
  author       = {Yingye Zheng and Douglas A. Corley and Chyke Doubeni and Ethan Halm and Susan M. Shortreed and William E. Barlow and Ann Zauber and Tor Devin Tosteson and Jessica Chubak},
  doi          = {10.1214/20-AOAS1342},
  journal      = {The Annals of Applied Statistics},
  number       = {2},
  pages        = {1030-1044},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Analyses of preventive care measures with incomplete historical data in electronic medical records: An example from colorectal cancer screening},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A random effects stochastic block model for joint community
detection in multiple networks with applications to neuroimaging.
<em>AOAS</em>, <em>14</em>(2), 993–1029. (<a
href="https://doi.org/10.1214/20-AOAS1339">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To analyze data from multisubject experiments in neuroimaging studies, we develop a modeling framework for joint community detection in a group of related networks that can be considered as a sample from a population of networks. The proposed random effects stochastic block model facilitates the study of group differences and subject-specific variations in the community structure. The model proposes a putative mean community structure, which is representative of the group or the population under consideration but is not the community structure of any individual component network. Instead, the community memberships of nodes vary in each component network with a transition matrix, thus modeling the variation in community structure across a group of subjects. To estimate the quantities of interest, we propose two methods: a variational EM algorithm and a model-free “two-step” method called Co-OSNTF which is based on nonnegative matrix factorization. We also develop a resampling-based hypothesis test for differences between community structure in two populations both at the whole network level and node level. The methodology is applied to the COBRE dataset, a publicly available fMRI dataset from multisubject experiments involving schizophrenia patients. Our methods reveal an overall putative community structure representative of the group as well as subject-specific variations within each of the two groups, healthy controls and schizophrenia patients. The model has good predictive ability for predicting community structure in subjects from the same population but outside the training sample. Using our network level hypothesis tests, we are able to ascertain statistically significant difference in community structure between the two groups, while our node level tests help determine the nodes that are driving the difference.},
  archive      = {J_AOAS},
  author       = {Subhadeep Paul and Yuguo Chen},
  doi          = {10.1214/20-AOAS1339},
  journal      = {The Annals of Applied Statistics},
  number       = {2},
  pages        = {993-1029},
  shortjournal = {Ann. Appl. Stat.},
  title        = {A random effects stochastic block model for joint community detection in multiple networks with applications to neuroimaging},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Early identification of an impending rockslide location via
a spatially-aided gaussian mixture model. <em>AOAS</em>, <em>14</em>(2),
977–992. (<a href="https://doi.org/10.1214/20-AOAS1326">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Movement of soil and rocks in an unstable slope under gravitational forces is an example of a complex system that is highly dynamic in space and time. A typical failure in such systems is a landslide. Fundamental studies of granular media failure combined with a complex network analysis of radar monitoring data show that distinct partitions emerge in the kinematic field in the early stages of the prefailure regime, and these patterns yield clues to the ultimate location of failure. In this study we address this partitioning of constituent units in complex systems by clustering the kinematic data, specifically, with a Gaussian mixture model. In addition, we assume that neighboring units should move together. As a result, spatial information is taken into account in our model so that spatial proximity is retained. Our case study of a rockslide from high resolution radar monitoring data shows that, by incorporating spatial information, our approach is more effective in revealing the dynamics of the system and detecting the location of a potential landslide, compared to the use of only the kinematics.},
  archive      = {J_AOAS},
  author       = {Shuo Zhou and Howard Bondell and Antoinette Tordesillas and Benjamin I. P. Rubinstein and James Bailey},
  doi          = {10.1214/20-AOAS1326},
  journal      = {The Annals of Applied Statistics},
  number       = {2},
  pages        = {977-992},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Early identification of an impending rockslide location via a spatially-aided gaussian mixture model},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Generalized accelerated recurrence time model in the
presence of a dependent terminal event. <em>AOAS</em>, <em>14</em>(2),
956–976. (<a href="https://doi.org/10.1214/20-AOAS1335">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recurrent events are commonly encountered in longitudinal studies. The observation of recurrent events is often stopped by a dependent terminal event in practice. For this data scenario, we propose two sensible adaptations of the generalized accelerated recurrence time (GART) model (J. Amer. Statist. Assoc. 111 (2016) 145–156) to provide useful alternative analyses that can offer physical interpretations while rendering extra flexibility beyond the existing work based on the accelerated failure time model. Our modeling strategies align with the rationale underlying the use of the survivors’ rate function or the adjusted rate function to account for the presence of the dependent terminal event. For the proposed models, we identify and develop estimation and inference procedures which can be readily implemented based on existing software. We establish the asymptotic properties of the new estimator. Simulation studies demonstrate good finite-sample performance of the proposed methods. An application to a dataset from the Cystic Fibrosis Foundation Patient Registry (CFFPR) illustrates the practical utility of the new methods.},
  archive      = {J_AOAS},
  author       = {Bo Wei and Zhumin Zhang and HuiChuan J. Lai and Limin Peng},
  doi          = {10.1214/20-AOAS1335},
  journal      = {The Annals of Applied Statistics},
  number       = {2},
  pages        = {956-976},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Generalized accelerated recurrence time model in the presence of a dependent terminal event},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Seasonal warranty prediction based on recurrent event data.
<em>AOAS</em>, <em>14</em>(2), 929–955. (<a
href="https://doi.org/10.1214/20-AOAS1333">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Warranty return data from repairable systems, such as home appliances, lawn mowers, computers and automobiles, result in recurrent event data. The nonhomogeneous Poisson process (NHPP) model is used widely to describe such data. Seasonality in the repair frequencies and other variabilities, however, complicate the modeling of recurrent event data. Not much work has been done to address the seasonality, and this paper provides a general approach for the application of NHPP models with dynamic covariates to predict seasonal warranty returns. The methods presented here, however, can be applied to other applications that result in seasonal recurrent event data. A hierarchical clustering method is used to stratify the population into groups that are more homogeneous than the overall population. The stratification facilitates modeling the recurrent event data with both time-varying and time-constant covariates. We demonstrate and validate the models using warranty claims data for two different types of products. The results show that our approach provides important improvements in the predictive power of monthly events compared with models that do not take the seasonality and covariates into account.},
  archive      = {J_AOAS},
  author       = {Qianqian Shan and Yili Hong and William Q. Meeker},
  doi          = {10.1214/20-AOAS1333},
  journal      = {The Annals of Applied Statistics},
  number       = {2},
  pages        = {929-955},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Seasonal warranty prediction based on recurrent event data},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A global-local approach for detecting hotspots in
multiple-response regression. <em>AOAS</em>, <em>14</em>(2), 905–928.
(<a href="https://doi.org/10.1214/20-AOAS1332">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We tackle modelling and inference for variable selection in regression problems with many predictors and many responses. We focus on detecting hotspots, that is, predictors associated with several responses. Such a task is critical in statistical genetics, as hotspot genetic variants shape the architecture of the genome by controlling the expression of many genes and may initiate decisive functional mechanisms underlying disease endpoints. Existing hierarchical regression approaches designed to model hotspots suffer from two limitations: their discrimination of hotspots is sensitive to the choice of top-level scale parameters for the propensity of predictors to be hotspots, and they do not scale to large predictor and response vectors, for example, of dimensions $10^{3}$–$10^{5}$ in genetic applications. We address these shortcomings by introducing a flexible hierarchical regression framework that is tailored to the detection of hotspots and scalable to the above dimensions. Our proposal implements a fully Bayesian model for hotspots based on the horseshoe shrinkage prior. Its global-local formulation shrinks noise globally and, hence, accommodates the highly sparse nature of genetic analyses while being robust to individual signals, thus leaving the effects of hotspots unshrunk. Inference is carried out using a fast variational algorithm coupled with a novel simulated annealing procedure that allows efficient exploration of multimodal distributions.},
  archive      = {J_AOAS},
  author       = {Hélène Ruffieux and Anthony C. Davison and Jörg Hager and Jamie Inshaw and Benjamin P. Fairfax and Sylvia Richardson and Leonardo Bottolo},
  doi          = {10.1214/20-AOAS1332},
  journal      = {The Annals of Applied Statistics},
  number       = {2},
  pages        = {905-928},
  shortjournal = {Ann. Appl. Stat.},
  title        = {A global-local approach for detecting hotspots in multiple-response regression},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Focused model selection for linear mixed models with an
application to whale ecology. <em>AOAS</em>, <em>14</em>(2), 872–904.
(<a href="https://doi.org/10.1214/20-AOAS1331">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A central point of disagreement, in certain long-standing discussions about a particular whaling dataset in the Scientific Committee of the International Whaling Commission, has directly involved model selection issues for linear mixed effect models. The biological question under discussion is associated with a clearly defined parameter of primary interest, a focus parameter, which makes model selection with the Focused Information Criterion (FIC) more appropriate than other selection methods. Since the existing FIC methodology has not covered the case of linear mixed effects models, this article sets up the required framework and develops the necessary formulae for the relevant FIC. Our new criterion requires the asymptotic distribution of estimators derived for a given candidate linear mixed model but with behaviour examined under a wider linear mixed model. These results, needed here to build our FIC, also have independent interest.},
  archive      = {J_AOAS},
  author       = {Céline Cunen and Lars Walløe and Nils Lid Hjort},
  doi          = {10.1214/20-AOAS1331},
  journal      = {The Annals of Applied Statistics},
  number       = {2},
  pages        = {872-904},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Focused model selection for linear mixed models with an application to whale ecology},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A causal exposure response function with local adjustment
for confounding: Estimating health effects of exposure to low levels of
ambient fine particulate matter. <em>AOAS</em>, <em>14</em>(2), 850–871.
(<a href="https://doi.org/10.1214/20-AOAS1330">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the last two decades ambient levels of air pollution have declined substantially. At the same time the Clean Air Act mandates that the National Ambient Air Quality Standards (NAAQS) must be routinely assessed to protect populations based on the latest science. Therefore, researchers should continue to address the following question: is exposure to levels of air pollution below the NAAQS harmful to human health? Furthermore, the contentious nature surrounding environmental regulations urges us to cast this question within a causal inference framework. Several parametric and semiparametric regression approaches have been used to estimate the exposure-response (ER) curve between long-term exposure to ambient air pollution concentrations and health outcomes. However, most of the existing approaches are not formulated within a formal framework for causal inference, adjust for the same set of potential confounders across all levels of exposure and do not account for model uncertainty regarding covariate selection and the shape of the ER. In this paper we introduce a Bayesian framework for the estimation of a causal ER curve called LERCA (Local Exposure Response Confounding Adjustment), which: (a) allows for different confounders and different strength of confounding at the different exposure levels, and (b) propagates model uncertainty regarding confounders’ selection and the shape of the ER. Importantly, LERCA provides a principled way of assessing the observed covariates’ confounding importance at different exposure levels, providing researchers with important information regarding the set of variables to measure and adjust for in regression models. Using simulation studies, we show that state-of-the-art approaches perform poorly in estimating the ER curve in the presence of local confounding. LERCA is used to evaluate the relationship between long-term exposure to ambient PM$_{2.5}$, a key regulated pollutant, and cardiovascular hospitalizations for 5,362 zip codes in the continental U.S. and located near a pollution monitoring site, while adjusting for a potentially varying set of confounders across the exposure range. Our data set includes rich health, weather, demographic and pollution information for the years of 2011–2013. The estimated exposure-response curve is increasingly indicating that higher ambient concentrations lead to higher cardiovascular hospitalization rates, and ambient PM$_{2.5}$ was estimated to lead to an increase in cardiovascular hospitalization rates when focusing at the low-exposure range. Our results indicate that there is no threshold for the effect of PM$_{2.5}$ on cardiovascular hospitalizations.},
  archive      = {J_AOAS},
  author       = {Georgia Papadogeorgou and Francesca Dominici},
  doi          = {10.1214/20-AOAS1330},
  journal      = {The Annals of Applied Statistics},
  number       = {2},
  pages        = {850-871},
  shortjournal = {Ann. Appl. Stat.},
  title        = {A causal exposure response function with local adjustment for confounding: Estimating health effects of exposure to low levels of ambient fine particulate matter},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Evidence factors in a case-control study with application to
the effect of flexible sigmoidoscopy screening on colorectal cancer.
<em>AOAS</em>, <em>14</em>(2), 829–849. (<a
href="https://doi.org/10.1214/20-AOAS1329">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As in any observational study, in a case-control study a primary concern is potential unmeasured confounders. Bias, due to unmeasured confounders, can result in a false discovery of an apparent treatment effect when there is none. Replication of an observational study, which tries to provide multiple analyses of the data where the biases affecting each analysis are thought to be different, is one way to strengthen the evidence from an observational study. Evidence factors allow for internal replication by testing a hypothesis using multiple comparisons in a way that the comparisons yield independent evidence and differ in the sources of potential bias. We construct evidence factors in a case-control study in which there are two types of cases, “narrow” cases which are thought to be potentially more affected by the exposure and “marginal” cases which are thought to have more heterogeneous causes. We develop and study an inference procedure for using such evidence factors and apply it to a study of the effect of sigmoidoscopy screening on colorectal cancer.},
  archive      = {J_AOAS},
  author       = {Bikram Karmakar and Chyke A. Doubeni and Dylan S. Small},
  doi          = {10.1214/20-AOAS1329},
  journal      = {The Annals of Applied Statistics},
  number       = {2},
  pages        = {829-849},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Evidence factors in a case-control study with application to the effect of flexible sigmoidoscopy screening on colorectal cancer},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Bayesian variable selection for survival data using inverse
moment priors. <em>AOAS</em>, <em>14</em>(2), 809–828. (<a
href="https://doi.org/10.1214/20-AOAS1325">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Efficient variable selection in high-dimensional cancer genomic studies is critical for discovering genes associated with specific cancer types and for predicting response to treatment. Censored survival data is prevalent in such studies. In this article we introduce a Bayesian variable selection procedure that uses a mixture prior composed of a point mass at zero and an inverse moment prior in conjunction with the partial likelihood defined by the Cox proportional hazard model. The procedure is implemented in the R package BVSNLP, which supports parallel computing and uses a stochastic search method to explore the model space. Bayesian model averaging is used for prediction. The proposed algorithm provides better performance than other variable selection procedures in simulation studies and appears to provide more consistent variable selection when applied to actual genomic datasets.},
  archive      = {J_AOAS},
  author       = {Amir Nikooienejad and Wenyi Wang and Valen E. Johnson},
  doi          = {10.1214/20-AOAS1325},
  journal      = {The Annals of Applied Statistics},
  number       = {2},
  pages        = {809-828},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Bayesian variable selection for survival data using inverse moment priors},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Estimation and inference in metabolomics with nonrandom
missing data and latent factors. <em>AOAS</em>, <em>14</em>(2), 789–808.
(<a href="https://doi.org/10.1214/20-AOAS1328">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {High-throughput metabolomics data are fraught with both nonignorable missing observations and unobserved factors that influence a metabolite’s measured concentration, and it is well known that ignoring either of these complications can compromise estimators. However, current methods to analyze these data can only account for the missing data or unobserved factors, but not both. We therefore developed MetabMiss, a statistically rigorous method to account for both nonrandom missing data and latent factors in high-throughput metabolomics data. Our methodology does not require the practitioner specify a likelihood for the missing data, and makes investigating the relationship between the metabolome and tens, or even hundreds, of phenotypes computationally tractable. We demonstrate the fidelity of MetabMiss’s estimates using both simulated and real metabolomics data and prove their asymptotic correctness when the sample size and number of metabolites grows to infinity.},
  archive      = {J_AOAS},
  author       = {Chris McKennan and Carole Ober and Dan Nicolae},
  doi          = {10.1214/20-AOAS1328},
  journal      = {The Annals of Applied Statistics},
  number       = {2},
  pages        = {789-808},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Estimation and inference in metabolomics with nonrandom missing data and latent factors},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Semiparametric bayesian markov analysis of personalized
benefit–risk assessment. <em>AOAS</em>, <em>14</em>(2), 768–788. (<a
href="https://doi.org/10.1214/20-AOAS1323">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The development of systematic and structured approaches to assess benefit–risk of medical products is a major challenge for regulatory decision makers. Existing benefit–risk methods depend only on the frequencies of mutually exclusive and exhaustive categories in which the subjects fall, and the responses of individuals are allowed to belong to any of the other categories during their postwithdrawal visits. In this article we introduce a semiparametric Bayesian Markov model (SBMM) that treats the withdrawal category as an absorbing state and analyzes subject-level data for multiple visits, accounting for any within-patient dependencies in the response profiles. A log-odds ratio model is used to model the subject-level effects by assuming a ratio of transition probabilities with respect to a “reference” category. A Dirichlet process is used as a semiparametric model for the subject-level effects to flexibly capture the underlying distributions of the personalized response profiles without making strong parametric assumptions. This also allows the borrowing of strength between the patients and achieves dimension reduction by allocating similar response profiles patterns into an unknown number of latent clusters. We analyze a motivating clinical trial dataset to assess the personalized benefit–risks in each arm and evaluate the aggregated benefits and risks associated with the drug Exalgo.},
  archive      = {J_AOAS},
  author       = {Dongyan Yan and Subharup Guha and Chul Ahn and Ram Tiwari},
  doi          = {10.1214/20-AOAS1323},
  journal      = {The Annals of Applied Statistics},
  number       = {2},
  pages        = {768-788},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Semiparametric bayesian markov analysis of personalized benefit–risk assessment},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Multiview cluster aggregation and splitting, with an
application to multiomic breast cancer data. <em>AOAS</em>,
<em>14</em>(2), 752–767. (<a
href="https://doi.org/10.1214/19-AOAS1317">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multiview data, which represent distinct but related groupings of variables, can be useful for identifying relevant and robust clustering structures among observations. A large number of multiview classification algorithms have been proposed in the fields of computer science and genomics; here, we instead focus on the task of merging or splitting an existing hard or soft cluster partition based on multiview data. This article is specifically motivated by an application involving multiomic breast cancer data from The Cancer Genome Atlas, where multiple molecular profiles (gene expression, microRNA expression, methylation and copy number alterations) are used to further subdivide the five currently accepted intrinsic tumor subtypes into distinct subgroups of patients. In addition, we investigate the performance of the proposed multiview splitting and aggregation algorithms, as compared to single- and concatenated-view alternatives, in a set of simulations. The multiview splitting and aggregation algorithms developed here are implemented in the maskmeans R package.},
  archive      = {J_AOAS},
  author       = {Antoine Godichon-Baggioni and Cathy Maugis-Rabusseau and Andrea Rau},
  doi          = {10.1214/19-AOAS1317},
  journal      = {The Annals of Applied Statistics},
  number       = {2},
  pages        = {752-767},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Multiview cluster aggregation and splitting, with an application to multiomic breast cancer data},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Sequential importance sampling for multiresolution
kingman–tajima coalescent counting. <em>AOAS</em>, <em>14</em>(2),
727–751. (<a href="https://doi.org/10.1214/19-AOAS1313">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Statistical inference of evolutionary parameters from molecular sequence data relies on coalescent models to account for the shared genealogical ancestry of the samples. However, inferential algorithms do not scale to available data sets. A strategy to improve computational efficiency is to rely on simpler coalescent and mutation models, resulting in smaller hidden state spaces. An estimate of the cardinality of the state space of genealogical trees at different resolutions is essential to decide the best modeling strategy for a given dataset. To our knowledge, there is neither an exact nor approximate method to determine these cardinalities. We propose a sequential importance sampling algorithm to estimate the cardinality of the sample space of genealogical trees under different coalescent resolutions. Our sampling scheme proceeds sequentially across the set of combinatorial constraints imposed by the data which, in this work, are completely linked sequences of DNA at a nonrecombining segment. We analyze the cardinality of different genealogical tree spaces on simulations to study the settings that favor coarser resolutions. We apply our method to estimate the cardinality of genealogical tree spaces from mtDNA data from the 1000 genomes and a sample from a Melanesian population at the $\beta $-globin locus.},
  archive      = {J_AOAS},
  author       = {Lorenzo Cappello and Julia A. Palacios},
  doi          = {10.1214/19-AOAS1313},
  journal      = {The Annals of Applied Statistics},
  number       = {2},
  pages        = {727-751},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Sequential importance sampling for multiresolution Kingman–Tajima coalescent counting},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Estimation of dyadic characteristics of family networks
using sample survey data. <em>AOAS</em>, <em>14</em>(2), 706–726. (<a
href="https://doi.org/10.1214/19-AOAS1308">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider the use of sample survey data to estimate dyadic characteristics of family networks, with an application to noncoresident parent–child dyads. We suppose that survey respondents report either from a parent or child perspective about a dyad, depending on their membership of the dyad. We construct separate estimators of common dyadic characteristics using data from both a parent and a child perspective and show how comparisons of these estimators can shed light on data quality issues including differential missingness and reporting error. In our application we find that a simple missingness model explains some striking patterns of discrepancies between the estimators and consider the use of poststratification and a related redefinition of count variables to adjust for these discrepancies. We also develop approaches to combining the separate estimators efficiently to estimate means and frequency distributions within subpopulations.},
  archive      = {J_AOAS},
  author       = {Chris Skinner and Fiona Steele},
  doi          = {10.1214/19-AOAS1308},
  journal      = {The Annals of Applied Statistics},
  number       = {2},
  pages        = {706-726},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Estimation of dyadic characteristics of family networks using sample survey data},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Accounting for uncertainty about past values in
probabilistic projections of the total fertility rate for most
countries. <em>AOAS</em>, <em>14</em>(2), 685–705. (<a
href="https://doi.org/10.1214/19-AOAS1294">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Since the 1940s, population projections have in most cases been produced using the deterministic cohort component method. However, in 2015, for the first time and in a major advance, the United Nations issued official probabilistic population projections for all countries based on Bayesian hierarchical models for total fertility and life expectancy. The estimates of these models and the resulting projections are conditional on the U.N.’s official estimates of past values. However, these past values are themselves uncertain, particularly for the majority of the world’s countries that do not have longstanding high-quality vital registration systems, when they rely on surveys and censuses with their own biases and measurement errors. This paper extends the U.N. model for projecting future total fertility rates to take account of uncertainty about past values. This is done by adding an additional level to the hierarchical model to represent the multiple data sources, in each case estimating their bias and measurement error variance. We assess the method by out-of-sample predictive validation. While the prediction intervals produced by the extant method (which does not account for this source of uncertainty) have somewhat less than nominal coverage, we find that our proposed method achieves closer to nominal coverage. The prediction intervals become wider for countries for which the estimates of past total fertility rates rely heavily on surveys rather than on vital registration data, especially in high fertility countries.},
  archive      = {J_AOAS},
  author       = {Peiran Liu and Adrian E. Raftery},
  doi          = {10.1214/19-AOAS1294},
  journal      = {The Annals of Applied Statistics},
  number       = {2},
  pages        = {685-705},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Accounting for uncertainty about past values in probabilistic projections of the total fertility rate for most countries},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). The stratified micro-randomized trial design: Sample size
considerations for testing nested causal effects of time-varying
treatments. <em>AOAS</em>, <em>14</em>(2), 661–684. (<a
href="https://doi.org/10.1214/19-AOAS1293">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Technological advancements in the field of mobile devices and wearable sensors have helped overcome obstacles in the delivery of care, making it possible to deliver behavioral treatments anytime and anywhere. Here, we discuss our work on the design of a mobile health smoking cessation intervention study with the goal of assessing whether reminders, delivered at times of stress, result in a reduction/prevention of stress in the near-term and whether this effect changes with time in study. Multiple statistical challenges arose in this effort, leading to the development of the stratified micro-randomized trial design. In these designs each individual is randomized to treatment repeatedly at times determined by predictions of risk. These risk times may be impacted by prior treatment. We describe the statistical challenges and detail how they can be met.},
  archive      = {J_AOAS},
  author       = {Walter Dempsey and Peng Liao and Santosh Kumar and Susan A. Murphy},
  doi          = {10.1214/19-AOAS1293},
  journal      = {The Annals of Applied Statistics},
  number       = {2},
  pages        = {661-684},
  shortjournal = {Ann. Appl. Stat.},
  title        = {The stratified micro-randomized trial design: Sample size considerations for testing nested causal effects of time-varying treatments},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Torus graphs for multivariate phase coupling analysis.
<em>AOAS</em>, <em>14</em>(2), 635–660. (<a
href="https://doi.org/10.1214/19-AOAS1300">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Angular measurements are often modeled as circular random variables, where there are natural circular analogues of moments, including correlation. Because a product of circles is a torus, a $d$-dimensional vector of circular random variables lies on a $d$-dimensional torus. For such vectors we present here a class of graphical models, which we call torus graphs, based on the full exponential family with pairwise interactions. The topological distinction between a torus and Euclidean space has several important consequences. Our development was motivated by the problem of identifying phase coupling among oscillatory signals recorded from multiple electrodes in the brain: oscillatory phases across electrodes might tend to advance or recede together, indicating coordination across brain areas. The data analyzed here consisted of 24 phase angles measured repeatedly across 840 experimental trials (replications) during a memory task, where the electrodes were in 4 distinct brain regions, all known to be active while memories are being stored or retrieved. In realistic numerical simulations, we found that a standard pairwise assessment, known as phase locking value, is unable to describe multivariate phase interactions, but that torus graphs can accurately identify conditional associations. Torus graphs generalize several more restrictive approaches that have appeared in various scientific literatures, and produced intuitive results in the data we analyzed. Torus graphs thus unify multivariate analysis of circular data and present fertile territory for future research.},
  archive      = {J_AOAS},
  author       = {Natalie Klein and Josue Orellana and Scott L. Brincat and Earl K. Miller and Robert E. Kass},
  doi          = {10.1214/19-AOAS1300},
  journal      = {The Annals of Applied Statistics},
  number       = {2},
  pages        = {635-660},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Torus graphs for multivariate phase coupling analysis},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A fast particle-based approach for calibrating a 3-d model
of the antarctic ice sheet. <em>AOAS</em>, <em>14</em>(2), 605–634. (<a
href="https://doi.org/10.1214/19-AOAS1305">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider the scientifically challenging and policy-relevant task of understanding the past and projecting the future dynamics of the Antarctic ice sheet. The Antarctic ice sheet has shown a highly nonlinear threshold response to past climate forcings. Triggering such a threshold response through anthropogenic greenhouse gas emissions would drive drastic and potentially fast sea level rise with important implications for coastal flood risks. Previous studies have combined information from ice sheet models and observations to calibrate model parameters. These studies have broken important new ground but have either adopted simple ice sheet models or have limited the number of parameters to allow for the use of more complex models. These limitations are largely due to the computational challenges posed by calibration as models become more computationally intensive or when the number of parameters increases. Here, we propose a method to alleviate this problem: a fast sequential Monte Carlo method that takes advantage of the massive parallelization afforded by modern high-performance computing systems. We use simulated examples to demonstrate how our sample-based approach provides accurate approximations to the posterior distributions of the calibrated parameters. The drastic reduction in computational times enables us to provide new insights into important scientific questions, for example, the impact of Pliocene era data and prior parameter information on sea level projections. These studies would be computationally prohibitive with other computational approaches for calibration such as Markov chain Monte Carlo or emulation-based methods. We also find considerable differences in the distributions of sea level projections when we account for a larger number of uncertain parameters. For example, based on the same ice sheet model and data set, the 99th percentile of the Antarctic ice sheet contribution to sea level rise in 2300 increases from 6.5 m to 13.1 m when we increase the number of calibrated parameters from three to 11. With previous calibration methods, it would be challenging to go beyond five parameters. This work provides an important next step toward improving the uncertainty quantification of complex, computationally intensive and decision-relevant models.},
  archive      = {J_AOAS},
  author       = {Ben Seiyon Lee and Murali Haran and Robert W. Fuller and David Pollard and Klaus Keller},
  doi          = {10.1214/19-AOAS1305},
  journal      = {The Annals of Applied Statistics},
  number       = {2},
  pages        = {605-634},
  shortjournal = {Ann. Appl. Stat.},
  title        = {A fast particle-based approach for calibrating a 3-D model of the antarctic ice sheet},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Baseline drift estimation for air quality data using
quantile trend filtering. <em>AOAS</em>, <em>14</em>(2), 585–604. (<a
href="https://doi.org/10.1214/19-AOAS1318">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We address the problem of estimating smoothly varying baseline trends in time series data. This problem arises in a wide range of fields, including chemistry, macroeconomics and medicine; however, our study is motivated by the analysis of data from low cost air quality sensors. Our methods extend the quantile trend filtering framework to enable the estimation of multiple quantile trends simultaneously while ensuring that the quantiles do not cross. To handle the computational challenge posed by very long time series, we propose a parallelizable alternating direction method of multipliers (ADMM) algorithm. The ADMM algorthim enables the estimation of trends in a piecewise manner, both reducing the computation time and extending the limits of the method to larger data sizes. We also address smoothing parameter selection and propose a modified criterion based on the extended Bayesian information criterion. Through simulation studies and our motivating application to low cost air quality sensor data, we demonstrate that our model provides better quantile trend estimates than existing methods and improves signal classification of low-cost air quality sensor output.},
  archive      = {J_AOAS},
  author       = {Halley L. Brantley and Joseph Guinness and Eric C. Chi},
  doi          = {10.1214/19-AOAS1318},
  journal      = {The Annals of Applied Statistics},
  number       = {2},
  pages        = {585-604},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Baseline drift estimation for air quality data using quantile trend filtering},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). An efficient and computationally robust statistical method
for analyzing case-control mother–offspring pair genetic association
studies. <em>AOAS</em>, <em>14</em>(2), 560–584. (<a
href="https://doi.org/10.1214/19-AOAS1298">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Case-control mother–offspring pair design has been widely adopted for studying early-life and women’s pregnancy health. It allows assessment of pre- and perinatal environmental risk factors as well as both maternal and offspring genetic risk factors. Data arising from this design is routinely analyzed using standard prospective logistic regression. Such data has two unique features: the offspring genotypes are not correlated with maternal environmental risk factors given maternal genotypes, and offspring and maternal genotypes are related through mendelian transmission. In this work, built upon a novel regression model relating maternal genotypes to environmental risk factors, we proposed a novel retrospective likelihood method that effectively utilized the two data features to increase statistical efficiency for detecting maternal and offspring genetic effects. The inference procedure was based on a profile likelihood derived using the Lagrange multiplier method, but we replaced the multipliers with their large sample limits to enable highly efficient and computationally stable estimation. We showed that our proposed estimates of odds ratio association parameters are consistent and asymptotically normally distributed and demonstrated the finite sample performance through extensive simulation studies and application to genetic association studies of birth weight and gestational diabetes mellitus.},
  archive      = {J_AOAS},
  author       = {Hong Zhang and Bhramar Mukherjee and Victoria Arthur and Gang Hu and Hagit Hochner and Jinbo Chen},
  doi          = {10.1214/19-AOAS1298},
  journal      = {The Annals of Applied Statistics},
  number       = {2},
  pages        = {560-584},
  shortjournal = {Ann. Appl. Stat.},
  title        = {An efficient and computationally robust statistical method for analyzing case-control mother–offspring pair genetic association studies},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Compression of climate simulations with a nonstationary
global SpatioTemporal SPDE model. <em>AOAS</em>, <em>14</em>(2),
542–559. (<a href="https://doi.org/10.1214/20-AOAS1340">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Modern climate models pose an ever-increasing storage burden to computational facilities, and the upcoming generation of global simulations from the next Intergovernmental Panel on Climate Change will require a substantial share of the budget of research centers worldwide to be allocated just for this task. A statistical model can be used as a means to mitigate the storage burden by providing a stochastic approximation of the climate simulations. Indeed, if a suitably validated statistical model can be formulated to draw realizations whose spatiotemporal structure is similar to that of the original computer simulations, then the estimated parameters are effectively all the information that needs to be stored. In this work we propose a new statistical model defined via a stochastic partial differential equation (SPDE) on the sphere and in evolving time. The model is able to capture nonstationarities across latitudes, longitudes and land/ocean domains for more than 300 million data points while also overcoming the fundamental limitations of current global statistical models available for compression. Once the model is trained, surrogate runs can be instantaneously generated on a laptop by storing just 20 Megabytes of parameters as opposed to more than six Gigabytes of the original ensemble.},
  archive      = {J_AOAS},
  author       = {Geir-Arne Fuglstad and Stefano Castruccio},
  doi          = {10.1214/20-AOAS1340},
  journal      = {The Annals of Applied Statistics},
  number       = {2},
  pages        = {542-559},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Compression of climate simulations with a nonstationary global SpatioTemporal SPDE model},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Function-on-scalar quantile regression with application to
mass spectrometry proteomics data. <em>AOAS</em>, <em>14</em>(2),
521–541. (<a href="https://doi.org/10.1214/19-AOAS1319">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mass spectrometry proteomics, characterized by spiky, spatially heterogeneous functional data, can be used to identify potential cancer biomarkers. Existing mass spectrometry analyses utilize mean regression to detect spectral regions that are differentially expressed across groups. However, given the interpatient heterogeneity that is a key hallmark of cancer, many biomarkers are only present at aberrant levels for a subset of, not all, cancer samples. Differences in these biomarkers can easily be missed by mean regression but might be more easily detected by quantile-based approaches. Thus, we propose a unified Bayesian framework to perform quantile regression on functional responses. Our approach utilizes an asymmetric Laplace working likelihood, represents the functional coefficients with basis representations which enable borrowing of strength from nearby locations and places a global-local shrinkage prior on the basis coefficients to achieve adaptive regularization. Different types of basis transform and continuous shrinkage priors can be used in our framework. A scalable Gibbs sampler is developed to generate posterior samples that can be used to perform Bayesian estimation and inference while accounting for multiple testing. Our framework performs quantile regression and coefficient regularization in a unified manner, allowing them to inform each other and leading to improvement in performance over competing methods, as demonstrated by simulation studies. We also introduce an adjustment procedure to the model to improve its frequentist properties of posterior inference. We apply our model to identify proteomic biomarkers of pancreatic cancer that are differentially expressed for a subset of cancer patients compared to the normal controls which were missed by previous mean-regression based approaches. Supplementary Material for this article is available online.},
  archive      = {J_AOAS},
  author       = {Yusha Liu and Meng Li and Jeffrey S. Morris},
  doi          = {10.1214/19-AOAS1319},
  journal      = {The Annals of Applied Statistics},
  number       = {2},
  pages        = {521-541},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Function-on-scalar quantile regression with application to mass spectrometry proteomics data},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Correction: Sensitivity analysis for an unobserved moderator
in RCT-to-target-population generalization of treatment effects.
<em>AOAS</em>, <em>14</em>(1), 518–520. (<a
href="https://doi.org/10.1214/19-AOAS1320">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_AOAS},
  author       = {Trang Quynh Nguyen and Elizabeth A. Stuart},
  doi          = {10.1214/19-AOAS1320},
  journal      = {The Annals of Applied Statistics},
  number       = {1},
  pages        = {518-520},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Correction: Sensitivity analysis for an unobserved moderator in RCT-to-target-population generalization of treatment effects},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Bayesian mixed effects models for zero-inflated compositions
in microbiome data analysis. <em>AOAS</em>, <em>14</em>(1), 494–517. (<a
href="https://doi.org/10.1214/19-AOAS1295">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Detecting associations between microbial compositions and sample characteristics is one of the most important tasks in microbiome studies. Most of the existing methods apply univariate models to single microbial species separately, with adjustments for multiple hypothesis testing. We propose a Bayesian analysis for a generalized mixed effects linear model tailored to this application. The marginal prior on each microbial composition is a Dirichlet process, and dependence across compositions is induced through a linear combination of individual covariates, such as disease biomarkers or the subject’s age, and latent factors. The latent factors capture residual variability and their dimensionality is learned from the data in a fully Bayesian procedure. The proposed model is tested in data analyses and simulation studies with zero-inflated compositions. In these settings and within each sample, a large proportion of counts per microbial species are equal to zero. In our Bayesian model a priori the probability of compositions with absent microbial species is strictly positive. We propose an efficient algorithm to sample from the posterior and visualizations of model parameters which reveal associations between covariates and microbial compositions. We evaluate the proposed method in simulation studies, and then analyze a microbiome dataset for infants with type 1 diabetes which contains a large proportion of zeros in the sample-specific microbial compositions.},
  archive      = {J_AOAS},
  author       = {Boyu Ren and Sergio Bacallado and Stefano Favaro and Tommi Vatanen and Curtis Huttenhower and Lorenzo Trippa},
  doi          = {10.1214/19-AOAS1295},
  journal      = {The Annals of Applied Statistics},
  number       = {1},
  pages        = {494-517},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Bayesian mixed effects models for zero-inflated compositions in microbiome data analysis},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A hierarchical dependent dirichlet process prior for
modelling bird migration patterns in the UK. <em>AOAS</em>,
<em>14</em>(1), 473–493. (<a
href="https://doi.org/10.1214/19-AOAS1315">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Environmental changes in recent years have been linked to phenological shifts which in turn are linked to the survival of species. The work in this paper is motivated by capture-recapture data on blackcaps collected by the British Trust for Ornithology as part of the Constant Effort Sites monitoring scheme. Blackcaps overwinter abroad and migrate to the UK annually for breeding purposes. We propose a novel Bayesian nonparametric approach for expressing the bivariate density of individual arrival and departure times at different sites across a number of years as a mixture model. The new model combines the ideas of the hierarchical and the dependent Dirichlet process, allowing the estimation of site-specific weights and year-specific mixture locations, which are modelled as functions of environmental covariates using a multivariate extension of the Gaussian process. The proposed modelling framework is extremely general and can be used in any context where multivariate density estimation is performed jointly across different groups and in the presence of a continuous covariate.},
  archive      = {J_AOAS},
  author       = {Alex Diana and Eleni Matechou and Jim Griffin and Alison Johnston},
  doi          = {10.1214/19-AOAS1315},
  journal      = {The Annals of Applied Statistics},
  number       = {1},
  pages        = {473-493},
  shortjournal = {Ann. Appl. Stat.},
  title        = {A hierarchical dependent dirichlet process prior for modelling bird migration patterns in the UK},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Estimating causal effects in studies of human brain
function: New models, methods and estimands. <em>AOAS</em>,
<em>14</em>(1), 452–472. (<a
href="https://doi.org/10.1214/19-AOAS1316">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Neuroscientists often use functional magnetic resonance imaging (fMRI) to infer effects of treatments on neural activity in brain regions. In a typical fMRI experiment, each subject is observed at several hundred time points. At each point, the blood oxygenation level dependent (BOLD) response is measured at 100,000 or more locations (voxels). Typically, these responses are modeled treating each voxel separately, and no rationale for interpreting associations as effects is given. Building on Sobel and Lindquist (J. Amer. Statist. Assoc. 109 (2014) 967–976), who used potential outcomes to define unit and average effects at each voxel and time point, we define and estimate both “point” and “cumulated” effects for brain regions. Second, we construct a multisubject, multivoxel, multirun whole brain causal model with explicit parameters for regions. We justify estimation using BOLD responses averaged over voxels within regions, making feasible estimation for all regions simultaneously, thereby also facilitating inferences about association between effects in different regions. We apply the model to a study of pain, finding effects in standard pain regions. We also observe more cerebellar activity than observed in previous studies using prevailing methods.},
  archive      = {J_AOAS},
  author       = {Michael E. Sobel and Martin A. Lindquist},
  doi          = {10.1214/19-AOAS1316},
  journal      = {The Annals of Applied Statistics},
  number       = {1},
  pages        = {452-472},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Estimating causal effects in studies of human brain function: New models, methods and estimands},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A comparison of principal component methods between multiple
phenotype regression and multiple SNP regression in genetic association
studies. <em>AOAS</em>, <em>14</em>(1), 433–451. (<a
href="https://doi.org/10.1214/19-AOAS1312">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Principal component analysis (PCA) is a popular method for dimension reduction in unsupervised multivariate analysis. However, existing ad hoc uses of PCA in both multivariate regression (multiple outcomes) and multiple regression (multiple predictors) lack theoretical justification. The differences in the statistical properties of PCAs in these two regression settings are not well understood. In this paper we provide theoretical results on the power of PCA in genetic association testings in both multiple phenotype and SNP-set settings. The multiple phenotype setting refers to the case when one is interested in studying the association between a single SNP and multiple phenotypes as outcomes. The SNP-set setting refers to the case when one is interested in studying the association between multiple SNPs in a SNP set and a single phenotype as the outcome. We demonstrate analytically that the properties of the PC-based analysis in these two regression settings are substantially different. We show that the lower order PCs, that is, PCs with large eigenvalues, are generally preferred and lead to a higher power in the SNP-set setting, while the higher-order PCs, that is, PCs with small eigenvalues, are generally preferred in the multiple phenotype setting. We also investigate the power of three other popular statistical methods, the Wald test, the variance component test and the minimum $p$-value test, in both multiple phenotype and SNP-set settings. We use theoretical power, simulation studies, and two real data analyses to validate our findings.},
  archive      = {J_AOAS},
  author       = {Zhonghua Liu and Ian Barnett and Xihong Lin},
  doi          = {10.1214/19-AOAS1312},
  journal      = {The Annals of Applied Statistics},
  number       = {1},
  pages        = {433-451},
  shortjournal = {Ann. Appl. Stat.},
  title        = {A comparison of principal component methods between multiple phenotype regression and multiple SNP regression in genetic association studies},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Measuring human activity spaces from GPS data with density
ranking and summary curves. <em>AOAS</em>, <em>14</em>(1), 409–432. (<a
href="https://doi.org/10.1214/19-AOAS1311">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Activity spaces are fundamental to the assessment of individuals’ dynamic exposure to social and environmental risk factors associated with multiple spatial contexts that are visited during activities of daily living. In this paper we survey existing approaches for measuring the geometry, size and structure of activity spaces, based on GPS data, and explain their limitations. We propose addressing these shortcomings through a nonparametric approach called density ranking and also through three summary curves: the mass-volume curve, the Betti number curve and the persistence curve. We introduce a novel mixture model for human activity spaces and study its asymptotic properties. We prove that the kernel density estimator, which at the present time, is one of the most widespread methods for measuring activity spaces, is not a stable estimator of their structure. We illustrate the practical value of our methods with a simulation study and with a recently collected GPS dataset that comprises the locations visited by 10 individuals over a six months period.},
  archive      = {J_AOAS},
  author       = {Yen-Chi Chen and Adrian Dobra},
  doi          = {10.1214/19-AOAS1311},
  journal      = {The Annals of Applied Statistics},
  number       = {1},
  pages        = {409-432},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Measuring human activity spaces from GPS data with density ranking and summary curves},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Estimating and forecasting the smoking-attributable
mortality fraction for both genders jointly in over 60 countries.
<em>AOAS</em>, <em>14</em>(1), 381–408. (<a
href="https://doi.org/10.1214/19-AOAS1306">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Smoking is one of the leading preventable threats to human health and a major risk factor for lung cancer, upper aerodigestive cancer and chronic obstructive pulmonary disease. Estimating and forecasting the smoking attributable fraction (SAF) of mortality can yield insights into smoking epidemics and also provide a basis for more accurate mortality and life expectancy projection. Peto et al. (Lancet 339 (1992) 1268–1278) proposed a method to estimate the SAF using the lung cancer mortality rate as an indicator of exposure to smoking in the population of interest. Here, we use the same method to estimate the all-age SAF (ASAF) for both genders for over 60 countries. We document a strong and cross-nationally consistent pattern of the evolution of the SAF over time. We use this as the basis for a new Bayesian hierarchical model to project future male and female ASAF from over 60 countries simultaneously. This gives forecasts as well as predictive distributions that can be used to find uncertainty intervals for any quantity of interest. We assess the model using out-of-sample predictive validation and find that it provides good forecasts and well-calibrated forecast intervals, comparing favorably with other methods.},
  archive      = {J_AOAS},
  author       = {Yicheng Li and Adrian E. Raftery},
  doi          = {10.1214/19-AOAS1306},
  journal      = {The Annals of Applied Statistics},
  number       = {1},
  pages        = {381-408},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Estimating and forecasting the smoking-attributable mortality fraction for both genders jointly in over 60 countries},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Regression for copula-linked compound distributions with
applications in modeling aggregate insurance claims. <em>AOAS</em>,
<em>14</em>(1), 357–380. (<a
href="https://doi.org/10.1214/19-AOAS1299">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In actuarial research a task of particular interest and importance is to predict the loss cost for individual risks so that informative decisions are made in various insurance operations such as underwriting, ratemaking and capital management. The loss cost is typically viewed to follow a compound distribution where the summation of the severity variables is stopped by the frequency variable. A challenging issue in modeling such outcomes is to accommodate the potential dependence between the number of claims and the size of each individual claim. In this article we introduce a novel regression framework for compound distributions that uses a copula to accommodate the association between the frequency and the severity variables and, thus, allows for arbitrary dependence between the two components. We further show that the new model is very flexible and is easily modified to account for incomplete data due to censoring or truncation. The flexibility of the proposed model is illustrated using both simulated and real data sets. In the analysis of granular claims data from property insurance, we find substantive negative relationship between the number and the size of insurance claims. In addition, we demonstrate that ignoring the frequency-severity association could lead to biased decision-making in insurance operations.},
  archive      = {J_AOAS},
  author       = {Peng Shi and Zifeng Zhao},
  doi          = {10.1214/19-AOAS1299},
  journal      = {The Annals of Applied Statistics},
  number       = {1},
  pages        = {357-380},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Regression for copula-linked compound distributions with applications in modeling aggregate insurance claims},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Modeling wildfire ignition origins in southern california
using linear network point processes. <em>AOAS</em>, <em>14</em>(1),
339–356. (<a href="https://doi.org/10.1214/19-AOAS1309">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper focuses on spatial and temporal modeling of point processes on linear networks. Point processes on linear networks can simply be defined as point events occurring on or near line segment network structures embedded in a certain space. A separable modeling framework is introduced that posits separate formation and dissolution models of point processes on linear networks over time. While the model was inspired by spider web building activity in brick mortar lines, the focus is on modeling wildfire ignition origins near road networks over a span of 14 years. As most wildfires in California have human-related origins, modeling the origin locations with respect to the road network provides insight into how human, vehicular and structural densities affect ignition occurrence. Model results show that roads that traverse different types of regions such as residential, interface and wildland regions have higher ignition intensities compared to roads that only exist in each of the mentioned region types.},
  archive      = {J_AOAS},
  author       = {Medha Uppala and Mark S. Handcock},
  doi          = {10.1214/19-AOAS1309},
  journal      = {The Annals of Applied Statistics},
  number       = {1},
  pages        = {339-356},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Modeling wildfire ignition origins in southern california using linear network point processes},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Optimal asset allocation with multivariate bayesian dynamic
linear models. <em>AOAS</em>, <em>14</em>(1), 299–338. (<a
href="https://doi.org/10.1214/19-AOAS1303">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce a fast, closed-form, simulation-free method to model and forecast multiple asset returns and employ it to investigate the optimal ensemble of features to include when jointly predicting monthly stock and bond excess returns. Our approach builds on the Bayesian dynamic linear models of West and Harrison (Bayesian Forecasting and Dynamic Models (1997) Springer), and it can objectively determine, through a fully automated procedure, both the optimal set of regressors to include in the predictive system and the degree to which the model coefficients, volatilities and covariances should vary over time. When applied to a portfolio of five stock and bond returns, we find that our method leads to large forecast gains, both in statistical and economic terms. In particular, we find that relative to a standard no-predictability benchmark, the optimal combination of predictors, stochastic volatility and time-varying covariances increases the annualized certainty equivalent returns of a leverage-constrained power utility investor by more than 500 basis points.},
  archive      = {J_AOAS},
  author       = {Jared D. Fisher and Davide Pettenuzzo and Carlos M. Carvalho},
  doi          = {10.1214/19-AOAS1303},
  journal      = {The Annals of Applied Statistics},
  number       = {1},
  pages        = {299-338},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Optimal asset allocation with multivariate bayesian dynamic linear models},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Feature selection for generalized varying coefficient
mixed-effect models with application to obesity GWAS. <em>AOAS</em>,
<em>14</em>(1), 276–298. (<a
href="https://doi.org/10.1214/19-AOAS1310">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Motivated by an empirical analysis of data from a genome-wide association study on obesity, measured by the body mass index (BMI), we propose a two-step gene-detection procedure for generalized varying coefficient mixed-effects models with ultrahigh dimensional covariates. The proposed procedure selects significant single nucleotide polymorphisms (SNPs) impacting the mean BMI trend, some of which have already been biologically proven to be “fat genes.” The method also discovers SNPs that significantly influence the age-dependent variability of BMI. The proposed procedure takes into account individual variations of genetic effects and can also be directly applied to longitudinal data with continuous, binary or count responses. We employ Monte Carlo simulation studies to assess the performance of the proposed method and further carry out causal inference for the selected SNPs.},
  archive      = {J_AOAS},
  author       = {Wanghuan Chu and Runze Li and Jingyuan Liu and Matthew Reimherr},
  doi          = {10.1214/19-AOAS1310},
  journal      = {The Annals of Applied Statistics},
  number       = {1},
  pages        = {276-298},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Feature selection for generalized varying coefficient mixed-effect models with application to obesity GWAS},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Estimating the health effects of environmental mixtures
using bayesian semiparametric regression and sparsity inducing priors.
<em>AOAS</em>, <em>14</em>(1), 257–275. (<a
href="https://doi.org/10.1214/19-AOAS1307">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Humans are routinely exposed to mixtures of chemical and other environmental factors, making the quantification of health effects associated with environmental mixtures a critical goal for establishing environmental policy sufficiently protective of human health. The quantification of the effects of exposure to an environmental mixture poses several statistical challenges. It is often the case that exposure to multiple pollutants interact with each other to affect an outcome. Further, the exposure-response relationship between an outcome and some exposures, such as some metals, can exhibit complex, nonlinear forms, since some exposures can be beneficial and detrimental at different ranges of exposure. To estimate the health effects of complex mixtures, we propose a flexible Bayesian approach that allows exposures to interact with each other and have nonlinear relationships with the outcome. We induce sparsity using multivariate spike and slab priors to determine which exposures are associated with the outcome and which exposures interact with each other. The proposed approach is interpretable, as we can use the posterior probabilities of inclusion into the model to identify pollutants that interact with each other. We utilize our approach to study the impact of exposure to metals on child neurodevelopment in Bangladesh and find a nonlinear, interactive relationship between arsenic and manganese.},
  archive      = {J_AOAS},
  author       = {Joseph Antonelli and Maitreyi Mazumdar and David Bellinger and David Christiani and Robert Wright and Brent Coull},
  doi          = {10.1214/19-AOAS1307},
  journal      = {The Annals of Applied Statistics},
  number       = {1},
  pages        = {257-275},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Estimating the health effects of environmental mixtures using bayesian semiparametric regression and sparsity inducing priors},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Bayesian factor models for probabilistic cause of death
assessment with verbal autopsies. <em>AOAS</em>, <em>14</em>(1),
241–256. (<a href="https://doi.org/10.1214/19-AOAS1253">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The distribution of deaths by cause provides crucial information for public health planning, response and evaluation. About 60\% of deaths globally are not registered or given a cause, limiting our ability to understand disease epidemiology. Verbal autopsy (VA) surveys are increasingly used in such settings to collect information on the signs, symptoms and medical history of people who have recently died. This article develops a novel Bayesian method for estimation of population distributions of deaths by cause using verbal autopsy data. The proposed approach is based on a multivariate probit model where associations among items in questionnaires are flexibly induced by latent factors. Using the Population Health Metrics Research Consortium labeled data that include both VA and medically certified causes of death, we assess performance of the proposed method. Further, we estimate important questionnaire items that are highly associated with causes of death. This framework provides insights that will simplify future data},
  archive      = {J_AOAS},
  author       = {Tsuyoshi Kunihama and Zehang Richard Li and Samuel J. Clark and Tyler H. McCormick},
  doi          = {10.1214/19-AOAS1253},
  journal      = {The Annals of Applied Statistics},
  number       = {1},
  pages        = {241-256},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Bayesian factor models for probabilistic cause of death assessment with verbal autopsies},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A hierarchical bayesian model for predicting ecological
interactions using scaled evolutionary relationships. <em>AOAS</em>,
<em>14</em>(1), 221–240. (<a
href="https://doi.org/10.1214/19-AOAS1296">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Identifying undocumented or potential future interactions among species is a challenge facing modern ecologists. Recent link prediction methods rely on trait data; however, large species interaction databases are typically sparse and covariates are limited to only a fraction of species. On the other hand, evolutionary relationships, encoded as phylogenetic trees, can act as proxies for underlying traits and historical patterns of parasite sharing among hosts. We show that, using a network-based conditional model, phylogenetic information provides strong predictive power in a recently published global database of host-parasite interactions. By scaling the phylogeny using an evolutionary model, our method allows for biological interpretation often missing from latent variable models. To further improve on the phylogeny-only model, we combine a hierarchical Bayesian latent score framework for bipartite graphs that accounts for the number of interactions per species with host dependence informed by phylogeny. Combining the two information sources yields significant improvement in predictive accuracy over each of the submodels alone. As many interaction networks are constructed from presence-only data, we extend the model by integrating a correction mechanism for missing interactions which proves valuable in reducing uncertainty in unobserved interactions.},
  archive      = {J_AOAS},
  author       = {Mohamad Elmasri and Maxwell J. Farrell and T. Jonathan Davies and David A. Stephens},
  doi          = {10.1214/19-AOAS1296},
  journal      = {The Annals of Applied Statistics},
  number       = {1},
  pages        = {221-240},
  shortjournal = {Ann. Appl. Stat.},
  title        = {A hierarchical bayesian model for predicting ecological interactions using scaled evolutionary relationships},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Modifying the chi-square and the CMH test for population
genetic inference: Adapting to overdispersion. <em>AOAS</em>,
<em>14</em>(1), 202–220. (<a
href="https://doi.org/10.1214/19-AOAS1301">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Evolve and resequence studies provide a popular approach to simulate evolution in the lab and explore its genetic basis. In this context, Pearson’s chi-square test, Fisher’s exact test as well as the Cochran–Mantel–Haenszel test are commonly used to infer genomic positions affected by selection from temporal changes in allele frequency. However, the null model associated with these tests does not match the null hypothesis of actual interest. Indeed, due to genetic drift and possibly other additional noise components such as pool sequencing, the null variance in the data can be substantially larger than accounted for by these common test statistics. This leads to $p$-values that are systematically too small and, therefore, a huge number of false positive results. Even, if the ranking rather than the actual $p$-values is of interest, a naive application of the mentioned tests will give misleading results, as the amount of overdispersion varies from locus to locus. We therefore propose adjusted statistics that take the overdispersion into account while keeping the formulas simple. This is particularly useful in genome-wide applications, where millions of SNPs can be handled with little computational effort. We then apply the adapted test statistics to real data from Drosophila and investigate how information from intermediate generations can be included when available. We also discuss further applications such as genome-wide association studies based on pool sequencing data and tests for local adaptation.},
  archive      = {J_AOAS},
  author       = {Kerstin Spitzer and Marta Pelizzola and Andreas Futschik},
  doi          = {10.1214/19-AOAS1301},
  journal      = {The Annals of Applied Statistics},
  number       = {1},
  pages        = {202-220},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Modifying the chi-square and the CMH test for population genetic inference: Adapting to overdispersion},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). TFisher: A powerful truncation and weighting procedure for
combining <span class="math inline"><em>p</em></span>-values.
<em>AOAS</em>, <em>14</em>(1), 178–201. (<a
href="https://doi.org/10.1214/19-AOAS1302">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The $p$-value combination approach is an important statistical strategy for testing global hypotheses with broad applications in signal detection, meta-analysis, data integration, etc. In this paper we extend the classic Fisher’s combination method to a unified family of statistics, called TFisher, which allows a general truncation-and-weighting scheme of input $p$-values. TFisher can significantly improve statistical power over the Fisher and related truncation-only methods for detecting both rare and dense “signals.” To address wide applications, analytical calculations for TFisher’s size and power are deduced under any two continuous distributions in the null and the alternative hypotheses. The corresponding omnibus test (oTFisher) and its size calculation are also provided for data-adaptive analysis. We study the asymptotic optimal parameters of truncation and weighting based on Bahadur efficiency (BE). A new asymptotic measure, called the asymptotic power efficiency (APE), is also proposed for better reflecting the statistics’ performance in real data analysis. Interestingly, under the Gaussian mixture model in the signal detection problem, both BE and APE indicate that the soft-thresholding scheme is the best, the truncation and weighting parameters should be equal. By simulations of various signal patterns, we systematically compare the power of statistics within TFisher family as well as some rare-signal-optimal tests. We illustrate the use of TFisher in an exome-sequencing analysis for detecting novel genes of amyotrophic lateral sclerosis. Relevant computation has been implemented into an R package TFisher published on the Comprehensive R Archive Network to cater for applications.},
  archive      = {J_AOAS},
  author       = {Hong Zhang and Tiejun Tong and John Landers and Zheyang Wu},
  doi          = {10.1214/19-AOAS1302},
  journal      = {The Annals of Applied Statistics},
  number       = {1},
  pages        = {178-201},
  shortjournal = {Ann. Appl. Stat.},
  title        = {TFisher: A powerful truncation and weighting procedure for combining $p$-values},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Assessing wage status transition and stagnation using
quantile transition regression. <em>AOAS</em>, <em>14</em>(1), 160–177.
(<a href="https://doi.org/10.1214/19-AOAS1304">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Workers in Taiwan overall have been suffering from long-lasting wage stagnation since the mid-1990s. In particular, there seems to be little mobility for the wages of Taiwanese workers to transit across wage quantile groups. It is of interest to see if certain groups of workers, such as female, lower educated and younger generation workers, suffer from the problem more seriously than the others. This work tries to apply a systematic statistical approach to study this issue, based on the longitudinal data from the Panel Study of Family Dynamics (PSFD) survey conducted in Taiwan since 1999. We propose the quantile transition regression model, generalizing recent methodology for quantile association, to assess the wage status transition with respect to the marginal wage quantiles over time as well as the effects of certain demographic and job factors on the wage status transition. Estimation of the model can be based on the composite likelihoods utilizing the binary, or ordinal-data information regarding the quantile transition, with the associated asymptotic theory established. A goodness-of-fit procedure for the proposed model is developed. The performances of the estimation and the goodness-of-fit procedures for the quantile transition model are illustrated through simulations. The application of the proposed methodology to the PSFD survey data suggests that female, private-sector workers with higher age and education below postgraduate level suffer from more severe wage status stagnation than the others.},
  archive      = {J_AOAS},
  author       = {Chih-Yuan Hsu and Yi-Hau Chen and Ruoh-Rong Yu and Tsung-Wei Hung},
  doi          = {10.1214/19-AOAS1304},
  journal      = {The Annals of Applied Statistics},
  number       = {1},
  pages        = {160-177},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Assessing wage status transition and stagnation using quantile transition regression},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Surface temperature monitoring in liver procurement via
functional variance change-point analysis. <em>AOAS</em>,
<em>14</em>(1), 143–159. (<a
href="https://doi.org/10.1214/19-AOAS1297">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Liver procurement experiments with surface-temperature monitoring motivated Gao et al. (J. Amer. Statist. Assoc. 114 (2019) 773–781) to develop a variance change-point detection method under a smoothly-changing mean trend. However, the spotwise change points yielded from their method do not offer immediate information to surgeons since an organ is often transplanted as a whole or in part. We develop a new practical method that can analyze a defined portion of the organ surface at a time. It also provides a novel addition to the developing field of functional data monitoring. Furthermore, numerical challenge emerges for simultaneously modeling the variance functions of 2D locations and the mean function of location and time. The respective sample sizes in the scales of 10,000 and 1,000,000 for modeling these functions make standard spline estimation too costly to be useful. We introduce a multistage subsampling strategy with steps educated by quickly-computable preliminary statistical measures. Extensive simulations show that the new method can efficiently reduce the computational cost and provide reasonable parameter estimates. Application of the new method to our liver surface temperature monitoring data shows its effectiveness in providing accurate status change information for a selected portion of the organ in the experiment.},
  archive      = {J_AOAS},
  author       = {Zhenguo Gao and Pang Du and Ran Jin and John L. Robertson},
  doi          = {10.1214/19-AOAS1297},
  journal      = {The Annals of Applied Statistics},
  number       = {1},
  pages        = {143-159},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Surface temperature monitoring in liver procurement via functional variance change-point analysis},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A statistical analysis of noisy crowdsourced weather data.
<em>AOAS</em>, <em>14</em>(1), 116–142. (<a
href="https://doi.org/10.1214/19-AOAS1290">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Spatial prediction of weather elements like temperature, precipitation, and barometric pressure are generally based on satellite imagery or data collected at ground stations. None of these data provide information at a more granular or “hyperlocal” resolution. On the other hand, crowdsourced weather data, which are captured by sensors installed on mobile devices and gathered by weather-related mobile apps like WeatherSignal and AccuWeather, can serve as potential data sources for analyzing environmental processes at a hyperlocal resolution. However, due to the low quality of the sensors and the nonlaboratory environment, the quality of the observations in crowdsourced data is compromised. This paper describes methods to improve hyperlocal spatial prediction using this varying-quality, noisy crowdsourced information. We introduce a reliability metric, namely Veracity Score (VS), to assess the quality of the crowdsourced observations using a coarser, but high-quality, reference data. A VS-based methodology to analyze noisy spatial data is proposed and evaluated through extensive simulations. The merits of the proposed approach are illustrated through case studies analyzing crowdsourced daily average ambient temperature readings for one day in the contiguous United States.},
  archive      = {J_AOAS},
  author       = {Arnab Chakraborty and Soumendra Nath Lahiri and Alyson Wilson},
  doi          = {10.1214/19-AOAS1290},
  journal      = {The Annals of Applied Statistics},
  number       = {1},
  pages        = {116-142},
  shortjournal = {Ann. Appl. Stat.},
  title        = {A statistical analysis of noisy crowdsourced weather data},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Modeling microbial abundances and dysbiosis with
beta-binomial regression. <em>AOAS</em>, <em>14</em>(1), 94–115. (<a
href="https://doi.org/10.1214/19-AOAS1283">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Using a sample from a population to estimate the proportion of the population with a certain category label is a broadly important problem. In the context of microbiome studies, this problem arises when researchers wish to use a sample from a population of microbes to estimate the population proportion of a particular taxon, known as the taxon’s relative abundance. In this paper, we propose a beta-binomial model for this task. Like existing models, our model allows for a taxon’s relative abundance to be associated with covariates of interest. However, unlike existing models, our proposal also allows for the overdispersion in the taxon’s counts to be associated with covariates of interest. We exploit this model in order to propose tests not only for differential relative abundance, but also for differential variability. The latter is particularly valuable in light of speculation that dysbiosis, the perturbation from a normal microbiome that can occur in certain disease conditions, may manifest as a loss of stability, or increase in variability, of the counts associated with each taxon. We demonstrate the performance of our proposed model using a simulation study and an application to soil microbial data.},
  archive      = {J_AOAS},
  author       = {Bryan D. Martin and Daniela Witten and Amy D. Willis},
  doi          = {10.1214/19-AOAS1283},
  journal      = {The Annals of Applied Statistics},
  number       = {1},
  pages        = {94-115},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Modeling microbial abundances and dysbiosis with beta-binomial regression},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Efficient real-time monitoring of an emerging influenza
pandemic: How feasible? <em>AOAS</em>, <em>14</em>(1), 74–93. (<a
href="https://doi.org/10.1214/19-AOAS1278">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A prompt public health response to a new epidemic relies on the ability to monitor and predict its evolution in real time as data accumulate. The 2009 A/H1N1 outbreak in the UK revealed pandemic data as noisy, contaminated, potentially biased and originating from multiple sources. This seriously challenges the capacity for real-time monitoring. Here, we assess the feasibility of real-time inference based on such data by constructing an analytic tool combining an age-stratified SEIR transmission model with various observation models describing the data generation mechanisms. As batches of data become available, a sequential Monte Carlo (SMC) algorithm is developed to synthesise multiple imperfect data streams, iterate epidemic inferences and assess model adequacy amidst a rapidly evolving epidemic environment, substantially reducing computation time in comparison to standard MCMC, to ensure timely delivery of real-time epidemic assessments. In application to simulated data designed to mimic the 2009 A/H1N1 epidemic, SMC is shown to have additional benefits in terms of assessing predictive performance and coping with parameter nonidentifiability.},
  archive      = {J_AOAS},
  author       = {Paul J. Birrell and Lorenz Wernisch and Brian D. M. Tom and Leonhard Held and Gareth O. Roberts and Richard G. Pebody and Daniela De Angelis},
  doi          = {10.1214/19-AOAS1278},
  journal      = {The Annals of Applied Statistics},
  number       = {1},
  pages        = {74-93},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Efficient real-time monitoring of an emerging influenza pandemic: How feasible?},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Integrative survival analysis with uncertain event times in
application to a suicide risk study. <em>AOAS</em>, <em>14</em>(1),
51–73. (<a href="https://doi.org/10.1214/19-AOAS1287">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The concept of integrating data from disparate sources to accelerate scientific discovery has generated tremendous excitement in many fields. The potential benefits from data integration, however, may be compromised by the uncertainty due to incomplete/imperfect record linkage. Motivated by a suicide risk study, we propose an approach for analyzing survival data with uncertain event times arising from data integration. Specifically, in our problem deaths identified from the hospital discharge records together with reported suicidal deaths determined by the Office of Medical Examiner may still not include all the death events of patients, and the missing deaths can be recovered from a complete database of death records. Since the hospital discharge data can only be linked to the death record data by matching basic patient characteristics, a patient with a censored death time from the first dataset could be linked to multiple potential event records in the second dataset. We develop an integrative Cox proportional hazards regression in which the uncertainty in the matched event times is modeled probabilistically. The estimation procedure combines the ideas of profile likelihood and the expectation conditional maximization algorithm (ECM). Simulation studies demonstrate that under realistic settings of imperfect data linkage the proposed method outperforms several competing approaches including multiple imputation. A marginal screening analysis using the proposed integrative Cox model is performed to identify risk factors associated with death following suicide-related hospitalization in Connecticut. The identified diagnostics codes are consistent with existing literature and provide several new insights on suicide risk, prediction and prevention.},
  archive      = {J_AOAS},
  author       = {Wenjie Wang and Robert Aseltine and Kun Chen and Jun Yan},
  doi          = {10.1214/19-AOAS1287},
  journal      = {The Annals of Applied Statistics},
  number       = {1},
  pages        = {51-73},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Integrative survival analysis with uncertain event times in application to a suicide risk study},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). BART with targeted smoothing: An analysis of
patient-specific stillbirth risk. <em>AOAS</em>, <em>14</em>(1), 28–50.
(<a href="https://doi.org/10.1214/19-AOAS1268">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article introduces BART with Targeted Smoothing, or tsBART, a new Bayesian tree-based model for nonparametric regression. The goal of tsBART is to introduce smoothness over a single target covariate $t$ while not necessarily requiring smoothness over other covariates $x$. tsBART is based on the Bayesian Additive Regression Trees (BART) model, an ensemble of regression trees. tsBART extends BART by parameterizing each tree’s terminal nodes with smooth functions of $t$ rather than independent scalars. Like BART, tsBART captures complex nonlinear relationships and interactions among the predictors. But unlike BART, tsBART guarantees that the response surface will be smooth in the target covariate. This improves interpretability and helps to regularize the estimate. After introducing and benchmarking the tsBART model, we apply it to our motivating example—pregnancy outcomes data from the National Center for Health Statistics. Our aim is to provide patient-specific estimates of stillbirth risk across gestational age $(t)$ and based on maternal and fetal risk factors $(x)$. Obstetricians expect stillbirth risk to vary smoothly over gestational age but not necessarily over other covariates, and tsBART has been designed precisely to reflect this structural knowledge. The results of our analysis show the clear superiority of the tsBART model for quantifying stillbirth risk, thereby providing patients and doctors with better information for managing the risk of fetal mortality. All methods described here are implemented in the R package tsbart.},
  archive      = {J_AOAS},
  author       = {Jennifer E. Starling and Jared S. Murray and Carlos M. Carvalho and Radek K. Bukowski and James G. Scott},
  doi          = {10.1214/19-AOAS1268},
  journal      = {The Annals of Applied Statistics},
  number       = {1},
  pages        = {28-50},
  shortjournal = {Ann. Appl. Stat.},
  title        = {BART with targeted smoothing: An analysis of patient-specific stillbirth risk},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). SHOPPER: A probabilistic model of consumer choice with
substitutes and complements. <em>AOAS</em>, <em>14</em>(1), 1–27. (<a
href="https://doi.org/10.1214/19-AOAS1265">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We develop SHOPPER, a sequential probabilistic model of shopping data. SHOPPER uses interpretable components to model the forces that drive how a customer chooses products; in particular, we designed SHOPPER to capture how items interact with other items. We develop an efficient posterior inference algorithm to estimate these forces from large-scale data, and we analyze a large dataset from a major chain grocery store. We are interested in answering counterfactual queries about changes in prices. We found that SHOPPER provides accurate predictions even under price interventions, and that it helps identify complementary and substitutable pairs of products.},
  archive      = {J_AOAS},
  author       = {Francisco J. R. Ruiz and Susan Athey and David M. Blei},
  doi          = {10.1214/19-AOAS1265},
  journal      = {The Annals of Applied Statistics},
  number       = {1},
  pages        = {1-27},
  shortjournal = {Ann. Appl. Stat.},
  title        = {SHOPPER: A probabilistic model of consumer choice with substitutes and complements},
  volume       = {14},
  year         = {2020},
}
</textarea>
</details></li>
</ul>

</body>
</html>
