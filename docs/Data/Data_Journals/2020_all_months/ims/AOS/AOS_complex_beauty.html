<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>AOS_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="aos---155">AOS - 155</h2>
<ul>
<li><details>
<summary>
(2020). Robust bayes-like estimation: Rho-bayes estimation.
<em>AOS</em>, <em>48</em>(6), 3699–3720. (<a
href="https://doi.org/10.1214/20-AOS1948">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We observe $n$ independent random variables with joint distribution ${\mathbf{P}}$ and pretend that they are i.i.d. with some common density $s$ (with respect to a known measure $\mu $) that we wish to estimate. We consider a density model $\overline{S}$ for $s$ that we endow with a prior distribution $\pi $ (with support in $\overline{S}$) and build a robust alternative to the classical Bayes posterior distribution which possesses similar concentration properties around $s$ whenever the data are truly i.i.d. and their density $s$ belongs to the model $\overline{S}$. Furthermore, in this case, the Hellinger distance between the classical and the robust posterior distributions tends to 0, as the number of observations tends to infinity, under suitable assumptions on the model and the prior. However, unlike what happens with the classical Bayes posterior distribution, we show that the concentration properties of this new posterior distribution are still preserved when the model is misspecified or when the data are not i.i.d. but the marginal densities of their joint distribution are close enough in Hellinger distance to the model $\overline{S}$.},
  archive      = {J_AOS},
  author       = {Yannick Baraud and Lucien Birgé},
  doi          = {10.1214/20-AOS1948},
  journal      = {The Annals of Statistics},
  number       = {6},
  pages        = {3699-3720},
  shortjournal = {Ann. Statist.},
  title        = {Robust bayes-like estimation: Rho-bayes estimation},
  volume       = {48},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020b). Isotonic regression in multi-dimensional spaces and graphs.
<em>AOS</em>, <em>48</em>(6), 3672–3698. (<a
href="https://doi.org/10.1214/20-AOS1947">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we study minimax and adaptation rates in general isotonic regression. For uniform deterministic and random designs in $[0,1]^{d}$ with ${d\ge2}$ and $N(0,1)$ noise, the minimax rate for the $\ell_{2}$ risk is known to be bounded from below by $n^{-1/d}$ when the unknown mean function $f$ is nondecreasing and its range is bounded by a constant, while the least squares estimator (LSE) is known to nearly achieve the minimax rate up to a factor $(\log n)^{\gamma}$ where $n$ is the sample size, $\gamma=4$ in the lattice design and $\gamma=\max{9/2,(d^{2}+d+1)/2}$ in the random design. Moreover, the LSE is known to achieve the adaptation rate $(K/n)^{-2/d}{1\vee\log(n/K)}^{2\gamma}$ when $f$ is piecewise constant on $K$ hyperrectangles in a partition of $[0,1]^{d}$. Due to the minimax theorem, the LSE is identical on every design point to both the max-min and min-max estimators over all upper and lower sets containing the design point. This motivates our consideration of estimators which lie in-between the max-min and min-max estimators over possibly smaller classes of upper and lower sets, including a subclass of block estimators. Under a $q$th moment condition on the noise, we develop $\ell_{q}$ risk bounds for such general estimators for isotonic regression on graphs. For uniform deterministic and random designs in $[0,1]^{d}$ with $d\ge3$, our $\ell_{2}$ risk bound for the block estimator matches the minimax rate $n^{-1/d}$ when the range of $f$ is bounded and achieves the near parametric adaptation rate $(K/n){1\vee\log(n/K)}^{d}$ when $f$ is $K$-piecewise constant. Furthermore, the block estimator possesses the following oracle property in variable selection: When $f$ depends on only a subset $S$ of variables, the $\ell_{2}$ risk of the block estimator automatically achieves up to a poly-logarithmic factor the minimax rate based on the oracular knowledge of $S$.},
  archive      = {J_AOS},
  author       = {Hang Deng and Cun-Hui Zhang},
  doi          = {10.1214/20-AOS1947},
  journal      = {The Annals of Statistics},
  number       = {6},
  pages        = {3672-3698},
  shortjournal = {Ann. Statist.},
  title        = {Isotonic regression in multi-dimensional spaces and graphs},
  volume       = {48},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020a). Beyond gaussian approximation: Bootstrap for maxima of sums
of independent random vectors. <em>AOS</em>, <em>48</em>(6), 3643–3671.
(<a href="https://doi.org/10.1214/20-AOS1946">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Bonferroni adjustment, or the union bound, is commonly used to study rate optimality properties of statistical methods in high-dimensional problems. However, in practice, the Bonferroni adjustment is overly conservative. The extreme value theory has been proven to provide more accurate multiplicity adjustments in a number of settings, but only on an ad hoc basis. Recently, Gaussian approximation has been used to justify bootstrap adjustments in large scale simultaneous inference in some general settings when $n\gg (\log p)^{7}$, where $p$ is the multiplicity of the inference problem and $n$ is the sample size. The thrust of this theory is the validity of the Gaussian approximation for maxima of sums of independent random vectors in high dimension. In this paper, we reduce the sample size requirement to $n\gg (\log p)^{5}$ for the consistency of the empirical bootstrap and the multiplier/wild bootstrap in the Kolmogorov–Smirnov distance, possibly in the regime where the Gaussian approximation is not available. New comparison and anticoncentration theorems, which are of considerable interest in and of themselves, are developed as existing ones interweaved with Gaussian approximation are no longer applicable or strong enough to produce desired results.},
  archive      = {J_AOS},
  author       = {Hang Deng and Cun-Hui Zhang},
  doi          = {10.1214/20-AOS1946},
  journal      = {The Annals of Statistics},
  number       = {6},
  pages        = {3643-3671},
  shortjournal = {Ann. Statist.},
  title        = {Beyond gaussian approximation: Bootstrap for maxima of sums of independent random vectors},
  volume       = {48},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Analysis of a two-layer neural network via displacement
convexity. <em>AOS</em>, <em>48</em>(6), 3619–3642. (<a
href="https://doi.org/10.1214/20-AOS1945">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fitting a function by using linear combinations of a large number $N$ of “simple” components is one of the most fruitful ideas in statistical learning. This idea lies at the core of a variety of methods, from two-layer neural networks to kernel regression, to boosting. In general, the resulting risk minimization problem is nonconvex and is solved by gradient descent or its variants. Unfortunately, little is known about global convergence properties of these approaches. Here, we consider the problem of learning a concave function $f$ on a compact convex domain $\Omega \subset {\mathbb{R}}^{d}$, using linear combinations of “bump-like” components (neurons). The parameters to be fitted are the centers of $N$ bumps, and the resulting empirical risk minimization problem is highly nonconvex. We prove that, in the limit in which the number of neurons diverges, the evolution of gradient descent converges to a Wasserstein gradient flow in the space of probability distributions over $\Omega $. Further, when the bump width $\delta $ tends to $0$, this gradient flow has a limit which is a viscous porous medium equation. Remarkably, the cost function optimized by this gradient flow exhibits a special property known as displacement convexity, which implies exponential convergence rates for $N\to \infty $, $\delta \to 0$. Surprisingly, this asymptotic theory appears to capture well the behavior for moderate values of $\delta $, $N$. Explaining this phenomenon, and understanding the dependence on $\delta $, $N$ in a quantitative manner remains an outstanding challenge.},
  archive      = {J_AOS},
  author       = {Adel Javanmard and Marco Mondelli and Andrea Montanari},
  doi          = {10.1214/20-AOS1945},
  journal      = {The Annals of Statistics},
  number       = {6},
  pages        = {3619-3642},
  shortjournal = {Ann. Statist.},
  title        = {Analysis of a two-layer neural network via displacement convexity},
  volume       = {48},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Optimal estimation of variance in nonparametric regression
with random design. <em>AOS</em>, <em>48</em>(6), 3589–3618. (<a
href="https://doi.org/10.1214/20-AOS1944">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Consider the heteroscedastic nonparametric regression model with random design \begin{equation*}Y_{i}=f(X_{i})+V^{1/2}(X_{i})\varepsilon _{i},\quad i=1,2,\ldots ,n,\end{equation*} with $f(\cdot )$ and $V(\cdot )$ $\alpha $- and $\beta $-Hölder smooth, respectively. We show that the minimax rate of estimating $V(\cdot )$ under both local and global squared risks is of the order \begin{equation*}n^{-\frac{8\alpha \beta }{4\alpha \beta +2\alpha +\beta }}\vee n^{-\frac{2\beta }{2\beta +1}},\end{equation*} where $a\vee b:=\max {a,b}$ for any two real numbers $a$, $b$. This result extends the fixed design rate $n^{-4\alpha }\vee n^{-2\beta /(2\beta +1)}$ derived in (Ann. Statist. 36 (2008) 646–664) in a nontrivial manner, as indicated by the appearances of both $\alpha $ and $\beta $ in the first term. In the special case of constant variance, we show that the minimax rate is $n^{-8\alpha /(4\alpha +1)}\vee n^{-1}$ for variance estimation, which further implies the same rate for quadratic functional estimation and thus unifies the minimax rate under the nonparametric regression model with those under the density model and the white noise model. To achieve the minimax rate, we develop a U-statistic-based local polynomial estimator and a lower bound that is constructed over a specified distribution family of randomness designed for both $\varepsilon _{i}$ and $X_{i}$.},
  archive      = {J_AOS},
  author       = {Yandi Shen and Chao Gao and Daniela Witten and Fang Han},
  doi          = {10.1214/20-AOS1944},
  journal      = {The Annals of Statistics},
  number       = {6},
  pages        = {3589-3618},
  shortjournal = {Ann. Statist.},
  title        = {Optimal estimation of variance in nonparametric regression with random design},
  volume       = {48},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Test for high dimensional covariance matrices. <em>AOS</em>,
<em>48</em>(6), 3565–3588. (<a
href="https://doi.org/10.1214/20-AOS1943">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The paper introduces a new test for testing structures of covariances for high dimensional vectors and the data dimension can be much larger than the sample size. Under proper normalization, central and noncentral limit theorems are established. The asymptotic theory is attained without imposing any explicit restriction between data dimension and sample size. To facilitate the related statistical inference, we propose the balanced Rademacher weighted differencing scheme, which is also the delete-half jackknife, to approximate the distribution of the proposed test statistics. We also develop a new testing procedure for substructures of precision matrices. The simulation results show that the tests outperform the exiting methods both in terms of size and power. Our test procedure is applied to a colorectal cancer dataset.},
  archive      = {J_AOS},
  author       = {Yuefeng Han and Wei Biao Wu},
  doi          = {10.1214/20-AOS1943},
  journal      = {The Annals of Statistics},
  number       = {6},
  pages        = {3565-3588},
  shortjournal = {Ann. Statist.},
  title        = {Test for high dimensional covariance matrices},
  volume       = {48},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Irreducibility and geometric ergodicity of hamiltonian monte
carlo. <em>AOS</em>, <em>48</em>(6), 3545–3564. (<a
href="https://doi.org/10.1214/19-AOS1941">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hamiltonian Monte Carlo (HMC) is currently one of the most popular Markov Chain Monte Carlo algorithms to sample smooth distributions over continuous state space. This paper discusses the irreducibility and geometric ergodicity of the HMC algorithm. We consider cases where the number of steps of the Störmer–Verlet integrator is either fixed or random. Under mild conditions on the potential $U$ associated with target distribution $\pi$, we first show that the Markov kernel associated to the HMC algorithm is irreducible and positive recurrent. Under more stringent conditions, we then establish that the Markov kernel is Harris recurrent. We provide verifiable conditions on $U$ under which the HMC sampler is geometrically ergodic. Finally, we illustrate our results on several examples.},
  archive      = {J_AOS},
  author       = {Alain Durmus and Éric Moulines and Eero Saksman},
  doi          = {10.1214/19-AOS1941},
  journal      = {The Annals of Statistics},
  number       = {6},
  pages        = {3545-3564},
  shortjournal = {Ann. Statist.},
  title        = {Irreducibility and geometric ergodicity of hamiltonian monte carlo},
  volume       = {48},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Model selection and local geometry. <em>AOS</em>,
<em>48</em>(6), 3513–3544. (<a
href="https://doi.org/10.1214/19-AOS1940">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider problems in model selection caused by the geometry of models close to their points of intersection. In some cases—including common classes of causal or graphical models, as well as time series models—distinct models may nevertheless have identical tangent spaces. This has two immediate consequences: first, in order to obtain constant power to reject one model in favour of another we need local alternative hypotheses that decrease to the null at a slower rate than the usual parametric $n^{-1/2}$ (typically we will require $n^{-1/4}$ or slower); in other words, to distinguish between the models we need large effect sizes or very large sample sizes. Second, we show that under even weaker conditions on their tangent cones, models in these classes cannot be made simultaneously convex by a reparameterization. This shows that Bayesian network models, amongst others, cannot be learned directly with a convex method similar to the graphical lasso. However, we are able to use our results to suggest methods for model selection that learn the tangent space directly, rather than the model itself. In particular, we give a generic algorithm for learning Bayesian network models.},
  archive      = {J_AOS},
  author       = {Robin J. Evans},
  doi          = {10.1214/19-AOS1940},
  journal      = {The Annals of Statistics},
  number       = {6},
  pages        = {3513-3544},
  shortjournal = {Ann. Statist.},
  title        = {Model selection and local geometry},
  volume       = {48},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Clustering in block markov chains. <em>AOS</em>,
<em>48</em>(6), 3488–3512. (<a
href="https://doi.org/10.1214/19-AOS1939">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper considers cluster detection in Block Markov Chains (BMCs). These Markov chains are characterized by a block structure in their transition matrix. More precisely, the $n$ possible states are divided into a finite number of $K$ groups or clusters, such that states in the same cluster exhibit the same transition rates to other states. One observes a trajectory of the Markov chain, and the objective is to recover, from this observation only, the (initially unknown) clusters. In this paper, we devise a clustering procedure that accurately, efficiently and provably detects the clusters. We first derive a fundamental information-theoretical lower bound on the detection error rate satisfied under any clustering algorithm. This bound identifies the parameters of the BMC, and trajectory lengths, for which it is possible to accurately detect the clusters. We next develop two clustering algorithms that can together accurately recover the cluster structure from the shortest possible trajectories, whenever the parameters allow detection. These algorithms thus reach the fundamental detectability limit, and are optimal in that sense.},
  archive      = {J_AOS},
  author       = {Jaron Sanders and Alexandre Proutière and Se-Young Yun},
  doi          = {10.1214/19-AOS1939},
  journal      = {The Annals of Statistics},
  number       = {6},
  pages        = {3488-3512},
  shortjournal = {Ann. Statist.},
  title        = {Clustering in block markov chains},
  volume       = {48},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Simultaneous high-probability bounds on the false discovery
proportion in structured, regression and online settings. <em>AOS</em>,
<em>48</em>(6), 3465–3487. (<a
href="https://doi.org/10.1214/19-AOS1938">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {While traditional multiple testing procedures prohibit adaptive analysis choices made by users, Goeman and Solari (Statist. Sci. 26 (2011) 584–597) proposed a simultaneous inference framework that allows users such flexibility while preserving high-probability bounds on the false discovery proportion (FDP) of the chosen set. In this paper, we propose a new class of such simultaneous FDP bounds, tailored for nested sequences of rejection sets. While most existing simultaneous FDP bounds are based on closed testing using global null tests based on sorted $p$-values, we additionally consider the setting where side information can be leveraged to boost power, the variable selection setting where knockoff statistics can be used to order variables, and the online setting where decisions about rejections must be made as data arrives. Our finite-sample, closed form bounds are based on repurposing the FDP estimates from false discovery rate (FDR) controlling procedures designed for each of the above settings. These results establish a novel connection between the parallel literatures of simultaneous FDP bounds and FDR control methods, and use proof techniques employing martingales and filtrations that are new to both these literatures. We demonstrate the utility of our results by augmenting a recent knockoffs analysis of the UK Biobank dataset.},
  archive      = {J_AOS},
  author       = {Eugene Katsevich and Aaditya Ramdas},
  doi          = {10.1214/19-AOS1938},
  journal      = {The Annals of Statistics},
  number       = {6},
  pages        = {3465-3487},
  shortjournal = {Ann. Statist.},
  title        = {Simultaneous high-probability bounds on the false discovery proportion in structured, regression and online settings},
  volume       = {48},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Inference for conditional value-at-risk of a predictive
regression. <em>AOS</em>, <em>48</em>(6), 3442–3464. (<a
href="https://doi.org/10.1214/19-AOS1937">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Conditional value-at-risk is a popular risk measure in risk management. We study the inference problem of conditional value-at-risk under a linear predictive regression model. We derive the asymptotic distribution of the least squares estimator for the conditional value-at-risk. Our results relax the model assumptions made in (Oper. Res. 60 (2012) 739–756) and correct their mistake in the asymptotic variance expression. We show that the asymptotic variance depends on the quantile density function of the unobserved error and whether the model has a predictor with infinite variance, which makes it challenging to actually quantify the uncertainty of the conditional risk measure. To make the inference feasible, we then propose a smooth empirical likelihood based method for constructing a confidence interval for the conditional value-at-risk based on either independent errors or GARCH errors. Our approach not only bypasses the challenge of directly estimating the asymptotic variance but also does not need to know whether there exists an infinite variance predictor in the predictive model. Furthermore, we apply the same idea to the quantile regression method, which allows infinite variance predictors and generalizes the parameter estimation in (Econometric Theory 22 (2006) 173–205) to conditional value-at-risk in the Supplementary Material. We demonstrate the finite sample performance of the derived confidence intervals through numerical studies before applying them to real data.},
  archive      = {J_AOS},
  author       = {Yi He and Yanxi Hou and Liang Peng and Haipeng Shen},
  doi          = {10.1214/19-AOS1937},
  journal      = {The Annals of Statistics},
  number       = {6},
  pages        = {3442-3464},
  shortjournal = {Ann. Statist.},
  title        = {Inference for conditional value-at-risk of a predictive regression},
  volume       = {48},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Robust multivariate nonparametric tests via projection
averaging. <em>AOS</em>, <em>48</em>(6), 3417–3441. (<a
href="https://doi.org/10.1214/19-AOS1936">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this work, we generalize the Cramér–von Mises statistic via projection averaging to obtain a robust test for the multivariate two-sample problem. The proposed test is consistent against all fixed alternatives, robust to heavy-tailed data and minimax rate optimal against a certain class of alternatives. Our test statistic is completely free of tuning parameters and is computationally efficient even in high dimensions. When the dimension tends to infinity, the proposed test is shown to have comparable power to the existing high-dimensional mean tests under certain location models. As a by-product of our approach, we introduce a new metric called the angular distance which can be thought of as a robust alternative to the Euclidean distance. Using the angular distance, we connect the proposed method to the reproducing kernel Hilbert space approach. In addition to the Cramér–von Mises statistic, we demonstrate that the projection-averaging technique can be used to define robust multivariate tests in many other problems.},
  archive      = {J_AOS},
  author       = {Ilmun Kim and Sivaraman Balakrishnan and Larry Wasserman},
  doi          = {10.1214/19-AOS1936},
  journal      = {The Annals of Statistics},
  number       = {6},
  pages        = {3417-3441},
  shortjournal = {Ann. Statist.},
  title        = {Robust multivariate nonparametric tests via projection averaging},
  volume       = {48},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). The distance standard deviation. <em>AOS</em>,
<em>48</em>(6), 3395–3416. (<a
href="https://doi.org/10.1214/19-AOS1935">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The distance standard deviation, which arises in distance correlation analysis of multivariate data, is studied as a measure of spread. The asymptotic distribution of the empirical distance standard deviation is derived under the assumption of finite second moments. Applications are provided to hypothesis testing on a data set from materials science and to multivariate statistical quality control. The distance standard deviation is compared to classical scale measures for inference on the spread of heavy-tailed distributions. Inequalities for the distance variance are derived, proving that the distance standard deviation is bounded above by the classical standard deviation and by Gini’s mean difference. New expressions for the distance standard deviation are obtained in terms of Gini’s mean difference and the moments of spacings of order statistics. It is also shown that the distance standard deviation satisfies the axiomatic properties of a measure of spread.},
  archive      = {J_AOS},
  author       = {Dominic Edelmann and Donald Richards and Daniel Vogel},
  doi          = {10.1214/19-AOS1935},
  journal      = {The Annals of Statistics},
  number       = {6},
  pages        = {3395-3416},
  shortjournal = {Ann. Statist.},
  title        = {The distance standard deviation},
  volume       = {48},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Distance-based and RKHS-based dependence metrics in high
dimension. <em>AOS</em>, <em>48</em>(6), 3366–3394. (<a
href="https://doi.org/10.1214/19-AOS1934">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we study distance covariance, Hilbert–Schmidt covariance (aka Hilbert–Schmidt independence criterion [In Advances in Neural Information Processing Systems (2008) 585–592]) and related independence tests under the high dimensional scenario. We show that the sample distance/Hilbert–Schmidt covariance between two random vectors can be approximated by the sum of squared componentwise sample cross-covariances up to an asymptotically constant factor, which indicates that the standard distance/Hilbert–Schmidt covariance based test can only capture linear dependence in high dimension. Under the assumption that the components within each high dimensional vector are weakly dependent, the distance correlation based $t$ test developed by Székely and Rizzo (J. Multivariate Anal. 117 (2013) 193–213) for independence is shown to have trivial limiting power when the two random vectors are nonlinearly dependent but component-wisely uncorrelated. This new and surprising phenomenon, which seems to be discovered and carefully studied for the first time, is further confirmed in our simulation study. As a remedy, we propose tests based on an aggregation of marginal sample distance/Hilbert–Schmidt covariances and show their superior power behavior against their joint counterparts in simulations. We further extend the distance correlation based $t$ test to those based on Hilbert–Schmidt covariance and marginal distance/Hilbert–Schmidt covariance. A novel unified approach is developed to analyze the studentized sample distance/Hilbert–Schmidt covariance as well as the studentized sample marginal distance covariance under both null and alternative hypothesis. Our theoretical and simulation results shed light on the limitation of distance/Hilbert–Schmidt covariance when used jointly in the high dimensional setting and suggest the aggregation of marginal distance/Hilbert–Schmidt covariance as a useful alternative.},
  archive      = {J_AOS},
  author       = {Changbo Zhu and Xianyang Zhang and Shun Yao and Xiaofeng Shao},
  doi          = {10.1214/19-AOS1934},
  journal      = {The Annals of Statistics},
  number       = {6},
  pages        = {3366-3394},
  shortjournal = {Ann. Statist.},
  title        = {Distance-based and RKHS-based dependence metrics in high dimension},
  volume       = {48},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Nonparametric drift estimation for i.i.d. Paths of
stochastic differential equations. <em>AOS</em>, <em>48</em>(6),
3336–3365. (<a href="https://doi.org/10.1214/19-AOS1933">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider $N$ independent stochastic processes $(X_{i}(t),t\in [0,T])$, $i=1,\ldots ,N$, defined by a one-dimensional stochastic differential equation, which are continuously observed throughout a time interval $[0,T]$ where $T$ is fixed. We study nonparametric estimation of the drift function on a given subset $A$ of ${\mathbb{R}}$. Projection estimators are defined on finite dimensional subsets of ${\mathbb{L}}^{2}(A,dx)$. We stress that the set $A$ may be compact or not and the diffusion coefficient may be bounded or not. A data-driven procedure to select the dimension of the projection space is proposed where the dimension is chosen within a random collection of models. Upper bounds of risks are obtained, the assumptions are discussed and simulation experiments are reported.},
  archive      = {J_AOS},
  author       = {Fabienne Comte and Valentine Genon-Catalot},
  doi          = {10.1214/19-AOS1933},
  journal      = {The Annals of Statistics},
  number       = {6},
  pages        = {3336-3365},
  shortjournal = {Ann. Statist.},
  title        = {Nonparametric drift estimation for i.i.d. paths of stochastic differential equations},
  volume       = {48},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Fréchet change-point detection. <em>AOS</em>,
<em>48</em>(6), 3312–3335. (<a
href="https://doi.org/10.1214/19-AOS1930">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a method to infer the presence and location of change-points in the distribution of a sequence of independent data taking values in a general metric space, where change-points are viewed as locations at which the distribution of the data sequence changes abruptly in terms of either its Fréchet mean, Fréchet variance or both. The proposed method is based on comparisons of Fréchet variances before and after putative change-point locations and does not require a tuning parameter, except for the specification of cut-off intervals near the endpoints where change-points are assumed not to occur. Our results include theoretical guarantees for consistency of the test under contiguous alternatives when a change-point exists and also for consistency of the estimated location of the change-point, if it exists, where, under the null hypothesis of no change-point, the limit distribution of the proposed scan function is the square of a standardized Brownian bridge. These consistency results are applicable for a broad class of metric spaces under mild entropy conditions. Examples include the space of univariate probability distributions and the space of graph Laplacians for networks. Simulation studies demonstrate the effectiveness of the proposed methods, both for inferring the presence of a change-point and estimating its location. We also develop theory that justifies bootstrap-based inference and illustrate the new approach with sequences of maternal fertility distributions and communication networks.},
  archive      = {J_AOS},
  author       = {Paromita Dubey and Hans-Georg Müller},
  doi          = {10.1214/19-AOS1930},
  journal      = {The Annals of Statistics},
  number       = {6},
  pages        = {3312-3335},
  shortjournal = {Ann. Statist.},
  title        = {Fréchet change-point detection},
  volume       = {48},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Assessment of the extent of corroboration of an elaborate
theory of a causal hypothesis using partial conjunctions of evidence
factors. <em>AOS</em>, <em>48</em>(6), 3283–3311. (<a
href="https://doi.org/10.1214/19-AOS1929">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {An elaborate theory of predictions of a causal hypothesis consists of several falsifiable statements derived from the causal hypothesis. Statistical tests for the various pieces of the elaborate theory help to clarify how much the causal hypothesis is corroborated. In practice, the degree of corroboration of the causal hypothesis has been assessed by a verbal description of which of the several tests provides evidence for which of the several predictions. This verbal approach can miss quantitative patterns. In this paper, we develop a quantitative approach. We first decompose these various tests of the predictions into independent factors with different sources of potential biases. Support for the causal hypothesis is enhanced when many of these evidence factors support the predictions. A sensitivity analysis is used to assess the potential bias that could make the finding of the tests spurious. Along with this multiparameter sensitivity analysis, we consider the partial conjunctions of the tests. These partial conjunctions quantify the evidence supporting various fractions of the collection of predictions. A partial conjunction test involves combining tests of the components in the partial conjunction. We find the asymptotically optimal combination of tests in the context of a sensitivity analysis. Our analysis of an elaborate theory of a causal hypothesis controls for the familywise error rate.},
  archive      = {J_AOS},
  author       = {Bikram Karmakar and Dylan S. Small},
  doi          = {10.1214/19-AOS1929},
  journal      = {The Annals of Statistics},
  number       = {6},
  pages        = {3283-3311},
  shortjournal = {Ann. Statist.},
  title        = {Assessment of the extent of corroboration of an elaborate theory of a causal hypothesis using partial conjunctions of evidence factors},
  volume       = {48},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Limit distribution theory for block estimators in multiple
isotonic regression. <em>AOS</em>, <em>48</em>(6), 3251–3282. (<a
href="https://doi.org/10.1214/19-AOS1928">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study limit distributions for the tuning-free max–min block estimator originally proposed in (Fokianos, Leucht and Neumann (2017)) in the problem of multiple isotonic regression, under both fixed lattice design and random design settings. We show that, if the regression function $f_{0}$ admits vanishing derivatives up to order $\alpha_{k}$ along the $k$th dimension ($k=1,\ldots,d$) at a fixed point $x_{0}\in(0,1)^{d}$, and the errors have variance $\sigma^{2}$, then the max–min block estimator $\hat{f}_{n}$ satisfies \begin{equation*}\bigl(n_{\ast}/\sigma^{2}\bigr)^{\frac{1}{2+\sum_{k\in\mathcal{D}_{\ast}}\alpha_{k}^{-1}}}\bigl(\hat{f}_{n}(x_{0})-f_{0}(x_{0})\bigr)\rightsquigarrow \mathbb{C}(f_{0},x_{0}).\end{equation*} Here, $\mathcal{D}_{\ast},n_{\ast}$, depending on ${\alpha_{k}}$ and the design points, are the set of all “effective dimensions” and the size of “effective samples” that drive the asymptotic limiting distribution, respectively. If furthermore either ${\alpha_{k}}$ are relative primes to each other or all mixed derivatives of $f_{0}$ of certain critical order vanish at $x_{0}$, then the limiting distribution can be represented as $\mathbb{C}(f_{0},x_{0})=_{d}K(f_{0},x_{0})\cdot\mathbb{D}_{\boldsymbol{\alpha}}$, where $K(f_{0},x_{0})$ is a constant depending on the local structure of the regression function $f_{0}$ at $x_{0}$, and $\mathbb{D}_{\boldsymbol{\alpha}}$ is a nonstandard limiting distribution generalizing the well-known Chernoff distribution in univariate problems. The above limit theorem is also shown to be optimal both in terms of the local rate of convergence and the dependence on the unknown regression function whenever such dependence is explicit (i.e., $K(f_{0},x_{0})$), for the full range of ${\alpha_{k}}$ in a local asymptotic minimax sense. There are two interesting features in our local theory. First, the max–min block estimator automatically adapts to the local smoothness and the intrinsic dimension of the isotonic regression function at the optimal rate. Second, the optimally adaptive local rates are in general not the same in fixed lattice and random designs. In fact, the local rate in the fixed lattice design case is no slower than that in the random design case, and can be much faster when the local smoothness levels of the isotonic regression function or the sizes of the lattice differ substantially along different dimensions.},
  archive      = {J_AOS},
  author       = {Qiyang Han and Cun-Hui Zhang},
  doi          = {10.1214/19-AOS1928},
  journal      = {The Annals of Statistics},
  number       = {6},
  pages        = {3251-3282},
  shortjournal = {Ann. Statist.},
  title        = {Limit distribution theory for block estimators in multiple isotonic regression},
  volume       = {48},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Optimal rates of entropy estimation over lipschitz balls.
<em>AOS</em>, <em>48</em>(6), 3228–3250. (<a
href="https://doi.org/10.1214/19-AOS1927">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider the problem of minimax estimation of the entropy of a density over Lipschitz balls. Dropping the usual assumption that the density is bounded away from zero, we obtain the minimax rates $(n\ln n)^{-s/(s+d)}+n^{-1/2}$ for $0&lt;s\leq 2$ for densities supported on $[0,1]^{d}$, where $s$ is the smoothness parameter and $n$ is the number of independent samples. We generalize the results to densities with unbounded support: given an Orlicz functions $\Psi $ of rapid growth (such as the subexponential and sub-Gaussian classes), the minimax rates for densities with bounded $\Psi $-Orlicz norm increase to $(n\ln n)^{-s/(s+d)}(\Psi ^{-1}(n))^{d(1-d/p(s+d))}+n^{-1/2}$, where $p$ is the norm parameter in the Lipschitz ball. We also show that the integral-form plug-in estimators with kernel density estimates fail to achieve the minimax rates, and characterize their worst case performances over the Lipschitz ball. One of the key steps in analyzing the bias relies on a novel application of the Hardy–Littlewood maximal inequality, which also leads to a new inequality on the Fisher information that may be of independent interest.},
  archive      = {J_AOS},
  author       = {Yanjun Han and Jiantao Jiao and Tsachy Weissman and Yihong Wu},
  doi          = {10.1214/19-AOS1927},
  journal      = {The Annals of Statistics},
  number       = {6},
  pages        = {3228-3250},
  shortjournal = {Ann. Statist.},
  title        = {Optimal rates of entropy estimation over lipschitz balls},
  volume       = {48},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). High-dimensional consistent independence testing with maxima
of rank correlations. <em>AOS</em>, <em>48</em>(6), 3206–3227. (<a
href="https://doi.org/10.1214/19-AOS1926">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Testing mutual independence for high-dimensional observations is a fundamental statistical challenge. Popular tests based on linear and simple rank correlations are known to be incapable of detecting nonlinear, nonmonotone relationships, calling for methods that can account for such dependences. To address this challenge, we propose a family of tests that are constructed using maxima of pairwise rank correlations that permit consistent assessment of pairwise independence. Built upon a newly developed Cramér-type moderate deviation theorem for degenerate U-statistics, our results cover a variety of rank correlations including Hoeffding’s $D$, Blum–Kiefer–Rosenblatt’s $R$ and Bergsma–Dassios–Yanagimoto’s $\tau^{*}$. The proposed tests are distribution-free in the class of multivariate distributions with continuous margins, implementable without the need for permutation, and are shown to be rate-optimal against sparse alternatives under the Gaussian copula model. As a by-product of the study, we reveal an identity between the aforementioned three rank correlation statistics, and hence make a step towards proving a conjecture of Bergsma and Dassios.},
  archive      = {J_AOS},
  author       = {Mathias Drton and Fang Han and Hongjian Shi},
  doi          = {10.1214/19-AOS1926},
  journal      = {The Annals of Statistics},
  number       = {6},
  pages        = {3206-3227},
  shortjournal = {Ann. Statist.},
  title        = {High-dimensional consistent independence testing with maxima of rank correlations},
  volume       = {48},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Towards optimal estimation of bivariate isotonic matrices
with unknown permutations. <em>AOS</em>, <em>48</em>(6), 3183–3205. (<a
href="https://doi.org/10.1214/19-AOS1925">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many applications, including rank aggregation, crowd-labeling and graphon estimation, can be modeled in terms of a bivariate isotonic matrix with unknown permutations acting on its rows and/or columns. We consider the problem of estimating an unknown matrix in this class, based on noisy observations of (possibly, a subset of) its entries. We design and analyze polynomial-time algorithms that improve upon the state of the art in two distinct metrics, showing, in particular, that minimax optimal, computationally efficient estimation is achievable in certain settings. Along the way, we prove matching upper and lower bounds on the minimax radii of certain cone testing problems, which may be of independent interest. (A corollary of Theorem 3.5 of this paper was presented at the Conference on Learning Theory (COLT) 2018, and a statement of this result appears in the abstract (In Proceedings of the 31st Conference On Learning Theory (2018) 2037–2042 PMLR).)},
  archive      = {J_AOS},
  author       = {Cheng Mao and Ashwin Pananjady and Martin J. Wainwright},
  doi          = {10.1214/19-AOS1925},
  journal      = {The Annals of Statistics},
  number       = {6},
  pages        = {3183-3205},
  shortjournal = {Ann. Statist.},
  title        = {Towards optimal estimation of bivariate isotonic matrices with unknown permutations},
  volume       = {48},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Singularity, misspecification and the convergence rate of
EM. <em>AOS</em>, <em>48</em>(6), 3161–3182. (<a
href="https://doi.org/10.1214/19-AOS1924">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A line of recent work has analyzed the behavior of the Expectation-Maximization (EM) algorithm in the well-specified setting, in which the population likelihood is locally strongly concave around its maximizing argument. Examples include suitably separated Gaussian mixture models and mixtures of linear regressions. We consider over-specified settings in which the number of fitted components is larger than the number of components in the true distribution. Such mis-specified settings can lead to singularity in the Fisher information matrix, and moreover, the maximum likelihood estimator based on $n$ i.i.d. samples in $d$ dimensions can have a nonstandard $\mathcal{O}((d/n)^{\frac{1}{4}})$ rate of convergence. Focusing on the simple setting of two-component mixtures fit to a $d$-dimensional Gaussian distribution, we study the behavior of the EM algorithm both when the mixture weights are different (unbalanced case), and are equal (balanced case). Our analysis reveals a sharp distinction between these two cases: in the former, the EM algorithm converges geometrically to a point at Euclidean distance of $\mathcal{O}((d/n)^{\frac{1}{2}})$ from the true parameter, whereas in the latter case, the convergence rate is exponentially slower, and the fixed point has a much lower $\mathcal{O}((d/n)^{\frac{1}{4}})$ accuracy. Analysis of this singular case requires the introduction of some novel techniques: in particular, we make use of a careful form of localization in the associated empirical process, and develop a recursive argument to progressively sharpen the statistical rate.},
  archive      = {J_AOS},
  author       = {Raaz Dwivedi and Nhat Ho and Koulik Khamaru and Martin J. Wainwright and Michael I. Jordan and Bin Yu},
  doi          = {10.1214/19-AOS1924},
  journal      = {The Annals of Statistics},
  number       = {6},
  pages        = {3161-3182},
  shortjournal = {Ann. Statist.},
  title        = {Singularity, misspecification and the convergence rate of EM},
  volume       = {48},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Asymptotic joint distribution of extreme eigenvalues and
trace of large sample covariance matrix in a generalized spiked
population model. <em>AOS</em>, <em>48</em>(6), 3138–3160. (<a
href="https://doi.org/10.1214/19-AOS1882">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper studies the joint limiting behavior of extreme eigenvalues and trace of large sample covariance matrix in a generalized spiked population model, where the asymptotic regime is such that the dimension and sample size grow proportionally. The form of the joint limiting distribution is applied to conduct Johnson–Graybill-type tests, a family of approaches testing for signals in a statistical model. For this, higher order correction is further made, helping alleviate the impact of finite-sample bias. The proof rests on determining the joint asymptotic behavior of two classes of spectral processes, corresponding to the extreme and linear spectral statistics, respectively.},
  archive      = {J_AOS},
  author       = {Zeng Li and Fang Han and Jianfeng Yao},
  doi          = {10.1214/19-AOS1882},
  journal      = {The Annals of Statistics},
  number       = {6},
  pages        = {3138-3160},
  shortjournal = {Ann. Statist.},
  title        = {Asymptotic joint distribution of extreme eigenvalues and trace of large sample covariance matrix in a generalized spiked population model},
  volume       = {48},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Empirical bayes oracle uncertainty quantification for
regression. <em>AOS</em>, <em>48</em>(6), 3113–3137. (<a
href="https://doi.org/10.1214/19-AOS1845">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose an empirical Bayes method for high-dimensional linear regression models. Following an oracle approach that quantifies the error locally for each possible value of the parameter, we show that an empirical Bayes posterior contracts at the optimal rate at all parameters and leads to uniform size-optimal credible balls with guaranteed coverage under an “excessive bias restriction” condition. This condition gives rise to a new slicing of the entire space that is suitable for ensuring uniformity in uncertainty quantification. The obtained results immediately lead to optimal contraction and coverage properties for many conceivable classes simultaneously. The results are also extended to high-dimensional additive nonparametric regression models.},
  archive      = {J_AOS},
  author       = {Eduard Belitser and Subhashis Ghosal},
  doi          = {10.1214/19-AOS1845},
  journal      = {The Annals of Statistics},
  number       = {6},
  pages        = {3113-3137},
  shortjournal = {Ann. Statist.},
  title        = {Empirical bayes oracle uncertainty quantification for regression},
  volume       = {48},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Asymptotic risk and phase transition of <span
class="math inline"><em>l</em><sub>1</sub></span>-penalized robust
estimator. <em>AOS</em>, <em>48</em>(5), 3090–3111. (<a
href="https://doi.org/10.1214/19-AOS1923">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mean square error (MSE) of the estimator can be used to evaluate the performance of a regression model. In this paper, we derive the asymptotic MSE of $l_{1}$-penalized robust estimators in the limit of both sample size $n$ and dimension $p$ going to infinity with fixed ratio $n/p\rightarrow \delta $. We focus on the $l_{1}$-penalized least absolute deviation and $l_{1}$-penalized Huber’s regressions. Our analytic study shows the appearance of a sharp phase transition in the two-dimensional sparsity-undersampling phase space. We derive the explicit formula of the phase boundary. Remarkably, the phase boundary is identical to the phase transition curve of LASSO which is also identical to the previously known Donoho–Tanner phase transition for sparse recovery. Our derivation is based on the asymptotic analysis of the generalized approximation passing (GAMP) algorithm. We establish the asymptotic MSE of the $l_{1}$-penalized robust estimator by connecting it to the asymptotic MSE of the corresponding GAMP estimator. Our results provide some theoretical insight into the high-dimensional regression methods. Extensive computational experiments have been conducted to validate the correctness of our analytic results. We obtain fairly good agreement between theoretical prediction and numerical simulations on finite-size systems.},
  archive      = {J_AOS},
  author       = {Hanwen Huang},
  doi          = {10.1214/19-AOS1923},
  journal      = {The Annals of Statistics},
  number       = {5},
  pages        = {3090-3111},
  shortjournal = {Ann. Statist.},
  title        = {Asymptotic risk and phase transition of $l_{1}$-penalized robust estimator},
  volume       = {48},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Coupled conditional backward sampling particle filter.
<em>AOS</em>, <em>48</em>(5), 3066–3089. (<a
href="https://doi.org/10.1214/19-AOS1922">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The conditional particle filter (CPF) is a promising algorithm for general hidden Markov model smoothing. Empirical evidence suggests that the variant of CPF with backward sampling (CBPF) performs well even with long time series. Previous theoretical results have not been able to demonstrate the improvement brought by backward sampling, whereas we provide rates showing that CBPF can remain effective with a fixed number of particles independent of the time horizon. Our result is based on analysis of a new coupling of two CBPFs, the coupled conditional backward sampling particle filter (CCBPF). We show that CCBPF has good stability properties in the sense that with fixed number of particles, the coupling time in terms of iterations increases only linearly with respect to the time horizon under a general (strong mixing) condition. The CCBPF is useful not only as a theoretical tool, but also as a practical method that allows for unbiased estimation of smoothing expectations, following the recent developments by Jacob, Lindsten and Schön (2020). Unbiased estimation has many advantages, such as enabling the construction of asymptotically exact confidence intervals and straightforward parallelisation.},
  archive      = {J_AOS},
  author       = {Anthony Lee and Sumeetpal S. Singh and Matti Vihola},
  doi          = {10.1214/19-AOS1922},
  journal      = {The Annals of Statistics},
  number       = {5},
  pages        = {3066-3089},
  shortjournal = {Ann. Statist.},
  title        = {Coupled conditional backward sampling particle filter},
  volume       = {48},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Analytical nonlinear shrinkage of large-dimensional
covariance matrices. <em>AOS</em>, <em>48</em>(5), 3043–3065. (<a
href="https://doi.org/10.1214/19-AOS1921">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper establishes the first analytical formula for nonlinear shrinkage estimation of large-dimensional covariance matrices. We achieve this by identifying and mathematically exploiting a deep connection between nonlinear shrinkage and nonparametric estimation of the Hilbert transform of the sample spectral density. Previous nonlinear shrinkage methods were of numerical nature: QuEST requires numerical inversion of a complex equation from random matrix theory whereas NERCOME is based on a sample-splitting scheme. The new analytical method is more elegant and also has more potential to accommodate future variations or extensions. Immediate benefits are (i) that it is typically 1000 times faster with basically the same accuracy as QuEST and (ii) that it accommodates covariance matrices of dimension up to 10,000 and more. The difficult case where the matrix dimension exceeds the sample size is also covered.},
  archive      = {J_AOS},
  author       = {Olivier Ledoit and Michael Wolf},
  doi          = {10.1214/19-AOS1921},
  journal      = {The Annals of Statistics},
  number       = {5},
  pages        = {3043-3065},
  shortjournal = {Ann. Statist.},
  title        = {Analytical nonlinear shrinkage of large-dimensional covariance matrices},
  volume       = {48},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Relaxing the assumptions of knockoffs by conditioning.
<em>AOS</em>, <em>48</em>(5), 3021–3042. (<a
href="https://doi.org/10.1214/19-AOS1920">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The recent paper Candès et al. (J. R. Stat. Soc. Ser. B. Stat. Methodol. 80 (2018) 551–577) introduced model-X knockoffs, a method for variable selection that provably and nonasymptotically controls the false discovery rate with no restrictions or assumptions on the dimensionality of the data or the conditional distribution of the response given the covariates. The one requirement for the procedure is that the covariate samples are drawn independently and identically from a precisely-known (but arbitrary) distribution. The present paper shows that the exact same guarantees can be made without knowing the covariate distribution fully, but instead knowing it only up to a parametric model with as many as $\Omega (n^{*}p)$ parameters, where $p$ is the dimension and $n^{*}$ is the number of covariate samples (which may exceed the usual sample size $n$ of labeled samples when unlabeled samples are also available). The key is to treat the covariates as if they are drawn conditionally on their observed value for a sufficient statistic of the model. Although this idea is simple, even in Gaussian models conditioning on a sufficient statistic leads to a distribution supported on a set of zero Lebesgue measure, requiring techniques from topological measure theory to establish valid algorithms. We demonstrate how to do this for three models of interest, with simulations showing the new approach remains powerful under the weaker assumptions.},
  archive      = {J_AOS},
  author       = {Dongming Huang and Lucas Janson},
  doi          = {10.1214/19-AOS1920},
  journal      = {The Annals of Statistics},
  number       = {5},
  pages        = {3021-3042},
  shortjournal = {Ann. Statist.},
  title        = {Relaxing the assumptions of knockoffs by conditioning},
  volume       = {48},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Semiparametric bayesian causal inference. <em>AOS</em>,
<em>48</em>(5), 2999–3020. (<a
href="https://doi.org/10.1214/19-AOS1919">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We develop a semiparametric Bayesian approach for estimating the mean response in a missing data model with binary outcomes and a nonparametrically modelled propensity score. Equivalently, we estimate the causal effect of a treatment, correcting nonparametrically for confounding. We show that standard Gaussian process priors satisfy a semiparametric Bernstein–von Mises theorem under smoothness conditions. We further propose a novel propensity score-dependent prior that provides efficient inference under strictly weaker conditions. We also show that it is theoretically preferable to model the covariate distribution with a Dirichlet process or Bayesian bootstrap, rather than modelling its density.},
  archive      = {J_AOS},
  author       = {Kolyan Ray and Aad van der Vaart},
  doi          = {10.1214/19-AOS1919},
  journal      = {The Annals of Statistics},
  number       = {5},
  pages        = {2999-3020},
  shortjournal = {Ann. Statist.},
  title        = {Semiparametric bayesian causal inference},
  volume       = {48},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Inference for spherical location under high concentration.
<em>AOS</em>, <em>48</em>(5), 2982–2998. (<a
href="https://doi.org/10.1214/19-AOS1918">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Motivated by the fact that circular or spherical data are often much concentrated around a location $\pmb{\theta }$, we consider inference about $\pmb{\theta }$ under high concentration asymptotic scenarios for which the probability of any fixed spherical cap centered at $\pmb{\theta }$ converges to one as the sample size $n$ diverges to infinity. Rather than restricting to Fisher–von Mises–Langevin distributions, we consider a much broader, semiparametric, class of rotationally symmetric distributions indexed by the location parameter $\pmb{\theta }$, a scalar concentration parameter $\kappa $ and a functional nuisance $f$. We determine the class of distributions for which high concentration is obtained as $\kappa $ diverges to infinity. For such distributions, we then consider inference (point estimation, confidence zone estimation, hypothesis testing) on $\pmb{\theta }$ in asymptotic scenarios where $\kappa _{n}$ diverges to infinity at an arbitrary rate with the sample size $n$. Our asymptotic investigation reveals that, interestingly, optimal inference procedures on $\pmb{\theta }$ show consistency rates that depend on $f$. Using asymptotics “à la Le Cam,” we show that the spherical mean is, at any $f$, a parametrically superefficient estimator of ${\pmb{\theta }}$ and that the Watson and Wald tests for $\mathcal{H}_{0}:{\pmb{\theta }}={\pmb{\theta }}_{0}$ enjoy similar, nonstandard, optimality properties. We illustrate our results through simulations and treat a real data example. On a technical point of view, our asymptotic derivations require challenging expansions of rotationally symmetric functionals for large arguments of $f$.},
  archive      = {J_AOS},
  author       = {Davy Paindaveine and Thomas Verdebout},
  doi          = {10.1214/19-AOS1918},
  journal      = {The Annals of Statistics},
  number       = {5},
  pages        = {2982-2998},
  shortjournal = {Ann. Statist.},
  title        = {Inference for spherical location under high concentration},
  volume       = {48},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Valid post-selection inference in model-free linear
regression. <em>AOS</em>, <em>48</em>(5), 2953–2981. (<a
href="https://doi.org/10.1214/19-AOS1917">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Modern data-driven approaches to modeling make extensive use of covariate/model selection. Such selection incurs a cost: it invalidates classical statistical inference. A conservative remedy to the problem was proposed by Berk et al. (Ann. Statist. 41 (2013) 802–837) and further extended by Bachoc, Preinerstorfer and Steinberger (2016). These proposals, labeled “PoSI methods,” provide valid inference after arbitrary model selection. They are computationally NP-hard and have limitations in their theoretical justifications. We therefore propose computationally efficient confidence regions, named “UPoSI’ (“U” is for “uniform” or “universal.”) and prove large-$p$ asymptotics for them. We do this for linear OLS regression allowing misspecification of the normal linear model, for both fixed and random covariates, and for independent as well as some types of dependent data. We start by proving a general equivalence result for the post-selection inference problem and a simultaneous inference problem in a setting that strips inessential features still present in a related result of Berk et al. (Ann. Statist. 41 (2013) 802–837). We then construct valid PoSI confidence regions that are the first to have vastly improved computational efficiency in that the required computation times grow only quadratically rather than exponentially with the total number $p$ of covariates. These are also the first PoSI confidence regions with guaranteed asymptotic validity when the total number of covariates $p$ diverges (almost exponentially) with the sample size $n$. Under standard tail assumptions, we only require $(\log p)^{7}=o(n)$ and $k=o(\sqrt{n/\log p})$ where $k$ ($\le p$) is the largest number of covariates (model size) considered for selection. We study various properties of these confidence regions, including their Lebesgue measures, and compare them theoretically with those proposed previously.},
  archive      = {J_AOS},
  author       = {Arun K. Kuchibhotla and Lawrence D. Brown and Andreas Buja and Junhui Cai and Edward I. George and Linda H. Zhao},
  doi          = {10.1214/19-AOS1917},
  journal      = {The Annals of Statistics},
  number       = {5},
  pages        = {2953-2981},
  shortjournal = {Ann. Statist.},
  title        = {Valid post-selection inference in model-free linear regression},
  volume       = {48},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A framework for adaptive MCMC targeting multimodal
distributions. <em>AOS</em>, <em>48</em>(5), 2930–2952. (<a
href="https://doi.org/10.1214/19-AOS1916">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a new Monte Carlo method for sampling from multimodal distributions. The idea of this technique is based on splitting the task into two: finding the modes of a target distribution $\pi$ and sampling, given the knowledge of the locations of the modes. The sampling algorithm relies on steps of two types: local ones, preserving the mode; and jumps to regions associated with different modes. Besides, the method learns the optimal parameters of the algorithm, while it runs, without requiring user intervention. Our technique should be considered as a flexible framework, in which the design of moves can follow various strategies known from the broad MCMC literature. In order to design an adaptive scheme that facilitates both local and jump moves, we introduce an auxiliary variable representing each mode, and we define a new target distribution $\tilde{\pi}$ on an augmented state space $\mathcal{X}\times\mathcal{I}$, where $\mathcal{X}$ is the original state space of $\pi$ and $\mathcal{I}$ is the set of the modes. As the algorithm runs and updates its parameters, the target distribution $\tilde{\pi}$ also keeps being modified. This motivates a new class of algorithms, Auxiliary Variable Adaptive MCMC. We prove general ergodic results for the whole class before specialising to the case of our algorithm.},
  archive      = {J_AOS},
  author       = {Emilia Pompe and Chris Holmes and Krzysztof Łatuszyński},
  doi          = {10.1214/19-AOS1916},
  journal      = {The Annals of Statistics},
  number       = {5},
  pages        = {2930-2952},
  shortjournal = {Ann. Statist.},
  title        = {A framework for adaptive MCMC targeting multimodal distributions},
  volume       = {48},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Controlled sequential monte carlo. <em>AOS</em>,
<em>48</em>(5), 2904–2929. (<a
href="https://doi.org/10.1214/19-AOS1914">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sequential Monte Carlo methods, also known as particle methods, are a popular set of techniques for approximating high-dimensional probability distributions and their normalizing constants. These methods have found numerous applications in statistics and related fields; for example, for inference in nonlinear non-Gaussian state space models, and in complex static models. Like many Monte Carlo sampling schemes, they rely on proposal distributions which crucially impact their performance. We introduce here a class of controlled sequential Monte Carlo algorithms, where the proposal distributions are determined by approximating the solution to an associated optimal control problem using an iterative scheme. This method builds upon a number of existing algorithms in econometrics, physics and statistics for inference in state space models, and generalizes these methods so as to accommodate complex static models. We provide a theoretical analysis concerning the fluctuation and stability of this methodology that also provides insight into the properties of related algorithms. We demonstrate significant gains over state-of-the-art methods at a fixed computational complexity on a variety of applications.},
  archive      = {J_AOS},
  author       = {Jeremy Heng and Adrian N. Bishop and George Deligiannidis and Arnaud Doucet},
  doi          = {10.1214/19-AOS1914},
  journal      = {The Annals of Statistics},
  number       = {5},
  pages        = {2904-2929},
  shortjournal = {Ann. Statist.},
  title        = {Controlled sequential monte carlo},
  volume       = {48},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Asymptotic distribution and detection thresholds for
two-sample tests based on geometric graphs. <em>AOS</em>,
<em>48</em>(5), 2879–2903. (<a
href="https://doi.org/10.1214/19-AOS1913">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we consider the problem of testing the equality of two multivariate distributions based on geometric graphs constructed using the interpoint distances between the observations. These include the tests based on the minimum spanning tree and the $K$-nearest neighbor (NN) graphs, among others. These tests are asymptotically distribution-free, universally consistent and computationally efficient, making them particularly useful in modern applications. However, very little is known about the power properties of these tests. In this paper, using the theory of stabilizing geometric graphs, we derive the asymptotic distribution of these tests under general alternatives, in the Poissonized setting. Using this, the detection threshold and the limiting local power of the test based on the $K$-NN graph are obtained, where interesting exponents depending on dimension emerge. This provides a way to compare and justify the performance of these tests in different examples.},
  archive      = {J_AOS},
  author       = {Bhaswar B. Bhattacharya},
  doi          = {10.1214/19-AOS1913},
  journal      = {The Annals of Statistics},
  number       = {5},
  pages        = {2879-2903},
  shortjournal = {Ann. Statist.},
  title        = {Asymptotic distribution and detection thresholds for two-sample tests based on geometric graphs},
  volume       = {48},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A general framework for bayes structured linear models.
<em>AOS</em>, <em>48</em>(5), 2848–2878. (<a
href="https://doi.org/10.1214/19-AOS1909">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {High dimensional statistics deals with the challenge of extracting structured information from complex model settings. Compared with a large number of frequentist methodologies, there are rather few theoretically optimal Bayes methods for high dimensional models. This paper provides a unified approach to both Bayes high dimensional statistics and Bayes nonparametrics in a general framework of structured linear models. With a proposed two-step prior, we prove a general oracle inequality for posterior contraction under an abstract setting that allows model misspecification. The general result can be used to derive new results on optimal posterior contraction under many complex model settings including recent works for stochastic block model, graphon estimation and dictionary learning. It can also be used to improve upon posterior contraction results in literature including sparse linear regression and nonparametric aggregation. The key of the success lies in the novel two-step prior distribution: one for model structure, that is, model selection, and the other one for model parameters. The prior on the parameters of a model is an elliptical Laplace distribution that is capable of modeling signals with large magnitude, and the prior on the model structure involves a factor that compensates the effect of the normalizing constant of the elliptical Laplace distribution, which is important to attain rate-optimal posterior contraction.},
  archive      = {J_AOS},
  author       = {Chao Gao and Aad W. van der Vaart and Harrison H. Zhou},
  doi          = {10.1214/19-AOS1909},
  journal      = {The Annals of Statistics},
  number       = {5},
  pages        = {2848-2878},
  shortjournal = {Ann. Statist.},
  title        = {A general framework for bayes structured linear models},
  volume       = {48},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Permutation methods for factor analysis and PCA.
<em>AOS</em>, <em>48</em>(5), 2824–2847. (<a
href="https://doi.org/10.1214/19-AOS1907">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Researchers often have datasets measuring features $x_{ij}$ of samples, such as test scores of students. In factor analysis and PCA, these features are thought to be influenced by unobserved factors, such as skills. Can we determine how many components affect the data? This is an important problem, because decisions made here have a large impact on all downstream data analysis. Consequently, many approaches have been developed. Parallel Analysis is a popular permutation method: it randomly scrambles each feature of the data. It selects components if their singular values are larger than those of the permuted data. Despite widespread use, as well as empirical evidence for its accuracy, it currently has no theoretical justification. In this paper, we show that parallel analysis (or permutation methods) consistently select the large components in certain high-dimensional factor models. However, when the signals are too large, the smaller components are not selected. The intuition is that permutations keep the noise invariant, while “destroying” the low-rank signal. This provides justification for permutation methods. Our work also uncovers drawbacks of permutation methods, and paves the way to improvements.},
  archive      = {J_AOS},
  author       = {Edgar Dobriban},
  doi          = {10.1214/19-AOS1907},
  journal      = {The Annals of Statistics},
  number       = {5},
  pages        = {2824-2847},
  shortjournal = {Ann. Statist.},
  title        = {Permutation methods for factor analysis and PCA},
  volume       = {48},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Which bridge estimator is the best for variable selection?
<em>AOS</em>, <em>48</em>(5), 2791–2823. (<a
href="https://doi.org/10.1214/19-AOS1906">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study the problem of variable selection for linear models under the high-dimensional asymptotic setting, where the number of observations $n$ grows at the same rate as the number of predictors $p$. We consider two-stage variable selection techniques (TVS) in which the first stage uses bridge estimators to obtain an estimate of the regression coefficients, and the second stage simply thresholds this estimate to select the “important” predictors. The asymptotic false discovery proportion ($\operatorname{AFDP}$) and true positive proportion (ATPP) of these TVS are evaluated. We prove that for a fixed ATPP, in order to obtain a smaller $\operatorname{AFDP}$, one should pick a bridge estimator with smaller asymptotic mean square error in the first stage of TVS. Based on such principled discovery, we present a sharp comparison of different TVS, via an in-depth investigation of the estimation properties of bridge estimators. Rather than “orderwise” error bounds with loose constants, our analysis focuses on precise error characterization. Various interesting signal-to-noise ratio and sparsity settings are studied. Our results offer new and thorough insights into high-dimensional variable selection. For instance, we prove that a TVS with Ridge in its first stage outperforms TVS with other bridge estimators in large noise settings; two-stage LASSO becomes inferior when the signal is rare and weak. As a by-product, we show that two-stage methods outperform some standard variable selection techniques, such as $\operatorname{LASSO}$ and Sure Independence Screening, under certain conditions.},
  archive      = {J_AOS},
  author       = {Shuaiwen Wang and Haolei Weng and Arian Maleki},
  doi          = {10.1214/19-AOS1906},
  journal      = {The Annals of Statistics},
  number       = {5},
  pages        = {2791-2823},
  shortjournal = {Ann. Statist.},
  title        = {Which bridge estimator is the best for variable selection?},
  volume       = {48},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Variational analysis of constrained m-estimators.
<em>AOS</em>, <em>48</em>(5), 2759–2790. (<a
href="https://doi.org/10.1214/19-AOS1905">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a unified framework for establishing existence of nonparametric $M$-estimators, computing the corresponding estimates, and proving their strong consistency when the class of functions is exceptionally rich. In particular, the framework addresses situations where the class of functions is complex involving information and assumptions about shape, pointwise bounds, location of modes, height at modes, location of level-sets, values of moments, size of subgradients, continuity, distance to a “prior” function, multivariate total positivity and any combination of the above. The class might be engineered to perform well in a specific setting even in the presence of little data. The framework views the class of functions as a subset of a particular metric space of upper semicontinuous functions under the Attouch–Wets distance. In addition to allowing a systematic treatment of numerous $M$-estimators, the framework yields consistency of plug-in estimators of modes of densities, maximizers of regression functions, level-sets of classifiers and related quantities, and also enables computation by means of approximating parametric classes. We establish consistency through a one-sided law of large numbers, here extended to sieves, that relaxes assumptions of uniform laws, while ensuring global approximations even under model misspecification.},
  archive      = {J_AOS},
  author       = {Johannes O. Royset and Roger J-B Wets},
  doi          = {10.1214/19-AOS1905},
  journal      = {The Annals of Statistics},
  number       = {5},
  pages        = {2759-2790},
  shortjournal = {Ann. Statist.},
  title        = {Variational analysis of constrained M-estimators},
  volume       = {48},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Hypothesis testing for high-dimensional time series via
self-normalization. <em>AOS</em>, <em>48</em>(5), 2728–2758. (<a
href="https://doi.org/10.1214/19-AOS1904">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Self-normalization has attracted considerable attention in the recent literature of time series analysis, but its scope of applicability has been limited to low-/fixed-dimensional parameters for low-dimensional time series. In this article, we propose a new formulation of self-normalization for inference about the mean of high-dimensional stationary processes. Our original test statistic is a U-statistic with a trimming parameter to remove the bias caused by weak dependence. Under the framework of nonlinear causal processes, we show the asymptotic normality of our U-statistic with the convergence rate dependent upon the order of the Frobenius norm of the long-run covariance matrix. The self-normalized test statistic is then constructed on the basis of recursive subsampled U-statistics and its limiting null distribution is shown to be a functional of time-changed Brownian motion, which differs from the pivotal limit used in the low-dimensional setting. An interesting phenomenon associated with self-normalization is that it works in the high-dimensional context even if the convergence rate of original test statistic is unknown. We also present applications to testing for bandedness of the covariance matrix and testing for white noise for high-dimensional stationary time series and compare the finite sample performance with existing methods in simulation studies. At the root of our theoretical arguments, we extend the martingale approximation to the high-dimensional setting, which could be of independent theoretical interest.},
  archive      = {J_AOS},
  author       = {Runmin Wang and Xiaofeng Shao},
  doi          = {10.1214/19-AOS1904},
  journal      = {The Annals of Statistics},
  number       = {5},
  pages        = {2728-2758},
  shortjournal = {Ann. Statist.},
  title        = {Hypothesis testing for high-dimensional time series via self-normalization},
  volume       = {48},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Nonparametric bayesian estimation for multivariate hawkes
processes. <em>AOS</em>, <em>48</em>(5), 2698–2727. (<a
href="https://doi.org/10.1214/19-AOS1903">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper studies nonparametric estimation of parameters of multivariate Hawkes processes. We consider the Bayesian setting and derive posterior concentration rates. First, rates are derived for $\mathbb{L}_{1}$-metrics for stochastic intensities of the Hawkes process. We then deduce rates for the $\mathbb{L}_{1}$-norm of interactions functions of the process. Our results are exemplified by using priors based on piecewise constant functions, with regular or random partitions and priors based on mixtures of Betas distributions. We also present a simulation study to illustrate our results and to study empirically the inference on functional connectivity graphs of neurons},
  archive      = {J_AOS},
  author       = {Sophie Donnet and Vincent Rivoirard and Judith Rousseau},
  doi          = {10.1214/19-AOS1903},
  journal      = {The Annals of Statistics},
  number       = {5},
  pages        = {2698-2727},
  shortjournal = {Ann. Statist.},
  title        = {Nonparametric bayesian estimation for multivariate hawkes processes},
  volume       = {48},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Additive regression with hilbertian responses. <em>AOS</em>,
<em>48</em>(5), 2671–2697. (<a
href="https://doi.org/10.1214/19-AOS1902">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper develops a foundation of methodology and theory for the estimation of structured nonparametric regression models with Hilbertian responses. Our method and theory are focused on the additive model, while the main ideas may be adapted to other structured models. For this, the notion of Bochner integration is introduced for Banach-space-valued maps as a generalization of Lebesgue integration. Several statistical properties of Bochner integrals, relevant for our method and theory and also of importance in their own right, are presented for the first time. Our theory is complete. The existence of our estimators and the convergence of a practical algorithm that evaluates the estimators are established. These results are nonasymptotic as well as asymptotic. Furthermore, it is proved that the estimators achieve the univariate rates in pointwise, $L^{2}$ and uniform convergence, and that the estimators of the component maps converge jointly in distribution to Gaussian random elements. Our numerical examples include the cases of functional, density-valued and simplex-valued responses, demonstrating the validity of our approach.},
  archive      = {J_AOS},
  author       = {Jeong Min Jeon and Byeong U. Park},
  doi          = {10.1214/19-AOS1902},
  journal      = {The Annals of Statistics},
  number       = {5},
  pages        = {2671-2697},
  shortjournal = {Ann. Statist.},
  title        = {Additive regression with hilbertian responses},
  volume       = {48},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Geometrizing rates of convergence under local differential
privacy constraints. <em>AOS</em>, <em>48</em>(5), 2646–2670. (<a
href="https://doi.org/10.1214/19-AOS1901">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study the problem of estimating a functional $\theta ({\mathbb{P}})$ of an unknown probability distribution ${\mathbb{P}}\in {\mathcal{P}}$ in which the original iid sample $X_{1},\dots ,X_{n}$ is kept private even from the statistician via an $\alpha $-local differential privacy constraint. Let $\omega _{\mathrm{TV}}$ denote the modulus of continuity of the functional $\theta $ over ${\mathcal{P}}$ with respect to total variation distance. For a large class of loss functions $l$ and a fixed privacy level $\alpha $, we prove that the privatized minimax risk is equivalent to $l(\omega _{\mathrm{TV}}(n^{-1/2}))$ to within constants, under regularity conditions that are satisfied, in particular, if $\theta $ is linear and ${\mathcal{P}}$ is convex. Our results complement the theory developed by Donoho and Liu (1991) with the nowadays highly relevant case of privatized data. Somewhat surprisingly, the difficulty of the estimation problem in the private case is characterized by $\omega _{\mathrm{TV}}$, whereas, it is characterized by the Hellinger modulus of continuity if the original data $X_{1},\dots ,X_{n}$ are available. We also find that for locally private estimation of linear functionals over a convex model a simple sample mean estimator, based on independently and binary privatized observations, always achieves the minimax rate. We further provide a general recipe for choosing the functional parameter in the optimal binary privatization mechanisms and illustrate the general theory in numerous examples. Our theory allows us to quantify the price to be paid for local differential privacy in a large class of estimation problems. This price appears to be highly problem specific.},
  archive      = {J_AOS},
  author       = {Angelika Rohde and Lukas Steinberger},
  doi          = {10.1214/19-AOS1901},
  journal      = {The Annals of Statistics},
  number       = {5},
  pages        = {2646-2670},
  shortjournal = {Ann. Statist.},
  title        = {Geometrizing rates of convergence under local differential privacy constraints},
  volume       = {48},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Test of significance for high-dimensional longitudinal data.
<em>AOS</em>, <em>48</em>(5), 2622–2645. (<a
href="https://doi.org/10.1214/19-AOS1900">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper concerns statistical inference for longitudinal data with ultrahigh dimensional covariates. We first study the problem of constructing confidence intervals and hypothesis tests for a low-dimensional parameter of interest. The major challenge is how to construct a powerful test statistic in the presence of high-dimensional nuisance parameters and sophisticated within-subject correlation of longitudinal data. To deal with the challenge, we propose a new quadratic decorrelated inference function approach which simultaneously removes the impact of nuisance parameters and incorporates the correlation to enhance the efficiency of the estimation procedure. When the parameter of interest is of fixed dimension, we prove that the proposed estimator is asymptotically normal and attains the semiparametric information bound, based on which we can construct an optimal Wald test statistic. We further extend this result and establish the limiting distribution of the estimator under the setting with the dimension of the parameter of interest growing with the sample size at a polynomial rate. Finally, we study how to control the false discovery rate (FDR) when a vector of high-dimensional regression parameters is of interest. We prove that applying the Storey (J. R. Stat. Soc. Ser. B. Stat. Methodol. 64 (2002) 479–498) procedure to the proposed test statistics for each regression parameter controls FDR asymptotically in longitudinal data. We conduct simulation studies to assess the finite sample performance of the proposed procedures. Our simulation results imply that the newly proposed procedure can control both Type I error for testing a low dimensional parameter of interest and the FDR in the multiple testing problem. We also apply the proposed procedure to a real data example.},
  archive      = {J_AOS},
  author       = {Ethan X. Fang and Yang Ning and Runze Li},
  doi          = {10.1214/19-AOS1900},
  journal      = {The Annals of Statistics},
  number       = {5},
  pages        = {2622-2645},
  shortjournal = {Ann. Statist.},
  title        = {Test of significance for high-dimensional longitudinal data},
  volume       = {48},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Minimax optimal sequential hypothesis tests for markov
processes. <em>AOS</em>, <em>48</em>(5), 2599–2621. (<a
href="https://doi.org/10.1214/19-AOS1899">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Under mild Markov assumptions, sufficient conditions for strict minimax optimality of sequential tests for multiple hypotheses under distributional uncertainty are derived. First, the design of optimal sequential tests for simple hypotheses is revisited, and it is shown that the partial derivatives of the corresponding cost function are closely related to the performance metrics of the underlying sequential test. Second, an implicit characterization of the least favorable distributions for a given testing policy is stated. By combining the results on optimal sequential tests and least favorable distributions, sufficient conditions for a sequential test to be minimax optimal under general distributional uncertainties are obtained. The cost function of the minimax optimal test is further identified as a generalized $f$-dissimilarity and the least favorable distributions as those that are most similar with respect to this dissimilarity. Numerical examples for minimax optimal sequential tests under different uncertainties illustrate the theoretical results.},
  archive      = {J_AOS},
  author       = {Michael Fauß and Abdelhak M. Zoubir and H. Vincent Poor},
  doi          = {10.1214/19-AOS1899},
  journal      = {The Annals of Statistics},
  number       = {5},
  pages        = {2599-2621},
  shortjournal = {Ann. Statist.},
  title        = {Minimax optimal sequential hypothesis tests for markov processes},
  volume       = {48},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Theoretical and computational guarantees of mean field
variational inference for community detection. <em>AOS</em>,
<em>48</em>(5), 2575–2598. (<a
href="https://doi.org/10.1214/19-AOS1898">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The mean field variational Bayes method is becoming increasingly popular in statistics and machine learning. Its iterative coordinate ascent variational inference algorithm has been widely applied to large scale Bayesian inference. See Blei et al. (2017) for a recent comprehensive review. Despite the popularity of the mean field method, there exist remarkably little fundamental theoretical justifications. To the best of our knowledge, the iterative algorithm has never been investigated for any high-dimensional and complex model. In this paper, we study the mean field method for community detection under the stochastic block model. For an iterative batch coordinate ascent variational inference algorithm, we show that it has a linear convergence rate and converges to the minimax rate within $\log n$ iterations. This complements the results of Bickel et al. (2013) which studied the global minimum of the mean field variational Bayes and obtained asymptotic normal estimation of global model parameters. In addition, we obtain similar optimality results for Gibbs sampling and an iterative procedure to calculate maximum likelihood estimation, which can be of independent interest.},
  archive      = {J_AOS},
  author       = {Anderson Y. Zhang and Harrison H. Zhou},
  doi          = {10.1214/19-AOS1898},
  journal      = {The Annals of Statistics},
  number       = {5},
  pages        = {2575-2598},
  shortjournal = {Ann. Statist.},
  title        = {Theoretical and computational guarantees of mean field variational inference for community detection},
  volume       = {48},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). On spike and slab empirical bayes multiple testing.
<em>AOS</em>, <em>48</em>(5), 2548–2574. (<a
href="https://doi.org/10.1214/19-AOS1897">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper explores a connection between empirical Bayes posterior distributions and false discovery rate (FDR) control. In the Gaussian sequence model this work shows that empirical Bayes-calibrated spike and slab posterior distributions allow a correct FDR control under sparsity. Doing so, it offers a frequentist theoretical validation of empirical Bayes methods in the context of multiple testing. Our theoretical results are illustrated with numerical experiments.},
  archive      = {J_AOS},
  author       = {Ismaël Castillo and Étienne Roquain},
  doi          = {10.1214/19-AOS1897},
  journal      = {The Annals of Statistics},
  number       = {5},
  pages        = {2548-2574},
  shortjournal = {Ann. Statist.},
  title        = {On spike and slab empirical bayes multiple testing},
  volume       = {48},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Testing for stationarity of functional time series in the
frequency domain. <em>AOS</em>, <em>48</em>(5), 2505–2547. (<a
href="https://doi.org/10.1214/19-AOS1895">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Interest in functional time series has spiked in the recent past with papers covering both methodology and applications being published at a much increased pace. This article contributes to the research in this area by proposing a new stationarity test for functional time series based on frequency domain methods. The proposed test statistics is based on joint dimension reduction via functional principal components analysis across the spectral density operators at all Fourier frequencies, explicitly allowing for frequency-dependent levels of truncation to adapt to the dynamics of the underlying functional time series. The properties of the test are derived both under the null hypothesis of stationary functional time series and under the smooth alternative of locally stationary functional time series. The methodology is theoretically justified through asymptotic results. Evidence from simulation studies and an application to annual temperature curves suggests that the test works well in finite samples.},
  archive      = {J_AOS},
  author       = {Alexander Aue and Anne van Delft},
  doi          = {10.1214/19-AOS1895},
  journal      = {The Annals of Statistics},
  number       = {5},
  pages        = {2505-2547},
  shortjournal = {Ann. Statist.},
  title        = {Testing for stationarity of functional time series in the frequency domain},
  volume       = {48},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Isotropic covariance functions on graphs and their edges.
<em>AOS</em>, <em>48</em>(4), 2478–2503. (<a
href="https://doi.org/10.1214/19-AOS1896">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We develop parametric classes of covariance functions on linear networks and their extension to graphs with Euclidean edges, that is, graphs with edges viewed as line segments or more general sets with a coordinate system allowing us to consider points on the graph which are vertices or points on an edge. Our covariance functions are defined on the vertices and edge points of these graphs and are isotropic in the sense that they depend only on the geodesic distance or on a new metric called the resistance metric (which extends the classical resistance metric developed in electrical network theory on the vertices of a graph to the continuum of edge points). We discuss the advantages of using the resistance metric in comparison with the geodesic metric as well as the restrictions these metrics impose on the investigated covariance functions. In particular, many of the commonly used isotropic covariance functions in the spatial statistics literature (the power exponential, Matérn, generalized Cauchy and Dagum classes) are shown to be valid with respect to the resistance metric for any graph with Euclidean edges, whilst they are only valid with respect to the geodesic metric in more special cases.},
  archive      = {J_AOS},
  author       = {Ethan Anderes and Jesper Møller and Jakob G. Rasmussen},
  doi          = {10.1214/19-AOS1896},
  journal      = {The Annals of Statistics},
  number       = {4},
  pages        = {2478-2503},
  shortjournal = {Ann. Statist.},
  title        = {Isotropic covariance functions on graphs and their edges},
  volume       = {48},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Estimation and inference for precision matrices of
nonstationary time series. <em>AOS</em>, <em>48</em>(4), 2455–2477. (<a
href="https://doi.org/10.1214/19-AOS1894">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider the estimation of and inference on precision matrices of a rich class of univariate locally stationary linear and nonlinear time series, assuming that only one realization of the time series is observed. Using a Cholesky decomposition technique, we show that the precision matrices can be directly estimated via a series of least squares linear regressions with smoothly time-varying coefficients. The method of sieves is utilized for the estimation and is shown to be optimally adaptive in terms of estimation accuracy and efficient in terms of computational complexity. We establish an asymptotic theory for a class of $\mathcal{L}^{2}$ tests based on the nonparametric sieve estimators. The latter are used for testing whether the precision matrices are diagonal or banded. A Gaussian approximation result is established for a wide class of quadratic forms of nonstationary and possibly nonlinear processes of diverging dimensions which is of interest by itself.},
  archive      = {J_AOS},
  author       = {Xiucai Ding and Zhou Zhou},
  doi          = {10.1214/19-AOS1894},
  journal      = {The Annals of Statistics},
  number       = {4},
  pages        = {2455-2477},
  shortjournal = {Ann. Statist.},
  title        = {Estimation and inference for precision matrices of nonstationary time series},
  volume       = {48},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Minimax estimation of large precision matrices with bandable
cholesky factor. <em>AOS</em>, <em>48</em>(4), 2428–2454. (<a
href="https://doi.org/10.1214/19-AOS1893">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The last decade has witnessed significant methodological and theoretical advances in estimating large precision matrices. In particular, there are scientific applications such as longitudinal data, meteorology and spectroscopy in which the ordering of the variables can be interpreted through a bandable structure on the Cholesky factor of the precision matrix. However, the minimax theory has still been largely unknown, as opposed to the well established minimax results over the corresponding bandable covariance matrices. In this paper we focus on two commonly used types of parameter spaces and develop the optimal rates of convergence under both the operator norm and the Frobenius norm. A striking phenomenon is found. Two types of parameter spaces are fundamentally different under the operator norm but enjoy the same rate optimality under the Frobenius norm which is in sharp contrast to the equivalence of corresponding two types of bandable covariance matrices under both norms. This fundamental difference is established by carefully constructing the corresponding minimax lower bounds. Two new estimation procedures are developed. For the operator norm our optimal procedure is based on a novel local cropping estimator, targeting on all principle submatrices of the precision matrix, while for the Frobenius norm our optimal procedure relies on a delicate regression-based thresholding rule. Lepski’s method is considered to achieve optimal adaptation. We further establish rate optimality in the nonparanormal model. Numerical studies are carried out to confirm our theoretical findings.},
  archive      = {J_AOS},
  author       = {Yu Liu and Zhao Ren},
  doi          = {10.1214/19-AOS1893},
  journal      = {The Annals of Statistics},
  number       = {4},
  pages        = {2428-2454},
  shortjournal = {Ann. Statist.},
  title        = {Minimax estimation of large precision matrices with bandable cholesky factor},
  volume       = {48},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Extending the validity of frequency domain bootstrap methods
to general stationary processes. <em>AOS</em>, <em>48</em>(4),
2404–2427. (<a href="https://doi.org/10.1214/19-AOS1892">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing frequency domain methods for bootstrapping time series have a limited range. Essentially, these procedures cover the case of linear time series with independent innovations, and some even require the time series to be Gaussian. In this paper we propose a new frequency domain bootstrap method—the hybrid periodogram bootstrap (HPB)—which is consistent for a much wider range of stationary, even nonlinear, processes and which can be applied to a large class of periodogram-based statistics. The HPB is designed to combine desirable features of different frequency domain techniques while overcoming their respective limitations. It is capable to imitate the weak dependence structure of the periodogram by invoking the concept of convolved subsampling in a novel way that is tailor-made for periodograms. We show consistency for the HPB procedure for a general class of stationary time series, ranging clearly beyond linear processes, and for spectral means and ratio statistics on which we mainly focus. The finite sample performance of the new bootstrap procedure is illustrated via simulations.},
  archive      = {J_AOS},
  author       = {Marco Meyer and Efstathios Paparoditis and Jens-Peter Kreiss},
  doi          = {10.1214/19-AOS1892},
  journal      = {The Annals of Statistics},
  number       = {4},
  pages        = {2404-2427},
  shortjournal = {Ann. Statist.},
  title        = {Extending the validity of frequency domain bootstrap methods to general stationary processes},
  volume       = {48},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Bayesian analysis of the covariance matrix of a multivariate
normal distribution with a new class of priors. <em>AOS</em>,
<em>48</em>(4), 2381–2403. (<a
href="https://doi.org/10.1214/19-AOS1891">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Bayesian analysis for the covariance matrix of a multivariate normal distribution has received a lot of attention in the last two decades. In this paper, we propose a new class of priors for the covariance matrix, including both inverse Wishart and reference priors as special cases. The main motivation for the new class is to have available priors—both subjective and objective—that do not “force eigenvalues apart,” which is a criticism of inverse Wishart and Jeffreys priors. Extensive comparison of these “shrinkage priors” with inverse Wishart and Jeffreys priors is undertaken, with the new priors seeming to have considerably better performance. A number of curious facts about the new priors are also observed, such as that the posterior distribution will be proper with just three vector observations from the multivariate normal distribution—regardless of the dimension of the covariance matrix—and that useful inference about features of the covariance matrix can be possible. Finally, a new MCMC algorithm is developed for this class of priors and is shown to be computationally effective for matrices of up to 100 dimensions.},
  archive      = {J_AOS},
  author       = {James O. Berger and Dongchu Sun and Chengyuan Song},
  doi          = {10.1214/19-AOS1891},
  journal      = {The Annals of Statistics},
  number       = {4},
  pages        = {2381-2403},
  shortjournal = {Ann. Statist.},
  title        = {Bayesian analysis of the covariance matrix of a multivariate normal distribution with a new class of priors},
  volume       = {48},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Adaptive distributed methods under communication
constraints. <em>AOS</em>, <em>48</em>(4), 2347–2380. (<a
href="https://doi.org/10.1214/19-AOS1890">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study estimation methods under communication constraints in a distributed version of the nonparametric random design regression model. We derive minimax lower bounds and exhibit methods that attain those bounds. Moreover, we show that adaptive estimation is possible in this setting.},
  archive      = {J_AOS},
  author       = {Botond Szabó and Harry van Zanten},
  doi          = {10.1214/19-AOS1890},
  journal      = {The Annals of Statistics},
  number       = {4},
  pages        = {2347-2380},
  shortjournal = {Ann. Statist.},
  title        = {Adaptive distributed methods under communication constraints},
  volume       = {48},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A general approach for cure models in survival analysis.
<em>AOS</em>, <em>48</em>(4), 2323–2346. (<a
href="https://doi.org/10.1214/19-AOS1889">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In survival analysis it often happens that some subjects under study do not experience the event of interest; they are considered to be “cured.” The population is thus a mixture of two subpopulations, one of cured subjects and one of “susceptible” subjects. We propose a novel approach to estimate a mixture cure model when covariates are present and the lifetime is subject to random right censoring. We work with a parametric model for the cure proportion, while the conditional survival function of the uncured subjects is unspecified. The approach is based on an inversion which allows us to write the survival function as a function of the distribution of the observable variables. This leads to a very general class of models which allows a flexible and rich modeling of the conditional survival function. We show the identifiability of the proposed model as well as the consistency and the asymptotic normality of the model parameters. We also consider in more detail the case where kernel estimators are used for the nonparametric part of the model. The new estimators are compared with the estimators from a Cox mixture cure model via simulations. Finally, we apply the new model on a medical data set.},
  archive      = {J_AOS},
  author       = {Valentin Patilea and Ingrid Van Keilegom},
  doi          = {10.1214/19-AOS1889},
  journal      = {The Annals of Statistics},
  number       = {4},
  pages        = {2323-2346},
  shortjournal = {Ann. Statist.},
  title        = {A general approach for cure models in survival analysis},
  volume       = {48},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A test for separability in covariance operators of random
surfaces. <em>AOS</em>, <em>48</em>(4), 2303–2322. (<a
href="https://doi.org/10.1214/19-AOS1888">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The assumption of separability is a simplifying and very popular assumption in the analysis of spatiotemporal or hypersurface data structures. It is often made in situations where the covariance structure cannot be easily estimated, for example, because of a small sample size or because of computational storage problems. In this paper we propose a new and very simple test to validate this assumption. Our approach is based on a measure of separability which is zero in the case of separability and positive otherwise. We derive the asymptotic distribution of a corresponding estimate under the null hypothesis and the alternative and develop an asymptotic and a bootstrap test which are very easy to implement. In particular, our approach does neither require projections on subspaces generated by the eigenfunctions of the covariance operator nor distributional assumptions as recently used by (Ann. Statist. 45 (2017) 1431–1461) and (Biometrika 104 425–437) to construct tests for separability. We investigate the finite sample performance by means of a simulation study and also provide a comparison with the currently available methodology. Finally, the new procedure is illustrated analyzing a data example.},
  archive      = {J_AOS},
  author       = {Pramita Bagchi and Holger Dette},
  doi          = {10.1214/19-AOS1888},
  journal      = {The Annals of Statistics},
  number       = {4},
  pages        = {2303-2322},
  shortjournal = {Ann. Statist.},
  title        = {A test for separability in covariance operators of random surfaces},
  volume       = {48},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Identifiability of nonparametric mixture models and bayes
optimal clustering. <em>AOS</em>, <em>48</em>(4), 2277–2302. (<a
href="https://doi.org/10.1214/19-AOS1887">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Motivated by problems in data clustering, we establish general conditions under which families of nonparametric mixture models are identifiable by introducing a novel framework involving clustering overfitted parametric (i.e., misspecified) mixture models. These identifiability conditions generalize existing conditions in the literature and are flexible enough to include, for example, mixtures of infinite Gaussian mixtures. In contrast to the recent literature, we allow for general nonparametric mixture components and instead impose regularity assumptions on the underlying mixing measure. As our primary application we apply these results to partition-based clustering, generalizing the notion of a Bayes optimal partition from classical parametric model-based clustering to nonparametric settings. Furthermore, this framework is constructive, so that it yields a practical algorithm for learning identified mixtures, which is illustrated through several examples on real data. The key conceptual device in the analysis is the convex, metric geometry of probability measures on metric spaces and its connection to the Wasserstein convergence of mixing measures. The result is a flexible framework for nonparametric clustering with formal consistency guarantees.},
  archive      = {J_AOS},
  author       = {Bryon Aragam and Chen Dan and Eric P. Xing and Pradeep Ravikumar},
  doi          = {10.1214/19-AOS1887},
  journal      = {The Annals of Statistics},
  number       = {4},
  pages        = {2277-2302},
  shortjournal = {Ann. Statist.},
  title        = {Identifiability of nonparametric mixture models and bayes optimal clustering},
  volume       = {48},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Minimax optimal rates for mondrian trees and forests.
<em>AOS</em>, <em>48</em>(4), 2253–2276. (<a
href="https://doi.org/10.1214/19-AOS1886">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Introduced by Breiman (Mach. Learn. 45 (2001) 5–32), Random Forests are widely used classification and regression algorithms. While being initially designed as batch algorithms, several variants have been proposed to handle online learning. One particular instance of such forests is the Mondrian forest (In Adv. Neural Inf. Process. Syst. (2014) 3140–3148; In Proceedings of the 19th International Conference on Artificial Intelligence and Statistics (AISTATS) (2016)), whose trees are built using the so-called Mondrian process, therefore allowing to easily update their construction in a streaming fashion. In this paper we provide a thorough theoretical study of Mondrian forests in a batch learning setting, based on new results about Mondrian partitions. Our results include consistency and convergence rates for Mondrian trees and forests, that turn out to be minimax optimal on the set of $s$-Hölder function with $s\in (0,1]$ (for trees and forests) and $s\in (1,2]$ (for forests only), assuming a proper tuning of their complexity parameter in both cases. Furthermore, we prove that an adaptive procedure (to the unknown $s\in (0,2]$) can be constructed by combining Mondrian forests with a standard model aggregation algorithm. These results are the first demonstrating that some particular random forests achieve minimax rates in arbitrary dimension. Owing to their remarkably simple distributional properties, which lead to minimax rates, Mondrian trees are a promising basis for more sophisticated yet theoretically sound random forests variants.},
  archive      = {J_AOS},
  author       = {Jaouad Mourtada and Stéphane Gaïffas and Erwan Scornet},
  doi          = {10.1214/19-AOS1886},
  journal      = {The Annals of Statistics},
  number       = {4},
  pages        = {2253-2276},
  shortjournal = {Ann. Statist.},
  title        = {Minimax optimal rates for mondrian trees and forests},
  volume       = {48},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Beyond HC: More sensitive tests for rare/weak alternatives.
<em>AOS</em>, <em>48</em>(4), 2230–2252. (<a
href="https://doi.org/10.1214/19-AOS1885">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Higher criticism (HC) is a popular method for large-scale inference problems based on identifying unusually high proportions of small $p$-values. It has been shown to enjoy a lower-order optimality property in a simple normal location mixture model which is shared by the ‘tailor-made’ parametric generalised likelihood ratio test (GLRT) for the same model; however, HC has also been shown to perform well outside this ‘narrow’ model. We develop a higher-order framework for analysing the power of these and similar procedures, which reveals the perhaps unsurprising fact that the GLRT enjoys an edge in power over HC for the normal location mixture model. We also identify a similar parametric mixture model to which HC is similarly ‘tailor-made’ and show that the situation is (at least partly) reversed there. We also show that in the normal location mixture model a procedure based on the empirical moment-generating function enjoys the same local power properties as the GLRT and may be recommended as an easy to implement (and interpret), complementary procedure to HC. Some other practical advice regarding the implementation of these procedures is provided. Finally, we provide some simulation results to help interpret our theoretical findings.},
  archive      = {J_AOS},
  author       = {Thomas Porter and Michael Stewart},
  doi          = {10.1214/19-AOS1885},
  journal      = {The Annals of Statistics},
  number       = {4},
  pages        = {2230-2252},
  shortjournal = {Ann. Statist.},
  title        = {Beyond HC: More sensitive tests for rare/weak alternatives},
  volume       = {48},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Two-sample hypothesis testing for inhomogeneous random
graphs. <em>AOS</em>, <em>48</em>(4), 2208–2229. (<a
href="https://doi.org/10.1214/19-AOS1884">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The study of networks leads to a wide range of high-dimensional inference problems. In many practical applications, one needs to draw inference from one or few large sparse networks. The present paper studies hypothesis testing of graphs in this high-dimensional regime, where the goal is to test between two populations of inhomogeneous random graphs defined on the same set of $n$ vertices. The size of each population $m$ is much smaller than $n$, and can even be a constant as small as 1. The critical question in this context is whether the problem is solvable for small $m$. We answer this question from a minimax testing perspective. Let $P$, $Q$ be the population adjacencies of two sparse inhomogeneous random graph models, and $d$ be a suitably defined distance function. Given a population of $m$ graphs from each model, we derive minimax separation rates for the problem of testing $P=Q$ against $d(P,Q)&gt;\rho $. We observe that if $m$ is small, then the minimax separation is too large for some popular choices of $d$, including total variation distance between corresponding distributions. This implies that some models that are widely separated in $d$ cannot be distinguished for small $m$, and hence, the testing problem is generally not solvable in these cases. We also show that if $m&gt;1$, then the minimax separation is relatively small if $d$ is the Frobenius norm or operator norm distance between $P$ and $Q$. For $m=1$, only the latter distance provides small minimax separation. Thus, for these distances, the problem is solvable for small $m$. We also present near-optimal two-sample tests in both cases, where tests are adaptive with respect to sparsity level of the graphs.},
  archive      = {J_AOS},
  author       = {Debarghya Ghoshdastidar and Maurilio Gutzeit and Alexandra Carpentier and Ulrike von Luxburg},
  doi          = {10.1214/19-AOS1884},
  journal      = {The Annals of Statistics},
  number       = {4},
  pages        = {2208-2229},
  shortjournal = {Ann. Statist.},
  title        = {Two-sample hypothesis testing for inhomogeneous random graphs},
  volume       = {48},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Convergence rates of variational posterior distributions.
<em>AOS</em>, <em>48</em>(4), 2180–2207. (<a
href="https://doi.org/10.1214/19-AOS1883">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study convergence rates of variational posterior distributions for nonparametric and high-dimensional inference. We formulate general conditions on prior, likelihood and variational class that characterize the convergence rates. Under similar “prior mass and testing” conditions considered in the literature, the rate is found to be the sum of two terms. The first term stands for the convergence rate of the true posterior distribution, and the second term is contributed by the variational approximation error. For a class of priors that admit the structure of a mixture of product measures, we propose a novel prior mass condition, under which the variational approximation error of the mean-field class is dominated by convergence rate of the true posterior. We demonstrate the applicability of our general results for various models, prior distributions and variational classes by deriving convergence rates of the corresponding variational posteriors.},
  archive      = {J_AOS},
  author       = {Fengshuo Zhang and Chao Gao},
  doi          = {10.1214/19-AOS1883},
  journal      = {The Annals of Statistics},
  number       = {4},
  pages        = {2180-2207},
  shortjournal = {Ann. Statist.},
  title        = {Convergence rates of variational posterior distributions},
  volume       = {48},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Asymptotic frequentist coverage properties of bayesian
credible sets for sieve priors. <em>AOS</em>, <em>48</em>(4), 2155–2179.
(<a href="https://doi.org/10.1214/19-AOS1881">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We investigate the frequentist coverage properties of (certain) Bayesian credible sets in a general, adaptive, nonparametric framework. It is well known that the construction of adaptive and honest confidence sets is not possible in general. To overcome this problem (in context of sieve type of priors), we introduce an extra assumption on the functional parameters, the so-called “general polished tail” condition. We then show that under standard assumptions, both the hierarchical and empirical Bayes methods, result in honest confidence sets for sieve type of priors in general settings and we characterize their size. We apply the derived abstract results to various examples, including the nonparametric regression model, density estimation using exponential families of priors, density estimation using histogram priors and the nonparametric classification model, for which we show that their size is near minimax adaptive with respect to the considered specific pseudometrics.},
  archive      = {J_AOS},
  author       = {Judith Rousseau and Botond Szabo},
  doi          = {10.1214/19-AOS1881},
  journal      = {The Annals of Statistics},
  number       = {4},
  pages        = {2155-2179},
  shortjournal = {Ann. Statist.},
  title        = {Asymptotic frequentist coverage properties of bayesian credible sets for sieve priors},
  volume       = {48},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Double-slicing assisted sufficient dimension reduction for
high-dimensional censored data. <em>AOS</em>, <em>48</em>(4), 2132–2154.
(<a href="https://doi.org/10.1214/19-AOS1880">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper provides a unified framework and an efficient algorithm for analyzing high-dimensional survival data under weak modeling assumptions. In particular, it imposes neither parametric distributional assumption nor linear regression assumption. It only assumes that the survival time $T$ depends on a high-dimensional covariate vector $\mathbf{X}$ through low-dimensional linear combinations of covariates $\Gamma ^{T}\mathbf{X}$. The censoring time is allowed to be conditionally independent of the survival time given the covariates. This general framework includes many popular parametric and semiparametric survival regression models as special cases. The proposed algorithm produces a number of practically useful outputs with theoretical guarantees, including a consistent estimate of the sufficient dimension reduction subspace of $T\mid \mathbf{X}$, a uniformly consistent Kaplan–Meier-type estimator of the conditional distribution function of $T$ and a consistent estimator of the conditional quantile survival time. Our asymptotic results significantly extend the classical theory of sufficient dimension reduction for censored data (particularly that of Li, Wang and Chen in Ann. Statist. 27 (1999) 1–23) and the celebrated nonparametric Kaplan–Meier estimator to the setting where the number of covariates $p$ diverges exponentially fast with the sample size $n$. We demonstrate the promising performance of the proposed new estimators through simulations and a real data example.},
  archive      = {J_AOS},
  author       = {Shanshan Ding and Wei Qian and Lan Wang},
  doi          = {10.1214/19-AOS1880},
  journal      = {The Annals of Statistics},
  number       = {4},
  pages        = {2132-2154},
  shortjournal = {Ann. Statist.},
  title        = {Double-slicing assisted sufficient dimension reduction for high-dimensional censored data},
  volume       = {48},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Posterior concentration for bayesian regression trees and
forests. <em>AOS</em>, <em>48</em>(4), 2108–2131. (<a
href="https://doi.org/10.1214/19-AOS1879">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Since their inception in the 1980s, regression trees have been one of the more widely used nonparametric prediction methods. Tree-structured methods yield a histogram reconstruction of the regression surface, where the bins correspond to terminal nodes of recursive partitioning. Trees are powerful, yet susceptible to overfitting. Strategies against overfitting have traditionally relied on pruning greedily grown trees. The Bayesian framework offers an alternative remedy against overfitting through priors. Roughly speaking, a good prior charges smaller trees where overfitting does not occur. While the consistency of random histograms, trees and their ensembles has been studied quite extensively, the theoretical understanding of the Bayesian counterparts has been missing. In this paper, we take a step toward understanding why/when do Bayesian trees and forests not overfit. To address this question, we study the speed at which the posterior concentrates around the true smooth regression function. We propose a spike-and-tree variant of the popular Bayesian CART prior and establish new theoretical results showing that regression trees (and forests) (a) are capable of recovering smooth regression surfaces (with smoothness not exceeding one), achieving optimal rates up to a log factor, (b) can adapt to the unknown level of smoothness and (c) can perform effective dimension reduction when $p&gt;n$. These results provide a piece of missing theoretical evidence explaining why Bayesian trees (and additive variants thereof?) have worked so well in practice.},
  archive      = {J_AOS},
  author       = {Veronika Ročková and Stéphanie van der Pas},
  doi          = {10.1214/19-AOS1879},
  journal      = {The Annals of Statistics},
  number       = {4},
  pages        = {2108-2131},
  shortjournal = {Ann. Statist.},
  title        = {Posterior concentration for bayesian regression trees and forests},
  volume       = {48},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Partial identifiability of restricted latent class models.
<em>AOS</em>, <em>48</em>(4), 2082–2107. (<a
href="https://doi.org/10.1214/19-AOS1878">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Latent class models have wide applications in social and biological sciences. In many applications, prespecified restrictions are imposed on the parameter space of latent class models, through a design matrix, to reflect practitioners’ assumptions about how the observed responses depend on subjects’ latent traits. Though widely used in various fields, such restricted latent class models suffer from nonidentifiability due to their discreteness nature and complex structure of restrictions. This work addresses the fundamental identifiability issue of restricted latent class models by developing a general framework for strict and partial identifiability of the model parameters. Under correct model specification, the developed identifiability conditions only depend on the design matrix and are easily checkable, which provide useful practical guidelines for designing statistically valid diagnostic tests. Furthermore, the new theoretical framework is applied to establish, for the first time, identifiability of several designs from cognitive diagnosis applications.},
  archive      = {J_AOS},
  author       = {Yuqi Gu and Gongjun Xu},
  doi          = {10.1214/19-AOS1878},
  journal      = {The Annals of Statistics},
  number       = {4},
  pages        = {2082-2107},
  shortjournal = {Ann. Statist.},
  title        = {Partial identifiability of restricted latent class models},
  volume       = {48},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Adaptive estimation in structured factor models with
applications to overlapping clustering. <em>AOS</em>, <em>48</em>(4),
2055–2081. (<a href="https://doi.org/10.1214/19-AOS1877">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This work introduces a novel estimation method, called LOVE, of the entries and structure of a loading matrix $A$ in a latent factor model $X=AZ+E$, for an observable random vector $X\in \mathbb{R}^{p}$, with correlated unobservable factors $Z\in \mathbb{R}^{K}$, with $K$ unknown, and uncorrelated noise $E$. Each row of $A$ is scaled, and allowed to be sparse. In order to identify the loading matrix $A$, we require the existence of pure variables, which are components of $X$ that are associated, via $A$, with one and only one latent factor. Despite the fact that the number of factors $K$, the number of the pure variables and their location are all unknown, we only require a mild condition on the covariance matrix of $Z$, and a minimum of only two pure variables per latent factor to show that $A$ is uniquely defined, up to signed permutations. Our proofs for model identifiability are constructive, and lead to our novel estimation method of the number of factors and of the set of pure variables, from a sample of size $n$ of observations on $X$. This is the first step of our LOVE algorithm, which is optimization-free, and has low computational complexity of order $p^{2}$. The second step of LOVE is an easily implementable linear program that estimates $A$. We prove that the resulting estimator is near minimax rate optimal for $A$, with respect to the $\|\ \|_{\infty ,q}$ loss, for $q\geq 1$, up to logarithmic factors in $p$, and that it can be minimax-rate optimal in many cases of interest. The model structure is motivated by the problem of overlapping variable clustering, ubiquitous in data science. We define the population level clusters as groups of those components of $X$ that are associated, via the matrix $A$, with the same unobservable latent factor, and multifactor association is allowed. Clusters are respectively anchored by the pure variables, and form overlapping subgroups of the $p$-dimensional random vector $X$. The Latent model approach to OVErlapping clustering is reflected in the name of our algorithm, LOVE. The third step of LOVE estimates the clusters from the support of the columns of the estimated $A$. We guarantee cluster recovery with zero false positive proportion, and with false negative proportion control. The practical relevance of LOVE is illustrated through the analysis of a RNA-seq data set, devoted to determining the functional annotation of genes with unknown function.},
  archive      = {J_AOS},
  author       = {Xin Bing and Florentina Bunea and Yang Ning and Marten Wegkamp},
  doi          = {10.1214/19-AOS1877},
  journal      = {The Annals of Statistics},
  number       = {4},
  pages        = {2055-2081},
  shortjournal = {Ann. Statist.},
  title        = {Adaptive estimation in structured factor models with applications to overlapping clustering},
  volume       = {48},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Empirical risk minimization and complexity of dynamical
models. <em>AOS</em>, <em>48</em>(4), 2031–2054. (<a
href="https://doi.org/10.1214/19-AOS1876">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A dynamical model consists of a continuous self-map $T:\mathcal{X}\to \mathcal{X}$ of a compact state space $\mathcal{X}$ and a continuous observation function $f:\mathcal{X}\to \mathbb{R}$. This paper considers the fitting of a parametrized family of dynamical models to an observed real-valued stochastic process using empirical risk minimization. The limiting behavior of the minimum risk parameters is studied in a general setting. We establish a general convergence theorem for minimum risk estimators and ergodic observations. We then study conditions under which empirical risk minimization can effectively separate signal from noise in an additive observational noise model. The key condition in the latter results is that the family of dynamical models has limited complexity, which is quantified through a notion of entropy for families of infinite sequences that connects covering number based entropies with topological entropy studied in dynamical systems. We establish close connections between entropy and limiting average mean widths for stationary processes, and discuss several examples of dynamical models.},
  archive      = {J_AOS},
  author       = {Kevin McGoff and Andrew B. Nobel},
  doi          = {10.1214/19-AOS1876},
  journal      = {The Annals of Statistics},
  number       = {4},
  pages        = {2031-2054},
  shortjournal = {Ann. Statist.},
  title        = {Empirical risk minimization and complexity of dynamical models},
  volume       = {48},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Sharp instruments for classifying compliers and generalizing
causal effects. <em>AOS</em>, <em>48</em>(4), 2008–2030. (<a
href="https://doi.org/10.1214/19-AOS1874">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {It is well known that, without restricting treatment effect heterogeneity, instrumental variable (IV) methods only identify “local” effects among compliers, that is, those subjects who take treatment only when encouraged by the IV. Local effects are controversial since they seem to only apply to an unidentified subgroup; this has led many to denounce these effects as having little policy relevance. However, we show that such pessimism is not always warranted: it can be possible to accurately predict who compliers are, and obtain tight bounds on more generalizable effects in identifiable subgroups. We propose methods for doing so and study estimation error and asymptotic properties, showing that these tasks can sometimes be accomplished even with very weak IVs. We go on to introduce a new measure of IV quality called “sharpness,” which reflects the variation in compliance explained by covariates, and captures how well one can identify compliers and obtain tight bounds on identifiable subgroup effects. We develop an estimator of sharpness and show that it is asymptotically efficient under weak conditions. Finally, we explore finite-sample properties via simulation, and apply the methods to study canvassing effects on voter turnout. We propose that sharpness should be presented alongside strength to assess IV quality.},
  archive      = {J_AOS},
  author       = {Edward H. Kennedy and Sivaraman Balakrishnan and Max G’Sell},
  doi          = {10.1214/19-AOS1874},
  journal      = {The Annals of Statistics},
  number       = {4},
  pages        = {2008-2030},
  shortjournal = {Ann. Statist.},
  title        = {Sharp instruments for classifying compliers and generalizing causal effects},
  volume       = {48},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Optimal estimation of gaussian mixtures via denoised method
of moments. <em>AOS</em>, <em>48</em>(4), 1981–2007. (<a
href="https://doi.org/10.1214/19-AOS1873">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The method of moments (Philos. Trans. R. Soc. Lond. Ser. A 185 (1894) 71–110) is one of the most widely used methods in statistics for parameter estimation, by means of solving the system of equations that match the population and estimated moments. However, in practice and especially for the important case of mixture models, one frequently needs to contend with the difficulties of non-existence or nonuniqueness of statistically meaningful solutions, as well as the high computational cost of solving large polynomial systems. Moreover, theoretical analyses of the method of moments are mainly confined to asymptotic normality style of results established under strong assumptions. This paper considers estimating a $k$-component Gaussian location mixture with a common (possibly unknown) variance parameter. To overcome the aforementioned theoretic and algorithmic hurdles, a crucial step is to denoise the moment estimates by projecting to the truncated moment space (via semidefinite programming) before solving the method of moments equations. Not only does this regularization ensure existence and uniqueness of solutions, it also yields fast solvers by means of Gauss quadrature. Furthermore, by proving new moment comparison theorems in the Wasserstein distance via polynomial interpolation and majorization techniques, we establish the statistical guarantees and adaptive optimality of the proposed procedure, as well as oracle inequality in misspecified models. These results can also be viewed as provable algorithms for generalized method of moments (Econometrica 50 (1982) 1029–1054) which involves nonconvex optimization and lacks theoretical guarantees.},
  archive      = {J_AOS},
  author       = {Yihong Wu and Pengkun Yang},
  doi          = {10.1214/19-AOS1873},
  journal      = {The Annals of Statistics},
  number       = {4},
  pages        = {1981-2007},
  shortjournal = {Ann. Statist.},
  title        = {Optimal estimation of gaussian mixtures via denoised method of moments},
  volume       = {48},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Model selection for high-dimensional linear regression with
dependent observations. <em>AOS</em>, <em>48</em>(4), 1959–1980. (<a
href="https://doi.org/10.1214/19-AOS1872">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We investigate the prediction capability of the orthogonal greedy algorithm (OGA) in high-dimensional regression models with dependent observations. The rates of convergence of the prediction error of OGA are obtained under a variety of sparsity conditions. To prevent OGA from overfitting, we introduce a high-dimensional Akaike’s information criterion (HDAIC) to determine the number of OGA iterations. A key contribution of this work is to show that OGA, used in conjunction with HDAIC, can achieve the optimal convergence rate without knowledge of how sparse the underlying high-dimensional model is.},
  archive      = {J_AOS},
  author       = {Ching-Kang Ing},
  doi          = {10.1214/19-AOS1872},
  journal      = {The Annals of Statistics},
  number       = {4},
  pages        = {1959-1980},
  shortjournal = {Ann. Statist.},
  title        = {Model selection for high-dimensional linear regression with dependent observations},
  volume       = {48},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). On the validity of the formal edgeworth expansion for
posterior densities. <em>AOS</em>, <em>48</em>(4), 1940–1958. (<a
href="https://doi.org/10.1214/19-AOS1871">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider a fundamental open problem in parametric Bayesian theory, namely the validity of the formal Edgeworth expansion of the posterior density. While the study of valid asymptotic expansions for posterior distributions constitutes a rich literature, the validity of the formal Edgeworth expansion has not been rigorously established. Several authors have claimed connections of various posterior expansions with the classical Edgeworth expansion, or have simply assumed its validity. Our main result settles this open problem. We also prove a lemma concerning the order of posterior cumulants which is of independent interest in Bayesian parametric theory. The most relevant literature is synthesized and compared to the newly-derived Edgeworth expansions. Numerical investigations illustrate that our expansion has the behavior expected of an Edgeworth expansion, and that it has better performance than the other existing expansion which was previously claimed to be of Edgeworth type.},
  archive      = {J_AOS},
  author       = {John E. Kolassa and Todd A. Kuffner},
  doi          = {10.1214/19-AOS1871},
  journal      = {The Annals of Statistics},
  number       = {4},
  pages        = {1940-1958},
  shortjournal = {Ann. Statist.},
  title        = {On the validity of the formal edgeworth expansion for posterior densities},
  volume       = {48},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Nonclassical berry–esseen inequalities and accuracy of the
bootstrap. <em>AOS</em>, <em>48</em>(4), 1922–1939. (<a
href="https://doi.org/10.1214/18-AOS1802">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study accuracy of bootstrap procedures for estimation of quantiles of a smooth function of a sum of independent sub-Gaussian random vectors. We establish higher-order approximation bounds with error terms depending on a sample size and a dimension explicitly. These results lead to improvements of accuracy of a weighted bootstrap procedure for general log-likelihood ratio statistics. The key element of our proofs of the bootstrap accuracy is a multivariate higher-order Berry–Esseen inequality. We consider a problem of approximation of distributions of two sums of zero mean independent random vectors, such that summands with the same indices have equal moments up to at least the second order. The derived approximation bound is uniform on the sets of all Euclidean balls. The presented approach extends classical Berry–Esseen type inequalities to higher-order approximation bounds. The theoretical results are illustrated with numerical experiments.},
  archive      = {J_AOS},
  author       = {Mayya Zhilova},
  doi          = {10.1214/18-AOS1802},
  journal      = {The Annals of Statistics},
  number       = {4},
  pages        = {1922-1939},
  shortjournal = {Ann. Statist.},
  title        = {Nonclassical Berry–Esseen inequalities and accuracy of the bootstrap},
  volume       = {48},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020b). Rejoinder: “Nonparametric regression using deep neural
networks with ReLU activation function.” <em>AOS</em>, <em>48</em>(4),
1916–1921. (<a href="https://doi.org/10.1214/19-AOS1931">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_AOS},
  author       = {Johannes Schmidt-Hieber},
  doi          = {10.1214/19-AOS1931},
  journal      = {The Annals of Statistics},
  number       = {4},
  pages        = {1916-1921},
  shortjournal = {Ann. Statist.},
  title        = {Rejoinder: “Nonparametric regression using deep neural networks with ReLU activation function”},
  volume       = {48},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Discussion of: “Nonparametric regression using deep neural
networks with ReLU activation function.” <em>AOS</em>, <em>48</em>(4),
1911–1915. (<a href="https://doi.org/10.1214/19-AOS1915">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_AOS},
  author       = {Ohad Shamir},
  doi          = {10.1214/19-AOS1915},
  journal      = {The Annals of Statistics},
  number       = {4},
  pages        = {1911-1915},
  shortjournal = {Ann. Statist.},
  title        = {Discussion of: “Nonparametric regression using deep neural networks with ReLU activation function”},
  volume       = {48},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Discussion of: “Nonparametric regression using deep neural
networks with ReLU activation function.” <em>AOS</em>, <em>48</em>(4),
1906–1910. (<a href="https://doi.org/10.1214/19-AOS1912">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_AOS},
  author       = {Michael Kohler and Sophie Langer},
  doi          = {10.1214/19-AOS1912},
  journal      = {The Annals of Statistics},
  number       = {4},
  pages        = {1906-1910},
  shortjournal = {Ann. Statist.},
  title        = {Discussion of: “Nonparametric regression using deep neural networks with ReLU activation function”},
  volume       = {48},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Discussion of: “Nonparametric regression using deep neural
networks with ReLU activation function.” <em>AOS</em>, <em>48</em>(4),
1902–1905. (<a href="https://doi.org/10.1214/19-AOS1911">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {I would like to congratulate Johannes Schmidt–Hieber on a very interesting paper in which he considers regression functions belonging to the class of so-called compositional functions and analyzes the ability of estimators based on the multivariate nonparametric regression model of deep neural networks to achieve minimax rates of convergence. In my discussion, I will first regard such a type of result from the general viewpoint of the theoretical foundations of deep neural networks. This will be followed by a discussion from the viewpoint of expressivity, optimization and generalization. Finally, I will consider some specific aspects of the main result.},
  archive      = {J_AOS},
  author       = {Gitta Kutyniok},
  doi          = {10.1214/19-AOS1911},
  journal      = {The Annals of Statistics},
  number       = {4},
  pages        = {1902-1905},
  shortjournal = {Ann. Statist.},
  title        = {Discussion of: “Nonparametric regression using deep neural networks with ReLU activation function”},
  volume       = {48},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Discussion of: “Nonparametric regression using deep neural
networks with ReLU activation function.” <em>AOS</em>, <em>48</em>(4),
1898–1901. (<a href="https://doi.org/10.1214/19-AOS1910">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_AOS},
  author       = {Behrooz Ghorbani and Song Mei and Theodor Misiakiewicz and Andrea Montanari},
  doi          = {10.1214/19-AOS1910},
  journal      = {The Annals of Statistics},
  number       = {4},
  pages        = {1898-1901},
  shortjournal = {Ann. Statist.},
  title        = {Discussion of: “Nonparametric regression using deep neural networks with ReLU activation function”},
  volume       = {48},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020a). Nonparametric regression using deep neural networks with
ReLU activation function. <em>AOS</em>, <em>48</em>(4), 1875–1897. (<a
href="https://doi.org/10.1214/19-AOS1875">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Consider the multivariate nonparametric regression model. It is shown that estimators based on sparsely connected deep neural networks with ReLU activation function and properly chosen network architecture achieve the minimax rates of convergence (up to $\log n$-factors) under a general composition assumption on the regression function. The framework includes many well-studied structural constraints such as (generalized) additive models. While there is a lot of flexibility in the network architecture, the tuning parameter is the sparsity of the network. Specifically, we consider large networks with number of potential network parameters exceeding the sample size. The analysis gives some insights into why multilayer feedforward neural networks perform well in practice. Interestingly, for ReLU activation function the depth (number of layers) of the neural network architectures plays an important role, and our theory suggests that for nonparametric regression, scaling the network depth with the sample size is natural. It is also shown that under the composition assumption wavelet estimators can only achieve suboptimal rates.},
  archive      = {J_AOS},
  author       = {Johannes Schmidt-Hieber},
  doi          = {10.1214/19-AOS1875},
  journal      = {The Annals of Statistics},
  number       = {4},
  pages        = {1875-1897},
  shortjournal = {Ann. Statist.},
  title        = {Nonparametric regression using deep neural networks with ReLU activation function},
  volume       = {48},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). GRID: A variable selection and structure discovery method
for high dimensional nonparametric regression. <em>AOS</em>,
<em>48</em>(3), 1848–1874. (<a
href="https://doi.org/10.1214/19-AOS1846">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider nonparametric regression in high dimensions where only a relatively small subset of a large number of variables are relevant and may have nonlinear effects on the response. We develop methods for variable selection, structure discovery and estimation of the true low-dimensional regression function, allowing any degree of interactions among the relevant variables that need not be specified a-priori. The proposed method, called the GRID, combines empirical likelihood based marginal testing with the local linear estimation machinery in a novel way to select the relevant variables. Further, it provides a simple graphical tool for identifying the low dimensional nonlinear structure of the regression function. Theoretical results establish consistency of variable selection and structure discovery, and also Oracle risk property of the GRID estimator of the regression function, allowing the dimension $d$ of the covariates to grow with the sample size $n$ at the rate $d=O(n^{a})$ for any $a\in(0,\infty)$ and the number of relevant covariates $r$ to grow at a rate $r=O(n^{\gamma})$ for some $\gamma\in(0,1)$ under some regularity conditions that, in particular, require finiteness of certain absolute moments of the error variables depending on $a$. Finite sample properties of the GRID are investigated in a moderately large simulation study.},
  archive      = {J_AOS},
  author       = {Francesco Giordano and Soumendra Nath Lahiri and Maria Lucia Parrella},
  doi          = {10.1214/19-AOS1846},
  journal      = {The Annals of Statistics},
  number       = {3},
  pages        = {1848-1874},
  shortjournal = {Ann. Statist.},
  title        = {GRID: A variable selection and structure discovery method for high dimensional nonparametric regression},
  volume       = {48},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). An adaptable generalization of hotelling’s <span
class="math inline"><em>T</em><sup>2</sup></span> test in high
dimension. <em>AOS</em>, <em>48</em>(3), 1815–1847. (<a
href="https://doi.org/10.1214/19-AOS1869">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a two-sample test for detecting the difference between mean vectors in a high-dimensional regime based on a ridge-regularized Hotelling’s $T^{2}$. To choose the regularization parameter, a method is derived that aims at maximizing power within a class of local alternatives. We also propose a composite test that combines the optimal tests corresponding to a specific collection of local alternatives. Weak convergence of the stochastic process corresponding to the ridge-regularized Hotelling’s $T^{2}$ is established and used to derive the cut-off values of the proposed test. Large sample properties are verified for a class of sub-Gaussian distributions. Through an extensive simulation study, the composite test is shown to compare favorably against a host of existing two-sample test procedures in a wide range of settings. The performance of the proposed test procedures is illustrated through an application to a breast cancer data set where the goal is to detect the pathways with different DNA copy number alterations across breast cancer subtypes.},
  archive      = {J_AOS},
  author       = {Haoran Li and Alexander Aue and Debashis Paul and Jie Peng and Pei Wang},
  doi          = {10.1214/19-AOS1869},
  journal      = {The Annals of Statistics},
  number       = {3},
  pages        = {1815-1847},
  shortjournal = {Ann. Statist.},
  title        = {An adaptable generalization of hotelling’s $T^{2}$ test in high dimension},
  volume       = {48},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Local nearest neighbour classification with applications to
semi-supervised learning. <em>AOS</em>, <em>48</em>(3), 1789–1814. (<a
href="https://doi.org/10.1214/19-AOS1868">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We derive a new asymptotic expansion for the global excess risk of a local-$k$-nearest neighbour classifier, where the choice of $k$ may depend upon the test point. This expansion elucidates conditions under which the dominant contribution to the excess risk comes from the decision boundary of the optimal Bayes classifier, but we also show that if these conditions are not satisfied, then the dominant contribution may arise from the tails of the marginal distribution of the features. Moreover, we prove that, provided the $d$-dimensional marginal distribution of the features has a finite $\rho $th moment for some $\rho &gt;4$ (as well as other regularity conditions), a local choice of $k$ can yield a rate of convergence of the excess risk of $O(n^{-4/(d+4)})$, where $n$ is the sample size, whereas for the standard $k$-nearest neighbour classifier, our theory would require $d\geq 5$ and $\rho &gt;4d/(d-4)$ finite moments to achieve this rate. These results motivate a new $k$-nearest neighbour classifier for semi-supervised learning problems, where the unlabelled data are used to obtain an estimate of the marginal feature density, and fewer neighbours are used for classification when this density estimate is small. Our worst-case rates are complemented by a minimax lower bound, which reveals that the local, semi-supervised $k$-nearest neighbour classifier attains the minimax optimal rate over our classes for the excess risk, up to a subpolynomial factor in $n$. These theoretical improvements over the standard $k$-nearest neighbour classifier are also illustrated through a simulation study.},
  archive      = {J_AOS},
  author       = {Timothy I. Cannings and Thomas B. Berrett and Richard J. Samworth},
  doi          = {10.1214/19-AOS1868},
  journal      = {The Annals of Statistics},
  number       = {3},
  pages        = {1789-1814},
  shortjournal = {Ann. Statist.},
  title        = {Local nearest neighbour classification with applications to semi-supervised learning},
  volume       = {48},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Local uncertainty sampling for large-scale multiclass
logistic regression. <em>AOS</em>, <em>48</em>(3), 1770–1788. (<a
href="https://doi.org/10.1214/19-AOS1867">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A major challenge for building statistical models in the big data era is that the available data volume far exceeds the computational capability. A common approach for solving this problem is to employ a subsampled dataset that can be handled by available computational resources. We propose a general subsampling scheme for large-scale multiclass logistic regression and examine the variance of the resulting estimator. We show that asymptotically, the proposed method always achieves a smaller variance than that of the uniform random sampling. Moreover, when the classes are conditionally imbalanced, significant improvement over uniform sampling can be achieved. Empirical performance of the proposed method is evaluated and compared to other methods via both simulated and real-world datasets, and these results match and confirm our theoretical analysis.},
  archive      = {J_AOS},
  author       = {Lei Han and Kean Ming Tan and Ting Yang and Tong Zhang},
  doi          = {10.1214/19-AOS1867},
  journal      = {The Annals of Statistics},
  number       = {3},
  pages        = {1770-1788},
  shortjournal = {Ann. Statist.},
  title        = {Local uncertainty sampling for large-scale multiclass logistic regression},
  volume       = {48},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Statistical inference in two-sample summary-data mendelian
randomization using robust adjusted profile score. <em>AOS</em>,
<em>48</em>(3), 1742–1769. (<a
href="https://doi.org/10.1214/19-AOS1866">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mendelian randomization (MR) is a method of exploiting genetic variation to unbiasedly estimate a causal effect in presence of unmeasured confounding. MR is being widely used in epidemiology and other related areas of population science. In this paper, we study statistical inference in the increasingly popular two-sample summary-data MR design. We show a linear model for the observed associations approximately holds in a wide variety of settings when all the genetic variants satisfy the exclusion restriction assumption, or in genetic terms, when there is no pleiotropy. In this scenario, we derive a maximum profile likelihood estimator with provable consistency and asymptotic normality. However, through analyzing real datasets, we find strong evidence of both systematic and idiosyncratic pleiotropy in MR, echoing the omnigenic model of complex traits that is recently proposed in genetics. We model the systematic pleiotropy by a random effects model, where no genetic variant satisfies the exclusion restriction condition exactly. In this case, we propose a consistent and asymptotically normal estimator by adjusting the profile score. We then tackle the idiosyncratic pleiotropy by robustifying the adjusted profile score. We demonstrate the robustness and efficiency of the proposed methods using several simulated and real datasets.},
  archive      = {J_AOS},
  author       = {Qingyuan Zhao and Jingshu Wang and Gibran Hemani and Jack Bowden and Dylan S. Small},
  doi          = {10.1214/19-AOS1866},
  journal      = {The Annals of Statistics},
  number       = {3},
  pages        = {1742-1769},
  shortjournal = {Ann. Statist.},
  title        = {Statistical inference in two-sample summary-data mendelian randomization using robust adjusted profile score},
  volume       = {48},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Large sample properties of partitioning-based series
estimators. <em>AOS</em>, <em>48</em>(3), 1718–1741. (<a
href="https://doi.org/10.1214/19-AOS1865">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present large sample results for partitioning-based least squares nonparametric regression, a popular method for approximating conditional expectation functions in statistics, econometrics and machine learning. First, we obtain a general characterization of their leading asymptotic bias. Second, we establish integrated mean squared error approximations for the point estimator and propose feasible tuning parameter selection. Third, we develop pointwise inference methods based on undersmoothing and robust bias correction. Fourth, employing different coupling approaches, we develop uniform distributional approximations for the undersmoothed and robust bias-corrected $t$-statistic processes and construct valid confidence bands. In the univariate case, our uniform distributional approximations require seemingly minimal rate restrictions and improve on approximation rates known in the literature. Finally, we apply our general results to three partitioning-based estimators: splines, wavelets and piecewise polynomials. The Supplemental Appendix includes several other general and example-specific technical and methodological results. A companion $\mathsf{R}$ package is provided.},
  archive      = {J_AOS},
  author       = {Matias D. Cattaneo and Max H. Farrell and Yingjie Feng},
  doi          = {10.1214/19-AOS1865},
  journal      = {The Annals of Statistics},
  number       = {3},
  pages        = {1718-1741},
  shortjournal = {Ann. Statist.},
  title        = {Large sample properties of partitioning-based series estimators},
  volume       = {48},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). On the optimal reconstruction of partially observed
functional data. <em>AOS</em>, <em>48</em>(3), 1692–1717. (<a
href="https://doi.org/10.1214/19-AOS1864">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a new reconstruction operator that aims to recover the missing parts of a function given the observed parts. This new operator belongs to a new, very large class of functional operators which includes the classical regression operators as a special case. We show the optimality of our reconstruction operator and demonstrate that the usually considered regression operators generally cannot be optimal reconstruction operators. Our estimation theory allows for autocorrelated functional data and considers the practically relevant situation in which each of the $n$ functions is observed at $m_{i}$, $i=1,\dots ,n$, discretization points. We derive rates of consistency for our nonparametric estimation procedures using a double asymptotic. For data situations, as in our real data application where $m_{i}$ is considerably smaller than $n$, we show that our functional principal components based estimator can provide better rates of convergence than conventional nonparametric smoothing methods.},
  archive      = {J_AOS},
  author       = {Alois Kneip and Dominik Liebl},
  doi          = {10.1214/19-AOS1864},
  journal      = {The Annals of Statistics},
  number       = {3},
  pages        = {1692-1717},
  shortjournal = {Ann. Statist.},
  title        = {On the optimal reconstruction of partially observed functional data},
  volume       = {48},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Robust inference via multiplier bootstrap. <em>AOS</em>,
<em>48</em>(3), 1665–1691. (<a
href="https://doi.org/10.1214/19-AOS1863">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper investigates the theoretical underpinnings of two fundamental statistical inference problems, the construction of confidence sets and large-scale simultaneous hypothesis testing, in the presence of heavy-tailed data. With heavy-tailed observation noise, finite sample properties of the least squares-based methods, typified by the sample mean, are suboptimal both theoretically and empirically. In this paper, we demonstrate that the adaptive Huber regression, integrated with the multiplier bootstrap procedure, provides a useful robust alternative to the method of least squares. Our theoretical and empirical results reveal the effectiveness of the proposed method, and highlight the importance of having inference methods that are robust to heavy tailedness.},
  archive      = {J_AOS},
  author       = {Xi Chen and Wen-Xin Zhou},
  doi          = {10.1214/19-AOS1863},
  journal      = {The Annals of Statistics},
  number       = {3},
  pages        = {1665-1691},
  shortjournal = {Ann. Statist.},
  title        = {Robust inference via multiplier bootstrap},
  volume       = {48},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Robust covariance estimation under <span
class="math inline"><em>L</em><sub>4</sub> − <em>l</em><sub>2</sub></span>
norm equivalence. <em>AOS</em>, <em>48</em>(3), 1648–1664. (<a
href="https://doi.org/10.1214/19-AOS1862">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Let $X$ be a centered random vector taking values in $\mathbb{R}^{d}$ and let $\Sigma=\mathbb{E}(X\otimes X)$ be its covariance matrix. We show that if $X$ satisfies an $L_{4}-L_{2}$ norm equivalence (sometimes referred to as the bounded kurtosis assumption), there is a covariance estimator $\hat{\Sigma}$ that exhibits almost the same performance one would expect had $X$ been a Gaussian vector. The procedure also improves the current state-of-the-art regarding high probability bounds in the sub-Gaussian case (sharp results were only known in expectation or with constant probability). In both scenarios the new bounds do not depend explicitly on the dimension $d$, but rather on the effective rank of the covariance matrix $\Sigma$.},
  archive      = {J_AOS},
  author       = {Shahar Mendelson and Nikita Zhivotovskiy},
  doi          = {10.1214/19-AOS1862},
  journal      = {The Annals of Statistics},
  number       = {3},
  pages        = {1648-1664},
  shortjournal = {Ann. Statist.},
  title        = {Robust covariance estimation under $L_{4}-l_{2}$ norm equivalence},
  volume       = {48},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Segmentation and estimation of change-point models: False
positive control and confidence regions. <em>AOS</em>, <em>48</em>(3),
1615–1647. (<a href="https://doi.org/10.1214/19-AOS1861">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To segment a sequence of independent random variables at an unknown number of change-points, we introduce new procedures that are based on thresholding the likelihood ratio statistic, and give approximations for the probability of a false positive error when there are no change-points. We also study confidence regions based on the likelihood ratio statistic for the change-points and joint confidence regions for the change-points and the parameter values. Applications to segment array CGH data are discussed.},
  archive      = {J_AOS},
  author       = {Xiao Fang and Jian Li and David Siegmund},
  doi          = {10.1214/19-AOS1861},
  journal      = {The Annals of Statistics},
  number       = {3},
  pages        = {1615-1647},
  shortjournal = {Ann. Statist.},
  title        = {Segmentation and estimation of change-point models: False positive control and confidence regions},
  volume       = {48},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Statistical and computational limits for sparse matrix
detection. <em>AOS</em>, <em>48</em>(3), 1593–1614. (<a
href="https://doi.org/10.1214/19-AOS1860">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper investigates the fundamental limits for detecting a high-dimensional sparse matrix contaminated by white Gaussian noise from both the statistical and computational perspectives. We consider $p\times p$ matrices whose rows and columns are individually $k$-sparse. We provide a tight characterization of the statistical and computational limits for sparse matrix detection, which precisely describe when achieving optimal detection is easy, hard or impossible, respectively. Although the sparse matrices considered in this paper have no apparent submatrix structure and the corresponding estimation problem has no computational issue at all, the detection problem has a surprising computational barrier when the sparsity level $k$ exceeds the cubic root of the matrix size $p$: attaining the optimal detection boundary is computationally at least as hard as solving the planted clique problem. The same statistical and computational limits also hold in the sparse covariance matrix model, where each variable is correlated with at most $k$ others. A key step in the construction of the statistically optimal test is a structural property of sparse matrices, which can be of independent interest.},
  archive      = {J_AOS},
  author       = {T. Tony Cai and Yihong Wu},
  doi          = {10.1214/19-AOS1860},
  journal      = {The Annals of Statistics},
  number       = {3},
  pages        = {1593-1614},
  shortjournal = {Ann. Statist.},
  title        = {Statistical and computational limits for sparse matrix detection},
  volume       = {48},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). On post dimension reduction statistical inference.
<em>AOS</em>, <em>48</em>(3), 1567–1592. (<a
href="https://doi.org/10.1214/19-AOS1859">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The methodologies of sufficient dimension reduction have undergone extensive developments in the past three decades. However, there has been a lack of systematic and rigorous development of post dimension reduction inference, which has seriously hindered its applications. The current common practice is to treat the estimated sufficient predictors as the true predictors and use them as the starting point of the downstream statistical inference. However, this naive inference approach would grossly overestimate the confidence level of an interval, or the power of a test, leading to the distorted results. In this paper, we develop a general and comprehensive framework of post dimension reduction inference, which can accommodate any dimension reduction method and model building method, as long as their corresponding influence functions are available. Within this general framework, we derive the influence functions and present the explicit post reduction formulas for the combinations of numerous dimension reduction and model building methods. We then develop post reduction inference methods for both confidence interval and hypothesis testing. We investigate the finite-sample performance of our procedures by simulations and a real data analysis.},
  archive      = {J_AOS},
  author       = {Kyongwon Kim and Bing Li and Zhou Yu and Lexin Li},
  doi          = {10.1214/19-AOS1859},
  journal      = {The Annals of Statistics},
  number       = {3},
  pages        = {1567-1592},
  shortjournal = {Ann. Statist.},
  title        = {On post dimension reduction statistical inference},
  volume       = {48},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Some theoretical properties of GANS. <em>AOS</em>,
<em>48</em>(3), 1539–1566. (<a
href="https://doi.org/10.1214/19-AOS1858">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Generative Adversarial Networks (GANs) are a class of generative algorithms that have been shown to produce state-of-the-art samples, especially in the domain of image creation. The fundamental principle of GANs is to approximate the unknown distribution of a given data set by optimizing an objective function through an adversarial game between a family of generators and a family of discriminators. In this paper, we offer a better theoretical understanding of GANs by analyzing some of their mathematical and statistical properties. We study the deep connection between the adversarial principle underlying GANs and the Jensen–Shannon divergence, together with some optimality characteristics of the problem. An analysis of the role of the discriminator family via approximation arguments is also provided. In addition, taking a statistical point of view, we study the large sample properties of the estimated distribution and prove in particular a central limit theorem. Some of our results are illustrated with simulated examples.},
  archive      = {J_AOS},
  author       = {Gérard Biau and Benoît Cadre and Maxime Sangnier and Ugo Tanielian},
  doi          = {10.1214/19-AOS1858},
  journal      = {The Annals of Statistics},
  number       = {3},
  pages        = {1539-1566},
  shortjournal = {Ann. Statist.},
  title        = {Some theoretical properties of GANS},
  volume       = {48},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). The hardness of conditional independence testing and the
generalised covariance measure. <em>AOS</em>, <em>48</em>(3), 1514–1538.
(<a href="https://doi.org/10.1214/19-AOS1857">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {It is a common saying that testing for conditional independence, that is, testing whether whether two random vectors $X$ and $Y$ are independent, given $Z$, is a hard statistical problem if $Z$ is a continuous random variable (or vector). In this paper, we prove that conditional independence is indeed a particularly difficult hypothesis to test for. Valid statistical tests are required to have a size that is smaller than a pre-defined significance level, and different tests usually have power against a different class of alternatives. We prove that a valid test for conditional independence does not have power against any alternative. Given the nonexistence of a uniformly valid conditional independence test, we argue that tests must be designed so their suitability for a particular problem may be judged easily. To address this need, we propose in the case where $X$ and $Y$ are univariate to nonlinearly regress $X$ on $Z$, and $Y$ on $Z$ and then compute a test statistic based on the sample covariance between the residuals, which we call the generalised covariance measure (GCM). We prove that validity of this form of test relies almost entirely on the weak requirement that the regression procedures are able to estimate the conditional means $X$ given $Z$, and $Y$ given $Z$, at a slow rate. We extend the methodology to handle settings where $X$ and $Y$ may be multivariate or even high dimensional. While our general procedure can be tailored to the setting at hand by combining it with any regression technique, we develop the theoretical guarantees for kernel ridge regression. A simulation study shows that the test based on GCM is competitive with state of the art conditional independence tests. Code is available as the R package $\mathtt{GeneralisedCovarianceMeasure}$ on CRAN.},
  archive      = {J_AOS},
  author       = {Rajen D. Shah and Jonas Peters},
  doi          = {10.1214/19-AOS1857},
  journal      = {The Annals of Statistics},
  number       = {3},
  pages        = {1514-1538},
  shortjournal = {Ann. Statist.},
  title        = {The hardness of conditional independence testing and the generalised covariance measure},
  volume       = {48},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Robust and rate-optimal gibbs posterior inference on the
boundary of a noisy image. <em>AOS</em>, <em>48</em>(3), 1498–1513. (<a
href="https://doi.org/10.1214/19-AOS1856">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Detection of an image boundary when the pixel intensities are measured with noise is an important problem in image segmentation. From a statistical point of view, a challenge is that likelihood-based methods require modeling the pixel intensities inside and outside the image boundary, even though these distributions are typically not of interest. Since misspecification of the pixel intensity distributions can negatively affect inference on the image boundary, it would be desirable to avoid this modeling step altogether. Toward this, we develop a robust Gibbsian approach that constructs a posterior distribution for the image boundary directly, without modeling the pixel intensities. We prove that the Gibbs posterior concentrates asymptotically at the minimax optimal rate, adaptive to the boundary smoothness. Monte Carlo computation of the Gibbs posterior is straightforward, and simulation results show that the corresponding inference is more accurate than that based on existing Bayesian methodology.},
  archive      = {J_AOS},
  author       = {Nicholas Syring and Ryan Martin},
  doi          = {10.1214/19-AOS1856},
  journal      = {The Annals of Statistics},
  number       = {3},
  pages        = {1498-1513},
  shortjournal = {Ann. Statist.},
  title        = {Robust and rate-optimal gibbs posterior inference on the boundary of a noisy image},
  volume       = {48},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Concentration of tempered posteriors and of their
variational approximations. <em>AOS</em>, <em>48</em>(3), 1475–1497. (<a
href="https://doi.org/10.1214/19-AOS1855">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {While Bayesian methods are extremely popular in statistics and machine learning, their application to massive data sets is often challenging, when possible at all. The classical MCMC algorithms are prohibitively slow when both the model dimension and the sample size are large. Variational Bayesian methods aim at approximating the posterior by a distribution in a tractable family $\mathcal{F}$. Thus, MCMC are replaced by an optimization algorithm which is orders of magnitude faster. VB methods have been applied in such computationally demanding applications as collaborative filtering, image and video processing or NLP to name a few. However, despite nice results in practice, the theoretical properties of these approximations are not known. We propose a general oracle inequality that relates the quality of the VB approximation to the prior $\pi $ and to the structure of $\mathcal{F}$. We provide a simple condition that allows to derive rates of convergence from this oracle inequality. We apply our theory to various examples. First, we show that for parametric models with log-Lipschitz likelihood, Gaussian VB leads to efficient algorithms and consistent estimators. We then study a high-dimensional example: matrix completion, and a nonparametric example: density estimation.},
  archive      = {J_AOS},
  author       = {Pierre Alquier and James Ridgway},
  doi          = {10.1214/19-AOS1855},
  journal      = {The Annals of Statistics},
  number       = {3},
  pages        = {1475-1497},
  shortjournal = {Ann. Statist.},
  title        = {Concentration of tempered posteriors and of their variational approximations},
  volume       = {48},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Entrywise eigenvector analysis of random matrices with low
expected rank. <em>AOS</em>, <em>48</em>(3), 1452–1474. (<a
href="https://doi.org/10.1214/19-AOS1854">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recovering low-rank structures via eigenvector perturbation analysis is a common problem in statistical machine learning, such as in factor analysis, community detection, ranking, matrix completion, among others. While a large variety of bounds are available for average errors between empirical and population statistics of eigenvectors, few results are tight for entrywise analyses, which are critical for a number of problems such as community detection. This paper investigates entrywise behaviors of eigenvectors for a large class of random matrices whose expectations are low rank, which helps settle the conjecture in Abbe, Bandeira and Hall (2014) that the spectral algorithm achieves exact recovery in the stochastic block model without any trimming or cleaning steps. The key is a first-order approximation of eigenvectors under the $\ell _{\infty }$ norm: \begin{equation*}u_{k}\approx \frac{Au_{k}^{*}}{\lambda _{k}^{*}},\end{equation*} where ${u_{k}}$ and ${u_{k}^{*}}$ are eigenvectors of a random matrix $A$ and its expectation $\mathbb{E}A$, respectively. The fact that the approximation is both tight and linear in $A$ facilitates sharp comparisons between $u_{k}$ and $u_{k}^{*}$. In particular, it allows for comparing the signs of $u_{k}$ and $u_{k}^{*}$ even if $\|u_{k}-u_{k}^{*}\|_{\infty }$ is large. The results are further extended to perturbations of eigenspaces, yielding new $\ell _{\infty }$-type bounds for synchronization ($\mathbb{Z}_{2}$-spiked Wigner model) and noisy matrix completion.},
  archive      = {J_AOS},
  author       = {Emmanuel Abbe and Jianqing Fan and Kaizheng Wang and Yiqiao Zhong},
  doi          = {10.1214/19-AOS1854},
  journal      = {The Annals of Statistics},
  number       = {3},
  pages        = {1452-1474},
  shortjournal = {Ann. Statist.},
  title        = {Entrywise eigenvector analysis of random matrices with low expected rank},
  volume       = {48},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Nonparametric bayesian analysis of the compound poisson
prior for support boundary recovery. <em>AOS</em>, <em>48</em>(3),
1432–1451. (<a href="https://doi.org/10.1214/19-AOS1853">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Given data from a Poisson point process with intensity $(x,y)\mapston\mathbf{1}(f(x)\leq y)$, frequentist properties for the Bayesian reconstruction of the support boundary function $f$ are derived. We mainly study compound Poisson process priors with fixed intensity proving that the posterior contracts with nearly optimal rate for monotone support boundaries and adapts to Hölder smooth boundaries. We then derive a limiting shape result for a compound Poisson process prior and a function space with increasing parameter dimension. It is shown that the marginal posterior of the mean functional performs an automatic bias correction and contracts with a faster rate than the MLE. In this case, $(1-\alpha )$-credible sets are also asymptotic $(1-\alpha )$-confidence intervals. As a negative result, it is shown that the frequentist coverage of credible sets is lost for linear functions $f$ outside the function class.},
  archive      = {J_AOS},
  author       = {Markus Reiß and Johannes Schmidt-Hieber},
  doi          = {10.1214/19-AOS1853},
  journal      = {The Annals of Statistics},
  number       = {3},
  pages        = {1432-1451},
  shortjournal = {Ann. Statist.},
  title        = {Nonparametric bayesian analysis of the compound poisson prior for support boundary recovery},
  volume       = {48},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Robust inference with knockoffs. <em>AOS</em>,
<em>48</em>(3), 1409–1431. (<a
href="https://doi.org/10.1214/19-AOS1852">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider the variable selection problem, which seeks to identify important variables influencing a response $Y$ out of many candidate features $X_{1},\ldots ,X_{p}$. We wish to do so while offering finite-sample guarantees about the fraction of false positives—selected variables $X_{j}$ that in fact have no effect on $Y$ after the other features are known. When the number of features $p$ is large (perhaps even larger than the sample size $n$), and we have no prior knowledge regarding the type of dependence between $Y$ and $X$, the model-X knockoffs framework nonetheless allows us to select a model with a guaranteed bound on the false discovery rate, as long as the distribution of the feature vector $X=(X_{1},\dots ,X_{p})$ is exactly known. This model selection procedure operates by constructing “knockoff copies” of each of the $p$ features, which are then used as a control group to ensure that the model selection algorithm is not choosing too many irrelevant features. In this work, we study the practical setting where the distribution of $X$ can only be estimated, rather than known exactly, and the knockoff copies of the $X_{j}$’s are therefore constructed somewhat incorrectly. Our results, which are free of any modeling assumption whatsoever, show that the resulting model selection procedure incurs an inflation of the false discovery rate that is proportional to our errors in estimating the distribution of each feature $X_{j}$ conditional on the remaining features ${X_{k}:k\neq j}$. The model-X knockoffs framework is therefore robust to errors in the underlying assumptions on the distribution of $X$, making it an effective method for many practical applications, such as genome-wide association studies, where the underlying distribution on the features $X_{1},\dots ,X_{p}$ is estimated accurately but not known exactly.},
  archive      = {J_AOS},
  author       = {Rina Foygel Barber and Emmanuel J. Candès and Richard J. Samworth},
  doi          = {10.1214/19-AOS1852},
  journal      = {The Annals of Statistics},
  number       = {3},
  pages        = {1409-1431},
  shortjournal = {Ann. Statist.},
  title        = {Robust inference with knockoffs},
  volume       = {48},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Nonparametric statistical inference for drift vector fields
of multi-dimensional diffusions. <em>AOS</em>, <em>48</em>(3),
1383–1408. (<a href="https://doi.org/10.1214/19-AOS1851">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The problem of determining a periodic Lipschitz vector field $b=(b_{1},\ldots ,b_{d})$ from an observed trajectory of the solution $(X_{t}:0\le t\le T)$ of the multi-dimensional stochastic differential equation \begin{equation*}dX_{t}=b(X_{t})\,dt+dW_{t},\quad t\geq 0,\end{equation*} where $W_{t}$ is a standard $d$-dimensional Brownian motion, is considered. Convergence rates of a penalised least squares estimator, which equals the maximum a posteriori (MAP) estimate corresponding to a high-dimensional Gaussian product prior, are derived. These results are deduced from corresponding contraction rates for the associated posterior distributions. The rates obtained are optimal up to log-factors in $L^{2}$-loss in any dimension, and also for supremum norm loss when $d\le 4$. Further, when $d\le 3$, nonparametric Bernstein–von Mises theorems are proved for the posterior distributions of $b$. From this, we deduce functional central limit theorems for the implied estimators of the invariant measure $\mu _{b}$. The limiting Gaussian process distributions have a covariance structure that is asymptotically optimal from an information-theoretic point of view.},
  archive      = {J_AOS},
  author       = {Richard Nickl and Kolyan Ray},
  doi          = {10.1214/19-AOS1851},
  journal      = {The Annals of Statistics},
  number       = {3},
  pages        = {1383-1408},
  shortjournal = {Ann. Statist.},
  title        = {Nonparametric statistical inference for drift vector fields of multi-dimensional diffusions},
  volume       = {48},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Bridging the gap between constant step size stochastic
gradient descent and markov chains. <em>AOS</em>, <em>48</em>(3),
1348–1382. (<a href="https://doi.org/10.1214/19-AOS1850">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider the minimization of a strongly convex objective function given access to unbiased estimates of its gradient through stochastic gradient descent (SGD) with constant step size. While the detailed analysis was only performed for quadratic functions, we provide an explicit asymptotic expansion of the moments of the averaged SGD iterates that outlines the dependence on initial conditions, the effect of noise and the step size, as well as the lack of convergence in the general (nonquadratic) case. For this analysis we bring tools from Markov chain theory into the analysis of stochastic gradient. We then show that Richardson–Romberg extrapolation may be used to get closer to the global optimum, and we show empirical improvements of the new extrapolation scheme.},
  archive      = {J_AOS},
  author       = {Aymeric Dieuleveut and Alain Durmus and Francis Bach},
  doi          = {10.1214/19-AOS1850},
  journal      = {The Annals of Statistics},
  number       = {3},
  pages        = {1348-1382},
  shortjournal = {Ann. Statist.},
  title        = {Bridging the gap between constant step size stochastic gradient descent and markov chains},
  volume       = {48},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Just interpolate: Kernel “ridgeless” regression can
generalize. <em>AOS</em>, <em>48</em>(3), 1329–1347. (<a
href="https://doi.org/10.1214/19-AOS1849">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the absence of explicit regularization, Kernel “Ridgeless” Regression with nonlinear kernels has the potential to fit the training data perfectly. It has been observed empirically, however, that such interpolated solutions can still generalize well on test data. We isolate a phenomenon of implicit regularization for minimum-norm interpolated solutions which is due to a combination of high dimensionality of the input data, curvature of the kernel function and favorable geometric properties of the data such as an eigenvalue decay of the empirical covariance and kernel matrices. In addition to deriving a data-dependent upper bound on the out-of-sample error, we present experimental evidence suggesting that the phenomenon occurs in the MNIST dataset.},
  archive      = {J_AOS},
  author       = {Tengyuan Liang and Alexander Rakhlin},
  doi          = {10.1214/19-AOS1849},
  journal      = {The Annals of Statistics},
  number       = {3},
  pages        = {1329-1347},
  shortjournal = {Ann. Statist.},
  title        = {Just interpolate: Kernel “Ridgeless” regression can generalize},
  volume       = {48},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Distribution and correlation-free two-sample test of
high-dimensional means. <em>AOS</em>, <em>48</em>(3), 1304–1328. (<a
href="https://doi.org/10.1214/19-AOS1848">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a two-sample test for high-dimensional means that requires neither distributional nor correlational assumptions, besides some weak conditions on the moments and tail properties of the elements in the random vectors. This two-sample test based on a nontrivial extension of the one-sample central limit theorem (Ann. Probab. 45 (2017) 2309–2352) provides a practically useful procedure with rigorous theoretical guarantees on its size and power assessment. In particular, the proposed test is easy to compute and does not require the independently and identically distributed assumption, which is allowed to have different distributions and arbitrary correlation structures. Further desired features include weaker moments and tail conditions than existing methods, allowance for highly unequal sample sizes, consistent power behavior under fairly general alternative, data dimension allowed to be exponentially high under the umbrella of such general conditions. Simulated and real data examples have demonstrated favorable numerical performance over existing methods.},
  archive      = {J_AOS},
  author       = {Kaijie Xue and Fang Yao},
  doi          = {10.1214/19-AOS1848},
  journal      = {The Annals of Statistics},
  number       = {3},
  pages        = {1304-1328},
  shortjournal = {Ann. Statist.},
  title        = {Distribution and correlation-free two-sample test of high-dimensional means},
  volume       = {48},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Post hoc confidence bounds on false positives using
reference families. <em>AOS</em>, <em>48</em>(3), 1281–1303. (<a
href="https://doi.org/10.1214/19-AOS1847">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We follow a post hoc, “user-agnostic” approach to false discovery control in a large-scale multiple testing framework, as introduced by Genovese and Wasserman [J. Amer. Statist. Assoc. 101 (2006) 1408–1417], Goeman and Solari [Statist. Sci. 26 (2011) 584–597]: the statistical guarantee on the number of correct rejections must hold for any set of candidate items, possibly selected by the user after having seen the data. To this end, we introduce a novel point of view based on a family of reference rejection sets and a suitable criterion, namely the joint familywise error rate over that family (JER for short). First, we establish how to derive post hoc bounds from a given JER control and analyze some general properties of this approach. We then develop procedures for controlling the JER in the case where reference regions are $p$-value level sets. These procedures adapt to dependencies and to the unknown quantity of signal (via a step-down principle). We also show interesting connections to confidence envelopes of Meinshausen [Scand. J. Stat. 33 (2006) 227–237]; Genovese and Wasserman [J. Amer. Statist. Assoc. 101 (2006) 1408–1417], the closed testing based approach of Goeman and Solari [Statist. Sci. 26 (2011) 584–597] and to the higher criticism of Donoho and Jin [Ann. Statist. 32 (2004) 962–994]. Our theoretical statements are supported by numerical experiments.},
  archive      = {J_AOS},
  author       = {Gilles Blanchard and Pierre Neuvial and Etienne Roquain},
  doi          = {10.1214/19-AOS1847},
  journal      = {The Annals of Statistics},
  number       = {3},
  pages        = {1281-1303},
  shortjournal = {Ann. Statist.},
  title        = {Post hoc confidence bounds on false positives using reference families},
  volume       = {48},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Limiting laws for divergent spiked eigenvalues and largest
nonspiked eigenvalue of sample covariance matrices. <em>AOS</em>,
<em>48</em>(3), 1255–1280. (<a
href="https://doi.org/10.1214/18-AOS1798">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study the asymptotic distributions of the spiked eigenvalues and the largest nonspiked eigenvalue of the sample covariance matrix under a general covariance model with divergent spiked eigenvalues, while the other eigenvalues are bounded but otherwise arbitrary. The limiting normal distribution for the spiked sample eigenvalues is established. It has distinct features that the asymptotic mean relies on not only the population spikes but also the nonspikes and that the asymptotic variance in general depends on the population eigenvectors. In addition, the limiting Tracy–Widom law for the largest nonspiked sample eigenvalue is obtained. Estimation of the number of spikes and the convergence of the leading eigenvectors are also considered. The results hold even when the number of the spikes diverges. As a key technical tool, we develop a central limit theorem for a type of random quadratic forms where the random vectors and random matrices involved are dependent. This result can be of independent interest.},
  archive      = {J_AOS},
  author       = {T. Tony Cai and Xiao Han and Guangming Pan},
  doi          = {10.1214/18-AOS1798},
  journal      = {The Annals of Statistics},
  number       = {3},
  pages        = {1255-1280},
  shortjournal = {Ann. Statist.},
  title        = {Limiting laws for divergent spiked eigenvalues and largest nonspiked eigenvalue of sample covariance matrices},
  volume       = {48},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Testing in high-dimensional spiked models. <em>AOS</em>,
<em>48</em>(3), 1231–1254. (<a
href="https://doi.org/10.1214/18-AOS1697">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider the five classes of multivariate statistical problems identified by James (Ann. Math. Stat. 35 (1964) 475–501), which together cover much of classical multivariate analysis, plus a simpler limiting case, symmetric matrix denoising. Each of James’ problems involves the eigenvalues of $E^{-1}H$ where $H$ and $E$ are proportional to high-dimensional Wishart matrices. Under the null hypothesis, both Wisharts are central with identity covariance. Under the alternative, the noncentrality or the covariance parameter of $H$ has a single eigenvalue, a spike, that stands alone. When the spike is smaller than a case-specific phase transition threshold, none of the sample eigenvalues separate from the bulk, making the testing problem challenging. Using a unified strategy for the six cases, we show that the log likelihood ratio processes parameterized by the value of the subcritical spike converge to Gaussian processes with logarithmic correlation. We then derive asymptotic power envelopes for tests for the presence of a spike.},
  archive      = {J_AOS},
  author       = {Iain M. Johnstone and Alexei Onatski},
  doi          = {10.1214/18-AOS1697},
  journal      = {The Annals of Statistics},
  number       = {3},
  pages        = {1231-1254},
  shortjournal = {Ann. Statist.},
  title        = {Testing in high-dimensional spiked models},
  volume       = {48},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Bootstrapping max statistics in high dimensions:
Near-parametric rates under weak variance decay and application to
functional and multinomial data. <em>AOS</em>, <em>48</em>(2),
1214–1229. (<a href="https://doi.org/10.1214/19-AOS1844">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, bootstrap methods have drawn attention for their ability to approximate the laws of “max statistics” in high-dimensional problems. A leading example of such a statistic is the coordinatewise maximum of a sample average of $n$ random vectors in $\mathbb{R}^{p}$. Existing results for this statistic show that the bootstrap can work when $n\ll p$, and rates of approximation (in Kolmogorov distance) have been obtained with only logarithmic dependence in $p$. Nevertheless, one of the challenging aspects of this setting is that established rates tend to scale like $n^{-1/6}$ as a function of $n$. The main purpose of this paper is to demonstrate that improvement in rate is possible when extra model structure is available. Specifically, we show that if the coordinatewise variances of the observations exhibit decay, then a nearly $n^{-1/2}$ rate can be achieved, independent of $p$. Furthermore, a surprising aspect of this dimension-free rate is that it holds even when the decay is very weak. Lastly, we provide examples showing how these ideas can be applied to inference problems dealing with functional and multinomial data.},
  archive      = {J_AOS},
  author       = {Miles E. Lopes and Zhenhua Lin and Hans-Georg Müller},
  doi          = {10.1214/19-AOS1844},
  journal      = {The Annals of Statistics},
  number       = {2},
  pages        = {1214-1229},
  shortjournal = {Ann. Statist.},
  title        = {Bootstrapping max statistics in high dimensions: Near-parametric rates under weak variance decay and application to functional and multinomial data},
  volume       = {48},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Mean estimation with sub-gaussian rates in polynomial time.
<em>AOS</em>, <em>48</em>(2), 1193–1213. (<a
href="https://doi.org/10.1214/19-AOS1843">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study polynomial time algorithms for estimating the mean of a heavy-tailed multivariate random vector. We assume only that the random vector $X$ has finite mean and covariance. In this setting, the radius of confidence intervals achieved by the empirical mean are large compared to the case that $X$ is Gaussian or sub-Gaussian. We offer the first polynomial time algorithm to estimate the mean with sub-Gaussian-size confidence intervals under such mild assumptions. Our algorithm is based on a new semidefinite programming relaxation of a high-dimensional median. Previous estimators which assumed only existence of finitely many moments of $X$ either sacrifice sub-Gaussian performance or are only known to be computable via brute-force search procedures requiring time exponential in the dimension.},
  archive      = {J_AOS},
  author       = {Samuel B. Hopkins},
  doi          = {10.1214/19-AOS1843},
  journal      = {The Annals of Statistics},
  number       = {2},
  pages        = {1193-1213},
  shortjournal = {Ann. Statist.},
  title        = {Mean estimation with sub-gaussian rates in polynomial time},
  volume       = {48},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Functional data analysis in the banach space of continuous
functions. <em>AOS</em>, <em>48</em>(2), 1168–1192. (<a
href="https://doi.org/10.1214/19-AOS1842">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Functional data analysis is typically conducted within the $L^{2}$-Hilbert space framework. There is by now a fully developed statistical toolbox allowing for the principled application of the functional data machinery to real-world problems, often based on dimension reduction techniques such as functional principal component analysis. At the same time, there have recently been a number of publications that sidestep dimension reduction steps and focus on a fully functional $L^{2}$-methodology. This paper goes one step further and develops data analysis methodology for functional time series in the space of all continuous functions. The work is motivated by the fact that objects with rather different shapes may still have a small $L^{2}$-distance and are therefore identified as similar when using a $L^{2}$-metric. However, in applications it is often desirable to use metrics reflecting the visualization of the curves in the statistical analysis. The methodological contributions are focused on developing two-sample and change-point tests as well as confidence bands, as these procedures appear to be conducive to the proposed setting. Particular interest is put on relevant differences; that is, on not trying to test for exact equality, but rather for prespecified deviations under the null hypothesis. The procedures are justified through large-sample theory. To ensure practicability, nonstandard bootstrap procedures are developed and investigated addressing particular features that arise in the problem of testing relevant hypotheses. The finite sample properties are explored through a simulation study and an application to annual temperature profiles.},
  archive      = {J_AOS},
  author       = {Holger Dette and Kevin Kokot and Alexander Aue},
  doi          = {10.1214/19-AOS1842},
  journal      = {The Annals of Statistics},
  number       = {2},
  pages        = {1168-1192},
  shortjournal = {Ann. Statist.},
  title        = {Functional data analysis in the banach space of continuous functions},
  volume       = {48},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). High-frequency analysis of parabolic stochastic PDEs.
<em>AOS</em>, <em>48</em>(2), 1143–1167. (<a
href="https://doi.org/10.1214/19-AOS1841">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider the problem of estimating stochastic volatility for a class of second-order parabolic stochastic PDEs. Assuming that the solution is observed at high temporal frequency, we use limit theorems for multipower variations and related functionals to construct consistent nonparametric estimators and asymptotic confidence bounds for the integrated volatility process. As a byproduct of our analysis, we also obtain feasible estimators for the regularity of the spatial covariance function of the noise.},
  archive      = {J_AOS},
  author       = {Carsten Chong},
  doi          = {10.1214/19-AOS1841},
  journal      = {The Annals of Statistics},
  number       = {2},
  pages        = {1143-1167},
  shortjournal = {Ann. Statist.},
  title        = {High-frequency analysis of parabolic stochastic PDEs},
  volume       = {48},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Lasso guarantees for <span
class="math inline"><em>β</em></span>-mixing heavy-tailed time series.
<em>AOS</em>, <em>48</em>(2), 1124–1142. (<a
href="https://doi.org/10.1214/19-AOS1840">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many theoretical results for lasso require the samples to be i.i.d. Recent work has provided guarantees for lasso assuming that the time series is generated by a sparse Vector Autoregressive (VAR) model with Gaussian innovations. Proofs of these results rely critically on the fact that the true data generating mechanism (DGM) is a finite-order Gaussian VAR. This assumption is quite brittle: linear transformations, including selecting a subset of variables, can lead to the violation of this assumption. In order to break free from such assumptions, we derive nonasymptotic inequalities for estimation error and prediction error of lasso estimate of the best linear predictor without assuming any special parametric form of the DGM. Instead, we rely only on (strict) stationarity and geometrically decaying $\beta$-mixing coefficients to establish error bounds for lasso for sub-Weibull random vectors. The class of sub-Weibull random variables that we introduce includes sub-Gaussian and subexponential random variables but also includes random variables with tails heavier than an exponential. We also show that, for Gaussian processes, the $\beta$-mixing condition can be relaxed to summability of the $\alpha$-mixing coefficients. Our work provides an alternative proof of the consistency of lasso for sparse Gaussian VAR models. But the applicability of our results extends to non-Gaussian and nonlinear times series models as the examples we provide demonstrate.},
  archive      = {J_AOS},
  author       = {Kam Chung Wong and Zifan Li and Ambuj Tewari},
  doi          = {10.1214/19-AOS1840},
  journal      = {The Annals of Statistics},
  number       = {2},
  pages        = {1124-1142},
  shortjournal = {Ann. Statist.},
  title        = {Lasso guarantees for $\beta$-mixing heavy-tailed time series},
  volume       = {48},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Nonasymptotic upper bounds for the reconstruction error of
PCA. <em>AOS</em>, <em>48</em>(2), 1098–1123. (<a
href="https://doi.org/10.1214/19-AOS1839">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We analyse the reconstruction error of principal component analysis (PCA) and prove nonasymptotic upper bounds for the corresponding excess risk. These bounds unify and improve existing upper bounds from the literature. In particular, they give oracle inequalities under mild eigenvalue conditions. The bounds reveal that the excess risk differs significantly from usually considered subspace distances based on canonical angles. Our approach relies on the analysis of empirical spectral projectors combined with concentration inequalities for weighted empirical covariance operators and empirical eigenvalues.},
  archive      = {J_AOS},
  author       = {Markus Reiß and Martin Wahl},
  doi          = {10.1214/19-AOS1839},
  journal      = {The Annals of Statistics},
  number       = {2},
  pages        = {1098-1123},
  shortjournal = {Ann. Statist.},
  title        = {Nonasymptotic upper bounds for the reconstruction error of PCA},
  volume       = {48},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Worst-case versus average-case design for estimation from
partial pairwise comparisons. <em>AOS</em>, <em>48</em>(2), 1072–1097.
(<a href="https://doi.org/10.1214/19-AOS1838">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Pairwise comparison data arises in many domains, including tournament rankings, web search and preference elicitation. Given noisy comparisons of a fixed subset of pairs of items, we study the problem of estimating the underlying comparison probabilities under the assumption of strong stochastic transitivity (SST). We also consider the noisy sorting subclass of the SST model. We show that when the assignment of items to the topology is arbitrary, these permutation-based models, unlike their parametric counterparts, do not admit consistent estimation for most comparison topologies used in practice. We then demonstrate that consistent estimation is possible when the assignment of items to the topology is randomized, thus establishing a dichotomy between worst-case and average-case designs. We propose two computationally efficient estimators in the average-case setting and analyze their risk, showing that it depends on the comparison topology only through the degree sequence of the topology. We also provide explicit classes of graphs for which the rates achieved by these estimators are optimal. Our results are corroborated by simulations on multiple comparison topologies.},
  archive      = {J_AOS},
  author       = {Ashwin Pananjady and Cheng Mao and Vidya Muthukumar and Martin J. Wainwright and Thomas A. Courtade},
  doi          = {10.1214/19-AOS1838},
  journal      = {The Annals of Statistics},
  number       = {2},
  pages        = {1072-1097},
  shortjournal = {Ann. Statist.},
  title        = {Worst-case versus average-case design for estimation from partial pairwise comparisons},
  volume       = {48},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Admissible bayes equivariant estimation of location vectors
for spherically symmetric distributions with unknown scale.
<em>AOS</em>, <em>48</em>(2), 1052–1071. (<a
href="https://doi.org/10.1214/19-AOS1837">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper investigates estimation of the mean vector under invariant quadratic loss for a spherically symmetric location family with a residual vector with density of the form $f(x,u)=\eta ^{(p+n)/2}f(\eta {\|x-\theta \|^{2}+\|u\|^{2}})$, where $\eta $ is unknown. We show that the natural estimator $x$ is admissible for $p=1,2$. Also, for $p\geq 3$, we find classes of generalized Bayes estimators that are admissible within the class of equivariant estimators of the form ${1-\xi (x/\|u\|)}x$. In the Gaussian case, a variant of the James–Stein estimator, $[1-{(p-2)/(n+2)}/{\|x\|^{2}/\|u\|^{2}+(p-2)/(n+2)+1}]x$, which dominates the natural estimator $x$, is also admissible within this class. We also study the related regression model.},
  archive      = {J_AOS},
  author       = {Yuzo Maruyama and William E. Strawderman},
  doi          = {10.1214/19-AOS1837},
  journal      = {The Annals of Statistics},
  number       = {2},
  pages        = {1052-1071},
  shortjournal = {Ann. Statist.},
  title        = {Admissible bayes equivariant estimation of location vectors for spherically symmetric distributions with unknown scale},
  volume       = {48},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Inference for archimax copulas. <em>AOS</em>,
<em>48</em>(2), 1025–1051. (<a
href="https://doi.org/10.1214/19-AOS1836">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Archimax copula models can account for any type of asymptotic dependence between extremes and at the same time capture joint risks at medium levels. An Archimax copula is characterized by two functional parameters: the stable tail dependence function $\ell $, and the Archimedean generator $\psi $ which distorts the extreme-value dependence structure. This article develops semiparametric inference for Archimax copulas: a nonparametric estimator of $\ell $ and a moment-based estimator of $\psi $ assuming the latter belongs to a parametric family. Conditions under which $\psi $ and $\ell $ are identifiable are derived. The asymptotic behavior of the estimators is then established under broad regularity conditions; performance in small samples is assessed through a comprehensive simulation study. The Archimax copula model with the Clayton generator is then used to analyze monthly rainfall maxima at three stations in French Brittany. The model is seen to fit the data very well, both in the lower and in the upper tail. The nonparametric estimator of $\ell $ reveals asymmetric extremal dependence between the stations, which reflects heavy precipitation patterns in the area. Technical proofs, simulation results and $\mathsf{R}$ code are provided in the Online Supplement.},
  archive      = {J_AOS},
  author       = {Simon Chatelain and Anne-Laure Fougères and Johanna G. Nešlehová},
  doi          = {10.1214/19-AOS1836},
  journal      = {The Annals of Statistics},
  number       = {2},
  pages        = {1025-1051},
  shortjournal = {Ann. Statist.},
  title        = {Inference for archimax copulas},
  volume       = {48},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A unified study of nonparametric inference for monotone
functions. <em>AOS</em>, <em>48</em>(2), 1001–1024. (<a
href="https://doi.org/10.1214/19-AOS1835">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The problem of nonparametric inference on a monotone function has been extensively studied in many particular cases. Estimators considered have often been of so-called Grenander type, being representable as the left derivative of the greatest convex minorant or least concave majorant of an estimator of a primitive function. In this paper, we provide general conditions for consistency and pointwise convergence in distribution of a class of generalized Grenander-type estimators of a monotone function. This broad class allows the minorization or majoratization operation to be performed on a data-dependent transformation of the domain, possibly yielding benefits in practice. Additionally, we provide simpler conditions and more concrete distributional theory in the important case that the primitive estimator and data-dependent transformation function are asymptotically linear. We use our general results in the context of various well-studied problems, and show that we readily recover classical results established separately in each case. More importantly, we show that our results allow us to tackle more challenging problems involving parameters for which the use of flexible learning strategies appears necessary. In particular, we study inference on monotone density and hazard functions using informatively right-censored data, extending the classical work on independent censoring, and on a covariate-marginalized conditional mean function, extending the classical work on monotone regression functions.},
  archive      = {J_AOS},
  author       = {Ted Westling and Marco Carone},
  doi          = {10.1214/19-AOS1835},
  journal      = {The Annals of Statistics},
  number       = {2},
  pages        = {1001-1024},
  shortjournal = {Ann. Statist.},
  title        = {A unified study of nonparametric inference for monotone functions},
  volume       = {48},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). D-optimal designs for multinomial logistic models.
<em>AOS</em>, <em>48</em>(2), 983–1000. (<a
href="https://doi.org/10.1214/19-AOS1834">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider optimal designs for general multinomial logistic models, which cover baseline-category, cumulative, adjacent-categories and continuation-ratio logit models, with proportional odds, nonproportional odds or partial proportional odds assumption. We derive the corresponding Fisher information matrices in three different forms to facilitate their calculations, determine the conditions for their positive definiteness, and search for optimal designs. We conclude that, unlike the designs for binary responses, a feasible design for a multinomial logistic model may contain less experimental settings than parameters, which is of practical significance. We also conclude that even for a minimally supported design, a uniform allocation, which is typically used in practice, is not optimal in general for a multinomial logistic model. We develop efficient algorithms for searching D-optimal designs. Using examples based on real experiments, we show that the efficiency of an experiment can be significantly improved if our designs are adopted.},
  archive      = {J_AOS},
  author       = {Xianwei Bu and Dibyen Majumdar and Jie Yang},
  doi          = {10.1214/19-AOS1834},
  journal      = {The Annals of Statistics},
  number       = {2},
  pages        = {983-1000},
  shortjournal = {Ann. Statist.},
  title        = {D-optimal designs for multinomial logistic models},
  volume       = {48},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Convergence of eigenvector empirical spectral distribution
of sample covariance matrices. <em>AOS</em>, <em>48</em>(2), 953–982.
(<a href="https://doi.org/10.1214/19-AOS1832">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The eigenvector empirical spectral distribution (VESD) is a useful tool in studying the limiting behavior of eigenvalues and eigenvectors of covariance matrices. In this paper, we study the convergence rate of the VESD of sample covariance matrices to the deformed Marčenko–Pastur (MP) distribution. Consider sample covariance matrices of the form $\Sigma ^{1/2}XX^{*}\Sigma ^{1/2}$, where $X=(x_{ij})$ is an $M\times N$ random matrix whose entries are independent random variables with mean zero and variance $N^{-1}$, and $\Sigma $ is a deterministic positive-definite matrix. We prove that the Kolmogorov distance between the expected VESD and the deformed MP distribution is bounded by $N^{-1+\epsilon }$ for any fixed $\epsilon &gt;0$, provided that the entries $\sqrt{N}x_{ij}$ have uniformly bounded 6th moments and $|N/M-1|\ge \tau $ for some constant $\tau &gt;0$. This result improves the previous one obtained in (Ann. Statist. 41 (2013) 2572–2607), which gave the convergence rate $O(N^{-1/2})$ assuming i.i.d. $X$ entries, bounded 10th moment, $\Sigma =I$ and $M0$, which improves the previous bound $N^{-1/4+\epsilon }$ in (Ann. Statist. 41 (2013) 2572–2607).},
  archive      = {J_AOS},
  author       = {Haokai Xi and Fan Yang and Jun Yin},
  doi          = {10.1214/19-AOS1832},
  journal      = {The Annals of Statistics},
  number       = {2},
  pages        = {953-982},
  shortjournal = {Ann. Statist.},
  title        = {Convergence of eigenvector empirical spectral distribution of sample covariance matrices},
  volume       = {48},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Consistent maximum likelihood estimation using subsets with
applications to multivariate mixed models. <em>AOS</em>, <em>48</em>(2),
932–952. (<a href="https://doi.org/10.1214/19-AOS1830">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present new results for consistency of maximum likelihood estimators with a focus on multivariate mixed models. Our theory builds on the idea of using subsets of the full data to establish consistency of estimators based on the full data. It requires neither that the data consist of independent observations, nor that the observations can be modeled as a stationary stochastic process. Compared to existing asymptotic theory using the idea of subsets, we substantially weaken the assumptions, bringing them closer to what suffices in classical settings. We apply our theory in two multivariate mixed models for which it was unknown whether maximum likelihood estimators are consistent. The models we consider have nonstochastic predictors and multivariate responses which are possibly mixed-type (some discrete and some continuous).},
  archive      = {J_AOS},
  author       = {Karl Oskar Ekvall and Galin L. Jones},
  doi          = {10.1214/19-AOS1830},
  journal      = {The Annals of Statistics},
  number       = {2},
  pages        = {932-952},
  shortjournal = {Ann. Statist.},
  title        = {Consistent maximum likelihood estimation using subsets with applications to multivariate mixed models},
  volume       = {48},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Robust machine learning by median-of-means: Theory and
practice. <em>AOS</em>, <em>48</em>(2), 906–931. (<a
href="https://doi.org/10.1214/19-AOS1828">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Median-of-means (MOM) based procedures have been recently introduced in learning theory (Lugosi and Mendelson (2019); Lecué and Lerasle (2017)). These estimators outperform classical least-squares estimators when data are heavy-tailed and/or are corrupted. None of these procedures can be implemented, which is the major issue of current MOM procedures (Ann. Statist. 47 (2019) 783–794). In this paper, we introduce minmax MOM estimators and show that they achieve the same sub-Gaussian deviation bounds as the alternatives (Lugosi and Mendelson (2019); Lecué and Lerasle (2017)), both in small and high-dimensional statistics. In particular, these estimators are efficient under moments assumptions on data that may have been corrupted by a few outliers. Besides these theoretical guarantees, the definition of minmax MOM estimators suggests simple and systematic modifications of standard algorithms used to approximate least-squares estimators and their regularized versions. As a proof of concept, we perform an extensive simulation study of these algorithms for robust versions of the LASSO.},
  archive      = {J_AOS},
  author       = {Guillaume Lecué and Matthieu Lerasle},
  doi          = {10.1214/19-AOS1828},
  journal      = {The Annals of Statistics},
  number       = {2},
  pages        = {906-931},
  shortjournal = {Ann. Statist.},
  title        = {Robust machine learning by median-of-means: Theory and practice},
  volume       = {48},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). <span class="math inline"><em>α</em></span>-variational
inference with statistical guarantees. <em>AOS</em>, <em>48</em>(2),
886–905. (<a href="https://doi.org/10.1214/19-AOS1827">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We provide statistical guarantees for a family of variational approximations to Bayesian posterior distributions, called $\alpha $-VB, which has close connections with variational approximations of tempered posteriors in the literature. The standard variational approximation is a special case of $\alpha $-VB with $\alpha =1$. When $\alpha \in (0,1]$, a novel class of variational inequalities are developed for linking the Bayes risk under the variational approximation to the objective function in the variational optimization problem, implying that maximizing the evidence lower bound in variational inference has the effect of minimizing the Bayes risk within the variational density family. Operating in a frequentist setup, the variational inequalities imply that point estimates constructed from the $\alpha $-VB procedure converge at an optimal rate to the true parameter in a wide range of problems. We illustrate our general theory with a number of examples, including the mean-field variational approximation to (low)-high-dimensional Bayesian linear regression with spike and slab priors, Gaussian mixture models and latent Dirichlet allocation.},
  archive      = {J_AOS},
  author       = {Yun Yang and Debdeep Pati and Anirban Bhattacharya},
  doi          = {10.1214/19-AOS1827},
  journal      = {The Annals of Statistics},
  number       = {2},
  pages        = {886-905},
  shortjournal = {Ann. Statist.},
  title        = {$\alpha $-variational inference with statistical guarantees},
  volume       = {48},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Fundamental limits of detection in the spiked wigner model.
<em>AOS</em>, <em>48</em>(2), 863–885. (<a
href="https://doi.org/10.1214/19-AOS1826">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study the fundamental limits of detecting the presence of an additive rank-one perturbation, or spike, to a Wigner matrix. When the spike comes from a prior that is i.i.d. across coordinates, we prove that the log-likelihood ratio of the spiked model against the nonspiked one is asymptotically normal below a certain reconstruction threshold which is not necessarily of a “spectral” nature, and that it is degenerate above. This establishes the maximal region of contiguity between the planted and null models. It is known that this threshold also marks a phase transition for estimating the spike: the latter task is possible above the threshold and impossible below. Therefore, both estimation and detection undergo the same transition in this random matrix model. Further information on the performance of the optimal test is also provided. Our proofs are based on Gaussian interpolation methods and a rigorous incarnation of the cavity method, as devised by Guerra and Talagrand in their study of the Sherrington–Kirkpatrick spin-glass model.},
  archive      = {J_AOS},
  author       = {Ahmed El Alaoui and Florent Krzakala and Michael Jordan},
  doi          = {10.1214/19-AOS1826},
  journal      = {The Annals of Statistics},
  number       = {2},
  pages        = {863-885},
  shortjournal = {Ann. Statist.},
  title        = {Fundamental limits of detection in the spiked wigner model},
  volume       = {48},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Hurst function estimation. <em>AOS</em>, <em>48</em>(2),
838–862. (<a href="https://doi.org/10.1214/19-AOS1825">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper considers a wide range of issues concerning the estimation of the Hurst function of a multifractional Brownian motion when the process is observed on a regular grid. A theoretical lower bound for the minimax risk of this inference problem is established for a wide class of smooth Hurst functions. We also propose a new nonparametric estimator and show that it is rate optimal. Implementation issues of the estimator including how to overcome the presence of a nuisance parameter and choose the tuning parameter from data will be considered. An extensive numerical study is conducted to compare our approach with other approaches.},
  archive      = {J_AOS},
  author       = {Jinqi Shen and Tailen Hsing},
  doi          = {10.1214/19-AOS1825},
  journal      = {The Annals of Statistics},
  number       = {2},
  pages        = {838-862},
  shortjournal = {Ann. Statist.},
  title        = {Hurst function estimation},
  volume       = {48},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Model-assisted inference for treatment effects using
regularized calibrated estimation with high-dimensional data.
<em>AOS</em>, <em>48</em>(2), 811–837. (<a
href="https://doi.org/10.1214/19-AOS1824">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Consider the problem of estimating average treatment effects when a large number of covariates are used to adjust for possible confounding through outcome regression and propensity score models. We develop new methods and theory to obtain not only doubly robust point estimators for average treatment effects, which remain consistent if either the propensity score model or the outcome regression model is correctly specified, but also model-assisted confidence intervals, which are valid when the propensity score model is correctly specified but the outcome model may be misspecified. With a linear outcome model, the confidence intervals are doubly robust, that is, being also valid when the outcome model is correctly specified but the propensity score model may be misspecified. Our methods involve regularized calibrated estimators with Lasso penalties but carefully chosen loss functions, for fitting propensity score and outcome regression models. We provide high-dimensional analysis to establish the desired properties of our methods under comparable sparsity conditions to previous results, which give valid confidence intervals when both the propensity score and outcome models are correctly specified. We present simulation studies and an empirical application which demonstrate advantages of the proposed methods compared with related methods based on regularized maximum likelihood estimation. The methods are implemented in the R package $\mathtt{RCAL}$.},
  archive      = {J_AOS},
  author       = {Zhiqiang Tan},
  doi          = {10.1214/19-AOS1824},
  journal      = {The Annals of Statistics},
  number       = {2},
  pages        = {811-837},
  shortjournal = {Ann. Statist.},
  title        = {Model-assisted inference for treatment effects using regularized calibrated estimation with high-dimensional data},
  volume       = {48},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Joint estimation of parameters in ising model. <em>AOS</em>,
<em>48</em>(2), 785–810. (<a
href="https://doi.org/10.1214/19-AOS1822">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study joint estimation of the inverse temperature and magnetization parameters $(\beta ,B)$ of an Ising model with a nonnegative coupling matrix $A_{n}$ of size $n\times n$, given one sample from the Ising model. We give a general bound on the rate of consistency of the bi-variate pseudo-likelihood estimator. Using this, we show that estimation at rate $n^{-1/2}$ is always possible if $A_{n}$ is the adjacency matrix of a bounded degree graph. If $A_{n}$ is the scaled adjacency matrix of a graph whose average degree goes to $+\infty $, the situation is a bit more delicate. In this case, estimation at rate $n^{-1/2}$ is still possible if the graph is not regular (in an asymptotic sense). Finally, we show that consistent estimation of both parameters is impossible if the graph is Erdős–Renyi with parameter $p&gt;0$ independent of $n$, thus confirming that estimation is harder on approximately regular graphs with large degree.},
  archive      = {J_AOS},
  author       = {Promit Ghosal and Sumit Mukherjee},
  doi          = {10.1214/19-AOS1822},
  journal      = {The Annals of Statistics},
  number       = {2},
  pages        = {785-810},
  shortjournal = {Ann. Statist.},
  title        = {Joint estimation of parameters in ising model},
  volume       = {48},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Prediction error after model search. <em>AOS</em>,
<em>48</em>(2), 763–784. (<a
href="https://doi.org/10.1214/19-AOS1818">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Estimation of the prediction error of a linear estimation rule is difficult if the data analyst also uses data to select a set of variables and constructs the estimation rule using only the selected variables. In this work, we propose an asymptotically unbiased estimator for the prediction error after model search. Under some additional mild assumptions, we show that our estimator converges to the true prediction error in $L^{2}$ at the rate of $O(n^{-1/2})$, with $n$ being the number of data points. Our estimator applies to general selection procedures, not requiring analytical forms for the selection. The number of variables to select from can grow as an exponential factor of $n$, allowing applications in high-dimensional data. It also allows model misspecifications, not requiring linear underlying models. One application of our method is that it provides an estimator for the degrees of freedom for many discontinuous estimation rules like best subset selection or relaxed Lasso. Connection to Stein’s Unbiased Risk Estimator is discussed. We consider in-sample prediction errors in this work, with some extension to out-of-sample errors in low-dimensional, linear models. Examples such as best subset selection and relaxed Lasso are considered in simulations, where our estimator outperforms both $C_{p}$ and cross validation in various settings.},
  archive      = {J_AOS},
  author       = {Xiaoying Tian},
  doi          = {10.1214/19-AOS1818},
  journal      = {The Annals of Statistics},
  number       = {2},
  pages        = {763-784},
  shortjournal = {Ann. Statist.},
  title        = {Prediction error after model search},
  volume       = {48},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). On the nonparametric maximum likelihood estimator for
gaussian location mixture densities with application to gaussian
denoising. <em>AOS</em>, <em>48</em>(2), 738–762. (<a
href="https://doi.org/10.1214/19-AOS1817">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study the nonparametric maximum likelihood estimator (NPMLE) for estimating Gaussian location mixture densities in $d$-dimensions from independent observations. Unlike usual likelihood-based methods for fitting mixtures, NPMLEs are based on convex optimization. We prove finite sample results on the Hellinger accuracy of every NPMLE. Our results imply, in particular, that every NPMLE achieves near parametric risk (up to logarithmic multiplicative factors) when the true density is a discrete Gaussian mixture without any prior information on the number of mixture components. NPMLEs can naturally be used to yield empirical Bayes estimates of the oracle Bayes estimator in the Gaussian denoising problem. We prove bounds for the accuracy of the empirical Bayes estimate as an approximation to the oracle Bayes estimator. Here our results imply that the empirical Bayes estimator performs at nearly the optimal level (up to logarithmic factors) for denoising in clustering situations without any prior knowledge of the number of clusters.},
  archive      = {J_AOS},
  author       = {Sujayam Saha and Adityanand Guntuboyina},
  doi          = {10.1214/19-AOS1817},
  journal      = {The Annals of Statistics},
  number       = {2},
  pages        = {738-762},
  shortjournal = {Ann. Statist.},
  title        = {On the nonparametric maximum likelihood estimator for gaussian location mixture densities with application to gaussian denoising},
  volume       = {48},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Learning a tree-structured ising model in order to make
predictions. <em>AOS</em>, <em>48</em>(2), 713–737. (<a
href="https://doi.org/10.1214/19-AOS1808">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study the problem of learning a tree Ising model from samples such that subsequent predictions made using the model are accurate. The prediction task considered in this paper is that of predicting the values of a subset of variables given values of some other subset of variables. Virtually all previous work on graphical model learning has focused on recovering the true underlying graph. We define a distance (“small set TV” or ssTV) between distributions $P$ and $Q$ by taking the maximum, over all subsets $\mathcal{S}$ of a given size, of the total variation between the marginals of $P$ and $Q$ on $\mathcal{S}$; this distance captures the accuracy of the prediction task of interest. We derive nonasymptotic bounds on the number of samples needed to get a distribution (from the same class) with small ssTV relative to the one generating the samples. One of the main messages of this paper is that far fewer samples are needed than for recovering the underlying tree, which means that accurate predictions are possible using the wrong tree.},
  archive      = {J_AOS},
  author       = {Guy Bresler and Mina Karzand},
  doi          = {10.1214/19-AOS1808},
  journal      = {The Annals of Statistics},
  number       = {2},
  pages        = {713-737},
  shortjournal = {Ann. Statist.},
  title        = {Learning a tree-structured ising model in order to make predictions},
  volume       = {48},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Designs for estimating the treatment effect in networks with
interference. <em>AOS</em>, <em>48</em>(2), 679–712. (<a
href="https://doi.org/10.1214/18-AOS1807">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we introduce new, easily implementable designs for drawing causal inference from randomized experiments on networks with interference. Inspired by the idea of matching in observational studies, we introduce the notion of considering a treatment assignment as a “quasi-coloring” on a graph. Our idea of a perfect quasi-coloring strives to match every treated unit on a given network with a distinct control unit that has identical number of treated and control neighbors. For a wide range of interference functions encountered in applications, we show both by theory and simulations that the classical Neymanian estimator for the direct effect has desirable properties for our designs.},
  archive      = {J_AOS},
  author       = {Ravi Jagadeesan and Natesh S. Pillai and Alexander Volfovsky},
  doi          = {10.1214/18-AOS1807},
  journal      = {The Annals of Statistics},
  number       = {2},
  pages        = {679-712},
  shortjournal = {Ann. Statist.},
  title        = {Designs for estimating the treatment effect in networks with interference},
  volume       = {48},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Multidimensional multiscale scanning in exponential
families: Limit theory and statistical consequences. <em>AOS</em>,
<em>48</em>(2), 655–678. (<a
href="https://doi.org/10.1214/18-AOS1806">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider the problem of finding anomalies in a $d$-dimensional field of independent random variables ${Y_{i}}_{i\in {1,\ldots,n}^{d}}$, each distributed according to a one-dimensional natural exponential family $\mathcal{F}={F_{\theta }}_{\theta \in \Theta }$. Given some baseline parameter $\theta _{0}\in \Theta $, the field is scanned using local likelihood ratio tests to detect from a (large) given system of regions $\mathcal{R}$ those regions $R\subset {1,\ldots,n}^{d}$ with $\theta _{i}\neq \theta _{0}$ for some $i\in R$. We provide a unified methodology which controls the overall familywise error (FWER) to make a wrong detection at a given error rate. Fundamental to our method is a Gaussian approximation of the distribution of the underlying multiscale test statistic with explicit rate of convergence. From this, we obtain a weak limit theorem which can be seen as a generalized weak invariance principle to nonidentically distributed data and is of independent interest. Furthermore, we give an asymptotic expansion of the procedures power, which yields minimax optimality in case of Gaussian observations.},
  archive      = {J_AOS},
  author       = {Claudia König and Axel Munk and Frank Werner},
  doi          = {10.1214/18-AOS1806},
  journal      = {The Annals of Statistics},
  number       = {2},
  pages        = {655-678},
  shortjournal = {Ann. Statist.},
  title        = {Multidimensional multiscale scanning in exponential families: Limit theory and statistical consequences},
  volume       = {48},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). On estimation of isotonic piecewise constant signals.
<em>AOS</em>, <em>48</em>(2), 629–654. (<a
href="https://doi.org/10.1214/18-AOS1792">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Consider a sequence of real data points $X_{1},\ldots ,X_{n}$ with underlying means $\theta ^{*}_{1},\dots ,\theta ^{*}_{n}$. This paper starts from studying the setting that $\theta ^{*}_{i}$ is both piecewise constant and monotone as a function of the index $i$. For this, we establish the exact minimax rate of estimating such monotone functions, and thus give a nontrivial answer to an open problem in the shape-constrained analysis literature. The minimax rate under the loss of the sum of squared errors involves an interesting iterated logarithmic dependence on the dimension, a phenomenon that is revealed through characterizing the interplay between the isotonic shape constraint and model selection complexity. We then develop a penalized least-squares procedure for estimating the vector $\theta ^{*}=(\theta^{*}_{1},\dots ,\theta ^{*}_{n})^{\mathsf{T}}$. This estimator is shown to achieve the derived minimax rate adaptively. For the proposed estimator, we further allow the model to be misspecified and derive oracle inequalities with the optimal rates, and show there exists a computationally efficient algorithm to compute the exact solution.},
  archive      = {J_AOS},
  author       = {Chao Gao and Fang Han and Cun-Hui Zhang},
  doi          = {10.1214/18-AOS1792},
  journal      = {The Annals of Statistics},
  number       = {2},
  pages        = {629-654},
  shortjournal = {Ann. Statist.},
  title        = {On estimation of isotonic piecewise constant signals},
  volume       = {48},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Penalized generalized empirical likelihood with a diverging
number of general estimating equations for censored data. <em>AOS</em>,
<em>48</em>(1), 607–627. (<a
href="https://doi.org/10.1214/19-AOS1870">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article considers simultaneous variable selection and parameter estimation as well as hypothesis testing in censored survival models where a parametric likelihood is not available. For the problem, we utilize certain growing dimensional general estimating equations and propose a penalized generalized empirical likelihood, where the general estimating equations are constructed based on the semiparametric efficiency bound of estimation with given moment conditions. The proposed penalized generalized empirical likelihood estimators enjoy the oracle properties, and the estimator of any fixed dimensional vector of nonzero parameters achieves the semiparametric efficiency bound asymptotically. Furthermore, we show that the penalized generalized empirical likelihood ratio test statistic has an asymptotic central chi-square distribution. The conditions of local and restricted global optimality of weighted penalized generalized empirical likelihood estimators are also discussed. We present a two-layer iterative algorithm for efficient implementation, and investigate its convergence property. The performance of the proposed methods is demonstrated by extensive simulation studies, and a real data example is provided for illustration.},
  archive      = {J_AOS},
  author       = {Niansheng Tang and Xiaodong Yan and Xingqiu Zhao},
  doi          = {10.1214/19-AOS1870},
  journal      = {The Annals of Statistics},
  number       = {1},
  pages        = {607-627},
  shortjournal = {Ann. Statist.},
  title        = {Penalized generalized empirical likelihood with a diverging number of general estimating equations for censored data},
  volume       = {48},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Almost sure uniqueness of a global minimum without
convexity. <em>AOS</em>, <em>48</em>(1), 584–606. (<a
href="https://doi.org/10.1214/19-AOS1829">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper establishes the argmin of a random objective function to be unique almost surely. This paper first formulates a general result that proves almost sure uniqueness without convexity of the objective function. The general result is then applied to a variety of applications in statistics. Four applications are discussed, including uniqueness of M-estimators, both classical likelihood and penalized likelihood estimators, and two applications of the argmin theorem, threshold regression and weak identification.},
  archive      = {J_AOS},
  author       = {Gregory Cox},
  doi          = {10.1214/19-AOS1829},
  journal      = {The Annals of Statistics},
  number       = {1},
  pages        = {584-606},
  shortjournal = {Ann. Statist.},
  title        = {Almost sure uniqueness of a global minimum without convexity},
  volume       = {48},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Asymptotic genealogies of interacting particle systems with
an application to sequential monte carlo. <em>AOS</em>, <em>48</em>(1),
560–583. (<a href="https://doi.org/10.1214/19-AOS1823">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study weighted particle systems in which new generations are resampled from current particles with probabilities proportional to their weights. This covers a broad class of sequential Monte Carlo (SMC) methods, widely-used in applied statistics and cognate disciplines. We consider the genealogical tree embedded into such particle systems, and identify conditions, as well as an appropriate time-scaling, under which they converge to the Kingman $n$-coalescent in the infinite system size limit in the sense of finite-dimensional distributions. Thus, the tractable $n$-coalescent can be used to predict the shape and size of SMC genealogies, as we illustrate by characterising the limiting mean and variance of the tree height. SMC genealogies are known to be connected to algorithm performance, so that our results are likely to have applications in the design of new methods as well. Our conditions for convergence are strong, but we show by simulation that they do not appear to be necessary.},
  archive      = {J_AOS},
  author       = {Jere Koskela and Paul A. Jenkins and Adam M. Johansen and Dario Spanò},
  doi          = {10.1214/19-AOS1823},
  journal      = {The Annals of Statistics},
  number       = {1},
  pages        = {560-583},
  shortjournal = {Ann. Statist.},
  title        = {Asymptotic genealogies of interacting particle systems with an application to sequential monte carlo},
  volume       = {48},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Markov equivalence of marginalized local independence
graphs. <em>AOS</em>, <em>48</em>(1), 539–559. (<a
href="https://doi.org/10.1214/19-AOS1821">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Symmetric independence relations are often studied using graphical representations. Ancestral graphs or acyclic directed mixed graphs with $m$-separation provide classes of symmetric graphical independence models that are closed under marginalization. Asymmetric independence relations appear naturally for multivariate stochastic processes, for instance, in terms of local independence. However, no class of graphs representing such asymmetric independence relations, which is also closed under marginalization, has been developed. We develop the theory of directed mixed graphs with $\mu $-separation and show that this provides a graphical independence model class which is closed under marginalization and which generalizes previously considered graphical representations of local independence. Several graphs may encode the same set of independence relations and this means that in many cases only an equivalence class of graphs can be identified from observational data. For statistical applications, it is therefore pivotal to characterize graphs that induce the same independence relations. Our main result is that for directed mixed graphs with $\mu $-separation each equivalence class contains a maximal element which can be constructed from the independence relations alone. Moreover, we introduce the directed mixed equivalence graph as the maximal graph with dashed and solid edges. This graph encodes all information about the edges that is identifiable from the independence relations, and furthermore it can be computed efficiently from the maximal graph.},
  archive      = {J_AOS},
  author       = {Søren Wengel Mogensen and Niels Richard Hansen},
  doi          = {10.1214/19-AOS1821},
  journal      = {The Annals of Statistics},
  number       = {1},
  pages        = {539-559},
  shortjournal = {Ann. Statist.},
  title        = {Markov equivalence of marginalized local independence graphs},
  volume       = {48},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Averages of unlabeled networks: Geometric characterization
and asymptotic behavior. <em>AOS</em>, <em>48</em>(1), 514–538. (<a
href="https://doi.org/10.1214/19-AOS1820">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {It is becoming increasingly common to see large collections of network data objects, that is, data sets in which a network is viewed as a fundamental unit of observation. As a result, there is a pressing need to develop network-based analogues of even many of the most basic tools already standard for scalar and vector data. In this paper, our focus is on averages of unlabeled, undirected networks with edge weights. Specifically, we (i) characterize a certain notion of the space of all such networks, (ii) describe key topological and geometric properties of this space relevant to doing probability and statistics thereupon, and (iii) use these properties to establish the asymptotic behavior of a generalized notion of an empirical mean under sampling from a distribution supported on this space. Our results rely on a combination of tools from geometry, probability theory and statistical shape analysis. In particular, the lack of vertex labeling necessitates working with a quotient space modding out permutations of labels. This results in a nontrivial geometry for the space of unlabeled networks, which in turn is found to have important implications on the types of probabilistic and statistical results that may be obtained and the techniques needed to obtain them.},
  archive      = {J_AOS},
  author       = {Eric D. Kolaczyk and Lizhen Lin and Steven Rosenberg and Jackson Walters and Jie Xu},
  doi          = {10.1214/19-AOS1820},
  journal      = {The Annals of Statistics},
  number       = {1},
  pages        = {514-538},
  shortjournal = {Ann. Statist.},
  title        = {Averages of unlabeled networks: Geometric characterization and asymptotic behavior},
  volume       = {48},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Optimal prediction in the linearly transformed spiked model.
<em>AOS</em>, <em>48</em>(1), 491–513. (<a
href="https://doi.org/10.1214/19-AOS1819">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider the linearly transformed spiked model, where the observations $Y_{i}$ are noisy linear transforms of unobserved signals of interest $X_{i}$: \begin{equation*}Y_{i}=A_{i}X_{i}+\varepsilon_{i},\end{equation*} for $i=1,\ldots ,n$. The transform matrices $A_{i}$ are also observed. We model the unobserved signals (or regression coefficients) $X_{i}$ as vectors lying on an unknown low-dimensional space. Given only $Y_{i}$ and $A_{i}$ how should we predict or recover their values? The naive approach of performing regression for each observation separately is inaccurate due to the large noise level. Instead, we develop optimal methods for predicting $X_{i}$ by “borrowing strength” across the different samples. Our linear empirical Bayes methods scale to large datasets and rely on weak moment assumptions. We show that this model has wide-ranging applications in signal processing, deconvolution, cryo-electron microscopy, and missing data with noise. For missing data, we show in simulations that our methods are more robust to noise and to unequal sampling than well-known matrix completion methods.},
  archive      = {J_AOS},
  author       = {Edgar Dobriban and William Leeb and Amit Singer},
  doi          = {10.1214/19-AOS1819},
  journal      = {The Annals of Statistics},
  number       = {1},
  pages        = {491-513},
  shortjournal = {Ann. Statist.},
  title        = {Optimal prediction in the linearly transformed spiked model},
  volume       = {48},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Efficient estimation of linear functionals of principal
components. <em>AOS</em>, <em>48</em>(1), 464–490. (<a
href="https://doi.org/10.1214/19-AOS1816">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study principal component analysis (PCA) for mean zero i.i.d. Gaussian observations $X_{1},\dots,X_{n}$ in a separable Hilbert space $\mathbb{H}$ with unknown covariance operator $\Sigma $. The complexity of the problem is characterized by its effective rank $\mathbf{r}(\Sigma):=\frac{\operatorname{tr}(\Sigma)}{\|\Sigma \|}$, where $\mathrm{tr}(\Sigma)$ denotes the trace of $\Sigma $ and $\|\Sigma\|$ denotes its operator norm. We develop a method of bias reduction in the problem of estimation of linear functionals of eigenvectors of $\Sigma $. Under the assumption that $\mathbf{r}(\Sigma)=o(n)$, we establish the asymptotic normality and asymptotic properties of the risk of the resulting estimators and prove matching minimax lower bounds, showing their semiparametric optimality.},
  archive      = {J_AOS},
  author       = {Vladimir Koltchinskii and Matthias Löffler and Richard Nickl},
  doi          = {10.1214/19-AOS1816},
  journal      = {The Annals of Statistics},
  number       = {1},
  pages        = {464-490},
  shortjournal = {Ann. Statist.},
  title        = {Efficient estimation of linear functionals of principal components},
  volume       = {48},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Uniformly valid confidence intervals post-model-selection.
<em>AOS</em>, <em>48</em>(1), 440–463. (<a
href="https://doi.org/10.1214/19-AOS1815">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We suggest general methods to construct asymptotically uniformly valid confidence intervals post-model-selection. The constructions are based on principles recently proposed by Berk et al. (Ann. Statist. 41 (2013) 802–837). In particular, the candidate models used can be misspecified, the target of inference is model-specific, and coverage is guaranteed for any data-driven model selection procedure. After developing a general theory, we apply our methods to practically important situations where the candidate set of models, from which a working model is selected, consists of fixed design homoskedastic or heteroskedastic linear models, or of binary regression models with general link functions. In an extensive simulation study, we find that the proposed confidence intervals perform remarkably well, even when compared to existing methods that are tailored only for specific model selection procedures.},
  archive      = {J_AOS},
  author       = {François Bachoc and David Preinerstorfer and Lukas Steinberger},
  doi          = {10.1214/19-AOS1815},
  journal      = {The Annals of Statistics},
  number       = {1},
  pages        = {440-463},
  shortjournal = {Ann. Statist.},
  title        = {Uniformly valid confidence intervals post-model-selection},
  volume       = {48},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Consistent selection of the number of change-points via
sample-splitting. <em>AOS</em>, <em>48</em>(1), 413–439. (<a
href="https://doi.org/10.1214/19-AOS1814">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In multiple change-point analysis, one of the major challenges is to estimate the number of change-points. Most existing approaches attempt to minimize a Schwarz information criterion which balances a term quantifying model fit with a penalization term accounting for model complexity that increases with the number of change-points and limits overfitting. However, different penalization terms are required to adapt to different contexts of multiple change-point problems and the optimal penalization magnitude usually varies from the model and error distribution. We propose a data-driven selection criterion that is applicable to most kinds of popular change-point detection methods, including binary segmentation and optimal partitioning algorithms. The key idea is to select the number of change-points that minimizes the squared prediction error, which measures the fit of a specified model for a new sample. We develop a cross-validation estimation scheme based on an order-preserved sample-splitting strategy, and establish its asymptotic selection consistency under some mild conditions. Effectiveness of the proposed selection criterion is demonstrated on a variety of numerical experiments and real-data examples.},
  archive      = {J_AOS},
  author       = {Changliang Zou and Guanghui Wang and Runze Li},
  doi          = {10.1214/19-AOS1814},
  journal      = {The Annals of Statistics},
  number       = {1},
  pages        = {413-439},
  shortjournal = {Ann. Statist.},
  title        = {Consistent selection of the number of change-points via sample-splitting},
  volume       = {48},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). The numerical bootstrap. <em>AOS</em>, <em>48</em>(1),
397–412. (<a href="https://doi.org/10.1214/19-AOS1812">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes a numerical bootstrap method that is consistent in many cases where the standard bootstrap is known to fail and where the $m$-out-of-$n$ bootstrap and subsampling have been the most commonly used inference approaches. We provide asymptotic analysis under both fixed and drifting parameter sequences, and we compare the approximation error of the numerical bootstrap with that of the $m$-out-of-$n$ bootstrap and subsampling. Finally, we discuss applications of the numerical bootstrap, such as constrained and unconstrained M-estimators converging at both regular and nonstandard rates, Laplace-type estimators, and test statistics for partially identified models.},
  archive      = {J_AOS},
  author       = {Han Hong and Jessie Li},
  doi          = {10.1214/19-AOS1812},
  journal      = {The Annals of Statistics},
  number       = {1},
  pages        = {397-412},
  shortjournal = {Ann. Statist.},
  title        = {The numerical bootstrap},
  volume       = {48},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Concentration and consistency results for canonical and
curved exponential-family models of random graphs. <em>AOS</em>,
<em>48</em>(1), 374–396. (<a
href="https://doi.org/10.1214/19-AOS1810">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Statistical inference for exponential-family models of random graphs with dependent edges is challenging. We stress the importance of additional structure and show that additional structure facilitates statistical inference. A simple example of a random graph with additional structure is a random graph with neighborhoods and local dependence within neighborhoods. We develop the first concentration and consistency results for maximum likelihood and $M$-estimators of a wide range of canonical and curved exponential-family models of random graphs with local dependence. All results are nonasymptotic and applicable to random graphs with finite populations of nodes, although asymptotic consistency results can be obtained as well. In addition, we show that additional structure can facilitate subgraph-to-graph estimation, and present concentration results for subgraph-to-graph estimators. As an application, we consider popular curved exponential-family models of random graphs, with local dependence induced by transitivity and parameter vectors whose dimensions depend on the number of nodes.},
  archive      = {J_AOS},
  author       = {Michael Schweinberger and Jonathan Stewart},
  doi          = {10.1214/19-AOS1810},
  journal      = {The Annals of Statistics},
  number       = {1},
  pages        = {374-396},
  shortjournal = {Ann. Statist.},
  title        = {Concentration and consistency results for canonical and curved exponential-family models of random graphs},
  volume       = {48},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). The multi-armed bandit problem: An efficient nonparametric
solution. <em>AOS</em>, <em>48</em>(1), 346–373. (<a
href="https://doi.org/10.1214/19-AOS1809">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Lai and Robbins (Adv. in Appl. Math. 6 (1985) 4–22) and Lai (Ann. Statist. 15 (1987) 1091–1114) provided efficient parametric solutions to the multi-armed bandit problem, showing that arm allocation via upper confidence bounds (UCB) achieves minimum regret. These bounds are constructed from the Kullback–Leibler information of the reward distributions, estimated from specified parametric families. In recent years, there has been renewed interest in the multi-armed bandit problem due to new applications in machine learning algorithms and data analytics. Nonparametric arm allocation procedures like $\epsilon $-greedy, Boltzmann exploration and BESA were studied, and modified versions of the UCB procedure were also analyzed under nonparametric settings. However, unlike UCB these nonparametric procedures are not efficient under general parametric settings. In this paper, we propose efficient nonparametric procedures.},
  archive      = {J_AOS},
  author       = {Hock Peng Chan},
  doi          = {10.1214/19-AOS1809},
  journal      = {The Annals of Statistics},
  number       = {1},
  pages        = {346-373},
  shortjournal = {Ann. Statist.},
  title        = {The multi-armed bandit problem: An efficient nonparametric solution},
  volume       = {48},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Testing for principal component directions under weak
identifiability. <em>AOS</em>, <em>48</em>(1), 324–345. (<a
href="https://doi.org/10.1214/18-AOS1805">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider the problem of testing, on the basis of a $p$-variate Gaussian random sample, the null hypothesis $\mathcal{H}_{0}:\boldsymbol{\theta}_{1}=\boldsymbol{\theta}_{1}^{0}$ against the alternative $\mathcal{H}_{1}:\boldsymbol{\theta}_{1}\neq \boldsymbol{\theta}_{1}^{0}$, where $\boldsymbol{\theta}_{1}$ is the “first” eigenvector of the underlying covariance matrix and $\boldsymbol{\theta}_{1}^{0}$ is a fixed unit $p$-vector. In the classical setup where eigenvalues $\lambda_{1}&gt;\lambda_{2}\geq \cdots \geq \lambda_{p}$ are fixed, the Anderson (Ann. Math. Stat. 34 (1963) 122–148) likelihood ratio test (LRT) and the Hallin, Paindaveine and Verdebout (Ann. Statist. 38 (2010) 3245–3299) Le Cam optimal test for this problem are asymptotically equivalent under the null hypothesis, hence also under sequences of contiguous alternatives. We show that this equivalence does not survive asymptotic scenarios where $\lambda_{n1}/\lambda_{n2}=1+O(r_{n})$ with $r_{n}=O(1/\sqrt{n})$. For such scenarios, the Le Cam optimal test still asymptotically meets the nominal level constraint, whereas the LRT severely overrejects the null hypothesis. Consequently, the former test should be favored over the latter one whenever the two largest sample eigenvalues are close to each other. By relying on the Le Cam’s asymptotic theory of statistical experiments, we study the non-null and optimality properties of the Le Cam optimal test in the aforementioned asymptotic scenarios and show that the null robustness of this test is not obtained at the expense of power. Our asymptotic investigation is extensive in the sense that it allows $r_{n}$ to converge to zero at an arbitrary rate. While we restrict to single-spiked spectra of the form $\lambda_{n1}&gt;\lambda_{n2}=\cdots =\lambda_{np}$ to make our results as striking as possible, we extend our results to the more general elliptical case. Finally, we present an illustrative real data example.},
  archive      = {J_AOS},
  author       = {Davy Paindaveine and Julien Remy and Thomas Verdebout},
  doi          = {10.1214/18-AOS1805},
  journal      = {The Annals of Statistics},
  number       = {1},
  pages        = {324-345},
  shortjournal = {Ann. Statist.},
  title        = {Testing for principal component directions under weak identifiability},
  volume       = {48},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Sparse high-dimensional regression: Exact scalable
algorithms and phase transitions. <em>AOS</em>, <em>48</em>(1), 300–323.
(<a href="https://doi.org/10.1214/18-AOS1804">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a novel binary convex reformulation of the sparse regression problem that constitutes a new duality perspective. We devise a new cutting plane method and provide evidence that it can solve to provable optimality the sparse regression problem for sample sizes $n$ and number of regressors $p$ in the 100,000s, that is, two orders of magnitude better than the current state of the art, in seconds. The ability to solve the problem for very high dimensions allows us to observe new phase transition phenomena. Contrary to traditional complexity theory which suggests that the difficulty of a problem increases with problem size, the sparse regression problem has the property that as the number of samples $n$ increases the problem becomes easier in that the solution recovers 100\% of the true signal, and our approach solves the problem extremely fast (in fact faster than Lasso), while for small number of samples $n$, our approach takes a larger amount of time to solve the problem, but importantly the optimal solution provides a statistically more relevant regressor. We argue that our exact sparse regression approach presents a superior alternative over heuristic methods available at present.},
  archive      = {J_AOS},
  author       = {Dimitris Bertsimas and Bart Van Parys},
  doi          = {10.1214/18-AOS1804},
  journal      = {The Annals of Statistics},
  number       = {1},
  pages        = {300-323},
  shortjournal = {Ann. Statist.},
  title        = {Sparse high-dimensional regression: Exact scalable algorithms and phase transitions},
  volume       = {48},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Bootstrap confidence regions based on m-estimators under
nonstandard conditions. <em>AOS</em>, <em>48</em>(1), 274–299. (<a
href="https://doi.org/10.1214/18-AOS1803">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Suppose that a confidence region is desired for a subvector $\theta $ of a multidimensional parameter $\xi =(\theta ,\psi )$, based on an M-estimator $\hat{\xi }_{n}=(\hat{\theta }_{n},\hat{\psi }_{n})$ calculated from a random sample of size $n$. Under nonstandard conditions $\hat{\xi }_{n}$ often converges at a nonregular rate $r_{n}$, in which case consistent estimation of the distribution of $r_{n}(\hat{\theta }_{n}-\theta )$, a pivot commonly chosen for confidence region construction, is most conveniently effected by the $m$ out of $n$ bootstrap. The above choice of pivot has three drawbacks: (i) the shape of the region is either subjectively prescribed or controlled by a computationally intensive depth function; (ii) the region is not transformation equivariant; (iii) $\hat{\xi }_{n}$ may not be uniquely defined. To resolve the above difficulties, we propose a one-dimensional pivot derived from the criterion function, and prove that its distribution can be consistently estimated by the $m$ out of $n$ bootstrap, or by a modified version of the perturbation bootstrap. This leads to a new method for constructing confidence regions which are transformation equivariant and have shapes driven solely by the criterion function. A subsampling procedure is proposed for selecting $m$ in practice. Empirical performance of the new method is illustrated with examples drawn from different nonstandard M-estimation settings. Extension of our theory to row-wise independent triangular arrays is also explored.},
  archive      = {J_AOS},
  author       = {Stephen M. S. Lee and Puyudi Yang},
  doi          = {10.1214/18-AOS1803},
  journal      = {The Annals of Statistics},
  number       = {1},
  pages        = {274-299},
  shortjournal = {Ann. Statist.},
  title        = {Bootstrap confidence regions based on M-estimators under nonstandard conditions},
  volume       = {48},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Statistical inference for model parameters in stochastic
gradient descent. <em>AOS</em>, <em>48</em>(1), 251–273. (<a
href="https://doi.org/10.1214/18-AOS1801">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The stochastic gradient descent (SGD) algorithm has been widely used in statistical estimation for large-scale data due to its computational and memory efficiency. While most existing works focus on the convergence of the objective function or the error of the obtained solution, we investigate the problem of statistical inference of true model parameters based on SGD when the population loss function is strongly convex and satisfies certain smoothness conditions. Our main contributions are twofold. First, in the fixed dimension setup, we propose two consistent estimators of the asymptotic covariance of the average iterate from SGD: (1) a plug-in estimator, and (2) a batch-means estimator, which is computationally more efficient and only uses the iterates from SGD. Both proposed estimators allow us to construct asymptotically exact confidence intervals and hypothesis tests. Second, for high-dimensional linear regression, using a variant of the SGD algorithm, we construct a debiased estimator of each regression coefficient that is asymptotically normal. This gives a one-pass algorithm for computing both the sparse regression coefficients and confidence intervals, which is computationally attractive and applicable to online data.},
  archive      = {J_AOS},
  author       = {Xi Chen and Jason D. Lee and Xin T. Tong and Yichen Zhang},
  doi          = {10.1214/18-AOS1801},
  journal      = {The Annals of Statistics},
  number       = {1},
  pages        = {251-273},
  shortjournal = {Ann. Statist.},
  title        = {Statistical inference for model parameters in stochastic gradient descent},
  volume       = {48},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Spectral and matrix factorization methods for consistent
community detection in multi-layer networks. <em>AOS</em>,
<em>48</em>(1), 230–250. (<a
href="https://doi.org/10.1214/18-AOS1800">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider the problem of estimating a consensus community structure by combining information from multiple layers of a multi-layer network using methods based on the spectral clustering or a low-rank matrix factorization. As a general theme, these “intermediate fusion” methods involve obtaining a low column rank matrix by optimizing an objective function and then using the columns of the matrix for clustering. However, the theoretical properties of these methods remain largely unexplored. In the absence of statistical guarantees on the objective functions, it is difficult to determine if the algorithms optimizing the objectives will return good community structures. We investigate the consistency properties of the global optimizer of some of these objective functions under the multi-layer stochastic blockmodel. For this purpose, we derive several new asymptotic results showing consistency of the intermediate fusion techniques along with the spectral clustering of mean adjacency matrix under a high dimensional setup, where the number of nodes, the number of layers and the number of communities of the multi-layer graph grow. Our numerical study shows that the intermediate fusion techniques outperform late fusion methods, namely spectral clustering on aggregate spectral kernel and module allegiance matrix in sparse networks, while they outperform the spectral clustering of mean adjacency matrix in multi-layer networks that contain layers with both homophilic and heterophilic communities.},
  archive      = {J_AOS},
  author       = {Subhadeep Paul and Yuguo Chen},
  doi          = {10.1214/18-AOS1800},
  journal      = {The Annals of Statistics},
  number       = {1},
  pages        = {230-250},
  shortjournal = {Ann. Statist.},
  title        = {Spectral and matrix factorization methods for consistent community detection in multi-layer networks},
  volume       = {48},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Adaptive risk bounds in univariate total variation denoising
and trend filtering. <em>AOS</em>, <em>48</em>(1), 205–229. (<a
href="https://doi.org/10.1214/18-AOS1799">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study trend filtering, a relatively recent method for univariate nonparametric regression. For a given integer $r\geq1$, the $r$th order trend filtering estimator is defined as the minimizer of the sum of squared errors when we constrain (or penalize) the sum of the absolute $r$th order discrete derivatives of the fitted function at the design points. For $r=1$, the estimator reduces to total variation regularization which has received much attention in the statistics and image processing literature. In this paper, we study the performance of the trend filtering estimator for every $r\geq1$, both in the constrained and penalized forms. Our main results show that in the strong sparsity setting when the underlying function is a (discrete) spline with few “knots,” the risk (under the global squared error loss) of the trend filtering estimator (with an appropriate choice of the tuning parameter) achieves the parametric $n^{-1}$-rate, up to a logarithmic (multiplicative) factor. Our results therefore provide support for the use of trend filtering, for every $r\geq1$, in the strong sparsity setting.},
  archive      = {J_AOS},
  author       = {Adityanand Guntuboyina and Donovan Lieu and Sabyasachi Chatterjee and Bodhisattva Sen},
  doi          = {10.1214/18-AOS1799},
  journal      = {The Annals of Statistics},
  number       = {1},
  pages        = {205-229},
  shortjournal = {Ann. Statist.},
  title        = {Adaptive risk bounds in univariate total variation denoising and trend filtering},
  volume       = {48},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Optimal rates for community estimation in the weighted
stochastic block model. <em>AOS</em>, <em>48</em>(1), 183–204. (<a
href="https://doi.org/10.1214/18-AOS1797">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Community identification in a network is an important problem in fields such as social science, neuroscience and genetics. Over the past decade, stochastic block models (SBMs) have emerged as a popular statistical framework for this problem. However, SBMs have an important limitation in that they are suited only for networks with unweighted edges; in various scientific applications, disregarding the edge weights may result in a loss of valuable information. We study a weighted generalization of the SBM, in which observations are collected in the form of a weighted adjacency matrix and the weight of each edge is generated independently from an unknown probability density determined by the community membership of its endpoints. We characterize the optimal rate of misclustering error of the weighted SBM in terms of the Renyi divergence of order 1/2 between the weight distributions of within-community and between-community edges, substantially generalizing existing results for unweighted SBMs. Furthermore, we present a computationally tractable algorithm based on discretization that achieves the optimal error rate. Our method is adaptive in the sense that the algorithm, without assuming knowledge of the weight densities, performs as well as the best algorithm that knows the weight densities.},
  archive      = {J_AOS},
  author       = {Min Xu and Varun Jog and Po-Ling Loh},
  doi          = {10.1214/18-AOS1797},
  journal      = {The Annals of Statistics},
  number       = {1},
  pages        = {183-204},
  shortjournal = {Ann. Statist.},
  title        = {Optimal rates for community estimation in the weighted stochastic block model},
  volume       = {48},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Envelope-based sparse partial least squares. <em>AOS</em>,
<em>48</em>(1), 161–182. (<a
href="https://doi.org/10.1214/18-AOS1796">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sparse partial least squares (SPLS) is widely used in applied sciences as a method that performs dimension reduction and variable selection simultaneously in linear regression. Several implementations of SPLS have been derived, among which the SPLS proposed in Chun and Keleş (J. R. Stat. Soc. Ser. B. Stat. Methodol. 72 (2010) 3–25) is very popular and highly cited. However, for all of these implementations, the theoretical properties of SPLS are largely unknown. In this paper, we propose a new version of SPLS, called the envelope-based SPLS, using a connection between envelope models and partial least squares (PLS). We establish the consistency, oracle property and asymptotic normality of the envelope-based SPLS estimator. The large-sample scenario and high-dimensional scenario are both considered. We also develop the envelope-based SPLS estimators under the context of generalized linear models, and discuss its theoretical properties including consistency, oracle property and asymptotic distribution. Numerical experiments and examples show that the envelope-based SPLS estimator has better variable selection and prediction performance over the SPLS estimator (J. R. Stat. Soc. Ser. B. Stat. Methodol. 72 (2010) 3–25).},
  archive      = {J_AOS},
  author       = {Guangyu Zhu and Zhihua Su},
  doi          = {10.1214/18-AOS1796},
  journal      = {The Annals of Statistics},
  number       = {1},
  pages        = {161-182},
  shortjournal = {Ann. Statist.},
  title        = {Envelope-based sparse partial least squares},
  volume       = {48},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). New <span class="math inline"><em>G</em></span>-formula for
the sequential causal effect and blip effect of treatment in sequential
causal inference. <em>AOS</em>, <em>48</em>(1), 138–160. (<a
href="https://doi.org/10.1214/18-AOS1795">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In sequential causal inference, two types of causal effects are of practical interest, namely, the causal effect of the treatment regime (called the sequential causal effect) and the blip effect of treatment on the potential outcome after the last treatment. The well-known $G$-formula expresses these causal effects in terms of the standard parameters. In this article, we obtain a new $G$-formula that expresses these causal effects in terms of the point observable effects of treatments similar to treatment in the framework of single-point causal inference. Based on the new $G$-formula, we estimate these causal effects by maximum likelihood via point observable effects with methods extended from single-point causal inference. We are able to increase precision of the estimation without introducing biases by an unsaturated model imposing constraints on the point observable effects. We are also able to reduce the number of point observable effects in the estimation by treatment assignment conditions.},
  archive      = {J_AOS},
  author       = {Xiaoqin Wang and Li Yin},
  doi          = {10.1214/18-AOS1795},
  journal      = {The Annals of Statistics},
  number       = {1},
  pages        = {138-160},
  shortjournal = {Ann. Statist.},
  title        = {New $G$-formula for the sequential causal effect and blip effect of treatment in sequential causal inference},
  volume       = {48},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Model assisted variable clustering: Minimax-optimal recovery
and algorithms. <em>AOS</em>, <em>48</em>(1), 111–137. (<a
href="https://doi.org/10.1214/18-AOS1794">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The problem of variable clustering is that of estimating groups of similar components of a $p$-dimensional vector $X=(X_{1},\ldots ,X_{p})$ from $n$ independent copies of $X$. There exists a large number of algorithms that return data-dependent groups of variables, but their interpretation is limited to the algorithm that produced them. An alternative is model-based clustering, in which one begins by defining population level clusters relative to a model that embeds notions of similarity. Algorithms tailored to such models yield estimated clusters with a clear statistical interpretation. We take this view here and introduce the class of $G$-block covariance models as a background model for variable clustering. In such models, two variables in a cluster are deemed similar if they have similar associations will all other variables. This can arise, for instance, when groups of variables are noise corrupted versions of the same latent factor. We quantify the difficulty of clustering data generated from a $G$-block covariance model in terms of cluster proximity, measured with respect to two related, but different, cluster separation metrics. We derive minimax cluster separation thresholds, which are the metric values below which no algorithm can recover the model-defined clusters exactly, and show that they are different for the two metrics. We therefore develop two algorithms, COD and PECOK, tailored to $G$-block covariance models, and study their minimax-optimality with respect to each metric. Of independent interest is the fact that the analysis of the PECOK algorithm, which is based on a corrected convex relaxation of the popular $K$-means algorithm, provides the first statistical analysis of such algorithms for variable clustering. Additionally, we compare our methods with another popular clustering method, spectral clustering. Extensive simulation studies, as well as our data analyses, confirm the applicability of our approach.},
  archive      = {J_AOS},
  author       = {Florentina Bunea and Christophe Giraud and Xi Luo and Martin Royer and Nicolas Verzelen},
  doi          = {10.1214/18-AOS1794},
  journal      = {The Annals of Statistics},
  number       = {1},
  pages        = {111-137},
  shortjournal = {Ann. Statist.},
  title        = {Model assisted variable clustering: Minimax-optimal recovery and algorithms},
  volume       = {48},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Robust sparse covariance estimation by thresholding tyler’s
m-estimator. <em>AOS</em>, <em>48</em>(1), 86–110. (<a
href="https://doi.org/10.1214/18-AOS1793">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Estimating a high-dimensional sparse covariance matrix from a limited number of samples is a fundamental task in contemporary data analysis. Most proposals to date, however, are not robust to outliers or heavy tails. Toward bridging this gap, in this work we consider estimating a sparse shape matrix from $n$ samples following a possibly heavy-tailed elliptical distribution. We propose estimators based on thresholding either Tyler’s M-estimator or its regularized variant. We prove that in the joint limit as the dimension $p$ and the sample size $n$ tend to infinity with $p/n\to\gamma&gt;0$, our estimators are minimax rate optimal. Results on simulated data support our theoretical analysis.},
  archive      = {J_AOS},
  author       = {John Goes and Gilad Lerman and Boaz Nadler},
  doi          = {10.1214/18-AOS1793},
  journal      = {The Annals of Statistics},
  number       = {1},
  pages        = {86-110},
  shortjournal = {Ann. Statist.},
  title        = {Robust sparse covariance estimation by thresholding tyler’s M-estimator},
  volume       = {48},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Sparse SIR: Optimal rates and adaptive estimation.
<em>AOS</em>, <em>48</em>(1), 64–85. (<a
href="https://doi.org/10.1214/18-AOS1791">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sliced inverse regression (SIR) is an innovative and effective method for sufficient dimension reduction and data visualization. Recently, an impressive range of penalized SIR methods has been proposed to estimate the central subspace in a sparse fashion. Nonetheless, few of them considered the sparse sufficient dimension reduction from a decision-theoretic point of view. To address this issue, we in this paper establish the minimax rates of convergence for estimating the sparse SIR directions under various commonly used loss functions in the literature of sufficient dimension reduction. We also discover the possible trade-off between statistical guarantee and computational performance for sparse SIR. We finally propose an adaptive estimation scheme for sparse SIR which is computationally tractable and rate optimal. Numerical studies are carried out to confirm the theoretical properties of our proposed methods.},
  archive      = {J_AOS},
  author       = {Kai Tan and Lei Shi and Zhou Yu},
  doi          = {10.1214/18-AOS1791},
  journal      = {The Annals of Statistics},
  number       = {1},
  pages        = {64-85},
  shortjournal = {Ann. Statist.},
  title        = {Sparse SIR: Optimal rates and adaptive estimation},
  volume       = {48},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Rerandomization in <span
class="math inline">2<sup><em>K</em></sup></span> factorial experiments.
<em>AOS</em>, <em>48</em>(1), 43–63. (<a
href="https://doi.org/10.1214/18-AOS1790">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With many pretreatment covariates and treatment factors, the classical factorial experiment often fails to balance covariates across multiple factorial effects simultaneously. Therefore, it is intuitive to restrict the randomization of the treatment factors to satisfy certain covariate balance criteria, possibly conforming to the tiers of factorial effects and covariates based on their relative importances. This is rerandomization in factorial experiments. We study the asymptotic properties of this experimental design under the randomization inference framework without imposing any distributional or modeling assumptions of the covariates and outcomes. We derive the joint asymptotic sampling distribution of the usual estimators of the factorial effects, and show that it is symmetric, unimodal and more “concentrated” at the true factorial effects under rerandomization than under the classical factorial experiment. We quantify this advantage of rerandomization using the notions of “central convex unimodality” and “peakedness” of the joint asymptotic sampling distribution. We also construct conservative large-sample confidence sets for the factorial effects.},
  archive      = {J_AOS},
  author       = {Xinran Li and Peng Ding and Donald B. Rubin},
  doi          = {10.1214/18-AOS1790},
  journal      = {The Annals of Statistics},
  number       = {1},
  pages        = {43-63},
  shortjournal = {Ann. Statist.},
  title        = {Rerandomization in $2^{K}$ factorial experiments},
  volume       = {48},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). The phase transition for the existence of the maximum
likelihood estimate in high-dimensional logistic regression.
<em>AOS</em>, <em>48</em>(1), 27–42. (<a
href="https://doi.org/10.1214/18-AOS1789">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper rigorously establishes that the existence of the maximum likelihood estimate (MLE) in high-dimensional logistic regression models with Gaussian covariates undergoes a sharp “phase transition.” We introduce an explicit boundary curve $h_{\mathrm{MLE}}$, parameterized by two scalars measuring the overall magnitude of the unknown sequence of regression coefficients, with the following property: in the limit of large sample sizes $n$ and number of features $p$ proportioned in such a way that $p/n\rightarrow \kappa $, we show that if the problem is sufficiently high dimensional in the sense that $\kappa &gt;h_{\mathrm{MLE}}$, then the MLE does not exist with probability one. Conversely, if $\kappa &lt;h_{\mathrm{MLE}}$, the MLE asymptotically exists with probability one.},
  archive      = {J_AOS},
  author       = {Emmanuel J. Candès and Pragya Sur},
  doi          = {10.1214/18-AOS1789},
  journal      = {The Annals of Statistics},
  number       = {1},
  pages        = {27-42},
  shortjournal = {Ann. Statist.},
  title        = {The phase transition for the existence of the maximum likelihood estimate in high-dimensional logistic regression},
  volume       = {48},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Two-step semiparametric empirical likelihood inference.
<em>AOS</em>, <em>48</em>(1), 1–26. (<a
href="https://doi.org/10.1214/18-AOS1788">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In both parametric and certain nonparametric statistical models, the empirical likelihood ratio satisfies a nonparametric version of Wilks’ theorem. For many semiparametric models, however, the commonly used two-step (plug-in) empirical likelihood ratio is not asymptotically distribution-free, that is, its asymptotic distribution contains unknown quantities, and hence Wilks’ theorem breaks down. This article suggests a general approach to restore Wilks’ phenomenon in two-step semiparametric empirical likelihood inferences. The main insight consists in using as the moment function in the estimating equation the influence function of the plug-in sample moment. The proposed method is general; it leads to a chi-squared limiting distribution with known degrees of freedom; it is efficient; it does not require undersmoothing; and it is less sensitive to the first-step than alternative methods, which is particularly appealing for high-dimensional settings. Several examples and simulation studies illustrate the general applicability of the procedure and its excellent finite sample performance relative to competing methods.},
  archive      = {J_AOS},
  author       = {Francesco Bravo and Juan Carlos Escanciano and Ingrid Van Keilegom},
  doi          = {10.1214/18-AOS1788},
  journal      = {The Annals of Statistics},
  number       = {1},
  pages        = {1-26},
  shortjournal = {Ann. Statist.},
  title        = {Two-step semiparametric empirical likelihood inference},
  volume       = {48},
  year         = {2020},
}
</textarea>
</details></li>
</ul>

</body>
</html>
