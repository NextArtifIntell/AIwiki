<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>ASOC_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="asoc---832">ASOC - 832</h2>
<ul>
<li><details>
<summary>
(2020). Structural deep nonnegative matrix factorization for
community detection. <em>ASOC</em>, <em>97</em>, 106846. (<a
href="https://doi.org/10.1016/j.asoc.2020.106846">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to the important role in analyzing the topological structure of complex networks, community detection has attracted increasing attention recently. The network embedding methods have shown promising performances in community detection, aiming to learn low-dimensional representations of nodes in networks. Among them, Nonnegative Matrix Factorization (NMF) is proved to be an efficient approach. However, considering that the mapping between the original network and the community membership space contains rather complex hierarchical information, classic shallow-NMF approaches often fail to capture the complex underlying network structure, resulting in sub-optimal network representations. Therefore, how to find a method that is able to effectively capture the complex underlying network structure and preserve the global and local structure is an open and important topic. Inspired by the excellent ability of representation learning in deep autoencoder , we propose a Structural Deep Nonnegative Matrix Factorization model, named SDNMF, for community detection. Similar to deep autoencoder , the proposed multi-layer NMF-based model consists of a decoder module and an encoder module. Further, we propose to exploit the first-order similarity and second-order similarity jointly to preserve the structural information. The first-order similarity characterizes the local network information. Meanwhile, the global network information can be captured and the sparsity problem is alleviated by the second-order similarity. An efficient learning algorithm is developed to optimize the proposed DANMF model, which simultaneously optimizes the first-order and second-order similarity. The effectiveness of the proposed SDNMF is verified by comparing it with several state-of-the-art approaches for community detection on six real-word networks. The comparison is based on three performance metrics. Experimental results demonstrate that the proposed SDNMF can obtain a more accurate community membership matrix compared with the baselines. The convergence analysis is also performed to verify the efficiency of our approach.},
  archive      = {J_ASOC},
  author       = {Min Zhang and Zhiping Zhou},
  doi          = {10.1016/j.asoc.2020.106846},
  journal      = {Applied Soft Computing},
  pages        = {106846},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Structural deep nonnegative matrix factorization for community detection},
  volume       = {97},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). An adaptive new state recognition method based on density
peak clustering and voting probabilistic neural network. <em>ASOC</em>,
<em>97</em>, 106835. (<a
href="https://doi.org/10.1016/j.asoc.2020.106835">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The traditional fault diagnosis method based on classifiers relies heavily on complete prior knowledge of the existing faults and often requires complex denoising calculations. However, in practical applications, it is difficult to obtain the complete fault state data, especially degradation state data, in advance. This reduces the classification accuracy of unknown new states and severely limits fault diagnosis. To overcome these difficulties, a novel analog circuit fault diagnosis and unknown state recognition method based on density peak clustering and voting probabilistic neural network (VPNN) is presented. In this method, the novel VPNN model is constructed based on prior knowledge of faults, in which a majority voting algorithm is applied instead of the complex process of denoising , to restrain the environmental noise. Further, a newly designed discriminant layer is added after the summation layer to identify the unknown state data, pursuant to which a derivative-based method is proposed to eliminate the transition data. Moreover, to update the preliminary VPNN, the K-nearest neighbor and density peak clustering procedures are applied to automatically determine the number of new pattern neuron classes, and a data reduction algorithm based on the Gaussian mixture model is proposed to determine the pattern neuron samples. Accordingly, numerous redundant samples are reduced. Thus, new pattern neurons can be straightforwardly added to the preliminary VPNN. Our experimental results clearly demonstrate the robustness of the proposed method, which can reduce false alarms, identify unknown states, and determine the newly added pattern neuron classes and pattern neuron samples to update the VPNN automatically.},
  archive      = {J_ASOC},
  author       = {Junyou Shi and Yi Deng and Zili Wang and Xuhao Guo},
  doi          = {10.1016/j.asoc.2020.106835},
  journal      = {Applied Soft Computing},
  pages        = {106835},
  shortjournal = {Appl. Soft. Comput.},
  title        = {An adaptive new state recognition method based on density peak clustering and voting probabilistic neural network},
  volume       = {97},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A refreshing view of soft computing models for predicting
the deflection of reinforced concrete beams. <em>ASOC</em>, <em>97</em>,
106831. (<a href="https://doi.org/10.1016/j.asoc.2020.106831">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The efforts of this study are to address an essential technical issue in construction and civil engineering , namely predicting the deflection of reinforced concrete beams . Indeed, six new hybrid models (ensemble models) were developed to address this critical technical problem based on artificial intelligence models as well as machine learning algorithms , such as artificial neural network (ANN), support vector machine (SVM), and adaptive neuro-fuzzy inference system (ANFIS). Accordingly, the bagging (BA) technique was applied to create new ensemble models, including BA-SVM, BA-ANN, BA-ANFIS, SVM-ANN, SVM-ANFIS, and ANN-ANFIS models. They were developed based on 120 practical experiments on the deflection of reinforced concrete beams . A series of indicators of error, accuracy, as well as the statistical significance of the models, were analyzed to assess the overall efficiency of the forecasting models. The results showed that the ensemble models are capable of predicting the deflection of reinforced concrete beams with high accuracy, especially the SVM-ANFIS model. The results of this study have opened up many new research directions in the design and optimization of the structure of buildings, dangerous warning systems, and timely solutions to ensure the safety of buildings.},
  archive      = {J_ASOC},
  author       = {Chun Bai and Hoang Nguyen and Panagiotis G. Asteris and Trung Nguyen-Thoi and Jian Zhou},
  doi          = {10.1016/j.asoc.2020.106831},
  journal      = {Applied Soft Computing},
  pages        = {106831},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A refreshing view of soft computing models for predicting the deflection of reinforced concrete beams},
  volume       = {97},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020b). A completed local shrinkage pattern for texture
classification. <em>ASOC</em>, <em>97</em>, 106830. (<a
href="https://doi.org/10.1016/j.asoc.2020.106830">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Visual texture classification plays a critical role in computer vision and pattern recognition. As one of the most popular texture descriptors , local binary pattern(LBP) has achieved extensive development and applications due to its simplicity and high efficiency. However, it is hard for most LBP-based methods to represent completed local texture information efficiently as they either only encode the sign of local difference or fuse low-discriminative features with high dimensionality . To alleviate these problems, this paper groundbreaking introduces a new insight in analyzing completed local texture information, called completed local shrinkage pattern(CLSP), which achieves a high tradeoff between discriminativeness and dimensionality. First, we map the completed local difference to a new encoding space through a shrinkage function and present the local shrinkage pattern. As the important supplement of local texture information, the center pixel is also encoded to build the local center pattern. Finally, the two sub-features are combined to generate the completed local shrinkage pattern. Moreover, we design a multi-information integration texture classification framework, which utilizes the original texture images and the gradient texture images to enrich the diversity of texture information. Experimental results on four popular texture databases demonstrate that the proposed CLSP descriptor achieves superior classification performance with low dimensionality.},
  archive      = {J_ASOC},
  author       = {Xiaochun Xu and Yibing Li and Q.M. Jonathan Wu},
  doi          = {10.1016/j.asoc.2020.106830},
  journal      = {Applied Soft Computing},
  pages        = {106830},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A completed local shrinkage pattern for texture classification},
  volume       = {97},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Interpreting network knowledge with attention mechanism for
bearing fault diagnosis. <em>ASOC</em>, <em>97</em>, 106829. (<a
href="https://doi.org/10.1016/j.asoc.2020.106829">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Condition monitoring and fault diagnosis of bearings play important roles in production safety and limiting the cost of maintenance on a reasonable level. Nowadays, artificial intelligence and machine learning make fault diagnosis gradually become intelligent, and data-driven intelligent algorithms are receiving more and more attention. However, many methods use the existing deep learning models directly for the analysis of mechanical vibration signals , which is still lack of interpretability to researchers. In this paper, a method based on multilayer bidirectional gated recurrent units with attention mechanism is proposed to access the interpretability of neural networks in fault diagnosis, which combines the convolution neural network, gated recurrent unit, and the attention mechanism . Based on the attention mechanism, the attention distribution of input segments is visualized and thus the interpretability of neural networks can be further presented. Experimental validations and comparisons are conducted on bearings. The results present that the proposed model is effective for localizing the discriminative information from the input data, which provides a tool for better understanding the feature extraction process in neural networks, especially for mechanical vibration signals .},
  archive      = {J_ASOC},
  author       = {Zhi-bo Yang and Jun-peng Zhang and Zhi-bin Zhao and Zhi Zhai and Xue-feng Chen},
  doi          = {10.1016/j.asoc.2020.106829},
  journal      = {Applied Soft Computing},
  pages        = {106829},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Interpreting network knowledge with attention mechanism for bearing fault diagnosis},
  volume       = {97},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Adaptive segmentation of traditional cultural pattern based
on superpixel log-euclidean gaussian metric. <em>ASOC</em>, <em>97</em>,
106828. (<a href="https://doi.org/10.1016/j.asoc.2020.106828">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In order to improve the accuracy of objects similarity measurement of traditional cultural pattern segmentation and adaptively determine the number of segmentations, we propose an adaptive segmentation algorithm based on a new superpixel Log-Euclidean Gaussian metric (SLEGM) in this paper. We first propose to use the SLEGM to effectively characterize superpixels for more accurate measurement of their similarity. Because the space of Gaussians sample covariance matrix distribution is not a linear space but a Riemannian manifold, we map this manifold via matrix logarithm into a linear space, which enables us to handle Gaussians with Euclidean operations. Under the SLEGM framework, we develop an improved spectral clustering algorithm that can adaptively determine the number of clusters to achieve the adaptive segmentation for the traditional culture pattern. Extensive evaluations on the Berkeley Segmentation Data Set (BSDS500) benchmark verify that our algorithm outperforms the state-of-the-art techniques of the same category under four evaluation metrics , achieving 74.3\% F-measure, 13.52\% under segmentation error, 83.4\% boundary recall and 97.79\% achievable segmentation accuracy . Further experiments on our challenging Traditional Cultural Pattern Database (TCPD) indicate the effectiveness of our algorithm for segmenting the complex patterns.},
  archive      = {J_ASOC},
  author       = {Xiaogang Hou and Haiying Zhao and Yan Ma and Wei Zhou},
  doi          = {10.1016/j.asoc.2020.106828},
  journal      = {Applied Soft Computing},
  pages        = {106828},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Adaptive segmentation of traditional cultural pattern based on superpixel log-euclidean gaussian metric},
  volume       = {97},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Designing genetic programming classifiers with feature
selection and feature construction. <em>ASOC</em>, <em>97</em>, 106826.
(<a href="https://doi.org/10.1016/j.asoc.2020.106826">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to the flexibility of Genetic Programming (GP), GP has been used for feature construction, feature selection and classifier construction. In this paper, GP classifiers with feature selection and feature construction are investigated to obtain simple and effective classification rules. During the construction of a GP classifier, irrelevant and redundant features affect the search ability of GP, and make GP easily fall into local optimum. This paper proposes two new GP classifier construction methods to restrict bad impact of irrelevant and redundant features on GP classifier. The first is to use a multiple-objective fitness function that decreases both classification error rate and the number of selected features, which is named as GPMO. The second is to first use a feature selection method, i.e., linear forward selection (LFS) to remove irrelevant and redundant features and then use GPMO to construct classifiers, which is named as FSGPMO. Experiments on twelve datasets show that GPMO and FSGPMO have advantages over GP classifiers with a single-objective fitness function named GPSO in term of classification performance, the number of selected features, time cost and function complexity. The proposed FSGPMO can achieve better classification performance than GPMO on higher dimension datasets, however, FSGPMO may remove potential effective features for GP classifier and achieve much lower classification performance than GPMO on some datasets. Compared with two other GP-based classifiers, GPMO can significantly improve the classification performance. Comparisons with other classification algorithms show that GPMO can achieve better or comparable classification performance on most selected datasets. Our proposed GPMO can achieve better performance than wrapper-based feature construction methods using GP on applications with insufficient instances. Further investigations show that bloat phenomena exists in the process of GP evolution and overfitting phenomena is not obvious. Moreover, the benefits of GP over other machine learning algorithms are discussed.},
  archive      = {J_ASOC},
  author       = {Jianbin Ma and Xiaoying Gao},
  doi          = {10.1016/j.asoc.2020.106826},
  journal      = {Applied Soft Computing},
  pages        = {106826},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Designing genetic programming classifiers with feature selection and feature construction},
  volume       = {97},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Forecasting in non-stationary environments with fuzzy time
series. <em>ASOC</em>, <em>97</em>, 106825. (<a
href="https://doi.org/10.1016/j.asoc.2020.106825">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Time series arise in many fields of science such as engineering, economy and agriculture to cite a few. In the early 1990’s the so called Fuzzy Time Series were proposed to handle vague and imprecise knowledge in time series data and have since become competitive forecasting models. A common limitation of recent fuzzy time series models is their inability to handle non-stationary data. Thus, in this paper we introduce a Non-Stationary Fuzzy Time Series (NSFTS). In the proposed method, we employ Non-Stationary Fuzzy Sets, in which perturbation functions are used to adapt the membership function parameters in the knowledge base in response to statistical changes in the time series. The flexibility of the method by means of computational experiments was tested with eight synthetic non-stationary time series data with several kinds of concept drifts, four real market indices (Dow Jones, NASDAQ, SP500 and TAIEX), three real FOREX pairs (EUR-USD, EUR-GBP, GBP-USD), and two real cryptocoins exchange rates (Bitcoin-USD and Ethereum-USD). As competitor models the Time Variant fuzzy time series and the Incremental Ensemble were used, these are two of the major approaches for handling non-stationary data sets. The proposed method shows resilience to concept drift, by adapting parameters of the model, while preserving the symbolic structure of the knowledge base.},
  archive      = {J_ASOC},
  author       = {Petrônio Cândido de Lima e Silva and Carlos Alberto Severiano Junior and Marcos Antonio Alves and Rodrigo Silva and Miri Weiss Cohen and Frederico Gadelha Guimarães},
  doi          = {10.1016/j.asoc.2020.106825},
  journal      = {Applied Soft Computing},
  pages        = {106825},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Forecasting in non-stationary environments with fuzzy time series},
  volume       = {97},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). An adaptive genetic algorithm as a supporting mechanism for
microscopy image analysis in a cascade of convolution neural networks.
<em>ASOC</em>, <em>97</em>, 106824. (<a
href="https://doi.org/10.1016/j.asoc.2020.106824">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The analysis of microscopy images allows for making the description of given samples containing different microscopic organisms. It is important due to the life phase analysis of these organisms. In this paper, an adaptive technique composed of a genetic algorithm (GA) and a cascade of the convolutional classifiers for image analysis is proposed. A GA is proposed for an indication of the likelihood of belonging to the appropriate class. The indicated probability is important due to the next step of measuring the results obtained from the cascade of neural classifiers. The main idea is a hybridization of these two techniques and generalization of the heuristic solution to seeking fitness function coefficients. The proposed solution is described and tested on two databases.In the case of binary classification , the obtained accuracy was 7.5\% higher compared to the use of the classical approach like learning transfer . The proposed solution is discussed in relation to the analysis of individual components, other methods, and advantages/disadvantages.},
  archive      = {J_ASOC},
  author       = {Dawid Połap},
  doi          = {10.1016/j.asoc.2020.106824},
  journal      = {Applied Soft Computing},
  pages        = {106824},
  shortjournal = {Appl. Soft. Comput.},
  title        = {An adaptive genetic algorithm as a supporting mechanism for microscopy image analysis in a cascade of convolution neural networks},
  volume       = {97},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Special issue on soft computing for network and system
security of internet of everything. <em>ASOC</em>, <em>97</em>, 106821.
(<a href="https://doi.org/10.1016/j.asoc.2020.106821">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_ASOC},
  author       = {Jongsung Kim and Ken Choi and Damien Sauveron},
  doi          = {10.1016/j.asoc.2020.106821},
  journal      = {Applied Soft Computing},
  pages        = {106821},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Special issue on soft computing for network and system security of internet of everything},
  volume       = {97},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Truncation-learning-driven surrogate assisted social
learning particle swarm optimization for computationally expensive
problem. <em>ASOC</em>, <em>97</em>, 106812. (<a
href="https://doi.org/10.1016/j.asoc.2020.106812">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Surrogate-assisted evolutionary optimization greatly slashes the computational burden of evolutionary algorithms for computationally expensive problems. However, new issues arise concerning the compatibility and fault tolerance of surrogates, evolutionary learning operators, and problem property. To this end, this paper proposes a truncation-learning-driven surrogate assisted social learning particle swarm optimizer (TL-SSLPSO) to coordinate these three ingredients. For avoiding and correcting the deceptions induced by the low confidence exemplars due to the surrogate in behavior learning, TL-SSLPSO equally segments the iterative population into multiple sub-populations with different fitness levels and selects exemplars from the randomly selected high-level sub-populations for the behavior learning of low-level sub-population, while truncating the behavior learning of the highest-level sub-population composed of some of the best approximated or real evaluated particles and retaining the sub-population directly to the next generation. Besides, a greedy sampling strategy is employed to find promising solutions with better fitness versus the global best to complement the truncation learning. Extensive experiments on twenty-four widely used benchmark problems and a stepped cantilever beam design problem with 17 steps are conducted to assess the effectiveness of cooperation between truncation learning and greedy sampling and comparisons with several state-of-the-art algorithms demonstrate the superiority of the proposed method.},
  archive      = {J_ASOC},
  author       = {Haibo Yu and Li Kang and Ying Tan and Chaoli Sun and Jianchao Zeng},
  doi          = {10.1016/j.asoc.2020.106812},
  journal      = {Applied Soft Computing},
  pages        = {106812},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Truncation-learning-driven surrogate assisted social learning particle swarm optimization for computationally expensive problem},
  volume       = {97},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Classification of white blood cells using deep features
obtained from convolutional neural network models based on the
combination of feature selection methods. <em>ASOC</em>, <em>97</em>,
106810. (<a href="https://doi.org/10.1016/j.asoc.2020.106810">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {White blood cells are cells in the blood and lymph tissue produced by the bone marrow in the human body. White blood cells are an important part of the immune system. The most important task of these cells is to protect the human body against foreign invaders and infectious diseases. When the number of white blood cells in the blood is not enough for the human body, it can cause leukopenia. As a result of this situation, the resistance of the human body against infections and diseases decreases. In this respect, determining the number of these cells in the human body is a specialist task. Detection and treatment of this symptom is a labor-intensive process carried out by specialist doctors and radiologists . Image processing techniques have recently been widely used in biomedical systems for the diagnosis of various diseases. In this study, it is aimed to use image processing techniques to improve the classification performance of deep learning models in white blood cells classification . To perform the classification process more efficiently, the Maximal Information Coefficient and Ridge feature selection methods were used in conjunction with the Convolutional Neural Network models. The Maximal Information Coefficient and Ridge feature selection methods extracted the most relevant features. Afterward, the classification process was realized by using this feature set. In this study, AlexNet, GoogLeNet, and ResNet-50 were used as feature extractor and quadratic discriminant analysis was used as a classifier. As a result, the overall success rate was obtained as 97.95\% in the classification of white blood cells. The experimental results showed that the use of the convolutional neural network models with feature selection methods contributed to improving the classification success of white blood cell types.},
  archive      = {J_ASOC},
  author       = {Mesut Toğaçar and Burhan Ergen and Zafer Cömert},
  doi          = {10.1016/j.asoc.2020.106810},
  journal      = {Applied Soft Computing},
  pages        = {106810},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Classification of white blood cells using deep features obtained from convolutional neural network models based on the combination of feature selection methods},
  volume       = {97},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A novel hybrid model based on combined preprocessing method
and advanced optimization algorithm for power load forecasting.
<em>ASOC</em>, <em>97</em>, 106809. (<a
href="https://doi.org/10.1016/j.asoc.2020.106809">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Short-term power load forecasting occupies an important position in improving the operating efficiency and economic effects of power system . Aiming at improving forecast performance, a substantial number of load forecasting models are proposed. However, most of the previous studies ignored the limitations of individual prediction models and the necessity of data preprocessing , resulting in low forecast accuracy. In this study, a novel hybrid model which combines data preprocessing technology, individual forecasting algorithm and weight determination theory is successfully presented for obtaining higher accuracy and better forecasting ability. Among this model, the data preprocessing stage first uses a novel combination data preprocessing method , which overcomes the shortcomings of single preprocessing methods. In addition, a combined forecasting mechanism composed of RBF , GRNN and ELM is proposed using the weight determination theory , which exceeds the limits of individual prediction models and improves prediction accuracy. For the sake of assessing the availability of the proposed hybrid model, three datasets of half-hour power load of Queensland, South Australia and Victoria in Australia are selected in this study. The final experimental results show that the proposed model not only can approximate the actual power load very well, but also can be used as a helpful tool for power grid planning and dispatching.},
  archive      = {J_ASOC},
  author       = {Ying Nie and Ping Jiang and Haipeng Zhang},
  doi          = {10.1016/j.asoc.2020.106809},
  journal      = {Applied Soft Computing},
  pages        = {106809},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A novel hybrid model based on combined preprocessing method and advanced optimization algorithm for power load forecasting},
  volume       = {97},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Multifaceted fused-CNN based scoring of breast cancer
whole-slide histopathology images. <em>ASOC</em>, <em>97</em>, 106808.
(<a href="https://doi.org/10.1016/j.asoc.2020.106808">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Automating the scoring of Whole-Slide Images (WSIs) is a challenging task because the search space for selecting region of interest (ROI) is huge due to the very large sizes of WSIs. A Multifaceted Fused-CNN (MF-CNN) and a Hybrid-Descriptor are proposed to develop an integrated scoring system for Breast Cancer histopathology WSIs. Suitable color and textural features are identified to help mitotic count based selection of ROIs at lower resolution. To recognize complex patterns, the MF-CNN considers multiple facets of the input image. It counts mitoses, extracts handcrafted features from ROIs and utilizes global texture of the images to form a Hybrid-Descriptor for training a classifier assigning scores to WSIs. The proposed system is evaluated on a publicly available benchmark (TUPAC16) and produced the highest score of 0.582 in terms of Cohen’s Kappa. It surpassed human experts’ level accuracy of ROI selection and can therefore reduce the burden of manual ROI selection for WSIs.},
  archive      = {J_ASOC},
  author       = {Noorul Wahab and Asifullah Khan},
  doi          = {10.1016/j.asoc.2020.106808},
  journal      = {Applied Soft Computing},
  pages        = {106808},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Multifaceted fused-CNN based scoring of breast cancer whole-slide histopathology images},
  volume       = {97},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020a). A conformal prediction inspired approach for distribution
regression with random fourier features. <em>ASOC</em>, <em>97</em>,
106807. (<a href="https://doi.org/10.1016/j.asoc.2020.106807">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Distribution regression refers to the regression case whose input objects are probability measures. A lot of machine learning applications can fit into this framework, such as multi-instance learning and learning from noisy data. This paper proposes an interval prediction algorithm for distribution regression. The algorithm is based on conformal prediction which aims to build reliable prediction systems. To the best of our knowledge, this is the first work to extend conformal prediction to distribution regression problems . Our approach first embeds the input distributions to a reproducing kernel Hilbert space by kernel mean embedding, and then learns a conformal regressor from the embeddings to the outputs. In order to make the whole process faster, we also employ random Fourier features to approximate the kernel. The algorithm was tested on synthetic data sets and applied to statistical postprocessing of ensemble forecasts for temperature and precipitation, which is the first attempt of applying conformal prediction to this application area. The experimental results demonstrate the empirical validity and the effectiveness of our approach when compared with the other widely used algorithms for postprocessing.},
  archive      = {J_ASOC},
  author       = {Di Wang and Ping Wang and Cong Wang and Shuo Zhuang and Junzhi Shi},
  doi          = {10.1016/j.asoc.2020.106807},
  journal      = {Applied Soft Computing},
  pages        = {106807},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A conformal prediction inspired approach for distribution regression with random fourier features},
  volume       = {97},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). The modeling of human facial pain intensity based on
temporal convolutional networks trained with video frames in HSV color
space. <em>ASOC</em>, <em>97</em>, 106805. (<a
href="https://doi.org/10.1016/j.asoc.2020.106805">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {An accurate detection and management of pain, measured through its relative intensity, plays an important role in the treatment of disease and reducing a patient’s discomfort. As it is relatively difficult to assess, describe, evaluate and manage the pain level using a patient’s self-report, automated pain-detecting tools can provide useful information to assist in the management of pain intensity. This study proposes a new predictive modeling framework that employs a modified Temporal Convolutional Network (TCN) algorithm to recognize the pain intensity prevalent in patients’ video frames collected as part of UNBC-McMaster Shoulder Pain Archive and MIntPAIN databases. The inputs of the proposed TCN network is composed of the extracted and reduced face image features from a fine-tuned VGG-Face and principal component analysis (PCA) with Hue, Saturation, Value (HSV) color spaces video images. The results of TCN based predictive model , employing a long short-term memory (LSTM) model as well as other state-of-the art models, show that the proposed approach performs faster with a high level of efficiency. This is demonstrated by the low magnitude of error metrics ( i.e ., Mean Squared Error = 0.0629, Mean Absolute Error = 0.1021, correctness validation results represented by Area under Curve = 85\% and accuracy metric = 92.44\%). Considering the efficiency of the proposed TCN framework, integrating fine-tuned VGG-Face and PCA with Hue, Saturation, Value (HSV) color spaces video images for pain intensity estimation, the present study affirms that the new method can be adopted as an automatic health informatics tool, mainly for pain detection, and subsequently, implemented in the pain management area.},
  archive      = {J_ASOC},
  author       = {Ghazal Bargshady and Xujuan Zhou and Ravinesh C. Deo and Jeffrey Soar and Frank Whittaker and Hua Wang},
  doi          = {10.1016/j.asoc.2020.106805},
  journal      = {Applied Soft Computing},
  pages        = {106805},
  shortjournal = {Appl. Soft. Comput.},
  title        = {The modeling of human facial pain intensity based on temporal convolutional networks trained with video frames in HSV color space},
  volume       = {97},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Monocular image depth prediction without depth sensors: An
unsupervised learning method. <em>ASOC</em>, <em>97</em>, 106804. (<a
href="https://doi.org/10.1016/j.asoc.2020.106804">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Monocular image depth prediction is an interesting challenge in three-dimensional (3D) perception, the purpose of which is to obtain the geometric features of 3D scenes from two-dimensional (2D) images. At present, the deep learning method for monocular depth prediction has yielded good results, but this approach treats it as a supervised deep regression problem . A significant weakness of current methods is the need to collect reams of depth measurement data in actual scenarios for training. In this paper, we design a novel convolutional neural network (CNN) with an encoding and decoding structure to estimate the depth map from monocular RGB images based on basic principles of binocular stereo vision , and use rectified stereo pairs to train our network from scratch in an unsupervised learning method without any depth data. We also explore a new upsampling strategy to improve the output resolution, and introduce a new dynamic optimization strategy to enhance the training speed and prediction accuracy. Extensive experiments on the publicly available KITTI and Cityscapes datasets demonstrate that our approach is more accurate than competing methods. The findings of the proposed methodology illustrate that our CNN model can be utilized as depth completion from LIDAR images.},
  archive      = {J_ASOC},
  author       = {Songnan Chen and Mengxia Tang and Jiangming Kan},
  doi          = {10.1016/j.asoc.2020.106804},
  journal      = {Applied Soft Computing},
  pages        = {106804},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Monocular image depth prediction without depth sensors: An unsupervised learning method},
  volume       = {97},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A product ranking method combining the features–opinion
pairs mining and interval-valued pythagorean fuzzy sets. <em>ASOC</em>,
<em>97</em>, 106803. (<a
href="https://doi.org/10.1016/j.asoc.2020.106803">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mining online reviews has become an important means of identifying consumer behavior and the innovation direction of products. However, it is difficult for both producers and consumers to effectively analyze and extract relevant opinions from a vast number of online reviews. To overcome this problem, a product ranking method that combines feature–opinion pairs mining and interval-valued Pythagorean fuzzy (IVPF) sets was proposed in this study. First, three types of important feature–opinion pairs were clearly defined based on the diversity and complexity of opinion expression forms in Chinese ecommerce reviews. Two deep learning models were then designed to automatically extract the feature–opinion terms and match them into pairs. Afterwards, sentiment analysis techniques were applied to identify sentiment orientation , and the feature–opinion pairs were clustered into groups using K-means clustering algorithm. Meanwhile, considering the confidence level based on the number of online reviews on different products, sentiment value was transformed into interval-value from, including interval membership and non-membership. As the sum of the converted interval membership and non-membership was greater than 1 and their quadratic sum was less than 1, IVPF set was introduced to represent the interval-valued sentiment. Furthermore, based on the interrelationship between product attributes, we proposed an IVPF weighted Heronian mean operator to aggregate the attribute information. Product ranking was then achieved based on the operator and operations under the IVPF information. Finally, a case study was used to verify the feasibility of the proposed method, and comparisons and sensitivity analysis were performed to demonstrate the superiority of our method.},
  archive      = {J_ASOC},
  author       = {Xiangling Fu and Tianxiong Ouyang and Zaoli Yang and Shaohui Liu},
  doi          = {10.1016/j.asoc.2020.106803},
  journal      = {Applied Soft Computing},
  pages        = {106803},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A product ranking method combining the features–opinion pairs mining and interval-valued pythagorean fuzzy sets},
  volume       = {97},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Tour planning for multiple mobile sinks in wireless sensor
networks: A shark smell optimization approach. <em>ASOC</em>,
<em>97</em>, 106802. (<a
href="https://doi.org/10.1016/j.asoc.2020.106802">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sink mobility has been regarded as a widely accepted method for data collection in wireless sensor networks (WSNs) as it significantly improves network performance. Particularly, data collection using mobile sink based on rendezvous points (RPs) is a hot research topic which has been paid enormous attention to WSN community. However, efficient tour planning for the mobile sink (MS) is a challenging problem, especially for delay-harsh applications that require shorter paths of the MS. Existing literature finds this problem as NP-hard in nature, and thus nature-inspired algorithms are in demand as they can provide a near-optimal solution within acceptable time and space constraints. There are many schemes on MS tour planning that exist in the form of heuristics or nature-inspired algorithms; nevertheless, they leave out the scope for further research as most of them have not considered disjoint networks. Moreover, they fail to jointly optimize both the number of RPs and the number of MSs. To this end, this paper presents a novel scheme comprising of two algorithms based on the shark smell optimization (SSO) technique that solves the MS tour planning problem. The first algorithm is used to determine an optimal number of RPs and their locations. Based on this, the second algorithm optimizes the number of MSs so as to minimize the overall tour length. Each of the algorithms is developed with an efficient and novel particle encoding scheme along with the derivation of a fitness function. The tour planning is formulated as an Integer Linear Programming problem for the first algorithm and a Non-linear Programming problem for the second algorithm. Simulation results of our scheme confirm the improvement over the state-of-the-art algorithms. The results are also statistically validated through hypothesis testing using ANOVA and post hoc analysis.},
  archive      = {J_ASOC},
  author       = {Raj Anwit and Abhinav Tomar and Prasanta K. Jana},
  doi          = {10.1016/j.asoc.2020.106802},
  journal      = {Applied Soft Computing},
  pages        = {106802},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Tour planning for multiple mobile sinks in wireless sensor networks: A shark smell optimization approach},
  volume       = {97},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Evaluation of cloud vendors from probabilistic linguistic
information with unknown/partial weight values. <em>ASOC</em>,
<em>97</em>, 106801. (<a
href="https://doi.org/10.1016/j.asoc.2020.106801">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As IT industries grow at a faster pace, cloud technology becomes inevitable. Attracted by the scope, many cloud vendors (CVs) arise. A rational/systematic selection is an urge to tackle the scalability of CVs. To circumvent the issue, in this paper, a framework is proposed for CV selection under with probabilistic linguistic term sets (PLTSs). The PLTS is a flexible structure that allows partial ignorance of occurring probabilities. Initially, attributes’ weights are calculated using a programming model, which uses partial information effectively. Later, decision-makers’ (DMs’) weights are computed by integrating evidence theory with Bayes approximation . Preferences from DMs are aggregated by proposing a two-way operator, which aggregates linguistic preferences using the rule-based method and occurring probabilities using Maclaurin symmetric mean. Moreover, CVs are ranked by using an integrated PROMETHEE–Borda method under the PLTS. Further, to test the validity of the framework, a case study on CV selection is presented for a small-scale company. Finally, the advantages and limitations of the proposed framework are investigated by comparison with other methods and the results infer that (i) the proposed framework is 63.67\% robust even after adequate changes are made to the alternatives; (ii) the proposed framework is 87.67\% robust even after adequate changes are made to the attributes; (iii) from partial adequacy test, the robustness is determined as 77.67\% and 92.33\%; and (iv) from the broadness test, the proposed framework produces an average deviation of 9\% among their rank values, which is better than the extant models that produce an average deviation close to 7.8\%.},
  archive      = {J_ASOC},
  author       = {Sivagami Ramadass and Raghunathan Krishankumar and Kattur Soundarapandian Ravichandran and Huchang Liao and Samarjit Kar and Enrique Herrera-Viedma},
  doi          = {10.1016/j.asoc.2020.106801},
  journal      = {Applied Soft Computing},
  pages        = {106801},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Evaluation of cloud vendors from probabilistic linguistic information with unknown/partial weight values},
  volume       = {97},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Fuzzy electromagnetism optimization (FEMO) and its
application in biomedical image segmentation. <em>ASOC</em>,
<em>97</em>, 106800. (<a
href="https://doi.org/10.1016/j.asoc.2020.106800">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this work, a new unsupervised classification approach is proposed for the biomedical image segmentation . The proposed method will be known as Fuzzy Electromagnetism Optimization (FEMO). As the name suggests, the proposed approach is based on the electromagnetism-like optimization (EMO) method. The EMO method is extended, modified, and combined with the modified type 2 fuzzy C-Means algorithm to improve its efficiency especially for biomedical image segmentation. The proposed FEMO method uses fuzzy membership and the electromagnetism-like optimization method to locate the optimal positions for the cluster centers. The proposed FEMO approach does not have any dependency on the initial selection of the cluster centers. Moreover, this method is suitable for the biomedical images of different modalities. This method is compared with some standard metaheuristics and evolutionary methods (e.g. Genetic Algorithm (GA), Particle Swarm Optimization (PSO), Electromagnetism-like optimization (EMO), Ant Colony Optimization (ACO), etc.) based image segmentation approaches. Four different indices Davies–Bouldin, Xie–Beni, Dunn and β β index are used for the comparison and evaluation purpose. For the GA, PSO, ACO, EMO and the proposed FEMO approach, the optimal average value of the Davies–Bouldin index is 1.833578359 (8 clusters), 1.669359475 (3 clusters), 1.623119284 (3 clusters), 1.647743907 (4 clusters) and 1.456889343 (3 clusters) respectively. It shows that the proposed approach can efficiently determine the optimal clusters. Moreover, the results of the other quantitative indices are quite promising for the proposed approach compared to the other approaches The detailed comparison is performed in both qualitative and quantitative manner and it is found that the proposed method outperforms some of the existing methods concerning some standard evaluation parameters.},
  archive      = {J_ASOC},
  author       = {Shouvik Chakraborty and Kalyani Mali},
  doi          = {10.1016/j.asoc.2020.106800},
  journal      = {Applied Soft Computing},
  pages        = {106800},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Fuzzy electromagnetism optimization (FEMO) and its application in biomedical image segmentation},
  volume       = {97},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Variants of artificial bee colony algorithm and its
applications in medical image processing. <em>ASOC</em>, <em>97</em>,
106799. (<a href="https://doi.org/10.1016/j.asoc.2020.106799">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Artificial Bee Colony (ABC) technique is a highly effective method of optimization inspired by the behavior of bees. Notably, the importance of the ABC algorithm is increasing after artificial intelligence , and automatic decision-making techniques are popularized in almost every field. The analysis of images obtained from medical imaging devices attracts the attention of artificial intelligence researchers because of the importance of these images for human health. Although the ABC algorithm is very humid for medical image analysis, there is no comprehensive literature review of medical image analysis techniques. This study includes a comprehensive survey of academic studies including classification, enhancement, clustering, and segmentation of medical images using ABC. The academic studies between the years 2010–2020 are examined, and 95 studies are presented in total. 42 of these studies consist of medical image analysis studies. Of the selected studies, 20 studies are related to image classification , 15 studies are related to image enhancement, 18 academic studies are related to image clustering, and 42 studies are related to image segmentation methods. The findings of this study show that the ABC method for medical image analysis has positive effects on classification, segmentation, clustering, and enhancement methods, and the use of the ABC method has become more common. We hope that this study will help new researchers to use the ABC method.},
  archive      = {J_ASOC},
  author       = {Şaban Öztürk and Rehan Ahmad and Nadeem Akhtar},
  doi          = {10.1016/j.asoc.2020.106799},
  journal      = {Applied Soft Computing},
  pages        = {106799},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Variants of artificial bee colony algorithm and its applications in medical image processing},
  volume       = {97},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Bi-space interactive cooperative coevolutionary algorithm
for large scale black-box optimization. <em>ASOC</em>, <em>97</em>,
106798. (<a href="https://doi.org/10.1016/j.asoc.2020.106798">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Large scale black-box optimization problems arise in many fields of science and engineering, and many of existing algorithms for these problems still suffer from the “curse of dimensionality”. This paper proposes a generalized framework of Bi-space Interactive Cooperative Coevolutionary Algorithm (BICCA) with evolutions in two spaces. In the pattern space, the interacting patterns of variables are continuously excavated for the evolution of the groups for cooperative coevolution. In the search space, cooperative coevolution and global search are carried out adaptively to get better fitness. By adopting evolutions and interactions within two spaces, patterns evolve to provide better groupings while individuals evolve to reach better fitness. The problem decomposition is conducted along the optimization process, and no extra fitness evaluations are needed for problem decomposition. Experiments on widely-used benchmarks show that BICCA obtains competitive performance on high-dimensional optimization problems with different levels of dimensionality up to 10000.},
  archive      = {J_ASOC},
  author       = {Hongwei Ge and Mingde Zhao and Yaqing Hou and Zhang Kai and Liang Sun and Guozhen Tan and Qiang Zhang and C.L. Philip Chen},
  doi          = {10.1016/j.asoc.2020.106798},
  journal      = {Applied Soft Computing},
  pages        = {106798},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Bi-space interactive cooperative coevolutionary algorithm for large scale black-box optimization},
  volume       = {97},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Varying density method for data stream clustering.
<em>ASOC</em>, <em>97</em>, 106797. (<a
href="https://doi.org/10.1016/j.asoc.2020.106797">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, a new online-offline density-based clustering method for data stream with varying density is proposed. In the online phase, the summary of data is created (often known as micro-clusters) and in the offline phase, this synopsis of data is used to form the final clusters. Finding the accurate micro-clusters is the goal of online phase. When a new data point arrives, the procedure of finding the nearest and best fit micro-cluster is the time consuming process. This procedure can lead to increase the execution time. To address this problem, a new merging algorithm is proposed. For maintaining a limited number of micro-clusters, a pruning process is applied along with the summarization process. In the existing methods, this pruning process takes too long time to remove micro-clusters whose do not receive objects frequently that cause to increase the memory usage. In this paper, to solve this problem, a new pruning algorithm is introduced. Another problem with density-based methods is that they use global parameters in the data sets with varying density that can lead to dramatic decrease in the clustering quality . In our work, to create final clusters, a new density-based algorithm that works based on only MinPts parameter is proposed for increasing the clustering quality of data sets with varying density. The performance evaluation on both synthetic and real data sets illustrates the efficiency and effectiveness of the proposed method. The experimental results show that our method can increase the clustering quality in data sets with varying density along with limited time and memory usage.},
  archive      = {J_ASOC},
  author       = {Maryam Mousavi and Hassan Khotanlou and Azuraliza Abu Bakar and Mohammadmahdi Vakilian},
  doi          = {10.1016/j.asoc.2020.106797},
  journal      = {Applied Soft Computing},
  pages        = {106797},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Varying density method for data stream clustering},
  volume       = {97},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Optimal path planning approach based on q-learning algorithm
for mobile robots. <em>ASOC</em>, <em>97</em>, 106796. (<a
href="https://doi.org/10.1016/j.asoc.2020.106796">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In fact, optimizing path within short computation time still remains a major challenge for mobile robotics applications . In path planning and obstacles avoidance, Q-Learning ( QL ) algorithm has been widely used as a computational method of learning through environment interaction. However, less emphasis is placed on path optimization using QL because of its slow and weak convergence toward optimal solutions. Therefore, this paper proposes an Efficient Q-Learning ( EQL ) algorithm to overcome these limitations and ensure an optimal collision-free path in less possible time. In the QL algorithm, successful learning is closely dependent on the design of an effective reward function and an efficient selection strategy for an optimal action that ensures exploration and exploitation. In this regard, a new reward function is proposed to initialize the Q-table and provide the robot with prior knowledge of the environment, followed by a new efficient selection strategy proposal to accelerate the learning process through search space reduction while ensuring a rapid convergence toward an optimized solution. The main idea is to intensify research at each learning stage, around the straight-line segment linking the current position of the robot to T a r g e t Target (optimal path in terms of length). During the learning process, the proposed strategy favors promising actions that not only lead to an optimized path but also accelerate the convergence of the learning process. The proposed EQL algorithm is first validated using benchmarks from the literature, followed by a comparison with other existing QL -based algorithms. The achieved results showed that the proposed EQL gained good learning proficiency; besides, the training performance is significantly improved compared to the state-of-the-art. Concluded, EQL improves the quality of the paths in terms of length, computation time and robot safety , furthermore outperforms other optimization algorithms .},
  archive      = {J_ASOC},
  author       = {Abderraouf Maoudj and Abdelfetah Hentout},
  doi          = {10.1016/j.asoc.2020.106796},
  journal      = {Applied Soft Computing},
  pages        = {106796},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Optimal path planning approach based on Q-learning algorithm for mobile robots},
  volume       = {97},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Off-policy adversarial imitation learning for robotic tasks
with low-quality demonstrations. <em>ASOC</em>, <em>97</em>, 106795. (<a
href="https://doi.org/10.1016/j.asoc.2020.106795">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The goal of imitation learning (IL) is to enable the robot to imitate expert behavior given expert demonstrations. Adversarial imitation learning (AIL) is a recent successful IL architecture that has shown significant progress in complex continuous tasks, particularly robotic tasks. However, in most cases, the acquisition of high-quality demonstrations is costly and laborious, which poses a significant challenge for AILs. Although generative adversarial imitation learning (GAIL) and its extensions have shown that they are robust to sub-optimal experts, it is difficult for them to surpass the performance of experts by a large margin. To address this issue, in this paper, we propose a novel off-policy AIL method called robust adversarial imitation learning (RAIL). To enable the agent to significantly outperform a sub-optimal expert providing demonstrations, the hindsight idea of variable reward (VR) is first incorporated into the off-policy AIL framework. Then, a strategy called hindsight copy (HC) of demonstrations is designed to provide the discriminator and trained policy in the AIL framework with different demonstrations to maximize the use of such demonstrations and speed up the learning. Experiments were conducted on two multi-goal robotic tasks to test the proposed method. The results show that our method is not limited to the quality of expert demonstrations and can outperform other IL approaches.},
  archive      = {J_ASOC},
  author       = {Guoyu Zuo and Qishen Zhao and Kexin Chen and Jiangeng Li and Daoxiong Gong},
  doi          = {10.1016/j.asoc.2020.106795},
  journal      = {Applied Soft Computing},
  pages        = {106795},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Off-policy adversarial imitation learning for robotic tasks with low-quality demonstrations},
  volume       = {97},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Performance analysis of real-coded evolutionary algorithms
under a computationally expensive optimization scenario: 3D–2D
comparative radiography. <em>ASOC</em>, <em>97</em>, 106793. (<a
href="https://doi.org/10.1016/j.asoc.2020.106793">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Real-coded evolutionary algorithms have solved numerous real-world optimization problems . In this work, we aim to analyze the behavior and robustness of several real-coded evolutionary algorithms from the state of the art in a challenging real world optimization problem. This optimization problem consists on the superimposition of 3D and 2D images of skeletal structures (i.e. bones and cavities) based on their silhouette. This task is required for the automation of a forensic identification technique known as comparative radiography, via the generation of the best projection of the 3D image with respect to the 2D image. This superimposition problem was tackled in a recent proposal using an evolutionary 3D–2D image registration method based on differential evolution. However, the results obtained were insufficient for its use in real scenarios, due to: (1) the complexity and multi-modality of search space, despite the reduced number of parameters to be optimized (7 in its simple version and 9 in a more complex one, proposed in this work); and (2) the high computational cost of generating and evaluating a superimposition. Particularly, we have performed a rigorous comparative study of six state-of-the-art real-coded evolutionary algorithms (DE, L-SHADE, CMA-ES, BIPOP-CMA-ES, CRO-SL, and MVMO-SH) with synthetic images of three forensic anatomical structures (frontal sinuses, clavicles, and patellae), showing that the best results are always obtained by MVMO-SH in terms of precision, robustness and computational cost. Furthermore, we have validated the quality of the superimpositions obtained by the evolutionary image registration method using MVMO-SH with real images of frontal sinuses. We have performed the comparison of 50 head radiographs and 50 3D images, resulting in 2, 500 cross-comparisons (50 positive and 2, 450 negatives). The obtained results are promising since the superimpositions obtained allowed us to filter out 88\% of the possible candidates with 0 error rate in a fully automatic manner , showing the high quality of the superimposition obtained.},
  archive      = {J_ASOC},
  author       = {Oscar Gómez and Oscar Ibáñez and Andrea Valsecchi and Enrique Bermejo and Daniel Molina and Oscar Cordón},
  doi          = {10.1016/j.asoc.2020.106793},
  journal      = {Applied Soft Computing},
  pages        = {106793},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Performance analysis of real-coded evolutionary algorithms under a computationally expensive optimization scenario: 3D–2D comparative radiography},
  volume       = {97},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Assessing countries’ performances against COVID-19 via
WSIDEA and machine learning algorithms. <em>ASOC</em>, <em>97</em>,
106792. (<a href="https://doi.org/10.1016/j.asoc.2020.106792">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The COVID-19 pandemic, which first spread to the People of Republic of China and then to other countries in a short time, affected the whole world by infecting millions of people and have been increasing its impact day by day. Hundreds of researchers in many countries are in search of a solution to end up this pandemic. This study aims to contribute to the literature by performing detailed analyses via a new three-staged framework constructed based on data envelopment analysis and machine learning algorithms to assess the performances of 142 countries against the COVID-19 outbreak. Particularly, clustering analyses were made using k-means and hierarchic clustering methods . Subsequently, efficiency analysis of countries were performed by a novel model, the weighted stochastic imprecise data envelopment analysis. Finally, parameters were analyzed with decision tree and random forest algorithms. Results have been analyzed in detail, and the classification of countries are determined by providing the most influential parameters. The analysis showed that the optimum number of clusters for 142 countries is three. In addition, while 20 countries out of 142 countries were fully effective, 36\% of them were found to be effective at a rate of 90\%. Finally, it has been observed that the data such as GDP, smoking rates, and the rate of diabetes patients do not affect the effectiveness level of the countries.},
  archive      = {J_ASOC},
  author       = {Nezir Aydin and Gökhan Yurdakul},
  doi          = {10.1016/j.asoc.2020.106792},
  journal      = {Applied Soft Computing},
  pages        = {106792},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Assessing countries’ performances against COVID-19 via WSIDEA and machine learning algorithms},
  volume       = {97},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Integrated meta-heuristics finite difference method for the
dynamics of nonlinear unipolar electrohydrodynamic pump flow model.
<em>ASOC</em>, <em>97</em>, 106791. (<a
href="https://doi.org/10.1016/j.asoc.2020.106791">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this study, a novel design of integrated biological inspired computational heuristics is presented for the dynamics of nonlinear unipolar electrohydrodynamic (UP-EHD) pump flow model by exploiting the competency of finite difference method (FDM) for discretization , global search viability of genetic algorithms (GAs) and local search efficiency of active-set method (ASM), i.e., FDM-GA-ASM. The FDM is incorporated to transform the differential equations of the UP-EHD pump flow model into a system of nonlinear algebraic equations . The cost function is constructed through the mean-square residual error by mimicking forward, central and backward difference schemes viable for a broader range of physical models. The optimum solution is achieved by the integration of global search with GAs and local search of ASM for speedy refinements. The designed stochastic numerical solver FDM-GA-ASM investigates the critical physical parameters, i.e., charge density, electric field and electric potential by varying electrical slip, Reynolds number and source number of the UP-EHD model. Statistical observations in terms of probability plots, histogram illustrations, boxplots for the cost function, mean absolute error , root mean squared error and Nash–Sutcliffe efficiency metrics are used to validate the efficiency of the FDM-GA-ASM scheme for the three variants of the UP-EHD model. The designed FDM-GA-ASM is a promising numerical computing solver for nonlinear differential systems in engineering and technology.},
  archive      = {J_ASOC},
  author       = {Ihtesham Jadoon and Ashfaq Ahmed and Ata ur Rehman and Muhammad Shoaib and Muhammad Asif Zahoor Raja},
  doi          = {10.1016/j.asoc.2020.106791},
  journal      = {Applied Soft Computing},
  pages        = {106791},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Integrated meta-heuristics finite difference method for the dynamics of nonlinear unipolar electrohydrodynamic pump flow model},
  volume       = {97},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Real-time neural network scheduling of emergency medical
mask production during COVID-19. <em>ASOC</em>, <em>97</em>, 106790. (<a
href="https://doi.org/10.1016/j.asoc.2020.106790">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {During the outbreak of the novel coronavirus pneumonia (COVID-19), there is a huge demand for medical masks. A mask manufacturer often receives a large amount of orders that must be processed within a short response time. It is of critical importance for the manufacturer to schedule and reschedule mask production tasks as efficiently as possible. However, when the number of tasks is large, most existing scheduling algorithms require very long computational time and, therefore, cannot meet the needs of emergency response. In this paper, we propose an end-to-end neural network , which takes a sequence of production tasks as inputs and produces a schedule of tasks in a real-time manner. The network is trained by reinforcement learning using the negative total tardiness as the reward signal. We applied the proposed approach to schedule emergency production tasks for a medical mask manufacturer during the peak of COVID-19 in China. Computational results show that the neural network scheduler can solve problem instances with hundreds of tasks within seconds. The objective function value obtained by the neural network scheduler is significantly better than those of existing constructive heuristics, and is close to those of the state-of-the-art metaheuristics whose computational time is unaffordable in practice.},
  archive      = {J_ASOC},
  author       = {Chen-Xin Wu and Min-Hui Liao and Mumtaz Karatas and Sheng-Yong Chen and Yu-Jun Zheng},
  doi          = {10.1016/j.asoc.2020.106790},
  journal      = {Applied Soft Computing},
  pages        = {106790},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Real-time neural network scheduling of emergency medical mask production during COVID-19},
  volume       = {97},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A fuzzy-AHP based prioritization of trust criteria in fog
computing services. <em>ASOC</em>, <em>97</em>, 106789. (<a
href="https://doi.org/10.1016/j.asoc.2020.106789">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fog computing is a new promising paradigm that is capable of addressing the problems with the traditional cloud computing . However, prior to the development of this qualifiedly roseate technology, security and privacy issues must be addressed since fog servers may be used to process, manage, and store private and latency-sensitive information. One of such measures is to implement a trust management system . The trustworthiness of a node (or trustee) is assessed based on some criteria as determined by the trustor. It is therefore exigent to identify and prioritize these criteria to allow a trustor to determine the extent to which a parameter contributes to the overall trust value of a trustee and whether it is profitable to transact with the node. The prioritization of trust parameter is a multi-criteria decision-making (MCDM) problem since it involves different criteria which must be considered concurrently. In this study, a fuzzy analytic hierarchy process (Fuzzy-AHP) technique is utilized to identify and prioritize trust parameters in fog computing. The results indicate that quality of service (QoS) is the best prioritized parameter that a service requester can use to evaluate the trust level of a service provider, followed by quality of security (QoSec) while recommendations is the worst ranked. It is also revealed that social relationships is the most ranked trust parameter that a service provider can use to determine the level of truthfulness of a service requester while past reputation is the least considered. This study further considers the sub-criteria under each category with a view to prioritizing them using the Fuzzy-AHP method.},
  archive      = {J_ASOC},
  author       = {Sunday Oyinlola Ogundoyin and Ismaila Adeniyi Kamil},
  doi          = {10.1016/j.asoc.2020.106789},
  journal      = {Applied Soft Computing},
  pages        = {106789},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A fuzzy-AHP based prioritization of trust criteria in fog computing services},
  volume       = {97},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Inspection by exception: A new machine learning-based
approach for multistage manufacturing. <em>ASOC</em>, <em>97</em>,
106787. (<a href="https://doi.org/10.1016/j.asoc.2020.106787">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Manufacturing processes usually consist of multiple different stages, each of which is influenced by a multitude of factors. Therefore, variations in product quality at a certain stage are contributed to by the errors generated at the current, as well as preceding, stages. The high cost of each production stage in the manufacture of high-quality products has stimulated a drive towards decreasing the volume of non-added value processes such as inspection. This paper presents a new method for what the authors have referred to as ‘inspection by exception’ – the principle of actively detecting and then inspecting only the parts that cannot be categorized as healthy or unhealthy with a high degree of certainty. The key idea is that by inspecting only those parts that are in the corridor of uncertainty, the volume of inspections are considerably reduced. This possibility is explored using multistage manufacturing data and both unsupervised and supervised learning algorithms. A case study is presented whereby material conditions and time domain features for force, vibration and tempering temperature are used as input data. Fuzzy C-Means (FCM) clustering is implemented to achieve inspection by exception in an unsupervised manner based on the normalized Euclidean distances between the principal components and cluster centres. Also, deviation vectors for product health are obtained using a comparator system to train neural networks for supervised learning-based inspection by exception. It is shown that the volume of inspections can be reduced by as much as 82\% and 93\% using the unsupervised and supervised learning approaches, respectively.},
  archive      = {J_ASOC},
  author       = {Moschos Papananias and Thomas E. McLeay and Olusayo Obajemu and Mahdi Mahfouf and Visakan Kadirkamanathan},
  doi          = {10.1016/j.asoc.2020.106787},
  journal      = {Applied Soft Computing},
  pages        = {106787},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Inspection by exception: A new machine learning-based approach for multistage manufacturing},
  volume       = {97},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Bi-MARS: A bi-clustering based memetic algorithm for
recommender systems. <em>ASOC</em>, <em>97</em>, 106785. (<a
href="https://doi.org/10.1016/j.asoc.2020.106785">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the expansion of e-business, availability of items on the web is profuse as compared to the previous era. The task of finding relevant items from this pool of items available online is a time-consuming task. Collaborative Filtering (CF) is the foremost productive recommendation algorithm that helps the user find relevant items and thus increase the user’s engagement. However, several drawbacks of CF, especially data sparsity , scalability, and relevance of items recommended to users are open research issues that pose serious challenges to precision of the algorithm. We proposed a novel Bi-clustering based Memetic Algorithm for Recommender Systems (Bi-MARS) based on the collaborative behavior of memes. Bi-clusters are created for discovering precise and localized neighborhood of the target user. Further, a novel local search to refine similarity values associated with neighborhood users and prediction score function for unrated items are formulated. We evaluated the performance of Bi-MARS on MovieLens dataset of three different sizes. Additionally, results are compared with eight other approaches namely, traditional CF, Probabilistic Latent Semantic Analysis , Non-negative Matrix Factorization , Entropy-based CF, Jaccard Coefficient-based Bi-clustering and Fusion, Combined Bi-clustering and Entropy-based CF (CBE-CF), and evolutionary approaches: Genetic Algorithm based Recommender System and Memetic Algorithm based Recommender System (MARS). Experimental results depict that Bi-MARS outperforms all above-said approaches. The precision of MARS is improved by 66.3\% on Movielens dataset of size 100K, which is the most precise algorithm after Bi-MARS among all evolutionary algorithms considered. Further, CPU Time of CBE-CF, which portrays least mean absolute error , among all clustering techniques considered, is improved by 53.8\% with Bi-MARS. Bi-MARS generates recommendations by finding the closest similarity vector of the target user, which contributes to the computational accuracy of the algorithm and thus the relevance of items.},
  archive      = {J_ASOC},
  author       = {Saumya Bansal and Niyati Baliyan},
  doi          = {10.1016/j.asoc.2020.106785},
  journal      = {Applied Soft Computing},
  pages        = {106785},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Bi-MARS: A bi-clustering based memetic algorithm for recommender systems},
  volume       = {97},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). SLBRS: Network virus propagation model based on safety
entropy. <em>ASOC</em>, <em>97</em>, 106784. (<a
href="https://doi.org/10.1016/j.asoc.2020.106784">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Network viruses pose a great threat to cyberspace, which makes people study many different models of network virus propagation based on dynamic system theory. However, many models of network virus have not been carried out in-depth analysis on the security situation of network virus system, especially the research on the security of network virus propagation model using safety entropy is still in the early stage. In this paper, we propose a new type of SLBRS (Susceptible–Latent–Breaking-out–Recovered–Susceptible) network virus propagation model . Firstly, we analyze the dynamic characteristics of the SLBRS model such as stability, periodic orbit and basic reproduction number. Secondly, we analyzed the security of SLBRS model, and studied the security trend of the virus system on the basis of safety entropy and its derivative. Finally, numerical simulation experiments were carried out on SLBRS model and the trend of safety entropy, the simulation results verify the correctness of the model. Numerical simulation shows that SLBRS model avoids the shortcomings of state and parameter processing in previous studies, and the SLBRS model can describe the security trend of the system intuitively and effectively by using the method of safety entropy.},
  archive      = {J_ASOC},
  author       = {Wei Tang and Yu-Jun Liu and Yu-Ling Chen and Yi-Xian Yang and Xin-Xin Niu},
  doi          = {10.1016/j.asoc.2020.106784},
  journal      = {Applied Soft Computing},
  pages        = {106784},
  shortjournal = {Appl. Soft. Comput.},
  title        = {SLBRS: Network virus propagation model based on safety entropy},
  volume       = {97},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Re-training and parameter sharing with the hash trick for
compressing convolutional neural networks. <em>ASOC</em>, <em>97</em>,
106783. (<a href="https://doi.org/10.1016/j.asoc.2020.106783">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As an ubiquitous technology for improving machine intelligence , deep learning has largely taken the dominant position among nowadays most advanced computer vision systems. To achieve superior performance on large-scale datasets, convolutional neural networks (CNNs) are often designed as complex models with millions of parameters. This limits the deployment of CNNs in embedded intelligent computer vision systems, such as intelligent robots that are resource-constrained with real-time computing requirement. This paper proposes a simple and effective model compression scheme to improve the real-time sensing of the surrounding objects. In the proposed framework, the Hash trick is first applied to a modified convolutional layer , and the compression of the convolutional layer is realized via weight sharing. Subsequently, the Hash index matrix is introduced to represent the Hash function , and its relaxation regularization is introduced into the fine-tuned loss function. Through the dynamic retraining of the index matrix, the Hash function can be updated. We evaluate our method using several state-of-the-art CNNs. Experimental results showed that the proposed method can reduce the number of parameters in AlexNet by 24 × × with no accuracy loss. In addition, the compressed VGG16 and ResNet50 can achieve a more than 60 × × increased speed, which is significant.},
  archive      = {J_ASOC},
  author       = {Xu Gou and Linbo Qing and Yi Wang and Mulin Xin and Xianmin Wang},
  doi          = {10.1016/j.asoc.2020.106783},
  journal      = {Applied Soft Computing},
  pages        = {106783},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Re-training and parameter sharing with the hash trick for compressing convolutional neural networks},
  volume       = {97},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Secure video retrieval using image query on an untrusted
cloud. <em>ASOC</em>, <em>97</em>, 106782. (<a
href="https://doi.org/10.1016/j.asoc.2020.106782">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Video retrieval has been widely used in many applications such as video surveillance, object detection. Recently, the security of video retrieval systems has attracted much attention of researchers. However, the most of the previous works focus on secure image search, which usually utilize homomorphic encryption (HE) or order-preserving encryption (OPE) to achieve privacy-preserving. Specially, few works pay attention to secure video retrieval. In this paper, we propose a novel secure video retrieval (SVR, for simplicity) scheme by utilizing comparable encryption with a vector extending mechanism. The distance between images could be presented as range query. Security analysis demonstrate that the proposed scheme can preserve the privacy of data. The experimental results show that the ciphertext search results are consistent with the plaintext form.},
  archive      = {J_ASOC},
  author       = {Hongyang Yan and Mengqi Chen and Li Hu and Chunfu Jia},
  doi          = {10.1016/j.asoc.2020.106782},
  journal      = {Applied Soft Computing},
  pages        = {106782},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Secure video retrieval using image query on an untrusted cloud},
  volume       = {97},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Incentivizing fairness-aware task allocation in mobile
crowdsensing with sweep coverage and stability control. <em>ASOC</em>,
<em>97</em>, 106781. (<a
href="https://doi.org/10.1016/j.asoc.2020.106781">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mobile crowdsensing (MCS) harnesses the sensing capabilities of sensors built into a large number of smart devices to collect and analyze sensory data, which can be used by a large number of mobile participants to perform numerous sensing tasks. Sweep coverage and stability control are two key issues that need to be solved in task allocation in MCS, because improper matching of tasks and participants, as well as task overload or idleness will slump social welfare. However, most existing work made an optimistic assumption that all participants unconditionally participate in MCS, without considering the selfishness and rationality of participants in practical scenarios. To tackle these issues, this paper proposes a fairness-aware task allocation policy with sweep coverage and stability control, which consists of an online rating protocol and a stability control scheme. For the first module, we integrate the quality of sensing, rating update, collusion identification, and payment determination to develop an online rating protocol to deal with the “free-riding” and collusion of participants simultaneously. For the second module, despite the unpredictable future information of sensing tasks and participants, we design a stability control scheme that only relies on currently information to make the online control independent strategies for maximizing social welfare and balancing network stability in a proportional fairness , which can maintain system stability and achieve a time average social welfare within O ( 1 ∕ V ) O(1∕V) that is arbitrarily close to the optimum for any tunable parameter V &gt; 0 V&amp;gt; 0 . Finally, through rigorous theoretical analysis and experimental comparison with two benchmarks, the correctness and efficiency of our proposed policy is jointly demonstrated.},
  archive      = {J_ASOC},
  author       = {Jiaang Duan and Jianfeng Lu and Wenchao Jiang and Shasha Yang},
  doi          = {10.1016/j.asoc.2020.106781},
  journal      = {Applied Soft Computing},
  pages        = {106781},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Incentivizing fairness-aware task allocation in mobile crowdsensing with sweep coverage and stability control},
  volume       = {97},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). FLF-LSTM: A novel prediction system using forex loss
function. <em>ASOC</em>, <em>97</em>, 106780. (<a
href="https://doi.org/10.1016/j.asoc.2020.106780">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Foreign Exchange or Forex is the sale purchase market point of foreign currency pairs. Due to the high volatility in the forex market, it is difficult to predict the future price of any currency pair. This study shows that a significant enhancement in the prediction of forex price can be achieved by incorporating domain knowledge in the process of training machine learning models. The proposed system integrates the Forex Loss Function (FLF) into a Long Short-Term Memory model called FLF-LSTM — that minimizes the difference between the actual and predictive average of Forex candles. Using the data of 10, 078 four-hour candles of EURUSD pair, it is found that compared to the classic LSTM model, the proposed FLF-LSTM system shows a decrease in overall mean absolute error rate by 10.96\%. It is also reported that the error in forecasting the high and low prices is reduced by 10\% and 9\%, respectively. The proposed model, in comparison to the Recurrent Neural Network-based prediction system, shows an overall reduction of 73.57\% in mean absolute error , by exhibiting up to 68.71\% and 72.31\% error reduction in high and low prices, respectively. In comparison to Auto-Regressive Integrated Moving Average, our proposed model shows a 13\% reduced error. Specifically, in the open, high, and low prices, the error is reduced by 28.5\%, 14.2\%, 9.3\%, respectively. Finally, we compare our model with another well-known time series forecasting model, i.e., FB Prophet — where FLF-LSTM demonstrates 31.8\%, 47.7\%, 23.6\%, 47.7\% error reduction in open, high, low, and close prices, respectively. The data and the code used in this study can be accessed at the following URL: https://github.com/slab-itu/forex_flf_lstm .},
  archive      = {J_ASOC},
  author       = {Salman Ahmed and Saeed-Ul Hassan and Naif Radi Aljohani and Raheel Nawaz},
  doi          = {10.1016/j.asoc.2020.106780},
  journal      = {Applied Soft Computing},
  pages        = {106780},
  shortjournal = {Appl. Soft. Comput.},
  title        = {FLF-LSTM: A novel prediction system using forex loss function},
  volume       = {97},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Crosslingual named entity recognition for clinical
de-identification applied to a COVID-19 italian data set. <em>ASOC</em>,
<em>97</em>, 106779. (<a
href="https://doi.org/10.1016/j.asoc.2020.106779">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The COrona VIrus Disease 19 (COVID-19) pandemic required the work of all global experts to tackle it. Despite the abundance of new studies, privacy laws prevent their dissemination for medical investigations: through clinical de-identification, the Protected Health Information (PHI) contained therein can be anonymized so that medical records can be shared and published. The automation of clinical de-identification through deep learning techniques has proven to be less effective for languages other than English due to the scarcity of data sets. Hence a new Italian de-identification data set has been created from the COVID-19 clinical records made available by the Italian Society of Radiology (SIRM). Therefore, two multi-lingual deep learning systems have been developed for this low-resource language scenario: the objective is to investigate their ability to transfer knowledge between different languages while maintaining the necessary features to correctly perform the Named Entity Recognition task for de-identification. The systems were trained using four different strategies, using both the English Informatics for Integrating Biology &amp; the Bedside (i2b2) 2014 and the new Italian SIRM COVID-19 data sets, then evaluated on the latter. These approaches have demonstrated the effectiveness of cross-lingual transfer learning to de-identify medical records written in a low resource language such as Italian, using one with high resources such as English.},
  archive      = {J_ASOC},
  author       = {Rosario Catelli and Francesco Gargiulo and Valentina Casola and Giuseppe De Pietro and Hamido Fujita and Massimo Esposito},
  doi          = {10.1016/j.asoc.2020.106779},
  journal      = {Applied Soft Computing},
  pages        = {106779},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Crosslingual named entity recognition for clinical de-identification applied to a COVID-19 italian data set},
  volume       = {97},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Bounding edit distance for similarity-based sequence
classification on structural pattern recognition. <em>ASOC</em>,
<em>97</em>, 106778. (<a
href="https://doi.org/10.1016/j.asoc.2020.106778">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Pattern Recognition tasks in the structural domain generally exhibit high accuracy results, but their time efficiency is quite low. Furthermore, this low performance is more pronounced when dealing with instance-based classifiers, since, for each query, the entire corpus must be evaluated to find the closest prototype. In this work we address this efficiency issue for the Nearest Neighbor classifier when data are encoded as two-dimensional code sequences, and more precisely strings and sequences of vectors. For this, a set of bounds is proposed in the distance metric that avoid the calculation of unnecessary distances. Results obtained prove the effectiveness of the proposal as it reduces the classification time in percentages between 80\% and 90\% for string representations and between 60\% and 80\% for data codified as sequences of vectors with respect to their corresponding non-optimized version of the classifier.},
  archive      = {J_ASOC},
  author       = {Juan R. Rico-Juan and Jose J. Valero-Mas and José M. Iñesta},
  doi          = {10.1016/j.asoc.2020.106778},
  journal      = {Applied Soft Computing},
  pages        = {106778},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Bounding edit distance for similarity-based sequence classification on structural pattern recognition},
  volume       = {97},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). An online self-organizing modular neural network for
nonlinear system modeling. <em>ASOC</em>, <em>97</em>, 106777. (<a
href="https://doi.org/10.1016/j.asoc.2020.106777">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Modular neural network (MNN) has distinct advantage in many fields such as pattern recognition and pattern recognition. However it is still a challenge to dynamically adjust the MNN structure for dynamic nonlinear system modeling. This paper proposes a novel online self-organizing MNN (OSOMNN) for nonlinear system modeling. In OSOMNN, an online task decomposition algorithm and a self-organizing algorithm for subnetwork are introduced. Firstly, the task decomposition algorithm is implemented by the online clustering method based on distance and local density, which can online divide the original task into several simpler subtasks. Then subnetworks with single-layer feedforward neural network are built to learn the divided subtasks. Moreover, this paper develops a self-organizing algorithm for subnetwork , which can dynamically adjust its structure and is trained by the improved online gradient method with fixed memory mechanism (FMOGM). To demonstrate the effectiveness of OSOMNN for nonlinear system modeling, experimental investigations using four benchmark nonlinear systems and the monthly sunspots time series show that OSOMNN can automatically add or merge the subnetwork modules and optimize the structure of subnetworks for nonlinear system modeling with a better generalization performance than the established alternatives.},
  archive      = {J_ASOC},
  author       = {Junfei Qiao and Xin Guo and Wenjing Li},
  doi          = {10.1016/j.asoc.2020.106777},
  journal      = {Applied Soft Computing},
  pages        = {106777},
  shortjournal = {Appl. Soft. Comput.},
  title        = {An online self-organizing modular neural network for nonlinear system modeling},
  volume       = {97},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Simplified inverse filter tracked affective acoustic signals
classification incorporating deep convolutional neural networks.
<em>ASOC</em>, <em>97</em>, 106775. (<a
href="https://doi.org/10.1016/j.asoc.2020.106775">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Facial expressions, verbal, behavioral, such as limb movements, and physiological features are vital ways for affective human interactions. Researchers have given machines the ability to recognize affective communication through the above modalities in the past decades. In addition to facial expressions, changes in the level of sound, strength, weakness, and turbulence will also convey affective. Extracting affective feature parameters from the acoustic signals have been widely applied in customer service, education, and the medical field. In this research, an improved AlexNet-based deep convolutional neural network (A-DCNN) is presented for acoustic signal recognition. Firstly, preprocessed on signals using simplified inverse filter tracking (SIFT) and short-time Fourier transform (STFT), Mel frequency Cepstrum (MFCC) and waveform-based segmentation were deployed to create the input for the deep neural network (DNN), which was applied widely in signals preprocess for most neural networks . Secondly, acoustic signals were acquired from the public Ryerson Audio-Visual Database of Affective Speech and Song (RAVDESS) affective speech audio system. Through the acoustic signal preprocessing tools, the basic features of the kind of sound signals were calculated and extracted. The proposed DNN based on improved AlexNet has a 95.88\% accuracy on classifying eight affective of acoustic signals. By comparing with some linear classifications, such as decision table (DT) and Bayesian inference (BI) and other deep neural networks , such as AlexNet+SVM, recurrent convolutional neural network (R-CNN), etc., the proposed method achieves high effectiveness on the accuracy (A), sensitivity (S1), positive predictive (PP), and f1-score (F1). Acoustic signals affective recognition and classification can be potentially applied in industrial product design through measuring consumers’ affective responses to products; by collecting relevant affective sound data to understand the popularity of the product, and furthermore, to improve the product design and increase the market responsiveness.},
  archive      = {J_ASOC},
  author       = {Yuxiang Kuang and Qun Wu and Ying Wang and Nilanjan Dey and Fuqian Shi and Rubén González Crespo and R. Simon Sherratt},
  doi          = {10.1016/j.asoc.2020.106775},
  journal      = {Applied Soft Computing},
  pages        = {106775},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Simplified inverse filter tracked affective acoustic signals classification incorporating deep convolutional neural networks},
  volume       = {97},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Elastic net regularized kernel non-negative matrix
factorization algorithm for clustering guided image representation.
<em>ASOC</em>, <em>97</em>, 106774. (<a
href="https://doi.org/10.1016/j.asoc.2020.106774">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to the capacity of data representation, non-negative matrix factorization has been investigated widely by introducing a variety of constraints. The general processing of non-negative matrix factorization for image clustering consists of two steps: (i) achieving the r r -dimensional non-negative image representations, where the rank r r is set to the expected number of clusters; (ii) adopting the traditional clustering techniques to accomplish the clustering task . Nevertheless, the previous non-negative matrix factorization variants derive image representations from the original space which cannot handle the nonlinear structure of images. This paper focuses on the existing issues and proposes an elastic net regularized kernel non-negative matrix factorization algorithm for clustering guided image representation. In order to explore the nonlinear relations of images, this paper uses the kernel trick to extend original non-negative matrix factorization. A self-organized graph and elastic net regularization are incorporated into the proposed objective of kernel non-negative matrix factorization, besides, the rank is allowed to be larger than the expected number of clusters. By doing so, the graph defined in the feature space is more qualified to represent the intrinsic structure of images. As an accompanying advantage, the clusters of images can be determined using the graph directly without using the two-step trick. According to the proposed alternating update algorithm for solving the optimization problem , the image representation and clustering result can be obtained simultaneously. Extensive experiments on challenging data sets demonstrate the effectiveness of the proposed algorithm compared with the prominent non-negative matrix factorization variants for image clustering.},
  archive      = {J_ASOC},
  author       = {Wenjie Zhu and Yishu Peng},
  doi          = {10.1016/j.asoc.2020.106774},
  journal      = {Applied Soft Computing},
  pages        = {106774},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Elastic net regularized kernel non-negative matrix factorization algorithm for clustering guided image representation},
  volume       = {97},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). An evolutionary approach to black-box optimization on matrix
manifolds. <em>ASOC</em>, <em>97</em>, 106773. (<a
href="https://doi.org/10.1016/j.asoc.2020.106773">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Optimization on matrix manifolds is a class of methods for solving matrix optimization problems , subject to constraints which admit the structure of a Riemannian manifold. These problems are intractable for traditional evolutionary algorithms due to the non-Euclidean nature. This paper generalizes the classical technique of covariance matrix adaptation to matrix manifolds, and proposes a manifold evolution strategy named ManES. By exploiting the manifold structure, we turn an originally constrained problem into a sequence of unconstrained ones in the Euclidean subspace. The proposed algorithm is coordinate-free, in the sense that it is independent of the choice of basis and requires no global coordinate system . All genetic operators take the form of matrix transformations and thus are computationally efficient. The algorithm exhibits state-of-the-art performance on four benchmark problems and one real-world application posed on three different kinds of matrix manifolds.},
  archive      = {J_ASOC},
  author       = {Xiaoyu He and Yuren Zhou and Zefeng Chen and Siyu Jiang},
  doi          = {10.1016/j.asoc.2020.106773},
  journal      = {Applied Soft Computing},
  pages        = {106773},
  shortjournal = {Appl. Soft. Comput.},
  title        = {An evolutionary approach to black-box optimization on matrix manifolds},
  volume       = {97},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Attention distribution guided information transfer networks
for recommendation in practice. <em>ASOC</em>, <em>97</em>, 106772. (<a
href="https://doi.org/10.1016/j.asoc.2020.106772">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, an increasing number of deep learning-based methods have been applied in recommendation. Most such methods outperform traditional methods, especially when using the natural language processing (NLP) technique with review texts. Many deep learning-based recommender systems are used to learn latent representations of reviews written by target users and reviews written for target items. They are then combined to predict the rating of the target user for the target item. However, most previously proposed review-based deep learning methods do not conform to real-world application scenarios, in which we cannot obtain the reviews of the target user for the target item (called U2I review). In real-world recommendation settings, items are always recommended to users before they have experienced them. Therefore, the review of a target user for a target item would not be available during the testing and validation process. Many methods, such as DeepCoNN and D-ATT, do not exclude the U2I review in the process of validation and testing. Therefore, the process of testing is different from real-world application scenarios, and these methods obtain substantial valuable information from the U2I review that target users write for target items. We propose a model called ADGITN and a training strategy to solve this problem. When training, the auxiliary model learns two attention distributions that the U2I reviews over user reviews and item reviews by auxiliary tasks. These two distributions are used to guide the learning of attention distributions between user reviews and item reviews of the main model. Thus, the main model could learn how to extract attention distributions between user reviews and item reviews according to the valuable information extracted from U2I reviews. During validation, only the main model works, and it could extract better attention distributions even without the help of a U2I review. Extensive experiments show the effectiveness of our model. We validate our model on the Amazon and Yelp19 datasets, and the results show that our model outperforms existing excellent models, with up to 13.8\% relative improvement compared to the performance of MPCN, which is one of the best review-based deep learning models for recommendation.},
  archive      = {J_ASOC},
  author       = {Gang Sun and Yu Li and Hongfang Yu and Victor Chang},
  doi          = {10.1016/j.asoc.2020.106772},
  journal      = {Applied Soft Computing},
  pages        = {106772},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Attention distribution guided information transfer networks for recommendation in practice},
  volume       = {97},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A metric for filter bubble measurement in recommender
algorithms considering the news domain. <em>ASOC</em>, <em>97</em>,
106771. (<a href="https://doi.org/10.1016/j.asoc.2020.106771">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recommender systems have been constantly refined to improve the accuracy of rating prediction and ranking generation. However, when a recommender system is too accurate in predicting the users’ interests, negative impacts can arise. One of the most critical is the filter bubbles creation, a situation where a user receives less content diversity. In the news domain, such effect is critical once they are ways of opinion formation. In this paper, we aim to assess the role that a specific set of recommender algorithms has in the creation of filter bubbles and if diversification approaches can decrease such effect. We also verify the effects of such an environment in the users’ exposition and interaction to fake news in the Brazilian presidential election of 2018. To perform such a study, we developed a prototype that recommends news stories and presents these recommendations in a feed. To measure the filter bubble, we introduce a new metric based on the homogenization of a recommended items’ set. Our results show KNN item-based recommendation with the MMR diversification algorithm performs slightly better in putting the user in contact with less homogeneous content while presenting a lower index of likes in fake news.},
  archive      = {J_ASOC},
  author       = {Gabriel Machado Lunardi and Guilherme Medeiros Machado and Vinicius Maran and José Palazzo M. de Oliveira},
  doi          = {10.1016/j.asoc.2020.106771},
  journal      = {Applied Soft Computing},
  pages        = {106771},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A metric for filter bubble measurement in recommender algorithms considering the news domain},
  volume       = {97},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). TB-CoAuth: Text based continuous authentication for
detecting compromised accounts in social networks. <em>ASOC</em>,
<em>97</em>, 106770. (<a
href="https://doi.org/10.1016/j.asoc.2020.106770">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Commencement of research towards compromised account detection in email and web services foreshadows the growth of the same in social network scenario. In this paper, continuous authentication of textual content has been performed for incessant authorship verification to detect compromised accounts in social networks. Four categories of features namely, content free, content specific, stylometric and folksonomy are extracted and evaluated. Experiments are performed with 3057 twitter users taking 4000 latest tweets for each user. It is evident from the experiments that consistency maintained on features is different for each user. Hence, various statistical and similarity-based feature selection techniques are used to rank and select optimal features for each user which are further combined using a popular rank aggregation technique called BORDA. Also, performance of various supervised machine learning classifiers is analyzed on the basis of different evaluation metrics . Experimental results state that for the undertaken problem, SVM with rbf kernel outperformed other classifiers namely, kNN, Random Forest , Gradient Boosted and Multi Layer Perceptron , attaining a maximum F-score of 94.57\% under the varied parameter settings.},
  archive      = {J_ASOC},
  author       = {Ravneet Kaur and Sarbjeet Singh and Harish Kumar},
  doi          = {10.1016/j.asoc.2020.106770},
  journal      = {Applied Soft Computing},
  pages        = {106770},
  shortjournal = {Appl. Soft. Comput.},
  title        = {TB-CoAuth: Text based continuous authentication for detecting compromised accounts in social networks},
  volume       = {97},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Neural CAPTCHA networks. <em>ASOC</em>, <em>97</em>, 106769.
(<a href="https://doi.org/10.1016/j.asoc.2020.106769">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To protect against attacks by malicious computer programs, many websites apply the CAPTCHA (short for completely automated public turing test to tell computers and humans apart) technique for security protection. The distortion, rotation and deformation of the characters or puzzles in CAPTCHAs increase the difficulty for machines to automatically recognize them. State-of-the-art CAPTCHA recognition algorithms generally use convolutional neural networks (CNNs) without considering the spatially sequential property of the characters/image features. To address this problem, we propose a new CAPTCHA recognition algorithm called neural CAPTCHA networks (NCNs). NCNs use a convolutional structure to extract CAPTCHA image features , and use bidirectional recurrent modules to learn the spatially sequential information in CAPTCHAs. We have applied NCNs to recognize text-based CAPTCHAs, including arithmetic operation , character recognition and character matching CAPTCHAs, and puzzle-based CAPTCHAs. For arithmetic operation and character recognition CAPTCHAs, we obtained 100\% accuracy on the SOIEC CAPTCHA dataset, for the character matching task, we obtained 99.3\% accuracy on the SOIEC CAPTCHA dataset, while for the puzzle-based CAPTCHAs, we obtained 98.13\% accuracy. These experimental results demonstrate the advantages of NCNs over related state-of-the-art approaches for CAPTCHA recognition.},
  archive      = {J_ASOC},
  author       = {Ying Ma and Guoqiang Zhong and Wen Liu and Jinxuan Sun and Kaizhu Huang},
  doi          = {10.1016/j.asoc.2020.106769},
  journal      = {Applied Soft Computing},
  pages        = {106769},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Neural CAPTCHA networks},
  volume       = {97},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A T1OWA and aspect-based model for customizing
recommendations on eCommerce. <em>ASOC</em>, <em>97</em>, 106768. (<a
href="https://doi.org/10.1016/j.asoc.2020.106768">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Online reviews have a significant impact on the decisions of consumers, providing valuable information which must be managed from two different perspectives: that of the user who reads the review and the people who gave those opinions. These two perspectives are the basis of the novel fuzzy aspect-based sentiment analysis approach described in this paper to recommend the most suitable products for a specific user. This approach consists of a T1OWA-based mechanism to characterize the user profile, which is able to model whether the user can be more influenced by negative opinions or positive opinions, a mechanism for determining their preferences, and a variation coefficient method for weighting the importance of the aspects of the product reviews. Combining these ideas, our model outperforms other well-known methods for ranking products, while also having the advantage of being adaptable to the preferences and characteristics of a specific user.},
  archive      = {J_ASOC},
  author       = {Jesus Serrano-Guerrero and Jose A. Olivas and Francisco P. Romero},
  doi          = {10.1016/j.asoc.2020.106768},
  journal      = {Applied Soft Computing},
  pages        = {106768},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A T1OWA and aspect-based model for customizing recommendations on eCommerce},
  volume       = {97},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Automatic paper writing based on a RNN and the TextRank
algorithm. <em>ASOC</em>, <em>97</em>, 106767. (<a
href="https://doi.org/10.1016/j.asoc.2020.106767">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Academic research is crucial to the development of science and technology and is an important factor that affects national strength. When writing an academic research paper, a rhetorical structure is typically used to present the paper’s ideas, but this task is quite difficult for junior researchers. To solve this problem, some studies have adopted text mining to assist with the writing, but the existing methods still require human intervention to generate sentences. Recently, due to the increasing maturity of deep learning technology and the ability to address the problem of automatic text generation, progress has been made in this area. The highly complex deep learning operations can correctly generate sequences and find correlations between sequences. When a user provides a few keywords and key sentences, the proposed algorithm can generate an introduction section for the user. The results show that the generated introduction is more coherent, clearer, and more fluent than existing summarization methods. In addition, the method proposed in this study improves the accuracy compared with traditional text extraction methods. The manuscript produced by this study has been evaluated to show that the study can produce a comprehensive introduction compared with previous studies.},
  archive      = {J_ASOC},
  author       = {Hei-Chia Wang and Wei-Ching Hsiao and Sheng-Han Chang},
  doi          = {10.1016/j.asoc.2020.106767},
  journal      = {Applied Soft Computing},
  pages        = {106767},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Automatic paper writing based on a RNN and the TextRank algorithm},
  volume       = {97},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Fault tolerant tracking control for nonlinear systems with
actuator failures through particle swarm optimization-based adaptive
dynamic programming. <em>ASOC</em>, <em>97</em>, 106766. (<a
href="https://doi.org/10.1016/j.asoc.2020.106766">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, an adaptive dynamic programming based fault tolerant tracking control (FTTC) method is proposed for nonlinear systems with actuator failures . To solve the tracking control problem, the considered system is augmented by combining the tracking error dynamics and the desired trajectory dynamics. By establishing a critic neural network , whose weight vector is updated by particle swarm optimization algorithm, the value function with discount factor is approximated to solve the Hamilton–Jacobi–Bellman equation, and the optimal tracking control law is derived. The neural network-based fault observer is established to compensate the control input online. Then, the augmented system states are guaranteed to be uniformly ultimately bounded under the developed FTTC method according to the Lyapunov stability theorem. Two simulation examples are provided to illustrate the effectiveness of the proposed method.},
  archive      = {J_ASOC},
  author       = {Xi Liu and Bo Zhao and Derong Liu},
  doi          = {10.1016/j.asoc.2020.106766},
  journal      = {Applied Soft Computing},
  pages        = {106766},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Fault tolerant tracking control for nonlinear systems with actuator failures through particle swarm optimization-based adaptive dynamic programming},
  volume       = {97},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Segmentation and classification of knee joint ultrasonic
image via deep learning. <em>ASOC</em>, <em>97</em>, 106765. (<a
href="https://doi.org/10.1016/j.asoc.2020.106765">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The knee is one of the most complicated joints in the human body, but it could be easily injured. Ultrasound imaging is an important technology for the diagnosis of the knee disease. To assist doctors in the treatment and reduce errors of judgment, we investigate the segmentation of disease regions and the automated identification of the typical knee joint diseases. First, we use deep learning to segment the Region of Interest (ROI). To solve the mis-segmentation and poor edge segmentation that occur when the ultrasound image is directly fed into the deep neural network , an image segmentation framework is proposed that integrates snake preprocessing, dilated convolution to expand the receptive fields, and multi-channel learning. Second, due to the small difference in features among various categories of ultrasound images, a hybrid algorithm is proposed based on the Resnet rough classification and quadratic training with graph embedding. Finally, the experiments show that the proposed image segmentation framework achieves 10\% greater accuracy than a common segmentation network . By visualizing the feature vectors extracted from the classification network, we verify that the feature vectors are closer on similar images after quadratic training by graph embedding. Employing the optimization with quadratic training, we increase the classification accuracy by 11\% compared to the Resnet approach.},
  archive      = {J_ASOC},
  author       = {Zhili Long and Xiaobing Zhang and Cong Li and Jin Niu and Xiaojun Wu and Zuohua Li},
  doi          = {10.1016/j.asoc.2020.106765},
  journal      = {Applied Soft Computing},
  pages        = {106765},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Segmentation and classification of knee joint ultrasonic image via deep learning},
  volume       = {97},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Complex network graph embedding method based on shortest
path and MOEA/d for community detection. <em>ASOC</em>, <em>97</em>,
106764. (<a href="https://doi.org/10.1016/j.asoc.2020.106764">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As one of the main applications of graph embedding, community detection has always been a hot issue in the field of complex network data mining. This paper presents a complex network graph embedding method based on the shortest path matrix and decomposition multi-objective evolutionary algorithm (SP-MOEA/D) for community detection, which can better reflect the network structure at the level of network community structure. Firstly, by calculating the shortest path matrix between nodes in the network, the node relationship matrix is obtained by adding the node similarity. Next, aiming at the problem of community detection in disconnected networks, a decomposition-based multi-objective optimization method is proposed to assign distances to unrelated nodes. Then, the network similarity matrix is calculated based on the relationship matrix of network nodes, and the low-dimensional vector representation of nodes is obtained by random surfing strategy and multi-dimensional scaling method. Finally, the community structure of the network can be detected based on the obtained node representation structure. Starting from the essence of network structure and the tightness between nodes, this method can reflect the relationship characteristics of network nodes more effectively, and then obtain the vector representation of nodes which can more accurately reflect the information of community structure in networks. The test results on 11 networks show that the node vector representation results obtained by this method can better reflect the community structure information in complex networks.},
  archive      = {J_ASOC},
  author       = {Weitong Zhang and Ronghua Shang and Licheng Jiao},
  doi          = {10.1016/j.asoc.2020.106764},
  journal      = {Applied Soft Computing},
  pages        = {106764},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Complex network graph embedding method based on shortest path and MOEA/D for community detection},
  volume       = {97},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Fuzzy linear regression based on approximate bayesian
computation. <em>ASOC</em>, <em>97</em>, 106763. (<a
href="https://doi.org/10.1016/j.asoc.2020.106763">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fuzzy linear regression with crisp inputs and fuzzy output data constitutes an important modeling problem. Basic strategies used to solve this problem, i.e., the possibilistic method and the least squares method , together with their extensions, have some drawbacks. The possibilistic methods put emphasis on an inclusion property while the least squares methods focus on a central tendency property. Therefore, many researchers work on combining these two methods to obtain a better performance. In this paper, in contrast to most existing techniques which treat fuzzy linear regression as an optimization problem , we set the problem of constructing a fuzzy linear regression model in Bayesian statistics and propose a new fuzzy linear regression method based on approximate Bayesian computation (ABC). The method applies the likelihood-free inference algorithm ABC to generate independent samples of unknown model coefficients from Bayesian posterior distribution . This overcomes difficulty of defining likelihood function in fuzzy environment. By adjusting a prior distribution and a threshold of the ABC algorithm, the proposed approach can flexibly balance the inclusion property of the possibilistic methods and the central tendency property of the least squares methods. The convergence property of the proposed ABC algorithm is verified by a numerical example. Two measuring criteria, i.e., a distance metric and a degree of fitting index, which indicate the central tendency property and the inclusion property, respectively, are introduced to evaluate the quality of regression results. Three numerical examples are applied to show the performances of the proposed method. The numerical results are also compared with those obtained by some classical and recently proposed approaches. Additionally, a practical engineering application example is used to illustrate effectiveness of the proposed method.},
  archive      = {J_ASOC},
  author       = {Ning Wang and Marek Reformat and Wen Yao and Yong Zhao and Xiaoqian Chen},
  doi          = {10.1016/j.asoc.2020.106763},
  journal      = {Applied Soft Computing},
  pages        = {106763},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Fuzzy linear regression based on approximate bayesian computation},
  volume       = {97},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). New interactive agent based reinforcement learning approach
towards smart generator bidding in electricity market with micro grid
integration. <em>ASOC</em>, <em>97</em>, 106762. (<a
href="https://doi.org/10.1016/j.asoc.2020.106762">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In order to suit the needs of the dynamically changing electricity market, software developers have developed various tools taking in to account the need of artificial intelligence for the electricity market entities. Algorithms in artificial intelligence are often divided into either supervised, unsupervised and reinforcement learning approach. A reinforcement learning when compared to supervised and unsupervised learning makes use of agent to learn from interaction with an environment and receives rewards based on the action it takes. It either exploits or explores in finding a solution. In the deregulated power market, the GenCos are modeled as agent by which the GenCo learns the market environment as agent and explores to get profited The Multi-agent based simulation is an effective method to incorporate this sort of intelligence and for providing efficient communication among the market entities. Using Multi-agent system, the problem existing in electricity market can be reduced since each entity problem can be solved by an individual agent. Multi-agent based reinforcement learning algorithm is used to handle the electricity market data. Here an agent based computational framework named Agent Based Modeling of Electricity Systems (AMES) under Java platform is developed for the design of electricity market. Market Agents balances the supply and demand through Market Clearing Price (no congestion) and Locational Marginal Price (congestion management) by performing optimal power flow. The agent also maximizes the profit of the Generator Companies (GenCo’s) through new learning strategy proposed using Variant Roth–Erev (VRE) interactive reinforcement learning method towards smart bidding among GenCo’s. The congestion relieving action in the transmission line and its effects on GenCo learning is discussed in this paper. The analysis is carried out on the electricity whole sale market functioning on a day-ahead basis developed by means of location and timing of injection of power. IEEE-3 bus system and IEEE-30 bus system with microgrid considered as non-dispatchable load is considered using agent based analysis. This technique helps the GenCo’s to attain possible high net earnings even with microgrid integration thus helps to relieve the congestion in the transmission lines.},
  archive      = {J_ASOC},
  author       = {Kiran P. Ph.D. ( Scholar ) and K.R.M. Vijaya Chandrakala Ph.D. ( Assistant Professor (SG) )},
  doi          = {10.1016/j.asoc.2020.106762},
  journal      = {Applied Soft Computing},
  pages        = {106762},
  shortjournal = {Appl. Soft. Comput.},
  title        = {New interactive agent based reinforcement learning approach towards smart generator bidding in electricity market with micro grid integration},
  volume       = {97},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). MTDE: An effective multi-trial vector-based differential
evolution algorithm and its applications for engineering design
problems. <em>ASOC</em>, <em>97</em>, 106761. (<a
href="https://doi.org/10.1016/j.asoc.2020.106761">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, an effective metaheuristic algorithm named multi-trial vector-based differential evolution (MTDE) is proposed. The MTDE is distinguished by introducing an adaptive movement step designed based on a new multi-trial vector approach named MTV, which combines different search strategies in the form of trial vector producers (TVPs). In the developed MTV approach, the TVPs are applied on their dedicated subpopulation, which are distributed by a winner-based distribution policy, and share their experiences efficiently by using a life-time archive. The MTV can be deployed by different types of TVPs, particularly, we use the MTV approach in the MTDE algorithm by three TVPs: representative based trial vector producer, local random based trial vector producer, and global best history based trial vector producer. Therefore, this study introduces the MTV approach to boost the performance of the MTDE and demonstrates its advantages in dealing with problems of different levels of complexity. The performance of the proposed MTDE algorithm is evaluated on CEC 2018 benchmark suite which include unimodal, multimodal, hybrid, and composition functions and four complex engineering design problems . The experimental and statistical results are compared with state-of-the-art metaheuristic algorithms: GWO, WOA , SSA, HHO, CoDE, EPSDE, QUATRE, and MKE . The results demonstrate that the MTDE algorithm shows improved performance and benefits from high accuracy of optimal solutions obtained.},
  archive      = {J_ASOC},
  author       = {Mohammad H. Nadimi-Shahraki and Shokooh Taghian and Seyedali Mirjalili and Hossam Faris},
  doi          = {10.1016/j.asoc.2020.106761},
  journal      = {Applied Soft Computing},
  pages        = {106761},
  shortjournal = {Appl. Soft. Comput.},
  title        = {MTDE: An effective multi-trial vector-based differential evolution algorithm and its applications for engineering design problems},
  volume       = {97},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Hyper-heuristics based on reinforcement learning, balanced
heuristic selection and group decision acceptance. <em>ASOC</em>,
<em>97</em>, 106760. (<a
href="https://doi.org/10.1016/j.asoc.2020.106760">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we introduce a multi-objective selection hyper-heuristic approach combining Reinforcement Learning , (meta)heuristic selection, and group decision-making as acceptance methods, referred to as Hyper-Heuristic based on Reinforcement LearnIng, Balanced Heuristic Selection and Group Decision AccEptance (HRISE), controlling a set of Multi-Objective Evolutionary Algorithms (MOEAs) as Low-Level (meta)Heuristics (LLHs). Along with the use of multiple MOEAs, we believe that having a robust LLH selection method as well as several move acceptance methods at our disposal would lead to an improved general-purpose method producing most adequate solutions to the problem instances across multiple domains. We present two learning hyper-heuristics based on the HRISE framework for multi-objective optimisation, each embedding a group decision-making acceptance method under a different rule: majority rule (HRISE_M) and responsibility rule (HRISE_R). A third hyper-heuristic is also defined where both a random LLH selection and a random move acceptance strategy are used. We also propose two variants of the late acceptance method and a new quality indicator supporting the initialisation of selection hyper-heuristics using low computational budget. An extensive set of experiments were performed using 39 multi-objective problem instances from various domains where 24 are from four different benchmark function classes, and the remaining 15 instances are from four different real-world problems. The cross-domain search performance of the proposed learning hyper-heuristics indeed turned out to be the best, particularly HRISE_R, when compared to three other selection hyper-heuristics, including a recently proposed one, and all low-level MOEAs each run in isolation.},
  archive      = {J_ASOC},
  author       = {Valdivino Alexandre de Santiago Júnior and Ender Özcan and Vinicius Renan de Carvalho},
  doi          = {10.1016/j.asoc.2020.106760},
  journal      = {Applied Soft Computing},
  pages        = {106760},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Hyper-heuristics based on reinforcement learning, balanced heuristic selection and group decision acceptance},
  volume       = {97},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Deep transfer with minority data augmentation for imbalanced
breast cancer dataset. <em>ASOC</em>, <em>97</em>, 106759. (<a
href="https://doi.org/10.1016/j.asoc.2020.106759">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Clinical diagnosis of breast cancer is a challenging problem in the biomedical domain. The BreakHis breast cancer histopathological image dataset consists of two classes: Benign (Minority class) and Malignant (Majority class). The imbalanced class distribution results in the degradation of performance of the classifier model due to biased classification towards the majority class. To tackle this problem, a novel learning strategy that involves a deep transfer network has been proposed in this paper, in collaboration with Deep Convolution Generative Adversarial network (DCGAN). DCGAN is used in the initial phase for data augmentation of the minority class only. The dataset, with the class distribution now balanced, is applied as input to the deep transfer network. The proposed deep transfer architecture has at its core, the initial pre-trained layers (until block 4 pool layer) of the VGG16 deep network architecture pre-trained on the ImageNet object classification dataset. The higher end of our transfer network comprises of Batch Normalization , 2D Convolutional (CONV2D) layer, Global Average Pooling 2D, Dropout and Dense layers that are added to enhance the network’s performance. Experiments on the benchmark BreakHis dataset for different magnification factors : 40X, 100X, 200X and 400X validate the efficiency of the proposed deep transfer learning approach due to the high scores achieved as compared to the state-of-the-art deep networks.},
  archive      = {J_ASOC},
  author       = {Manisha Saini and Seba Susan},
  doi          = {10.1016/j.asoc.2020.106759},
  journal      = {Applied Soft Computing},
  pages        = {106759},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Deep transfer with minority data augmentation for imbalanced breast cancer dataset},
  volume       = {97},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). CUS-heterogeneous ensemble-based financial distress
prediction for imbalanced dataset with ensemble feature selection.
<em>ASOC</em>, <em>97</em>, 106758. (<a
href="https://doi.org/10.1016/j.asoc.2020.106758">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to the global financial crisis occurred in 2008, with a large amount of companies troubling in financial distress, the machine learning-based prediction of this dilemma has shown economic stakeholders’ great practicability. In the field of machine learning , most of the previous studies only focus on the improvement of the imbalanced datasets sampling methods or the introduction of multiple classifiers in the constructing stage for prediction model. In view of this, this paper attempts to improve the scope and depth of ensemble to achieve better prediction performance for a severely imbalanced dataset of financial data of Chinese listed companies. For the first time, this paper combines the clustering-based under-sampling (CUS) with the gradient boosting decision tree (GBDT) to construct the model, which is used along with the current widely used extreme gradient boosting (XGBoost) as heterogeneous classifier to build heterogeneous ensemble in financial distress prediction. In addition, based on the idea of ensemble, this paper uses five feature selection methods based on different theoretical backgrounds to select features, and introduces ensemble from the whole process of feature selection, data preprocessing and model construction. In the comparative experience, the method proposed by us achieves the best performance on the test set. This study demonstrates the broad application of CUS for financial data processing and the superior generalization performance of the ensemble model relative to individual learners .},
  archive      = {J_ASOC},
  author       = {Xudong Du and Wei Li and Sumei Ruan and Li Li},
  doi          = {10.1016/j.asoc.2020.106758},
  journal      = {Applied Soft Computing},
  pages        = {106758},
  shortjournal = {Appl. Soft. Comput.},
  title        = {CUS-heterogeneous ensemble-based financial distress prediction for imbalanced dataset with ensemble feature selection},
  volume       = {97},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A joint optimization framework to semi-supervised RVFL and
ELM networks for efficient data classification. <em>ASOC</em>,
<em>97</em>, 106756. (<a
href="https://doi.org/10.1016/j.asoc.2020.106756">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to the inefficiency of gradient-based iterative ways in network training, randomization-based neural networks usually offer non-iterative closed form solutions . The random vector functional link (RVFL) and extreme learning machine (ELM) are two popular randomized networks which provide us unified frameworks for both regression and multi-class classification. Currently, existing studies on RVFL and ELM focused mainly on supervised tasks even though we usually have only a small number of labeled samples but a large number of unlabeled samples . Therefore, it is necessary to make both models appropriately utilize both labeled and unlabeled samples; that is, we should develop their semi-supervised extensions. In this paper, we propose a joint optimization framework to semi-supervised RVFL and ELM networks. In the formulated JOSRVFL (jointly optimized semi-supervised RVFL) and JOSELM, the output weight matrix and the label indicator matrix of the unlabeled samples can be jointly optimized in an iterative manner. We provide a novel approach to optimize the JOSRVFL and JOSELM objective functions. Extensive experiments on benchmark data sets and Electroencephalography-based emotion recognition tasks showed the excellent performance of the proposed JOSRVFL and JOSELM models. Moreover, because the direct input–output connections help to regularize the randomization , JOSRVFL could obtain superior performance to JOSELM in most cases.},
  archive      = {J_ASOC},
  author       = {Yong Peng and Qingxi Li and Wanzeng Kong and Feiwei Qin and Jianhai Zhang and Andrzej Cichocki},
  doi          = {10.1016/j.asoc.2020.106756},
  journal      = {Applied Soft Computing},
  pages        = {106756},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A joint optimization framework to semi-supervised RVFL and ELM networks for efficient data classification},
  volume       = {97},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Applying sentiment analysis to automatically classify
consumer comments concerning marketing 4Cs aspects. <em>ASOC</em>,
<em>97</em>, 106755. (<a
href="https://doi.org/10.1016/j.asoc.2020.106755">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rapid development of science and technology, consumers are used to searching online for evaluations before purchasing products. Manufacturers can also utilize such information like users’ usage habits, browsed websites, comments, messages, etc. to formulate marketing strategies suitable for their products. Several researches developed opinion mining on predicting the polarity of consumers’ comments, but few of them were from marketing point of view. In this regards, this study looks to establish an automated way to collect and analyze consumers’ comments in social networks, automatically classify them into marketing 4Cs and non-marketing categories from a large number of consumer comments, and divide the category of marketing 4Cs articles into four types of attribute dimensions to analyze emotional polarity. Based on the marketing theory of 4Cs and LDA topic analysis, this study extracted the characteristic keywords from the collected consumer reviews for corpus classification and sentiment polarity analysis. This study further establishes a feature keyword library for specific fields, hoping to improve the accuracy of sentiment analysis through these keywords, simplify the process of consumers’ searches for product evaluations, and facilitate consumers to search for helpful target information.},
  archive      = {J_ASOC},
  author       = {Hao-Chiang Koong Lin and Tao-Hua Wang and Guo-Chung Lin and Shu-Chen Cheng and Hong-Ren Chen and Yueh-Min Huang},
  doi          = {10.1016/j.asoc.2020.106755},
  journal      = {Applied Soft Computing},
  pages        = {106755},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Applying sentiment analysis to automatically classify consumer comments concerning marketing 4Cs aspects},
  volume       = {97},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Sentiment analysis of COVID-19 tweets by deep learning
classifiers—a study to show how popularity is affecting accuracy in
social media. <em>ASOC</em>, <em>97</em>, 106754. (<a
href="https://doi.org/10.1016/j.asoc.2020.106754">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {COVID-19 originally known as Corona VIrus Disease of 2019, has been declared as a pandemic by World Health Organization (WHO) on 11th March 2020. Unprecedented pressures have mounted on each country to make compelling requisites for controlling the population by assessing the cases and properly utilizing available resources. The rapid number of exponential cases globally has become the apprehension of panic, fear and anxiety among people. The mental and physical health of the global population is found to be directly proportional to this pandemic disease. The current situation has reported more than twenty four million people being tested positive worldwide as of 27th August, 2020. Therefore, it is the need of the hour to implement different measures to safeguard the countries by demystifying the pertinent facts and information. This paper aims to bring out the fact that tweets containing all handles related to COVID-19 and WHO have been unsuccessful in guiding people around this pandemic outbreak appositely. This study analyzes two types of tweets gathered during the pandemic times. In one case, around twenty three thousand most re-tweeted tweets within the time span from 1st Jan 2019 to 23rd March 2020 have been analyzed and observation says that the maximum number of the tweets portrays neutral or negative sentiments. On the other hand, a dataset containing 226, 668 tweets collected within the time span between December 2019 and May 2020 have been analyzed which contrastingly show that there were a maximum number of positive and neutral tweets tweeted by netizens. The research demonstrates that though people have tweeted mostly positive regarding COVID-19, yet netizens were busy engrossed in re-tweeting the negative tweets and that no useful words could be found in WordCloud or computations using word frequency in tweets. The claims have been validated through a proposed model using deep learning classifiers with admissible accuracy up to 81\%. Apart from these the authors have proposed the implementation of a Gaussian membership function based fuzzy rule base to correctly identify sentiments from tweets. The accuracy for the said model yields up to a permissible rate of 79\%.},
  archive      = {J_ASOC},
  author       = {Koyel Chakraborty and Surbhi Bhatia and Siddhartha Bhattacharyya and Jan Platos and Rajib Bag and Aboul Ella Hassanien},
  doi          = {10.1016/j.asoc.2020.106754},
  journal      = {Applied Soft Computing},
  pages        = {106754},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Sentiment analysis of COVID-19 tweets by deep learning Classifiers—A study to show how popularity is affecting accuracy in social media},
  volume       = {97},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Predicting political sentiments of voters from twitter in
multi-party contexts. <em>ASOC</em>, <em>97</em>, 106743. (<a
href="https://doi.org/10.1016/j.asoc.2020.106743">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Prior Twitter-based electoral research has mostly ignored multi-party contexts and ‘mix tweets’ that jointly mention more than one party. Hence, we investigate the complex nature of these mix tweets in a multi-party context, and we argue mix tweeting patterns of users implicitly capture their political opinions. We predict the political leaning of users based on their mix tweeting patterns in the context of the 2014 Indian General Election. We have agglomerated 2.4 million tweets from 0.15 million unique users. Next, we employ a multinomial logit regression model to test the hypothesized causal relation between mix tweeting patterns and the political leaning of users. Additionally, we also employ neural network-based algorithms to predict political leaning. Our study demonstrates that user-level mix-tweeting patterns can reveal the political opinions of Twitter users.},
  archive      = {J_ASOC},
  author       = {Aparup Khatua and Apalak Khatua and Erik Cambria},
  doi          = {10.1016/j.asoc.2020.106743},
  journal      = {Applied Soft Computing},
  pages        = {106743},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Predicting political sentiments of voters from twitter in multi-party contexts},
  volume       = {97},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Benchmarking large-scale continuous optimizers: The
bbob-largescale testbed, a COCO software guide and beyond.
<em>ASOC</em>, <em>97</em>, 106737. (<a
href="https://doi.org/10.1016/j.asoc.2020.106737">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Benchmarking of optimization solvers is an important and compulsory task for performance assessment that in turn can help in improving the design of algorithms. It is a repetitive and tedious task. Yet, this task has been greatly automatized in the past ten years with the development of the Comparing Continuous Optimizers platform ( COCO ). In this context, this paper presents a new testbed , called bbob-largescale , that contains functions ranging from dimension 20 to 640, compatible with and extending the well-known single-objective noiseless bbob test suite to larger dimensions. The test suite contains 24 single-objective functions in continuous domain, built to model well-known difficulties in continuous optimization and to test the scaling behavior of algorithms. To reduce the computational demand of the orthogonal search space transformations that appear in the bbob test suite, while retaining some desired properties, we use permuted block diagonal orthogonal matrices . The paper discusses implementation technicalities and presents a guide for using the test suite within the COCO platform and for interpreting the postprocessed output. The source code of the new test suite is available on GitHub as part of the open source COCO benchmarking platform.},
  archive      = {J_ASOC},
  author       = {Konstantinos Varelas and Ouassim Ait El Hara and Dimo Brockhoff and Nikolaus Hansen and Duc Manh Nguyen and Tea Tušar and Anne Auger},
  doi          = {10.1016/j.asoc.2020.106737},
  journal      = {Applied Soft Computing},
  pages        = {106737},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Benchmarking large-scale continuous optimizers: The bbob-largescale testbed, a COCO software guide and beyond},
  volume       = {97},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Capsule neural networks for structural damage localization
and quantification using transmissibility data. <em>ASOC</em>,
<em>97</em>, 106732. (<a
href="https://doi.org/10.1016/j.asoc.2020.106732">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {One of the current challenges in structural health monitoring (SHM) is to take the most advantage of large amounts of data to deliver accurate damage measurements and predictions. Deep Learning methods tackle these problems by finding complex relations hidden in the data available. Amongst these, Capsule Neural Networks (CapsNets) have recently been developed, achieving promising results in benchmark Deep Learning problems. In this paper, Capsule Networks are expanded to locate and to quantify structural damage. The proposed approach is evaluated in two case studies: a system with springs and masses that simulate a structure, and a beam with different damage scenarios. For both case studies, training and validation sets are created using Finite Element (FE) models and calibrated with experimental data, which is also used for testing. The main contributions of this study are: A novel CapsNets-based method for dual classification–regression task in SHM, analysis of both routing algorithms (dynamic routing and Expectation–Maximization routing) in the context of SHM, and analysis of generalization between FE models and real-life experiments. The results show that the proposed Capsule Networks with dynamic routing achieve better results than Convolutional Neural Networks (CNN), especially when it comes to false positive values.},
  archive      = {J_ASOC},
  author       = {Joaquín Figueroa Barraza and Enrique Lopez Droguett and Viviana Meruane Naranjo and Marcelo Ramos Martins},
  doi          = {10.1016/j.asoc.2020.106732},
  journal      = {Applied Soft Computing},
  pages        = {106732},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Capsule neural networks for structural damage localization and quantification using transmissibility data},
  volume       = {97},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). An optimized method to calculate approximations in dominance
based rough set approach. <em>ASOC</em>, <em>97</em>, 106731. (<a
href="https://doi.org/10.1016/j.asoc.2020.106731">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Classical Rough Set Theory (RST) is a prominent tool to deal with uncertainty of categorical data . However, it does not consider the preference order between the values of the attributes. Dominance Based Rough Set Approach (DRSA) provides dominance relation in this regard. Computation of the upper and lower approximation is a critical step in DRSA. However, computing these approximations is a computationally expensive task. Efficiently computing approximations will thus be helpful in reducing the execution time of algorithms using these approximations. In this paper, we have proposed an efficient approach to compute these measures. The proposed approach directly calculates approximations without considering the objects that do not play any role in the approximations. In our approach, one instance of a dataset is compared with another instance only once which avoids unnecessary comparisons. The proposed approach is compared with the conventional method using sixteen benchmark data sets from UCI. Results show that the proposed approach significantly reduces the execution time. The average reduction in the execution time was found to be almost 85\%. This approach also reduces the memory consumption by 75\%. The Big-O complexity is also reduced. These measures justify that the proposed approach is more effective and efficient as compared to the conventional DRSA.},
  archive      = {J_ASOC},
  author       = {Aleena Ahmad and Usman Qamar and Muhammad Summair Raza},
  doi          = {10.1016/j.asoc.2020.106731},
  journal      = {Applied Soft Computing},
  pages        = {106731},
  shortjournal = {Appl. Soft. Comput.},
  title        = {An optimized method to calculate approximations in dominance based rough set approach},
  volume       = {97},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Urban fire situation forecasting: Deep sequence learning
with spatio-temporal dynamics. <em>ASOC</em>, <em>97</em>, 106730. (<a
href="https://doi.org/10.1016/j.asoc.2020.106730">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Understanding the evolving discipline of urban fire situations is a basic but challenging task for urban security and fire-fighting decisions. Traditional methods forecast the urban fire situation through mathematical modeling and statistical learning, which could be interpretable but generally lack of efficiency and practicality. Recently, some deep neural network methodologies, especially convolutional neural network (CNN) and recurrent neural network (RNN), are presented as paradigms to capture dynamics in spatial–temporal complex phenomenon, which tally with the characteristics of fire situation forecasting. In this paper, we propose a novel deep sequence learning model as the fire situation forecasting network (FSFN) to better process the information and spatio-temporal correlations in regional urban fire alarm dataset. FSFN model integrates structures of Variational auto-encoders and context-based sequence generative model Seq2seq to obtain the latent representation of the fire situation and learn the spatio-temporal dynamics. Furthermore, we augment the network structure of FSFN from a simple deep sequence generative model to adversarial fire situation forecasting network with auxiliary information(Adversarial FSFN-A). The experimental studies demonstrate the effectiveness of Adversarial FSFN-A has superior spatio-temporal distribution prediction of multi-type urban fire situation.},
  archive      = {J_ASOC},
  author       = {Guangyin Jin and Qi Wang and Cunchao Zhu and Yanghe Feng and Jincai Huang and Xingchen Hu},
  doi          = {10.1016/j.asoc.2020.106730},
  journal      = {Applied Soft Computing},
  pages        = {106730},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Urban fire situation forecasting: Deep sequence learning with spatio-temporal dynamics},
  volume       = {97},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Two-stage grasp strategy combining CNN-based classification
and adaptive detection on a flexible hand. <em>ASOC</em>, <em>97</em>,
106729. (<a href="https://doi.org/10.1016/j.asoc.2020.106729">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Robotic autonomous grasping of food-related objects requires a nondestructive and safe grasp system for picking up various objects. A novel underactuated flexible hand consisting of a variable palm and four soft fingers is designed and manufactured to enhance the grasp space and deformability during interaction with unknown objects. A position deviation formulation is fitted to estimate the free bend deformation of soft fingers approximately through finite element analysis. A modular convolutional neural network is presented to identify the grasp directions, shape features and anticipated input pressure levels of novel objects for achieving multitarget classification. A vision-based adaptive detection method is proposed to obtain an accurate wrist orientation and the best grasp candidate by using two means of grasp planning (i.e. cross grasp planning and equidistant optimal grasp planning). A two-stage grasp strategy combining the classification and detection methods is developed as an effective solution to estimate the grasp configuration accurately. Results show that our flexible hand achieves 91.1\% success rate in a physical grasp experiment on a UR robot, thereby demonstrating the reliability and adaptability of our grasp approach. The target object can be identified and detected within 0.263 s, which indicates the suitability of our approach in real-time applications.},
  archive      = {J_ASOC},
  author       = {Xiaoyan Chen and Yilin Sun and Qiuju Zhang and Fei Liu},
  doi          = {10.1016/j.asoc.2020.106729},
  journal      = {Applied Soft Computing},
  pages        = {106729},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Two-stage grasp strategy combining CNN-based classification and adaptive detection on a flexible hand},
  volume       = {97},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). An efficient approach for forgery detection in digital
images using hilbert–huang​ transform. <em>ASOC</em>, <em>97</em>,
106728. (<a href="https://doi.org/10.1016/j.asoc.2020.106728">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image manipulation plays important role in fake news spreading and it may cause ethical, economic, or political problems for people and sometimes for countries. Image integrity verification becomes a very important research issue due to increasing the forged images on the Internet and social media. The objective of this paper is presenting an accurate approach for digital image forgery detection has enough capability to sense any small image tampering and robustness against image manipulation attacks. The first step in the proposed approach is converting the RGB image into YCbCr space, then, the Hilbert–Huang Transform (HHT) features extracted from the chrominance-red component Cr, then, three different classifiers; Support Vector Machines (SVM), K-Nearest Neighbors (KNN), and Artificial Neuron Networks (ANN) have been tested and compared for image classification into authentic or forged. The results are verified using Structural-Similarity (SSIM) to calculate the forgery detection accuracy. The proposed approach has been tested with seven different manipulation images datasets; CASIA-V1, CASIA-V2, MICC-F2000, MICC-F600, MICC-F220, CoMoFoD and additional dataset collected from different Internet websites and social media. Furthermore, the proposed approach has been tested against post-processing attacks such as; image compression , adding Gaussian noises or adjusting the contrast of the image. The results show that, SVM classifier has achieved the highest accuracy compared to ANN and KNN classifiers . The proposed approach has been compared with other published approaches, and the comparison proved its superiority over the previously published approaches.},
  archive      = {J_ASOC},
  author       = {H. Kasban and Sabry Nassar},
  doi          = {10.1016/j.asoc.2020.106728},
  journal      = {Applied Soft Computing},
  pages        = {106728},
  shortjournal = {Appl. Soft. Comput.},
  title        = {An efficient approach for forgery detection in digital images using Hilbert–Huang​ transform},
  volume       = {97},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A critical literature survey and prospects on tampering and
anomaly detection in image data. <em>ASOC</em>, <em>97</em>, 106727. (<a
href="https://doi.org/10.1016/j.asoc.2020.106727">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Concernings related to image security have increased in the last years. One of the main reasons relies on the replacement of conventional photography to digital images, once the development of new technologies for image processing , as much as it has helped in the evolution of many new techniques in forensic studies , it also provided tools for image tampering. In this context, many companies and researchers devoted many efforts towards methods for detecting such tampered images, mostly aided by autonomous intelligent systems. Therefore, this work focuses on introducing a rigorous survey contemplating the state-of-the-art literature on computer-aided tampered image detection using machine learning techniques , as well as evolutionary computation, neural networks , fuzzy logic, Bayesian reasoning , among others. Besides, it also contemplates anomaly detection methods in the context of images due to the intrinsic relation between anomalies and tampering. Moreover, it aims at recent and in-depth researches relevant to the context of image tampering detection, performing a survey over more than 100 works related to the subject, spanning across different themes related to image tampering detection. Finally, a critical analysis is performed over this comprehensive compilation of literature, yielding some research opportunities and discussing some challenges in an attempt to align future efforts of the community with the niches and gaps remarked in this exciting field.},
  archive      = {J_ASOC},
  author       = {Kelton A.P. da Costa and João P. Papa and Leandro A. Passos and Danilo Colombo and Javier Del Ser and Khan Muhammad and Victor Hugo C. de Albuquerque},
  doi          = {10.1016/j.asoc.2020.106727},
  journal      = {Applied Soft Computing},
  pages        = {106727},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A critical literature survey and prospects on tampering and anomaly detection in image data},
  volume       = {97},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Fault diagnosis based on the quantification of the fault
features in a rotary machine. <em>ASOC</em>, <em>97</em>, 106726. (<a
href="https://doi.org/10.1016/j.asoc.2020.106726">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fault diagnosis of rotary machinery is essential to fixing defective rotary machines and preventing rotary systems from breaking. Recently, fault diagnosis techniques have moved from traditional methods to artificial intelligence techniques. Many research groups have reported on the development of various artificial intelligence-based classifiers to improve diagnostic performance. In classifier design , selecting the datasets that include standard or fault features is essential to obtaining a high-performance classifier. However, there are few studies on the techniques to evaluate the quality of datasets numerically. In this study, we have developed a fault severity criterion to quantify the faultless and fault features of measured data. Using the proposed criterion, we have determined the dominant direction of representative faults in a rotary machine. The standard and fault data have been obtained, considering the dominant direction of each fault. The intrinsic mode function (IMF) that presents the fault features has been obtained by an empirical mode decomposition and a sensitive IMF selection criterion. Finally, we have designed an accurate and memory-efficient classifier using the extracted data and verified its performance by diagnosing a 7.5 kW servo motor . A conventional support vector machine has been used to verify the effects of the proposed algorithm on the classifier’s performance improvement. The developed classifier demonstrated an increase of performance up to an average of 51.9\%, compared with the classifiers using training datasets measured in an arbitrary direction, with the detection rate of 99.9\%. The study results suggest that the proposed classifier design technique based on the quantification of the fault features is useful for creating high-quality training datasets, machine learning , and deep learning-based classifiers.},
  archive      = {J_ASOC},
  author       = {Jongsu Lee and Byeonghui Park and Changwoo Lee},
  doi          = {10.1016/j.asoc.2020.106726},
  journal      = {Applied Soft Computing},
  pages        = {106726},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Fault diagnosis based on the quantification of the fault features in a rotary machine},
  volume       = {97},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Co-clustering optimization using artificial bee colony (ABC)
algorithm. <em>ASOC</em>, <em>97</em>, 106725. (<a
href="https://doi.org/10.1016/j.asoc.2020.106725">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents an Artificial Bee Colony (ABC) optimization based algorithm for co-clustering of high-dimensional data. The ABC algorithm is used for optimization problems including data clustering . We incorporate aspects of co-clustering by embedding it into the objective function used for clustering by the ABC algorithm . Instead of a linear metric, such as the Euclidean distance , we propose the use of higher order correlations to build similarity between rows and columns, each based on the other. This measure uses co-evolving similarities which when embedded into the objective function results in optimizing the co-clusters. The search space is also explored in the vicinity of the solutions produced by the ABC algorithm using three local search methods — the first is a heuristic based on computing the cluster means; the second uses the analytical gradient of the objective with respect to a centroid to find lower cost solutions in the vicinity; and, the third is a hybrid of the first two methods. Numerical experiments show significant improvement in the search for optimal clustering by incorporating new similarity metric and optimized local search method. Finally, the algorithm is shown to be highly scalable for parallel architectures for both distributed and shared memory systems . Theoretically, the best iso-efficiency function of Θ Θ ( p log p ) for fully connected network with p processors is also computed for the parallel algorithm .},
  archive      = {J_ASOC},
  author       = {Syed Fawad Hussain and Adeel Pervez and Masroor Hussain},
  doi          = {10.1016/j.asoc.2020.106725},
  journal      = {Applied Soft Computing},
  pages        = {106725},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Co-clustering optimization using artificial bee colony (ABC) algorithm},
  volume       = {97},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Online state space generation by a growing self-organizing
map and differential learning for reinforcement learning. <em>ASOC</em>,
<em>97</em>, 106723. (<a
href="https://doi.org/10.1016/j.asoc.2020.106723">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this research, we develop a method integrating a growing self-organizing map and differential learning system for online reinforcement learning which adaptively builds the state structure. In the conventional method, models and information on the environment are required beforehand, whereas the proposed method automatically estimates the state transitions from differentials of input signals and from these builds the state space without reference to prior information on the environment. Also, since it is an online learning method, the proposed method requires less computation and no batch memory. Through numerical experiments, we show that the proposed method has the same performance as the conventional method with the information given and that the learning time is shortened by abstraction of the state space.},
  archive      = {J_ASOC},
  author       = {Akira Notsu and Koji Yasuda and Seiki Ubukata and Katsuhiro Honda},
  doi          = {10.1016/j.asoc.2020.106723},
  journal      = {Applied Soft Computing},
  pages        = {106723},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Online state space generation by a growing self-organizing map and differential learning for reinforcement learning},
  volume       = {97},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A clustering-based symbiotic organisms search algorithm for
high-dimensional optimization problems. <em>ASOC</em>, <em>97</em>,
106722. (<a href="https://doi.org/10.1016/j.asoc.2020.106722">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposed a fast metaheuristic method for high-dimensional optimizations problem with only one-control parameter in the setting. Essentially, the innovation of the proposed method is to apply automatic k -means clustering on the initial solutions of symbiotic organisms search to create subpopulations. Only the selected elite solutions in each cluster to interact with one another across clusters in the proposed model. This new elite solution searching process can be considered as a combination of local and global searching based on the solution clusters. The proposed method was compared to six representative methods in 28 benchmark problems and 10 composition problems . Also, the proposed method was also compared with four clustering-based metaheuristic methods. The experimental results show that the proposed model is more efficient in its computation and has a better searching quality. For high-dimensional problems, the performances of the proposed method was compared with the original symbiotic organisms search up to 1000 dimensions. The results show that the proposed method can alleviate the dimensionality effect to produce better solution quality with relatively fast computation.},
  archive      = {J_ASOC},
  author       = {Chao-Lung Yang and Hendri Sutrisno},
  doi          = {10.1016/j.asoc.2020.106722},
  journal      = {Applied Soft Computing},
  pages        = {106722},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A clustering-based symbiotic organisms search algorithm for high-dimensional optimization problems},
  volume       = {97},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). G-CREM: A GRASP approach to solve the container relocation
problem for multibays. <em>ASOC</em>, <em>97</em>, 106721. (<a
href="https://doi.org/10.1016/j.asoc.2020.106721">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Container Relocation Problem for multiple bays consists of finding the minimum number of moves to load a set of stacked containers on a ship according to a given loading sequence, and in minimizing the crane’s working time for an entire yard of multiple bays. This is a crucial problem for every commercial port in the world given the maximum time requirements and the costs associated with the containers’ retrieval. In this paper, we propose a Greedy Randomized Adaptive Search Procedure to solve this problem. We use a myopic function specially designed to produce feasible candidate solutions with a structure that allows a local search procedure to optimize relocations. In order to validate our approach, we use a large set of well-known Container Relocation Problems for multiples bays, as well as a statistical analysis of our results. Our experiments show new bounds for various instances.},
  archive      = {J_ASOC},
  author       = {Camila Díaz Cifuentes and María Cristina Riff},
  doi          = {10.1016/j.asoc.2020.106721},
  journal      = {Applied Soft Computing},
  pages        = {106721},
  shortjournal = {Appl. Soft. Comput.},
  title        = {G-CREM: A GRASP approach to solve the container relocation problem for multibays},
  volume       = {97},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Heterogenous adaptive ant colony optimization with 3-opt
local search for the travelling salesman problem. <em>ASOC</em>,
<em>97</em>, 106720. (<a
href="https://doi.org/10.1016/j.asoc.2020.106720">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The majority of optimization algorithms require proper parameter tuning to achieve the best performance. However, it is well-known that parameters are problem-dependent as different problems or even different instances have different optimal parameter settings. Parameter tuning through the testing of parameter combinations is a computationally expensive procedure that is infeasible on large-scale real-world problems. One method to mitigate this is to introduce adaptivity into the algorithm to discover good parameter settings during the search. Therefore, this study introduces an adaptive approach to a heterogeneous ant colony population that evolves the alpha and beta controlling parameters for ant colony optimization (ACO) to locate near-optimal solutions. This is achievable by introducing a set of rules for parameter adaptation to occur in order for the parameter values to be close to the optimal values by exploring and exploiting both the parameter and fitness landscape during the search to reflect the dynamic nature of search. In addition, the 3-opt local search heuristic is integrated into the proposed approach to further improve fitness. An empirical analysis of the proposed algorithm tested on a range of Travelling Salesman Problem (TSP) instances shows that the approach has better algorithmic performance when compared against state-of-the-art algorithms from the literature.},
  archive      = {J_ASOC},
  author       = {Ahamed Fayeez Tuani and Edward Keedwell and Matthew Collett},
  doi          = {10.1016/j.asoc.2020.106720},
  journal      = {Applied Soft Computing},
  pages        = {106720},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Heterogenous adaptive ant colony optimization with 3-opt local search for the travelling salesman problem},
  volume       = {97},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Joint cell formation, cell scheduling, and group layout
problem in virtual and classical cellular manufacturing systems.
<em>ASOC</em>, <em>97</em>, 106719. (<a
href="https://doi.org/10.1016/j.asoc.2020.106719">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cell formation, cell scheduling, and group layout are three important problems in designing and configuring a Cellular Manufacturing System (CMS). This paper addresses the integration of these problems in virtual and classical CMSs considering alternative processing routes. The objective is to minimize the total handling costs and cycle time. Due to the computational complexity of the problem, hybrid metaheuristic algorithms are proposed to solve the problem. Depending on the type of cells, which is either classical or virtual, an encoding scheme is proposed to effectively represent candidate solutions. Placement algorithms are developed to obtain the layout from an encoded solution; these algorithms are either based on running a heuristic or a linear program . A computer software, called ICFLSD (Integrated Cell Formation, Layout, and Scheduling Designer) is developed to simplify the problem-solving process from the data entry to getting results. Numerical examples adopted from the literature are solved using the ICFLSD and CPLEX to assess the performance of the metaheuristic algorithms . The comparison results demonstrated the superiority of the simulated annealing to the other solution approaches considered in this study.},
  archive      = {J_ASOC},
  author       = {Kamran Forghani and S.M.T. Fatemi Ghomi},
  doi          = {10.1016/j.asoc.2020.106719},
  journal      = {Applied Soft Computing},
  pages        = {106719},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Joint cell formation, cell scheduling, and group layout problem in virtual and classical cellular manufacturing systems},
  volume       = {97},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A self-organizing deep auto-encoder approach for
classification of complex diseases using SNP genomics data.
<em>ASOC</em>, <em>97</em>, 106718. (<a
href="https://doi.org/10.1016/j.asoc.2020.106718">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, many Machine Learning algorithms have been utilized to identify significant Single Nucleotide Polymorphisms (SNPs) in various human diseases. However, some principal obstacles are challenging in the field of SNP detection and healthy-patient classification. The curse of dimensionality is the main challenge. On the other hand, the number of samples is decidedly smaller than the number of SNPs. In addition, the number of healthy and patient samples can be unequal. These challenges make the feature selection and classification very difficult. The main goal of the current study is the combination of the various algorithms to find out the most effective way of SNP data analysis. Therefore, an efficient method is proposed to identify significant SNPs and classify healthy and patient samples. In this regard, firstly, the Mean Encoding, as an intelligent method, is utilized to convert the nominal SNP data to numeric. Then a two-step filter method is used for feature selection, which removes the irrelevant and redundant features. Finally, the proposed deep auto-encoder is employed to classify so that it can construct its structure based on input data, automatically. To evaluate, we apply the proposed approach to five different SNP datasets, including thyroid cancer, mental retardation , breast cancer, colorectal cancer, and autism, which obtained from the Gene Expression Omnibus (GEO) dataset. The proposed method has succeeded in feature selection and classification so that it can classify healthy and patient samples based on selected features in thyroid cancer, mental retardation , breast cancer, colorectal cancer, and autism with 100\%, 94.4\%, 100\%, 96\%, and 99.1\% accuracy, respectively. The results indicate that it has succeeded with high efficiency, compared with other published works.},
  archive      = {J_ASOC},
  author       = {Saeed Pirmoradi and Mohammad Teshnehlab and Nosratollah Zarghami and Arash Sharifi},
  doi          = {10.1016/j.asoc.2020.106718},
  journal      = {Applied Soft Computing},
  pages        = {106718},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A self-organizing deep auto-encoder approach for classification of complex diseases using SNP genomics data},
  volume       = {97},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Mutation operators for genetic programming using monte carlo
tree search. <em>ASOC</em>, <em>97</em>, 106717. (<a
href="https://doi.org/10.1016/j.asoc.2020.106717">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Expansion is a novel mutation operator for Genetic Programming (GP). It uses Monte Carlo simulation to repeatedly expand and evaluate programs using unit instructions, which extends the search beyond the immediate – often misleading – horizon of offspring programs. To evaluate expansion, a standard Koza-style tree-based representation is used and a comparison is carried out between expansion and sub-tree crossover as well as point mutation. Using a diverse set of benchmark symbolic regression problems, we prove that expansion provides for better fitness performance than point mutation, when included with crossover. Expansion also provides a significant boost to fitness when compared to GP using crossover only, with similar or lower levels of program bloat. Despite expansion’s success in improving evolutionary performance, it does not eliminate the problem of program bloat. In response, an analogous genetic operator, reduction, is proposed and tested for its ability to keep a check on program size. We conclude that the best fitness can be achieved by including these three operators in GP: crossover, point mutation and expansion.},
  archive      = {J_ASOC},
  author       = {Mohiul Islam and Nawwaf Kharma and Peter Grogono},
  doi          = {10.1016/j.asoc.2020.106717},
  journal      = {Applied Soft Computing},
  pages        = {106717},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Mutation operators for genetic programming using monte carlo tree search},
  volume       = {97},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A fast instance selection method for support vector machines
in building extraction. <em>ASOC</em>, <em>97</em>, 106716. (<a
href="https://doi.org/10.1016/j.asoc.2020.106716">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Training support vector machines (SVMs) for pixel-based feature extraction purposes from aerial images requires selecting representative pixels (instances) as a training dataset. In this research, locality-sensitive hashing (LSH) is adopted for developing a new instance selection method which is referred to as D R . L S H DR.LSH . The intuition of D R . L S H DR.LSH rests on rapidly finding similar and redundant training samples and excluding them from the original dataset. The simple idea of this method alongside its linear computational complexity make it expeditious in coping with massive training data (millions of pixels). D R . L S H DR.LSH is benchmarked against two recently proposed methods on a dataset for building extraction with 23, 750, 000 samples obtained from the fusion of aerial images and point clouds. The results reveal that D R . L S H DR.LSH outperforms them in terms of both preservation rate and maintaining the generalization ability (classification loss). The source code of D R . L S H DR.LSH can be found in https://github.com/mohaslani/DR.LSH .},
  archive      = {J_ASOC},
  author       = {Mohammad Aslani and Stefan Seipel},
  doi          = {10.1016/j.asoc.2020.106716},
  journal      = {Applied Soft Computing},
  pages        = {106716},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A fast instance selection method for support vector machines in building extraction},
  volume       = {97},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A data augmentation method for human action recognition
using dense joint motion images. <em>ASOC</em>, <em>97</em>, 106713. (<a
href="https://doi.org/10.1016/j.asoc.2020.106713">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the development of deep learning and neural network techniques, human action recognition has made great progress in recent years. However, it remains challenging to analyse temporal information and identify human actions with few training samples. In this paper, an effective motion image called a dense joint motion image (DJMI) was proposed to transform an action to an image. Our method was compared with state-of-the-art methods, and its contributions are mainly reflected in three characteristics. First, in contrast to the current classic joint trajectory map (JTM), every pixel of the DJMI is useful and contains essential spatio-temporal information. Thus, the input parameters of the deep neural network (DNN) are reduced by an order of magnitude, and the efficiency of action recognition is improved. Second, each frame of an action video is encoded as an independent slice of the DJMI, which avoids the information loss caused by action trajectory overlap. Third, by using DJMIs, proven algorithms for graphics and images can be used to generate training samples. Compared with the original image, the generated DJMIs contain new and different spatio-temporal information, which enables DNNs to be trained well on very few samples. Our method was evaluated on three benchmark datasets, namely, Florence-3D, UTKinect-Action3D and MSR Action3D. The results showed that our method achieved a recognition speed of 37 fps with competitive accuracy on these datasets. The time efficiency and few-shot learning capability of our method enable it to be used in real-time surveillance.},
  archive      = {J_ASOC},
  author       = {Leiyue Yao and Wei Yang and Wei Huang},
  doi          = {10.1016/j.asoc.2020.106713},
  journal      = {Applied Soft Computing},
  pages        = {106713},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A data augmentation method for human action recognition using dense joint motion images},
  volume       = {97},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). MetaNChemo: A meta-heuristic neural-based framework for
chemometric analysis. <em>ASOC</em>, <em>97</em>, 106712. (<a
href="https://doi.org/10.1016/j.asoc.2020.106712">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Gas sensors are present in almost every environmental monitoring application. They can be realized using different materials and production techniques while exploiting different physical/chemical processes to detect one or more target gas. Historically, their research and development are mainly driven by the materials science community. However, the seamless combination of artificial intelligence techniques with gas sensors in cyber–physical systems is recently attracting the interest of various computer science communities. In this paper, we propose MetaNChemo, a multidisciplinary end-to-end framework to build a chemometric system from the very production of chemoresistive sensors, through the data sampling and pre-processing phases necessary to calibrate such sensors, to the identification and the assessment of the most suitable artificial intelligence models able to supports the detection of the concentration of a target gas in the air. Without loss of generality, we focus our attention on carbon monoxide (CO) as our target gas. However, materials scientists may take full advantage of MetaNChemo, since its data-driven approach can accommodate very specific requirements of those environmental monitoring applications that have to properly detect one or more target gas. By resorting to state-of-the-art meta-heuristic techniques, we identify and train tiny neural networks (20-50 weights) able to achieve F1-scores greater than 0.95 over real environmental test conditions.},
  archive      = {J_ASOC},
  author       = {Mattia Antonini and Andrea Gaiardo and Massimo Vecchio},
  doi          = {10.1016/j.asoc.2020.106712},
  journal      = {Applied Soft Computing},
  pages        = {106712},
  shortjournal = {Appl. Soft. Comput.},
  title        = {MetaNChemo: A meta-heuristic neural-based framework for chemometric analysis},
  volume       = {97},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Deep reinforcement learning approach for MPPT control of
partially shaded PV systems in smart grids. <em>ASOC</em>, <em>97</em>,
106711. (<a href="https://doi.org/10.1016/j.asoc.2020.106711">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Photovoltaic systems (PV) are having an increased importance in modern smart grids systems . Usually, in order to maximize the energy output of the PV arrays a maximum power point tracking (MPPT) algorithm is used. However, once deployed, weather conditions such as clouds can cause shades in the PV arrays affecting the dynamics of each panel differently. These conditions directly affect the available energy output of the arrays and in turn make the MPPT task extremely difficult. For these reasons, under partial shading conditions, it is necessary to have algorithms that are able to learn and adapt online to the changing state of the system. In this work we propose the use of deep reinforcement learning (DRL) techniques to address the MPPT problem of a PV array under partial shading conditions. We develop a model free RL algorithm to maximize the efficiency in MPPT control. The agent’s policy is parameterized by neural networks , which take the sensory information as input and directly output the control signal. Furthermore, a PV environment under shading conditions was developed in the open source OpenAI Gym platform and is made available in an open repository. Several tests are performed, using the developed simulated environment, to test the robustness of the proposed control strategies to different climate conditions. The obtained results show the feasibility of our proposal with a successful performance with fast responses and stable behaviors. The best results for the presented methodology show that the maximum operating power point achieved has a deviation less than 1\% compared to the theoretical maximum power point .},
  archive      = {J_ASOC},
  author       = {Luis Avila and Mariano De Paula and Maximiliano Trimboli and Ignacio Carlucho},
  doi          = {10.1016/j.asoc.2020.106711},
  journal      = {Applied Soft Computing},
  pages        = {106711},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Deep reinforcement learning approach for MPPT control of partially shaded PV systems in smart grids},
  volume       = {97},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). On pairing huber support vector regression. <em>ASOC</em>,
<em>97</em>, 106708. (<a
href="https://doi.org/10.1016/j.asoc.2020.106708">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, a novel and efficient pairing support vector regression learning method using ε − ε− insensitive Huber loss function (PHSVR) is proposed where the ε − ε− insensitive zone having flexible shape is determined by tightly fitting the training samples. Our approach leads to solving a pair of unconstrained minimization problems in primal and the solutions are obtained by two algorithms: a functional iterative (FPHSVR) and Newton iterative (NPHSVR) algorithms. The finite termination of the Newton method to its global minimum solution is proved. The significant advantages of the proposed method are the robustness, generalization ability and learning speed. Experiments performed on a series of synthetic data sets, polluted by different types of noise including heteroscedastic noise and outliers, and on real-world benchmark data sets confirm the effectiveness and superiority of the proposed method.},
  archive      = {J_ASOC},
  author       = {S. Balasundaram and Subhash Chandra Prasad},
  doi          = {10.1016/j.asoc.2020.106708},
  journal      = {Applied Soft Computing},
  pages        = {106708},
  shortjournal = {Appl. Soft. Comput.},
  title        = {On pairing huber support vector regression},
  volume       = {97},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Design of a two-echelon freight distribution system in an
urban area considering third-party logistics and loading–unloading
zones. <em>ASOC</em>, <em>97</em>, 106707. (<a
href="https://doi.org/10.1016/j.asoc.2020.106707">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This research examines the problem of designing a two-echelon freight distribution system in a dense urban area that considers third-party logistics (TPL) and loading–unloading zones (LUZs). The proposed system takes advantage of outsourcing the last mile deliveries to a TPL provider and utilizing LUZs as temporary intermediate facilities instead of using permanent intermediate facilities to consolidate freight. A mathematical model and a simulated annealing (SA) algorithm are developed to solve the problem. The efficiency and effectiveness of the proposed SA heuristic are verified by testing it on existing benchmark instances. Computational results show that the performance of the proposed SA is comparable with that of another state-of-the-art algorithm. The model and algorithm are then used to design a two-echelon freight distribution system in Taipei City, Taiwan. Results of the case study indicate that the proposed model and algorithm provide a better distribution system.},
  archive      = {J_ASOC},
  author       = {Vincent F. Yu and Winarno and Shih-Wei Lin and Aldy Gunawan},
  doi          = {10.1016/j.asoc.2020.106707},
  journal      = {Applied Soft Computing},
  pages        = {106707},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Design of a two-echelon freight distribution system in an urban area considering third-party logistics and loading–unloading zones},
  volume       = {97},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Motion-encoded particle swarm optimization for moving target
search using UAVs. <em>ASOC</em>, <em>97</em>, 106705. (<a
href="https://doi.org/10.1016/j.asoc.2020.106705">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a novel algorithm named the motion-encoded particle swarm optimization (MPSO) for finding a moving target with unmanned aerial vehicles (UAVs). From the Bayesian theory, the search problem can be converted to the optimization of a cost function that represents the probability of detecting the target. Here, the proposed MPSO is developed to solve that problem by encoding the search trajectory as a series of UAV motion paths evolving over the generation of particles in a PSO algorithm. This motion-encoded approach allows for preserving important properties of the swarm including the cognitive and social coherence, and thus resulting in better solutions. Results from extensive simulations with existing methods show that the proposed MPSO improves the detection performance by 24\% and time performance by 4.71 times compared to the original PSO , and moreover, also outperforms other state-of-the-art metaheuristic optimization algorithms including the artificial bee colony (ABC), ant colony optimization (ACO), genetic algorithm (GA), differential evolution (DE), and tree-seed algorithm (TSA) in most search scenarios. Experiments have been conducted with real UAVs in searching for a dynamic target in different scenarios to demonstrate MPSO merits in a practical application.},
  archive      = {J_ASOC},
  author       = {Manh Duong Phung and Quang Phuc Ha},
  doi          = {10.1016/j.asoc.2020.106705},
  journal      = {Applied Soft Computing},
  pages        = {106705},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Motion-encoded particle swarm optimization for moving target search using UAVs},
  volume       = {97},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Model predictive control of non-domestic heating using
genetic programming dynamic models. <em>ASOC</em>, <em>97</em>, 106695.
(<a href="https://doi.org/10.1016/j.asoc.2020.106695">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a novel approach to obtaining dynamic nonlinear models using genetic programming (GP) for the model predictive control (MPC) of the indoor temperatures of buildings. Currently, the large-scale adoption of MPC in buildings is economically unviable due to the time and cost involved in the design and tuning of predictive models by expert control engineers. We show that GP is able to automate this process, and have performed open-loop system identification over the data produced by an industry grade building simulator. The simulated building was subject to an amplitude modulated pseudo-random binary sequence (APRBS), which allows the collected data to be sufficiently informative to capture the underlying system dynamics under relevant operating conditions. In this initial report, we detail how we employed GP to construct the predictive model for MPC for heating a single-zone building in simulation, and report results of using this model for controlling the internal environmental conditions of the simulated single-zone building. We conclude that GP shows great promise for producing models that allow the MPC of building to achieve the desired temperature band in a single zone space.},
  archive      = {J_ASOC},
  author       = {Tiantian Dou and Yuri Kaszubowski Lopes and Peter Rockett and Elizabeth A. Hathway and Esmail Saber},
  doi          = {10.1016/j.asoc.2020.106695},
  journal      = {Applied Soft Computing},
  pages        = {106695},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Model predictive control of non-domestic heating using genetic programming dynamic models},
  volume       = {97},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). ED-ACNN: Novel attention convolutional neural network based
on encoder–decoder framework for human traffic prediction.
<em>ASOC</em>, <em>97</em>, 106688. (<a
href="https://doi.org/10.1016/j.asoc.2020.106688">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate human traffic prediction , as a vital component of an intelligent transportation system (ITS), can not only reduce traffic congestion and resource consumption, but also provide a foundation for other tasks, such as risk assessment and public safety. Owing to the rapid development of computing power, massive data storage, and parallelization , deep-learning techniques, especially convolutional neural networks (CNNs), have become a powerful tool for traffic-flow forecasting. However, most of these methods in the literature over-emphasize the accuracy of traffic-flow forecasting and ignore its efficiency. It is often beneficial to develop smaller models (e.g., fewer model parameters) to improve efficiency. In this work, taking into account the efficiency and accuracy of the prediction, a novel attention CNN based on an encoder–decoder framework, called ED-ACNN, is proposed. First, the convolutional layer is considered the coding layer to extract spatial and temporal correlations. Then, the deconvolution layer as a decoding layer is expertly designed to reconstruct the future traffic-flow image. Next, the attention mechanism is introduced into the proposed model to capture the correlation between the spatial traffic-flow images’ channels. Finally, for the three characteristics of c l o s e n e s s closeness , p e r i o d period , and t r e n d trend , it is concluded that the c l o s e n e s s closeness feature is the most significant for human traffic prediction in the proposed approach. An extensive experimental evaluation of two types of real-world crowd flow (Beijing and New York City) is presented, and the results show that the proposed method can be very competitive with state-of-the-art baselines.},
  archive      = {J_ASOC},
  author       = {Bin Pu and Yuan Liu and Ningbo Zhu and Kenli Li and Keqin Li},
  doi          = {10.1016/j.asoc.2020.106688},
  journal      = {Applied Soft Computing},
  pages        = {106688},
  shortjournal = {Appl. Soft. Comput.},
  title        = {ED-ACNN: Novel attention convolutional neural network based on encoder–decoder framework for human traffic prediction},
  volume       = {97},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Efficient construction of an approximate similarity graph
for minimum spanning tree based clustering. <em>ASOC</em>, <em>97</em>,
106676. (<a href="https://doi.org/10.1016/j.asoc.2020.106676">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Minimum spanning tree (MST) based unsupervised learning techniques are popular due to their ability to identify intrinsic clusters of heterogeneous structures. One of the important factors which affects their effectiveness is how to construct a sparse similarity graph which can effectively capture the local neighborhood information in sub quadratic time . In this paper, we propose a technique which efficiently uses the local nearest neighbors of data points to construct a similarity graph . The proposed approach consists of two steps. In the first step, the dataset is divided into groups using the dispersion level of data points and then all pair intra-partition edges are computed. In the second step, the boundary data points across the neighboring partitions are considered to produce inter-partition edges for increasing the accuracy. The resulting graph is generated by considering all intra- and inter-partition edges. Approximate MST of the similarity graph is constructed to show its efficacy. Experimental analyses demonstrate that the similarity graph captures shorter edges and discards the longest edges, based on graph diameter, all pair shortest path and weight error of MST. Moreover, the quality of the approximate MST is also validated by applying clustering technique on various synthetic and real data sets of different characteristics and cluster quality analyses demonstrate that it has a satisfying performance over other competing approximate MST construction techniques.},
  archive      = {J_ASOC},
  author       = {Gaurav Mishra and Sraban Kumar Mohanty},
  doi          = {10.1016/j.asoc.2020.106676},
  journal      = {Applied Soft Computing},
  pages        = {106676},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Efficient construction of an approximate similarity graph for minimum spanning tree based clustering},
  volume       = {97},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Special issue on evolutionary computer vision, image
processing and pattern recognition. <em>ASOC</em>, <em>97</em>, 106675.
(<a href="https://doi.org/10.1016/j.asoc.2020.106675">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_ASOC},
  author       = {Stefano Cagnoni ( Special issue guest editors ) and Harith Al-Sahaf and Yanan Sun and Bing Xue and Mengjie Zhang},
  doi          = {10.1016/j.asoc.2020.106675},
  journal      = {Applied Soft Computing},
  pages        = {106675},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Special issue on evolutionary computer vision, image processing and pattern recognition},
  volume       = {97},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). HANMRE - an authenticated encryption secure against
side-channel attacks for nonce-misuse and lightweight approaches.
<em>ASOC</em>, <em>97</em>, 106663. (<a
href="https://doi.org/10.1016/j.asoc.2020.106663">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Side-channel threat is a form of cryptanalysis that takes advantage of secret information leaked during program implementations, through measurement and evaluation of systematic parameters, such as execution time, power consumption and electromagnetic field (EMF) radiation. Since various side-channel analysis techniques have applied successfully in gathering data and extracting cryptographic keys on variety of devices and platforms, including smartphones, smart cards, tablets, TVs, FPGAs and CPUs, these attacks constitute a significant risk to the security of cryptographic systems. Eliminating serious leakages is a major approach to mitigate side-channel vulnerabilities, in particular Simple Power Analysis (SPA) and Differential Power Analysis (DPA). During the last decade, several research aimed at securing cryptographic primitive algorithms against side-channel attacks, and validating possible countermeasures under assumption which its computational complexity can be estimated precisely. In this paper, we propose a hash-based authenticated nonce-misuse resistant encryption, namely HANMRE which is adaptable for a lightweight leakage resilient authenticated encryption with associated data (AEAD) scheme. The HANMRE construction has been designed for the side-channel security achievement (including SPA and DPA attacks) and highly integrated for restrained environments with limited resource. The advantage of this scheme is ensuring the strong security developed in misuse-resistant schemes against general adversaries for authenticated encryption [1] . It also presents reasonable implementation results (especially long message handling) compared to existing authenticated encryption schemes and is expected to be a novel idea for better approaches of authenticated encryption mechanisms design in the future.},
  archive      = {J_ASOC},
  author       = {Song Dat Phuc Tran and Byoungjin Seok and Changhoon Lee},
  doi          = {10.1016/j.asoc.2020.106663},
  journal      = {Applied Soft Computing},
  pages        = {106663},
  shortjournal = {Appl. Soft. Comput.},
  title        = {HANMRE - an authenticated encryption secure against side-channel attacks for nonce-misuse and lightweight approaches},
  volume       = {97},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). EvoTSC: An evolutionary computation-based traffic signal
controller for large-scale urban transportation networks. <em>ASOC</em>,
<em>97</em>, 106640. (<a
href="https://doi.org/10.1016/j.asoc.2020.106640">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traffic Signal Control (TSC) is a crucial component in the modern intelligent transportation systems . Typically, the TSC can be formulated as a bilevel optimization problem , which is comprised of the signal timing and the traffic assignment modules. The problem is challenging that the existing approaches usually endure a huge computational cost. As a result, many TSC approaches focus on relatively simple and small transportation networks, which do not satisfy the practical situations. To address above issues, this paper proposes an evolvable TSC (EvoTSC) system, which adopts nature-inspired techniques to realize the global optimization of the TSC in large-scale urban transportation networks. Particularly, it involves two evolutionary computation components. The first component is an Adaptive Differential Evolution (ADE) to optimize the signal timing. Meanwhile, the traffic assignment process is included in the solution evaluation of the ADE to react to the traffic flow dynamics. The second component is an off-line Niching Ant Colony Optimization (NACO), which aims to provide the traffic assignment with sets of multiple promising routes beforehand. This way, the EvoTSC system avoids repeatedly building candidate routes for the traffic assignment, which can greatly save the computational cost of the ADE to evaluate each solution in a large-scale transportation network. In experiments, we carry out comparisons of different TSC approaches on both synthetic and practical transportation networks. The experimental results validate the effectiveness of the proposed EvoTSC system.},
  archive      = {J_ASOC},
  author       = {Wei-Li Liu and Yue-Jiao Gong and Wei-Neng Chen and Jun Zhang},
  doi          = {10.1016/j.asoc.2020.106640},
  journal      = {Applied Soft Computing},
  pages        = {106640},
  shortjournal = {Appl. Soft. Comput.},
  title        = {EvoTSC: An evolutionary computation-based traffic signal controller for large-scale urban transportation networks},
  volume       = {97},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A novel medical diagnosis model for COVID-19 infection
detection based on deep features and bayesian optimization.
<em>ASOC</em>, <em>97</em>, 106580. (<a
href="https://doi.org/10.1016/j.asoc.2020.106580">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A pneumonia of unknown causes, which was detected in Wuhan, China, and spread rapidly throughout the world, was declared as Coronavirus disease 2019 (COVID-19). Thousands of people have lost their lives to this disease. Its negative effects on public health are ongoing. In this study, an intelligence computer-aided model that can automatically detect positive COVID-19 cases is proposed to support daily clinical applications. The proposed model is based on the convolution neural network (CNN) architecture and can automatically reveal discriminative features on chest X-ray images through its convolution with rich filter families, abstraction, and weight-sharing characteristics. Contrary to the generally used transfer learning approach, the proposed deep CNN model was trained from scratch. Instead of the pre-trained CNNs , a novel serial network consisting of five convolution layers was designed. This CNN model was utilized as a deep feature extractor. The extracted deep discriminative features were used to feed the machine learning algorithms, which were k-nearest neighbor, support vector machine (SVM), and decision tree . The hyperparameters of the machine learning models were optimized using the Bayesian optimization algorithm . The experiments were conducted on a public COVID-19 radiology database. The database was divided into two parts as training and test sets with 70\% and 30\% rates, respectively. As a result, the most efficient results were ensured by the SVM classifier with an accuracy of 98.97\%, a sensitivity of 89.39\%, a specificity of 99.75\%, and an F-score of 96.72\%. Consequently, a cheap, fast, and reliable intelligence tool has been provided for COVID-19 infection detection. The developed model can be used to assist field specialists, physicians, and radiologists in the decision-making process. Thanks to the proposed tool, the misdiagnosis rates can be reduced, and the proposed model can be used as a retrospective evaluation tool to validate positive COVID-19 infection cases.},
  archive      = {J_ASOC},
  author       = {Majid Nour and Zafer Cömert and Kemal Polat},
  doi          = {10.1016/j.asoc.2020.106580},
  journal      = {Applied Soft Computing},
  pages        = {106580},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A novel medical diagnosis model for COVID-19 infection detection based on deep features and bayesian optimization},
  volume       = {97},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A metaheuristic-driven approach to fine-tune deep boltzmann
machines. <em>ASOC</em>, <em>97</em>, 105717. (<a
href="https://doi.org/10.1016/j.asoc.2019.105717">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning techniques, such as Deep Boltzmann Machines (DBMs), have received considerable attention over the past years due to the outstanding results concerning a variable range of domains. One of the main shortcomings of these techniques involves the choice of their hyperparameters, since they have a significant impact on the final results. This work addresses the issue of fine-tuning hyperparameters of Deep Boltzmann Machines using metaheuristic optimization techniques with different backgrounds, such as swarm intelligence , memory- and evolutionary-based approaches. Experiments conducted in three public datasets for binary image reconstruction showed that metaheuristic techniques can obtain reasonable results.},
  archive      = {J_ASOC},
  author       = {Leandro Aparecido Passos and João Paulo Papa},
  doi          = {10.1016/j.asoc.2019.105717},
  journal      = {Applied Soft Computing},
  pages        = {105717},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A metaheuristic-driven approach to fine-tune deep boltzmann machines},
  volume       = {97},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Automatic adjustment of the pulse-coupled neural network
hyperparameters based on differential evolution and cluster validity
index for image segmentation. <em>ASOC</em>, <em>97</em>, 105547. (<a
href="https://doi.org/10.1016/j.asoc.2019.105547">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The pulse-coupled neural network (PCNN) is a cortical model that can be used in image segmentation applications. The performance of the PCNN depends on adjusting its hyperparameters, where population-based metaheuristics , such as evolutionary algorithms , have been used to perform this task by optimizing a fitness function. In this regard, the entropy criterion is a common fitness function used to evaluate the quality of potential PCNN solutions. However, maximizing the entropy is related to maximize the inter-group separation, but the intra-group cohesion is unconsidered. In this regard, a cluster validity index (CVI) can be used as a fitness function, which defines a relationship between inter-group separation and intra-group cohesion. Therefore, we propose using a CVI to quantify the segmentation quality generated by the PCNN given a set of hyperparameters adjusted by the differential evolution algorithm . The proposed approach is tested on a dataset of natural images, where every image has three reference segmentations; thus, the Jaccard index is used to measure the segmentation performance . The experimental results reveal that a simplified PCNN, when used jointly with the Silhouette index, obtains the best performance with a mean Jaccard value of 0.77, whereas the entropy criterion attains 0.41. Additionally, the proposed approach is tested on two modalities of medical images to show its applicability in other kinds of images. The results suggest that using a CVI instead of the entropy criterion can improve the segmentation performance of the PCNN.},
  archive      = {J_ASOC},
  author       = {Wilfrido Gómez-Flores and Juanita Hernández-López},
  doi          = {10.1016/j.asoc.2019.105547},
  journal      = {Applied Soft Computing},
  pages        = {105547},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Automatic adjustment of the pulse-coupled neural network hyperparameters based on differential evolution and cluster validity index for image segmentation},
  volume       = {97},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Investigating the impact of data normalization on
classification performance. <em>ASOC</em>, <em>97</em>, 105524. (<a
href="https://doi.org/10.1016/j.asoc.2019.105524">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data normalization is one of the pre-processing approaches where the data is either scaled or transformed to make an equal contribution of each feature. The success of machine learning algorithms depends upon the quality of the data to obtain a generalized predictive model of the classification problem. The importance of data normalization for improving data quality and subsequently the performance of machine learning algorithms has been presented in many studies. But, the work lacks for the feature selection and feature weighting approaches, a current research trend in machine learning for improving performance. Therefore, this study aims to investigate the impact of fourteen data normalization methods on classification performance considering full feature set, feature selection, and feature weighting. In this paper, we also present a modified Ant Lion optimization that search feature subsets and the best feature weights along with the parameter of Nearest Neighbor Classifier . Experiments are performed on 21 publicly available real and synthetic datasets , and results are analyzed based on the accuracy, the percentage of feature reduced and runtime. It has been observed from the results that no single method outperforms others. Therefore, we have suggested a set of the best and the worst methods combining the normalization procedure and empirical analysis of results. The better performers are z z -Score and Pareto Scaling for the full feature set and feature selection, and tanh and its variant for feature weighting. The worst performers are Mean Centered, Variable Stability Scaling and Median and Median Absolute Deviation methods along with un-normalized data.},
  archive      = {J_ASOC},
  author       = {Dalwinder Singh and Birmohan Singh},
  doi          = {10.1016/j.asoc.2019.105524},
  journal      = {Applied Soft Computing},
  pages        = {105524},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Investigating the impact of data normalization on classification performance},
  volume       = {97},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Hybridization of chaos and flower pollination algorithm over
k-means for data clustering. <em>ASOC</em>, <em>97</em>, 105523. (<a
href="https://doi.org/10.1016/j.asoc.2019.105523">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Classical clustering algorithms like K-means often converge to local optima and have slow convergence rates for larger datasets. To overcome such situations in clustering, swarm based algorithms have been proposed. Swarm based approaches attempt to achieve the optimal solution for such problems in reasonable time. Many swarm based algorithms such as Flower Pollination Algorithm (FPA), Cuckoo Search Algorithm (CSA), Black Hole Algorithm (BHA), Bat Algorithm (BA) Particle Swarm Optimization (PSO), Firefly Algorithm (FFA), Artificial Bee Colony (ABC) etc have been successfully applied to many non-linear optimization problems . In this paper, an algorithm is proposed which hybridizes Chaos Optimization and Flower Pollination over K-means to improve the efficiency of minimizing the cluster integrity. The proposed algorithm referred as Chaotic FPA (CFPA) is compared with FPA, CSA, BHA, BA, FFA, and PSO over K-Means for data clustering problem. Experiments are conducted on sixteen benchmark datasets. Algorithms are compared on four different performance parameters — cluster integrity, execution time, number of iterations to converge (NIC) and stability. Results obtained are analyzed statistically using Non-parametric Friedman test. If Friedman test rejects the Null hypothesis then pair wise comparison is done using Nemenyi test. Experimental Result demonstrates the following: (a) CFPA and BHA have better performance on the basis of cluster integrity as compared to other algorithms; (b) Prove the superiority of CFPA and CSA over others on the basis of execution time; (c) CFPA and FPA converges earlier than other algorithms to evaluate optimal cluster integrity; (d) CFPA and BHA produce more stable results than other algorithms.},
  archive      = {J_ASOC},
  author       = {Arvinder Kaur and Saibal Kumar Pal and Amrit Pal Singh},
  doi          = {10.1016/j.asoc.2019.105523},
  journal      = {Applied Soft Computing},
  pages        = {105523},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Hybridization of chaos and flower pollination algorithm over K-means for data clustering},
  volume       = {97},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Kapur’s entropy based optimal multilevel image segmentation
using crow search algorithm. <em>ASOC</em>, <em>97</em>, 105522. (<a
href="https://doi.org/10.1016/j.asoc.2019.105522">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image segmentation is an essential part of image analysis, which has a direct impact on the quality of image analysis results. Thresholding is one of the simplest and widely used methods for image segmentation. Thresholding can be either bi-level, which involves partitioning of an image into two segments, or multilevel, which partitions an image into multiple segments using multiple thresholds values. This paper focuses on multilevel thresholding. A good segmentation scheme through multilevel thresholding identifies suitable threshold values to optimize between-class variance or entropy criterion. For such optimizations, nature inspired metaheuristic algorithms are commonly used. This paper presents a Kapur’s entropy based Crow Search Algorithm (CSA) to estimate optimal values of multilevel thresholds. Crow Search Algorithm is based on the intelligent behavior of crow flock. Crow Search Algorithm have shown better results because of less number of parameters, no premature convergence, and better exploration–exploitation balance in the search strategy. Kapur’s entropy is used as an objective function during the optimization process. The experiments have been performed on benchmarked images for different threshold values (i.e. 2, 4, 8, 16, 32 thresholds). The proposed method has been assessed and performance is compared with well-known metaheuristic optimization methods like Particle Swarm Optimization (PSO), Differential Evolution (DE), Grey Wolf Optimizer (GWO), Moth-Flame Optimization (MFO) and Cuckoo Search (CS). Experimental results have been evaluated qualitatively and quantitatively by using well-performed evaluation methods namely PSNR , SSIM, and FSIM. Computational time and Wilcoxon p-type value also compared. Experimental results show that proposed algorithm performed better than PSO, DE, GWO , MFO and CS in terms of quality and consistency.},
  archive      = {J_ASOC},
  author       = {Pankaj Upadhyay and Jitender Kumar Chhabra},
  doi          = {10.1016/j.asoc.2019.105522},
  journal      = {Applied Soft Computing},
  pages        = {105522},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Kapur’s entropy based optimal multilevel image segmentation using crow search algorithm},
  volume       = {97},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A combination of spectral graph theory and quantum genetic
algorithm to find relevant set of electrodes for motor imagery
classification. <em>ASOC</em>, <em>97</em>, 105519. (<a
href="https://doi.org/10.1016/j.asoc.2019.105519">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {At present, more number of electrodes are used to develop brain computer interface (BCI) devices based on motor imagery. However, the number of trials for a given subject is usually less. Under this situation, the performance of motor imagery task classification may degrade. In this research work, we propose a combination of graph theoretic spectral method and quantum genetic algorithm (QGA) to obtain a subset of relevant and non-redundant electrodes for effective motor imagery task classification . Stationary Common Spatial Pattern method, which can handle non-stationarity issue, is used for extraction of features from the reduced set of electrodes. Support Vector Machine (SVM) is used as a classifier. Improvement in classification performance on publicly available dataset signifies efficacy of the proposed method. Friedman statistical test demonstrates that the performance of the proposed method is significantly better in comparison to existing CSP and its variants.},
  archive      = {J_ASOC},
  author       = {Jyoti Singh Kirar and R.K. Agrawal},
  doi          = {10.1016/j.asoc.2019.105519},
  journal      = {Applied Soft Computing},
  pages        = {105519},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A combination of spectral graph theory and quantum genetic algorithm to find relevant set of electrodes for motor imagery classification},
  volume       = {97},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Mining significant fuzzy association rules with differential
evolution algorithm. <em>ASOC</em>, <em>97</em>, 105518. (<a
href="https://doi.org/10.1016/j.asoc.2019.105518">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article presents a new differential evolution (DE) algorithm for mining optimized statistically significant fuzzy association rules that are abundant in number and high in rule interestingness measure (RIM) values, with strict control over the risk of spurious rules. The risk control over spurious rules, as the most distinctive feature of the proposed DE compared with existing evolutionary algorithms (EAs) for association rule mining (ARM), is realized via two new statistically sound significance tests on the rules. The two tests, in the experimentwise and generationwise adjustment approach, can respectively limit the familywise error rate (the probability that any spurious rules occur in the ARM result) and percentage of spurious rules upon the user specified level. Experiments on variously sized data show that the proposed DE can keep the risk of spurious rules well below the user specified level, which is beyond the ability of existing EA-based ARM. The new method also carries forward the advantages of EA-based ARM and distinctive merits of DE in optimizing the rules: it can obtain several times as many rules and as high RIM values as conventional non-evolutionary ARM, and even more informative rules and better RIM values than genetic-algorithm-based ARM. Case studies on hotel room price determinants and wildfire risk factors demonstrate the practical usefulness of the proposed DE.},
  archive      = {J_ASOC},
  author       = {Anshu Zhang and Wenzhong Shi},
  doi          = {10.1016/j.asoc.2019.105518},
  journal      = {Applied Soft Computing},
  pages        = {105518},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Mining significant fuzzy association rules with differential evolution algorithm},
  volume       = {97},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). An improved runner-root algorithm for solving feature
selection problems based on rough sets and neighborhood rough sets.
<em>ASOC</em>, <em>97</em>, 105517. (<a
href="https://doi.org/10.1016/j.asoc.2019.105517">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Solving the feature selection problem is considered an important issue when addressing data from real applications that contain a large number of features. However, not all of these features are important; therefore, the redundant features must be removed because they affect the accuracy of the data representation and introduce time complexity into the analysis of these data. For these reasons, the feature selection problem is considered an NP-complete nonlinearly constrained optimization problem . The rough set (RS) and neighborhood rough set (NRS) are the most powerful methods used to solve the feature selection problem; however, both approaches suffer from high time complexity. To avoid these limitations, we combined the RS and NRS with a new metaheuristic algorithm called the runner-root algorithm (RRA). The spirit of the RRA originated from real-life plants called running plants, which have roots and runners that spread the plants in search of minerals and water resources through their root and runner development. To validate the proposed algorithm, several UCI Machine Learning Repository datasets are used to compute the performance of our algorithm employing two effective classifiers, the random forest and the K-nearest neighbor, in addition to some other measures for the performance evaluation. The experimental results illustrate that the proposed algorithm is superior to the state-of-the-art metaheuristic algorithms in terms of the performance measures . Additionally, the NRS increases the performance of the proposed method more than the RS as an objective function.},
  archive      = {J_ASOC},
  author       = {Rehab Ali Ibrahim and Mohamed Abd Elaziz and Diego Oliva and Songfeng Lu},
  doi          = {10.1016/j.asoc.2019.105517},
  journal      = {Applied Soft Computing},
  pages        = {105517},
  shortjournal = {Appl. Soft. Comput.},
  title        = {An improved runner-root algorithm for solving feature selection problems based on rough sets and neighborhood rough sets},
  volume       = {97},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). An evolutionary hyper-heuristic to optimise deep belief
networks for image reconstruction. <em>ASOC</em>, <em>97</em>, 105510.
(<a href="https://doi.org/10.1016/j.asoc.2019.105510">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep Belief Networks (DBN) have become a powerful tools to deal with a wide range of applications. On complex tasks like image reconstruction, DBN’s performance is highly sensitive to parameter settings. Manually trying out different parameters is tedious and time consuming however often required in practice as there are not many better options. This work proposes an evolutionary hyper-heuristic framework for automatic parameter optimisation of DBN. The hyper-heuristic framework introduced here is the first of its kind in this domain. It involves a high level strategy and a pool of evolutionary operators such as crossover and mutation to generates DBN parameter settings by perturbing or modifying the current setting of a DBN. Providing a large set of operators could be beneficial to form a more effective high level strategy, but in the same time would increase the search space hence make it more difficulty to form a good strategy. To address this issue, a non-parametric statistical test is introduced to identify a subset of effective operators for different phases of the hyper-heuristic search. Three well-known image reconstruction datasets were used to evaluate the performance of the proposed framework. The results reveal that the proposed hyper-heuristic framework is very competitive when compared to the state of art methods .},
  archive      = {J_ASOC},
  author       = {Nasser R. Sabar and Ayad Turky and Andy Song and Abdul Sattar},
  doi          = {10.1016/j.asoc.2019.105510},
  journal      = {Applied Soft Computing},
  pages        = {105510},
  shortjournal = {Appl. Soft. Comput.},
  title        = {An evolutionary hyper-heuristic to optimise deep belief networks for image reconstruction},
  volume       = {97},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Toxicity risks evaluation of unknown FDA biotransformed
drugs based on a multi-objective feature selection approach.
<em>ASOC</em>, <em>97</em>, 105509. (<a
href="https://doi.org/10.1016/j.asoc.2019.105509">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The risk factors evaluation of the unknown biotransformed drugs is important in the drug development. However, the experimental methods that are used to perform this task are time-consuming and expensive, therefore, these methods are not suitable to assess a large dataset of drugs at the early stage of the drug development. To avoid these problems, the computational approaches can be used to predict the risk factors of the unknown biotransformed drugs. The dataset used in this study consists of 5909 drugs with 33 chemical descriptors. However, most of these descriptors are irrelevant and this may reduce the prediction accuracy; therefore, the descriptor selection approach is needed. Descriptor (Feature) selection can be considered as a multi-objective optimization problem which has two conflicting objectives, minimizing the number of the selected features and maximizing the dependency degree of the descriptors. In this paper, a new multi-objective approach is developed for the descriptor selection based on the sine-cosine algorithm and the rough set. The proposed approach consists of two stages, the feature selection stage and the predicting of an unknown drug stage. The experimental results proved that the proposed approach achieved high accuracy to all toxic effects and this indicates that it could be used for the prediction of the drug toxicity in the early stage of the drug development.},
  archive      = {J_ASOC},
  author       = {Mohamed Abd Elaziz and Yasmine S. Moemen and Aboul Ella Hassanien and Shengwu Xiong},
  doi          = {10.1016/j.asoc.2019.105509},
  journal      = {Applied Soft Computing},
  pages        = {105509},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Toxicity risks evaluation of unknown FDA biotransformed drugs based on a multi-objective feature selection approach},
  volume       = {97},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Particle swarm optimization–markov chain monte carlo for
accurate visual tracking with adaptive template update. <em>ASOC</em>,
<em>97</em>, 105443. (<a
href="https://doi.org/10.1016/j.asoc.2019.04.014">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A novel tracking method is proposed, which infers a target state and appearance template simultaneously. With this simultaneous inference, the method accurately estimates the target state and robustly updates the target template. The joint inference is performed by using the proposed particle swarm optimization–Markov chain Monte Carlo (PSO–MCMC) sampling method. PSO–MCMC is a combination of the particle swarm optimization (PSO) and Markov chain Monte Carlo sampling (MCMC), in which the PSO evolutionary algorithm and MCMC aim to find the target state and appearance template, respectively. The PSO can handle multi-modality in the target state and is therefore superior to a standard particle filter. Thus, PSO–MCMC achieves better performance in terms of accuracy when compared to the recently proposed particle MCMC. Experimental results demonstrate that the proposed tracker adaptively updates the target template and outperforms state-of-the-art tracking methods on a benchmark dataset.},
  archive      = {J_ASOC},
  author       = {Junseok Kwon},
  doi          = {10.1016/j.asoc.2019.04.014},
  journal      = {Applied Soft Computing},
  pages        = {105443},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Particle swarm optimization–Markov chain monte carlo for accurate visual tracking with adaptive template update},
  volume       = {97},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Human mental search-based multilevel thresholding for image
segmentation. <em>ASOC</em>, <em>97</em>, 105427. (<a
href="https://doi.org/10.1016/j.asoc.2019.04.002">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multilevel thresholding is one of the principal methods of image segmentation . These methods enjoy image histogram for segmentation. The quality of segmentation depends on the value of the selected thresholds. Since an exhaustive search is made for finding the optimum value of the objective function, the conventional methods of multilevel thresholding are time-consuming computationally, especially when the number of thresholds increases. Use of evolutionary algorithms has attracted a lot of attention under such circumstances. Human mental search algorithm is a population-based evolutionary algorithm inspired by the manner of human mental search in online auctions . This algorithm has three interesting operators: (1) clustering for finding the promising areas, (2) mental search for exploring the surrounding of every solution using Levy distribution , and (3) moving the solutions toward the promising area. In the present study, multilevel thresholding is proposed for image segmentation using human mental search algorithm. Kapur (entropy) and Otsu (between-class variance) criteria were used for this purpose. The advantages of the proposed method are described using twelve images and in comparison with other existing approaches, including genetic algorithm , particle swarm optimization , differential evolution, firefly algorithm , bat algorithm , gravitational search algorithm , and teaching-learning-based optimization. The obtained results indicated that the proposed method is highly efficient in multilevel image thresholding in terms of objective function value, peak signal to noise, structural similarity index, feature similarity index, and the curse of dimensionality. In addition, two nonparametric statistical tests verified the efficiency of the proposed algorithm, statistically.},
  archive      = {J_ASOC},
  author       = {Seyed Jalaleddin Mousavirad and Hossein Ebrahimpour-Komleh},
  doi          = {10.1016/j.asoc.2019.04.002},
  journal      = {Applied Soft Computing},
  pages        = {105427},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Human mental search-based multilevel thresholding for image segmentation},
  volume       = {97},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). REMOVED: Detecting anomalies within unmanned aerial vehicle
(UAV) video based on contextual saliency. <em>ASOC</em>, <em>96</em>,
106715. (<a href="https://doi.org/10.1016/j.asoc.2020.106715">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article has been removed: please see Elsevier Policy on Article Withdrawal ( https://www.elsevier.com/about/our-business/policies/article-withdrawal ). This article has been removed at the request of the Editor-in-Chief of Applied Soft Computing. The author has plagiarized the thesis work “Contextual Saliency for Detecting Anomalies within Unmanned Aerial Vehicle (UAV) Video,” (Simon Gokstorp), Master’s Thesis for the degree of Master of Engineering, Durham University, (Supervisor: T.P. Breckon), 2019” by duplicating material from the online repository of the Master Thesis project ( https://github.com/Hoclor/CoSADUV-Contextual-Saliency-for-Detecting-Anomalies-in-UAV-Video ). One of the conditions of submission of a paper for publication is that authors declare explicitly that their work is original and has not appeared in a publication elsewhere. Re-use of any data should be appropriately cited. As such this article represents a severe abuse of the scientific publishing system. The scientific community takes a very strong view on this matter and apologies are offered to readers of the journal that this was not detected during the submission process.},
  archive      = {J_ASOC},
  author       = {Mostafa Al-Gabalawy},
  doi          = {10.1016/j.asoc.2020.106715},
  journal      = {Applied Soft Computing},
  month        = {11},
  pages        = {106715},
  shortjournal = {Appl. Soft. Comput.},
  title        = {REMOVED: Detecting anomalies within unmanned aerial vehicle (UAV) video based on contextual saliency},
  volume       = {96},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Local mean decomposition and artificial neural network
approach to mitigate tool chatter and improve material removal rate in
turning operation. <em>ASOC</em>, <em>96</em>, 106714. (<a
href="https://doi.org/10.1016/j.asoc.2020.106714">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Productivity has always been a major concern in the industry. It can be improved by increasing material removal rate. Regenerative chatter during machining is the major obstacle to attain this. In the present work, a methodology has been proposed to select a proper combination of input cutting parameters for stable turning with improved metal removal rate (MRR). Chatter signals generated during the turning of Al 6061 have been acquired using a microphone. Initially, acquired signals have been processed using local mean decomposition (LMD) signal processing technique. The decomposed signals have been analyzed using different statistical chatter indicators considering Nakagami distribution approach for ascertaining the thresholds of chatter severity. Prediction models of most effective statistical chatter indicator and MRR have been developed using an artificial neural network (ANN). Moreover, this prediction models have been optimized using multi-objective genetic algorithm for ascertaining the optimal range of cutting parameters for stable turning with higher MRR. Finally, obtained stable range has been validated by performing more experiments.},
  archive      = {J_ASOC},
  author       = {Pankaj Gupta and Bhagat Singh},
  doi          = {10.1016/j.asoc.2020.106714},
  journal      = {Applied Soft Computing},
  month        = {11},
  pages        = {106714},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Local mean decomposition and artificial neural network approach to mitigate tool chatter and improve material removal rate in turning operation},
  volume       = {96},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Control of constrained high dimensional nonlinear liquid
level processes using a novel neural network based rapidly exploring
random tree algorithm. <em>ASOC</em>, <em>96</em>, 106709. (<a
href="https://doi.org/10.1016/j.asoc.2020.106709">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The control of constrained nonlinear liquid level systems is a problem of fundamental importance in pharmaceutical, chemical, food-processing, oil refining and natural liquid gas separation industries. This paper proposes a novel control strategy for the control of such constrained high-dimensional interacting liquid level systems. The nonlinear liquid level regulation problem is formulated as a path planning problem in high-dimensional state space where constraint satisfaction is viewed as obstacle avoidance. An approximate control policy to steer the system to the goal state while satisfying numerous level and flow-rate constraints is computed using the famous RRT path planning algorithm which can efficiently explore non-convex spaces. To further improve performance a neural network was trained to generalize the approximate control policy computed by the RRT to unexplored states and provide smooth control. The generalized control policy learnt by the neural network is then used to achieve large changes in state and bring the system close to the goal state after which computationally cheap linear control is used to keep the system close to the goal state. The effectiveness of the proposed ANN-RRT control approach is demonstrated by applying it to the control of constrained high dimensional 5, 10, 20, 30 and 50 interacting tank systems. Experimental results for a highly interacting quadruple tank system indicate that the ANN-RRT algorithm significantly outperforms alternate approaches like PID, Fuzzy control, MPC , IMC and SMC from recent literature.},
  archive      = {J_ASOC},
  author       = {B. Jaganatha Pandian and Mathew Mithra Noel},
  doi          = {10.1016/j.asoc.2020.106709},
  journal      = {Applied Soft Computing},
  month        = {11},
  pages        = {106709},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Control of constrained high dimensional nonlinear liquid level processes using a novel neural network based rapidly exploring random tree algorithm},
  volume       = {96},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Prediction and analysis of cold rolling mill vibration based
on a data-driven method. <em>ASOC</em>, <em>96</em>, 106706. (<a
href="https://doi.org/10.1016/j.asoc.2020.106706">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mill chatter is one of the most common problems in cold rolling. Thus, it is important to investigate the mill chatter phenomenon to ensure a high-speed and stable rolling process. The traditional mill chatter mechanism model cannot meet the monitoring and rapid diagnosis needs of the rolling process in the field. In this paper, a data-driven mill vibration analysis method is proposed. The main objective of this study was to develop a mill vibration monitoring method and an intelligent algorithm for mill chatter early warning. Rolling experiments showed that the proposed monitoring method could be a promising and effective technique for assessing the chatter phenomenon. The mill vibration acceleration amplitude prediction performance of a support vector regression , neural-network-based method, and extreme gradient boosting method were evaluated. The results proved that the prediction performances of the proposed extreme gradient boosting method were highly reliable with the highest determination coefficient value of 0.779, lowest mean absolute percentage error of 9.7\%, and better forecast robustness under all of the dataset ratios. Meanwhile, the contribution rates of the variables on the mill vibrations were investigated, and results showed that an effective way to eliminate the mill chatter was to control the rolling speed, cumulative rolling strip length, tension and roll radius .},
  archive      = {J_ASOC},
  author       = {Xing Lu and Jie Sun and Zhixin Song and Guangtao Li and Zhenhua Wang and Yunjian Hu and Qinglong Wang and Dianhua Zhang},
  doi          = {10.1016/j.asoc.2020.106706},
  journal      = {Applied Soft Computing},
  month        = {11},
  pages        = {106706},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Prediction and analysis of cold rolling mill vibration based on a data-driven method},
  volume       = {96},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Application of interval type-2 fuzzy logic systems to gas
turbine fault diagnosis. <em>ASOC</em>, <em>96</em>, 106703. (<a
href="https://doi.org/10.1016/j.asoc.2020.106703">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Several approaches have been employed for gas turbine Fault Detection and Identification (FDI), since a reliable FDI system with minimum false alarm rate can effectively reduce maintenance cost and downtime. This paper introduces the application of Interval Type-2 Fuzzy Logic Systems (IT2FLSs) to gas turbine fault diagnosis for the first time. The proposed FDI system is composed of a bank of IT2FLSs, trained for state detection and health assessment of an industrial gas turbine at various operating conditions. For this purpose, train and test data are generated by applying mechanical fault signatures to gas turbine’s mathematical model. Fuzzy Rule Base is then developed by means of Interval Type-2 Fuzzy C-Means (IT2FCM) clustering, and parameters of the IT2FLSs are optimized using a metaheuristic algorithm. Finally, the performance of the IT2FL based FDI system is compared to several classification techniques. It is concluded that as a compromise among the objectives of online applicability, accuracy, reliability against measurement uncertainty, incipient fault diagnosis, robustness against abrupt sensor failure and generalization capacity, the proposed method demonstrates a promising performance.},
  archive      = {J_ASOC},
  author       = {Morteza Montazeri-Gh and Shabnam Yazdani},
  doi          = {10.1016/j.asoc.2020.106703},
  journal      = {Applied Soft Computing},
  month        = {11},
  pages        = {106703},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Application of interval type-2 fuzzy logic systems to gas turbine fault diagnosis},
  volume       = {96},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Clustering time-series by a novel slope-based similarity
measure considering particle swarm optimization. <em>ASOC</em>,
<em>96</em>, 106701. (<a
href="https://doi.org/10.1016/j.asoc.2020.106701">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently there has been an increase in the studies on time-series data mining specifically time-series clustering due to the vast existence of time-series in various domains. The large volume of data in the form of time-series makes it necessary to employ techniques such as clustering to understand the data and to extract information and hidden patterns. The most important aspect of time-series clustering is the similarity measure used to compare a pair of time-series. In this paper, we develop a new similarity measure specifically for the task of time-series clustering. The proposed similarity measure is developed based on a combination of a simple representation of time-series, slope of each segment of time-series, and Euclidean distance with the capability to be implemented by the so-called dynamic time warping. We prove in this paper that the proposed distance measure is metric and thus indexing can be applied. For the task of clustering, the Particle Swarm Optimization algorithm is employed. We evaluate the proposed similarity measure by comparing it to three well-known existing similarity measures in terms of various criteria used for the evaluation of clustering performances. The results indicate that the proposed similarity measure outperforms the rest in almost every dataset used in this paper.},
  archive      = {J_ASOC},
  author       = {Hossein Kamalzadeh and Abbas Ahmadi and Saeed Mansour},
  doi          = {10.1016/j.asoc.2020.106701},
  journal      = {Applied Soft Computing},
  month        = {11},
  pages        = {106701},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Clustering time-series by a novel slope-based similarity measure considering particle swarm optimization},
  volume       = {96},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). ML-MDLText: An efficient and lightweight multilabel text
classifier with incremental learning. <em>ASOC</em>, <em>96</em>,
106699. (<a href="https://doi.org/10.1016/j.asoc.2020.106699">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Single-label text classification has been extensively studied in the last decades, and usually, more attention has been given to offline learning scenarios, where all of the training data is available in advance. However, real-world text classification problems often involve multilabel instances and have dynamic textual patterns that can change frequently. In this context, the methods must predict a subset of target labels rather than a single one, and ideally should be able to update their model incrementally to be scalable and adaptable to changes in data patterns using limited time and memory. In this study, we present a text classification method based on the minimum description length principle that can be applied to multilabel classification without requiring the transformation of the classification problem. It also takes advantage of dependency information among labels and naturally supports online learning. We evaluated its performance using fifteen datasets from different application domains and compared it with traditional benchmark classifiers, considering three online learning scenarios. Even without requiring problem transformation tricks, the results obtained by the proposed method were very competitive with existing state-of-the-art online learning methods and those that transform multilabel problems into several single-label ones.},
  archive      = {J_ASOC},
  author       = {Marciele M. Bittencourt and Renato M. Silva and Tiago A. Almeida},
  doi          = {10.1016/j.asoc.2020.106699},
  journal      = {Applied Soft Computing},
  month        = {11},
  pages        = {106699},
  shortjournal = {Appl. Soft. Comput.},
  title        = {ML-MDLText: An efficient and lightweight multilabel text classifier with incremental learning},
  volume       = {96},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Real-time deep reinforcement learning based vehicle
navigation. <em>ASOC</em>, <em>96</em>, 106694. (<a
href="https://doi.org/10.1016/j.asoc.2020.106694">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traffic congestion has become one of the most serious contemporary city issues as it leads to unnecessary high energy consumption, air pollution and extra traveling time. During the past decade, many optimization algorithms have been designed to achieve the optimal usage of existing roadway capacity in cities to leverage the problem. However, it is still a challenging task for the vehicles to interact with the complex city environment in a real time manner. In this paper, we propose a deep reinforcement learning (DRL) method to build a real-time intelligent vehicle routing and navigation system by formulating the task as a sequence of decisions. In addition, an integrated framework is provided to facilitate the intelligent vehicle navigation research by embedding smart agents into the SUMO simulator. Nine realistic traffic scenarios are simulated to test the proposed navigation method . The experimental results have demonstrated the efficient convergence of the vehicle navigation agents and their effectiveness to make optimal decisions under the volatile traffic conditions. The results also show that the proposed method provides a better navigation solution comparing to the benchmark routing optimization algorithms. The performance has been further validated by using the Wilcoxon test. It is found that the achieved improvement of our proposed method becomes more significant under the maps with more edges (roads) and more complicated traffics comparing to the state-of-the-art navigation methods.},
  archive      = {J_ASOC},
  author       = {Songsang Koh and Bo Zhou and Hui Fang and Po Yang and Zaili Yang and Qiang Yang and Lin Guan and Zhigang Ji},
  doi          = {10.1016/j.asoc.2020.106694},
  journal      = {Applied Soft Computing},
  month        = {11},
  pages        = {106694},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Real-time deep reinforcement learning based vehicle navigation},
  volume       = {96},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A fitness landscape ruggedness multiobjective differential
evolution algorithm with a reinforcement learning strategy.
<em>ASOC</em>, <em>96</em>, 106693. (<a
href="https://doi.org/10.1016/j.asoc.2020.106693">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Optimization is the process of finding and comparing feasible solutions and adopting the best one until no better solution can be found. Because solving real-world problems often involves simulations and multiobjective optimization , the results and solutions of these problems are conceptually different from those of single-objective problems. In single-objective optimization problems , the global optimal solution is the solution that yields the optimal value of the objective function. However, for multiobjective optimization problems , the optimal solutions are Pareto-optimal solutions produced by balancing multiple objective functions. The strategic variables calculated in multiobjective problems produce different effects on the mapping imbalance and the search redundancy in the search space . Therefore, this paper proposes a fitness landscape ruggedness multiobjective differential evolution (LRMODE) algorithm with a reinforcement learning strategy. The proposed algorithm analyses the ruggedness of landscapes using information entropy to estimate whether the local landscape has a unimodal or multimodal topology and then combines the outcome with a reinforcement learning strategy to determine the optimal probability distribution of the algorithm’s search strategy set. The experimental results show that this novel algorithm can ameliorate the problem of search redundancy and search-space mapping imbalances, effectively improving the convergence of the search algorithm during the optimization process.},
  archive      = {J_ASOC},
  author       = {Ying Huang and Wei Li and Furong Tian and Xiang Meng},
  doi          = {10.1016/j.asoc.2020.106693},
  journal      = {Applied Soft Computing},
  month        = {11},
  pages        = {106693},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A fitness landscape ruggedness multiobjective differential evolution algorithm with a reinforcement learning strategy},
  volume       = {96},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Music auto-tagging using scattering transform and
convolutional neural network with self-attention. <em>ASOC</em>,
<em>96</em>, 106702. (<a
href="https://doi.org/10.1016/j.asoc.2020.106702">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As a branch of machine learning , deep learning has been used for tackling with the music auto-tagging problem. Deep learning methods, especially those with convolutional neural network (CNN) architecture, have exhibited good performance on this multi-label classification task . However, the feature extracting part and preprocessing part of this architecture need to be improved. In this paper, we propose a deep-learning model based on CNN with scattering transform and self-attention mechanism for music automatic tagging. To get a balance between information integrity and feature extraction in the preprocessing phase , we employ the scattering transform. Then, a multi-layer CNN is used to extract higher-level features from the scattering coefficients . In order to select better receptive fields of the CNN, self-attention sub-network is appended at the last layer of CNN. Experimental results on the MagnaTagATune dataset and Million Song Dataset (MSD) show the proposed model is a good choice for music auto-tagging task, since the scores of the area under the receiver operating characteristic curve (ROC-AUC) and the area under the precision–recall curve (PR-AUC) obtained in this paper surpass the state-of-the-art models. Furthermore, we visualize the distributions of attention weights, activations of the CNN and ROC-AUC scores on each tag for better understanding of the model.},
  archive      = {J_ASOC},
  author       = {Guangxiao Song and Zhijie Wang and Fang Han and Shenyi Ding and Xiaochun Gu},
  doi          = {10.1016/j.asoc.2020.106702},
  journal      = {Applied Soft Computing},
  pages        = {106702},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Music auto-tagging using scattering transform and convolutional neural network with self-attention},
  volume       = {96},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Novel hybrid algorithm for team orienteering problem with
time windows for rescue applications. <em>ASOC</em>, <em>96</em>,
106700. (<a href="https://doi.org/10.1016/j.asoc.2020.106700">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Robots for rescue operations after a disaster are an interesting and challenging research problem that has the potential to save lives and reduce economic losses after a disaster. We developed TOPTWR, an extension of the popular TOPTW model, to model the issues in task allocation for teams of rescue robots. Our hybrid algorithm is based on a team of heterogeneous humanoid robots trying to optimize five objectives (task rewards, task completion time , total energy, maximum energy consumption for a single robot, and missed deadline penalties). A common approach to solve these kinds of problems are multi-objective evolutionary algorithms (MOEAs), but their major disadvantage is that they cannot deal with dynamic environments easily. This paper presents an efficient solution for TOPTWR by combining MOEAs with learning algorithms. A novel Extended Multi-Start Simulated Annealing Iterated Local Search (EMSAILS) operator using a modern state-of-the-art NSGA-III algorithm is proposed. In addition, we applied Q-Learning to learn the likely changes in the environment and how to react to them. This algorithm, HMO-TOPTWR-NSGA-III (HMO-N-L), uses an artificial neural network (ANN) as a function approximator to make the huge state and action spaces tractable. This paper includes a thorough empirical evaluation demonstrating the effectiveness of the multi-objective algorithm in both static and dynamic environments. The evaluation shows that the proposed algorithm reduces the error by up to 42\% against three state-of-the-art approaches to TOPTW (HMO-N, MSA, and IPI).},
  archive      = {J_ASOC},
  author       = {Saeed Saeedvand and Hadi S. Aghdasi and Jacky Baltes},
  doi          = {10.1016/j.asoc.2020.106700},
  journal      = {Applied Soft Computing},
  pages        = {106700},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Novel hybrid algorithm for team orienteering problem with time windows for rescue applications},
  volume       = {96},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). An efficient model for predicting setting time of cement
based on broad learning system. <em>ASOC</em>, <em>96</em>, 106698. (<a
href="https://doi.org/10.1016/j.asoc.2020.106698">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cement is the main building material in the construction industry. Its setting time directly affects the setting time and strength of concrete, which further affects construction schedule and building quality. However, traditional measurement technology not only has high labor intensity and high time consumption, but has a high technical requirement. Various human factors , such as insufficient operation, will result in great errors in the measurements. The accurate prediction of setting time enables manpower savings, avoids large errors caused by insufficient operation, and guides the production of high-performance cement. In this paper, an efficient model based on the broad learning system is proposed to predict the initial and final setting time. It is committed to directly predicting setting time from clinker composition and physical properties, which is of great significance to the optimization of clinker formula. The experimental results show that it can accurately predict the setting time and behave good generalization ability , which addresses the problem of labor intensity in measurement and saves many resources. In addition, the broad learning system can rapidly build a setting time prediction model with few errors, satisfying industrial demands for the rapid modeling of various specialty cements.},
  archive      = {J_ASOC},
  author       = {Jifeng Guo and Lin Wang and Kaipeng Fan and Bo Yang},
  doi          = {10.1016/j.asoc.2020.106698},
  journal      = {Applied Soft Computing},
  pages        = {106698},
  shortjournal = {Appl. Soft. Comput.},
  title        = {An efficient model for predicting setting time of cement based on broad learning system},
  volume       = {96},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Natural gas pipeline network expansion under load-evolution
uncertainty based on multi-criteria analysis. <em>ASOC</em>,
<em>96</em>, 106697. (<a
href="https://doi.org/10.1016/j.asoc.2020.106697">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes a new approach to the design of natural gas network expansion under load evolution uncertainty in a time horizon. A predefined network topology is assumed with pipe diameters as the design variables. A dedicated optimization tool is developed so as to find not only an estimate of the optimal network, but also a set of local optimal ones, for a given most probable scenario. A Monte-Carlo simulation of the future load conditions is performed, evaluating each solution within a set of other possible scenarios. A dominance analysis is performed to compare the candidate solutions, considering the objectives of smaller installation cost, smaller infeasibility rate, smaller mean fault-cost and smaller sensitivity. A real case study is conducted concerning the design of a gas network by three engineers with 6 years of experience, on average, in the design office of a gas utility company; and their results are compared against the ones obtained from the proposed methodology. The results show that the proposed approach leads to networks that can be rather different from those obtained using a conventional, time consuming, design procedure, reaching more robust performances under load evolution uncertainties.},
  archive      = {J_ASOC},
  author       = {Eduardo Santiago Ramos and Lucas S. Batista},
  doi          = {10.1016/j.asoc.2020.106697},
  journal      = {Applied Soft Computing},
  pages        = {106697},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Natural gas pipeline network expansion under load-evolution uncertainty based on multi-criteria analysis},
  volume       = {96},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Pioneer pareto artificial bee colony algorithm for
three-dimensional objective space optimization of composite-based
layered radar absorber. <em>ASOC</em>, <em>96</em>, 106696. (<a
href="https://doi.org/10.1016/j.asoc.2020.106696">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A three-dimensional objective space (3DOS) optimization strategy using an enhanced multi-objective artificial bee colony (ABC) algorithm for the design optimization of layered radar absorbing material (LRAM) is presented in this study. The multi-objective exploitation ability of ABC is improved with regard to the convergence and diversity by integrating a pioneer Pareto (PP) solution to the onlooker bee phase, which is selected from the Pareto optimal set . Initially, the performance of PP-ABC is successfully verified by a comparison with ABC and the well-known multi-objective counterparts like particle swarm optimization (PSO) and differential evolution (DE) algorithms. The comparison is carried out through five multi-objective benchmark functions with respect to three favorable and reliable multi-objective indicators such as hypervolume (HV), HV ratio and Pareto sets proximity (PSP). The employed three objective functions to be the dimensions of 3DOS are weighted bandwidth-based total reflection coefficient involving sub-reflection waves of a wide oblique incident angular range 0 ° ° –75 ° ° , the total thickness and the number of layers. By using PP-ABC, a 3D designed LRAM operating at a large frequency band of 2–18 GHz is then designed for synchronously minimizing the three objective vectors by finding out the design variables: thickness and material types. Meanwhile, the material types of the proposed LRAM are optimally picked up from a composite material database with 51 specimens from 9 previously reported studies (51/9#database). In order to point out the effectiveness of the proposed 3DOS optimization strategy , three LRAMs are also compared with respective reported designs whose material type is selected from a database with 6 specimens (6/1#database). The results show that the proposed LRAMs are hence the global optimal designs in terms of all objective functions thanks to the proposed 3DOS optimization strategy based on PP-ABC.},
  archive      = {J_ASOC},
  author       = {Abdurrahim Toktas and Deniz Ustun and Nursev Erdogan},
  doi          = {10.1016/j.asoc.2020.106696},
  journal      = {Applied Soft Computing},
  pages        = {106696},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Pioneer pareto artificial bee colony algorithm for three-dimensional objective space optimization of composite-based layered radar absorber},
  volume       = {96},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A data-driven understanding of COVID-19 dynamics using
sequential genetic algorithm based probabilistic cellular automata.
<em>ASOC</em>, <em>96</em>, 106692. (<a
href="https://doi.org/10.1016/j.asoc.2020.106692">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {COVID-19 pandemic is severely impacting the lives of billions across the globe. Even after taking massive protective measures like nation-wide lockdowns, discontinuation of international flight services, rigorous testing etc., the infection spreading is still growing steadily, causing thousands of deaths and serious socio-economic crisis. Thus, the identification of the major factors of this infection spreading dynamics is becoming crucial to minimize impact and lifetime of COVID-19 and any future pandemic. In this work, a probabilistic cellular automata based method has been employed to model the infection dynamics for a significant number of different countries. This study proposes that for an accurate data-driven modelling of this infection spread, cellular automata provides an excellent platform, with a sequential genetic algorithm for efficiently estimating the parameters of the dynamics. To the best of our knowledge, this is the first attempt to understand and interpret COVID-19 data using optimized cellular automata, through genetic algorithm. It has been demonstrated that the proposed methodology can be flexible and robust at the same time, and can be used to model the daily active cases, total number of infected people and total death cases through systematic parameter estimation. Elaborate analyses for COVID-19 statistics of forty countries from different continents have been performed, with markedly divergent time evolution of the infection spreading because of demographic and socioeconomic factors. The substantial predictive power of this model has been established with conclusions on the key players in this pandemic dynamics.},
  archive      = {J_ASOC},
  author       = {Sayantari Ghosh and Saumik Bhattacharya},
  doi          = {10.1016/j.asoc.2020.106692},
  journal      = {Applied Soft Computing},
  pages        = {106692},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A data-driven understanding of COVID-19 dynamics using sequential genetic algorithm based probabilistic cellular automata},
  volume       = {96},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Automated medical diagnosis of COVID-19 through EfficientNet
convolutional neural network. <em>ASOC</em>, <em>96</em>, 106691. (<a
href="https://doi.org/10.1016/j.asoc.2020.106691">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {COVID-19 infection was reported in December 2019 at Wuhan, China. This virus critically affects several countries such as the USA, Brazil , India and Italy. Numerous research units are working at their higher level of effort to develop novel methods to prevent and control this pandemic scenario. The main objective of this paper is to propose a medical decision support system using the implementation of a convolutional neural network (CNN). This CNN has been developed using EfficientNet architecture. To the best of the authors’ knowledge, there is no similar study that proposes an automated method for COVID-19 diagnosis using EfficientNet. Therefore, the main contribution is to present the results of a CNN developed using EfficientNet and 10-fold stratified cross-validation. This paper presents two main experiments. First, the binary classification results using images from COVID-19 patients and normal patients are shown. Second, the multi-class results using images from COVID-19, pneumonia and normal patients are discussed. The results show average accuracy values for binary and multi-class of 99.62\% and 96.70\%, respectively. On the one hand, the proposed CNN model using EfficientNet presents an average recall value of 99.63\% and 96.69\% concerning binary and multi-class, respectively. On the other hand, 99.64\% is the average precision value reported by binary classification, and 97.54\% is presented in multi-class. Finally, the average F1-score for multi-class is 97.11\%, and 99.62\% is presented for binary classification. In conclusion, the proposed architecture can provide an automated medical diagnostics system to support healthcare specialists for enhanced decision making during this pandemic scenario.},
  archive      = {J_ASOC},
  author       = {Gonçalo Marques and Deevyankar Agarwal and Isabel de la Torre Díez},
  doi          = {10.1016/j.asoc.2020.106691},
  journal      = {Applied Soft Computing},
  pages        = {106691},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Automated medical diagnosis of COVID-19 through EfficientNet convolutional neural network},
  volume       = {96},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Hospital readmission prediction based on long-term and
short-term information fusion. <em>ASOC</em>, <em>96</em>, 106690. (<a
href="https://doi.org/10.1016/j.asoc.2020.106690">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The hospital readmission prediction becomes a significant task for healthcare systems and patients. Many predictive models have been developed and make progress in this task. However, most of them roughly combine the patient’s long-term(e.g., the history of present illness) and short-term(e.g., the performed laboratory test results when the patients are discharged) information without considering the inner distinction between them. In this paper, we propose a new approach for hospital readmission prediction based on transformation from numerical features to natural language, which makes better fusion of these two kinds of information. Through a rule-based transformation, the original numerical features are transformed into corresponding descriptive short sentences based on medical knowledge. Meanwhile, with the help of public well pre-trained character embeddings, our model can incorporate the prior semantic knowledge into the data. Moreover, by using the long-term information as the query of short-term feature attention mechanism , our model can capture the effective information in the short-term features from a more global perspective, and better incorporates the long-term and short-term information. Extensive experiment results on a real dataset demonstrate the effectiveness and superiority of our proposed model compared with the baseline methods .},
  archive      = {J_ASOC},
  author       = {Ziheng Chen and Chaojie Lai and Jiangtao Ren},
  doi          = {10.1016/j.asoc.2020.106690},
  journal      = {Applied Soft Computing},
  pages        = {106690},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Hospital readmission prediction based on long-term and short-term information fusion},
  volume       = {96},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A manufacturing failure mode and effect analysis based on
fuzzy and probabilistic risk analysis. <em>ASOC</em>, <em>96</em>,
106689. (<a href="https://doi.org/10.1016/j.asoc.2020.106689">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study proposes an improved failure mode and effect analysis (FMEA) with fuzzy Bayesian Network (FBN) and fuzzy best-worst method (FBWM) to assess failures in plastic production. To eliminate the drawbacks of the classical risk priority number (RPN) computation of FMEA, this approach is developed. Unlike classical RPN, a new parameter hierarchy is constructed, and three sub-parameters are injected into the approach. These parameters are weighted by the aid of FBWM. Hereafter, a fuzzy rule-based system is constructed by incorporating Bayesian Network (BN). Also, a sensitivity analysis is performed to observe the final FMEA score changes in accordance with the change of subjective probability values. Finally, a comparative analysis with two approaches of classical FMEA and FBWM-based FMEA (without a fuzzy rule-based system incorporating BN) is fulfilled. The results of the study are strengthened with the experts’ opinions regarding the importance of failure modes for the final product and the whole system and supported them by experience feedback in the observed facility.},
  archive      = {J_ASOC},
  author       = {Muhammet Gul and Melih Yucesan and Erkan Celik},
  doi          = {10.1016/j.asoc.2020.106689},
  journal      = {Applied Soft Computing},
  pages        = {106689},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A manufacturing failure mode and effect analysis based on fuzzy and probabilistic risk analysis},
  volume       = {96},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Comparison matrix geometric index: A qualitative online
reputation metric. <em>ASOC</em>, <em>96</em>, 106687. (<a
href="https://doi.org/10.1016/j.asoc.2020.106687">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Previous scientific studies as well as consulting firms have developed numerous Online Reputation Indices (ORIs), i.e. custom-tailored metrics intended to measure the emotions that people express towards a brand, product, or service in social media. These ORIs can provide useful information to assess the impact of marketing campaigns, social approval, and viral behavior of news and memes, among others. However, traditional ORIs are isolated metrics; thus, they are not suitable for determining the relative preference between two alternatives; e.g.: twice the number of “likes” does not imply that a given product is preferred two times as much as its competitor. This is an important constraint for ORIs, because stakeholders are driven to make relative and qualitative comparisons to weigh the alternatives’ value. Furthermore, relative comparisons are crucial for the systematic evaluation of alternatives in social decision making processes. The aim of this paper is to present a novel qualitative online reputation metric, a Comparison Matrix Geometric Index (CMGI), that considers both the accumulated emotions and the direct comparisons expressed in social media communications to infer the relative preferences among a set of alternatives.},
  archive      = {J_ASOC},
  author       = {Gustavo Vaccaro and Francisco E. Cabrera and Jose Ignacio Pelaez and L.G. Vargas},
  doi          = {10.1016/j.asoc.2020.106687},
  journal      = {Applied Soft Computing},
  pages        = {106687},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Comparison matrix geometric index: A qualitative online reputation metric},
  volume       = {96},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Empirical comparison and evaluation of artificial immune
systems in inter-release software fault prediction. <em>ASOC</em>,
<em>96</em>, 106686. (<a
href="https://doi.org/10.1016/j.asoc.2020.106686">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Artificial immune systems are bio-inspired machine learning algorithms based on the mammalian immune paradigms. One of the possible uses of these methods is Software Fault Prediction, which consists of classifying the modules of an application as being fault-prone or not, thus allowing a developer to better target the modules during the test phase leading to a high-quality software with lower cost. Despite the high number of works in the field, only five studies included Artificial Immune Systems in their approaches and exclusively focused on the intra-project fault prediction scheme. In this study, our objective is to appraise 8 immunological systems on the rarely treated inter-project software defect prediction scenario over three different benchmarks, hence, we selected 41 datasets corresponding to 11 java projects from the PROMISE data repository. According to the Friedman and Nemenyi Post-hoc test results, none of the performance of the studied algorithms was better than Immunos-1 and Immunos-99 in terms of the Recall measure. Furthermore, the outcomes of the Wilcoxon test suggest that the researches addressing the intra-projects defect prediction problems should also evaluate their models on inter-release scenarios.},
  archive      = {J_ASOC},
  author       = {Ahmed Taha Haouari and Labiba Souici-Meslati and Fadila Atil and Djamel Meslati},
  doi          = {10.1016/j.asoc.2020.106686},
  journal      = {Applied Soft Computing},
  pages        = {106686},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Empirical comparison and evaluation of artificial immune systems in inter-release software fault prediction},
  volume       = {96},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). LAVARNET: Neural network modeling of causal variable
relationships for multivariate time series forecasting. <em>ASOC</em>,
<em>96</em>, 106685. (<a
href="https://doi.org/10.1016/j.asoc.2020.106685">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multivariate time series forecasting is of great importance to many scientific disciplines and industrial sectors. The evolution of a multivariate time series depends on the dynamics of its variables and the connectivity network of causal interrelationships among them. Most of the existing time series models do not account for the causal effects among the system’s variables and even if they do, they rely just on determining the between-variables causality network. Knowing the structure of such a complex network, and even more specifically knowing the exact lagged variables that contribute to the underlying process is crucial for the task of multivariate time series forecasting. The latter is a rather unexplored source of information to leverage. In this direction, here a novel neural network-based architecture is proposed, termed LAgged VAriable Representation NETwork (LAVARNET), which intrinsically estimates the importance of lagged variables and combines high dimensional latent representations of them to predict future values of time series. Our model is compared with other baseline and state of the art neural network architectures on one simulated data set and four real data sets from the domains of meteorology, music, solar activity, and finance. The proposed architecture outperforms the competitive architectures in most of the experiments.},
  archive      = {J_ASOC},
  author       = {Christos Koutlis and Symeon Papadopoulos and Manos Schinas and Ioannis Kompatsiaris},
  doi          = {10.1016/j.asoc.2020.106685},
  journal      = {Applied Soft Computing},
  pages        = {106685},
  shortjournal = {Appl. Soft. Comput.},
  title        = {LAVARNET: Neural network modeling of causal variable relationships for multivariate time series forecasting},
  volume       = {96},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Predicting stock trend using an integrated term
frequency–inverse document frequency-based feature weight matrix with
neural networks. <em>ASOC</em>, <em>96</em>, 106684. (<a
href="https://doi.org/10.1016/j.asoc.2020.106684">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The financial market consists of various money-making strategies wherein trading through a stock market is an important example. The complex non-linear behaviors of volatile stock markets attract researchers to study inherent patterns. As the primary motivation for investment in such markets is to gain higher profits, potential stocks are given considerable attention using various weighting strategies that can enhance future returns. Term frequency–inverse document frequency (TF–IDF) is a statistical approach with remarkable applications in the financial domain for information retrieval from textual data; it identifies the importance of a term in the given document of a corpus. However, the application of TF–IDF for the numerical data representation is explored to a limited extent. In this article, we propose to extend the applicability of TF–IDF for the numerical time-series stock market data; we process the data and prepare them to be suitable for TF–IDF. We utilize this statistical approach to derive feature weight matrix from the historical stock market data and further, integrate it with the widely explored neural network architectures namely, backpropagation neural network (BPNN), long short-term memory (LSTM), and gated recurrent unit (GRU) for predicting stock market trend. Simulation results show that the proposed integrated approach using TF–IDF-based feature weight matrix and neural networks outperforms the considered recent approaches. The results are statistically supported with p p -value less than . 01 .01 using a Wilcoxon signed-rank test; our proposed approach is supported with illustrative examples to develop better understanding of the work. Also, remarks on the conclusions and potential future scope are discussed.},
  archive      = {J_ASOC},
  author       = {Ankit Thakkar and Kinjal Chaudhari},
  doi          = {10.1016/j.asoc.2020.106684},
  journal      = {Applied Soft Computing},
  pages        = {106684},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Predicting stock trend using an integrated term frequency–inverse document frequency-based feature weight matrix with neural networks},
  volume       = {96},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Analyzing COVID-19 virus based on enhanced fragmented
biological local aligner using improved ions motion optimization
algorithm. <em>ASOC</em>, <em>96</em>, 106683. (<a
href="https://doi.org/10.1016/j.asoc.2020.106683">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {SARS-CoV-2 (COVID-19) virus is a havoc pandemic that infects millions of people over the world and thousands of infected cases dead. So, it is vital to propose new intelligent data analysis tools and enhance the existed ones to aid scientists in analyzing the COVID-19 virus. Fragmented Local Aligner Technique (FLAT) is a data analysis tool that is used for detecting the longest common consecutive subsequence (LCCS) between a pair of biological data sequences. FLAT is an aligner tool that can be used to find the LCCS between COVID-19 virus and other viruses to help in other biochemistry and biological operations. In this study, the enhancement of FLAT based on modified Ions Motion Optimization (IMO) is developed to produce acceptable LCCS with efficient performance in a reasonable time. The proposed method was tested to find the LCCS between Orflab poly-protein and surface glycoprotein of COVID-19 and other viruses. The experimental results demonstrate that the proposed model succeeded in producing the best LCCS against other algorithms using real LCCS measured by the SW algorithm as a reference.},
  archive      = {J_ASOC},
  author       = {Mohamed Issa and Mohamed Abd Elaziz},
  doi          = {10.1016/j.asoc.2020.106683},
  journal      = {Applied Soft Computing},
  pages        = {106683},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Analyzing COVID-19 virus based on enhanced fragmented biological local aligner using improved ions motion optimization algorithm},
  volume       = {96},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). AGLNet: Towards real-time semantic segmentation of
self-driving images via attention-guided lightweight network.
<em>ASOC</em>, <em>96</em>, 106682. (<a
href="https://doi.org/10.1016/j.asoc.2020.106682">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The extensive computational burden limits the usage of convolutional neural networks (CNNs) in edge devices for image semantic segmentation , which plays a significant role in many real-world applications, such as augmented reality , robotics, and self-driving. To address this problem, this paper presents an attention-guided lightweight network, namely AGLNet , which employs an encoder–decoder architecture for real-time semantic segmentation . Specifically, the encoder adopts a novel residual module to abstract feature representations, where two new operations, channel split and shuffle, are utilized to greatly reduce computation cost while maintaining higher segmentation accuracy . On the other hand, instead of using complicated dilated convolution and artificially designed architecture, two types of attention mechanism are subsequently employed in the decoder to upsample features to match input resolution. Specifically, a factorized attention pyramid module (FAPM) is used to explore hierarchical spatial attention from high-level output, still remaining fewer model parameters. To delineate object shapes and boundaries, a global attention upsample module (GAUM) is adopted as global guidance for high-level features. The comprehensive experiments demonstrate that our approach achieves state-of-the-art results in terms of speed and accuracy on three self-driving datasets: CityScapes, CamVid, and Mapillary Vistas. AGLNet achieves 71.3\%, 69.4\%, and 30.7\% mean IoU on these datasets with only 1.12M model parameters. Our method also achieves 52 FPS, 90 FPS, and 53 FPS inference speed, respectively, using a single GTX 1080Ti GPU . Our code is open-source and available at https://github.com/xiaoyufenfei/Efficient-Segmentation-Networks .},
  archive      = {J_ASOC},
  author       = {Quan Zhou and Yu Wang and Yawen Fan and Xiaofu Wu and Suofei Zhang and Bin Kang and Longin Jan Latecki},
  doi          = {10.1016/j.asoc.2020.106682},
  journal      = {Applied Soft Computing},
  pages        = {106682},
  shortjournal = {Appl. Soft. Comput.},
  title        = {AGLNet: Towards real-time semantic segmentation of self-driving images via attention-guided lightweight network},
  volume       = {96},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A heuristic approach for optimal integrated airline schedule
design and fleet assignment with demand recapture. <em>ASOC</em>,
<em>96</em>, 106681. (<a
href="https://doi.org/10.1016/j.asoc.2020.106681">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Flight schedule design and fleet assignment are the two main elements of the airline scheduling process, which have the highest effect on cost and revenue. Although mixed-integer linear programming models were developed for integrated schedule design and fleet assignment, it has been shown that this approach was not efficient for large-scale models. Therefore, this paper aimed at developing a parallel master–slave Genetic Algorithm (PMS-GA) for solving the integrated flight schedule design and fleet assignment problem with demand recapture, particularly for large-scale problems. The integrated schedule design and fleet assignment problem was solved by the master GA, while the slave GA nested inside the master GA solved passenger flow adjustment problem. Considering the complexities of a large-scale integrated problem, we (1) proposed an innovative approach for creating feasible suboptimal initial population, (2) developed customized genetic operators to improve the performance of the PMS-GA compared to the conventional GAs, and (3) implemented migration and repopulation to prevent premature convergence. PMS-GA was tested on seven models with small-, medium-, and large-scales, and the results were compared with the gold-standard mixed-integer linear programming in terms of cost and runtime. The comparative study showed that the PMS-GA achieved suboptimal solutions with costs only 1.8\% to 3.0\% different than the optimal solution for medium- and large-scale models. However, these solutions were obtained in significantly shorter runtimes (over 500\% to 1000\%) compared to the mixed-integer linear programming. Also, the results showed that in contrast to the mixed-integer linear programming approach, runtimes of the proposed PMS-GA are highly predictable as a function of the problem size. Our results showed the importance of PMS-GA for integrated schedule design and fleet assignment, particularly for solving large-scale re-scheduling problems in a short time.},
  archive      = {J_ASOC},
  author       = {Esmaeel Khanmirza and Milad Nazarahari and Morteza Haghbeigi},
  doi          = {10.1016/j.asoc.2020.106681},
  journal      = {Applied Soft Computing},
  pages        = {106681},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A heuristic approach for optimal integrated airline schedule design and fleet assignment with demand recapture},
  volume       = {96},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Multi-species evolutionary algorithm for wireless visual
sensor networks coverage optimization with changeable field of views.
<em>ASOC</em>, <em>96</em>, 106680. (<a
href="https://doi.org/10.1016/j.asoc.2020.106680">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The coverage optimization of wireless visual sensor networks (WVSNs) with changeable field of views (FOVs) brings more challenges on the decision making of modern cyber–physical​ systems as it is a multi-objective decision problem where some contradictory indices, e.g. the sensor coverage and the redundancy, should be taken into consideration concurrently. In this paper, a novel multi-species evolutionary algorithm (MSEA) is proposed to address this issue by introducing a multi-species evolution scheme to enhance the search ability. A competition mechanism based on the deductive sort and the crowding distances is developed to facilitate the generation of Pareto front and the elitist individuals are evolved from a multi-species hybrid population. Comparative results show a better balancing performance between the exploration and the exploitation of the proposed algorithm which induces a strong approximation to the feasible WVSN managements.},
  archive      = {J_ASOC},
  author       = {Daifeng Zhang and Jiliang Zhang},
  doi          = {10.1016/j.asoc.2020.106680},
  journal      = {Applied Soft Computing},
  pages        = {106680},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Multi-species evolutionary algorithm for wireless visual sensor networks coverage optimization with changeable field of views},
  volume       = {96},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A fuzzy clustering algorithm for developing predictive
models in construction applications. <em>ASOC</em>, <em>96</em>, 106679.
(<a href="https://doi.org/10.1016/j.asoc.2020.106679">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fuzzy inference systems (FISs) are a predictive modeling technique based on fuzzy sets that utilize approximate reasoning to mimic the decision-making process of human experts. There are several expert- and data-driven methods for developing FISs, among which fuzzy clustering algorithms are the most frequently used data-driven methods. This paper introduces a new fuzzy clustering algorithm for developing FISs in construction applications that addresses two limitations of existing fuzzy clustering algorithms : the lack of capacity to determine the number of clusters automatically from the characteristics of the data, and the poor performance in predictive modeling of highly dimensional problems. Existing fuzzy clustering algorithms are limited in construction applications since determining the number of clusters based on subjective expert judgment reduces the accuracy of the resulting FIS, and construction systems are often highly dimensional with a large number of inputs affecting the system outputs. The fuzzy clustering algorithm proposed in this paper determines the number of clusters automatically based on the characteristics of the data, specifically the non-linearity observed within clusters, and assigns weights to the rules of FISs to improve their accuracy in highly dimensional problems. This paper advances the state-of-the-art of fuzzy clustering and contributes to construction modeling by providing a new data-driven technique for developing FISs that suits the characteristics of construction problems.},
  archive      = {J_ASOC},
  author       = {Nima Gerami Seresht ( Ph.D. ) and Rodolfo Lourenzutti ( Ph.D. ) and Aminah Robinson Fayek ( Ph.D., P.Eng. )},
  doi          = {10.1016/j.asoc.2020.106679},
  journal      = {Applied Soft Computing},
  pages        = {106679},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A fuzzy clustering algorithm for developing predictive models in construction applications},
  volume       = {96},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). QN-docking: An innovative molecular docking methodology
based on q-networks. <em>ASOC</em>, <em>96</em>, 106678. (<a
href="https://doi.org/10.1016/j.asoc.2020.106678">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Molecular docking is often used in computational chemistry to accelerate drug discovery at early stages. Many molecular simulations are performed to select the right pharmacological candidate. However, traditional docking methods are based on optimization heuristics such as Monte Carlo or genetic that try several hundreds of these candidates giving rise to expensive computations. Thus, an alternative methodology called QN-Docking is proposed for developing docking simulations more efficiently. This new approach is built upon Q-learning using a single-layer feedforward neural network to train a single ligand or drug candidate (the agent) to find its optimal interaction with the host molecule. In addition, the corresponding Reinforcement Learning environment and the reward function based on a force-field scoring function are implemented. The proposed method is evaluated in an exemplary molecular scenario based on the kaempferol and beta-cyclodextrin. Results for the prediction phase show that QN-Docking achieves 8 × × speedup compared to stochastic methods such as METADOCK 2, a novel high-throughput parallel metaheuristic software for docking. Moreover, these results could be extended to many other ligand-host pairs to ultimately develop a general and faster docking method.},
  archive      = {J_ASOC},
  author       = {Antonio Serrano and Baldomero Imbernón and Horacio Pérez-Sánchez and José M. Cecilia and Andrés Bueno-Crespo and José L. Abellán},
  doi          = {10.1016/j.asoc.2020.106678},
  journal      = {Applied Soft Computing},
  pages        = {106678},
  shortjournal = {Appl. Soft. Comput.},
  title        = {QN-docking: An innovative molecular docking methodology based on Q-networks},
  volume       = {96},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Multi-stage fuzzy swarm intelligence for automatic hepatic
lesion segmentation from CT scans. <em>ASOC</em>, <em>96</em>, 106677.
(<a href="https://doi.org/10.1016/j.asoc.2020.106677">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Segmentation of liver and hepatic lesions using computed tomography (CT) is a critical and challenging task for doctors to accurately identify liver abnormalities and to reduce the risk of liver surgery. This study proposed a novel dynamic approach to improve the fuzzy c-means (FCM) clustering algorithm for automatic localization and segmentation of liver and hepatic lesions from CT scans. More specifically, we developed a powerful optimization approach in terms of accuracy, speed, and optimal convergence based on fast-FCM, chaos theory, and bio-inspired ant lion optimizer (ALO), named (CALOFCM), for automatic liver and hepatic lesion segmentation . We employed ALO to guide the FCM to determine the optimal cluster centroids for segmentation processes . We used chaos theory to improve the performance of ALO in terms of convergence speed and local minima avoidance. In addition, chaos theory-based ALO prevented the FCM from getting stuck in local minima and increased computational performance, thus increasing stability, reducing sensitivity in the iterative process, and allowing the best centroids to be used by FCM. We validated the proposed approach on a group of patients with abdominal liver CT images, and the results showed good detection and segmentation performance compared with other popular techniques. This new hybrid approach allowed for the clinical diagnosis of hepatic lesions earlier and more systematically, thereby helping medical experts in their decision-making.},
  archive      = {J_ASOC},
  author       = {Ahmed M. Anter and Siddhartha Bhattacharyya and Zhiguo Zhang},
  doi          = {10.1016/j.asoc.2020.106677},
  journal      = {Applied Soft Computing},
  pages        = {106677},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Multi-stage fuzzy swarm intelligence for automatic hepatic lesion segmentation from CT scans},
  volume       = {96},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Modified newton integration neural algorithm for solving the
multi-linear m-tensor equation. <em>ASOC</em>, <em>96</em>, 106674. (<a
href="https://doi.org/10.1016/j.asoc.2020.106674">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper attends to solve the multi-linear equations with special structure, e.g., the multi-linear M M -tensor equation, which frequently appears in engineering applications such as deep learning and hypergraph. For its critical and promising role, there are numbers of resolving schemes devoting to obtain a high-performing solution of the multi-linear M M -tensor equation. However, few investigations are discovered with noise-suppression ability till now. To be proper with digital devices and further improve the solving effectiveness, it is desirable to design a discrete-time computational algorithm with noise-suppression ability and high-performing property. Inspired by the aforementioned requirements, this paper proposes a modified Newton integration (MNI) neural algorithm for solving the multi-linear M M -tensor equation with noise-suppression ability. Additionally, the corresponding robustness analyses on the proposed MNI neural algorithm are provided. Simultaneously, computer simulative experiments are generated to explain the capabilities and availabilities of the MNI neural algorithm in noise suppression . As a result, in terms of noise suppression , the proposed MNI neural algorithm is superior to other related algorithms, such as Newton–Raphson iterative (NRI) algorithm (Ding and Wei, 2016), discrete time neural network (DTNN) algorithm (Wang et al., 2019), and sufficient descent nonlinear conjugate gradient (SDNCG) algorithm (Liu et al., 2020).},
  archive      = {J_ASOC},
  author       = {Haoen Huang and Dongyang Fu and Jiazheng Zhang and Xiuchun Xiao and Guancheng Wang and Shan Liao},
  doi          = {10.1016/j.asoc.2020.106674},
  journal      = {Applied Soft Computing},
  pages        = {106674},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Modified newton integration neural algorithm for solving the multi-linear M-tensor equation},
  volume       = {96},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A physics-aware learning architecture with input transfer
networks for predictive modeling. <em>ASOC</em>, <em>96</em>, 106665.
(<a href="https://doi.org/10.1016/j.asoc.2020.106665">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hybrid modeling architectures seek to combine a machine learning model with a computationally efficient (simplified or partial) physics model to predict the behavior of physical systems. Existing sequential or parallel approaches to hybrid modeling do not typically exploit the potential relationship between the input or latent features of the partial and full physics. In addition, very few existing architectures take advantage of the provision to generously or on-demand sample the partial physics model. To address these gaps, we have developed a novel neural network-based hybrid architecture called “Opportunistic Physics-mining Transfer Mapping Architecture” or OPTMA. The goal of the OPTMA architecture is to facilitate greater exploitation of input space correlations between partial and full physics where they exist. To this end, a transfer neural network is used to transform the original inputs into modified inputs or latent features, where the partial physics operates on these artificially transformed features to produce the final prediction. An extended back-propagation approach and a Particle Swarm Optimization (to deal with multimodal loss functions) are used to train the network weights. The new architecture is first tested on a simple regression problem for analysis. It is then used to predict the behavior of more complex dynamic systems — an Inverted pendulum and the motion of an unmanned aerial vehicle , both under wind effects. Subsequent tests on unseen samples demonstrate OPTMA’s competitive performance compared to pure ANN and sequential hybrid models and provide empirical validation of the transfer concepts underlying OPTMA.},
  archive      = {J_ASOC},
  author       = {Amir Behjat and Chen Zeng and Rahul Rai and Ion Matei and David Doermann and Souma Chowdhury},
  doi          = {10.1016/j.asoc.2020.106665},
  journal      = {Applied Soft Computing},
  pages        = {106665},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A physics-aware learning architecture with input transfer networks for predictive modeling},
  volume       = {96},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A multiclass SVM classifier with teaching learning based
feature subset selection for enzyme subclass classification.
<em>ASOC</em>, <em>96</em>, 106664. (<a
href="https://doi.org/10.1016/j.asoc.2020.106664">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The functional characterization of proteins aims to understand life at macroscopic and microscopic levels , thus having wide and extensive applications in biological and pharmaceutical research. The classical methods used for the functional characterization of proteins rely on the sequence similarity approach i.e. identifying a similar protein whose functions are known. However, the performance of these methods reduces in the event of the sequences exhibiting less similarity. Hence, to tide over this impediment, many computational function prediction methods that do not use the sequence similarity approach were developed. This paper focuses on a Support Vector Machine (SVM) based method which classifies a protein at three levels. At the first level, a protein is identified and earmarked as an enzyme or non-enzyme. In the case of it being an enzyme, its functional class, and subclass are predicted in one step by taking 7 EC classes with 63 of their subclasses. Each protein is initially represented by its 32 physicochemical properties . Then a multiclass SVM (MSVM) classifier, that solves an n n class classification problem with log 2 n log2n binary classifiers , along with a Modified Teaching Learning Based Optimization method (MTLBO) to identify the significant features for classification, is designed to predict the functional class, and subclass of an enzyme in one step. The recall of the proposed MSVM-MTLBO using only 25 features ranges from 92.97\% to 98.14\% for class label prediction and 60\% to 98.25\% for subclass label prediction, which is significantly better than the existing methods for class and subclass classification. Moreover, the proposed method predicts the subclass labels with less number of binary classifiers than a standard multiclass SVM.},
  archive      = {J_ASOC},
  author       = {Debasmita Pradhan and Biswajit Sahoo and Bijan Bihari Misra and Sudarsan Padhy},
  doi          = {10.1016/j.asoc.2020.106664},
  journal      = {Applied Soft Computing},
  pages        = {106664},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A multiclass SVM classifier with teaching learning based feature subset selection for enzyme subclass classification},
  volume       = {96},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Privacy-preserving facial recognition based on temporal
features. <em>ASOC</em>, <em>96</em>, 106662. (<a
href="https://doi.org/10.1016/j.asoc.2020.106662">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes a novel approach for privacy-preserving facial recognition based on the new feature computation technique: Local Binary Pattern from Temporal Planes (LBP-TP) that extracts information from only the X T XT or Y T YT planes of a video sequence; in contrast to previous work that depend significantly on spatial information within the video frames. To our knowledge, this is the first known facial recognition work that does not rely on the spatial plane, nor that requires processing a facial input. The removal of this spatial reliance therefore withholds the facial appearance information from public view, where only one-dimensional spatial information that varies across time are extracted for recognition. Privacy is thus assured, yet without impeding the facial recognition task which is vital for many security applications such as street surveillance and perimeter access control. Experimental results indicate that the proposed method achieves accuracy of 99.56\%, 98.19\% and 100\% for the recent CASME II, CAS(ME) 2 and Honda/UCSD databases respectively. In addition, a 66\% reduction in the number of bytes required for storage and recognition was also observed from these experiments. The outcomes of this research demonstrate that privacy in face recognition can be preserved, without compromising its security (i.e., recognition accuracy) and efficiency.},
  archive      = {J_ASOC},
  author       = {Shu-Min Leong and Raphaël C.-W. Phan and Vishnu Monn Baskaran and Chee-Pun Ooi},
  doi          = {10.1016/j.asoc.2020.106662},
  journal      = {Applied Soft Computing},
  pages        = {106662},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Privacy-preserving facial recognition based on temporal features},
  volume       = {96},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A many-objective particle swarm optimization with grid
dominance ranking and clustering. <em>ASOC</em>, <em>96</em>, 106661.
(<a href="https://doi.org/10.1016/j.asoc.2020.106661">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The existing MOPSOs face a great challenge in dealing with many-objective problems, due to the low discriminability of particles in many-objective spaces, which will affect the selection of leaders, thereby deteriorating the effectiveness of the algorithm. By using the property of domination, with the mapping of grid coordinates, this paper presents a framework of grid-based ranking scheme to sort the particles in grid space, which can select the swarm leaders ( g b e s t gbest , p b e s t pbest ) efficiently in MOPSO and enhance the convergence. Moreover, the clustering operation is conducted to update and maintain the external archive in the grid space to output a more diverse Pareto front . The performance of the proposed algorithm is verified by benchmark comparisons with several state-of-the-art MOPSOs and MOEAs . The DTLZ and WFG test suites with 4 to 10 objectives are used to assess the performance of our proposal. Experimental results demonstrate the promising performance of the proposed algorithm in terms of both optimization quality and convergence speed. Additionally, we discuss the influence of grid partition on efficiency, which verifies the effectiveness of the proposal from the other aspect.},
  archive      = {J_ASOC},
  author       = {Li Li and Guangpeng Li and Liang Chang},
  doi          = {10.1016/j.asoc.2020.106661},
  journal      = {Applied Soft Computing},
  pages        = {106661},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A many-objective particle swarm optimization with grid dominance ranking and clustering},
  volume       = {96},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A modified interval valued intuitionistic fuzzy CODAS method
and its application to multi-criteria selection among renewable energy
alternatives in turkey. <em>ASOC</em>, <em>96</em>, 106660. (<a
href="https://doi.org/10.1016/j.asoc.2020.106660">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Combinative Distance based ASsesment (CODAS) method aims to perform multi-criteria selection process according to the largest Euclidean and Taxicab distance with respect to negative ideal solutions. Recently, several CODAS methods have been applied to multi-criteria decision making problems with interval valued intuitionistic fuzzy sets. This paper demonstrates the weaknesses of using Euclidean and Taxicab distance on interval valued intuitionistic fuzzy sets and provides alternative strategies to model the vagueness and uncertainty in decision maker evaluations more effectively. The contribution of this paper is twofold. First, a new selection metric is defined in order to eliminate the disadvantages of using Euclidean and Taxicab distance in interval valued intuitionistic fuzzy CODAS. Second, a new fuzzy aggregation operator is proposed for aggregating decision maker evaluations by using fuzzy weights rather than using crisp weights. To show the effectiveness of the modified CODAS method, an application is given for multi-criteria selection of renewable energy alternatives in Turkey and the results are compared with two other interval valued intuitionistic fuzzy CODAS methods in the literature.},
  archive      = {J_ASOC},
  author       = {Kaan Deveci and Rabia Cin and Ahmet Kağızman},
  doi          = {10.1016/j.asoc.2020.106660},
  journal      = {Applied Soft Computing},
  pages        = {106660},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A modified interval valued intuitionistic fuzzy CODAS method and its application to multi-criteria selection among renewable energy alternatives in turkey},
  volume       = {96},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Optimized compression and recovery of electrocardiographic
signal for IoT platform. <em>ASOC</em>, <em>96</em>, 106659. (<a
href="https://doi.org/10.1016/j.asoc.2020.106659">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Internet of Things (IoT) promises to a continuous, remote recording and monitoring of electrocardiogram (ECG). Thus it creates large volumes of data for healthcare purposes. The huge recordings result in the heavy burden of the communication, and the wearable devices require frequent charging since the huge data consumes energy quickly. To ameliorate this, we aim to compress the recordings and, in turn, to boost the battery life. We propose a new framework from two aspects: First, an optimization solution is proposed for the measurement matrix , which uses the shrinking singular value decomposition (SSVD) strategy at the compression terminal. Second, an accelerated method based on the non-uniform norm (ANN) is proposed to estimate and reconstruct the received signal. The proposed framework of the measurement matrix optimization and ANN estimator is firstly used for the monitoring of vital parameters such as electrocardiography (ECG). Experiments are conducted to confirm the superiority of the proposed SSVD and ANN methods.},
  archive      = {J_ASOC},
  author       = {Fei-Yun Wu and Kunde Yang and Xueli Sheng},
  doi          = {10.1016/j.asoc.2020.106659},
  journal      = {Applied Soft Computing},
  pages        = {106659},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Optimized compression and recovery of electrocardiographic signal for IoT platform},
  volume       = {96},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A robust cyberattack detection approach using optimal
features of SCADA power systems in smart grids. <em>ASOC</em>,
<em>96</em>, 106658. (<a
href="https://doi.org/10.1016/j.asoc.2020.106658">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Smart grids are a type of complex cyber–physical system (CPS) that integrates the communication capabilities of smart devices into the grid to facilitate remote operation and control of power systems . However, this integration exposes many existing vulnerabilities of conventional supervisory control and data acquisition (SCADA) systems, resulting in severe cyber threats to the smart grid and potential violation of security objectives. Stealing sensitive information , modifying firmware, or injecting function codes through compromised devices are examples of possible attacks on the smart grid. Therefore, early detection of cyberattacks on the grid is crucial to protect it from sabotage. Machine learning (ML) methods are conventional approaches for detecting cyberattacks that use features of smart grid networks . However, developing an effective, highly accurate detection method with reduced computational overload, is still a challenging research problem. In this work, an efficient and effective security control approach is proposed to detect cyberattacks on the smart grid. The proposed approach combines both feature reduction and detection techniques to reduce the extremely large number of features and achieve an improved detection rate. A correlation-based feature selection (CFS) method is used to remove irrelevant features, improving detection efficiency. An instance-based learning (IBL) algorithm classifies normal and cyberattack events using the selected optimal features. This study describes a set of experiments conducted on public datasets from a SCADA power system based on a 10-fold cross-validation technique. Experimental results show that the proposed approach achieves a high detection rate based on a small number of features drawn from SCADA power system measurements .},
  archive      = {J_ASOC},
  author       = {Abdu Gumaei and Mohammad Mehedi Hassan and Shamsul Huda and Md. Rafiul Hassan and David Camacho and Javier Del Ser and Giancarlo Fortino},
  doi          = {10.1016/j.asoc.2020.106658},
  journal      = {Applied Soft Computing},
  pages        = {106658},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A robust cyberattack detection approach using optimal features of SCADA power systems in smart grids},
  volume       = {96},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Distracted driver detection by combining in-vehicle and
image data using deep learning. <em>ASOC</em>, <em>96</em>, 106657. (<a
href="https://doi.org/10.1016/j.asoc.2020.106657">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Distracted driving is among the most important reasons for traffic accidents today. Recently, there is an increasing interest in building driver assistance systems that detect the actions of the drivers and help them drive safer. In these studies, although some distinct data types such as the physical conditions of the driver, audio and visual features, car information are used; the main data source is the images of the driver that include the face, arms, and hands taken with a camera placed inside the car. In this work, we propose to integrate sensor data into the vision-based distracted driver detection model to improve the generalization ability of the system. With this purpose, we created a new data set that includes driver images and sensor data collected from real-world drives. Then, we constructed a two-stage distracted driving detection system to detect nine distracted behaviors. In the first stage, vision-based Convolutional Neural Network (CNN) models were created by transfer learning and fine-tuning methods. In the second stage, Long-Short Term Memory-Recurrent Neural Network (LSTM-RNN) models were created using sensor and image data together. We evaluate our system by two different fusion techniques and show that integrating sensor data to image-based driver detection significantly increases the overall performance with both of the fusion techniques. We also show that the accuracy of the vision-based model increases by fine-tuning the pre-trained CNN model using a related public dataset.},
  archive      = {J_ASOC},
  author       = {Furkan Omerustaoglu and C. Okan Sakar and Gorkem Kar},
  doi          = {10.1016/j.asoc.2020.106657},
  journal      = {Applied Soft Computing},
  pages        = {106657},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Distracted driver detection by combining in-vehicle and image data using deep learning},
  volume       = {96},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A new artificial bee colony algorithm employing intelligent
forager forwarding strategies. <em>ASOC</em>, <em>96</em>, 106656. (<a
href="https://doi.org/10.1016/j.asoc.2020.106656">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Artificial Bee Colony (ABC) algorithm that mimics the intelligent foraging behaviors of real honey bees has been successfully applied to different types of optimization problems in recent years. Actually, the main reason lying behind the high preference of the ABC algorithm is related with its good performance on solving difficult optimization problems due to the effective search mechanisms existing in a single cycle and easily-implementable bee phases. However, with the purpose of increasing the implementation simplicity and generalizing the principal concept of the algorithm, some significant behaviors of the real foraging bees are not closely simulated and integrated into the workflow of the ABC or tried to be managed by employing simple randomized or conditional operations. In this study, in order to increase the performance of ABC algorithm, the complex behavior of the foraging employed bees related with how they decide to pass through to the dance area and how long they stay on there for informing onlookers is modeled in detail and then a new variant of ABC called intelligent forager forwarding ABC for short iff -ABC is proposed. For analyzing the possible contributions of the intelligent forager forwarding strategy on the performance of the ABC algorithm, thirteen classical benchmark problems and fifteen computationally expensive benchmark problems presented at the CEC 2015 were tested. The results obtained from the experimental studies were compared with the results of the different meta-heuristics in addition to the well-known variants of the standard ABC algorithm. From the experimental studies, it was concluded that the intelligent forager forwarding strategy significantly improves the quality of the final solutions and the convergence speed of the ABC algorithm.},
  archive      = {J_ASOC},
  author       = {Selcuk Aslan and Dervis Karaboga and Hasan Badem},
  doi          = {10.1016/j.asoc.2020.106656},
  journal      = {Applied Soft Computing},
  pages        = {106656},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A new artificial bee colony algorithm employing intelligent forager forwarding strategies},
  volume       = {96},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Multi-objective biofilm algorithm (MOBifi) for de novo drug
design with special focus to anti-diabetic drugs. <em>ASOC</em>,
<em>96</em>, 106655. (<a
href="https://doi.org/10.1016/j.asoc.2020.106655">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Swarm intelligence algorithms are inspired by the collective and intelligent behaviorof micro-organisms, insects, birds or animals, to solve real-world optimization problems . The extraordinary intelligence and coordination observed among bacteria in biofilm formed the basis of a novel Biofilm algorithm introduced earlier. It simulates the life cycle of bacteria in a biofilm in terms of attachment, maturation and dispersal phases. Here, the Biofilm algorithm is extended to solve multi-objective problems. For the multi-objective biofilm algorithm (MOBifi), the concept of pareto dominance is followed. MOBifi is evaluated using CEC 2009 tri-objective benchmark test functions and compared with well-known multi-objective algorithms. Further, the utility of the MOBifi algorithm is substantiated with a real world multi-objective optimization problem, viz., de novo drug design (DNDD) of anti-diabetic drug-like molecules. The newly designed drug-like molecules using MOBifi are evaluated for their novelty and ability to interact with the anti-diabetic drug targets. The results showed that MOBifi could design novel molecules that could be further tested in the laboratory for diabetes treatment. The MOBifi algorithm disclosed here could be extended for other multi-objective optimization problems as well.},
  archive      = {J_ASOC},
  author       = {R. Vasundhara Devi and S. Siva Sathya and Mohane Selvaraj Coumar},
  doi          = {10.1016/j.asoc.2020.106655},
  journal      = {Applied Soft Computing},
  pages        = {106655},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Multi-objective biofilm algorithm (MOBifi) for de novo drug design with special focus to anti-diabetic drugs},
  volume       = {96},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Soft multicriteria computing supporting decisions on the
forex market. <em>ASOC</em>, <em>96</em>, 106654. (<a
href="https://doi.org/10.1016/j.asoc.2020.106654">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The paper relates to decisions made by traders on the Forex market. Existing trading systems based on technical analysis use crisp relations–binary activation functions to generate BUY or SELL signals for the trader. A large number of signals generated independently by different indicators and relatively low accuracy of the signals is observed, so that they cannot be effectively used in decision-making processes. A new fuzzy multicriteria approach is proposed in this paper. It includes the application of the theory of fuzzy systems and multicriteria analysis. The crisp relations are replaced by fuzzy ones. The signals generated by different indicators at the same time are analyzed in a multicriteria space. Each criterion relates to different indicator with the value defined by a respective membership function. A multicriteria optimization problem is formulated in which the currency pairs — variants in the analysis, having maximum values of all criteria are looked for. An original dominance-based algorithm is presented. It calculates a selected set of non-dominated variants that can be presented to the trader. A computer-based system is constructed on the basis of the proposed approach and including the proposed algorithm. The system having data from the market derives selected variants for further analysis and also check their prediction accuracy. Numerical experiments on real data from the Forex market show advantages of the proposed approach in comparison to the existing traditional crisp systems.},
  archive      = {J_ASOC},
  author       = {Przemysław Juszczuk and Lech Kruś},
  doi          = {10.1016/j.asoc.2020.106654},
  journal      = {Applied Soft Computing},
  pages        = {106654},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Soft multicriteria computing supporting decisions on the forex market},
  volume       = {96},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Visual fixation prediction with incomplete attention map
based on brain storm optimization. <em>ASOC</em>, <em>96</em>, 106653.
(<a href="https://doi.org/10.1016/j.asoc.2020.106653">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We cannot see everything around us. Instead, the visual attention mechanism will select some fixations from extensive visual information for further processing. Many computational attention models have been proposed by imitating this mechanism. However, almost all of the state-of-the-art computational models output a complete saliency map , which means one needs to go through all the regions in a scene before figuring out which part is more salient. According to the findings of neuroscience researches, it is unnecessary to evaluate every part at the glance stage of visual perception. Many researchers believe that the attention maps in different parts of our brain should be an incomplete one. What illustrated in this paper is a visual fixation prediction model that calculates the attention regions in a partially random manner. The output saliency map indeed is incomplete. We first translate the fixation prediction problem to a 2-D searching problem, then apply a newly proposed swarm intelligence algorithm known as Brain Storm Optimization (BSO) to search the fixation in different scenes. The proposed method can guide the searching process, converging to relatively more salient positions during iterations without going through all parts of an image. We evaluate the proposed method on a large scale CAT2000 dataset and the Extended Complex Scene Saliency Dataset (ECSSD). By comparatively studying with the other eight fixation prediction models and 24 salient region detection models, results indicate that the proposed method is effective in predicting the fixation rapidly, which makes it a good candidate for computational visual attention modeling.},
  archive      = {J_ASOC},
  author       = {Jian Yang and Yang Shen and Yuhui Shi},
  doi          = {10.1016/j.asoc.2020.106653},
  journal      = {Applied Soft Computing},
  pages        = {106653},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Visual fixation prediction with incomplete attention map based on brain storm optimization},
  volume       = {96},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Lazy fine-tuning algorithms for naïve bayesian text
classification. <em>ASOC</em>, <em>96</em>, 106652. (<a
href="https://doi.org/10.1016/j.asoc.2020.106652">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The naïve Bayes (NB) learning algorithm is widely applied in many fields, particularly in text classification . However, its performance decreases when it is used in domains where its naïve assumption is violated or when the training set is too small to find accurate estimations of the probabilities. In this study, we propose a lazy fine-tuning naïve Bayes (LFTNB) method to address both problems. We propose a local fine-tuning algorithm that uses the nearest neighbors of a query instance to fine-tune the probability terms used by NB. Applying the nearest neighbors only makes the independence assumption more likely to be valid, whereas the fine-tuning algorithm is used to find more accurate estimations of the probability terms. The performance of the LFTNB approach was evaluated using 47 UCI datasets. The results show that the LFTNB method achieves superior performance than classical NB, eager FTNB, and k-nearest neighbor algorithms. We also propose eager and lazy fine-tuning versions of powerful NB-based text classification algorithms, namely, multinomial NB, complement NB, and one-versus-all NB. The empirical results using 18 UCI text classification datasets show that the proposed methods outperform untuned versions of these algorithms.},
  archive      = {J_ASOC},
  author       = {Khalil M. El Hindi and Reem R. Aljulaidan and Hussien AlSalman},
  doi          = {10.1016/j.asoc.2020.106652},
  journal      = {Applied Soft Computing},
  pages        = {106652},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Lazy fine-tuning algorithms for naïve bayesian text classification},
  volume       = {96},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Hybridizing gray wolf optimization (GWO) with grasshopper
optimization algorithm (GOA) for text feature selection and clustering.
<em>ASOC</em>, <em>96</em>, 106651. (<a
href="https://doi.org/10.1016/j.asoc.2020.106651">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Text analysis in the field of text mining requires complex techniques for handling several text documents. Text clustering is among the most effective tactics in the field of text mining, machine recruitment and pattern recognition. Computers can start organizing a corpus document in certain organizational structure of conceptual clusters using reasonable text-clustering method. Informative and un-informational functionalities of the text documents contain noisy, inconsequential and superfluous features. The main method of finding a new subset of informative feats for each document is the unsupervised selection of text features. The functional selection technique has two aims: (1) maximize text clustering algorithm reliability, (2) minimize the number of uninformative traits. The proposed technique is that it produces a mature convergence rate and requires minimal computational time and is trapped in local minima in a low dimensional space. The text data is fed as the input and pre-processing steps are performed in the document. Next, the text feature selection is processed by selecting the local optima from the text document and then selecting the best global optima from local optimum using hybrid GWO–GOA.​ Furthermore, the selected optima are clustered using the Fuzzy c-means (FCM) clustering algorithm. This algorithm improves the reliability and minimizes the computational time cost. Eight datasets are used in the proposed algorithm and the performance is envisaged efficaciously. The evaluation metrics used for performing text feature selection and text clustering are accuracy, precision, recall, F-measure, sensitivity, specificity and show better quality when comparing with various other algorithms. When comparing with GWO , GOA and the proposed hybrid GWO–GOA algorithm, the proposed methodology reveals 87.6\% of efficiency.},
  archive      = {J_ASOC},
  author       = {R. Purushothaman and S.P. Rajagopalan and Gopinath Dhandapani},
  doi          = {10.1016/j.asoc.2020.106651},
  journal      = {Applied Soft Computing},
  pages        = {106651},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Hybridizing gray wolf optimization (GWO) with grasshopper optimization algorithm (GOA) for text feature selection and clustering},
  volume       = {96},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A novel encoding for separable large-scale multi-objective
problems and its application to the optimisation of housing stock
improvements. <em>ASOC</em>, <em>96</em>, 106650. (<a
href="https://doi.org/10.1016/j.asoc.2020.106650">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Large-scale optimisation problems , having thousands of decision variables, are difficult as they have vast search spaces and the objectives lack sensitivity to each decision variable. Metaheuristics work well for large-scale single-objective optimisation, but there has been little work for large-scale, multi-objective optimisation. We show that, for the special case problem where the objectives are each additively-separable in isolation and share the same separability , the problem is not separable when considering the objectives together. We define a problem with this property: optimisation of housing stock improvements, which seeks to distribute limited public investment to achieve the optimal reduction in the housing stock’s energy demand. We then present a two-stage approach to encoding solutions for additively-separable, large-scale, multi-objective problems called Sequential Pareto Optimisation (SPO), which reformulates the global problem into a search over Pareto-optimal solutions for each sub-problem. SPO encoding is demonstrated for two popular MOEAs (NSGA-II and MOEA/D), and their relative performance is systematically analysed and explained using synthetic benchmark problems. We also show that reallocating seed solutions to the most appropriate sub-problems substantially improves the performance of MOEA/D, but overall NSGA-II still performs best. SPO outperforms a naive single-stage approach, in terms of the optimality of the solutions and the computational load, using both algorithms. SPO is then applied to a real-world housing stock optimisation problem with 4424 binary variables. SPO finds solutions that save 20\% of the cost of seed solutions yet obtain the same reduction in energy consumption. We also show how application of different intervention types vary along the Pareto front as cost increases but energy use decreases; e.g., solid wall insulation replacing cavity wall insulation, and condensing boilers giving way to heat pumps. We conclude with proposals for how this approach may be extended to non-separable and many-objective problems.},
  archive      = {J_ASOC},
  author       = {Alexander E.I. Brownlee and Jonathan A. Wright and Miaomiao He and Timothy Lee and Paul McMenemy},
  doi          = {10.1016/j.asoc.2020.106650},
  journal      = {Applied Soft Computing},
  pages        = {106650},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A novel encoding for separable large-scale multi-objective problems and its application to the optimisation of housing stock improvements},
  volume       = {96},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). An aggregated pairwise comparison-based evolutionary
algorithm for multi-objective and many-objective optimization.
<em>ASOC</em>, <em>96</em>, 106641. (<a
href="https://doi.org/10.1016/j.asoc.2020.106641">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As the number of objectives increases, the ability of Pareto optimality in providing enough comparability among alternative solutions would be deteriorated seriously. In order to address this issue, this paper proposes a simple yet efficient fitness evaluation approach based on the piecewise aggregated pairwise comparisons (PAPC) 1 with the assistance of the two-stage selection strategy. The advantages of the proposed PAPC are threefold: (1) all the optimal solutions of the PAPC fitness being less than 0 are non-dominated solutions, (2) for the dominated solutions, PAPC is a metric of the distance to the non-dominated solution front, and (3) for the non-dominated solutions, PAPC rewards the diversity and penalizes the clustering behavior. Accordingly, PAPC can increase the comparability among candidate solutions with an explicit consideration of the convergence and diversity. Then, we develop a two-stage selection strategy and a niching strategy to assist PAPC to maintain diversity of solutions. We conduct experiments on a suite of test problems with up to 10 objectives, where the algorithm is also extended to handle constrained problems. The experimental results validate the effectiveness of the proposed algorithm on both multi-objective optimization problems and many-objective optimization problems .},
  archive      = {J_ASOC},
  author       = {Xueyi Wang and Lianbo Ma and Shujun Yang and Min Huang and Xingwei Wang and Junfei Zhao and Xiaolong Shen},
  doi          = {10.1016/j.asoc.2020.106641},
  journal      = {Applied Soft Computing},
  pages        = {106641},
  shortjournal = {Appl. Soft. Comput.},
  title        = {An aggregated pairwise comparison-based evolutionary algorithm for multi-objective and many-objective optimization},
  volume       = {96},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Metaheuristic-based possibilistic multivariate fuzzy
weighted c-means algorithms for market segmentation. <em>ASOC</em>,
<em>96</em>, 106639. (<a
href="https://doi.org/10.1016/j.asoc.2020.106639">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study proposed the metaheuristic-based possibilistic multivariate fuzzy weighted c -means (PMFWCM) algorithm for clustering mixed data PMFWCM algorithm itself is normally used for numerical data. To implement in the real application of market segmentation, where the data usually contains both numerical and categorical attributes model improvement is a need. First, the distance between two mixed-attribute objects is calculated by using the object–cluster similarity measure. Then, three metaheuristics, i.e., genetic algorithm (GA), particle swarm optimization algorithm (PSO), and sine cosine algorithm (SCA), are employed to integrate with the PMFWCM algorithm for cluster analysis. This combination aims to improve the clustering performance of the PMFWCM algorithm and to make the clustering results more stable. To cluster a real-world dataset certainly, the experiment with benchmark datasets from UCI machine learning repository is conducted to verify the performance of the proposed algorithms. The experiment results show that the clustering performance of the SCA-PMFWCM, GA-PMFWCM and PSO-PMFWCM algorithms are better than that of the PMFWCM algorithm. Moreover, from case study results, the SCA-PMFWCM algorithm gives the smallest sum of squared error and computational time compared with the GA-PMFWCM, PSO-PMFWCM, and PMFWCM algorithms.},
  archive      = {J_ASOC},
  author       = {R.J. Kuo and Patipharn Amornnikun and Thi Phuong Quyen Nguyen},
  doi          = {10.1016/j.asoc.2020.106639},
  journal      = {Applied Soft Computing},
  pages        = {106639},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Metaheuristic-based possibilistic multivariate fuzzy weighted c-means algorithms for market segmentation},
  volume       = {96},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). An efficient learning based RFMFA technique for islanding
detection scheme in distributed generation systems. <em>ASOC</em>,
<em>96</em>, 106638. (<a
href="https://doi.org/10.1016/j.asoc.2020.106638">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes an effective hybrid system for islanding detection of distributed generation (DG). The proposed control scheme is a united execution of both the Random Forest (RF) and Moth–Flame Optimization (MFO) named as RFMFO. The fundamental objective of the proposed work is to diminish the Non-Detective Zone (NDZ) to as close as could be allowed and to keep the output power quality unaltered. Moreover, the issue of setting the discovery thresholds in the current methods is overwhelmed by this strategy. For intelligent islanding detection, Rate of Change of Frequency (ROCOF) is used; by this methodology at the target DG location is utilized as the input sets for an RF system. So as to extricate various features among islanding and grid disturbance, the precision of the RF is prepared by MFO algorithm . In the proposed work, the RF is utilized to classify the islanding and non-islanding events subject to the extracted features. A few conditions and diverse loading, switching operation, and network conditions are resolved to approve the practicality of the proposed method. The proposed technique is implemented in MATLAB/Simulink working platform. The sampling point of voltage swell is 0–1.4 × × 10 4 N and 2.7 × × 10 4 4 - 4 × × 10 4 4 N, voltage sag has the sampling point of 1.4 × × 10 4 4 – 2.7 × × 10 4 N and the PCC has the sampling point of 2.1 × × 10 4 4 - 4 × × 10 4 N. The performance of the DG is evaluated by using comparative analysis with the current methods.},
  archive      = {J_ASOC},
  author       = {Jetty Rajesh Reddy and Alagappan Pandian and Chilakala Rami Reddy},
  doi          = {10.1016/j.asoc.2020.106638},
  journal      = {Applied Soft Computing},
  pages        = {106638},
  shortjournal = {Appl. Soft. Comput.},
  title        = {An efficient learning based RFMFA technique for islanding detection scheme in distributed generation systems},
  volume       = {96},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Comparison of schedule generation schemes for designing
dispatching rules with genetic programming in the unrelated machines
environment. <em>ASOC</em>, <em>96</em>, 106637. (<a
href="https://doi.org/10.1016/j.asoc.2020.106637">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Automatically designing new dispatching rules (DRs) by genetic programming has become an increasingly researched topic. Such an approach enables that DRs can be designed efficiently for various scheduling problems. Furthermore, most automatically designed DRs outperform existing manually designed DRs. Most research focused solely on designing priority functions that were used to determine the order in which jobs should be scheduled. However, in some scheduling environments, besides only determining the order of the jobs, one has to additionally determine the allocation of jobs to machines. For that purpose, a schedule generation scheme (SGS), which constructs the schedule, has to be applied. Until now the influence of different choices in the design of the SGS has not been extensively researched, which could lead to the application of an SGS that would obtain inferior results. The main goal of this paper is to perform an analysis of different SGS variants. For that purpose, three SGS variants are tested, two of which are proposed in this paper. They are tested in several variations which differ in details like whether they insert idle times in the schedule, or if they select the job with the highest or lowest priority values. The obtained results demonstrate that the automatically designed DRs with the tested SGS variants perform better than manually designed DRs, but also that there is a significant difference in the performance between the different SGS types and variants. The best DRs are analysed and show that the main reason why they performed well was due to the more sophisticated decisions they made when selecting the appropriate machine for a job. The results suggest that it is best to apply SGS variants which use the evolved priority functions to choose both the next job and the appropriate machine for that job.},
  archive      = {J_ASOC},
  author       = {Marko Đurasević and Domagoj Jakobović},
  doi          = {10.1016/j.asoc.2020.106637},
  journal      = {Applied Soft Computing},
  pages        = {106637},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Comparison of schedule generation schemes for designing dispatching rules with genetic programming in the unrelated machines environment},
  volume       = {96},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Local ranking and global fusion for personalized
recommendation. <em>ASOC</em>, <em>96</em>, 106636. (<a
href="https://doi.org/10.1016/j.asoc.2020.106636">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most of top-n personalized recommendation algorithms perform a kind of global learning-to-rank based on the original user–item rating matrix, yet with an implicit presumption of a unified feature space for all users (all items). However, we argue that such a presumption is a limiting factor for better recommendation, because features learned in a global space could be too coarse-grained. As such, ranking a recommendation list against a unified space might be dominated by only a few significant aspects of users’ interests. In this paper, we propose a novel local ranking and global fusion (LRGF) framework for personalized recommendation, which allows using multiple fine-grained feature spaces for learning-to-rank. In LRGF, we first adopt two-stage random walk on the established user graph and item graph for selecting those mostly related users and items, so as to form multiple local user–item groups. Next, we propose a pairwise gap-aware Bayesian personalized ranking to obtain local ranking lists for each local user–item group. Finally, a global decision fusion is proposed to obtain the final top-n recommendation list for each user only from her local ranking lists. Experiments on the three real-world datasets show that the proposed LRGF outperforms the state-of-the-art schemes in terms of higher mean average precision and normalized discounted cumulative gain.},
  archive      = {J_ASOC},
  author       = {Xuejiao Yang and Bang Wang},
  doi          = {10.1016/j.asoc.2020.106636},
  journal      = {Applied Soft Computing},
  pages        = {106636},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Local ranking and global fusion for personalized recommendation},
  volume       = {96},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Enhanced multi-instant fuzzy switching control of nonlinear
system with unreliable communication channels. <em>ASOC</em>,
<em>96</em>, 106635. (<a
href="https://doi.org/10.1016/j.asoc.2020.106635">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The problem of enhancing the adaptability against unreliable communication channels for fuzzy system is investigated by proposing a new multi-instant fuzzy switching controller with a more flexible switching manner. Compared with recent methods in the reported literature, more switching modes have been designed according to the updated information at each sampling instant and thus much more freedom can be created by introducing and assigning additional variables for all the possible switching modes. As a result, the adaptability against unreliable communication channels can be enhanced from two aspects: for the same health quality of communication channels, much larger controllable domain can be provided; or for the same system parameters, a worse health quality of communication channels can be affordable. Finally, several advantages over previous fuzzy approaches are verified by means of numerical simulations.},
  archive      = {J_ASOC},
  author       = {Xiangpeng Xie and Hongfu Xie and Dong Yue and Chen Peng},
  doi          = {10.1016/j.asoc.2020.106635},
  journal      = {Applied Soft Computing},
  pages        = {106635},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Enhanced multi-instant fuzzy switching control of nonlinear system with unreliable communication channels},
  volume       = {96},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). An efficient semi-supervised manifold embedding for crowd
counting. <em>ASOC</em>, <em>96</em>, 106634. (<a
href="https://doi.org/10.1016/j.asoc.2020.106634">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Crowd counting is one of the most paramount tasks for safety and security. Many existing methods mainly focus on the predicted accuracy but ignore the efficiency, which hinders their applications in practice. Moreover, their performance heavily depends on the learning from a large number of labeled scene data, which is cost-expensive for crowd counting. In this paper, we present a novel crowd counting approach called semi-supervised manifold embedding (SSME) to address the above weaknesses. In the newly proposed method, we formulate the crowd counting as a semi-supervised classification problem and learn a linear mapping from the high-dimensional scene feature space to the low-dimension label space by simultaneously imposing the label fitness and the manifold smoothness, where the learned linear mapping facilitates the efficiency of crowd counting. In order to alleviate the issue that most supervised approaches to crowd counting require sufficient labeled data for improving the performance, we exploit the first neighbor propagation to select informative samples in the proposed SSME-based algorithm. Thorough validation experiments on three challenging benchmark datasets indicate that the proposed method is capable of achieving more impressive prediction accuracy on the number of pedestrians in a monitoring scene than other state-of-the-art competitors.},
  archive      = {J_ASOC},
  author       = {Kaibing Zhang and Huake Wang and Wei Liu and Minqi Li and Jian Lu and Zhonghua Liu},
  doi          = {10.1016/j.asoc.2020.106634},
  journal      = {Applied Soft Computing},
  pages        = {106634},
  shortjournal = {Appl. Soft. Comput.},
  title        = {An efficient semi-supervised manifold embedding for crowd counting},
  volume       = {96},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Discrete-time fractional-order control based on data-driven
equivalent model. <em>ASOC</em>, <em>96</em>, 106633. (<a
href="https://doi.org/10.1016/j.asoc.2020.106633">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Considering the case of highly uncertain nonlinear systems , a controller based on an equivalent data-driven model is proposed, such that, the implementation relies only on the input–output information of the controlled plant. With the aim of enhancing the closed-loop performance, a discrete-time fractional-order reaching law is studied. Firstly, the nonlinear system is expressed in terms of discrete-time input–output variations, by considering the pseudo partial derivative (PPD) concept. Then, a multi-input fuzzy rules emulating networks (MiFREN) system is considered to dynamically estimate the PPD. Finally, a discrete-time fractional-order controller is synthesized to enforce the asymptotic convergence of the tracking error. A comparison based on simulations and experimental results are provided to highlight the reliability of the proposed approach.},
  archive      = {J_ASOC},
  author       = {Chidentree Treesatayapun and Aldo Jonathan Muñoz-Vázquez},
  doi          = {10.1016/j.asoc.2020.106633},
  journal      = {Applied Soft Computing},
  pages        = {106633},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Discrete-time fractional-order control based on data-driven equivalent model},
  volume       = {96},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A bayesian regularized feed-forward neural network model for
conductivity prediction of PS/MWCNT nanocomposite film coatings.
<em>ASOC</em>, <em>96</em>, 106632. (<a
href="https://doi.org/10.1016/j.asoc.2020.106632">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In our present work, a multi-layered feed-forward neural network (FFNN) model was designed and developed to predict electrical conductivity of multi-walled carbon nanotube (MWCNT) doped polystyrene (PS) latex nanocomposite (PS/MWCNT) film coatings using data set gathered from several conductivity measurements . Surfactant concentrations ( C s Cs ), initiator concentrations ( C i Ci ), molecular weights ( M P S MPS ) and particle sizes of PS latex ( D P S DPS ) together with MWCNT concentrations ( R M W C N T RMWCNT ) were introduced as inputs while electrical conductivity ( σ σ ) was assigned as a single output in FFNN topology . Network training was carried out using a Bayesian regulation backpropagation algorithm . Optimal geometry of the hidden layer was first studied to search out the best FFNN topology providing the most accurate performance results. Mean squared error , MSE, mean absolute error , MAE, root-mean-squared error, RMSE, determination of coefficient, R 2 R2 , variance accounted for, VAF, and regression analysis were employed as performance assessment parameters for proposed network model. Correlation coefficients ( r r ) of each input variable together with relative importance-based sensitivity analysis results have shown that R M W C N T RMWCNT is the most significant input variable strongly affecting the σ σ value of PS/MWCNT nanocomposite film coatings and training performance of the neural network . Mathematical explicit function has been derived to model electrical conductivity by using weights and bias values at each neuron found in FFNN development. All predicted conductivity values are in a very good agreement with measured conductivity values, showing robustness and reliability of suggested FFNN model and it can be effectively used to predict electrical conductivity of PS/MWCNT nanocomposite film coatings.},
  archive      = {J_ASOC},
  author       = {Barış Demirbay and Duygu Bayram Kara and Şaziye Uğur},
  doi          = {10.1016/j.asoc.2020.106632},
  journal      = {Applied Soft Computing},
  pages        = {106632},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A bayesian regularized feed-forward neural network model for conductivity prediction of PS/MWCNT nanocomposite film coatings},
  volume       = {96},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Exact dynamic time warping calculation for weak sparse time
series. <em>ASOC</em>, <em>96</em>, 106631. (<a
href="https://doi.org/10.1016/j.asoc.2020.106631">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Dynamic Time Warping (DTW) technique is widely used in time series data mining. However, it should be pointed out that the calculation complexity of DTW is very high. In this paper, we propose an accurate and fast DTW calculation algorithm on weak sparse time series (WSTS). The algorithm takes the advantage of the weak sparse property, and it shows a remarkable time saving in DTW calculation. In addition, it should be emphasized that this algorithm for DTW calculation is an accurate one, which is one of the main contributions of this paper. The mathematical proof is also given to prove the accuracy of this algorithm. Several examples with different practical prospects are given to show the effectiveness of the proposed accurate and fast DTW calculation algorithm.},
  archive      = {J_ASOC},
  author       = {Lei Ge and Shun Chen},
  doi          = {10.1016/j.asoc.2020.106631},
  journal      = {Applied Soft Computing},
  pages        = {106631},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Exact dynamic time warping calculation for weak sparse time series},
  volume       = {96},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Cryptocurrency malware hunting: A deep recurrent neural
network approach. <em>ASOC</em>, <em>96</em>, 106630. (<a
href="https://doi.org/10.1016/j.asoc.2020.106630">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, cryptocurrency trades have increased dramatically, and this trend has attracted cyber-threat actors to exploit the existing vulnerabilities and infect their targets. The malicious actors use cryptocurrency malware to perform complex computational tasks using infected devices. Since cryptocurrency malware threats perform a legal process, it is a challenging task to detect this type of threat by a manual or heuristic method . In this paper, we propose a novel deep Recurrent Neural Network ( RNN ) learning model for hunting cryptocurrency malware threats. Specifically, our proposed model utilizes the RNN to analyze Windows applications’ operation codes (Opcodes) as a case study. We collect a real-world dataset that comprises of 500 cryptocurrency malware and 200 benign-ware samples, respectively. The proposed model trains with five different Long Short-Term Memory ( LSTM ) structures and is evaluated by a 10-fold cross-validation ( CV ) technique. The obtained results prove that a 3-layer configuration model gains 98\% of detection accuracy, which is the highest rate among other current configurations. We also applied traditional machine learning ( ML ) classifiers to show the applicability of deep learners ( LSTM ) versus traditional models in dealing with cryptocurrency malware.},
  archive      = {J_ASOC},
  author       = {Abbas Yazdinejad and Hamed HaddadPajouh and Ali Dehghantanha and Reza M. Parizi and Gautam Srivastava and Mu-Yen Chen},
  doi          = {10.1016/j.asoc.2020.106630},
  journal      = {Applied Soft Computing},
  pages        = {106630},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Cryptocurrency malware hunting: A deep recurrent neural network approach},
  volume       = {96},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). An effective iterated greedy algorithm for the distributed
permutation flowshop scheduling with due windows. <em>ASOC</em>,
<em>96</em>, 106629. (<a
href="https://doi.org/10.1016/j.asoc.2020.106629">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Distributed Permutation Flowshop Scheduling Problem (DPFSP) has become a research hotspot in recent years. However, as a service level objective , the Total Weighted Earliness and Tardiness (TWET) has not been addressed so far. Due to the importance of the service level objective in modern industry, we deal with the minimization of the TWET for the DPFSP with due windows . An Iterated Greedy (IG) algorithm, namely IG with Idle Time insertion Evaluation (IG I T E ITE ), is proposed. In the algorithm, an adapted NEH heuristic with five rules based on the unit earliness weight and unit tardiness weight, the due date, and the smallest slack on the last machine is used to generate an initial solution. Destruction procedure with a dynamic size is provided to enhance the exploration capability of the algorithm. Idle time insertion method is utilized to make the completion time of jobs within the due windows or as close to the due windows as possible. A large number of experiments show that the presented algorithm performs significantly better than the five competing algorithms adapted in the literature. The performance analysis shows that the IG I T E ITE is the most appropriate for the DPFSP with due windows among the tested algorithms.},
  archive      = {J_ASOC},
  author       = {Xue-Lei Jing and Quan-Ke Pan and Liang Gao and Yu-Long Wang},
  doi          = {10.1016/j.asoc.2020.106629},
  journal      = {Applied Soft Computing},
  pages        = {106629},
  shortjournal = {Appl. Soft. Comput.},
  title        = {An effective iterated greedy algorithm for the distributed permutation flowshop scheduling with due windows},
  volume       = {96},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Data-driven prognostics of rolling element bearings using a
novel error based evolving takagi–sugeno fuzzy model. <em>ASOC</em>,
<em>96</em>, 106628. (<a
href="https://doi.org/10.1016/j.asoc.2020.106628">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes a novel Error Based Evolving Takagi–Sugeno Fuzzy Model (EBeTS) and a new data-driven approach to fault prognostics based on that fuzzy model. The proposed evolving Takagi–Sugeno (TS) model is useful for fault prognostics when the degradation phenomena exhibit nonlinear and time-varying dynamics because the model can represent these characteristics. Since it is an evolving model, it learns the degradation behavior from stream data, although historical data can be used to improve it. Two well-established benchmarks are used to evaluate the EBeTS model and the proposed EBeTS-based prognostics approach. The experiments indicate that the proposed EBeTS-based prognostics approach can take advantage of both historical and new online data to estimate the Remaining Useful Life (RUL) and its uncertainties. Moreover, in most of the cases, it may outperform other methods that do not manage estimation errors and new data incorporation, e.g., fuzzy interacting multiple filters (IMMF) models, Evolving Extended Takagi–Sugeno (exTS) models, and Autoregressive Moving Average (ARMA) model models.},
  archive      = {J_ASOC},
  author       = {Murilo Osorio Camargos and Iury Bessa and Marcos Flávio Silveira Vasconcelos D’Angelo and Luciana Balieiro Cosme and Reinaldo Martínez Palhares},
  doi          = {10.1016/j.asoc.2020.106628},
  journal      = {Applied Soft Computing},
  pages        = {106628},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Data-driven prognostics of rolling element bearings using a novel error based evolving Takagi–Sugeno fuzzy model},
  volume       = {96},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A cascade perceptron and kohonen network approach to fault
location in rural distribution feeders. <em>ASOC</em>, <em>96</em>,
106627. (<a href="https://doi.org/10.1016/j.asoc.2020.106627">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The location of faults in the distribution system is one of the most important problems for utilities in the sector now since they directly impact the quality of the energy supply, as well as economical. Thus, the faster the fault identification and the electrical energy prompt reestablishment, the better are the quality indicators of these companies. In this scenario, this work presents an alternative and efficient method that uses artificial neural networks to locate faults in distribution feeders. The developed methodology was applied using real data from a rural distribution system. Thus, through simulations in the ATPDraw software, it is possible to obtain current and voltage values of the studied feeder, these being the inputs of the developed neural networks . The neural networks were developed in the Matlab® software in two stages: first, a Perceptron Multiple Layers networks perform the classification of the types of faults. These, in sequence, are used to select the Kohonen Self-Organizing Maps employed in the second stage to the location of the feeder fuse switch that acted at the time of the fault. Simulation results from the real system are presented to validate the proposed methodology, making it possible to accurately estimate the equipment that operated when the interruption occurred, which provides greater speed in reestablishing the electricity supply in the affected region, increasing the quality indicators and reducing maintenance and regulatory costs of electrical utilities.},
  archive      = {J_ASOC},
  author       = {Fabrício Augusto de Souza and Marcelo Favoretto Castoldi and Alessandro Goedtel and Murilo da Silva},
  doi          = {10.1016/j.asoc.2020.106627},
  journal      = {Applied Soft Computing},
  pages        = {106627},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A cascade perceptron and kohonen network approach to fault location in rural distribution feeders},
  volume       = {96},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Modelling and forecasting of COVID-19 spread using
wavelet-coupled random vector functional link networks. <em>ASOC</em>,
<em>96</em>, 106626. (<a
href="https://doi.org/10.1016/j.asoc.2020.106626">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Researchers around the world are applying various prediction models for COVID-19 to make informed decisions and impose appropriate control measures. Because of a high degree of uncertainty and lack of necessary data, the traditional models showed low accuracy over the long term forecast. Although the literature contains several attempts to address this issue, there is a need to improve the essential prediction capability of existing models. Therefore, this study focuses on modelling and forecasting of COVID-19 spread in the top 5 worst-hit countries as per the reports on 10th July 2020. They are Brazil , India, Peru, Russia and the USA. For this purpose, the popular and powerful random vector functional link (RVFL) network is hybridized with 1-D discrete wavelet transform and a wavelet-coupled RVFL (WCRVFL) network is proposed. The prediction performance of the proposed model is compared with the state-of-the-art support vector regression (SVR) model and the conventional RVFL model. A 60 day ahead daily forecasting is also shown for the proposed model. Experimental results indicate the potential of the WCRVFL model for COVID-19 spread forecasting.},
  archive      = {J_ASOC},
  author       = {Barenya Bikash Hazarika and Deepak Gupta},
  doi          = {10.1016/j.asoc.2020.106626},
  journal      = {Applied Soft Computing},
  pages        = {106626},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Modelling and forecasting of COVID-19 spread using wavelet-coupled random vector functional link networks},
  volume       = {96},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Distributed clustering in peer to peer networks using
multi-objective whale optimization. <em>ASOC</em>, <em>96</em>, 106625.
(<a href="https://doi.org/10.1016/j.asoc.2020.106625">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-objective clustering algorithms have superiority over its single objective counterparts as they include additional knowledge on properties of data in the form of objectives to discover the underlying clusters present in various datasets. This paper proposes a distributed clustering algorithm using multi-objective whale optimization (DMOWOA) for peer to peer network . The algorithm minimizes two objective functions to perform clustering, namely : Total Euclidean Deviation and Total Symmetrical Deviation. Both objective values are shared using diffusion method of cooperation to obtain correct partitioning at each peer. A single solution from the non-dominated solutions is selected as final solution based on its minimum distance to origin in the normalized objective space. The proposed algorithm’s performance is evaluated on four synthetic and five real-life wireless sensor network datasets (Canada weather station dataset, Delhi air pollution content dataset, Intel laboratory dataset, Washington cook agronomy farm dataset and Thames river water quality dataset). The comparison is carried out with multi-objective distributed particle swarm optimization (DMOPSO), distributed K-Means (DK-Means) and other seven recently developed nature inspired multi-objective clustering techniques . The proposed algorithm in most of the cases outperforms the existing techniques in terms of statistical measures Minkowski Score, Dunn index and Silhouette index. The average rank of the proposed algorithm is also better in Kruskal–Wallis statistical test.},
  archive      = {J_ASOC},
  author       = {Dinesh Kumar Kotary and Satyasai Jagannath Nanda},
  doi          = {10.1016/j.asoc.2020.106625},
  journal      = {Applied Soft Computing},
  pages        = {106625},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Distributed clustering in peer to peer networks using multi-objective whale optimization},
  volume       = {96},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A unified framework of deep networks for genre
classification using movie trailer. <em>ASOC</em>, <em>96</em>, 106624.
(<a href="https://doi.org/10.1016/j.asoc.2020.106624">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Affective video content analysis has emerged as one of the most challenging and essential research tasks as it aims to analyze the emotions elicited by videos automatically. However, little progress has been achieved in this field due to the enigmatic nature of emotions. This widens the gap between the human affective state and the structure of the video. In this paper, we propose a novel deep affect-based movie trailer classification framework. We also develop an EmoGDB dataset, which contains 100 Bollywood movie trailers annotated with popular movie genres: Action, Comedy, Drama, Horror, Romance, Thriller, and six different types of induced emotions: Anger, Fear, Happy, Neutral, Sad, Surprise. The affect-based features are learned via ILDNet architecture trained on the EmoGDB dataset. Our work aims to analyze the relationship between the emotions elicited by the movie trailers and how they contribute in solving the multi-label genre classification problem. The proposed novel framework is validated by performing cross-dataset testing on three large scale datasets, namely LMTD-9, MMTF-14K, and ML-25M datasets. Extensive experiments show that the proposed algorithm outperforms all the state-of-the-art methods significantly, as reported by the precision, recall, F1 score, precision–recall curves (PRC), and area under the PRC evaluation metrics .},
  archive      = {J_ASOC},
  author       = {Ashima Yadav and Dinesh Kumar Vishwakarma},
  doi          = {10.1016/j.asoc.2020.106624},
  journal      = {Applied Soft Computing},
  pages        = {106624},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A unified framework of deep networks for genre classification using movie trailer},
  volume       = {96},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A hybrid algorithm based on MOSFLA and GA for multi-UAVs
plant protection task assignment and sequencing optimization.
<em>ASOC</em>, <em>96</em>, 106623. (<a
href="https://doi.org/10.1016/j.asoc.2020.106623">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a Multi-Objective Shuffled Frog-Leaping Algorithm (MOSFLA) and Genetic Algorithm (GA) based task assignment and sequencing method, for multi-Unmanned Aerial Vehicles (multi-UAVs) plant-protection operation optimization. Based on full coverage of spray area, a dual decision model of non-operation flight distance and total operation time is developed considering energy consumption and operation efficiency. The proposed optimization method is hybridized using MOSFLA and GA: we first use modified MOSFLA for multi-UAVs operation assignment optimization, shrinking multi-UAVs operation cost including fields allocation, non-operation flight distance and operation time difference; we then employ GA for fields operation sequencing optimization, reducing the total operation time. Considering multi-UAVs’ take-off preparation delay effect, we established a latency time calculation model to determine total operation time for multi-UAVs. The test results show that: ① the non-operation flight distance cost for multi-UAVs using MOSFLA is less than that of single-UAV, which also performs better than that for multi-UAVs in traditional modes; ② the total operation time shrinks by using GA with known assignment matrix (including MOSFLA), which could save over 20 min compared with traditional modes; ③ the total operation time cost in MOSFLA–GA is less than that in traditional modes and optimized traditional modes, with same UAV number and preparation time for take-off; ④ the minimum iteration when assignment fitness value of MOSFLA attains maximum is small (&lt; 20); the minimum iteration when sequencing fitness value of GA attains maximum is less than 100, but is 5 in certain cases.},
  archive      = {J_ASOC},
  author       = {Yang Xu and Zhu Sun and Xinyu Xue and Wei Gu and Bin Peng},
  doi          = {10.1016/j.asoc.2020.106623},
  journal      = {Applied Soft Computing},
  pages        = {106623},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A hybrid algorithm based on MOSFLA and GA for multi-UAVs plant protection task assignment and sequencing optimization},
  volume       = {96},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A generalized framework for ANFIS synthesis procedures by
clustering techniques. <em>ASOC</em>, <em>96</em>, 106622. (<a
href="https://doi.org/10.1016/j.asoc.2020.106622">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The application of machine learning and soft computing techniques for function approximation is a widely explored topic in literature. Neural networks , evolutionary algorithms and support vector machines proved to be very effective, although these models suffer from very low level of interpretability by human operators. Conversely, Adaptive Neuro Fuzzy Inference Systems (ANFISs) demonstrated to be very accurate models featured by a considerable degree of interpretability . In this paper, a general framework for ANFIS training by clustering is proposed and investigated. In particular, different derivative-free ANFIS synthesis procedures are considered for performance evaluation, by taking into account different clustering algorithms , dissimilarity measures and by including an additional neuro-fuzzy classifier downstream the clustering phase targeted to rule base refinement. The resulting ANFISs have been compared, in terms of effectiveness and efficiency, on several benchmark datasets against three suitable competitors, namely a Support Vector Regression , MultiLayer Perceptron and a K K -Nearest Neighbour decision rule. Computational results show that the proposed techniques tend to outperform competing strategies while, at same time, featuring models with lower structural complexity. A complete software suite implementing the proposed framework is freely available under an open-source licence.},
  archive      = {J_ASOC},
  author       = {Stefano Leonori and Alessio Martino and Massimiliano Luzi and Fabio Massimo Frattale Mascioli and Antonello Rizzi},
  doi          = {10.1016/j.asoc.2020.106622},
  journal      = {Applied Soft Computing},
  pages        = {106622},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A generalized framework for ANFIS synthesis procedures by clustering techniques},
  volume       = {96},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A holonic intelligent decision support system for urban
project planning by ant colony optimization algorithm. <em>ASOC</em>,
<em>96</em>, 106621. (<a
href="https://doi.org/10.1016/j.asoc.2020.106621">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hitherto, urbanization reached unprecedented spreading, various problems in the field increase from day to day, and makes the urban phenomena more dynamic and more complex. Therefore, it is important to call in experts and provide all stuff to establish urban projects’ plans, which often need to be achieved in a brief time. Actually, decision-makers need more and more updated plans and even sustainable solutions to convey eventual urban changes with maintaining intrinsic features of urban areas, such as coverage, inter-dependency, and coherency . Due to decision-makers yearnings and the short time allocated to planners, urban project planning remains an exhausting task; it resorts to arbitrary choices to find a good match of projects according to the intended situations. On the other hand, it should take care of the available resources like funds, land, water, energy, underground, and raw materials, which ought to be rationally exploited, and preserved for future generations. In this paper, the proposed intelligent decision support system (IDSS) aims to find out the best urban plans that fit urban projects to appropriate areas. It also employs the holonic approach to model complex and large-scale urban systems, where agents of each level apply a new multi-objective ant colony optimization algorithm called BKPACS for the urban project planning problem, which is viewed as a bounded knapsack problem (BKP). To produce global optimal urban plans, the main algorithm called H-MACO coordinates between the different levels of this holonic system. The experimental results on a set of urban projects about a province of Algeria show good quality plans produced in less time.},
  archive      = {J_ASOC},
  author       = {Boudjemaa Khelifa and Mohamed Ridda Laouar},
  doi          = {10.1016/j.asoc.2020.106621},
  journal      = {Applied Soft Computing},
  pages        = {106621},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A holonic intelligent decision support system for urban project planning by ant colony optimization algorithm},
  volume       = {96},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A novel hybrid model based on multi-objective harris hawks
optimization algorithm for daily PM2.5 and PM10 forecasting.
<em>ASOC</em>, <em>96</em>, 106620. (<a
href="https://doi.org/10.1016/j.asoc.2020.106620">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {High levels of air pollution can severely affect the living environment and even endanger the human lives. To reduce air pollution concentrations, and warn the public regarding the occurrence of hazardous air pollutants , an accurate and reliable air pollutant forecasting model must be designed. However, previous studies had numerous deficiencies, such as ignoring the importance of predictive stability and poor initial parameters; these deficiencies significantly affected the air pollution prediction performance. Therefore, in this study a novel hybrid model is proposed to address these issues. Powerful data preprocessing techniques are applied to decompose the original time series into different modes from low frequency to high frequency, and a new multi-objective Harris hawks optimization algorithm is developed to tune the parameters of the extreme learning machine (ELM) model with high forecasting accuracy and stability for prediction air pollution. The optimized ELM model is then used to predict a time-series of air pollution. Finally, a scientific and robust evaluation system with several error criteria, benchmark models , and experiments conducted using twelve air pollutant concentration time series from three cities in China is designed to assess the presented hybrid forecasting model. The experimental results indicate that the proposed hybrid model can achieve a more stable and higher predictive performance than other models, and its superior prediction ability may aid in developing effective plans for mitigating air pollutant emissions and preventing the health issues caused by air pollution.},
  archive      = {J_ASOC},
  author       = {Pei Du and Jianzhou Wang and Yan Hao and Tong Niu and Wendong Yang},
  doi          = {10.1016/j.asoc.2020.106620},
  journal      = {Applied Soft Computing},
  pages        = {106620},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A novel hybrid model based on multi-objective harris hawks optimization algorithm for daily PM2.5 and PM10 forecasting},
  volume       = {96},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Multi-fidelity meta-optimization for nature inspired
optimization algorithms. <em>ASOC</em>, <em>96</em>, 106619. (<a
href="https://doi.org/10.1016/j.asoc.2020.106619">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The last couple of decades have witnessed a steadily increasing applications of nature inspired optimization (NIO) in vast fields such as power engineering , environmental engineering, and civil engineering . Behavioral parameters deeply affect the optimization performance for a NIO algorithm . Meta-optimization is good choice for parameter optimization of NIOs, which uses an optimizer to optimize another optimizer. However, meta-optimization is a very time-consuming process. For this reason, we propose a multi-fidelity strategy based meta-optimization approach to speed up the parameter optimization. Four types of fidelity control functions determine the fidelity level in the course of meta-optimization. We test the proposed method in the meta-optimization systems with diverse meta-NIOs (cuckoo search, fruit fly optimizer, gray wolf optimizer, krill herd, and whale optimization algorithm), diverse optimized-NIOs (cuckoo search, differential evolution, particle swarm optimizer, squirrel search algorithm, and water wave optimizer), and diverse benchmark problems (Ackley-50, Eggholder-2, Michalewicz-5, Shubert-2, Sphere-50, and F1–20). We also apply it to a real-world engineering problem to estimate the source terms of gas emission. Experimental results indicate that multi-fidelity strategy can substantially speed up meta-optimization systems and hence has the potential to be generalized to various NIOs.},
  archive      = {J_ASOC},
  author       = {Hui Li and Zhiguo Huang and Xiao Liu and Chenbo Zeng and Peng Zou},
  doi          = {10.1016/j.asoc.2020.106619},
  journal      = {Applied Soft Computing},
  pages        = {106619},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Multi-fidelity meta-optimization for nature inspired optimization algorithms},
  volume       = {96},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A set-based differential evolution algorithm for
QoS-oriented and cost-effective ridesharing. <em>ASOC</em>, <em>96</em>,
106618. (<a href="https://doi.org/10.1016/j.asoc.2020.106618">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The rapid proliferation of private cars brings serious problems such as traffic congestion and air pollution. Ridesharing provides a promising way to alleviate these issues by allocating riders to drivers with similar itineraries. In this work, we focus on a QoS-oriented and cost-effective ridesharing problem and propose an efficient algorithm to solve the problem. First, we formulate the ridesharing model and encode the solutions based on sets considering both service quality and cost. Then, we develop a set-based differential evolution algorithm to search the global optimum for the formulated problem. From the algorithm aspect, we specifically design new operators, such as the greedy initialization, the inter-vehicle mutation, and the route-sensitive selection, to enhance the performance of differential evolution for dealing with the ridesharing problem. The experimental results show that our method outperforms the state-of-the-art methods on metropolis transport datasets.},
  archive      = {J_ASOC},
  author       = {Xinyi Zhang and Xinglin Zhang},
  doi          = {10.1016/j.asoc.2020.106618},
  journal      = {Applied Soft Computing},
  pages        = {106618},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A set-based differential evolution algorithm for QoS-oriented and cost-effective ridesharing},
  volume       = {96},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Clonal selection algorithm for energy minimization in
software defined networks. <em>ASOC</em>, <em>96</em>, 106617. (<a
href="https://doi.org/10.1016/j.asoc.2020.106617">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the advancements of Information and Communication Technologies (ICT), large scale distributed computing and massive data center infrastructures are becoming more common these days. Such trends have drastically put a lot of load on the volumes of data transferred over networks, thus necessitating close to capacity link utilizations flexible forwarding decision-making. Software-defined networking (SDN), with its inherent segregation of control and data planes, provides flexible decision making that can leverage the global network information available to the SDN controller for dynamic and accurate solutions. However, contemporary researchers have focused on the flexibility and security aspects of SDN, widely ignoring energy consumption strategies in next-generation IP networks, which otherwise is a crucial driver in any research field. The scanty existing energy minimization strategies are mostly based on aggregate traffic, which leads to imbalanced utilization of links and affects the quality of service (QoS) adversely. In this paper, we leverage SDN’s key benefits for reducing energy network consumption while realizing dynamic load balance with a few QoS constraints. To this end, a multiobjective optimization problem (MOOP) is formulated that attempts to minimize power consumption and link utilization. With different capacities of switches and links, finding optimal configurations and deciding best paths, even for relatively small networks, become computationally challenging and is, in fact, an NP-hard problem. In this paper, we propose to employ the Clonal Selection Algorithm (CSA), a discrete, metaheuristic solution to find out optimal solutions for this MOOP, namely a Clonal Selection based Energy Minimization (CSEM). Simulations have been carried out for testing the efficacy of the proposed CSEM using real-life network topologies and link-traffic data. The results obtained by the proposed CSEM prove to be efficacious, and the same have also been validated with three different benchmark functions to test the suitability of SDN for CSA.},
  archive      = {J_ASOC},
  author       = {M.W. Hussain and B. Pradhan and X.Z. Gao and K.H.K. Reddy and D.S. Roy},
  doi          = {10.1016/j.asoc.2020.106617},
  journal      = {Applied Soft Computing},
  pages        = {106617},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Clonal selection algorithm for energy minimization in software defined networks},
  volume       = {96},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Recurrent neural network for electromyographic gesture
recognition in transhumeral amputees. <em>ASOC</em>, <em>96</em>,
106616. (<a href="https://doi.org/10.1016/j.asoc.2020.106616">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Gesture recognition is a key aspect of myoelectric control of upper-limb prostheses and is rather complex to achieve for transhumeral amputees. The prosthesis control of upper arm movements must rely only on the arm muscles, which were not involved in these gestures before the amputation. For decades, machine learning has been used in research for upper-limb gesture recognition. However, reported classification accuracies for transhumeral amputees have not improved significantly since the 1990s. Latest developments in deep learning suggest it can outperform classical machine learning both in accuracy and processing time. This study aims to determine if a deep learning approach, specifically a Recurrent Neural Network (RNN), could better recognize the movement intents in transhumeral amputees. To do so, the classification accuracy and the processing time of the RNN were measured and compared to two state-of-the-art approaches that use a linear discriminant analysis (LDA) and a multilayer perceptron (MLP) respectively. All three approaches were used to classify the signals of five transhumeral amputees between 6 upper-limb gestures. For subjects 1, 3 and 5, the classification accuracy was significantly higher (p = 0.0002) for the RNN (79.7\%) compared to the LDA (67, 1\%) and the MLP (74, 1\%). Additionally, the RNN had a much smaller processing time, under 7 ms, compared to 385 ms and 377 ms for the LDA and the MLP respectively. Consequently, the RNN is better suited for a real-time prosthesis control that occurs between 100–250 ms. Results suggest deep learning as a viable solution for gesture recognition in transhumeral amputees.},
  archive      = {J_ASOC},
  author       = {Olivier Barron and Maxime Raison and Guillaume Gaudet and Sofiane Achiche},
  doi          = {10.1016/j.asoc.2020.106616},
  journal      = {Applied Soft Computing},
  pages        = {106616},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Recurrent neural network for electromyographic gesture recognition in transhumeral amputees},
  volume       = {96},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A spatio-temporal attention-based spot-forecasting framework
for urban traffic prediction. <em>ASOC</em>, <em>96</em>, 106615. (<a
href="https://doi.org/10.1016/j.asoc.2020.106615">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Spatio-temporal forecasting is an open research field whose interest is growing exponentially. In this work we focus on creating a complex deep neural framework for spatio-temporal traffic forecasting with comparatively very good performance and that shows to be adaptable over several spatio-temporal conditions while remaining easy to understand and interpret. Our proposal is based on an interpretable attention-based neural network in which several modules are combined in order to capture key spatio-temporal time series components. Through extensive experimentation, we show how the results of our approach are stable and better than those of other state-of-the-art alternatives.},
  archive      = {J_ASOC},
  author       = {Rodrigo de Medrano and José L. Aznarte},
  doi          = {10.1016/j.asoc.2020.106615},
  journal      = {Applied Soft Computing},
  pages        = {106615},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A spatio-temporal attention-based spot-forecasting framework for urban traffic prediction},
  volume       = {96},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Intuitionistic fuzzy analytical network process models for
maritime supply chain. <em>ASOC</em>, <em>96</em>, 106614. (<a
href="https://doi.org/10.1016/j.asoc.2020.106614">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The aim of this study is to introduce three models along with four methods for intuitionistic fuzzy analytical network process (IFANP) and discuss the conceptual framework of process management for maritime supply chain. For the first time, in this study, centric consistency index is employed for all individual and aggregated judgment matrices of both membership and non membership functions together or separately. Secondly, consistency prioritization for each expert is implemented for all aggregated judgment matrices. Chang’s extent analysis method and Gaussian approximation for IFANP method are firstly employed to IFANP. A comparison between the conventional version and proposed models of IFANP is provided. Process optimization for maritime industry is conducted by considering several perspectives including financial, technical, operational and safety concerns.},
  archive      = {J_ASOC},
  author       = {Bekir Sahin and Ahmet Soylu},
  doi          = {10.1016/j.asoc.2020.106614},
  journal      = {Applied Soft Computing},
  pages        = {106614},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Intuitionistic fuzzy analytical network process models for maritime supply chain},
  volume       = {96},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A novel extended approach under hesitant fuzzy sets to
design a framework for assessing the key challenges of digital health
interventions adoption during the COVID-19 outbreak. <em>ASOC</em>,
<em>96</em>, 106613. (<a
href="https://doi.org/10.1016/j.asoc.2020.106613">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, Digital Technologies (DTs) are becoming an inseparable part of human lives. Thus, many scholars have conducted research to develop new tools and applications. Processing information, usually in the form of binary code , is the main task in DTs, which is happening through many devices, including computers, smartphones, robots, and applications. Surprisingly, the role of DTs has been highlighted in people’s life due to the COVID-19 pandemic. There are several different challenges to implement and intervene in DTs during the COVID-19 outbreak; therefore, the present study extended a new fuzzy approach under Hesitant Fuzzy Set (HFS) approach using Stepwise Weight Assessment Ratio Analysis (SWARA) and Weighted Aggregated Sum Product Assessment (WASPAS) method to evaluate and rank the critical challenges of DTs intervention to control the COVID-19 outbreak. In this regard, a comprehensive survey using literature and in-depth interviews have been carried out to identify the challenges under the SWOT (Strengths, Weaknesses, Opportunities, Threats) framework. Moreover, the SWARA procedure is applied to analyze and assess the challenges to DTs intervention during the COVID-19 outbreak, and the WASPAS approach is utilized to rank the DTs under hesitant fuzzy sets. Further, to demonstrate the efficacy and practicability of the developed framework, an illustrative case study has been analyzed. The results of this study found that Health Information Systems (HIS) was ranked as the first factor among other factors followed by a lack of digital knowledge, digital stratification, economic interventions, lack of reliable data, and cost inefficiency In conclusion, to confirm the steadiness and strength of the proposed framework, the obtained outputs are compared with other methods.},
  archive      = {J_ASOC},
  author       = {Abbas Mardani and Mahyar Kamali Saraji and Arunodaya Raj Mishra and Pratibha Rani},
  doi          = {10.1016/j.asoc.2020.106613},
  journal      = {Applied Soft Computing},
  pages        = {106613},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A novel extended approach under hesitant fuzzy sets to design a framework for assessing the key challenges of digital health interventions adoption during the COVID-19 outbreak},
  volume       = {96},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Application of artificial intelligence methods in vital
signs analysis of hospitalized patients: A systematic literature review.
<em>ASOC</em>, <em>96</em>, 106612. (<a
href="https://doi.org/10.1016/j.asoc.2020.106612">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In a hospital environment, patients are monitored continuously by electronic devices and health professionals. Therefore, a large amount of data is collected and stored in electronic health records systems for each patient. Among such data, vital signs are one of the most common and relevant types of information monitored to assess a patient’s health status. Artificial intelligence techniques can be used to analyze and learn useful standards from clinical datasets to provide better evidence to support the decisions of health professionals and thus help to improve patient health outcomes in hospitals. This systematic literature review aims to provide an updated computational perspective of how artificial intelligence has been applied to analyze the vital signs of adult hospitalized patients and the outcomes obtained. To this end, we reviewed 2899 scientific articles published between 2008 and 2018 and selected 78 articles that met our inclusion criteria to answer the research questions. Moreover, we used the information found in the reviewed articles to propose a taxonomy and identified the main concerns, challenges, and opportunities in this field. Our findings demonstrate that many researchers are exploring the use of artificial intelligence methods in tasks related to improving the health outcomes of hospitalized patients in distinct units. Additionally, although vital signs are significant predictors of clinical deterioration, they are not analyzed in isolation to predict or identify a clinical outcome. Our taxonomy and discussion contribute to the achievement of a significant degree of coverage regarding the aspects related to using machine learning to improve health outcomes in hospital environments, while highlighting gaps in the literature for future research.},
  archive      = {J_ASOC},
  author       = {Naira Kaieski and Cristiano André da Costa and Rodrigo da Rosa Righi and Priscila Schmidt Lora and Björn Eskofier},
  doi          = {10.1016/j.asoc.2020.106612},
  journal      = {Applied Soft Computing},
  pages        = {106612},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Application of artificial intelligence methods in vital signs analysis of hospitalized patients: A systematic literature review},
  volume       = {96},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Optimal designing and management of a stand-alone hybrid
energy system using meta-heuristic improved sine–cosine algorithm for
recreational center, case study for iran country. <em>ASOC</em>,
<em>96</em>, 106611. (<a
href="https://doi.org/10.1016/j.asoc.2020.106611">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, optimal designing and energy management of hybrid photovoltaic/wind/fuel cell (PV/WT/FC) system is presented with cost of hybrid system life span (CHSLS) minimizing and considering loss of load interruption probability (LOLIP) for Recreational Center of Gonbad (RCOG), a remote area region in Iran country using actual irradiance and wind speed data of this region. A meta-heuristic improved sine–cosine algorithm (ISCA) is used based on nonlinearly decreasing inertia weight strategy (NDIWS) for improving global and local search of the conventional SCA to find the optimal combination of hybrid system. In designing problem the optimal size of hybrid system components is determined using ISCA considering CHLS and LOLIP to satisfy the RCOG load demand in most cost-effective way. The performance of the proposed ISCA is compared with conventional SCA and well-known PSO methods in different combinations and under LOLIP changing conditions. Simulation results showed that the ISCA finds easily the optimal combination as PV/WT/FC system with lower CHSLS and better LOLIP. Also WT/FC system is not economical due to high CHSLS for RCOG. The results demonstrated that the average cost of per kW RCOG load supply using the PV/WT/FC for maximum LOLIP of 1\% is equal to 0.87$ and for maximum LOLIP of 5\% is 0.76$. The results showed the contribution and management of PV, WT and fuel cell together as PV/WT/FC is made a reliable and cost-effective energy system to supply the remote area application like RCOG. Moreover superiority of the ISCA than the SCA and PSO is proved with better convergence speed and accuracy. Also, the effect of some effective factors on the system designing is evaluated.},
  archive      = {J_ASOC},
  author       = {M. Jahannoush and S. Arabi Nowdeh},
  doi          = {10.1016/j.asoc.2020.106611},
  journal      = {Applied Soft Computing},
  pages        = {106611},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Optimal designing and management of a stand-alone hybrid energy system using meta-heuristic improved sine–cosine algorithm for recreational center, case study for iran country},
  volume       = {96},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Forecasting of COVID19 per regions using ARIMA models and
polynomial functions. <em>ASOC</em>, <em>96</em>, 106610. (<a
href="https://doi.org/10.1016/j.asoc.2020.106610">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {COVID-2019 is a global threat, for this reason around the world, researches have been focused on topics such as to detect it, prevent it, cure it, and predict it. Different analyses propose models to predict the evolution of this epidemic. These analyses propose models for specific geographical areas, specific countries, or create a global model. The models give us the possibility to predict the virus behavior, it could be used to make future response plans. This work presents an analysis of COVID-19 spread that shows a different angle for the whole world, through 6 geographic regions (continents). We propose to create a relationship between the countries, which are in the same geographical area to predict the advance of the virus. The countries in the same geographic region have variables with similar values (quantifiable and non-quantifiable), which affect the spread of the virus. We propose an algorithm to performed and evaluated the ARIMA model for 145 countries, which are distributed into 6 regions. Then, we construct a model for these regions using the ARIMA parameters, the population per 1M people, the number of cases, and polynomial functions. The proposal is able to predict the COVID-19 cases with a RMSE average of 144.81. The main outcome of this paper is showing a relation between COVID-19 behavior and population in a region, these results show us the opportunity to create more models to predict the COVID-19 behavior using variables as humidity, climate, culture, among others.},
  archive      = {J_ASOC},
  author       = {Andres Hernandez-Matamoros and Hamido Fujita and Toshitaka Hayashi and Hector Perez-Meana},
  doi          = {10.1016/j.asoc.2020.106610},
  journal      = {Applied Soft Computing},
  pages        = {106610},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Forecasting of COVID19 per regions using ARIMA models and polynomial functions},
  volume       = {96},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A collaborative LSHADE algorithm with comprehensive learning
mechanism. <em>ASOC</em>, <em>96</em>, 106609. (<a
href="https://doi.org/10.1016/j.asoc.2020.106609">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this study, a novel L-SHADE variant with collaborative scheme and comprehensive learning mechanism, named LSHADE-CLM, was proposed to improve the exploration and exploitation capabilities of the L-SHADE algorithm. In LSHADE-CLM, a novel cooperative mutation mechanism including “ D E ∕ c u r r e n t − t o − p b e t t e r ∕ r DE∕current−to−pbetter∕r ” and “ D E ∕ c u r r e n t − t o − p b e s t − w ∕ 1 DE∕current−to−pbest−w∕1 ” is proposed in the mutation operation. In the “ D E ∕ c u r r e n t − t o − p b e t t e r ∕ r DE∕current−to−pbetter∕r ” strategy with comprehensive learning mechanism, the population covariance matrix is utilized to generate candidate solutions and guide the search direction. Meanwhile, a competitive reward mechanism is implemented to control the mutation factor F F to generate a trial vector for the cooperative mechanism. Moreover, the dimensional reset strategy is applied to enhance the diversity of the population at the dimensional level when stagnation is identified at certain dimension. The proposed LSHADE-CLM is tested on the CEC2017 benchmark functions and compared with the other four state-of-the-art variants of L-SHADE. The experimental results demonstrated that the efficiency and effectiveness of the LSHADE-CLM algorithm for the non-separable optimization problem .},
  archive      = {J_ASOC},
  author       = {Fuqing Zhao and Lexi Zhao and Ling Wang and Houbin Song},
  doi          = {10.1016/j.asoc.2020.106609},
  journal      = {Applied Soft Computing},
  pages        = {106609},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A collaborative LSHADE algorithm with comprehensive learning mechanism},
  volume       = {96},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Task re-pricing model based on density-based spatial
clustering of applications. <em>ASOC</em>, <em>96</em>, 106608. (<a
href="https://doi.org/10.1016/j.asoc.2020.106608">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the retail market, gathering marketing data is essential at different stages of advertisement and promotion; currently, this is achieved via online crowdsourcing. The jobs involving such tasks must be reasonably priced to attract part-time employees depending on the retail budget. Herein, a new approach is presented to enhance the task repricing performance of online crowdsourcing platforms using the density-based spatial clustering of applications with noise ( DBSCAN ) algorithm, genetic general regression neural network ( G-GRNN ), and AdaBoost meta-algorithm. Initially, DBSCAN is used to analyze the agent task distribution, task density, and ambient agent credibility. Then, G-GRNN and AdaBoost are applied to reprice the tasks and evaluate the completeness of the repricing. Results show that the proposed method can optimize the repricing outcomes and enhance the efficiency of the repricing process. This study was conducted to address the crowdsourcing pricing problem from the perspective of firms. The results demonstrate the effectiveness of the machine learning methods ( DBSCAN, G-GRNN , and AdaBoost ) in solving the task repricing problem.},
  archive      = {J_ASOC},
  author       = {Chang Liu and Yang Cao},
  doi          = {10.1016/j.asoc.2020.106608},
  journal      = {Applied Soft Computing},
  pages        = {106608},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Task re-pricing model based on density-based spatial clustering of applications},
  volume       = {96},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). New complex fuzzy multiple objective programming procedure
for a portfolio making under uncertainty. <em>ASOC</em>, <em>96</em>,
106607. (<a href="https://doi.org/10.1016/j.asoc.2020.106607">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article deals with a still current decision making problem — where to invest and how much. A few “tools” can help to answer this question, namely fundamental, technical or psychological analysis. “Soft” aspects as a human intuition or mood on the capital market can also play a nonnegligible role. These approaches and techniques (mainly individually) provide a partial information on investment reality. In practice, an investment decision, or portfolio, is mostly made just based on such a limited information. Moreover, the assets’ shares in the portfolio must be implicitly added up. This “crumbled” approach may not be an expected support to make a satisfactory investment decision. Therefore, a complex decision making procedure is practically designed. It is based on the fuzzy multiple objective programming method whose unique algorithm, working with triangular fuzzy numbers, is proposed in this article. Although these methods of decision making theory are not widely used on the capital market, they can be very convenient. The developed approach enables multiple objectives to be simultaneously accepted, as well as vague information about the assets or vague investor preferences. Moreover, a valuable output of this multi-factor procedure is an explicit quantification of the assets’ share in the portfolio. The ability to include all these aspects, which plays a significant role in a decision making on the capital market, proves a strong application power (and advantage over the mentioned concepts) to make an investment portfolio satisfactorily. This strength is demonstrated on the Czech capital market with open unit trusts offered by Česká spořitelna. Based on the specified investment strategy, the satisfactory investment portfolio consisting of four open unit trusts is effectively made.},
  archive      = {J_ASOC},
  author       = {Adam Borovička},
  doi          = {10.1016/j.asoc.2020.106607},
  journal      = {Applied Soft Computing},
  pages        = {106607},
  shortjournal = {Appl. Soft. Comput.},
  title        = {New complex fuzzy multiple objective programming procedure for a portfolio making under uncertainty},
  volume       = {96},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Parameter estimation of a generalized lotka–volterra system
using a modified PSO algorithm. <em>ASOC</em>, <em>96</em>, 106606. (<a
href="https://doi.org/10.1016/j.asoc.2020.106606">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this study, parameter estimation of a generalized Lotka–Volterra system for three competing species is formulated as a multidimensional optimization problem . A modified version of a particle swarm algorithm is applied to solve it. The proposed algorithm, PSO + + , adds a new term to the standard PSO to diversify and improve search capability. First, a set of five benchmark functions was used to test the proposed algorithm, solving the global minimum localization problem . Once tested, the algorithm was used to estimate the parameters of a three-dimensional two-predator–one-prey system under conditions of chaotic behavior. Numerical simulations show the PSO + + increases accuracy by ∼ ∼ 10\% over standard PSO in the parameter estimation problem. Also, PSO + + can reconstruct the attractor and Lyapunov exponents of the system with MSE &amp;lt; 0.0001. Results show that the PSO + + algorithm can be a useful and powerful computational technique for parameter estimation of dynamical/chaotic systems, with accurate performance and very low deviations.},
  archive      = {J_ASOC},
  author       = {Juan A. Lazzús and Pedro Vega-Jorquera and Carlos H. López-Caraballo and Luis Palma-Chilla and Ignacio Salfate},
  doi          = {10.1016/j.asoc.2020.106606},
  journal      = {Applied Soft Computing},
  pages        = {106606},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Parameter estimation of a generalized Lotka–Volterra system using a modified PSO algorithm},
  volume       = {96},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Adversarial behaviours in mixing coins under incomplete
information. <em>ASOC</em>, <em>96</em>, 106605. (<a
href="https://doi.org/10.1016/j.asoc.2020.106605">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Criminals can launder crypto-currencies through mixing coins, whose original purpose is preservation of privacy in the presence of traceability. Therefore, it is essential to elaborately design mixing polices to achieve both privacy and anti-money laundering. Existing work on mixing policies relies on the knowledge of a blacklist. However, these policies are paralysed under the scenario where the blacklist is unknown or evolving. In this paper, we regard the above scenario as games under incomplete information where parties put down a deposit for the quality of coins, which is suitably managed by a smart contract in case of mixing bad coins. We extend the poison and haircut policies to incomplete information games, where the blacklist is updated after mixing. We prove the existence of equilibria for the improved polices, while it is known that there is no equilibria in the original poison and haircut policies, where blacklist is public known. Furthermore, we propose a seminal suicide policy: the one who mixes more bad coins will be punished by not having the deposit refunded. Thus, parties have no incentives to launder money by leveraging mixing coins. In effect, all three policies contrast money laundering while preserving privacy under incomplete information. Finally, we simulate and verify the validity of these policies.},
  archive      = {J_ASOC},
  author       = {Yilei Wang and Andrea Bracciali and Guoyu Yang and Tao Li and Xiaomei Yu},
  doi          = {10.1016/j.asoc.2020.106605},
  journal      = {Applied Soft Computing},
  pages        = {106605},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Adversarial behaviours in mixing coins under incomplete information},
  volume       = {96},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Automatic clustering using a local search-based human mental
search algorithm for image segmentation. <em>ASOC</em>, <em>96</em>,
106604. (<a href="https://doi.org/10.1016/j.asoc.2020.106604">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Clustering is a commonly employed approach to image segmentation . To overcome the problems of conventional algorithms such as getting trapped in local optima, in this paper, we propose an improved automatic clustering algorithm for image segmentation based on the human mental search (HMS) algorithm, a recently proposed method to solve complex optimisation problems . In contrast to most existing methods for image clustering, our approach does not require any prior knowledge about the number of clusters but rather determines the optimal number of clusters automatically. In addition, for further improved efficacy, we incorporate local search operators which are designed to make changes to the current cluster configuration . To evaluate the performance of our proposed algorithm, we perform an extensive comparison with several state-of-the-art algorithms on a benchmark set of images and using a variety of metrics including cost function, correctness of the obtained numbers of clusters, stability, as well as supervised and unsupervised segmentation criteria. The obtained results clearly indicate excellent performance compared to existing methods with our approach yielding the best result in 16 of 17 cases based on cost function evaluation, 9 of 11 cases based on number of identified clusters, 13 of 17 cases based on the unsupervised Borsotti image segmentation criterion, and 7 of 11 cases based on the supervised PRI image segmentation metric.},
  archive      = {J_ASOC},
  author       = {Seyed Jalaleddin Mousavirad and Hossein Ebrahimpour-Komleh and Gerald Schaefer},
  doi          = {10.1016/j.asoc.2020.106604},
  journal      = {Applied Soft Computing},
  pages        = {106604},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Automatic clustering using a local search-based human mental search algorithm for image segmentation},
  volume       = {96},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Multi-task allocation with an optimized quantum particle
swarm method. <em>ASOC</em>, <em>96</em>, 106603. (<a
href="https://doi.org/10.1016/j.asoc.2020.106603">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-task allocation in multi-agent systems aims to accomplish tasks efficiently and successfully, while obtaining more rewards to enhance the entire system operation at the same time. Most existing assignment methods are based on agent coalitions, which cannot balance the profit distribution and task execution success rate or ignore the coalition stability, leading to a low execution level and assignment failures. Few coalition scheduling methods exist for multi-task allocation based on a fixed agent population. In this paper, we propose an effective stability quantum particle swarm optimization (SQPSO) algorithm which includes high rewards obtaining, benefit dividing, coalition stability insuring, and a historical task mechanism for search acceleration. Secondly, we design an efficient establishment quantum particle swarm optimization (EQPSO) algorithm for coalition scheduling, which is equipped with coalition similarity judgment to reduce the coalition formation time cost. The experiment results show that SQPSO guarantees a superior coalition for every task and earlier convergence in the whole task set allocation, and EQPSO gives the optimal scheduling order which reduces the total execution time .},
  archive      = {J_ASOC},
  author       = {Mincan Li and Chubo Liu and Kenli Li and Xiangke Liao and Keqin Li},
  doi          = {10.1016/j.asoc.2020.106603},
  journal      = {Applied Soft Computing},
  pages        = {106603},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Multi-task allocation with an optimized quantum particle swarm method},
  volume       = {96},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Grey wolf optimizer with an enhanced hierarchy and its
application to the wireless sensor network coverage optimization
problem. <em>ASOC</em>, <em>96</em>, 106602. (<a
href="https://doi.org/10.1016/j.asoc.2020.106602">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Grey wolf optimizer (GWO), which is inspired by the social behaviours of grey wolf packs, is a nature-inspired and population-based algorithm. The GWO technique has the advantage of conceptual simplicity and shows good results for solving various real-world problems. However, this technique has the drawback of premature convergence and is prone to stagnation in local optima. The leadership hierarchy is the paramount characteristic of the GWO and influences its searching precision. Therefore, a grey wolf optimizer with enhanced hierarchy (GWO-EH) is proposed to overcome these deficiencies. Firstly, fitness-based self-adaptive weight coefficients are introduced to better imitate the hierarchy of the grey wolves, which also have a positive effect on the convergence speed. Then, we propose an improved position-updating equation to enhance the leadership of the high-ranking wolves, whereby the global exploration ability of the GWO is strengthened. Finally, the strategy of repositioning wolves around the leading wolves is designed to keep a perfect balance between exploration and exploitation. The search ability of the GWO-EH is thoroughly compared with the GWO itself, some promising GWO variants, and several well-established algorithms on twenty-three widely used benchmark functions . Empirical studies reveal that GWO-EH has a competitive overall performance according to the average value (standard deviation), Wilcoxon rank-sum test results, and convergence curve. Moreover, our method is applied to address the wireless sensor network coverage optimization problem , and the applicability and validity of the GWO-EH are indicated by the experimental results.},
  archive      = {J_ASOC},
  author       = {Zhaoming Miao and Xianfeng Yuan and Fengyu Zhou and Xuanjie Qiu and Yong Song and Ke Chen},
  doi          = {10.1016/j.asoc.2020.106602},
  journal      = {Applied Soft Computing},
  pages        = {106602},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Grey wolf optimizer with an enhanced hierarchy and its application to the wireless sensor network coverage optimization problem},
  volume       = {96},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Sparsifying parity-check matrices. <em>ASOC</em>,
<em>96</em>, 106601. (<a
href="https://doi.org/10.1016/j.asoc.2020.106601">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Parity check matrices (PCMs) are used to define linear error correcting codes and ensure reliable information transmission over noisy channels. The set of codewords of such a code is the null space of this binary matrix. We consider the problem of minimizing the number of one-entries in parity-check matrices. In the maximum-likelihood (ML) decoding method, the number of ones in PCMs is directly related to the time required to decode messages. We propose a simple matrix row manipulation heuristic which alters the PCM, but not the code itself. We apply simulated annealing and greedy local searches to obtain PCMs with a small number of one entries quickly, i.e. in a couple of minutes or hours when using mainstream hardware. The resulting matrices provide faster ML decoding procedures, especially for large codes.},
  archive      = {J_ASOC},
  author       = {Luís M.S. Russo and Tobias Dietz and José Rui Figueira and Alexandre P. Francisco and Stefan Ruzika},
  doi          = {10.1016/j.asoc.2020.106601},
  journal      = {Applied Soft Computing},
  pages        = {106601},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Sparsifying parity-check matrices},
  volume       = {96},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A new-structure grey verhulst model for china’s tight gas
production forecasting. <em>ASOC</em>, <em>96</em>, 106600. (<a
href="https://doi.org/10.1016/j.asoc.2020.106600">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Tight gas, shale gas and coalbed gas are recognized as the three sources of unconventional natural gas in the world. Currently, China’s tight gas production is at an absolute advantage. Hence, a reasonable prediction of tight gas production is of great value to China’s government in formulating energy policies. In this study, the data characteristics of China’s tight gas production were analysed. Then a new-structure grey Verhulst model for predicting China’s tight gas production was employed, and the time response function and initial value optimization method of the new model were deduced. Next, the new model was used to simulate and predict China’s tight gas production. The comprehensive error was 2.07\%, which was much smaller than that of the traditional grey Verhulst model (7.78\%) and the GM(1, 1) model (18.57\%). Finally, China’s tight gas production was predicted and analysed. The results show that the growth of China’s tight gas production will slow down in the next three years due to the high cost of tight gas exploitation. Therefore, Chinese government should speed up tight gas exploitation through policy support; meanwhile, China needs to continue importing large quantities of natural gas to ensure sufficient domestic supply of natural gas.},
  archive      = {J_ASOC},
  author       = {Bo Zeng and Xin Ma and Meng Zhou},
  doi          = {10.1016/j.asoc.2020.106600},
  journal      = {Applied Soft Computing},
  pages        = {106600},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A new-structure grey verhulst model for china’s tight gas production forecasting},
  volume       = {96},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Co-optimization of ampacity and lifetime with considering
harmonic and stochastic parameters by imperialist competition algorithm.
<em>ASOC</em>, <em>96</em>, 106599. (<a
href="https://doi.org/10.1016/j.asoc.2020.106599">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study presents a novel probabilistic algorithm for the optimal placement of underground power cables to maximize lifetime and ampacity and minimize total cost of the cable system at the same time. In this paper, the Imperialist Competition Algorithm (ICA) and Monte Carlo (MC) simulation are used to solve probabilistic optimization problem . In our previous study, we have just considered probabilistic parameters that are effective on ampacity. However, In fact, we need to consider all the other parameters such as cable’s lifetime and harmonic of demand side for co-optimizing the ampacity and lifetime to obtain the best arrangement of cable system. The cost function of optimization has been improved relative to the last one. Cable lifetime and harmonic of power system , as two important design factors, have been added to cost function in this paper. The novelty of this paper is consisted of economic co-optimization of ampacity and lifetime simultaneously. Furthermore, all statistical variations of cable system parameters, ambient temperature, dimensions of native/backfill soil and uncertainty in demand side such as load current fluctuation and harmonic are considered as design parameters at the same time. Since, all design parameters are stochastic; the change of seasons and climate can modify these parameters along the cable system. Hence, this paper indicates the necessity of consideration of uncertainty parameters at the design phase by taking into account cable lifetime based on a probabilistic approach and avoid using deterministic value. For a better explanation of this method, a numerical example is reported by using corrugated aluminum sheath cable model.},
  archive      = {J_ASOC},
  author       = {Hamed Shabani and Behrooz Vahidi},
  doi          = {10.1016/j.asoc.2020.106599},
  journal      = {Applied Soft Computing},
  pages        = {106599},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Co-optimization of ampacity and lifetime with considering harmonic and stochastic parameters by imperialist competition algorithm},
  volume       = {96},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Multi-objective optimization for attitude maneuver of
liquid-filled flexible spacecraft based on improved hierarchical
optimization algorithm. <em>ASOC</em>, <em>96</em>, 106598. (<a
href="https://doi.org/10.1016/j.asoc.2020.106598">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To solve the multi-objective optimization problem for attitude maneuver of liquid-filled flexible spacecraft , the authors propose a novel algorithm based on an improved hierarchical structure. This improved hierarchical optimization algorithm (IHOA) is composed of elitist non-dominated sorting genetic algorithm (NSGA-II) and bare-bones multi-objective particle swarm optimization based on r-dominance relation (r-BBMOPSO). Bottom layer algorithm NSGA-II provides elitist individuals for top layer algorithm. Top layer algorithm r-BBMOPSO employs r-domination instead of Pareto domination to strengthen selection pressure and guide the search toward the decision maker’s preference. The modification of hierarchical structure has reduced the influence of randomness and accelerated convergence speed while preserving the operating efficiency of the algorithm. The feasibility and effectiveness of IHOA are demonstrated by bench-mark test problems. Also, the simulation results proved that the proposed optimization algorithm IHOA has the capacity to stably generate solutions which are satisfied the decision maker’s demand in the case of a single run. Especially, based on the optimized parameters, the spacecraft can perform well in attitude maneuver tasks.},
  archive      = {J_ASOC},
  author       = {Liaoxue Liu and Yu Guo},
  doi          = {10.1016/j.asoc.2020.106598},
  journal      = {Applied Soft Computing},
  pages        = {106598},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Multi-objective optimization for attitude maneuver of liquid-filled flexible spacecraft based on improved hierarchical optimization algorithm},
  volume       = {96},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Uncertainty quantification for plant disease detection using
bayesian deep learning. <em>ASOC</em>, <em>96</em>, 106597. (<a
href="https://doi.org/10.1016/j.asoc.2020.106597">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Climate change is having an enormous impact on crop production in Latin America and the Caribbean. This problem not only concerns the volume of crop production but also the quality and safety of the food industry. Several research studies have proposed deep learning for plant disease detection. However, there is little information about the confidence of the prediction on unseen samples. Therefore, uncertainty in models of plant disease detection is required for effective crop management. In particular, uncertainty arising from sample selection bias makes it difficult to scale automatic plant disease detection systems to production. In this paper, we develop a probabilistic programming approach for plant disease detection using state-of-the-art Bayesian deep learning techniques and the uncertainty as a misclassification measurement. The results show that Bayesian inference achieves classification performance that is comparable to the standard optimization procedures for fine-tuning deep learning models. At the same time, the proposed method approximates the posterior density for the plant disease detection problem and quantify the uncertainty of the predictions for out-of-sample instances.},
  archive      = {J_ASOC},
  author       = {S. Hernández and Juan L. López},
  doi          = {10.1016/j.asoc.2020.106597},
  journal      = {Applied Soft Computing},
  pages        = {106597},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Uncertainty quantification for plant disease detection using bayesian deep learning},
  volume       = {96},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Human protein subcellular localization identification via
fuzzy model on kernelized neighborhood representation. <em>ASOC</em>,
<em>96</em>, 106596. (<a
href="https://doi.org/10.1016/j.asoc.2020.106596">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In traditional wet experiments, fluorescent proteins are generally used to detect subcellular localization of protein. However, it is time consuming and expensive for detecting large-scale biological data. Many computational biological methods have been developed to identify various subcellular localizations of proteins. In the last ten years, machine learning methods have been widely used in many research issues in the field of bioinformatics. In this work, Fuzzy Support Vector Machine based on Kernelized Neighborhood Representation (FSVM-KNR) is proposed to predict the subcellular localization of protein. Proteins are represented via six types of features (PsePSSM, PSSM-DWT, PSSM-AB, PsePP , PP-DWT and PP-AB). These features are constructed kernels and combined with Kernel Target Alignment-based Multiple Kernel Learning (KTA-MKL). Then, Kernelized Neighborhood Representation (KNR) algorithm is proposed to filter outliers via fuzzy membership scores. At last, the membership scores (with KNR) and integrated kernel (with KTA-MKL) are used to built FSVM-KNR model. To evaluate the performance of FSVM-KNR model, we test it on two benchmark datasets of protein subcellular localization. Our method achieves better performance (average precision: 0.7108 and 0.6916) on two datasets, respectively. In addition, our model is also compared with other FSVM model on 8 UCI datasets and the performance of FSVM-KNR is better or comparable.},
  archive      = {J_ASOC},
  author       = {Yijie Ding and Jijun Tang and Fei Guo},
  doi          = {10.1016/j.asoc.2020.106596},
  journal      = {Applied Soft Computing},
  pages        = {106596},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Human protein subcellular localization identification via fuzzy model on kernelized neighborhood representation},
  volume       = {96},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Fuzzy decision by opinion score method. <em>ASOC</em>,
<em>96</em>, 106595. (<a
href="https://doi.org/10.1016/j.asoc.2020.106595">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multicriteria decision making (MCDM) methods have been utilised to solve discrete problems in broad domains. Existing MCDM methods have been developed on the basis of two main contexts, namely, human and mathematical approaches. Each approach faces different challenges, such as inconsistency, time consumption in a pairwise comparison , unnatural comparison, vagueness, normalisation, distance measurement and the issue of criteria weighting in mathematical approach. To solve these challenges, this work presents fuzzy decision by opinion score method (FDOSM), which is a novel MCDM method under fuzzy environment. FDOSM consists of three stages, namely, data input, data transformation and data processing units. Two decision-making platforms, namely, single and group decision-making contexts, are performed on FDOSM. Two case studies (i.e. communication and sport) with multicriteria decision problems are applied to illustrate the significance of FDOSM. The behaviour of FDOSM works on the idea of ideal solutions and allows experts to select the best value and compare the best and other values under the same criterion. Subsequently, mathematical operations are performed to reach a final rank and select the best alternative from a set of available alternatives. In conclusion, FDOSM is debated on and compared with different MCDM methods from ranking and weighting perspectives. Results of FDOSM are more logical than those of other existing MCDM methods.},
  archive      = {J_ASOC},
  author       = {Mahmood M. Salih and B.B. Zaidan and A.A. Zaidan},
  doi          = {10.1016/j.asoc.2020.106595},
  journal      = {Applied Soft Computing},
  pages        = {106595},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Fuzzy decision by opinion score method},
  volume       = {96},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Hybrid adaptive simplified human learning optimization
algorithms for supply chain network design problem with possibility of
direct shipment. <em>ASOC</em>, <em>96</em>, 106594. (<a
href="https://doi.org/10.1016/j.asoc.2020.106594">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A two-stage supply chain network design problem with a minimization type cost-based objective function is focused in this study. Some important assumptions such as considering different transportation modes in the stages, fixed charge transportation cost, and possibility of direct shipment between plants and customers are respected in the problem. As the problem is of NP-hard class of optimization problems , some meta-heuristic algorithms are proposed as its solution approach. In this regard, the recently proposed adaptive simplified human learning optimization (ASHLO) algorithm is used to be hybridized by the genetic algorithm (GA) and Particle swarm optimization algorithm (PSO) separately. In addition, two more meta-heuristic algorithms of gravitational search algorithm (GSA) and cuckoo search (CS) are used for more comparisons. Therefore, totally thirteen classical and hybrid meta-heuristic algorithms are proposed to solve the problem. In the extensive computational experiments of the study, 40 test problems from various sizes are generated. A typical experimental study is done to tune the parameters of the proposed algorithms. Using the results of parameter tuning step, the final experiments on the test problems are performed. The obtained results prove the superiority of the ASHLO algorithm when hybridized by the PSO.},
  archive      = {J_ASOC},
  author       = {A. Shoja and S. Molla-Alizadeh-Zavardehi and S. Niroomand},
  doi          = {10.1016/j.asoc.2020.106594},
  journal      = {Applied Soft Computing},
  pages        = {106594},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Hybrid adaptive simplified human learning optimization algorithms for supply chain network design problem with possibility of direct shipment},
  volume       = {96},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Learning dynamic simultaneous clustering and classification
via automatic differential evolution and firework algorithm.
<em>ASOC</em>, <em>96</em>, 106593. (<a
href="https://doi.org/10.1016/j.asoc.2020.106593">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Semi-supervised learning is significant data analysis method in the age of Big Data. The Bayesian-based classifier is a classical classification method in semi-supervised learning. Wherein, the classifier with clustering and classification technology have experienced the transformation from sequential structure to simultaneous structure. There are two main difficulties in simultaneous structure: the limited accuracy and diversity caused by rigid optimization algorithm ; and the imbalance status between clustering and classification processes caused by insufficiently structure. To overcome these difficulties, a novel multi-objective differential evolution and firework algorithm for automatic simultaneous clustering and classification algorithm (MASCC-DE/FWA) is proposed. The main contributions of MASCC-DE/FWA contain: (1) Combination searching strategy for dynamic searching (2) Rapid and low-complexity silhouette coefficient as redesigned clustering objective function (3) Automatic clustering, opposition-based learning and adjusted mutual information for strengthening SCC-MOEA framework. The experimental result demonstrates that MASCC-DE/FWA performs better than other 8 state-of-art classification algorithms on synthetic dataset , 19 UCI datasets and image segmentation tasks.},
  archive      = {J_ASOC},
  author       = {Haoran Li and Fazhi He and Yilin Chen},
  doi          = {10.1016/j.asoc.2020.106593},
  journal      = {Applied Soft Computing},
  pages        = {106593},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Learning dynamic simultaneous clustering and classification via automatic differential evolution and firework algorithm},
  volume       = {96},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). An ensemble learning based prediction strategy for dynamic
multi-objective optimization. <em>ASOC</em>, <em>96</em>, 106592. (<a
href="https://doi.org/10.1016/j.asoc.2020.106592">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Prediction strategies are widely-used in dynamic multi-objective evolutionary algorithms (DMOEAs). However, the characteristics of the environmental changes are different and only use one single prediction model cannot react to the changes effectively. The mismatching of the changes and prediction models may make the predicted results inaccurate and unstable. To overcome this shortage, an ensemble learning based prediction strategy (ELPS) is proposed in this paper to help algorithms re-initialize a new population after a change is detected. There are four base prediction models in ELPS, i.e., linear prediction model (LP), knee point-based autoregression model (KP-AR), population-based autoregression model (P-AR) and random re-initialization model (RND). Once a change happens, these four base prediction models are trained by the historical information with ensemble learning and a strong prediction model can be constructed on these four base prediction models. The final re-initialized population is generated by this strong prediction model to react to the new environment. With the help of ELPS, the re-initialized population can adapt different environmental changes and improve the performance on prediction accuracy and robustness. The experimental results show that, compared with other state-of-the-art prediction strategies on benchmark test suite, ELPS has better performance on dealing with dynamic multi-objective optimization problems.},
  archive      = {J_ASOC},
  author       = {Feng Wang and Yixuan Li and Fanshu Liao and Hongyang Yan},
  doi          = {10.1016/j.asoc.2020.106592},
  journal      = {Applied Soft Computing},
  pages        = {106592},
  shortjournal = {Appl. Soft. Comput.},
  title        = {An ensemble learning based prediction strategy for dynamic multi-objective optimization},
  volume       = {96},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Application of random triangular and gaussian type-2 fuzzy
variable to solve fixed charge multi-item four dimensional
transportation problem. <em>ASOC</em>, <em>96</em>, 106589. (<a
href="https://doi.org/10.1016/j.asoc.2020.106589">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, two types of hybrid random type-2 uncertain variables such as random type-2 triangular and random type-2 Gaussian fuzzy variables are used to express the uncertain parameters in a four-dimensional transportation problem. Here selling price, purchasing costs, transportation costs, fixed charges, sources, demands, capacities of conveyances are considered as hybrid random type-2 uncertain parameters. The objective function of the proposed model is de-randomized using the expected value method and the constraints of the model are de-randomized using the probability chance constraint programming method. After that, the proposed model is de-fuzzified by CV based reduction technique. These two reduced crisp problems are solved by Generalized Reduced Gradient (GRG) technique using LINGO 14.0 software. The models are numerically illustrated. Some sensitivity analyses are also presented. Conventional solid (3D) and general (2D) transportation problems are derived as particular cases.},
  archive      = {J_ASOC},
  author       = {Sharmistha Halder Jana and Biswapati Jana},
  doi          = {10.1016/j.asoc.2020.106589},
  journal      = {Applied Soft Computing},
  pages        = {106589},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Application of random triangular and gaussian type-2 fuzzy variable to solve fixed charge multi-item four dimensional transportation problem},
  volume       = {96},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Multilevel minimum cross entropy thresholding: A comparative
study. <em>ASOC</em>, <em>96</em>, 106588. (<a
href="https://doi.org/10.1016/j.asoc.2020.106588">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multilevel thresholding method is one of the most popular techniques in image segmentation . However, the multilevel thresholding method is time-consuming, its time complexity increases exponentially with the number of threshold levels. In this paper, in order to improve the computation efficiency of the multilevel minimum cross entropy thresholding, the iterative formula of the multilevel cross entropy thresholding algorithm is proposed and compared with the modern meta-heuristic optimization algorithms. The iterative multilevel cross entropy thresholding algorithm can find the thresholds close to the global optimums with less time. The computation cost of the iterative multilevel cross entropy thresholding algorithm is linear in the number of the threshold levels. We prove the convergence of the iterative algorithm and compare the iterative multilevel cross entropy thresholding algorithm with multilevel cross entropy thresholding methods combined with the state-of-the-art meta-heuristic optimization techniques including particle swarm optimization (PSO), cuckoo search algorithm (CS), differential evolution (DE), crow search algorithm (CSA) and genetic algorithm (GA). Experimental results show that the iterative multilevel cross entropy thresholding algorithm is efficient and effective. Therefore, iterative algorithm for the multilevel cross entropy thresholding is an effective method to improve the computation efficiency.},
  archive      = {J_ASOC},
  author       = {Bo Lei and Jiulun Fan},
  doi          = {10.1016/j.asoc.2020.106588},
  journal      = {Applied Soft Computing},
  pages        = {106588},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Multilevel minimum cross entropy thresholding: A comparative study},
  volume       = {96},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Analysis and multi-objective optimization of slag powder
process. <em>ASOC</em>, <em>96</em>, 106587. (<a
href="https://doi.org/10.1016/j.asoc.2020.106587">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Slag powder is a process with characters of multivariables, strongly coupling and nonlinearity . The material layer thickness plays an important role in the process. It can reflect the dynamic balance between the feed volume and discharge volume in the vertical mill. Keeping the material layer thickness in a suitable range can not only improve the quality of powder, but also save electrical power. Previous studies on the material layer thickness did not consider the relationship among the material layer thickness, quality and yield. In this paper, the yield and quality factors are taken into account and the variables that affect the material layer thickness, yield and quality are analyzed. Then the models of material layer thickness, yield and quality are established based on generalized regression neural network . The production process demands for highest yield, best production quality and smallest error of material layer thickness at the same time. From this point of view, the slag powder process can be regarded as a multi-objective optimization problem. To improve the diversity of solutions, a CT-NSGAII algorithm is proposed by introducing the clustering-based truncation mechanism into solution selection process. Simulation shows that the proposed method can solve the multi-objective problem and obtain solutions with good diversity.},
  archive      = {J_ASOC},
  author       = {Xiaoli Li and Shiqi Shen and Shengxiang Yang and Kang Wang and Yang Li},
  doi          = {10.1016/j.asoc.2020.106587},
  journal      = {Applied Soft Computing},
  pages        = {106587},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Analysis and multi-objective optimization of slag powder process},
  volume       = {96},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A systematic approach to multi-task learning from
time-series data. <em>ASOC</em>, <em>96</em>, 106586. (<a
href="https://doi.org/10.1016/j.asoc.2020.106586">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {There have been significant advances in machine learning due to the profusion in data collection and computing resources. However, the need for large annotated datasets to train machine learning models remains a problematic constraint. To address the limitation of annotated data for personalized prediction, we propose a framework to enrich annotated time-series (TS) sensing data by way of transfer learning with new multi-task learning (MTL) models. Compared to previous MTL approaches for TS, this work introduces three contributions. First, we propose a systematic method to examine the efficiency of MTL approaches by exploring options for three key characteristics of MTL with TS: the choice of features that efficiently capture temporally dynamic information, the similarity measure that effectively models the commonality and uniqueness across tasks being learned, and the choice of regularization for achieving the best tradeoff between a model’s generalizability and accuracy. Second, we present an MTL deep learning model that is shown to achieve state-of-the-art performance for personalized human activity recognition from time-series. Experimental results on three benchmark activity recognition datasets and one activity recognition in-the-wild dataset show that the proposed framework provides performance gains over prior work while presenting a unified approach for designing MTL solutions for personalized time-series classification problems.},
  archive      = {J_ASOC},
  author       = {Reem A. Mahmoud and Hazem Hajj and Fadi N. Karameh},
  doi          = {10.1016/j.asoc.2020.106586},
  journal      = {Applied Soft Computing},
  pages        = {106586},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A systematic approach to multi-task learning from time-series data},
  volume       = {96},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A fuzzy ubiquitous traveler clustering and hotel
recommendation system by differentiating travelers’ decision-making
behaviors. <em>ASOC</em>, <em>96</em>, 106585. (<a
href="https://doi.org/10.1016/j.asoc.2020.106585">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For generating hotel recommendations, clustering travelers has been demonstrated to be a viable method to elevate traveler satisfaction with the recommendation results. However, most of the existing methods that adopt this approach cluster travelers according to a variety of traveler or hotel attributes, which may not necessarily be appropriate for use in an online application such as ubiquitous hotel recommendation. To overcome this problem, a fuzzy ubiquitous traveler clustering and hotel recommendation (FUTCHR) system was developed in this study. The FUTCHR system clustered travelers according to their decision-making mechanisms that are fitted by comparing travelers’ choices with the recommendation results in the historical data. To generate recommendations, a fuzzy mixed binary-nonlinear programming model was constructed and solved. The novelty of the proposed methodology is to cluster travelers without knowing their characteristics but according to the differences in their decision-making mechanisms. The FUTCHR system was employed in a regional study, and the successful recommendation rate was superior to three existing methods in this field.},
  archive      = {J_ASOC},
  author       = {Toly Chen},
  doi          = {10.1016/j.asoc.2020.106585},
  journal      = {Applied Soft Computing},
  pages        = {106585},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A fuzzy ubiquitous traveler clustering and hotel recommendation system by differentiating travelers’ decision-making behaviors},
  volume       = {96},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). An efficient kernel-based feature extraction using a
pull–push method. <em>ASOC</em>, <em>96</em>, 106584. (<a
href="https://doi.org/10.1016/j.asoc.2020.106584">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Supervised dimensionality reduction methods using nonlinear mappings for pattern recognition tasks are more appropriate for nonlinearly distributed data. Generally, for most algorithms, these samples (called hard samples) located at the edge or in other classes influence the performance of the proposed methods. In this study, the large margin nearest neighbor (LMNN) and weighted local modularity (WLM) in the complex network are introduced to deal with these hard samples to push and pull them rapidly toward the center of the class, and the samples with the same label shrink, as a whole, into the center of the class. A novel feature extraction method named KWLM–LMNN, which uses the kernel trick and combining WLM with LMNN, is proposed. Comparative experiments with state-of-the-art feature extraction methods demonstrate the effectiveness of the proposed method.},
  archive      = {J_ASOC},
  author       = {Guodong Zhao and Yan Wu},
  doi          = {10.1016/j.asoc.2020.106584},
  journal      = {Applied Soft Computing},
  pages        = {106584},
  shortjournal = {Appl. Soft. Comput.},
  title        = {An efficient kernel-based feature extraction using a pull–push method},
  volume       = {96},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Deep learning architectures in emerging cloud computing
architectures: Recent development, challenges and next research trend.
<em>ASOC</em>, <em>96</em>, 106582. (<a
href="https://doi.org/10.1016/j.asoc.2020.106582">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The challenges of the conventional cloud computing paradigms motivated the emergence of the next generation cloud computing architectures. The emerging cloud computing architectures generate voluminous amount of data that are beyond the capability of the shallow intelligent algorithms to process. Deep learning algorithms, with their ability to process large-scale datasets, have recently started gaining tremendous attentions from researchers to solve problem in the emerging cloud computing architectures. However, no comprehensive literature review exists on the applications of deep learning architectures to solve complex problems in emerging cloud computing architectures. To fill this gap, we conducted a comprehensive literature survey on the applications of deep learning architectures in emerging cloud computing architectures. The survey shows that the adoption of deep learning architectures in emerging cloud computing architectures are increasingly becoming an interesting research area. We introduce a new taxonomy of deep learning architectures for emerging cloud computing architectures and provide deep insights into the current state-of-the-art active research works on deep learning to solve complex problems in emerging cloud computing architectures. The synthesis and analysis of the articles as well as their limitation are presented. A lot of challenges were identified in the literature and new future research directions to solve the identified challenges are presented. We believed that this article can serve as a reference guide to new researchers and an update for expert researchers to explore and develop more deep learning applications in the emerging cloud computing architectures.},
  archive      = {J_ASOC},
  author       = {Fatsuma Jauro and Haruna Chiroma and Abdulsalam Y. Gital and Mubarak Almutairi and Shafi’i M. Abdulhamid and Jemal H. Abawajy},
  doi          = {10.1016/j.asoc.2020.106582},
  journal      = {Applied Soft Computing},
  pages        = {106582},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Deep learning architectures in emerging cloud computing architectures: Recent development, challenges and next research trend},
  volume       = {96},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A hybrid technique for path planning of humanoid robot NAO
in static and dynamic terrains. <em>ASOC</em>, <em>96</em>, 106581. (<a
href="https://doi.org/10.1016/j.asoc.2020.106581">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The humanoid robot is widely used because of its ability to imitate human actions. The selection of navigational techniques is of prime importance because the quality of the opted technique directly affects the success of output. In this paper, the hybridization of the Dynamic Window Approach (DWA) and the Teaching–Learning-Based Optimization (TLBO) technique and its implementation on the NAO humanoid robot for navigation have been presented. The input is based on the location of obstacles and the target. The parameters are provided to the DWA technique, which decides the optimum velocity. The intermediate result is feed to the TLBO technique, which operates based on the teacher phase and the learner phase. This hybridization provides an optimum angle to take a turn and avoids the obstacles while moving towards the target. The current article concentrates on implementing hybridized techniques in static and dynamic terrains. Single NAO and some random obstacles are chosen for static navigation. For dynamic terrains, multiple NAOs and some static obstacles are considered. In this case, one humanoid robot acts as a dynamic obstacle to another. In the dynamic terrain, there is a possibility of inter-collision amongst NAOs. To avoid inter-collision, a Petri-Net controller has been designed and implemented in all NAOs. Simulation and experimental results on humanoid NAOs demonstrate target attainment with collision-free optimal paths. Experimental and simulated results of the proposed technique present an acceptable relation under 5\% and 6\% for a single robot and multiple robots , respectively. The proposed technique has been compared with previously developed techniques in complex, danger and dynamic terrains. In comparison with previously developed techniques, it is evident that the proposed technique is robust and efficient for the path planning of humanoid robots.},
  archive      = {J_ASOC},
  author       = {Abhishek Kumar Kashyap and Dayal R. Parhi and Manoj Kumar Muni and Krishna Kant Pandey},
  doi          = {10.1016/j.asoc.2020.106581},
  journal      = {Applied Soft Computing},
  pages        = {106581},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A hybrid technique for path planning of humanoid robot NAO in static and dynamic terrains},
  volume       = {96},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Enhancing the privacy of negative surveys using negative
combined categories. <em>ASOC</em>, <em>96</em>, 106578. (<a
href="https://doi.org/10.1016/j.asoc.2020.106578">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, the negative survey, which can preserve the privacy of individuals when used for collecting sensitive information , has attracted significant attention. However, the privacy of the typical negative survey is limited by the number of categories. When the number of categories is small, the typical negative survey exhibits weak privacy preservation . In particular, when only two categories exist, the typical negative survey cannot preserve the privacy of individuals. Moreover, at times, the privacy requirements of participants are strict. In such a situation, the typical negative survey fails to provide satisfactory privacy preservation . In this paper, two novel negative survey models that use negative combined categories (NCCs), NCC-I and NCC-II, are proposed. They can provide improved individual privacy preservation, in particular for sensitive information with only two categories. The experimental results demonstrate that the proposed methods can achieve superior privacy preservation and provide accurate reconstructed results.},
  archive      = {J_ASOC},
  author       = {Hao Jiang and Wenjian Luo and Binyao Duan and Chenwang Wu},
  doi          = {10.1016/j.asoc.2020.106578},
  journal      = {Applied Soft Computing},
  pages        = {106578},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Enhancing the privacy of negative surveys using negative combined categories},
  volume       = {96},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). High-performance stock index trading via neural networks and
trees. <em>ASOC</em>, <em>96</em>, 106567. (<a
href="https://doi.org/10.1016/j.asoc.2020.106567">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Automated asset trading typically involves a price prediction model – of as high an accuracy as possible – together with a trading strategy, sometimes as simple as buying or selling when the price is predicted to rise or fall, respectively. Despite the fact that the model’s effectiveness in generating profits may depend on the particular trading strategy it is used with, these two components are often designed separately, in part because of the difficulty involved in jointly optimizing them. Motivated by this interplay between model performance and trading strategy, this work presents a novel automated trading architecture in which the prediction model is tuned to enhance profitability instead of accuracy, while the trading strategy attempts to be more sophisticated in its use of the model’s price predictions. In particular, instead of acting simply on whether the price is predicted to rise or fall we show that there is value in taking advantage of the model-specific distribution of predicted returns, and the fact that a prediction’s position within that distribution carries useful information about the expected profitability of a trade. Our proposed approach was tested with tree-based models as well as one deep long short-term memory (LSTM) neural networks , all of which were kept structurally simple and generated predictions based on price observations over a modest number of trading days. Tested over the period 2010–2019 on the S&amp;P 500, Dow Jones Industrial Average (DJIA), NASDAQ and Russell 2000 stock indices, and our best overall model achieved cumulative returns of 350\%, 403\%, 497\% and 333\%, respectively, outperforming the benchmark buy-and-hold strategy as well as other recent efforts.},
  archive      = {J_ASOC},
  author       = {Chariton Chalvatzis and Dimitrios Hristu-Varsakelis},
  doi          = {10.1016/j.asoc.2020.106567},
  journal      = {Applied Soft Computing},
  pages        = {106567},
  shortjournal = {Appl. Soft. Comput.},
  title        = {High-performance stock index trading via neural networks and trees},
  volume       = {96},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A new algorithm based on gray wolf optimizer and shuffled
frog leaping algorithm to solve the multi-objective optimization
problems. <em>ASOC</em>, <em>96</em>, 106560. (<a
href="https://doi.org/10.1016/j.asoc.2020.106560">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-objective optimization is many important since most of the real world problems are in multi-objective category. Looking at the literature, the algorithms proposed for the solution of multi-objective problems have increased in recent years, but there is no a convenient approach for all kind of problems. Therefore, researchers aim to contribute to the literature by offering new approaches. In this study, an algorithm based on gray wolf optimizer (GWO) with memeplex structure of the shuffled frog leaping algorithm (SFLA), which is named as multi-objective shuffled GWO (MOSG), is proposed to solve the multi-objective optimization problems . Additionally, some modifications are applied on the proposed algorithm to improve the performance from different angles. The performance of the proposed algorithm is compared with the performance of six multi-objective algorithms on a benchmark set consist of 36 problems. The experimental results are presented with four different comparison metrics and statistical tests. According to the results, it can easily be said that the proposed algorithm is generally successful to solve the multi-objective problems and has better or competitive results.},
  archive      = {J_ASOC},
  author       = {Murat Karakoyun and Ahmet Ozkis and Halife Kodaz},
  doi          = {10.1016/j.asoc.2020.106560},
  journal      = {Applied Soft Computing},
  pages        = {106560},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A new algorithm based on gray wolf optimizer and shuffled frog leaping algorithm to solve the multi-objective optimization problems},
  volume       = {96},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). An efficient equilibrium optimizer with mutation strategy
for numerical optimization. <em>ASOC</em>, <em>96</em>, 106542. (<a
href="https://doi.org/10.1016/j.asoc.2020.106542">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To alleviate the shortcomings of the standard Equilibrium Optimizer, a new improved algorithm called Modified Equilibrium Optimizer is proposed in this work. This algorithm utilizes the Gaussian mutation and an additional exploratory search mechanism based on the concept of population division and reconstruction. The population in each iteration of the proposed algorithm is constructed using these mechanisms and standard search procedure of the Equilibrium Optimizer. These strategies attempt to maintain the diversity of solutions during the search, so that the tendency of stagnation towards the sub-optimal solutions can be avoided and the convergence rate can be boosted to obtain more accurate optimal solutions. To validate and analyze the performance of the Modified Equilibrium Optimizer, a collection of 33 benchmark problems and four engineering design problems are adopted. Later, in the paper, the Modified Equilibrium Optimizer has been used to train multilayer perceptrons . The experimental results and comparison based on several metrics such as statistical analysis, scalability test, diversity analysis, performance index analysis and convergence analysis demonstrate that the proposed algorithm can be considered a better metaheuristic optimization approach than other compared algorithms.},
  archive      = {J_ASOC},
  author       = {Shubham Gupta and Kusum Deep and Seyedali Mirjalili},
  doi          = {10.1016/j.asoc.2020.106542},
  journal      = {Applied Soft Computing},
  pages        = {106542},
  shortjournal = {Appl. Soft. Comput.},
  title        = {An efficient equilibrium optimizer with mutation strategy for numerical optimization},
  volume       = {96},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Efficient point-of-interest recommendation with hierarchical
attention mechanism. <em>ASOC</em>, <em>96</em>, 106536. (<a
href="https://doi.org/10.1016/j.asoc.2020.106536">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Personalized Point-of-Interest (POI) Recommendation is very important for application platforms based on Location Based Social Networks (LBSNs). It can assist users in making decisions to alleviate the problem of information overload, and can also improve the user experience of these platforms and advance platform operators achieve personalized and accurate advertising. However, there exist some problems of data sparseness and cold start for a single user, and it is also difficult to mine valuable long-tailed POIs, although the size of the check-in data is large. Therefore, in order to address the above problems, we propose a personalized POI Recommendation approach based on Hierarchical Attention Mechanism (HAM-POIRec) which can effectively increase data utilization. Firstly, we define the concepts of explicit features and implicit features, which pave the ideas of selecting data and computational models for POI recommendation based on machine learning . Secondly, we propose a hierarchical attention mechanism with the structure of local-to-global, which extracts contributions and mines more hidden information from individual features, combination features, and overall features. Finally, we present the Natural Language Processing (NLP)-based “User-POI” matching mechanism for the first time in the field of POI recommendation to improve the recommendation accuracy by fine-tuning the POIs predicted by the recommendation system. Extensive experiments are conducted for demonstrating that the HAM-POIRec method outperforms state-of-the-art DeepPIM method and the other comparison methods (SAE-NAD, MGMPFM and LRT), especially in predicting sequence POIs and solving cold start problem.},
  archive      = {J_ASOC},
  author       = {Guangyao Pang and Xiaoming Wang and Fei Hao and Liang Wang and Xinyan Wang},
  doi          = {10.1016/j.asoc.2020.106536},
  journal      = {Applied Soft Computing},
  pages        = {106536},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Efficient point-of-interest recommendation with hierarchical attention mechanism},
  volume       = {96},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Fuzzy regression functions with a noise cluster and the
impact of outliers on mainstream machine learning methods in the
regression setting. <em>ASOC</em>, <em>96</em>, 106535. (<a
href="https://doi.org/10.1016/j.asoc.2020.106535">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The presence of outliers in the dependent and/or independent features distorts predictions with machine learning techniques and may lead to erroneous conclusions. It is important to implement methods that are robust against the outliers to make reliable predictions and to know the accuracy of the existing methods when data is contaminated with outliers. The focus of this study is to propose a robust fuzzy regression functions (FRFN) approach against the outliers and evaluate the performance of the proposed and several mainstream machine learning approaches in the presence of outliers for the regression problem . The proposed FRFN approach is based on fuzzy k-means clustering with a noise cluster. We compare the accuracy of Artificial Neural Networks (ANN), Support Vector Machines (SVM) and the proposed FRFN approaches with different training algorithms/kernel functions via simulated and real benchmark datasets. In total, accuracies of 36 ANN, SVM, and FRNF implementations with training algorithms and kernel and loss functions have been evaluated and compared to each other with samples containing outliers via a Monte Carlo simulation setting. It is observed in both Monte Carlo simulations and applications with benchmark dataset that FRFN with ANN trained with Bayes regularization algorithm and FRFN with SVM with Gaussian kernel outperforms the classical implementations of ANN and SVMs under the existence of outliers. The proposed noise cluster implementation considerably increases the robustness of fuzzy regression functions against outliers.},
  archive      = {J_ASOC},
  author       = {Srinivas Chakravarty and Haydar Demirhan and Furkan Baser},
  doi          = {10.1016/j.asoc.2020.106535},
  journal      = {Applied Soft Computing},
  pages        = {106535},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Fuzzy regression functions with a noise cluster and the impact of outliers on mainstream machine learning methods in the regression setting},
  volume       = {96},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Lightweight speaker verification for online identification
of new speakers with short segments. <em>ASOC</em>, <em>95</em>, 106704.
(<a href="https://doi.org/10.1016/j.asoc.2020.106704">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Verifying if two audio segments belong to the same speaker has been recently put forward as a flexible way to carry out speaker identification, since it does not require to be re-trained when new speakers appear on the auditory scene. Although many of the current techniques have achieved high performances, they require a considerably high amount of memory, and a specific minimum length for their input audio segments. These requirements limit the applicability of these techniques in scenarios such as service robots , internet of things and virtual assistants , where computational resources are limited and the users tend to speak in short segments. In this work we propose a BLSTM-based model that reaches a level of performance comparable to the current state of the art when using short input audio segments, while requiring a considerably less amount of memory. Further, as far as we know, a complete speaker identification system has not been reported using this verification paradigm. Thus, we present a complete online speaker identifier, based on a simple voting system, that shows that the proposed BLSTM-based model achieves a similar performance at identifying speakers online compared to the current state of the art.},
  archive      = {J_ASOC},
  author       = {Ivette Vélez and Caleb Rascon and Gibrán Fuentes-Pineda},
  doi          = {10.1016/j.asoc.2020.106704},
  journal      = {Applied Soft Computing},
  pages        = {106704},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Lightweight speaker verification for online identification of new speakers with short segments},
  volume       = {95},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Special issue on bio-inspired optimization techniques for
biomedical data analysis: Methods and applications. <em>ASOC</em>,
<em>95</em>, 106672. (<a
href="https://doi.org/10.1016/j.asoc.2020.106672">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_ASOC},
  author       = {Victor Hugo C. de Albuquerque and Deepak Gupta and Ivanoe De Falco and Giovanna Sannino and Nizar Bouguila},
  doi          = {10.1016/j.asoc.2020.106672},
  journal      = {Applied Soft Computing},
  pages        = {106672},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Special issue on bio-inspired optimization techniques for biomedical data analysis: Methods and applications},
  volume       = {95},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Learning to rank developers for bug report assignment.
<em>ASOC</em>, <em>95</em>, 106667. (<a
href="https://doi.org/10.1016/j.asoc.2020.106667">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Bug assignment is a burden for projects receiving many bug reports . To automate the process of assigning bug reports to the appropriate developers, several studies have relied on combining natural language processing and information retrieval techniques to extract two categories of features. One of these categories targets developers who have fixed similar bugs before, and the other determines developers working on source files similar to the description of the bug. Commit messages represent another rich source for profiling developer expertise as the language used in commit messages is closer to that used in bug reports. In this work, we propose a more enhanced profiling of developers through their commits, which are captured in a new set of features that we combine with features used in previous studies. More precisely, we propose an adaptive ranking approach that takes as input a given bug report and ranks the top developers who are most suitable to fix it. This approach learns from the history of previously fixed bugs to profile developers in terms of their expertise. With respect to a given bug report, the ranking score of each developer is computed as a weighted combination of an array of features encoding domain knowledge, where the weights are trained automatically on previously solved bug reports using a learning-to-rank technique. Our model was evaluated using around 22, 000 bug reports, exported from four large scale open-source Java projects. Results show that our model significantly outperformed two recent state-of-the-art methods in recommending the suitable developer to handle a certain bug report. Specifically, the percentage of recommending a developer within the top 5 ranked developers correctly was over 80\% for both the Eclipse UI Platform and Birt projects.},
  archive      = {J_ASOC},
  author       = {Bader Alkhazi and Andrew DiStasi and Wajdi Aljedaani and Hussein Alrubaye and Xin Ye and Mohamed Wiem Mkaouer},
  doi          = {10.1016/j.asoc.2020.106667},
  journal      = {Applied Soft Computing},
  pages        = {106667},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Learning to rank developers for bug report assignment},
  volume       = {95},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Multi-scale decomposition based supervised single channel
deep speech enhancement. <em>ASOC</em>, <em>95</em>, 106666. (<a
href="https://doi.org/10.1016/j.asoc.2020.106666">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Speech signals reaching our ears are in general contaminated by the background noise distortion which is detrimental to both speech quality and intelligibility. In this paper, we propose a nonlinear multi-scale decomposition-based deep speech enhancement method to improve the quality and intelligibility of the contaminated speech. In the proposed method, we have applied Hurst exponent-based Empirical Mode Decomposition (HEMD) to the noisy speech and obtained a set of intrinsic mode functions (IMFs) and a residual. The Deep Neural Networks (DNNs) are trained for each of the extracted IMF and residual to learn a non-linear mapping with a deep hidden structure to construct a time–frequency mask. We have formulated three deep speech enhancement structures, established on three time–frequency​ masks comprised of Ideal Ratio Mask (IRM), Ideal Binary Mask (IBM), and Phase Sensitive Mask (PSM). Background noise also degrades the original phase of the clean speech; therefore, introduces perceptual disturbance which leads to negative impacts on the speech quality and intelligibility. To avoid speech quality and intelligibility degradations, an iterative procedure is adopted to compensate the phase during noisy backgrounds. Nonlinear Mel-scale weighted MSE ( L MW−MSE LMW−MSE ) is used as a loss function during network training, and computed the gradients which are based on the perceptually motivated nonlinear frequency scale. Usually, the output features of the conventional deep neural networks are over-smoothed which deteriorates the quality of the speech. To alleviate over-smoothness; frequency-independent spectral variance equalization is applied as a post-filtering method. The performance of the proposed deep enhancement methods is extensively evaluated and compared to the DNNs established on same time–frequency mask in various adverse noisy environments . The results have demonstrated that the proposed deep speech enhancement performed better in terms of the perceived speech quality and intelligibility.},
  archive      = {J_ASOC},
  author       = {Nasir Saleem and Muhammad Irfan Khattak},
  doi          = {10.1016/j.asoc.2020.106666},
  journal      = {Applied Soft Computing},
  pages        = {106666},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Multi-scale decomposition based supervised single channel deep speech enhancement},
  volume       = {95},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). HSMA_WOA: A hybrid novel slime mould algorithm with whale
optimization algorithm for tackling the image segmentation problem of
chest x-ray images. <em>ASOC</em>, <em>95</em>, 106642. (<a
href="https://doi.org/10.1016/j.asoc.2020.106642">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, a novel virus called COVID-19 has pervasive worldwide, starting from China and moving to all the world to eliminate a lot of persons. Many attempts have been experimented to identify the infection with COVID-19. The X-ray images were one of the attempts to detect the influence of COVID-19 on the infected persons from involving those experiments. According to the X-ray analysis, bilateral pulmonary parenchymal ground-glass and consolidative pulmonary opacities can be caused by COVID-19 — sometimes with a rounded morphology and a peripheral lung distribution. But unfortunately, the specification or if the person infected with COVID-19 or not is so hard under the X-ray images. X-ray images could be classified using the machine learning techniques to specify if the person infected severely, mild, or not infected. To improve the classification accuracy of the machine learning, the region of interest within the image that contains the features of COVID-19 must be extracted. This problem is called the image segmentation problem (ISP). Many techniques have been proposed to overcome ISP. The most commonly used technique due to its simplicity, speed, and accuracy are threshold-based segmentation. This paper proposes a new hybrid approach based on the thresholding technique to overcome ISP for COVID-19 chest X-ray images by integrating a novel meta-heuristic algorithm known as a slime mold algorithm (SMA) with the whale optimization algorithm to maximize the Kapur’s entropy. The performance of integrated SMA has been evaluated on 12 chest X-ray images with threshold levels up to 30 and compared with five algorithms: Lshade algorithm, whale optimization algorithm (WOA), FireFly algorithm (FFA), Harris-hawks algorithm (HHA), salp swarm algorithms (SSA), and the standard SMA. The experimental results demonstrate that the proposed algorithm outperforms SMA under Kapur’s entropy for all the metrics used and the standard SMA could perform better than the other algorithms in the comparison under all the metrics.},
  archive      = {J_ASOC},
  author       = {Mohamed Abdel-Basset and Victor Chang and Reda Mohamed},
  doi          = {10.1016/j.asoc.2020.106642},
  journal      = {Applied Soft Computing},
  pages        = {106642},
  shortjournal = {Appl. Soft. Comput.},
  title        = {HSMA_WOA: A hybrid novel slime mould algorithm with whale optimization algorithm for tackling the image segmentation problem of chest X-ray images},
  volume       = {95},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Cluster validity index for irregular clustering results.
<em>ASOC</em>, <em>95</em>, 106583. (<a
href="https://doi.org/10.1016/j.asoc.2020.106583">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Different clustering algorithms with different parameter settings can produce various partitions on the input data. Without the priori knowledge , it is difficult for users to select the proper clustering algorithm and the parameters for the specific data in advance. Therefore, the cluster validity index (CVI) is crucial to help select the best partition that fits the underlying structure of the data. However, most existing CVIs (including some recent ones that designed for complex partitions) have strong assumptions. They only work well for partitions where clusters are spherically distributed, with similar sizes and densities, and with large separation distances. In complicated situations where irregular clustering results (i.e., clustering results having clusters in arbitrary shapes, different sizes and densities, and with small separation distances) exist, they usually fail to find the best fitting partition. Focusing on the insufficiencies of the existing CVIs (designed for hard clustering), this paper presents a new index that helps to find the best partition produced by hard clustering algorithms when irregular clustering results exist. The proposed index uses the density changes inside a cluster, and the density changes from the inner to the inter-cluster regions of the cluster to evaluate its quality. Experiments are implemented on both real and synthetic datasets . 11 other CVIs including some well known as well as recently proposed ones are also tested for comparison. Experimental results are given to demonstrate the effectiveness of the new index.},
  archive      = {J_ASOC},
  author       = {Shaoyi Liang and Deqiang Han and Yi Yang},
  doi          = {10.1016/j.asoc.2020.106583},
  journal      = {Applied Soft Computing},
  pages        = {106583},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Cluster validity index for irregular clustering results},
  volume       = {95},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020b). Prediction model of energy market by long short term memory
with random system and complexity evaluation. <em>ASOC</em>,
<em>95</em>, 106579. (<a
href="https://doi.org/10.1016/j.asoc.2020.106579">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to the frequent and violent fluctuation of energy futures prices, the investment risk of energy investors is increased. Forecasting energy futures prices has progressively become the focus of research. However, traditional prediction model only conducts forecasting based on historical data without considering the behavior of the market, resulting in poor accuracy. In this paper, the random time effective function that considers the timeliness of historical data and the random change of market environment is applied to the long short term memory model to establish a novel prediction model, which is denoted by long short term memory with random time effective function model (LSTMRT). LSTM model has the characteristics of selective memory and the internal influence of time series, which is very suitable for the prediction of price time series. Random time effective function can give different weights to historical data. Furthermore, using multiscale cross-sample entropy (MCSE) as an innovative method to reveal the performance of prediction. Finally, comparing with other models selected in this paper, error evaluations and statistical comparisons are utilized to demonstrate the advantages and superiority of the proposed model. LSTMRT model has the effect of random movement and keeps the trend fluctuation of the original nonlinear data, which makes the prediction more accurate and more credible.},
  archive      = {J_ASOC},
  author       = {Yu Yang and Jun Wang and Bin Wang},
  doi          = {10.1016/j.asoc.2020.106579},
  journal      = {Applied Soft Computing},
  pages        = {106579},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Prediction model of energy market by long short term memory with random system and complexity evaluation},
  volume       = {95},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Hybrid attribute conditional adversarial denoising
autoencoder for zero-shot classification of mechanical intelligent fault
diagnosis. <em>ASOC</em>, <em>95</em>, 106577. (<a
href="https://doi.org/10.1016/j.asoc.2020.106577">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data-based intelligent fault diagnosis method is a research hotspot in modern mechanical systems . However, due to practical limitations, fault samples under all working conditions cannot be obtained, which would cause the data-based model lack of particular training data, resulting in unsatisfied testing performance. Therefore, zero-shot classification of mechanical intelligent fault diagnosis is a very practical work. Inspired by the zero-shot learning method, hybrid attribute conditional adversarial denoising autoencoder (CADAE), which uses hybrid attribute as condition, is proposed to solve the zero-shot classification problem. CADAE consists of three network modules: an encoder, a generator and a discriminator . The discriminator is applied to control the data distribution of hidden layer encoded by the encoder, and we add hybrid attribute condition into hidden layer to control the reconstruction process of generator. Finally, the generator module of the trained CADAE would be used to generate samples to train a classifier for missing classes. The proposed method is verified with three datasets under different data missing conditions. The results show that the proposed method could effectively solve the zero-shot classification problem with high classification accuracy exceeds 95\%.},
  archive      = {J_ASOC},
  author       = {Haixin Lv and Jinglong Chen and Tongyang Pan and Zitong Zhou},
  doi          = {10.1016/j.asoc.2020.106577},
  journal      = {Applied Soft Computing},
  pages        = {106577},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Hybrid attribute conditional adversarial denoising autoencoder for zero-shot classification of mechanical intelligent fault diagnosis},
  volume       = {95},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Comprehensive system based on a DNN and LSTM for predicting
sinter composition. <em>ASOC</em>, <em>95</em>, 106574. (<a
href="https://doi.org/10.1016/j.asoc.2020.106574">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Because of the lag in sinter composition detection, fluctuations in production conditions are not conducive to making timely adjustments to sintering. In this paper, the characteristics of sintering production data are studied, and three core conclusions are drawn. We find that these data have (1) noise, (2) high dimensionality , and (3) time correlation. Based on these findings, an integrated model based on a deep neural network (DNN) and a long short-term memory (LSTM) network is proposed; using a DNN and an LSTM network solves the problem of developing a system model according to the given input and output data to predict the chemical composition of sinter. Specifically, first, we use a box plot and an isolated forest (iForest) algorithm to detect and filter noise in the data preparation stage and then propose using key feature selection and the Pearson correlation coefficient to reduce the high dimensionality of the data. Then, both an online component monitoring model based on a DNN and an advance component prediction model based on an LSTM are proposed to help the field operators to control the change of the sinter composition in real time. The results of many experiments show that the proposed method performs better than the current methods: the goodness of fit (R 2 ) score of the better model is above 0.92, and both the mean square error (MSE) and mean absolute error (MAE) are approximately zero. The deep neural network-based method proposed in this paper is more suitable for the online monitoring and advance prediction of sinter components.},
  archive      = {J_ASOC},
  author       = {Song Liu and Xiaojie Liu and Qing Lyu and Fumin Li},
  doi          = {10.1016/j.asoc.2020.106574},
  journal      = {Applied Soft Computing},
  pages        = {106574},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Comprehensive system based on a DNN and LSTM for predicting sinter composition},
  volume       = {95},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Bi-directional long short-term memory model to analyze
psychological effects on gamers. <em>ASOC</em>, <em>95</em>, 106573. (<a
href="https://doi.org/10.1016/j.asoc.2020.106573">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the increasing popularity of android gaming applications on smart phones, detection of emotional states of hard-core gamers become the interest of study among psychologists. Although there exist a few interesting research works on the impact of video games over the child and adult group, most of them are only able to throw light on psychological aspects associated with the said cognitive task. The real-time detection of emotional states of the player while playing video game is still an unexplored area of research. The present work feels the void by proposing a novel scheme of detecting the emotional changes of human subject from their electroencephalographic (EEG) signal acquired during their engagement in playing video games . The problem is formulated in the settings of pattern classification, which involves four main steps: Data collection, pre-processing and artifact removal, feature extraction and classification. The novelty of the work lies in extracting the emotional content with a high recognition rate from the acquired EEG response using a deep learning algorithm. The primary contribution of the paper lies in efficient usage of a novel phase-sensitive Common Spatial Pattern algorithm for feature extraction and design of an attention-based Bi-directional Long Short-Term Memory (Bi-LSTM) network for classifying the emotional states of a video-game player into five classes: happiness, sadness, surprise, anger and neutral. Moreover, the scarcity of labeled data in EEG-based brain–computer​ interfacing (BCI) tasks is a serious issue while understanding the performance capabilities of the data-driven deep-learning models. Therefore, the present work also makes an attempt to handle the scarcity in the dimension of the extracted feature using a novel feature augmentation algorithm before feeding the feature-vector to the proposed Bi-LSTM network. Experiments undertaken yield productive and conclusive results that validate the efficacy of the proposed framework with the accuracy rate of 88.71\%.},
  archive      = {J_ASOC},
  author       = {Lidia Ghosh and Sriparna Saha and Amit Konar},
  doi          = {10.1016/j.asoc.2020.106573},
  journal      = {Applied Soft Computing},
  pages        = {106573},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Bi-directional long short-term memory model to analyze psychological effects on gamers},
  volume       = {95},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Hybrid SVM-CIPSO methods for optimal operation of reservoir
considering unknown future condition. <em>ASOC</em>, <em>95</em>,
106572. (<a href="https://doi.org/10.1016/j.asoc.2020.106572">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, new hybrid methods have been proposed to solve reservoir operation optimization problem for uncertain water inflows condition by equipping improved particle swarm optimization (IPSO) algorithm with support vector machine (SVM) method. The constrained version of IPSO algorithm (CIPSO) has been used here to improve the efficiency of the IPSO algorithm. In CIPSO algorithm, the problem constraints have been explicitly satisfied leading to smaller search space and finally smaller computational cost. Two approaches have been considered to propose the hybrid methods . In the first approach, named SVM-CIPSO1, water inflows into the dam reservoir have been predicted using SVM model and these predicted values have been used to solve reservoir operation optimization problem using CIPSO algorithm. However, in the second approach, named SVM-CIPSO2, at first, the CIPSO algorithm has been applied to solve reservoir operation optimization problem using the historical data and finally the optimal water release values have been used as input and output data to create a SVM model for predicting optimal water release from reservoir for the future condition. For comparison purpose, the ANN model has been also used to predict the water inflow or release values for the future condition and the standard form of IPSO algorithm has been also used to solve the optimization problem. Here, to evaluate the proposed approaches, the optimal water release values form Zayandehroud dam reservoir have been obtained using proposed methods and the reliability, resiliency, vulnerability and sustainability indexes have been computed. Comparison of the results indicates the capability of the proposed methods to predict the optimal water release values for future condition with acceptable accuracy. In other words, the RMSE values of SVM model for test, validation and training processes are 9.7104 (23. 56196), 11.2553 (42.69093), and 7.9556 (47.9346) MCM, respectively, which are obtained using second (first) approach. In addition, the best reliability, resiliency, vulnerability and sustainability index values are 51.95\% (45.45\%), 45.95\% (38.10\%), 3.539 (0.0041) MCM and 62.02\% (55.74\%), respectively, which are obtained using SVM-CIPSO2 (SVM-CIPSO1) method.},
  archive      = {J_ASOC},
  author       = {Ramtin Moeini and Mohammad Babaei},
  doi          = {10.1016/j.asoc.2020.106572},
  journal      = {Applied Soft Computing},
  pages        = {106572},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Hybrid SVM-CIPSO methods for optimal operation of reservoir considering unknown future condition},
  volume       = {95},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Modeling and control of nonlinear systems using an adaptive
LAMDA approach. <em>ASOC</em>, <em>95</em>, 106571. (<a
href="https://doi.org/10.1016/j.asoc.2020.106571">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a soft computing technique for modeling and control of nonlinear systems using the online learning criteria. In order to obtain an accurate modeling, and therefore a controller with good performance, a method based on the fundamentals of the artificial intelligence algorithm, called LAMDA (Learning Algorithm for Multivariate Data Analysis), is proposed, with a modification of its structure and learning method that allows the creation of an adaptive approach. The novelty of this proposal is that for the first time LAMDA is used for fuzzy modeling and control of complex systems, which is a great advantage if the mathematical model is not available, partially known, or variable. The adaptive LAMDA consists of a training stage to establish initial parameters for the controller, and the application stage in which the control strategy is computed and updated using an online learning that evaluates the closed-loop system. We validate the method in several control tasks: (1) Regulation of mixing tank with variable dead-time (slow variable dynamics), (2) Regulation of a Heating, Ventilation and Air-Conditioning (HVAC) system (multivariable slow nonlinear dynamics), and (3) trajectory tracking of a mobile robot (multivariable fast nonlinear dynamics). The results of these experiments are analyzed and compared with other soft computing control techniques, demonstrating that the proposed method is able to perform an accurate control through the proposed learning technique.},
  archive      = {J_ASOC},
  author       = {Luis Morales and Jose Aguilar and Andrés Rosales and Danilo Chávez and Paulo Leica},
  doi          = {10.1016/j.asoc.2020.106571},
  journal      = {Applied Soft Computing},
  pages        = {106571},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Modeling and control of nonlinear systems using an adaptive LAMDA approach},
  volume       = {95},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). 2-stage modified random forest model for credit risk
assessment of P2P network lending to “three rurals” borrowers.
<em>ASOC</em>, <em>95</em>, 106570. (<a
href="https://doi.org/10.1016/j.asoc.2020.106570">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rapid growth of the P2P online loan industry in the “Three Rurals” (agriculture, rural areas, and farmers) sector, it is imperative to manage the borrowing risk of borrowers in the rural areas. A credit risk assessment model is proposed to classify the credit worthiness of the “Three Rurals” borrowers. We select the loan data of the Pterosaur Loan platform as the research sample, and establish a 2-stage Syncretic Cost-sensitive Random Forest (SCSRF) model to evaluate the credit risk of the borrowers. From the random forest, we construct a cost relationship from the actual distribution of the data categories, introduce a weighted Mahalanobis distance using the entropy weight method in the cost function, and adopt a weighted voting for the cost-sensitive decision tree base classifier . The parameters of the SCSRF model are optimized via a grid search. We validate the SCSRF classification model against several established credit evaluation models.},
  archive      = {J_ASOC},
  author       = {Congjun Rao and Ming Liu and Mark Goh and Jianghui Wen},
  doi          = {10.1016/j.asoc.2020.106570},
  journal      = {Applied Soft Computing},
  pages        = {106570},
  shortjournal = {Appl. Soft. Comput.},
  title        = {2-stage modified random forest model for credit risk assessment of P2P network lending to “Three rurals” borrowers},
  volume       = {95},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). New metric learning model using statistical inference for
kinship verification. <em>ASOC</em>, <em>95</em>, 106569. (<a
href="https://doi.org/10.1016/j.asoc.2020.106569">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Kinship verification aims to predict whether there is a kin relationship between a pair of facial images . Previous representative research has proved that learning an appropriate similarity metric plays an important role in this task. Most of the related metric learning methods focus on learning a single Mahalanobis distance metric under the assumption that kin data has a unimodal distribution. Instead, in this paper, we propose a novel nonlinear multi-metric learning (NMML) method to learn a similarity measure where the pairwise difference is drawn from a mixture distribution. Particularly, the similarity measure is derived based on a statistical inference perspective, which can indicate the characteristics of a likelihood ratio that captures the subtle resemblance between identities. To make better use of multiple estimators, we further propose an ensemble NMML (ENMML) method to perform multiple estimators fusion to improve the performance of kinship verification. Extensive experiments on three publicly available kinship datasets demonstrate the feasibility and effectiveness of our proposed methods.},
  archive      = {J_ASOC},
  author       = {Xiaoqian Qin and Dakun Liu and Bin Gui and Dong Wang},
  doi          = {10.1016/j.asoc.2020.106569},
  journal      = {Applied Soft Computing},
  pages        = {106569},
  shortjournal = {Appl. Soft. Comput.},
  title        = {New metric learning model using statistical inference for kinship verification},
  volume       = {95},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Training set selection and swarm intelligence for enhanced
integration in multiple classifier systems. <em>ASOC</em>, <em>95</em>,
106568. (<a href="https://doi.org/10.1016/j.asoc.2020.106568">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multiple classifier systems (MCS s ) constitute one of the most competitive paradigms for obtaining more accurate predictions in the field of machine learning . Systems of this type should be designed efficiently in all of their stages, from data preprocessing to multioutput decision fusion. In this article, we present a framework for utilizing the power of instance selection methods and the search capabilities of swarm intelligence to train learning models and to aggregate their decisions. The process consists of three steps: First, the essence of the complete training data set is captured in a reduced set via the application of intelligent data sampling. Second, the reduced set is used to train a group of heterogeneous classifiers using bagging and distance-based feature sampling. Finally, swarm intelligence techniques are applied to identify a pattern among multiple decisions to enhance the fusion process by assigning class-specific weights for each classifier. The proposed methodology yielded competitive results in experiments that were conducted on 25 benchmark datasets. The Matthews correlation coefficient (MCC) is regarded as the objective to be maximized by various nature-inspired metaheuristics , which include the moth-flame optimization algorithm (MFO), the grey wolf optimizer (GWO) and the whale optimization algorithm (WOA).},
  archive      = {J_ASOC},
  author       = {Amgad M. Mohammed and Enrique Onieva and Michał Woźniak},
  doi          = {10.1016/j.asoc.2020.106568},
  journal      = {Applied Soft Computing},
  pages        = {106568},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Training set selection and swarm intelligence for enhanced integration in multiple classifier systems},
  volume       = {95},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Symplectic incremental matrix machine and its application in
roller bearing condition monitoring. <em>ASOC</em>, <em>95</em>, 106566.
(<a href="https://doi.org/10.1016/j.asoc.2020.106566">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For roller bearing condition monitoring, the collected signals have complex internal structure, which can be naturally represented as matrices. Support matrix machine (SMM), as a new classifier with matrices as inputs, makes full use of the correlation between rows and columns of matrices and achieves ideal classification results . Unfortunately, SMM have ignored the issue of redundant features, which seriously affects the operational efficiency and recognition accuracy of algorithm. In this paper, we introduce symplectic geometry, l 1 l1 -norm and incremental proximal descent (IPD) to SMM, and symplectic incremental matrix machine (SIMM) is proposed. In SIMM, through symplectic geometry similarity transformation, the de-noising symplectic geometry coefficient matrix is obtained, and the noise robustness of SMM method is therefore improved. Moreover, l 1 l1 -norm is used to constrain the objective function, which can weaken the influence of redundant features, and thus greatly improving the recognition accuracy of SMM. Meanwhile, we use IPD to solve the objective function, which can obviously enhance the algorithm efficiency under the constant recognition rates. The experimental results of two kinds of roller bearings show that the proposed method has a good effectiveness in roller bearing condition monitoring, and the achieved recognition rate can reach 3\%–25\% much higher than those of the traditional recognition methods in 5-cross validation.},
  archive      = {J_ASOC},
  author       = {Haiyang Pan and Yu Yang and Ping Wang and Jian Wang and Junsheng Cheng},
  doi          = {10.1016/j.asoc.2020.106566},
  journal      = {Applied Soft Computing},
  pages        = {106566},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Symplectic incremental matrix machine and its application in roller bearing condition monitoring},
  volume       = {95},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Monitoring agriculture areas with satellite images and deep
learning. <em>ASOC</em>, <em>95</em>, 106565. (<a
href="https://doi.org/10.1016/j.asoc.2020.106565">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Agriculture applications rely on accurate land monitoring, especially paddy areas, for timely food security control and support actions. However, traditional monitoring requires field works or surveys performed by experts, which is costly, slow, and sparse. Agriculture monitoring systems are looking for sustainable land use monitoring solutions, starting with remote sensing on satellite data for cheap and timely paddy mapping. The aim of this study is to develop an autonomous and intelligent system built on top of imagery data streams, which is available from low-Earth orbiting satellites, to differentiate crop areas from non-crop areas. However, such agriculture mapping framework poses unique challenges for satellite image processing , including the seasonal nature of crop, the complexity of spectral channels, and adversarial conditions such as cloud and solar radiance. In this paper, we propose a novel multi-temporal high-spatial resolution classification method with an advanced spatio-temporal–spectral deep neural network to locate paddy fields at the pixel level for a whole year long and for each temporal instance. Our method is built and tested on the case study of Landsat 8 data due to its high spatial resolution. Empirical evaluations on real imagery datasets of different landscapes from 2016 to 2018 show the superior of our mapping model against the baselines with over 0.93 F1-score, the importance of each model design, the robustness against seasonal effects, and the visual mapping results.},
  archive      = {J_ASOC},
  author       = {Thanh Tam Nguyen and Thanh Dat Hoang and Minh Tam Pham and Tuyet Trinh Vu and Thanh Hung Nguyen and Quyet-Thang Huynh and Jun Jo},
  doi          = {10.1016/j.asoc.2020.106565},
  journal      = {Applied Soft Computing},
  pages        = {106565},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Monitoring agriculture areas with satellite images and deep learning},
  volume       = {95},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A new approach to generate diversified clusters for small
data sets. <em>ASOC</em>, <em>95</em>, 106564. (<a
href="https://doi.org/10.1016/j.asoc.2020.106564">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Clustering is a common data mining technique whose main principle states that the samples within a cluster are similar to one another and dissimilar to those in other clusters. This means that samples in the same cluster possess high homogeneity, while different clusters possess high heterogeneity. However, a user may require a result of diversified clustering. Compared to traditional clustering methods , the aim of diversified clustering is to make samples of the same cluster possess high heterogeneity, and different clusters possess high homogeneity. Diversified clustering can be practically applied to aspects of our daily lives such as normal class grouping, student grouping in learning, cluster sampling, balanced diets and assignment of jobs. Nevertheless, our survey of related papers in the research field of data mining found that there has been no proposed research for diversified clustering. In this paper, we formal define the problem of diversified clustering and propose a new method to solve this problem. Experimental results showed that our method can generate good diversified clustering. However, our method is currently only appropriate for small data sets since the execution time of our method increases quickly as the number of diversified clusters increases. We also hope this paper will garner interest in more research on effective methods to generate diversified clusters for use in data mining.},
  archive      = {J_ASOC},
  author       = {Chun-Cheng Peng and Cheng-Jung Tsai and Ting-Yi Chang and Jen-Yuan Yeh and Po-Wei Hua},
  doi          = {10.1016/j.asoc.2020.106564},
  journal      = {Applied Soft Computing},
  pages        = {106564},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A new approach to generate diversified clusters for small data sets},
  volume       = {95},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Metaheuristics for solving the vehicle routing problem with
the time windows and energy consumption in cold chain logistics.
<em>ASOC</em>, <em>95</em>, 106561. (<a
href="https://doi.org/10.1016/j.asoc.2020.106561">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this study, we consider a canonical vehicle routing problem (VRP) in the cold chain logistic system , where three special constraints are included, i.e., the dispatching time windows for each customer, different types of vehicles, and different energy consumptions and capacities for each vehicle. The objective is to minimize the total cost including the fixed cost and the energy consumptions. An improved artificial fish swarm (IAFS) algorithm is proposed, where a special encoding approach is designed to consider the problem feature with different type of vehicles. Then, improved preying and following heuristics are developed to perform the exploitation and exploration tasks. A novel customer satisfaction heuristic is embedded in the proposed algorithm, which makes the problem close to the reality. To further improve the performance of the algorithm, a right-shifting heuristic is designed to increase the customer satisfaction without increasing the energy consumption. An initialization heuristic based on the canonical Put Forward Insertion Heuristics (PFIH) is proposed to generate initial solutions with better performance. Finally, a set of realistic instances is generated to test the performance of the proposed algorithm, and after detailed experimental comparisons, the competitive performance of the proposed algorithm is verified.},
  archive      = {J_ASOC},
  author       = {Mei-xian Song and Jun-qing Li and Yun-qi Han and Yu-yan Han and Li-li Liu and Qun Sun},
  doi          = {10.1016/j.asoc.2020.106561},
  journal      = {Applied Soft Computing},
  pages        = {106561},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Metaheuristics for solving the vehicle routing problem with the time windows and energy consumption in cold chain logistics},
  volume       = {95},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Use of stochastic nature-inspired population-based
algorithms within an online adaptive controller for mechatronic devices.
<em>ASOC</em>, <em>95</em>, 106559. (<a
href="https://doi.org/10.1016/j.asoc.2020.106559">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Stochastic nature-inspired population-based algorithms are very powerful tools for solving stationary and deterministic, NP-hard optimization problems . These algorithms have rarely been applied to real-world dynamic and uncertain optimization due to their complexity. In this paper, this kind of algorithms were ported onto real hardware (i.e., the velocity controller of a one degree of freedom robot mechanism), where they were used to control the behavior of a non-linear system online. This means that the feedback response from the system must be less than 5 ms. Due to the complexity of the fitness function evaluation, a surrogate linear model was used, implemented as a single-layer artificial neural network , consisting of two phases: learning and simulation. In the first phase, the model of the nonlinear plant is learned during online operation, while in the second, the value of the fitness function needed by the optimization algorithms is predicted. Six algorithms were compared with the PI-controller in our experimental work. This were: classical evolution strategies, contemporary evolution strategies, differential evolution, self-adaptive differential evolution, particle swarm optimization , and the bat algorithm . The results showed that the algorithms outperformed PI-controller in the sense of stability, flexibility and adaptability.},
  archive      = {J_ASOC},
  author       = {Jakob Šafarič and Primož Bencak and Dušan Fister and Riko Šafarič and Iztok Fister},
  doi          = {10.1016/j.asoc.2020.106559},
  journal      = {Applied Soft Computing},
  pages        = {106559},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Use of stochastic nature-inspired population-based algorithms within an online adaptive controller for mechatronic devices},
  volume       = {95},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Identifying and prioritizing factors affecting in-cabin
passenger comfort on high-speed rail in china: A fuzzy-based linguistic
approach. <em>ASOC</em>, <em>95</em>, 106558. (<a
href="https://doi.org/10.1016/j.asoc.2020.106558">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Factors affecting customer comfort are crucial for the success of many services such as public transportation, health facilities, and so on. Therefore, the identification and prioritization of such factors is an important demand from stakeholders. This research aims to identify and prioritize the factors that affect in-cabin passenger comfort on high-speed rail (HSR) based on empirical evidence collected from China. For the identification process, the quality-management tool known as quality function deployment (QFD) to capture the voice of the customer is used to discover the most important demands as the driving influencing factors of in-cabin passenger comfort, by utilizing passengers’ feedback regarding their experiences with HSR posted in social media. Such factors will be prioritized by a fuzzy linguistic group decision-making approach based on the adoption of generalized comparative linguistic expressions obtained from a questionnaire investigation on randomly selected HSR passengers, and accomplishing a decision-solving procedure that includes consistency-checking and consensus-reaching processes to achieve an effective, reliable, and agreed prioritization of the factors. The research outputs will suggest the factors of greatest concern among all HSR passenger demands. This study gives HSR operators and designers not only insights and tools for managing HSR passenger demands but also advice for refining the design and service quality of HSR in China.},
  archive      = {J_ASOC},
  author       = {Zhen-Song Chen and Xiao-Lu Liu and Rosa M. Rodríguez and Xian-Jia Wang and Kwai-Sang Chin and Kwok-Leung Tsui and Luis Martínez},
  doi          = {10.1016/j.asoc.2020.106558},
  journal      = {Applied Soft Computing},
  pages        = {106558},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Identifying and prioritizing factors affecting in-cabin passenger comfort on high-speed rail in china: A fuzzy-based linguistic approach},
  volume       = {95},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Prioritization based taxonomy of cloud-based outsource
software development challenges: Fuzzy AHP analysis. <em>ASOC</em>,
<em>95</em>, 106557. (<a
href="https://doi.org/10.1016/j.asoc.2020.106557">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cloud-Based Outsource Software Development (COSD) is a new methodology adopted by organizations to develop software using teams of knowledge workers located across the globe using cloud computing services . However, there is a lack of understanding of challenges associated with successful execution of COSD projects. The objective of this study is to identify and prioritize the challenges that influence COSD projects. First, we conducted a Systematic Literature Review (SLR) and identified 21 challenges that impact COSD projects. Next, a questionnaire survey was developed based on the SLR findings to collect feedback from industry practitioners. Finally, we applied the Fuzzy Analytical Hierarchy Process (FAHP) to rank the identified challenges for COSD projects. We also present a prioritization-based taxonomy of the identified challenges which will help practitioners to focus on the critical areas for successful implementation of COSD projects.},
  archive      = {J_ASOC},
  author       = {Muhammad Azeem Akbar and Mohammad Shameem and Sajjad Mahmood and Ahmed Alsanad and Abdu Gumaei},
  doi          = {10.1016/j.asoc.2020.106557},
  journal      = {Applied Soft Computing},
  pages        = {106557},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Prioritization based taxonomy of cloud-based outsource software development challenges: Fuzzy AHP analysis},
  volume       = {95},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). An atanassov intuitionistic fuzzy programming method for
group decision making with interval-valued atanassov intuitionistic
fuzzy preference relations. <em>ASOC</em>, <em>95</em>, 106556. (<a
href="https://doi.org/10.1016/j.asoc.2020.106556">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The focus of this paper is on group decision making (GDM) problems with interval-valued Atanassov intuitionistic fuzzy preference relations (IV-AIFPRs). A new consistency index of an AIFPR is introduced to check the additive consistency degree of an AIFPR. Then, an additive consistency definition and an acceptable additive consistency definition of an IV-AIFPR are respectively defined by splitting an IV-AIFPR into two AIFPRs. For several IV-AIFPRs with unacceptably additive consistency, a goal program-based approach is proposed to improve their consistency simultaneously. Employing consistency degrees of individual IV-AIFPRs, decision makers’ (DMs’) weights are determined objectively and applied to integrate individual IV-AIFPRs into a collective one. Further, it is proved that the collective IV-AIFPR is acceptably additive consistent if all individual IV-AIFPRs are acceptably additive consistent. To derive priority weights of alternatives, an Atanassov intuitionistic fuzzy programming model is established and solved by three approaches considering DMs’ different risk attitudes. Thus, a novel method is put forward for GDM with IV-AIFPRs. A material selection example is analyzed to verify the effectiveness of the proposed method.},
  archive      = {J_ASOC},
  author       = {Shu-ping Wan and Gai-li Xu and Jiu-ying Dong},
  doi          = {10.1016/j.asoc.2020.106556},
  journal      = {Applied Soft Computing},
  pages        = {106556},
  shortjournal = {Appl. Soft. Comput.},
  title        = {An atanassov intuitionistic fuzzy programming method for group decision making with interval-valued atanassov intuitionistic fuzzy preference relations},
  volume       = {95},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Predicting china’s energy consumption using a novel grey
riccati model. <em>ASOC</em>, <em>95</em>, 106555. (<a
href="https://doi.org/10.1016/j.asoc.2020.106555">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper studies the China’s oil consumption and the China’s nuclear energy consumption by a grey Riccati model. The newly developed model is analysed by the trapezoidal formula of definite integrals, the theory of ordinary differential equations and the grey technique. And some special cases including the GM(1, 1) model, the grey Verhulst model and the grey Bass model are all discussed. Meanwhile, the hybrid of the simulated annealing algorithm and the genetic algorithm is utilized to search optimal background values. Further, the performance of the new model is verified through some experiments. Finally, the model is applied to study China’s energy consumption with original sequences from 2001 to 2018 claimed by British Petroleum Statistical Review of World Energy 2019 , and the results show that the new model can obtain competitive results and better than other comparative models.},
  archive      = {J_ASOC},
  author       = {Wenqing Wu and Xin Ma and Yong Wang and Wei Cai and Bo Zeng},
  doi          = {10.1016/j.asoc.2020.106555},
  journal      = {Applied Soft Computing},
  pages        = {106555},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Predicting china’s energy consumption using a novel grey riccati model},
  volume       = {95},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Energy-efficient steelmaking-continuous casting scheduling
problem with temperature constraints and its solution using a
multi-objective hybrid genetic algorithm with local search.
<em>ASOC</em>, <em>95</em>, 106554. (<a
href="https://doi.org/10.1016/j.asoc.2020.106554">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the increasing energy price and the urgent demand of manufacturing enterprises for energy conservation, energy-efficient scheduling (EES) technology has been widely investigated and applied in industry and academia. The steelmaking-continuous casting (SCC) production is the main energy-consuming sector and the key process for quality control of steel manufacturing. Due to the high-temperature characteristics of SCC production, the temperature drop deriving from non-processing process could directly lead to energy loss and increase the energy consumption of each procedure, which has important influence on the total energy consumption . Therefore, the energy-efficient steelmaking-continuous casting scheduling problem with temperature constraints (EESCCSPT) was concerned and a multi-objective mathematical programming model was introduced to minimize the penalty of due date deviation and the extra energy consumption measured by temperature drop. Comparing to the general scheduling problem, the constraint of minimum casting superheat and the constraint of target tapping temperature generated by the high-temperature technical requirements were directly considered to ensure schedule feasibility in terms of temperature. A multi-objective hybrid genetic algorithm combined with local search (MOHGALS) was presented, in which the enhanced evolutionary mechanisms combined with the improved genetic operators and the local search were also designed. Results of computational experiments showed that MOHGALS was more feasible and effective than NSGA-II and SPEA2 on the EESCCSPT.},
  archive      = {J_ASOC},
  author       = {Zhaojun Xu and Zhong Zheng and Xiaoqiang Gao},
  doi          = {10.1016/j.asoc.2020.106554},
  journal      = {Applied Soft Computing},
  pages        = {106554},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Energy-efficient steelmaking-continuous casting scheduling problem with temperature constraints and its solution using a multi-objective hybrid genetic algorithm with local search},
  volume       = {95},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A novel two-stage method for matching the technology
suppliers and demanders based on prospect theory and evidence theory
under intuitionistic fuzzy environment. <em>ASOC</em>, <em>95</em>,
106553. (<a href="https://doi.org/10.1016/j.asoc.2020.106553">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Matching the technology suppliers and demanders in the market is essential because such a matching facilitates the technology commercialization . Under the intuitionistic fuzzy environment, this paper proposes a novel two-stage matching method to match the technology suppliers and demanders by fully considering the characteristics of the real matching process. In the first stage, following the prospect theory and evidence theory, the satisfactions of the two parties with the attributes’ performances are calculated. These satisfactions are further used as the basis for the ranking of cooperation preferences. In the second stage, the price satisfaction under the influence of the cooperation preferences is considered, and the final matching pairs are obtained by maximizing the price satisfaction. We also conduct a numerical example to show the application of the novel two-stage method and compare its performance with the single-stage method The results show that the two-stage method can better identify both sides’ cooperation intentions and enhance their transaction satisfactions.},
  archive      = {J_ASOC},
  author       = {Aiping Wu and Hua Li and Ming Dong},
  doi          = {10.1016/j.asoc.2020.106553},
  journal      = {Applied Soft Computing},
  pages        = {106553},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A novel two-stage method for matching the technology suppliers and demanders based on prospect theory and evidence theory under intuitionistic fuzzy environment},
  volume       = {95},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Assessment of compressive strength of ultra-high performance
concrete using deep machine learning techniques. <em>ASOC</em>,
<em>95</em>, 106552. (<a
href="https://doi.org/10.1016/j.asoc.2020.106552">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The compressive strength of Ultra-High Performance Concrete (UHPC) is a function of the type, property and quantities of its material constituents. Empirically capturing this relationship often requires the utilization of intelligent algorithms, such as the Artificial Neural Network (ANN), to derive a predictive model that fits into an experimental dataset . However, its black-box nature prevents researchers from mathematically describing its contents. This paper attempts to address this ambiguity by employing two deep machine learning techniques – Sequential​ Feature Selection (SFS) and Neural Interpretation Diagram (NID) – to identify the critical material constituents that affect the ANN. 110 UHPC compressive strength tests varying based on the material quantities were compiled into a database to train the ANN. As a result, four material constituents were selected; mainly, cement, fly ash, silica fume and water. These material constituents were then employed into the ANN to compute more accurate predictions (r 2 = 80 2=80 .1\% and NMSE = 0.012) than the model with all eight material constituents (r 2 = 21 2=21 .5\% and NMSE = 0.035). Finally, a nonlinear regression model based on the four selected material constituents was developed and a parametric study was conducted. It was concluded that the utilization of ANN with SFS and NID drastically improved the accuracy of the model, and provided valuable insights on the ANN compressive strength predictions for different UHPC mixes.},
  archive      = {J_ASOC},
  author       = {Omar R. Abuodeh and Jamal A. Abdalla and Rami A. Hawileh},
  doi          = {10.1016/j.asoc.2020.106552},
  journal      = {Applied Soft Computing},
  pages        = {106552},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Assessment of compressive strength of ultra-high performance concrete using deep machine learning techniques},
  volume       = {95},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). An improved artificial bee colony algorithm for solving
multi-objective low-carbon flexible job shop scheduling problem.
<em>ASOC</em>, <em>95</em>, 106544. (<a
href="https://doi.org/10.1016/j.asoc.2020.106544">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Based on the analysis of multi-objective flexible job-shop scheduling problem (FJSP), a multi-objective low-carbon job-shop scheduling problem(MLFJSP) with variable processing speed constraint is proposed in this paper. The optimization objectives of MLFJSP include minimizing the makespan, total carbon emission and machine loading. Meanwhile, an improved artificial bee colony algorithm (IABC) is designed to solve the MLFJSP. The improvement of algorithm mainly includes: (1) an effective three-dimensions encoding/decoding mechanism and a mixed initialization strategy are designed to generate a better initial population; (2) special crossover operators and mutation operators were designed to increase the diversity of the population in the employed bee phase; (3)an efficient dynamic neighbor search (DNS) is applied to enhance local search capabilities in the onlooker bee phase; (4) the new food sources generation strategy was proposed to reduce the blindness in the scout bee phase. Finally, this paper carried out a series of comparative experimental studies, including the comparison before and after algorithm improvement, and the comparison between the improved algorithm with MOPSO, MODE and NSGA-II. The results demonstrate that the IABC can achieve a better performance for solving the MLFJSP.},
  archive      = {J_ASOC},
  author       = {Yibing Li and Weixing Huang and Rui Wu and Kai Guo},
  doi          = {10.1016/j.asoc.2020.106544},
  journal      = {Applied Soft Computing},
  pages        = {106544},
  shortjournal = {Appl. Soft. Comput.},
  title        = {An improved artificial bee colony algorithm for solving multi-objective low-carbon flexible job shop scheduling problem},
  volume       = {95},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A new grey prediction model and its application to
predicting landslide displacement. <em>ASOC</em>, <em>95</em>, 106543.
(<a href="https://doi.org/10.1016/j.asoc.2020.106543">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Landslide displacement prediction is an important part of reducing landslide hazard losses. The existing methods to predict landslide displacement are too complicated to be applied to engineering practice. The landslide displacement evolution and the grey model prediction mechanism show a good consistency. However, the existing grey prediction model also has some shortcomings including neglecting time term. A new grey prediction model called the background value optimization nonlinear grey prediction model (BNGM(1, 1, t 2 t2 )) is proposed to overcome the deficiencies. BNGM(1, 1, t 2 t2 ) is a univariate prediction model that incorporates the influence of the time term. The integration method is used to determine the background value, and the minimum value method is employed to obtain a constant term of time response functions. Five accuracy test methods for BNGM(1, 1, t 2 t2 ) are examined. BNGM(1, 1, t 2 t2 ) can show better performances than other multivariate prediction models including the recursive discrete multivariate grey prediction model. BNGM(1, 1, t 2 t2 ) is applied to four typical landslide case studies. The results indicate that the BNGM(1, 1, t 2 t2 ) has the best prediction accuracy. The complexity of BNGM(1, 1, t 2 t2 ) is lower than the nonlinear grey Bernoulli model, the Weibull–Bernoulli grey model , and the fractional accumulation nonlinear grey Bernoulli model. The comparison of comprehensive results demonstrates that the BNGM(1, 1, t 2 t2 )-based method has a wide application potential to predict landslide displacement.},
  archive      = {J_ASOC},
  author       = {L.Z. Wu and S.H. Li and R.Q. Huang and Q. Xu},
  doi          = {10.1016/j.asoc.2020.106543},
  journal      = {Applied Soft Computing},
  pages        = {106543},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A new grey prediction model and its application to predicting landslide displacement},
  volume       = {95},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Testing of multi layered soil models based on data obtained
from finite element models with known soil structures using
metaheuristics for parameters’ determination. <em>ASOC</em>,
<em>95</em>, 106541. (<a
href="https://doi.org/10.1016/j.asoc.2020.106541">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Grounding systems are an important part of protection systems, which protect people and devices in case of lightning strikes and defects in electro energetic systems. Grounding systems are often dimensioned using numerical models, among other numerical methods, also the Finite Element Method . Data about the soil in the surroundings of the grounding system are obtained using measurements. Soil parameters can be determined using analytical soil models. The determination of the soil models’ parameters is an optimisation problem which is based on the measured data. In this paper, different soil models are tested on different data, and compared with each other. Test data are also obtained using finite element models of different soil structures, which offer better analysis of the soil models because the soil structure is known. The horizontally, vertically layered soil and inhomogeneity in the soil are modelled. Different metaheuristics are used and tested for the determination of soil parameters: A Genetic Algorithm , Differential Evolution with two different strategies, Artificial Bee Colony , Teaching-Learning Based Optimisation and a combination of Artificial Bee Colony and Teaching-Learning Based Optimisation. Analysis of the models and solving methods are made based on the test results. As a result, the appropriate soil models among those tested are selected, which are 4, 5 and 6 layered models, and appropriate methods for parameters‘ determination are presented, which are Artificial Bee Colony and a combination of Artificial Bee Colony and Teaching-Learning Based Optimisation. Also, analysis is made of the usefulness of the horizontally multi-layered model for differently structured soils.},
  archive      = {J_ASOC},
  author       = {Marko Jesenik and Mladen Trlep},
  doi          = {10.1016/j.asoc.2020.106541},
  journal      = {Applied Soft Computing},
  pages        = {106541},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Testing of multi layered soil models based on data obtained from finite element models with known soil structures using metaheuristics for parameters’ determination},
  volume       = {95},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Heart sound segmentation via duration long–short term memory
neural network. <em>ASOC</em>, <em>95</em>, 106540. (<a
href="https://doi.org/10.1016/j.asoc.2020.106540">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Heart sound segmentation, which aims at detecting the first and second heart sound in phonocardiogram, is an essential step to automatically analyze heart valve diseases. Recently, the neural network-based methods have demonstrated their promising performance in segmenting the heart sound data. However, the methods also suffer from serious limitations due to the used envelope features. The reason is largely due to that the envelope features cannot effectively model the intrinsic sequential characteristic, resulting in the poor utilization of the duration information of heart cycles. In this paper, we propose a Duration Long–Short Term Memory network (Duration LSTM) to effectively address this problem by incorporating the duration features. The proposed method is investigated in the real-world phonocardiogram dataset (Massachusetts Institute of Technology heart sounds database) and compared with the two representatives of the existing state-of-the-art methods, the experimental results demonstrate that the proposed method has the promising performance on different tolerance windows. In addition, the proposed model also has some advantages in the impact of recording length and the phenomenon of the end effect.},
  archive      = {J_ASOC},
  author       = {Yao Chen and Jiancheng Lv and Yanan Sun and Bijue Jia},
  doi          = {10.1016/j.asoc.2020.106540},
  journal      = {Applied Soft Computing},
  pages        = {106540},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Heart sound segmentation via duration Long–Short term memory neural network},
  volume       = {95},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Cognitive visual anomaly detection with constrained latent
representations for industrial inspection robot. <em>ASOC</em>,
<em>95</em>, 106539. (<a
href="https://doi.org/10.1016/j.asoc.2020.106539">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the fast growth of intelligent manufacturing industry, developing advanced industrial inspection robots is becoming a research and application hotspot in the fields of both computer vision and robotics. This kind of industrial inspection robots is expected to automatically detect anomalous structures ( e.g. , defects, damages, rejects, etc.) from the images of the manufactured products. Generally, the existing visual anomaly detection (VAD) methods mainly focus on modeling the complex and high-dimensional distribution of normal data, while neglecting the specific visual properties of abnormal data since their frequency of occurrence is much less than that of the normal data. In this paper, inspired by the human cognition on extracting abstractly visual properties and to distinguish the anomaly patterns from the observed data, we propose a novel cognitive VAD method for industrial inspection robot. Specifically, we introduce a constrained latent space to mimic the cognitive ability of humans, where the abstraction learned from the observed normal and anomaly data are represented. We build our method based on a convolutional generative adversarial network and a denoising auto-encoder, where the adversarial learning mechanism is adopted to establish the boundary between the normal and anomaly data. In the experiment, we evaluate our method on a real-world dataset where the images are captured for the manufactured products. The comprehensive results comparing with several recent VAD methods show that the proposed method is effective to detect the anomaly images of different categories with a high accuracy.},
  archive      = {J_ASOC},
  author       = {Jie Li and Xing Xu and Lianli Gao and Zheng Wang and Jie Shao},
  doi          = {10.1016/j.asoc.2020.106539},
  journal      = {Applied Soft Computing},
  pages        = {106539},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Cognitive visual anomaly detection with constrained latent representations for industrial inspection robot},
  volume       = {95},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Parameter optimization for nonlinear grey bernoulli model on
biomass energy consumption prediction. <em>ASOC</em>, <em>95</em>,
106538. (<a href="https://doi.org/10.1016/j.asoc.2020.106538">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nonlinear Grey Bernoulli Model (NGBM(1, 1)) and its derivative model utilize the specific power exponent function to manifest the nonlinear characteristics of the energy consumption data pattern. Because the modeling constraint conditions and the data processing mechanism are rarely considered in parameter optimization of NGBM(1, 1) the aim of this paper is just to establish a novel NGBM(1, 1) optimization model with constraints using Box–Cox transformation (BC-NGBM*), in which the constraint conditions of the power index in the power function transformation are discussed according to the principle of difference information and the data processing mechanism. Parameter optimization of BC-NGBM* would be solved collectively using Quantum Adiabatic Evolution (QAE) algorithm. 143 data sets from M4-competition are studied for confirming the effectiveness of BC-NGBM* with QAE algorithm Finally, using data from 2010 to 2018, BC-NGBM* is used to forecast biomass energy consumption in China, the United States, Brazil , and Germany . The proposed model demonstrates high accuracy in all cases and is efficient for short-term biomass energy consumption forecasting.},
  archive      = {J_ASOC},
  author       = {Qinzi Xiao and Miyuan Shan and Mingyun Gao and Xinping Xiao and Mark Goh},
  doi          = {10.1016/j.asoc.2020.106538},
  journal      = {Applied Soft Computing},
  pages        = {106538},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Parameter optimization for nonlinear grey bernoulli model on biomass energy consumption prediction},
  volume       = {95},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Feature selection considering uncertainty change ratio of
the class label. <em>ASOC</em>, <em>95</em>, 106537. (<a
href="https://doi.org/10.1016/j.asoc.2020.106537">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The topic of feature selection in high-dimensional data sets has attracted considerable attention. Feature selection can reduce the dimension of feature and improve the prediction accuracy of the classification model . Information-theoretical-based feature selection methods intend to obtain classification information regarding class labels from the already-selected feature subset as much as possible. Existing methods focus on the reduced uncertainty of class labels while ignoring the change of the remained uncertainty of class labels. In the process of feature selection, the large reduced uncertainty of class labels does not signify the few remained uncertainty of class labels when different candidate features are given. In this paper, we analyze the difference between the reduced uncertainty of class labels and the remained uncertainty of class labels and propose a new term named Uncertainty Change Ratio that considers the change of uncertainty of class labels. Finally, a novel method named Feature Selection considering Uncertainty Change Ratio (UCRFS) is proposed. To prove the classification superiority of the proposed method, UCRFS is compared to three traditional methods and four state-of-the-art methods on fourteen benchmark data sets. The experimental results demonstrate that UCRFS outperforms seven other methods in terms of average classification accuracy , AUC and F1 score.},
  archive      = {J_ASOC},
  author       = {Ping Zhang and Wanfu Gao},
  doi          = {10.1016/j.asoc.2020.106537},
  journal      = {Applied Soft Computing},
  pages        = {106537},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Feature selection considering uncertainty change ratio of the class label},
  volume       = {95},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A new hybrid SSA-TA: Salp swarm algorithm with threshold
accepting for band selection in hyperspectral images. <em>ASOC</em>,
<em>95</em>, 106534. (<a
href="https://doi.org/10.1016/j.asoc.2020.106534">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hyperspectral images classification is a primordial step to produce the Land Use maps. Unfortunately, the classification accuracy depends largely on the quality of spectral bands . Several bands are non-informative and the adjacent bands are generally highly correlated. This paper presents a novel band selection approach named SSA-TA based on Salp Swarm Algorithm (SSA) which a new metaheuristic recently developed and Threshold Acceptance(TA). The proposed approach SSA-TA is a hybrid metaheuristic used to select the relevant bands by eliminating the irrelevant and redundant bands to enhance the hyperspectral image classification. This work presents two main ideas. Firstly, we propose a hybridization model based on SSA and Threshold Acceptance (TA). The basic idea is using SSA to find the promising region and use TA to enhance the exploration of the best solution. Secondly, the fitness function is designed to take into consideration three important terms: (1) the maximization of classification accuracy rate (2) the minimization of the number of selected bands (3) the minimization of correlated bands. The performance evaluation of the proposed approach is tested on three hyperspectral images widely used on remote sensing. The proposed approach is compared to other algorithms. The experimental results demonstrate the efficiency of our approach in improving the classification accuracy rate.},
  archive      = {J_ASOC},
  author       = {Seyyid Ahmed Medjahed and Mohammed Ouali},
  doi          = {10.1016/j.asoc.2020.106534},
  journal      = {Applied Soft Computing},
  pages        = {106534},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A new hybrid SSA-TA: Salp swarm algorithm with threshold accepting for band selection in hyperspectral images},
  volume       = {95},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A multi-task faster r-CNN method for 3D vehicle detection
based on a single image. <em>ASOC</em>, <em>95</em>, 106533. (<a
href="https://doi.org/10.1016/j.asoc.2020.106533">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Vehicle detection is an important part of robot environmental perception. In this paper, a 3D vehicle detection method using a single image is proposed to generate the 3D space coordinate information of the object using monocular vision for autonomous driving . The proposed method works under the multi-task framework and integrates 2D object detection, 3D object detection , orientation estimation and key point detection into one unified deep convolution neural network (DCNN) which could be trained by end-to-end learning. Besides, our proposed method is built by modifying Fast R-CNN using multi-task learning, and thus our proposed method is named multi-task Faster R-CNN (MT-Faster R-CNN). The experiments on KITTI dataset are conducted to evaluate our proposed method and the other 3D vehicle detection methods. The experimental results demonstrate that our proposed method is competitive and could significantly assist autonomous driving .},
  archive      = {J_ASOC},
  author       = {Wankou Yang and Ziyu Li and Chao Wang and Jun Li},
  doi          = {10.1016/j.asoc.2020.106533},
  journal      = {Applied Soft Computing},
  pages        = {106533},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A multi-task faster R-CNN method for 3D vehicle detection based on a single image},
  volume       = {95},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Public opinion spread risk assessment model on third-party
payment rough network. <em>ASOC</em>, <em>95</em>, 106532. (<a
href="https://doi.org/10.1016/j.asoc.2020.106532">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The spread of the public opinion in a third-party payment rough network may endanger the platform’s economic stability. Hence, network public opinion spread risk assessment is urgently needed. Unfortunately, there has been no research on this problem. Based on research of the characteristics of the public opinion spread of a third-party payment rough network, we determined that the essence of the public opinion spread risk assessment in a network is to evaluate the important nodes and the risks of customer mining. The paper establishes a node-option model and node-search algorithm based on game theory. The game results show that managers select the customers who buy the degree nodes with the greatest weight as their public opinion customers, and they then perform risk assessment so that they can better determine the risk rating and index weights. The risk ratings and index weights can help managers control and adjust policies related to the spread of public opinion spread to prevent crises.},
  archive      = {J_ASOC},
  author       = {Lixia Cao and Guo Wei and Jia Su},
  doi          = {10.1016/j.asoc.2020.106532},
  journal      = {Applied Soft Computing},
  pages        = {106532},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Public opinion spread risk assessment model on third-party payment rough network},
  volume       = {95},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). High-performance differential evolution algorithm guided by
information from individuals with potential. <em>ASOC</em>, <em>95</em>,
106531. (<a href="https://doi.org/10.1016/j.asoc.2020.106531">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the differential evolution (DE) algorithm, many adaptive methods have been studied in terms of fitness values. However, few studies exist on the information from individuals with potential, which presents a large difference in fitness values from that of previous individuals and contains much evolution information. This study proposes a high-performance DE (PDE) algorithm guided by information from individuals with potential. In PDE, all individuals are divided into individuals with potential and individuals without potential according to their improvement in fitness values. The experience learned from the generation of individuals with potential is used to guide future individuals. At each generation, the selection probability of each strategy in the strategy pool is determined by the strategy’s contribution to the improvement in fitness values when generating individuals with potential. The parameters are randomly generated with two distributions, and the location parameters of the two distributions are adjusted on the basis of the improvement in fitness values of individuals with potential. Different individuals (with or without potential) may have different characteristics and evolution methods. Therefore, the generation process of individuals with potential is separated into two cases according to whether they are from previous individuals with or without potential. The study results of the two cases are applied to guide the evolution of current individuals with and without potential. The proposed algorithm is evaluated by comparing it with five advanced DE variants on CEC2005 and seven up-to-date evolutionary algorithms on CEC2014. Comparison results demonstrate the competitive performance of the proposed algorithm. The PDE is also applied to estimate the parameters of a kinetic model of p-xylene oxidation process .},
  archive      = {J_ASOC},
  author       = {Li Tian and Zhichao Li and Xuefeng Yan},
  doi          = {10.1016/j.asoc.2020.106531},
  journal      = {Applied Soft Computing},
  pages        = {106531},
  shortjournal = {Appl. Soft. Comput.},
  title        = {High-performance differential evolution algorithm guided by information from individuals with potential},
  volume       = {95},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). An asynchronously deep reservoir computing for predicting
chaotic time series. <em>ASOC</em>, <em>95</em>, 106530. (<a
href="https://doi.org/10.1016/j.asoc.2020.106530">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Chaotic time series prediction is a research topic in both theoretical and real-life area. Its aim is to predict the future of the time series based on past observations. Reservoir computing (RC) is a promising tool widely used in time series prediction. Short-term memory (STM) is very important to model time-dependent time series by the RC approach. However, traditional RC hardly achieves sufficient STM capacity required by a complicated time series prediction task. For this reason, this paper proposes an asynchronous deep RC (ADRC), which is composed of a number of sub-reservoirs that are connected one by one in sequence. Moreover, delayed modules are inserted between every two adjacent sub-reservoirs. The sub-reservoirs in the proposed ADRC preserve the input characteristics by a relay mode and deal with them asynchronously. This makes the reservoir achieve large STM capacity and rich dynamics. The experimental results demonstrate that the proposed ADRC is prominent in modeling chaotic time series signals with high performance.},
  archive      = {J_ASOC},
  author       = {Ying-Chun Bo and Ping Wang and Xin Zhang},
  doi          = {10.1016/j.asoc.2020.106530},
  journal      = {Applied Soft Computing},
  pages        = {106530},
  shortjournal = {Appl. Soft. Comput.},
  title        = {An asynchronously deep reservoir computing for predicting chaotic time series},
  volume       = {95},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Regenerative and combinatorial random variable based
particle swarm optimization towards optimal transmission switching.
<em>ASOC</em>, <em>95</em>, 106529. (<a
href="https://doi.org/10.1016/j.asoc.2020.106529">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper describes the development of a meta-heuristic based algorithm to solve the optimal transmission switching (OTS) problem. The approach solves the mixed-integer non-linear problem considering simultaneous optimization of transmission topology and generation dispatch. The algorithm is the first ever application of particle swarm optimization (PSO) to model OTS and operates using combined real and binary (CRB) variables to solve a weighted sum of interdependent multiple objective functions. The unique stochastic generation principle of combinatorial variables is based on a blend of both uniform and biased Gaussian probability distribution functions . The required binary values of swarms are generated from the continuous values of the random Gaussian distribution with the application of the Heaviside function . All the transmission lines are considered as potential switch variables the operations of which are limited by different aspects. The algorithm can tackle the complex optimization problem with many constraints of varying difficulty. The randomness in CRB variables and unpredictability in switching may generate infeasible particle(s) resulting in island formation. The algorithm also proposes regeneration of these infeasible particle(s) by modifying them to a feasible one to avoid this islanding as well as to have stable switching possibilities. The improvement in the computational efficiency of the algorithm is proposed with the adoption of Micro-PSO for small load deviations. A wide range of unique solutions are obtained based on the preferences of the system operator. This OTS algorithm is tested using the IEEE 57 bus and the IEEE 118 bus system and encouraging results are obtained.},
  archive      = {J_ASOC},
  author       = {Sananda Pal and Sawan Sen and Jitendranath Bera and Samarjit Sengupta},
  doi          = {10.1016/j.asoc.2020.106529},
  journal      = {Applied Soft Computing},
  pages        = {106529},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Regenerative and combinatorial random variable based particle swarm optimization towards optimal transmission switching},
  volume       = {95},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Fast and accurate online sequential learning of respiratory
motion with random convolution nodes for radiotherapy applications.
<em>ASOC</em>, <em>95</em>, 106528. (<a
href="https://doi.org/10.1016/j.asoc.2020.106528">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate prediction of tumor motion for motion adaptive radiotherapy has been a challenge as respiration-induced motion is non-stationary in nature and often subjected to irregularities. Despite having a plethora of works for predicting this motion, their tracking capabilities are usually prone to large prediction errors due to the time-varying irregularities and intra-trace variabilities. To overcome this, prediction models are re-trained at regular intervals. This solution however demands a trade-off between the re-training interval and prediction accuracy in estimating the future tumor location. This is because re-training with small interval increases the computational requirements whereas a larger interval hampers the prediction performance. To address these issues, a prediction model that relies on random convolution nodes (RCN) governed by local receptive fields (LRFs) is proposed for respiratory motion prediction. The innate nature of LRFs extracts the features that contribute to the local-patterns as well as the non-stationary patterns in recent samples and subsequently learn them using extreme learning machine (ELM) theories. To address the re-training issue, we propose an online sequential learning framework (OS-fRCN) that can update the model parameters at regular intervals. Suitability of the proposed OS-fRCN for respiratory motion prediction is evaluated on 304 respiratory motion traces. Performance analysis conducted at four prediction horizons (in-line with the commercially available radiotherapy systems) demonstrated that the proposed OS-fRCN method requires less computational complexity and yields robust, accurate prediction performance when compared with existing prediction methods.},
  archive      = {J_ASOC},
  author       = {Yubo Wang and Zhibin Yu and Tatinati Sivanagaraja and Kalyana C. Veluvolu},
  doi          = {10.1016/j.asoc.2020.106528},
  journal      = {Applied Soft Computing},
  pages        = {106528},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Fast and accurate online sequential learning of respiratory motion with random convolution nodes for radiotherapy applications},
  volume       = {95},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A flight maneuver recognition method based on multi-strategy
affine canonical time warping. <em>ASOC</em>, <em>95</em>, 106527. (<a
href="https://doi.org/10.1016/j.asoc.2020.106527">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Maneuver recognition for unmanned combat air vehicle (UCAV) is a necessary technique for autonomous air combat. As a spatiotemporal alignment problem of multidimensional time series, the flight maneuver recognition is solved by a novel alignment measure, multi-strategy affine canonical time warping approach (MACTW) and its derivative form, which are extensions of affine canonical time warping (ACTW). MACTW makes several contributions: (1) it proposes multi-strategy success-history based adaptive differential evolution algorithm with linear population size reduction (MLSHADE) to accelerate the search of warping path of dynamic time warping (DTW); (2) it introduces affine strategy to address offset and scaling of canonical time warping (CTW), which is a combination of DTW and canonical correlation analysis (CCA); and (3) it extends ACTW based on MLSHADE to align multidimensional time series In addition, MLSHADE is a novel optimization technique that employs weighted mutation, inferior solution search, and eigen Gaussian walk strategies to improve the optimization efficiency. The experimental results on the CEC 2018 test suite illustrate the superior benefits of MLSHADE. UCAV flight maneuver recognition system which includes segmentation, preprocessing and recognition modules is modeled. The experimental results on UCR datasets and UCAV maneuver datasets including action units and long maneuver datasets illustrate the superiority of MACTW and its derivative form compared with other state-of-the-art alignment measures.},
  archive      = {J_ASOC},
  author       = {Zhenglei Wei and Dali Ding and Huan Zhou and Zhuoran Zhang and Lei Xie and Le Wang},
  doi          = {10.1016/j.asoc.2020.106527},
  journal      = {Applied Soft Computing},
  pages        = {106527},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A flight maneuver recognition method based on multi-strategy affine canonical time warping},
  volume       = {95},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). An adaptive harris hawks optimization technique for two
dimensional grey gradient based multilevel image thresholding.
<em>ASOC</em>, <em>95</em>, 106526. (<a
href="https://doi.org/10.1016/j.asoc.2020.106526">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A metaheuristic algorithm called Harris hawks optimization (HHO) is gaining its popularity among its clan and useful for optimization. In this algorithm, the prey gets completely exhausted when the escape energy is equal to zero, therefore it fails to explore further. The random operator chosen in the existing method is a wastage of search agents (Harris hawk). To overcome this issue, we propose an adaptive Harris Hawks optimization (AHHO) technique. In this work, the mutation is employed to restrict the escape energy within the range 0 , 2 0, 2 , except for the mutation interval. Our method adaptively decides the chance of the Harris hawk would do perch along with the other family members or move to a random tall tree with the help of average fitness. The proposed AHHO algorithm is benchmarked with 23 classical test functions and 30 modern test function from CEC 2014 test suite consisting of unimodal, multimodal, hybrid and composite functions. The qualitative and quantitative analysis , which include metrics such as statistical results, convergence curves, p p -value from Wilcoxon rank-sum test and Friedman mean rank. It reveals that AHHO provides good results when compared with other well-known nature-inspired algorithms. It can be used for multilevel thresholding which is an optimization problem . Recently, 2D histogram based multilevel image thresholding techniques are becoming more popular for different image processing applications. The local averaging scheme used for the construction of a 2D histogram in existing methods fails to preserve the edge information. The choice of the diagonal pixels only results in the loss of information making the earlier multi-level thresholding methods inefficient to retain the spatial correlation information. Although the computation of 2D histogram based on grey gradient information is a better way to threshold an image, it faces problems due to the presence of the edge magnitude peaks. These problems are solved by investigating an improved 2D grey gradient (I2DGG) method, a new technique is suggested in this paper to suppress high edge magnitudes. The I2DGG is a maximization problem, which requires an exhaustive search process. Therefore, AHHO is used to obtain the optimal threshold values. The result of our proposed AHHO based multilevel thresholding using the I2DGG method is obtained using all the 500 images from the Berkeley Segmentation Data set (BSDS 500). When we compare the proposed method I2DGG with 2D Tsallis entropy and 1D Tsallis entropy based multilevel thresholding, the I2DGG outperforms other methods. The experimental results are also compared with the state-of-art optimization-based multilevel thresholding methods, which shows our proposed method is beneficial to the segmentation field of image processing .},
  archive      = {J_ASOC},
  author       = {Aneesh Wunnava and Manoj Kumar Naik and Rutuparna Panda and Bibekananda Jena and Ajith Abraham},
  doi          = {10.1016/j.asoc.2020.106526},
  journal      = {Applied Soft Computing},
  pages        = {106526},
  shortjournal = {Appl. Soft. Comput.},
  title        = {An adaptive harris hawks optimization technique for two dimensional grey gradient based multilevel image thresholding},
  volume       = {95},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Multiscale intelligent fault detection system based on
agglomerative hierarchical clustering using stacked denoising
autoencoder with temporal information. <em>ASOC</em>, <em>95</em>,
106525. (<a href="https://doi.org/10.1016/j.asoc.2020.106525">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning-based process monitoring has achieved remarkable progress. Generally, a deep model is empirically selected before the data features are learned. In this study, the interpretability and suitability of stacked denoising autoencoder (SDAE) in process monitoring territory are theoretically analyzed and validated. Considering that the data will show different feature representations at different scales, such as overall outline, local information, and microscopic details, this study utilizes the concept of multiscale analysis to mine the feature information of raw data deeply in different scales. The multiscale analysis is performed on the basis of agglomerative hierarchical clustering and silhouette coefficient, which makes the analysis data characteristics-based and intelligently abandons the intervention of manual prior knowledge. Then, the SDAE models are established under each scale to learn the high-order and robust features from the data with noise and fluctuation, and all monitoring results of the different scales are integrated using the Bayesian inference. Finally, given the temporal information in sequence data, the state representation of previous events is embedded into the current decision through a sliding window. The numerical process, benchmark Tennessee Eastman and real steel plate process are used to analyze the superiority of the proposed method (MSDAE-TP) over other deep learning-based monitoring methods.},
  archive      = {J_ASOC},
  author       = {Jianbo Yu and Xuefeng Yan},
  doi          = {10.1016/j.asoc.2020.106525},
  journal      = {Applied Soft Computing},
  pages        = {106525},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Multiscale intelligent fault detection system based on agglomerative hierarchical clustering using stacked denoising autoencoder with temporal information},
  volume       = {95},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Modified social group optimization—a meta-heuristic
algorithm to solve short-term hydrothermal scheduling. <em>ASOC</em>,
<em>95</em>, 106524. (<a
href="https://doi.org/10.1016/j.asoc.2020.106524">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Social Group Optimization (SGO), developed by Satapathy et al. in the year 2016, is a class of meta-heuristic optimization inspired by social behavior. It has two phases: improving phase and acquiring phase. In the improving phase, each individual improves its knowledge by interacting with the best person/solution and in acquiring phase, the individuals interact with randomly selected individuals and the best person simultaneously to acquire knowledge. Modified Social Group Optimization (MSGO) is the improved version of SGO, where the acquiring phase is modified. A self-awareness probability factor is added in the acquiring phase, which enhances the learning capability of an individual from the best-learned person in the societal setup. It is observed that this modification has improved both exploration and exploitation abilities in comparison with the conventional SGO. To analyze the performance of the MSGO, an exhaustive performance comparison is made with GA , PSO , DE, ABC , and a few newer algorithms of the years 2010–2019. The results are tabulated in six experiments. Later, MSGO is applied to solve the short-term hydrothermal scheduling (HTS) problem. The central objective of the HTS problem is to ascertain the optimal plan of action for hydro and thermal generation minimizing the fuel cost of thermal plants and, at the same time satisfying various operational and physical constraints. The valve point loading effect related to the thermal power plants, transmission loss, and other constraints lead HTS as a complex non-linear, non-convex, and non-smooth optimization problem . Simulation results clearly show that the MSGO method is capable of obtaining a better solution.},
  archive      = {J_ASOC},
  author       = {Anima Naik and Suresh Chandra Satapathy and Ajith Abraham},
  doi          = {10.1016/j.asoc.2020.106524},
  journal      = {Applied Soft Computing},
  pages        = {106524},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Modified social group optimization—a meta-heuristic algorithm to solve short-term hydrothermal scheduling},
  volume       = {95},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A multi-objective immune algorithm for intrusion feature
selection. <em>ASOC</em>, <em>95</em>, 106522. (<a
href="https://doi.org/10.1016/j.asoc.2020.106522">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Feature selection plays a crucial role in classification problems, which tries to remove redundant or irrelevant features by mapping high-dimensional data to low-dimensional ones. Thus, this approach can improve the classification accuracy and reduce the computational cost to train the classification model . In this paper, we suggest an improved multi-objective immune algorithm (MOIA) for feature selection in intrusion detection . Specifically, the feature subsets for intrusion detection are treated as the individuals for immune optimization, which will select suitable feature subsets in order to reduce the dimensions of the dataset. After that, a neural network is used to train the classification model using the selected suitable feature subsets, and the output of the classification model is regarded as the target fitness value for each individual. As multiple attack types are considered in this paper, a traditional MOIA is modified by using an elite selection strategy based on the reference vectors, which can maintain the individuals with more promising performance when distinguishing more than five attack types in intrusion detection. By this way, the proposed algorithm can accelerate the convergence speed of classification, which also improves the classification accuracy . Experimental results on the NSL-KDD and UNSW-NB15 datasets validate the higher classification accuracy of the proposed algorithm.},
  archive      = {J_ASOC},
  author       = {Wenhong Wei and Shuo Chen and Qiuzhen Lin and Junkai Ji and Jianyong Chen},
  doi          = {10.1016/j.asoc.2020.106522},
  journal      = {Applied Soft Computing},
  pages        = {106522},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A multi-objective immune algorithm for intrusion feature selection},
  volume       = {95},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Decision-making in cognitive paradoxes with contextuality
and quantum formalism. <em>ASOC</em>, <em>95</em>, 106521. (<a
href="https://doi.org/10.1016/j.asoc.2020.106521">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Contextuality in human cognition can be defined as the existence of thoughts that are locally consistent and globally inconsistent. Absence of contextuality in the representational theories that are used for modelling cognitive processes results in inconsistencies between expected and actual decisions. These inconsistencies are usually regarded as paradoxes. However, recent literature reports that the quantum formalism is successful in explaining these paradoxes. In this article, we show the remarkable connection between contextuality, cognitive paradoxes and quantum formalism with a focus on decision-making. In this regard, we first analyse the paradoxes of human cognition from contextuality perspective as they share the pattern of local consistency and global inconsistency. This analysis is carried out using the proposed two-step contextuality analysis framework using the theories such as contextuality-by-Default and State Context Property formalism. Subsequently, this analysis provides an understanding on the dynamics of thoughts that has led to the paradoxical decisions which varies from the theoretical decisions. On the other hand, the success of quantum approach to cognitive paradoxes is due to their support for contextuality which is facilitated by the natural quantum characteristics. In this regard, we also narrate on how natural quantum characteristics facilitate contextuality with an example on decision-making. As a result, this article brings out the importance of contextuality in cognitive paradoxes and quantum formalism. Inferences obtained from this analysis emphasize on how and why quantum formalism is suitable for cognitive paradoxes. Our contribution stands out from the literature due to its comprehensive nature.},
  archive      = {J_ASOC},
  author       = {Ishwarya M.S. and Aswani Kumar Cherukuri},
  doi          = {10.1016/j.asoc.2020.106521},
  journal      = {Applied Soft Computing},
  pages        = {106521},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Decision-making in cognitive paradoxes with contextuality and quantum formalism},
  volume       = {95},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Hyper-heuristics using multi-armed bandit models for
multi-objective optimization. <em>ASOC</em>, <em>95</em>, 106520. (<a
href="https://doi.org/10.1016/j.asoc.2020.106520">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this work, we explore different multi-armed bandit-based hyper-heuristics applied to the multi-objective permutation flow shop problem. It is a scheduling problem which has been extensively studied due to its relevance for industrial engineering. Three multi-armed bandit basic formulations are used in the hyper-heuristic selection mechanism: (i) classic, (ii) restless, and (iii) contextual. All the three approaches are considered online selection perturbative hyper-heuristics as they are designed to choose the low level heuristic (crossover and mutation operators) that should be applied to modify each solution during the search process. Performances of the proposed approaches combined with MOEA/DD (Multi-objective Evolutionary Algorithm based on Dominance and Decomposition) are evaluated using the hypervolume indicator and nonparametric statistical tests on a set of 220 problem instances with two and three objectives. The best proposed approach (contextual with a linear realizability assumption) is compared with a stand-alone version of MOEA/DD using the best considered crossover and mutation operator. It is also compared with three state-of-the-art multi-objective algorithms. Results show that this best approach is able to outperform MOEA/DD in scenarios with two and three objectives and by encompassing both, Pareto and decomposition strategies, is competitive with the state of the art in those scenarios.},
  archive      = {J_ASOC},
  author       = {Carolina P. Almeida and Richard A. Gonçalves and Sandra Venske and Ricardo Lüders and Myriam Delgado},
  doi          = {10.1016/j.asoc.2020.106520},
  journal      = {Applied Soft Computing},
  pages        = {106520},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Hyper-heuristics using multi-armed bandit models for multi-objective optimization},
  volume       = {95},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Centroid of polygonal fuzzy sets. <em>ASOC</em>,
<em>95</em>, 106519. (<a
href="https://doi.org/10.1016/j.asoc.2020.106519">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In real applications, calculation of centroid , as a common defuzzification method, requires discretization of the fuzzy sets which makes the calculation on one hand time-consuming and on the other hand approximated. Particularly, centroid calculation for higher order fuzzy sets, is a bottleneck toward their real use. In this paper, we introduce polygonal fuzzy sets of type-1, interval type-2, and general type-2. Then we propose a closed form formula for exact and fast calculation of the centroid of polygonal type-1 fuzzy sets defined on continuous domain without the need of discretization . With the integration of the proposed method with EKM algorithm – that is a well-known algorithm for calculating the centroid of interval type-2 fuzzy sets – a fast and precise method for calculating the centroid of polygonal interval type-2 fuzzy sets and polygonal type-2 fuzzy sets on discrete and continuous domain are presented. It is shown that the proposed algorithms calculate the centroids of polygonal fuzzy sets faster and more accurately.},
  archive      = {J_ASOC},
  author       = {Mohammad Naimi and Hooman Tahayori},
  doi          = {10.1016/j.asoc.2020.106519},
  journal      = {Applied Soft Computing},
  pages        = {106519},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Centroid of polygonal fuzzy sets},
  volume       = {95},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Wavelet neural networks based solutions for elliptic partial
differential equations with improved butterfly optimization algorithm
training. <em>ASOC</em>, <em>95</em>, 106518. (<a
href="https://doi.org/10.1016/j.asoc.2020.106518">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this study, a machine learning approach based on the unsupervised version of wavelet neural networks (WNNs) is used to solve two-dimensional elliptic partial differential equations (PDEs). The design of the WNNs must be judiciously addressed, particularly, the adopted training algorithm , since it greatly influences the generalization performance and the convergence rate of WNNs. Although the gradient information of the commonly used gradient descent training algorithm in WNNs may direct the search to optimal weight solutions that minimize the error function, the learning process is slow due to the complex calculation of the partial derivatives. To date, on account of the derivative free characteristic and adaptability to respond to the complex dynamic changes of the interdependencies , numerous studies explored the potential benefit of integrating a meta-heuristic algorithm as the training algorithm of WNNs, where encouraging results are achieved. In this paper, an improved butterfly optimization algorithm (IBOA) is proposed and subsequently integrated into the training process of the WNNs. To evaluate the performance of the proposed IBOA training method, the obtained results are compared to the results of the momentum backpropagation (MBP), the particle swarm optimization (PSO) and the standard butterfly optimization algorithm (BOA) training methods. Statistical analyses of the results based on a sufficient number of independent runs validate the effectiveness of the proposed method in terms of accuracy, robustness and convergence.},
  archive      = {J_ASOC},
  author       = {Lee Sen Tan and Zarita Zainuddin and Pauline Ong},
  doi          = {10.1016/j.asoc.2020.106518},
  journal      = {Applied Soft Computing},
  pages        = {106518},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Wavelet neural networks based solutions for elliptic partial differential equations with improved butterfly optimization algorithm training},
  volume       = {95},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Social image mining for fashion analysis and forecasting.
<em>ASOC</em>, <em>95</em>, 106517. (<a
href="https://doi.org/10.1016/j.asoc.2020.106517">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fashion industries need to be attentive towards the changing fashion and its upcoming market demands to grow their business, optimally. This paper describes research work involved in image mining for fashion analysis and forecasting using fashion-related images collected from the social network. A novel soft clustering technique is proposed for grouping the social fashion images. This technique is robust against uncertainty found in given images. The proposed clustering approach is compared with existing soft clustering approaches. It is found that the proposed approach performs well. Attributes of fashion items found in each cluster are analyzed through correlation, causal analysis, and fashion cycle visualization. Predictive models are applied to the clustered fashion items for style forecasting. A comparative study of predictive models is also done to find an optimal technique for various fashion items. As social visual perception is helpful for decision making, the proposed system is very useful in fashion industries to uplift their business.},
  archive      = {J_ASOC},
  author       = {Seema Wazarkar and Bettahally N. Keshavamurthy},
  doi          = {10.1016/j.asoc.2020.106517},
  journal      = {Applied Soft Computing},
  pages        = {106517},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Social image mining for fashion analysis and forecasting},
  volume       = {95},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Nonlinear systems modelling based on self-organizing fuzzy
neural network with hierarchical pruning scheme. <em>ASOC</em>,
<em>95</em>, 106516. (<a
href="https://doi.org/10.1016/j.asoc.2020.106516">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, a self-organizing fuzzy neural network with hierarchical pruning scheme (SOFNN-HPS) is proposed for nonlinear systems modelling in industrial processes. In SOFNN-HPS, to strike the optimal balance between system accuracy and network complexity, an online self-organizing scheme for identifying the structure and parameters of the network simultaneously is developed. First, to enhance the characterization ability of the fuzzy rules for nonlinear systems , the asymmetric Gaussian functions that can partition the input space more flexibly are introduced as membership functions. Second, a hierarchical pruning scheme, which is designed by rule density and rule significance, is used to delete the redundant fuzzy rules while using the geometric growing criteria to generate fuzzy rules automatically, which can avoid the requirement of pre-setting the pruning threshold and prevent the mistaken deletion of significant rules. Third, an adaptive allocation strategy is adopted to set the antecedent parameters of the fuzzy rules in the learning process, which can not only adjust the region of generalized ellipsoidal basis functions for better local approximation , but also balance the accuracy of the system and the interpretability of the rule base obtained. Finally, to speed up the convergence of the estimation error, a modified recursive least square algorithm is used to update the consequent parameters of the resulting fuzzy rules online. In addition, the convergence proofs of the estimation error and the network linear parameters of SOFNN-HPS are given, and they are helpful in successfully applying the SOFNN-HPS in practical engineering. To verify the effectiveness of SOFNN-HPS, two benchmark test problems and a key water quality parameter prediction experiment in the wastewater treatment process are examined. The simulation results demonstrate that the proposed SOFNN-HPS algorithm can obtain a self-organizing fuzzy neural network with compact structure and powerful generalization performance . The source codes of SOFNN-HPS and other competitors can be downloaded from https://github.com/hyitzhb/SOFNN-HPS .},
  archive      = {J_ASOC},
  author       = {Hongbiao Zhou and Yu Zhang and Weiping Duan and Huanyu Zhao},
  doi          = {10.1016/j.asoc.2020.106516},
  journal      = {Applied Soft Computing},
  pages        = {106516},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Nonlinear systems modelling based on self-organizing fuzzy neural network with hierarchical pruning scheme},
  volume       = {95},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020c). Fault diagnosis of rolling bearing of wind turbines based
on the variational mode decomposition and deep convolutional neural
networks. <em>ASOC</em>, <em>95</em>, 106515. (<a
href="https://doi.org/10.1016/j.asoc.2020.106515">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Machine learning techniques have been successfully applied in intelligent fault diagnosis of rolling bearings in recent years. However, in the real world industrial application, the dissimilarity of data due to changes in the working conditions and data acquisition environment often cause a poor performance of the existing fault diagnosis methods . Consequently, to address these inadequacies, this paper developed a novel method by integrating the Convolutional Neural Networks (CNNs) with the Variational Mode Decomposition (VMD) algorithms. Named as “Variational Mode Decomposition with Deep Convolutional Neural Networks (VMD-DCNNs)”, the method, in an end-to-end way, directly processes raw vibration signals without artificial experiences and manual intervention to realize the fault diagnosis of rolling bearings . In addition, the CNN technique is used to extract features from each Intrinsic Mode Function (IMF) in order to address the deficiency in extracting features from a single source and to achieve an effective and efficient fault diagnosis of rolling bearings under different environments and states. The value of parameter K of the VMD-DCNNs model is optimized by considering time complexity and generalization ability of the model. Lastly, bearing experiments are conducted to verify the superiority of the VMD-DCNNs in diagnosing fault under different conditions. The visualizations of the signals in the convolutional layer explain the reasonability in selecting the value of parameter K and they also indicate that the translational invariances in a raw IMF component have been learned by the VMD-DCNNs model.},
  archive      = {J_ASOC},
  author       = {Zifei Xu and Chun Li and Yang Yang},
  doi          = {10.1016/j.asoc.2020.106515},
  journal      = {Applied Soft Computing},
  pages        = {106515},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Fault diagnosis of rolling bearing of wind turbines based on the variational mode decomposition and deep convolutional neural networks},
  volume       = {95},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Similar case matching with explicit knowledge-enhanced text
representation. <em>ASOC</em>, <em>95</em>, 106514. (<a
href="https://doi.org/10.1016/j.asoc.2020.106514">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Similar case matching is one of the important practical applications of text matching, which is a crucial issue in natural language processing . For a well-structured natural language document, the sentence-based representation and explicit knowledge elements should be effectively integrated so as to fit the original structure of the text, when constructing the document representation. In this paper, we propose a multi-task learning framework with “de- and re-construction”, which leverages the extraction of sub-tasks based on sentence-level knowledge elements to enhance the representation at the document level, to improve the performance of the model in the main task of similar case matching. Extensive experiments have proved that our proposed model outperforms methods like latent dirichlet allocation (LDA), LSTM-RNN, BERT , etc, which only focus on constructing document representations.},
  archive      = {J_ASOC},
  author       = {Dunlu Peng and Jiyin Yang and Jing Lu},
  doi          = {10.1016/j.asoc.2020.106514},
  journal      = {Applied Soft Computing},
  pages        = {106514},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Similar case matching with explicit knowledge-enhanced text representation},
  volume       = {95},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Human action recognition in videos based on spatiotemporal
features and bag-of-poses. <em>ASOC</em>, <em>95</em>, 106513. (<a
href="https://doi.org/10.1016/j.asoc.2020.106513">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Currently, there is a large number of methods that use 2D poses to represent and recognize human action in videos. Most of these methods use information computed from raw 2D poses based on the straight line segments that form the body parts in a 2D pose model in order to extract features (e.g., angles and trajectories). In our work, we propose a new method of representing 2D poses. Instead of directly using the straight line segments, firstly, the 2D pose is converted to the parameter space in which each segment is mapped to a point. Then, from the parameter space, spatiotemporal features are extracted and encoded using a Bag-of-Poses approach, then used for human action recognition in the video. Experiments on two well-known public datasets, Weizmann and KTH, showed that the proposed method using 2D poses encoded in parameter space can improve the recognition rates, obtaining competitive accuracy rates compared to state-of-the-art methods.},
  archive      = {J_ASOC},
  author       = {Murilo Varges da Silva and Aparecido Nilceu Marana},
  doi          = {10.1016/j.asoc.2020.106513},
  journal      = {Applied Soft Computing},
  pages        = {106513},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Human action recognition in videos based on spatiotemporal features and bag-of-poses},
  volume       = {95},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Artificial neural network based crossover for evolutionary
algorithms. <em>ASOC</em>, <em>95</em>, 106512. (<a
href="https://doi.org/10.1016/j.asoc.2020.106512">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recombination is a powerful way of generating new solutions in Evolutionary Algorithms . There are many ways to implement recombination. Traditional recombination operators do not use information about parents, evolutionary process, or models for variable interaction in order to find better ways to recombine solutions. Some modern recombination operators use information about parents and models for variable interaction, but they cannot always be efficiently applied. We propose to use an artificial neural network to compute the recombination mask, given two parents. Here, a radial basis function network (RBFN) is trained online using past successful recombination cases obtained during the optimization performed by the evolutionary algorithm. The RBFN crossover (RBFNX) is used together with other recombination operators (here, uniform crossover is employed). Applying RBFNX has O ( N ) O(N) time complexity, where N N is the dimension of the optimization problem . Results of experiments with genetic algorithms , applied to two binary optimization problems, and evolution strategies, applied to continuous optimization test problems, indicate that RBFNX is generally able to improve the successful recombination rates.},
  archive      = {J_ASOC},
  author       = {Renato Tinós},
  doi          = {10.1016/j.asoc.2020.106512},
  journal      = {Applied Soft Computing},
  pages        = {106512},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Artificial neural network based crossover for evolutionary algorithms},
  volume       = {95},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Efficient well placement optimization coupling hybrid
objective function with particle swarm optimization algorithm.
<em>ASOC</em>, <em>95</em>, 106511. (<a
href="https://doi.org/10.1016/j.asoc.2020.106511">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Well placement optimization is a critical part of the oil field development planning which aims to find the optimal locations of wells to maximize a traditional objective function (TOF), e.g., cumulative oil production (COP) or the net present value (NPV). However, the optimization process can be quite time-consuming since it requires iterative evaluations of the objective function and each evaluation requires one simulation run of the fluid flow in the discretized time domain and space domain. This paper examined the consistency between productivity potential value (PPV) and cumulative oil production (COP), and proposed to use PPV as the objective function, whose evaluation does not require simulation run, to improve computational efficiency. However, since PPV is a static measure of a reservoir, we use PPV only in early iterations of the well placement optimization followed by TOF in later iterations in order to capture reservoir dynamics. The use of PPV and TOF in a sequential manner is referred to as a hybrid objective function (HOF). In this work, a naturally parallelizable optimization algorithm , particle swarm optimization (PSO), where simulation runs can be conducted in batches is used as the optimizer. The effectiveness of the proposed procedure is validated based on three numerical examples including a 2D model, the PUNQ-S3 model and the Egg model. Results demonstrate the well placement optimization strategy using HOF finds comparable COP within much less simulation runs compared to the optimization using TOF. In summary, well placement optimization with the objective function defined as PPV in the first 25\% iterations and TOF in the following 75\% iterations is the best combination.},
  archive      = {J_ASOC},
  author       = {Shuaiwei Ding and Ranran Lu and Yi Xi and Guangwei Liu and Jinfeng Ma},
  doi          = {10.1016/j.asoc.2020.106511},
  journal      = {Applied Soft Computing},
  pages        = {106511},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Efficient well placement optimization coupling hybrid objective function with particle swarm optimization algorithm},
  volume       = {95},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Α-cut induced fuzzy deep neural network for change detection
of SAR images. <em>ASOC</em>, <em>95</em>, 106510. (<a
href="https://doi.org/10.1016/j.asoc.2020.106510">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Change detection (CD) is a process of identifying dissimilarities from two or more co-registered multitemporal images. In this paper, we have introduced a α α -cut induced Fuzzy layer to the Deep Neural Network ( α α FDNN). Deep neural networks for change detection normally rely on the pre-classified labels of the clustering. But the pre-classified labels are more coarse and ambiguous, which is not able to highlight the changed information accurately. This challenge can be addressed by encapsulating the local information and fuzzy logic into the deep neural network. This takes the advantage of enhancing the changed information and of reducing the effect of speckle noise. As the first step in change detection, a fused difference image is generated from the mean and log ratio image with the advent of Stationary Wavelet Transform (SWT). It not only eliminates the impact of speckle noise but also it has good ability to identify the trend of change thanks to the shift invariance property . Pseudo classification is performed as the next step using Fuzzy C Means (FCM) clustering. Then, we apply reformulated α α -cut induced Fuzzy Deep Neural Network to generate the final change map which facilitates a final representation of data more suitable for the process of classification and clustering. It also results into a noteworthy improvement in the change detection result. The efficacy of the algorithm is analyzed through the parameter study. Experimental results on three Synthetic Aperture Radar (SAR) datasets demonstrate the superior performance of the proposed method compared to state-of-the art change detection methods .},
  archive      = {J_ASOC},
  author       = {S. Kalaiselvi and V. Gomathi},
  doi          = {10.1016/j.asoc.2020.106510},
  journal      = {Applied Soft Computing},
  pages        = {106510},
  shortjournal = {Appl. Soft. Comput.},
  title        = {α-cut induced fuzzy deep neural network for change detection of SAR images},
  volume       = {95},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Ensemble probabilistic prediction approach for modeling
uncertainty in crude oil price. <em>ASOC</em>, <em>95</em>, 106509. (<a
href="https://doi.org/10.1016/j.asoc.2020.106509">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The quantification of the uncertainty in crude oil price is of significance to improve the related financial decision-making. However, studies in this field have remained limited because the nonlinearity inherent in the crude oil price makes it challenging to model its uncertainty. In this paper, a novel learning system of ensemble probabilistic prediction combining five popular machine learning methods and an improved optimizer is presented to effectively model the uncertainty in crude oil price and establish the corresponding prediction interval with satisfactory reliability and resolution. An improved grey wolf optimizer based on the adaptive Cuckoo search algorithm (AGWOCS) is proposed in the learning system to integrate the prediction intervals produced by the above machine learning methods. In addition, the superiority of the proposed AGWOCS is validated based on an algorithm test, compared to three benchmark optimizers. To validate the effectiveness of the proposed learning system, the uncertainties in daily and weekly Europe Brent spot prices are modeled as a case study. The evaluation results based on the reliability, resolution, and sharpness demonstrate that the proposed learning system can yield the prediction interval with a higher quality, which has distinct advantages over eight benchmarks as a whole. The convergence and scalability of the learning system are also investigated, which reveals its feasibility.},
  archive      = {J_ASOC},
  author       = {Jianzhou Wang and Tong Niu and Pei Du and Wendong Yang},
  doi          = {10.1016/j.asoc.2020.106509},
  journal      = {Applied Soft Computing},
  pages        = {106509},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Ensemble probabilistic prediction approach for modeling uncertainty in crude oil price},
  volume       = {95},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A phase change material selection using the interval-valued
target-based BWM-CoCoMULTIMOORA approach: A case-study on interior
building applications. <em>ASOC</em>, <em>95</em>, 106508. (<a
href="https://doi.org/10.1016/j.asoc.2020.106508">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The move towards sustainable development and energy efficient solutions in the built environment has led to develop innovative solutions, particularly in the area of indoor environmental comfort in buildings. Phase change materials (PCMs) are a potential approach to target building energy use reduction. PCMs are smart materials that have been commercialized and used in buildings for effective regulation of surface and indoor air temperature fluctuations and peak energy reductions. Appropriate PCM selection is the critical step in PCM system design that determines the full effectiveness and applicability of PCM integrated building applications. Optimal selection of PCMs can be effectively executed with the support of multiple attribute decision making (MADM) approaches. Although, the traditional MADM methods have usually focused on beneficial and non-beneficial attributes, in real-world and practical problems, decision-makers tend to determine the rank of an optimal alternative based on the target values of their attributes. In response to the knowledge gap of an existing practical and functional PCM selection solution, this study proposed a hybrid and novel target-based MADM approach that combines the best-worst method (BWM) with COmbined COmpromise SOlution (CoCoSo) and multi-objective optimization of ratio analysis plus the full multiplicative form (MULTIMOORA) with an interval-valued structure called the IV-T-BWM-CoCoMULTIMOORA approach. A case study is examined to select the optimal PCM for interior building surface applications based on a case-specific construction project in Toronto, Canada. Two separate scenarios are considered, one based on thermophysical specifications and one based on managerial preferences. The connection between both technical and managerial criteria was demonstrated to clearly affect the decision-making process to take into account both thermophysical properties of PCM alternatives in the context of risk factors and design considerations. Nevertheless, the influence of the technical parameters was shown to be dominant in the final selection of the best case scenario.},
  archive      = {J_ASOC},
  author       = {Abtin Ijadi Maghsoodi and Shahrzad Soudian and Luis Martínez and Enrique Herrera-Viedma and Edmundas Kazimieras Zavadskas},
  doi          = {10.1016/j.asoc.2020.106508},
  journal      = {Applied Soft Computing},
  pages        = {106508},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A phase change material selection using the interval-valued target-based BWM-CoCoMULTIMOORA approach: A case-study on interior building applications},
  volume       = {95},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A fuzzy additive regression model with exact predictors and
fuzzy responses. <em>ASOC</em>, <em>95</em>, 106507. (<a
href="https://doi.org/10.1016/j.asoc.2020.106507">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fuzzy regression analysis is aimed at modeling the relationship between a set of fuzzy responses and a set of non-fuzzy/fuzzy predictors. However, compared to parametric methods, nonparametric regression often provides a very flexible approach to exploring the relationship between a response and the associated predictors without specifying a parametric model . In this paper, a novel fuzzy additive regression model with non-fuzzy predictors and fuzzy responses was proposed. For this purpose, a back-fitting stepwise regression approach with kernel smoothing was introduced to estimate a fuzzy smooth function corresponding to each predictor. An extended cross-validation criterion was also utilized to evaluate the unknown bandwidths. Some common goodness-of-fit criteria were employed to evaluate the performance of the proposed method. Effectiveness of the developed method was demonstrated through four numerical examples including two simulation studies based on three common kernels. The proposed method was further compared with several conventional fuzzy linear/nonlinear regression models, clearly indicating superior accuracy of the proposed model over other methods. Thus, it can be successfully applied to improve the prediction accuracy and interpretability of the fuzzy regression models for real-life applications in the context of intelligence systems.},
  archive      = {J_ASOC},
  author       = {Gholamreza Hesamian and Mohammad Ghasem Akbari},
  doi          = {10.1016/j.asoc.2020.106507},
  journal      = {Applied Soft Computing},
  pages        = {106507},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A fuzzy additive regression model with exact predictors and fuzzy responses},
  volume       = {95},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A fuzzy nonlinear programming approach for planning
energy-efficient wafer fabrication factories. <em>ASOC</em>,
<em>95</em>, 106506. (<a
href="https://doi.org/10.1016/j.asoc.2020.106506">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Wafer fabrication is an energy-consuming process. Achieving energy efficiency is, therefore, crucial for wafer fabrication factories (wafer fabs). However, limited studies have focused on resolving product quality problems for improving energy efficiency. This study proposed a novel fuzzy nonlinear programming (FNLP) approach for minimizing energy wastage caused by product quality problems. In the proposed methodology, first, the process of resolving the quality problems of a product is modeled as a fuzzy yield learning process. Then, the energy saved by the yield learning process is quantified. The total power consumption is obtained by summing the power consumptions of all the products in a wafer fab. Subsequently, an FNLP model is formulated and optimized to minimize the total energy consumption in the wafer fab by optimizing the product mix. The data from a wafer fab is used to demonstrate the applicability of the proposed FNLP approach. According to the experimental results, the total power consumption at a future period can be reduced by 17.2\% by optimizing the product mix.},
  archive      = {J_ASOC},
  author       = {Yi-Chi Wang and Min-Chi Chiu and Toly Chen},
  doi          = {10.1016/j.asoc.2020.106506},
  journal      = {Applied Soft Computing},
  pages        = {106506},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A fuzzy nonlinear programming approach for planning energy-efficient wafer fabrication factories},
  volume       = {95},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). DTOF-ANN: An artificial neural network phishing detection
model based on decision tree and optimal features. <em>ASOC</em>,
<em>95</em>, 106505. (<a
href="https://doi.org/10.1016/j.asoc.2020.106505">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, phishing emerges as one of the biggest threats to human’s daily networking environments. Phishing attackers disguise illegal URLs as normal ones to steal user’s private information with the social engineering techniques, such as emails and SMS, which calls for an effective method of preventing phishing attacks to relieve the loss by them. Neural networks can be used to detect and prevent phishing attacks because of their strong active learning abilities from massive datasets and high accuracy in data classification . However, duplicate points in the public datasets and negative and useless features in the feature vectors will trap the training of the neural networks into the problem of over-fitting, which will make the trained classifier weak when detect phishing websites . This paper proposes DTOF-ANN (Decision Tree and Optimal Features based Artificial Neural Network) to tackle this shortcoming, which is a neural-network phishing detection model based on decision tree and optimal feature selection. First, the traditional K-medoids clustering algorithm is improved with an incremental selection of initial centers to remove the duplicate points from the public datasets. Then, an optimal feature selection algorithm based on the new defined feature evaluation index, decision tree and local search method is designed to prune out the negative and useless features. Finally, the optimal structure of the neural network classifier is constructed through properly adjusting parameters and trained by the selected optimal features. Experimental results have demonstrated that DTOF-ANN exhibits higher performance than many of the existing methods.},
  archive      = {J_ASOC},
  author       = {Erzhou Zhu and Yinyin Ju and Zhile Chen and Feng Liu and Xianyong Fang},
  doi          = {10.1016/j.asoc.2020.106505},
  journal      = {Applied Soft Computing},
  pages        = {106505},
  shortjournal = {Appl. Soft. Comput.},
  title        = {DTOF-ANN: An artificial neural network phishing detection model based on decision tree and optimal features},
  volume       = {95},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Improved recognition of bacterial species using novel
fractional-order orthogonal descriptors. <em>ASOC</em>, <em>95</em>,
106504. (<a href="https://doi.org/10.1016/j.asoc.2020.106504">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Detection and distinguishing between different species of bacteria using experimental microbiology is an expensive, time-consuming, and risky process. Automatic computer-based methods for accurate detection and classification of bacteria species significantly reduce the cost, time, and avoiding scientists the risk of infection. This paper presents a novel computer-based approach for highly accurate recognition of bacterial species . The proposed method consists of two main stages. First, a novel set of fractional-order orthogonal moments proposed to extract the fine features from the color images of bacteria. Second, a new method for feature selection, SSATLBO, is proposed. In this method, the teaching-based learning optimization (TLBO) as local operators is used to improve the exploitation ability of the Salp Swarm Algorithm (SSA) to avoid the local point. The proposed detection and classification method tested by using the DIBaS dataset (Digital Image of Bacterial Species), which includes 660 images with 33 various genera and classes of bacteria. The proposed method achieved a bacterial species recognition rate, 98.68\%. The obtained results ensure the superiority of the proposed method over the traditional SSA and TLBO methods and the other Metaheuristic methods.},
  archive      = {J_ASOC},
  author       = {Mohamed Abd Elaziz and Khalid M. Hosny and Ahmed A. Hemedan and Mohamed M. Darwish},
  doi          = {10.1016/j.asoc.2020.106504},
  journal      = {Applied Soft Computing},
  pages        = {106504},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Improved recognition of bacterial species using novel fractional-order orthogonal descriptors},
  volume       = {95},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Adolescent identity search algorithm (AISA): A novel
metaheuristic approach for solving optimization problems. <em>ASOC</em>,
<em>95</em>, 106503. (<a
href="https://doi.org/10.1016/j.asoc.2020.106503">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes a novel population-based metaheuristic optimization algorithm , called Adolescent Identity Search Algorithm (AISA), which is inspired by the process of identity development/search of adolescents. AISA simulates the identity formation behavior of adolescents in the peer group. This behavior is modeled mathematically to solve optimization problems . The proposed algorithm is evaluated on thirty-nine well-known unimodal, multimodal, fixed-dimensional multimodal, composite and CEC 2019 benchmark functions to test exploration, exploitation, local optima avoidance, and convergence properties . The results are verified by an extensive comparative study with thirteen state-of-art metaheuristic algorithms . Furthermore, AISA has been used to solve IIR system identification and inverse kinematics problem of a seven Degrees Of Freedom (7-DOF) robot manipulator considered as the real-life engineering applications . The overall optimization results demonstrate that AISA possesses a strong and robust capability to produce superior performance over other competitor metaheuristic algorithms in solving various complex numerical optimization problems .},
  archive      = {J_ASOC},
  author       = {Esref Bogar and Selami Beyhan},
  doi          = {10.1016/j.asoc.2020.106503},
  journal      = {Applied Soft Computing},
  pages        = {106503},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Adolescent identity search algorithm (AISA): A novel metaheuristic approach for solving optimization problems},
  volume       = {95},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Grey–lotka–volterra model for the competition and
cooperation between third-party online payment systems and online
banking in china. <em>ASOC</em>, <em>95</em>, 106501. (<a
href="https://doi.org/10.1016/j.asoc.2020.106501">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In China, online banking and third-party online payment systems have a special relationship in which cooperation and competition co-exist. Quantifying the degree of competition and collaboration between them and predicting their future development trends will help to achieve a win-win situation. This paper uses the Lotka–Volterra model to quantitatively analyse and predict the impact of commercial banks’ online payment systems on the development of third-party online payment systems. The predator is an online bank, and the prey is a third online payment system. In the modelling process, the continuous model is discretized by using the grey theory. To improve modelling accuracy, a background coefficient is introduced. The parameters of the model are estimated by the least-squares method, and the Grey–Lotka–Volterra model for fitting the relationship between third-party payment and bank competition and cooperation is obtained. This paper empirically analyses quarterly transaction volume data for online banking and third payment. The results show that before 2011, the relationship between the two is more inhibition than cooperation, and the inhibition of online banking on third-party payment transactions is more significant than the third payment reaction. Only by strengthening cooperation can they win-win; the growth rate of third-party online payment transactions is more durable than that of online banking, but the market share is lower than that of banks; after 2011, the inhibition of third-party payment by banks is more significant than that of online banking. Compared with the period before 2011, this use decreased from inhibition to cooperation, and the growth of the overall turnover of the two plays a role in improving their development.},
  archive      = {J_ASOC},
  author       = {Shuhua Mao and Min Zhu and Xianpeng Wang and Xinping Xiao},
  doi          = {10.1016/j.asoc.2020.106501},
  journal      = {Applied Soft Computing},
  pages        = {106501},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Grey–Lotka–Volterra model for the competition and cooperation between third-party online payment systems and online banking in china},
  volume       = {95},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Finding longest common subsequences: New anytime a∗ search
results. <em>ASOC</em>, <em>95</em>, 106499. (<a
href="https://doi.org/10.1016/j.asoc.2020.106499">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Longest Common Subsequence (LCS) problem aims at finding a longest string that is a subsequence of each string from a given set of input strings. This problem has applications, in particular, in the context of bioinformatics, where strings represent DNA or protein sequences. Existing approaches include numerous heuristics, but only a few exact approaches, limited to rather small problem instances. Adopting various aspects from leading heuristics for the LCS, we first propose an exact A ∗ ∗ search approach, which performs well in comparison to earlier exact approaches in the context of small instances. On the basis of A ∗ ∗ search we then develop two hybrid A ∗ ∗ –based algorithms in which classical A ∗ ∗ iterations are alternated with beam search and anytime column search, respectively. A key feature to guide the heuristic search in these approaches is the usage of an approximate expected length calculation for the LCS of uniform random strings. Even for large problem instances these anytime A ∗ ∗ variants yield reasonable solutions early during the search and improve on them over time. Moreover, they terminate with proven optimality if enough time and memory is given. Furthermore, they yield upper bounds and, thus, quality guarantees when terminated early. We comprehensively evaluate the proposed methods using most of the available benchmark sets from the literature and compare to the current state-of-the-art methods. In particular, our algorithms are able to obtain new best results for 82 out of 117 instance groups. Moreover, in most cases they also provide significantly smaller optimality gaps than other anytime algorithms.},
  archive      = {J_ASOC},
  author       = {Marko Djukanovic and Günther R. Raidl and Christian Blum},
  doi          = {10.1016/j.asoc.2020.106499},
  journal      = {Applied Soft Computing},
  pages        = {106499},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Finding longest common subsequences: New anytime a∗ search results},
  volume       = {95},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Multi-point shortest path planning based on an improved
discrete bat algorithm. <em>ASOC</em>, <em>95</em>, 106498. (<a
href="https://doi.org/10.1016/j.asoc.2020.106498">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-point shortest path planning problem is a typical problems in discrete optimization . The bat algorithm is a nature-inspired metaheuristic optimization algorithm that is used in a wide range of fields. However, there is one problem with the BA, which is easy to premature. To solve multi-point shortest path planning problem , an improved discrete bat algorithm (IDBA) is proposed in this paper. In this algorithm, the Floyd–Warshall algorithm is first used to transform an incomplete connected graph into a complete graph whose vertex set consists of a start point and necessary points. Then the algorithm simulates the bats’ foraging and obstacle avoidance process to find the shortest path in the complete graph to satisfy the constraints. Finally, the path is transferred to the original incomplete graph to get the solution. In order to overcome the premature phenomenon of a discrete bat algorithm, the modified neighborhood operator is proposed. To prove the effectiveness of our method, we compared its performance in 26 instances with the results obtained by three different algorithms: DBA, IBA and GSA-ACS-PSOT. We also performed a sensitivity analysis on the parameters. The results indicate that the improved bat algorithm outperforms all the other alternatives in most cases.},
  archive      = {J_ASOC},
  author       = {Lijue Liu and Shuning Luo and Fan Guo and Shiyang Tan},
  doi          = {10.1016/j.asoc.2020.106498},
  journal      = {Applied Soft Computing},
  pages        = {106498},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Multi-point shortest path planning based on an improved discrete bat algorithm},
  volume       = {95},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Parallel and distributed architecture of genetic algorithm
on apache hadoop and spark. <em>ASOC</em>, <em>95</em>, 106497. (<a
href="https://doi.org/10.1016/j.asoc.2020.106497">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The genetic algorithm (GA), one of the best-known metaheuristic algorithms , has been extensively utilized in various fields of management science, operational research, and industrial engineering. The efficiency of GAs in solving large-scale optimization problems would be enhanced if the iterative processes required by the genetic operators can be implemented in a parallel and distributed computing architecture. Apache Hadoop has recently been one of the most popular systems for distributed storage and parallel processing of big data. By integrating the GA highly into Apache Hadoop , this study proposes an advanced GA parallel and distributed computing architecture that achieves the effectiveness and efficiency of GA evolution. Characterized by the sophisticated mechanism of dispatching the GA core operators into Apache Hadoop, the developed computing framework fits well with the cloud computing model . The presented GA parallelization architecture outperforms the state-of-the-art reference architectures according to the computational experiments where the testing instances of traveling salesman problems are employed. Our numerical experiments also demonstrate that the proposed architecture can readily be extended to Apache Spark .},
  archive      = {J_ASOC},
  author       = {Hao-Chun Lu and F.J. Hwang and Yao-Huei Huang},
  doi          = {10.1016/j.asoc.2020.106497},
  journal      = {Applied Soft Computing},
  pages        = {106497},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Parallel and distributed architecture of genetic algorithm on apache hadoop and spark},
  volume       = {95},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Journalistic transparency using CRFs to identify the
reporter of newspaper articles in spanish. <em>ASOC</em>, <em>95</em>,
106496. (<a href="https://doi.org/10.1016/j.asoc.2020.106496">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Journalistic transparency rises as a key issue against the lack of credibility to which journalists are exposed, as well as the media manipulators and fake news providers. With the use of Natural Language Processing (NLP) and Machine Learning (ML), it is possible to automate the extraction of information from newspaper articles to know what the sources of information are to verify their veracity. Along with this article, we present the application of Conditional Random Fields (CRFs) for a specific type of Entity Recognition (ER) task, namely, to identify what we have called the “reporter” in newspaper articles, i.e., who or what is the provider of the information. Thus, we have created a labelled corpus for the Spanish language and trained and analysed several CRFs models with a set of specific features. The obtained results suppose a solid baseline for our goal.},
  archive      = {J_ASOC},
  author       = {Francisco Jurado},
  doi          = {10.1016/j.asoc.2020.106496},
  journal      = {Applied Soft Computing},
  pages        = {106496},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Journalistic transparency using CRFs to identify the reporter of newspaper articles in spanish},
  volume       = {95},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Colonoscopy contrast-enhanced by intuitionistic fuzzy soft
sets for polyp cancer localization. <em>ASOC</em>, <em>95</em>, 106492.
(<a href="https://doi.org/10.1016/j.asoc.2020.106492">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Medical images often suffer from low contrast, irregular gray-level spacing and contain a lot of uncertainties due to constraints of imaging devices and environment (various lighting conditions) when capturing images. In order to achieve any clinical-diagnosis method for medical imaging with better comprehensibility , image contrast enhancement algorithms would be appropriate to improve the visual quality of medical images. In this paper, an automated image enhancement method is presented for colonoscopy images based on the intuitionistic fuzzy soft set. The fuzzy soft set is used to model the intuitionistic fuzzy soft image matrix based on a set of soft features of the colonoscopy images. The technique decomposes the fuzzy image into multiple blocks and estimates a soft-score based on an adaptive soft parametric hesitancy map by using the hesitant entropy for each block to quantify the uncertainties. In the processing stage, an adaptive intensity modification process is done for each block according to its soft-score. These scores are accurately addressed the gray-level ambiguities in colonoscopy images that lead to better results. Finally, the enhanced image achieved by performing a defuzzification together with all unprocessed blocks. Qualitative and quantitative assessments demonstrate that the proposed method improves image contrast and region-of-interest of polyps in colonogram. Experimental results on enhancing a large CVC-Clinic-DB and ASU-Mayo clinic colonoscopy benchmark datasets show that the proposed method outperforms the state-of-the-art medical image enhancement methods.},
  archive      = {J_ASOC},
  author       = {Biswajit Biswas and Siddhartha Bhattacharyya and Amlan Chakrabarti and Kashi Nath Dey and Jan Platos and Vaclav Snasel},
  doi          = {10.1016/j.asoc.2020.106492},
  journal      = {Applied Soft Computing},
  pages        = {106492},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Colonoscopy contrast-enhanced by intuitionistic fuzzy soft sets for polyp cancer localization},
  volume       = {95},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Target tracking strategy using deep deterministic policy
gradient. <em>ASOC</em>, <em>95</em>, 106490. (<a
href="https://doi.org/10.1016/j.asoc.2020.106490">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To address the challenge of maintaining high robustness of target tracking in a 3D dynamic high-altitude scenario, this paper presents a method to formulate continuous strategic maneuvers for unmanned combat air vehicles (UCAVs) based on deep deterministic policy gradient (DDPG). DDPG is an efficient reinforcement learning approach that helps UCAV perform a variety of navigation tasks in real-time in a dynamic and random electronic warfare environment, and therefore possesses clear advantages over other technologies. First, create a target tracking simulator, Tracker, in the cognitive electronic warfare framework, and conduct a theoretical analysis of maneuvering bias produced by environmental observational errors. Tracker can automatically correlate the maximum physical overload with UCAV’s attitude angles and desired movement commands. Second, shape the agent’s behavior rewards under the inspiration of vector-based navigation to ensure that the DDPG’s output is reliable. Finally, a DRL-based navigation decision framework is employed to validate the simulation for target tracking tasks in different environments and bring excellent results. In terms of behavior assessment, the agile maneuvers mastered by the agent are dissected by time segmentation of high-quality trajectories.},
  archive      = {J_ASOC},
  author       = {Shixun You and Ming Diao and Lipeng Gao and Fulong Zhang and Huan Wang},
  doi          = {10.1016/j.asoc.2020.106490},
  journal      = {Applied Soft Computing},
  pages        = {106490},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Target tracking strategy using deep deterministic policy gradient},
  volume       = {95},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Optimization of varying-parameter drilling for multi-hole
parts using metaheuristic algorithm coupled with self-adaptive penalty
method. <em>ASOC</em>, <em>95</em>, 106489. (<a
href="https://doi.org/10.1016/j.asoc.2020.106489">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-hole parts made of difficult-to-cut materials like discs, blisks and casings are common and require high surface quality in aero engines . In workshops, a large number of holes in these parts are drilled successively in one process to ensure their positional accuracy. Due to the fast time-varying drill wear, the surface roughness of the holes is unstable and difficult to be satisfied. To this end, this paper presents a varying-parameter drilling (VPD) method to improve machining efficiency and hole surface roughness for multi-hole parts made of Ni-based superalloy. This method uses varying cutting parameters for each hole to adapt to the varying drill wear. The main issue of this method lies in an optimization problem in which the optimal sequence of cutting parameters need to be found, with the objective of the processing time and the constraint of the hole surface roughness. As the cutting parameter sequence has a high dimension and the surface roughness of all the holes must be guaranteed, the challenge of this optimization problem is the strict constraint with a complicated non-linear boundary of the feasible zone. To address the convergence difficulty of the searching algorithm , a soft computing method based on particle swarm optimization (PSO) algorithm with a self-adaptive penalty method (SAPM) is applied. The hole surface roughness is predicted with a radial basis function (RBF) neural network. Different types of drill wear comprising flank wear , crater wear , chisel wear and outer corner wear are considered, and the grey relational analysis (GRA) is employed to select the input drill wear parameters to the network. The PSO algorithm coupled with the SAPM is used to search the global optimal solution of the optimization problem. It is found that the satisfied solutions can be searched in all the three trials with the proposed algorithm, even though the proportion of feasible solutions is severely fluctuant during the searching process. The drilling experiment confirm that, when compared with the fixed-parameter drilling, the proposed VPD and the soft computing method for solving the optimization problem can effectively improve machining efficiency and surface quality for drilling Ni-based superalloy multi-hole parts.},
  archive      = {J_ASOC},
  author       = {Ce Han and Ming Luo and Dinghua Zhang},
  doi          = {10.1016/j.asoc.2020.106489},
  journal      = {Applied Soft Computing},
  pages        = {106489},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Optimization of varying-parameter drilling for multi-hole parts using metaheuristic algorithm coupled with self-adaptive penalty method},
  volume       = {95},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Application of hybrid multi-objective moth flame
optimization technique for optimal performance of hybrid micro-grid
system. <em>ASOC</em>, <em>95</em>, 106487. (<a
href="https://doi.org/10.1016/j.asoc.2020.106487">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The research work carried out here deals with the application of a Hybrid Micro-Grid System (HMGS), which includes solar/wind/battery storage/diesel generator applied to three different parts of India. For a better analysis of all the three cases, an efficient and recent metaheuristic optimization method named hybrid multi-objective moth flame optimization (HMOMFO) technique has been used in MATLAB. The aim is to find better candidate solutions for which particle swarm optimization (PSO) technique and levy flight method are integrated, with the moth flame optimization (MFO) algorithm Moreover a new enhanced differential evolution algorithm (EDE) with self-adjustable parameters has been integrated with the second phase of the hybrid algorithm to enhance the searching and exploitation capabilities of the proposed algorithm. Here, for an initial load of 15 households, simulation and statistical results show that HMOMFO proves to be successful in terms of minimizing the price of electrical energy (PEE). Results further assure that minimum values of loss of power supply probability (LPSP) for Durgapur, Hospet, and Tirunelveli, are obtained using HMOMFO, with fewer iterations. The results also contain optimum photovoltaic (PV) power, battery bank performance in terms of autonomy days (AD), the optimum number of wind turbine generators (WT), and diesel generators (DG). The results demonstrate the mastery and effectiveness of the proposed HMOMFO against three area hybrid micro-grid systems.},
  archive      = {J_ASOC},
  author       = {Joy Bandopadhyay and Provas Kumar Roy},
  doi          = {10.1016/j.asoc.2020.106487},
  journal      = {Applied Soft Computing},
  pages        = {106487},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Application of hybrid multi-objective moth flame optimization technique for optimal performance of hybrid micro-grid system},
  volume       = {95},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Two effective methods for the irregular knapsack problem.
<em>ASOC</em>, <em>95</em>, 106485. (<a
href="https://doi.org/10.1016/j.asoc.2020.106485">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Two methods are developed for a two-dimensional cutting problem with irregular shaped items. The concepts of inner-fit raster and no-fit raster are used to search for a feasible positioning of items on a rectangular container. The first method is a Biased Random Key Genetic Algorithm , which is a population method, while the other is a Variable Neighborhood Search , which is a single trajectory method. In the proposed methods, a solution is represented by a vector of items, and the positioning of items is achieved with three rules inspired by the bottom-left strategy. When positioning items, feasible positions can be skipped as a strategy to diversify the search and escape from local optima solutions. Numerical experiments performed on literature instances show that the methods are better than the current state-of-the-art method since they obtained equal or better solutions for all the instances. On average, the occupied area increased 6.44\%, and the known optimal solution was obtained for 60\% of the instances. The population-based method performed better overall, obtaining solutions with better-occupied areas.},
  archive      = {J_ASOC},
  author       = {Layane Rodrigues de Souza Queiroz and Marina Andretta},
  doi          = {10.1016/j.asoc.2020.106485},
  journal      = {Applied Soft Computing},
  pages        = {106485},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Two effective methods for the irregular knapsack problem},
  volume       = {95},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Multi-reservoir echo state computing for solar irradiance
prediction: A fast yet efficient deep learning approach. <em>ASOC</em>,
<em>95</em>, 106481. (<a
href="https://doi.org/10.1016/j.asoc.2020.106481">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate solar irradiance prediction plays an important role in renewable energy systems . Based on time series analysis, a serially connected multi-reservoir echo state network (MR-ESN) is developed to predict solar irradiance. MR-ESN is a fast yet efficient approach, which makes use of the high efficiency of ESN and the advantages of deep learning . MR-ESN consists of multiple reservoirs in series, which are responsible for encoding the input signals into a richer state representation. The time series analysis is adopted to provide more appropriate input and output for MR-ESN. Various prediction horizons including one-hour-ahead and multi-hour-ahead prediction are conducted, respectively. The effect of reservoir layer number on the MR-ESN performance is explored in detail. Three internal qualitative indicators are adopted to investigate the performance differences of MR-ESN, i.e., probability distribution, correlation analysis, and principal component analysis (PCA) of network states. Simulation results demonstrate that MR-ESN outperforms than traditional ESN, backpropagation (BP) and Elman neural networks .},
  archive      = {J_ASOC},
  author       = {Qian Li and Zhou Wu and Rui Ling and Liang Feng and Kai Liu},
  doi          = {10.1016/j.asoc.2020.106481},
  journal      = {Applied Soft Computing},
  pages        = {106481},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Multi-reservoir echo state computing for solar irradiance prediction: A fast yet efficient deep learning approach},
  volume       = {95},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). DeGAN: Mixed noise removal via generative adversarial
networks. <em>ASOC</em>, <em>95</em>, 106478. (<a
href="https://doi.org/10.1016/j.asoc.2020.106478">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Restoration of images corrupted by mixed noise (e.g., additive white Gaussian noise and impulse noise) is very difficult due to the complexity of the mixed noise distribution. Various mixed noise removal models involve the preprocessing based on outlier detection . However, the performance of these models largely depends on the accuracy of pixel location detection of outliers, and artifacts and missing image details are prone to occur when the mixture noise is strong. In this paper, a new denoising model based on generative adversarial network (DeGAN) is proposed to remove mixed noise in images. The proposed model combines generator, discriminator , and feature extractor networks. Through the mutual game between the generator and discriminator networks combined with additional training from the feature extractor network, the generator network implements a direct mapping from the noisy image domain to the noise-free image domain. In addition, we design a new joint loss function to incorporate information from image features and human visual perception into the mixed noise elimination task, which further improves the image quality and the visual effect. Abundant experiments show that the performance of our model is better than the state-of-the-art mixed noise removal methods in three different types of mixed noise scenarios , and the joint loss function does improve the denoising performance.},
  archive      = {J_ASOC},
  author       = {Qiongshuai Lyu and Min Guo and Zhao Pei},
  doi          = {10.1016/j.asoc.2020.106478},
  journal      = {Applied Soft Computing},
  pages        = {106478},
  shortjournal = {Appl. Soft. Comput.},
  title        = {DeGAN: Mixed noise removal via generative adversarial networks},
  volume       = {95},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Adaptive verifiability-driven strategy for evolutionary
approximation of arithmetic circuits. <em>ASOC</em>, <em>95</em>,
106466. (<a href="https://doi.org/10.1016/j.asoc.2020.106466">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a novel approach for designing complex approximate arithmetic circuits that trade correctness for power consumption and play important role in many energy-aware applications. Our approach integrates in a unique way formal methods providing formal guarantees on the approximation error into an evolutionary circuit optimisation algorithm . The key idea is to employ a novel adaptive search strategy that drives the evolution towards promptly verifiable approximate circuits. As demonstrated in an extensive evaluation including several structurally different arithmetic circuits and target precisions, the search strategy provides superior scalability and versatility with respect to various approximation scenarios. Our approach significantly improves capabilities of the existing methods and paves a way towards an automated design process of provably-correct circuit approximations.},
  archive      = {J_ASOC},
  author       = {Milan Češka and Jiří Matyáš and Vojtech Mrazek and Lukas Sekanina and Zdenek Vasicek and Tomáš Vojnar},
  doi          = {10.1016/j.asoc.2020.106466},
  journal      = {Applied Soft Computing},
  pages        = {106466},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Adaptive verifiability-driven strategy for evolutionary approximation of arithmetic circuits},
  volume       = {95},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Pseudoinverse learning of fuzzy cognitive maps for
multivariate time series forecasting. <em>ASOC</em>, <em>95</em>,
106461. (<a href="https://doi.org/10.1016/j.asoc.2020.106461">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Forecasting multivariate time series is an important problem considered in many real-world scenarios. To deal with that problem, several forecasting models have already been proposed, where Fuzzy Cognitive Maps (FCMs) are proved to be a suitable alternative. The key limitation of the existing FCM-based forecasting models is the lack of time-efficient learning algorithms. In this paper, we plug that gap by proposing a new FCM learning algorithm which is based on Moore–Penrose inverse. Moreover, we propose an innovative approach that equips FCM with long-term, multistep prediction capabilities. A huge advantage of our method is the lack of parameters which in the case of competitive approaches require laborious adjustment or tuning. The other added value of our method is the reduction of the processing time required to train FCM. The performed experiments revealed that FCM trained using our method outperforms the best FCM-based forecasting model reported in the literature.},
  archive      = {J_ASOC},
  author       = {Frank Vanhoenshoven and Gonzalo Nápoles and Wojciech Froelich and Jose L. Salmeron and Koen Vanhoof},
  doi          = {10.1016/j.asoc.2020.106461},
  journal      = {Applied Soft Computing},
  pages        = {106461},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Pseudoinverse learning of fuzzy cognitive maps for multivariate time series forecasting},
  volume       = {95},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A novel solution approach for multiobjective linguistic
optimization problems based on the 2-tuple fuzzy linguistic
representation model. <em>ASOC</em>, <em>95</em>, 106395. (<a
href="https://doi.org/10.1016/j.asoc.2020.106395">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes a novel solution technique for the multi-objective linguistic optimization problems (MOLOPs) based on the 2-tuple fuzzy linguistic approach. The proposed approach has two main advantages. First, it can handle the MOLOPs in which the linguistic information are represented through either monotonic or non-monotonic functions. Second, for both the scenarios, it provides unique solutions in the linguistic form . On the other hand, the existing MOLOP solution approach which is based on the Tsukamoto’s inference method, provides unique solutions only for those MOLOPs in which the linguistic information are expressed as monotonic functions. For the MOLOPs, in which the linguistic information are expressed as non-monotonic functions, the Tsukamoto’s inference method based solution approach cannot provide unique solutions. Moreover, for both monotonic and non-monotonic cases, the Tsukamoto’s inference method based solution approach cannot provide linguistic solutions, but gives only numeric solutions. We have demonstrated the applicability of the proposed MOLOP solution approach considering a case study on student’s performance evaluation, and compared the results with the Tsukamoto’s inference method based solution approach. It is observed that the proposed approach is capable of addressing the limitations of the Tsukamoto’s inference method and hence is more suitable in solving MOLOPs.},
  archive      = {J_ASOC},
  author       = {Pranab K. Muhuri and Prashant K. Gupta},
  doi          = {10.1016/j.asoc.2020.106395},
  journal      = {Applied Soft Computing},
  pages        = {106395},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A novel solution approach for multiobjective linguistic optimization problems based on the 2-tuple fuzzy linguistic representation model},
  volume       = {95},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). An input shaping based active vibration control and adaptive
RBF impedance control for suppressing the myospasm in upper-limb
rehabilitation. <em>ASOC</em>, <em>95</em>, 106380. (<a
href="https://doi.org/10.1016/j.asoc.2020.106380">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {During the upper limb rehabilitation training exercise, the stroke patients always suffers from the myospasm. The myospasm may last from several seconds to tens of seconds, causing the vibration of the flexible rehabilitation manipulators as well as exerting a sudden force on the end of the rehabilitation manipulator. It is harmful to both of patients and the upper limb rehabilitation manipulator. In this paper, an input shaping based active vibration control is proposed to suppress the vibration in order to guarantee the stability of the flexible upper limb rehabilitation system. In the meantime, an adaptive RBF impedance control is applied to compensate the sudden force caused by the myospasm so as to ensure the safety. The proposed method is analyzed by Lyapunov stability . Numerical simulations are performed and results show the effectiveness of the proposed method.},
  archive      = {J_ASOC},
  author       = {Ting Wang and Tingting Zhang and Aiguo Song and Yuan Zhang},
  doi          = {10.1016/j.asoc.2020.106380},
  journal      = {Applied Soft Computing},
  pages        = {106380},
  shortjournal = {Appl. Soft. Comput.},
  title        = {An input shaping based active vibration control and adaptive RBF impedance control for suppressing the myospasm in upper-limb rehabilitation},
  volume       = {95},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A competitive chain-based harris hawks optimizer for global
optimization and multi-level image thresholding problems. <em>ASOC</em>,
<em>95</em>, 106347. (<a
href="https://doi.org/10.1016/j.asoc.2020.106347">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents an enhanced Harris Hawks Optimizer (HHO) to tackle global optimization and determine the optimal threshold values for multi-level image segmentation problems. HHO is a new swarm-based metaheuristic technique that simulates the behaviors of Harris hawks during the process of catching the rabbits. The HHO established its strong performance as a swarm-based optimization technique. However, population-based HHO still may face some limitations in dealing with more multi-modal and composition problems . For example, this optimizer may be stagnated to local optima and turned to immature convergence when performing phases of exploration and exploitation. To mitigate these drawbacks, an improved HHO is proposed that considers the salp swarm algorithm (SSA) as a competitive method to enhance the balance between its exploration and exploitation trends. Firstly, a set of solutions is generated. Then, we divide those solutions into two halves, where the exploratory and exploitative phases of HHO will be applied to the first half, and the searching stages of SSA will be used to update the solutions in the second half. Thereafter, the best solutions from the union sub-populations are selected to continue the iterative process. According to the improved HHO, which is called HHOSSA, an effective multi-level image segmentation approach is also developed in this research. A comprehensive set of experiments are performed using 36 IEEE CEC 2005 benchmark functions and 11 natural gray-scale images. Extensive results and comparisons show the high ability of the SSA to improve the HHO’s performance since the proposed HHOSSA achieves a more stable performance compared to HHO, SSA, and many other well-known methods.},
  archive      = {J_ASOC},
  author       = {Mohamed Abd Elaziz and Ali Asghar Heidari and Hamido Fujita and Hossein Moayedi},
  doi          = {10.1016/j.asoc.2020.106347},
  journal      = {Applied Soft Computing},
  pages        = {106347},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A competitive chain-based harris hawks optimizer for global optimization and multi-level image thresholding problems},
  volume       = {95},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Particle swarm optimization-based strategy for detecting
border-collision bifurcation points in piecewise smooth maps.
<em>ASOC</em>, <em>95</em>, 106319. (<a
href="https://doi.org/10.1016/j.asoc.2020.106319">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A border-collision bifurcation point detection strategy based on particle swarm optimization (PSO) is proposed in this paper, and it facilitates a bifurcation point detection in piecewise smooth maps. This method is tested via detection experiments of border-collision bifurcation points in two popular piecewise smooth maps with one or more borders. The algorithm design and analysis part shows that the proposed algorithm is fairly simple, easily understandable, and easily implementable. The simulation results show that it accurately detects the border-collision parameters of the target period regardless of its stability, but requires no careful initialization or gradient information of the system.},
  archive      = {J_ASOC},
  author       = {H. Matsushita and W. Kinoshita and H. Kurokawa and T. Kousaka},
  doi          = {10.1016/j.asoc.2020.106319},
  journal      = {Applied Soft Computing},
  pages        = {106319},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Particle swarm optimization-based strategy for detecting border-collision bifurcation points in piecewise smooth maps},
  volume       = {95},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A hybrid grey wolf optimization and particle swarm
optimization with c4.5 approach for prediction of rheumatoid arthritis.
<em>ASOC</em>, <em>94</em>, 106500. (<a
href="https://doi.org/10.1016/j.asoc.2020.106500">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Rheumatoid Arthritis (RA) is a type of dreadful autoimmune disease that affects the entire human body, especially joints . Early diagnosis of RA is a challenging task for General Physicians, since the actual triggering mechanism is unpredictable. The capability of C4.5 was explored using the hybridization of Grey Wolf Optimization (GWO) - Particle Swarm Optimization (PSO) to develop an effective RA prediction system. In this work, firstly, PSO was applied for selecting the diversified initial positions. Secondly GWO was used to update the current positions of the population from the search space to get the optimal features for better classification. Subsequently, the selected features were given as an input to the C4.5 approach and developed a final RA predictor model. The proposed HGWO-C4.5 was meticulously examined based on real time patient’s data, which included factors that influence RA prediction by utilizing both RA and Non-RA information. To validate the proposed HGWO-C4.5, with other meta-heuristics based methods including GWO based C4.5, PSO based C4.5 and individual C4.5 method. The Cross-validation results show that HGWO-C4.5 has achieved an overall average accuracy of 86.36\% from three other approaches, which was ∼ ∼ 6\%–14\% higher than those attainable using the individual predictors. Furthermore, HGWO-C4.5 has achieved an overall average accuracy of 84\% on independent datasets evaluation, which was 8.61\% higher than those yielded by the state-of-the-art predictors. This is the first predictor model that includes feature selection and classification for RA prediction to the best of our knowledge.},
  archive      = {J_ASOC},
  author       = {Shanmugam Sundaramurthy ( Assistant Professor ) and Preethi Jayavel ( Assistant Professor )},
  doi          = {10.1016/j.asoc.2020.106500},
  journal      = {Applied Soft Computing},
  pages        = {106500},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A hybrid grey wolf optimization and particle swarm optimization with c4.5 approach for prediction of rheumatoid arthritis},
  volume       = {94},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A fast self-attention cascaded network for object detection
in large scene remote sensing images. <em>ASOC</em>, <em>94</em>,
106495. (<a href="https://doi.org/10.1016/j.asoc.2020.106495">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Aiming at the real-time detection of multiple objects and micro-objects in large-scene remote sensing images , a cascaded convolutional neural network real-time object-detection framework for remote sensing images is proposed, which integrates visual perception and convolutional memory network reasoning. The detection framework is composed of two fully convolutional networks , namely, the strengthened object self-attention pre-screening fully convolutional network (SOSA-FCN) and the object accurate detection fully convolutional network (AD-FCN). SOSA-FCN introduces a self-attention module to extract attention feature maps and constructs a deep feature pyramid to optimize the attention feature maps by combining convolutional long-term and short-term memory networks. It guides the acquisition of potential sub-regions of the object in the scene, reduces the computational complexity , and enhances the network’s ability to extract multi-scale object features. It adapts to the complex background and small object characteristics of a large-scene remote sensing image. In AD-FCN, the object mask and object orientation estimation layer are designed to achieve fine positioning of candidate frames. The performance of the proposed algorithm is compared with that of other advanced methods on NWPU_VHR-10, DOTA, UCAS-AOD, and other open datasets. The experimental results show that the proposed algorithm significantly improves the efficiency of object detection while ensuring detection accuracy and has high adaptability. It has extensive engineering application prospects.},
  archive      = {J_ASOC},
  author       = {Xia Hua and Xinqing Wang and Ting Rui and Haitao Zhang and Dong Wang},
  doi          = {10.1016/j.asoc.2020.106495},
  journal      = {Applied Soft Computing},
  pages        = {106495},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A fast self-attention cascaded network for object detection in large scene remote sensing images},
  volume       = {94},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Supervised machine learning based gait classification system
for early detection and stage classification of parkinson’s disease.
<em>ASOC</em>, <em>94</em>, 106494. (<a
href="https://doi.org/10.1016/j.asoc.2020.106494">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {While diagnosing Parkinson’s disease (PD), neurologists often use several clinical manifestations of the subject and rate the severity level based on the Unified Parkinson Disease Rating Scale (UPDRS). This kind of rating largely depends on the expertise of the doctors, which is not only subjective but also inefficient. Hence, in this paper, a machine learning based gait classification system which can assist the clinician to diagnose the stages of PD is presented. Gait pattern , which plays a significant role in assessing the human mobility, is a significant biomarker to classify whether the subject is healthy or affected with PD. Hence, we utilize the vertical ground reaction force (VGRF) gait dataset and extract the minimal feature vector using the statistical analysis. Subsequently, the normal distribution of the data is validated using the Shapiro–Wilk test, and from the spatial and temporal features of gait pattern, the salient biomarkers are identified using the correlation based feature selection technique. Four supervised machine learning algorithms namely decision tree (DT), support vector machine (SVM), ensemble classifier (EC) and Bayes classifier (BC) are used for statistical and kinematic analyses which predict the severity of PD. The classifier efficacy quantified using the accuracy, sensitivity and specificity highlights that the proposed framework can effectively rate the severity of PD based on Hohen and Yahr (H&amp;Y) scale. Moreover, comparing the accuracy of the proposed PD classification approach with those of the other state-of-the-art approaches, which utilized the same gait dataset, reveal that the proposed method outperforms several other PD classification methods.},
  archive      = {J_ASOC},
  author       = {Balaji E. and Brindha D. and Balakrishnan R.},
  doi          = {10.1016/j.asoc.2020.106494},
  journal      = {Applied Soft Computing},
  pages        = {106494},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Supervised machine learning based gait classification system for early detection and stage classification of parkinson’s disease},
  volume       = {94},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Feature-based hesitant fuzzy aggregation method for
satisfaction with life scale. <em>ASOC</em>, <em>94</em>, 106493. (<a
href="https://doi.org/10.1016/j.asoc.2020.106493">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Satisfaction with life scale is a comprehensive cognitive judgement of individual’s own life and becomes the dominant measure of life satisfaction. However, the individual may hesitate to assess his/her life satisfaction with a single value due to multiple criteria. Consequently, we propose the hesitant fuzzy satisfaction with life scale (HFSWLS) with the same form as hesitant fuzzy element (HFE), consisting of several possible values of the cognitive judgements to describe the uncertainties and hesitancies. In this paper, we propose a novel aggregation method for large scale HFEs and apply it to HFSWLSs. Primarily, we present the necessary requirements of the extension rules for HFEs to guarantee the reversibility and linearity of the proposed operators. In the proposed aggregation method, we cluster the individuals based on their feature values and further define the hesitant fuzzy feature pair to find out the abnormal HFEs. We generate each cluster’s center HFE by the proposed operators in the first-round aggregation. Based on the orness degree relating to the cluster’s feature value and the decision makers’ preferences to cluster’s feature value, we derive the weights of the center HFEs by O’Hagan’s nonlinear optimization , leading to the final aggregation result taking into account the actual demand. Finally, the validity and effectiveness of the proposed aggregation method for HFEs are testified by the application of HFSWLSs of the citizens.},
  archive      = {J_ASOC},
  author       = {Xiaoyi Mo and Hua Zhao and Zeshui Xu},
  doi          = {10.1016/j.asoc.2020.106493},
  journal      = {Applied Soft Computing},
  pages        = {106493},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Feature-based hesitant fuzzy aggregation method for satisfaction with life scale},
  volume       = {94},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Artificial neural network based software fault detection and
correction prediction models considering testing effort. <em>ASOC</em>,
<em>94</em>, 106491. (<a
href="https://doi.org/10.1016/j.asoc.2020.106491">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Software reliability is an important attribute of software quality . To achieve higher reliability, software development must include a testing phase in which faults can be detected and corrected. The software reliability growth model (SRGM) has evolved from modeling merely the fault detection process (FDP) into incorporating the fault correction process (FCP) as well. However, restricted by mathematical tractability, it is difficult to incorporate into analytical models with more complicated factors, such as the dependency between faults and the influence of staffing levels. This limits the application of analytical models. Therefore, it is promising to adopt data-driven methods such as the artificial neural network (ANN) to model the FDP and the FCP as no specific assumptions are needed. In this study, a stepwise prediction model is proposed to model the FDP and the FCP based on the ANN. Testing effort is considered in our model since it has a great influence on fault detection and correction process. Using real data, the performance of different types of neural networks are compared with the analytical model. The empirical study has confirmed the effectiveness of the proposed models. Further, the optimal policy of the software release time is also presented to illustrate the applications.},
  archive      = {J_ASOC},
  author       = {Hui Xiao and Minhao Cao and Rui Peng},
  doi          = {10.1016/j.asoc.2020.106491},
  journal      = {Applied Soft Computing},
  pages        = {106491},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Artificial neural network based software fault detection and correction prediction models considering testing effort},
  volume       = {94},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Neuro-genetic programming for multigenre classification of
music content. <em>ASOC</em>, <em>94</em>, 106488. (<a
href="https://doi.org/10.1016/j.asoc.2020.106488">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A machine learning approach based on hybridization of genetic programming and neural networks is used to derive mathematical models for music genre classification. We design three multi-label classifiers with different trade-offs between complexity and accuracy, which are able to identify the degree of belonging of music content to ten different music genres. Our approach is innovative as it entirely relies on simple analytical functions and a reduced number of features. Resulting classifiers have an extremely low computational complexity and are suitable to be easily integrated in low-cost embedded systems for real-time applications. The GTZAN dataset is used for model training and to evaluate the accuracy of the proposed classifiers. Despite of the reduced number of features used in our approach, the accuracy of our models is found to be similar to that of more complex music genre classification tools previously published in the literature.},
  archive      = {J_ASOC},
  author       = {G. Campobello and D. Dell’Aquila and M. Russo and A. Segreto},
  doi          = {10.1016/j.asoc.2020.106488},
  journal      = {Applied Soft Computing},
  pages        = {106488},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Neuro-genetic programming for multigenre classification of music content},
  volume       = {94},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A multi-response optimal design of bistable compliant
mechanism using efficient approach of desirability, fuzzy logic, ANFIS
and LAPO algorithm. <em>ASOC</em>, <em>94</em>, 106486. (<a
href="https://doi.org/10.1016/j.asoc.2020.106486">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Compliant mechanisms are promising candidates in precision engineering , soft robotics, space, and bioengineering due to their advantages of free friction, free lubricant, no backlash , monolithic structure, and minimal assembly. However, designing and analyzing of compliant mechanisms are facing the high complexity due to a coupling of kinematic and mechanical behavior in comparison to rigid-body mechanisms. Especially, considering a multi-objective optimization design for compliant mechanisms, the problem is more complicated. Thus, this paper presents a new efficient hybrid methodology for solving the multi-objective optimization design. A hybridization is developed through a combination of finite element method , statistical technique, desirability function approach, fuzzy logic system , adaptive neuro-fuzzy inference system (ANFIS), and Lightning attachment procedure optimization (LAPO). A bistable compliant mechanism is investigated as an application example of the proposed method. First, design variables of the mechanism are determined, and then central composite design is employed to construct a numerically experimental matrix. Though using analysis of variance and Taguchi approach, the design variables are refined to make new populations. Subsequently, desirability values of two performances of the mechanism are computed, and the results are transferred into the fuzzy logic system . The output of fuzzy logic system is considered as single combined objective function. By developing the ANFIS model, the relation between the refined design variables and the output of fuzzy logic system is established. Finally, LAPO algorithm is adopted for solving the multi-objective optimization problem for the mechanism. Three numerical examples are investigated to validate the performance efficiency of the proposed method. The results demonstrate that the proposed method is more efficient than Taguchi-based fuzzy logic. Besides, through Wilcoxon signed rank test and Friedman test, it reveals that the performances of proposed approach are superior to those of the Jaya algorithm and TLBO algorithm. The results of this article can be extended for other complex compliant mechanisms as well as optimization problems with multiple objective functions and more complex constraints.},
  archive      = {J_ASOC},
  author       = {Ngoc Le Chau and Ngoc Thoai Tran and Thanh-Phong Dao},
  doi          = {10.1016/j.asoc.2020.106486},
  journal      = {Applied Soft Computing},
  pages        = {106486},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A multi-response optimal design of bistable compliant mechanism using efficient approach of desirability, fuzzy logic, ANFIS and LAPO algorithm},
  volume       = {94},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Modeling operation problem of active distribution networks
with retailers and microgrids: A multi-objective bi-level approach.
<em>ASOC</em>, <em>94</em>, 106484. (<a
href="https://doi.org/10.1016/j.asoc.2020.106484">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Implementation of distributed energy resources (DERs) has led to a decrement in the cost of supplying demand in distribution networks . Integration of DERs in the forms of micro-grids (MGs) is a solution to enhance the operation of these resources in the low voltage networks. To meet the demand by MG operator, both technical and economic characteristics as well as the prices offered by retailers are considered to schedule DERs optimally. In these networks, the profit of retailers is maximized by power trading with MGs and optimally purchasing the energy from wholesale markets. Due to the existence of several retailers and MGs in active distribution networks (ADNs), hierarchical decision-making frameworks are needed to model their operation problem. For this purpose, a bi-level optimization technique is proposed in this paper to model the operation problem of retailers and MGs as decision-making variables in distribution networks in the upper and lower levels, respectively. To solve the proposed model, multi-objective particle swarm optimization (MOPSO) algorithm is used. The proposed model and its solution method are applied to a hypothetical distribution network with several retailers and MGs to validate the theories and discussions. Numerical results show that the maximum capacity of DG and the amount of demand have an important effect on this decision and the prices of purchased power from wholesale markets determine the amount of retailers’ offers to MGs.},
  archive      = {J_ASOC},
  author       = {Hadi Fateh and Salah Bahramara and Amin Safari},
  doi          = {10.1016/j.asoc.2020.106484},
  journal      = {Applied Soft Computing},
  pages        = {106484},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Modeling operation problem of active distribution networks with retailers and microgrids: A multi-objective bi-level approach},
  volume       = {94},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Identifying expertise through semantic modeling: A modified
BBPSO algorithm for the reviewer assignment problem. <em>ASOC</em>,
<em>94</em>, 106483. (<a
href="https://doi.org/10.1016/j.asoc.2020.106483">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Reviewers play a significant role in academic peer review activities, including conference paper assignment and funding selection, because their evaluation of proposals impacts the final decision. Several studies have proposed reviewer selection strategies or reviewer evaluation methods for solving the problem of selecting appropriate reviewers. Identifying reviewers who are familiar with the proposals to be reviewed is the objective of the reviewer assignment problem. However, the majority of the existing studies ignore quantitative constraints with respect to the articles assigned to the reviewers during the review process. In this study, we propose a novel optimization model with several review condition constraints to address the reviewer assignment problem. In the proposed model, the expertise and research areas of the candidate reviewers and proposals are identified using semantic topic models, which are demonstrated to be effective when measuring the relevance of the reviewers with respect to the proposals to be reviewed; further, the computational efficiency is improved owing to the reduced representation dimensionality. Herein, an improved heuristic algorithm is proposed to match reviewers and papers based on specific topic areas, and candidate reviewers are assigned to each proposal under the global optimum condition based on their overall performance values. Subsequently, an empirical test is conducted using a conference reviewer dataset. The obtained results show that the proposed model can help the managers to efficiently and effectively select reviewers in terms of the convergence rate and convergence level when compared with several classic benchmarks.},
  archive      = {J_ASOC},
  author       = {Chen Yang and Tingting Liu and Wenjie Yi and Xiaohong Chen and Ben Niu},
  doi          = {10.1016/j.asoc.2020.106483},
  journal      = {Applied Soft Computing},
  pages        = {106483},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Identifying expertise through semantic modeling: A modified BBPSO algorithm for the reviewer assignment problem},
  volume       = {94},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A hybrid genetic and lagrangian relaxation algorithm for
resource-constrained project scheduling under nonrenewable resources.
<em>ASOC</em>, <em>94</em>, 106482. (<a
href="https://doi.org/10.1016/j.asoc.2020.106482">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Scheduling under nonrenewable resources is one of the challenging issues in project scheduling problems. There are many cases where the projects are subject to some nonrenewable resources. In most of the literature, nonrenewable resources are assumed to be available in full amount at the beginning of the project. However, in practice, it is prevalent that these resources are procured along the project horizon. This paper studies the generalized resource-constrained project scheduling problem (RCPSP) where, in addition to renewable resources , nonrenewable resources are considered, such as budget or consuming materials by the project activities. As the problem is NP-hard, some sub-algorithm elements are developed, which can be used in the structure of inexact approaches for solving the problem. These elements include constraint propagation , priority rules, schedule generation schemes, and local search improvement procedures. Also, a lower bounding algorithm is developed based on the Lagrangian Relaxation (LR) approach, and the problem is optimized by the Genetic Algorithm (GA). The hybrid GA–LR​ algorithm produces a result reasonably near to optimum solutions. Comprehensive computational experiments based on standard project scheduling problems are performed to evaluate these developments. The experiments showed the performance and robustness of the proposed algorithm.},
  archive      = {J_ASOC},
  author       = {Ali Shirzadeh Chaleshtarti and Shahram Shadrokh and Marzieh Khakifirooz and Mahdi Fathi and Panos M. Pardalos},
  doi          = {10.1016/j.asoc.2020.106482},
  journal      = {Applied Soft Computing},
  pages        = {106482},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A hybrid genetic and lagrangian relaxation algorithm for resource-constrained project scheduling under nonrenewable resources},
  volume       = {94},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Grey wolf production scheduling for the capital goods
industry. <em>ASOC</em>, <em>94</em>, 106480. (<a
href="https://doi.org/10.1016/j.asoc.2020.106480">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The capital goods industry produces physical assets used for current and future production. Capital goods are highly customised. Production scheduling aims to synchronise material supply, component manufacturing, sub-assembly and final assembly processes to minimise the total costs of earliness and tardiness, whilst satisfying finite capacity, machining and assembly precedence constraints. This paper presents the first application of Grey Wolf Optimisation (GWO) together with modified and hybridised versions for solving the capital goods scheduling problem. A novel GWO-based production scheduling tool was developed and validated using four realistic case studies obtained from a collaborating company. The first experiment identified appropriate parameter settings for the GWO. The performance of the GWO was then evaluated and compared with a modified GWO and a hybridised GWO. The computational results obtained from the proposed methods were statistical analysed. The outperformed other metaheuristics .},
  archive      = {J_ASOC},
  author       = {Saisumpan Sooncharoen and Pupong Pongcharoen and Christian Hicks},
  doi          = {10.1016/j.asoc.2020.106480},
  journal      = {Applied Soft Computing},
  pages        = {106480},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Grey wolf production scheduling for the capital goods industry},
  volume       = {94},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A rough–fuzzy approach integrating best–worst method and
data envelopment analysis to multi-criteria selection of smart product
service module. <em>ASOC</em>, <em>94</em>, 106479. (<a
href="https://doi.org/10.1016/j.asoc.2020.106479">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The revolutionary development and implementation of smart technologies have triggered the manufacturers’ servitization trend towards smart product service system (PSS). Accurate selection of smart product service (SPS) module is critical to successful planning and development of smart PSS concept. This study constructs a list of criteria for SPS module selection from the perspectives of service implementation, value symbiosis and smart capability. The selection can be deemed as a multi-criteria decision-making process including two parts: weight determination of criteria and module ranking, in which the intrapersonal linguistic ambiguousness and interpersonal preference randomness are involved. The best–worst method (BWM) is widely acknowledged as an efficient method for weight determination due to its superiority in quickly finding optimal weight with scant decision data. The data envelopment analysis (DEA) method is proven feasible to prioritize alternatives with cost-based and benefit-based criteria. However, these two methods cannot handle the uncertainties involved in the selection process which may lead to imprecise results. Moreover, the previous research rarely studies simultaneous handling of these two types of uncertainty in the realm of BWM and DEA. Therefore, the current study proposes a novel rough–fuzzy BWM-DEA approach to SPS module selection, with fully capturing both the intrapersonal and interpersonal uncertainties. The application of the proposed approach in the smart vehicle service module selection and the comparisons with other methods demonstrate the validity and effectiveness of the proposed approach.},
  archive      = {J_ASOC},
  author       = {Zhihua Chen and Xinguo Ming},
  doi          = {10.1016/j.asoc.2020.106479},
  journal      = {Applied Soft Computing},
  pages        = {106479},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A rough–fuzzy approach integrating best–worst method and data envelopment analysis to multi-criteria selection of smart product service module},
  volume       = {94},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). The two-echelon multi-objective location routing problem
inspired by realistic waste collection applications: The composable
model and a metaheuristic algorithm. <em>ASOC</em>, <em>94</em>, 106477.
(<a href="https://doi.org/10.1016/j.asoc.2020.106477">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Waste collection has always been a major research area in waste management. It plays an important role in social development and environmental sustainability. However, the past research often makes great efforts to formulate dedicated models to some specific waste collection applications, and relatively speaking, fewer efforts have been devoted to relevant method development. Inspired by these issues, in this work, we first develop a more general two-echelon multi-objective location routing problem model (2E-MOLRP) in consideration of the inherent similarities of many realistic waste collection applications. In the model, various commonly-seen and potential costs are classified in a straightforward way and different objectives can hence be flexibly defined to satisfy different requirements. Furthermore, to solve the model, an improved non-dominated sorting genetic algorithm with directed local search (INSGA-dLS) is proposed. In order to validate its effectiveness, experiments are conducted in comparison with existing representative metaheuristics and the results show that our proposed algorithm can achieve better performance even without using local search. Also, we prove that the specially-designed directed locate search is able to further improve our algorithm’s performance significantly in experiments.},
  archive      = {J_ASOC},
  author       = {Xue Yu and Yuren Zhou and Xiao-Fang Liu},
  doi          = {10.1016/j.asoc.2020.106477},
  journal      = {Applied Soft Computing},
  pages        = {106477},
  shortjournal = {Appl. Soft. Comput.},
  title        = {The two-echelon multi-objective location routing problem inspired by realistic waste collection applications: The composable model and a metaheuristic algorithm},
  volume       = {94},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Modeling and performance evaluation of wind turbine based on
ant colony optimization-extreme learning machine. <em>ASOC</em>,
<em>94</em>, 106476. (<a
href="https://doi.org/10.1016/j.asoc.2020.106476">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, an innovative hybrid multi-variable generator’s actual-output-power predicting model is proposed based on ant colony optimization algorithm and extreme learning machine network, and a data-driven performance evaluation model is presented based on the two indices, K-means clustering algorithm and Markov chain for the performance evaluation of the wind turbines . Ant colony optimization algorithm is used to optimize the initial weights and thresholds of the extreme learning machine network, then the optimized combinations of weights and thresholds are provided into the extreme learning machine models to overcome the sensitivity problem of initialization setting and the disadvantage of easily falling into local optimum. Through the actual-output-power prediction of the WTs in a wind farm, the results show that the proposed model has more higher prediction accuracy than other methods mentioned in this paper. The optimization process also shows that the prediction accuracy is sensitive to the number of hidden-layer nodes and is relatively insensitive to other model parameters. Then, the data-driven performance evaluation models are proposed based on the error sequences obtained above. The case study is conducted and the results show that the method can evaluate the operating performance of the wind turbines correctly. The effectiveness of the evaluation results is also verified by the actual operation results.},
  archive      = {J_ASOC},
  author       = {Xiaoqiang Wen},
  doi          = {10.1016/j.asoc.2020.106476},
  journal      = {Applied Soft Computing},
  pages        = {106476},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Modeling and performance evaluation of wind turbine based on ant colony optimization-extreme learning machine},
  volume       = {94},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Forecasting the monthly iron ore import of china using a
model combining empirical mode decomposition, non-linear autoregressive
neural network, and autoregressive integrated moving average.
<em>ASOC</em>, <em>94</em>, 106475. (<a
href="https://doi.org/10.1016/j.asoc.2020.106475">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Considering that the non-linear path of monthly time-series for the iron ore imported to China is under reciprocal influences of multiple factors, the process of data generation is not easily represented in a time-series model. Based on the decomposition–integration method, superiorities of empirical mode decomposition (EMD), non-linear autoregressive neural network (NARNN), and autoregressive integrated moving average (ARIMA) models are integrated to establish a combined model EMD-NARNN-ARIMA. The empirical results show that, compared with the NARNN or seasonal autoregressive integrated moving average (SARIMA) models, the proposed model is more suitable for predicting data pertaining to the import of iron ore to China. The prediction error of EMD-NARNN-ARIMA is significantly lower than that of NAR and SARIMA, and, more importantly, it does not increase the time-complexity. The predicted result attained through the proposed model reveals that the import of iron ore to China from January 2019 to December 2020 will gradually decrease, accompanied by reasonable seasonal fluctuations, which is consistent with the decreasing trend in the demand for iron and steel as a result of the adjustment of China’s current industrial structure.},
  archive      = {J_ASOC},
  author       = {Zheng-Xin Wang and Yu-Feng Zhao and Ling-Yang He},
  doi          = {10.1016/j.asoc.2020.106475},
  journal      = {Applied Soft Computing},
  pages        = {106475},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Forecasting the monthly iron ore import of china using a model combining empirical mode decomposition, non-linear autoregressive neural network, and autoregressive integrated moving average},
  volume       = {94},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Similarity-based particle filter for remaining useful life
prediction with enhanced performance. <em>ASOC</em>, <em>94</em>,
106474. (<a href="https://doi.org/10.1016/j.asoc.2020.106474">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes a similarity-based Particle Filter (PF) method for Remaining Useful Life (RUL) prediction with improved performance. In the proposed methodology, Maximum Mean Discrepancy (MMD) and Kernel Two Sample Test are firstly adopted to query similar Run-To-Failure (R2F) profiles from historical data library. The states and parameters of degradation are initialized based on the similar R2F profiles. Next, Rao–Blackwellized​ Particle Filter (RBPF) is employed to update the degradation states based on the initialization. The RUL prediction results are obtained by extrapolating the degradation states updated by RBPF. The proposed RUL prediction method holds several advantages: (1) compared with other PF methods, the proposed model includes historical knowledge from similar R2F profiles; (2) compared with similarity-based methods, the proposed model presents good probabilistic interpretation of prediction uncertainties based on RUL distribution. The effectiveness and superiority over other peer algorithms are justified based on a public aero-engine dataset for prognostics.},
  archive      = {J_ASOC},
  author       = {Haoshu Cai and Jianshe Feng and Wenzhe Li and Yuan-Ming Hsu and Jay Lee},
  doi          = {10.1016/j.asoc.2020.106474},
  journal      = {Applied Soft Computing},
  pages        = {106474},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Similarity-based particle filter for remaining useful life prediction with enhanced performance},
  volume       = {94},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020b). A new asymmetric ϵ-insensitive pinball loss function based
support vector quantile regression model. <em>ASOC</em>, <em>94</em>,
106473. (<a href="https://doi.org/10.1016/j.asoc.2020.106473">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose a novel asymmetric ϵ ϵ -insensitive pinball loss function for quantile estimation . There exists some pinball loss functions which attempt to incorporate the ϵ ϵ -insensitive zone approach in it but, they fail to extend the ϵ ϵ -insensitive approach for quantile estimation in true sense. The proposed asymmetric ϵ ϵ -insensitive pinball loss function can make an asymmetric ϵ ϵ - insensitive zone of fixed width around the data and divide it using τ τ value for the estimation of the τ τ th quantile . The use of the proposed asymmetric ϵ ϵ -insensitive pinball loss function in Support Vector Quantile Regression (SVQR) model improves its prediction ability significantly. It also brings the sparsity back in SVQR model. Further, the numerical results obtained by several experiments carried on simulated and real world datasets empirically show the efficacy of the proposed ‘ ϵ ϵ -Support Vector Quantile Regression’ ( ϵ ϵ -SVQR) model over other existing SVQR models.},
  archive      = {J_ASOC},
  author       = {Pritam Anand and Reshma Rastogi nee Khemchandani and Suresh Chandra},
  doi          = {10.1016/j.asoc.2020.106473},
  journal      = {Applied Soft Computing},
  pages        = {106473},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A new asymmetric ϵ-insensitive pinball loss function based support vector quantile regression model},
  volume       = {94},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Joint resource allocation algorithm based on multi-objective
optimization for wireless sensor networks. <em>ASOC</em>, <em>94</em>,
106470. (<a href="https://doi.org/10.1016/j.asoc.2020.106470">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the limitations of the network resources and battery energy of wireless sensors , the competition of resources in the process of communication will increase the network energy consumption and reduce the Quality of Service (QoS), resulting in that the application of Multi-Radio Multi-Channel (MRMC) Wireless Sensor Networks (WSNs) face many challenges. In this paper, we concentrate on the resource allocation of joint time slot assignment, channel allocation and power control for MRMC WSNs. Due to the diversity of research objectives and the computational complexity of the non-convex problem, this paper develops a two-stage resource allocation optimization algorithm by analyzing the interdependence of various resources. Specifically, to exchange information with conflict-free transmission among all sensors, a graph coloring algorithm for time slot assignment is designed firstly. Then based on the first stage of this algorithm, the problem of joint power control and channel allocation is studied and formulated as a multi-objective optimization problem to achieve the trade-off between energy efficiency and network capacity maximization under the constraints of link interference and load balance. Multi-objective hybrid particle swarm optimization is introduced to obtain the Pareto optimal solutions . The simulation results show that the proposed algorithm significantly performs better in terms of achieving the trade-off of multi-performance.},
  archive      = {J_ASOC},
  author       = {Xiaochen Hao and Ning Yao and Liyuan Wang and Jiaojiao Wang},
  doi          = {10.1016/j.asoc.2020.106470},
  journal      = {Applied Soft Computing},
  pages        = {106470},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Joint resource allocation algorithm based on multi-objective optimization for wireless sensor networks},
  volume       = {94},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Capability-based distributed layout formation with or
without demand and process flow information. <em>ASOC</em>, <em>94</em>,
106469. (<a href="https://doi.org/10.1016/j.asoc.2020.106469">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, a binary integer programming model of an unbiased capability-based distributed layout (UBCB-DL) problem without demand and process flow information is first developed. Then, it is extended to a mixed-integer program for a biased capability-based distributed layout (BCB-DL) problem where the demand information and processing requirements of several parts are taken into account. Since the complex nature of the problems, a recently proposed new generation metaheuristic optimizer namely, weighted superposition attraction (WSA) algorithm is also applied. In order to show validity and practicality of the proposed WSA algorithm and compare its performance with the proposed mathematical programs , a real-life case study is presented. The computational experiments have shown that both of the proposed binary integer program and WSA algorithm are able to find alternative optimal solutions for the UBCB-DL problem under reasonable computation times. However, just a feasible solution with 5.93\% optimality gap is found by the proposed mixed-integer program for the BCB-DL problem under 24-hour running time limit. Fortunately, its optimal solution is achieved by the proposed WSA algorithm. Consequently, the proposed WSA algorithm provided the most effective solutions for both UBCB-DL and BCB-DL problems under shortest computation times.},
  archive      = {J_ASOC},
  author       = {Adil Baykasoğlu and Kemal Subulan},
  doi          = {10.1016/j.asoc.2020.106469},
  journal      = {Applied Soft Computing},
  pages        = {106469},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Capability-based distributed layout formation with or without demand and process flow information},
  volume       = {94},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Total bregman divergence-based fuzzy local information
c-means clustering for robust image segmentation. <em>ASOC</em>,
<em>94</em>, 106468. (<a
href="https://doi.org/10.1016/j.asoc.2020.106468">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The fuzzy local information C-means clustering algorithm (FLICM) is an important robust fuzzy clustering segmentation method , which has attracted considerable attention over the years. However, it lacks certain robustness to high noise or severe outliers. To improve the accuracy and robustness of the FLICM algorithm for images corrupted by high noise, a novel fuzzy local information c-means clustering utilizing total Bregman divergence (TFLICM) is proposed in this paper. The total Bregman divergence is modified by the local neighborhood information of sample to further enhance the ability to suppress noise, and then modified total Bregman divergence is introduced into the FLICM to construct a new objective function of robust fuzzy clustering , and the iterative clustering algorithm with high robustness is obtained through optimization theory . The convergence of the TFLICM algorithm is proved by the Zangwill theorem. In addition, the validity of the TFLICM algorithm applied in noise image segmentation is explained by means of sample weighting fuzzy clustering. Meanwhile, the generalized total Bregman divergence unifies the Bregman divergence with the total Bregman divergence and enhances the universality of the TFLICM algorithm applied in segmenting complex medical and remote sensing images . Some experimental results show that the TFLICM algorithm can obtain better segmentation quality and stronger anti-noise robustness than the existing FLICM algorithm.},
  archive      = {J_ASOC},
  author       = {Chengmao Wu and Xue Zhang},
  doi          = {10.1016/j.asoc.2020.106468},
  journal      = {Applied Soft Computing},
  pages        = {106468},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Total bregman divergence-based fuzzy local information C-means clustering for robust image segmentation},
  volume       = {94},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Bibliometric analysis of rough sets research. <em>ASOC</em>,
<em>94</em>, 106467. (<a
href="https://doi.org/10.1016/j.asoc.2020.106467">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Rough set (RS) is a mathematical framework used to deal with incomplete and uncertain information. It has been widely used in decision analysis, data mining, artificial intelligence , economic management and many other fields. Up to now, there have been tens of thousands of research papers on this topic, and the area has made a rapid growth. In light of these factors, a comprehensive and systematic review of this area becomes essential. The purpose of this study is to present a coherent overview of the theory and applications of the RS, reveal its current research focal points, and identify future development trends. We conduct a thorough bibliometric review and perform co-occurrence and co-citation analysis. First, the fundamental characteristics, productive authors, preferred journals and leading countries in the field of RS are identified. Second, the co-citation and citation burst detection methods are used to explore research hotspot and trends. In light of the undertaken methodology, this study can offer tangible value to scholars in understanding the content structure and development process of the RS field.},
  archive      = {J_ASOC},
  author       = {Dejian Yu and Zeshui Xu and Witold Pedrycz},
  doi          = {10.1016/j.asoc.2020.106467},
  journal      = {Applied Soft Computing},
  pages        = {106467},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Bibliometric analysis of rough sets research},
  volume       = {94},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A neural network enhanced hidden markov model for tourism
demand forecasting. <em>ASOC</em>, <em>94</em>, 106465. (<a
href="https://doi.org/10.1016/j.asoc.2020.106465">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, tourism demand forecasting has attracted more interests not only in tourism area but in data science field. In this study, we follow the previous relevant data science literatures and propose a new neural network enhanced hidden Markovian structural time series model (NehM-STSM). This model takes a multiplicative error structure of a trend and a seasonal element. The trend is modelled by an artificial neural network while the seasonal element is captured by a tailor-made hidden Markovian model with four components: a persistence replicative cycle, a jump component capturing an unexpected event, an amplitude component reflecting the event strength and a random error term. The empirical research is conducted using US incoming tourism data from twelve major source countries across January 1996–September 2017. The proposed NehM-STSM achieves a better performance than the chosen benchmark models for two error measures and most forecasting horizons.},
  archive      = {J_ASOC},
  author       = {Yuan Yao and Yi Cao},
  doi          = {10.1016/j.asoc.2020.106465},
  journal      = {Applied Soft Computing},
  pages        = {106465},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A neural network enhanced hidden markov model for tourism demand forecasting},
  volume       = {94},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Towards the behavior analysis of chemical reactors utilizing
data-driven trend analysis and machine learning techniques.
<em>ASOC</em>, <em>94</em>, 106464. (<a
href="https://doi.org/10.1016/j.asoc.2020.106464">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The concept of modeling the behavior of industrial processes is of great importance as it describes the possible states of equipment used in large industries, which once damaged, it usually costs both in time and money. In this paper, we propose a data-driven methodology for depicting three distinct states of a chemical reactor, (1) normal, (2) warning, (3) alert, by using machine learning techniques . A method for predicting the classification of data input, assists in prevention (early prognosis) of possible malfunctions. This method uses a combined linear trend analysis of the involved data which form the warning state of the reactor where the pre-incident conditions are fulfilled. Afterwards, it checks the possibility of the subsequent input to be classified in the alert state which is an indication that the reactor’s active equipment, such as heating resistance, will start malfunctioning. The objective of the three main steps of the proposed methodology are: first, to reveal the number of clusters based on past data, second to train normal, warning and alert behavior-models and validate them and third to test them as well as verify the accuracy of linear trend analysis. The proposed methodology is based on the analysis of real data sets​ derived from the automation system of a chemical process located at CERTH/CPERI in order to identify real-life models for prognostic behavior for malfunction prevention. This approach is especially suitable for modern industrial systems that follow Industry 4.0 principles. The results reveal a robust modeling of the reactor’s behavior with accuracy reaching 88, 94\%.},
  archive      = {J_ASOC},
  author       = {E. Lithoxoidou and C. Ziogou and T. Vafeiadis and S. Krinidis and D. Ioannidis and S. Voutetakis and D. Tzovaras},
  doi          = {10.1016/j.asoc.2020.106464},
  journal      = {Applied Soft Computing},
  pages        = {106464},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Towards the behavior analysis of chemical reactors utilizing data-driven trend analysis and machine learning techniques},
  volume       = {94},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A combined forecasting system based on modified
multi-objective optimization and sub-model selection strategy for
short-term wind speed. <em>ASOC</em>, <em>94</em>, 106463. (<a
href="https://doi.org/10.1016/j.asoc.2020.106463">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Forecasting models have been widely used in wind-speed time series forecasting that are often nonlinear, irregular, and non-stationary. Current forecasting models based on artificial neural network can adapt to various wind-speed time series. However, they cannot simultaneously and effectively forecast the entire wind-speed time series of a wind farm . In this paper, a novel combined forecasting system is developed for a wind farm that includes that SSAWD secondary de-noising algorithm is used to pre-process original wind speed data, and then the sub-model selection strategy is used to select five optimal sub models for the combined model. Meanwhile, a modified multi-objective optimization algorithm optimizes weight of the combined model, and the experimental results show that this forecasting system outperforms other traditional systems and can be effectively used to forecast wind-speed time series of a large wind farm.},
  archive      = {J_ASOC},
  author       = {Qingguo Zhou and Chen Wang and Gaofeng Zhang},
  doi          = {10.1016/j.asoc.2020.106463},
  journal      = {Applied Soft Computing},
  pages        = {106463},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A combined forecasting system based on modified multi-objective optimization and sub-model selection strategy for short-term wind speed},
  volume       = {94},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Green vehicle routing and scheduling problem with
heterogeneous fleet including reverse logistics in the form of
collecting returned goods. <em>ASOC</em>, <em>94</em>, 106462. (<a
href="https://doi.org/10.1016/j.asoc.2020.106462">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Vehicle routing problem (VRP) is about finding optimal routes for a fixed fleet of vehicles in order that they can meet the demands for a set of given customers by traveling through those paths. This problem and its numerous expansions are one of the most important and most applicable transportation and logistics problems. In this study, the green vehicle routing and scheduling problem with heterogeneous fleet including reverse logistics in the form of collecting returned goods along with weighted earliness and tardiness costs is studied to establish a trade-off between operational and environmental costs and to minimize both simultaneously. In this regard, a mixed integer non-linear programming (MINLP) model is proposed. Since the problem is categorized as NP-hard, two meta-heuristics, a simulated annealing (SA) and a genetic algorithm (GA) are suggested in order to find near-optimal solutions for large instances in a reasonable computational time. The performances of the proposed algorithms are evaluated in comparison with the mathematical model for small-sized problems and with each other for problems of all size using a set of defined test problems. Analysis of the results considering two criteria: solutions quality and computational times, indicates the satisfactory performance of the presented algorithms in a proper computational time. Meanwhile, a statistical hypothesis testing (T-test) is conducted. It can generally be observed that SA achieves relatively better results in terms of solution quality, while GA spends less computational time for all-sized test problems. Eventually, sensitivity analysis is conducted to investigate the effect of collecting returned goods on the cost of total CO 2 2 emissions, variable costs of the fleet and the objective function value.},
  archive      = {J_ASOC},
  author       = {Reza Alizadeh Foroutan and Javad Rezaeian and Iraj Mahdavi},
  doi          = {10.1016/j.asoc.2020.106462},
  journal      = {Applied Soft Computing},
  pages        = {106462},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Green vehicle routing and scheduling problem with heterogeneous fleet including reverse logistics in the form of collecting returned goods},
  volume       = {94},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Multi-frame decision fusion based on evidential association
rule mining for target identification. <em>ASOC</em>, <em>94</em>,
106460. (<a href="https://doi.org/10.1016/j.asoc.2020.106460">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the multi-sensor target identification problem involving multiple frames, it is important to fuse the potential information characterizing inherent relations among frames with uncertain decision inputs for enhancing the decision-making process. However, due to the influence of environments or other interference factors, the priori knowledge that accurately represents these relations is usually hard to obtain. To overcome this difficulty, we propose a rule mining-based multi-frame decision fusion (abbreviated as RMDF) method, in which the unknown relations can be discovered from a series of historical sensor reports in the framework of belief functions. First, to accommodate data uncertainty, new measures of evidential support and confidence are defined for a constructed multi-frame evidential database, which are generalizations of the support and confidence measures in binary and probabilistic databases. Then, with these measures, an evidential association rule mining algorithm is developed to discover the relations among frames from a series of historical reports. Finally, how these mined rules are properly combined with uncertain decision information using belief function theory is explored. The key benefit of the RMDF method is that it enables modeling the uncertain relations among frames for deriving more accurate decision results. To demonstrate the feasibility and effectiveness of our proposal, an airborne target identification problem is studied under different conditions and the numerical results show that the identification performance of our method is significantly better than the traditional expert knowledge-based method where the available knowledge is inevitably incomplete or inaccurate.},
  archive      = {J_ASOC},
  author       = {Xiaojiao Geng and Yan Liang and Lianmeng Jiao},
  doi          = {10.1016/j.asoc.2020.106460},
  journal      = {Applied Soft Computing},
  pages        = {106460},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Multi-frame decision fusion based on evidential association rule mining for target identification},
  volume       = {94},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Fuzzy expert analysis of the severity of mining machinery
failure. <em>ASOC</em>, <em>94</em>, 106459. (<a
href="https://doi.org/10.1016/j.asoc.2020.106459">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mining machinery failure is almost an everyday occurrence. Usually the failures bare certain consequences, which require additional financial costs to repair and restore the system to its operational state. The consequences are viewed through negative and damaging effects a failure has on the machine, health and safety of the employees, work environment, and on the environment. The removal of the consequences of the failure requires additional financial investment, which has a negative impact on the company’s business. In order to prevent this, it is necessary to have Risk Centered Maintenance, where the risk assessment would include all the negative consequences of the risky event. The fuzzy expert assessment is presented in this paper, as well as the failure severity assessment based on the harmful effects of the failure. The negative effects of the machine component failure, such as the time needed for the repair, the possibility of workplace injury caused by the failure, and the impact it has on the environment, are analyzed in this paper as well. This approach to the severity of failure assessment enables a more comprehensive view of this risk indicator. It also enables the risk indicator “severity of failure” to gain greater significance in combination with other risk indicators. The developed model was presented on the example of typical failures of the hydraulic subsystems of a mobile crushing machine Lokotrack LT 1213S.},
  archive      = {J_ASOC},
  author       = {Dejan V. Petrović and Miloš Tanasijević and Saša Stojadinović and Jelena Ivaz and Pavle Stojković},
  doi          = {10.1016/j.asoc.2020.106459},
  journal      = {Applied Soft Computing},
  pages        = {106459},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Fuzzy expert analysis of the severity of mining machinery failure},
  volume       = {94},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A memetic algorithm with novel semi-constructive evolution
operators for permutation flowshop scheduling problem. <em>ASOC</em>,
<em>94</em>, 106458. (<a
href="https://doi.org/10.1016/j.asoc.2020.106458">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes a memetic algorithm (MA) with novel semi-constructive crossover and mutation operators (MASC) to minimize makespan in permutation flowshop scheduling problem (PFSP). MASC combines the strengths of genetic algorithm (GA), simulated annealing (SA), and Nawaz–Enscore–Ham (NEH) algorithm. The aim is to enhance GA in identifying promising areas in the search space, whose local optima will be subsequently located by SA. This is achieved by means of novel crossover and mutation operators that construct chromosomes by using two different types of genes: static and dynamic genes. MASC is tested on the well-known Taillard’s benchmark instances. The proposed operators are compared with traditional operators. The results show that the proposed operators produce considerable improvements. These improvements reach up to 20.79\% in the average relative error of best solution and 11.86\% in the average relative error of average solution. MASC is compared with fourteen well-known and state-of-the-art algorithms. These algorithms include MA, whale optimization, ant colony optimization , particle swarm optimization , artificial bee colony , monkey search, and iterated greedy. The results show that MASC outperforms all the compared algorithms except three iterated greedy algorithms . Moreover, the improvement in the average relative error of best solution achieved on the best-so-far MA is 37.92\%. Therefore, MASC can be considered as one of the best-so-far methods for PFSP.},
  archive      = {J_ASOC},
  author       = {Mohamed Kurdi},
  doi          = {10.1016/j.asoc.2020.106458},
  journal      = {Applied Soft Computing},
  pages        = {106458},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A memetic algorithm with novel semi-constructive evolution operators for permutation flowshop scheduling problem},
  volume       = {94},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A new method for multivariable nonlinear coupling relations
analysis in complex electromechanical system. <em>ASOC</em>,
<em>94</em>, 106457. (<a
href="https://doi.org/10.1016/j.asoc.2020.106457">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Coupling relations analysis of monitoring variables in complex electromechanical system is a powerful and useful means for abnormal detection, false monitoring information identification and fault diagnosis. However, due to the characteristics of multivariable, nonlinear and non-stationarity in the actual production process, it is difficult to achieve multivariable coupling modeling of complex electromechanical systems. In this paper, a new coupling modeling method is proposed by combining causality analysis with RBF neural network. First, considering the multivariate and nonlinearity of complex system, the conditional Granger and nonlinear Granger is combined to analyze the causal relations of process variables and obtain the cause variable set of any variable in complex system. Second, the RBF neural network is applied to achieve the nonlinear fitting and the parameter is optimized to ensure the fitting precision. Finally, the effectiveness of the proposed method is verified by an analysis of one case study of real compressor groups data set in chemical production system. This new approach can handle general coupling modeling problems and obtain a quantitative nonlinear coupling model by determine the dependent and independent variables in system and the functional relationship between them. Which dedicated to studying quantitative functional relationship of variable coupling, not just considering the direction or strength of coupling as in the existing, and does not need any prior knowledge about the physical structure. Thus, the proposed method can be effectively used in coupling modeling of complex electromechanical systems and formulate the foundation of anomaly detection , information quality assessment, and failure propagation mechanism, as well as other engineering applications .},
  archive      = {J_ASOC},
  author       = {Yanjie Liang and Zhiyong Gao and Jianmin Gao and Rongxi Wang and Qianqian Liu and Yahui Cheng},
  doi          = {10.1016/j.asoc.2020.106457},
  journal      = {Applied Soft Computing},
  pages        = {106457},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A new method for multivariable nonlinear coupling relations analysis in complex electromechanical system},
  volume       = {94},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A multi-stage multi-criteria hierarchical decision-making
approach for sustainable supplier selection. <em>ASOC</em>, <em>94</em>,
106456. (<a href="https://doi.org/10.1016/j.asoc.2020.106456">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sustainable supplier selection is known as a crucial objective in supply chains due to its impact on profitability, adorability, flexibility, and agility of the system. This study proposes a new multi-stage hierarchical fuzzy index-based approach with which decision-makers are empowered to select the most sustainable supplier based on sustainability triple bottom line criteria. Besides, a new fuzzy extension for the best-worst method is proposed considering trapezoidal fuzzy membership functions that can cover uncertainty under imprecise environments. This study makes a contribution to the literature of sustainable supply chains in the sense that it facilitates the computational complexity of previous decision-making approaches by collecting the most relevant criteria and a straightforward fuzzy process. Besides, the graded mean integration representation method has been adopted for prioritizing the supplier based on their performance of sustainable development, which enhances the accuracy of selection compared with previous ranking methods. The proposed study can be utilized as a benchmark for sustainability evaluations between suppliers. A real-world case study is resolved to illustrate the superiority and broad application of the proposed model.},
  archive      = {J_ASOC},
  author       = {Sepehr Hendiani and Amin Mahmoudi and Huchang Liao},
  doi          = {10.1016/j.asoc.2020.106456},
  journal      = {Applied Soft Computing},
  pages        = {106456},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A multi-stage multi-criteria hierarchical decision-making approach for sustainable supplier selection},
  volume       = {94},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A fuzzy collaborative forecasting approach considering
experts’ unequal levels of authority. <em>ASOC</em>, <em>94</em>,
106455. (<a href="https://doi.org/10.1016/j.asoc.2020.106455">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Experts typically have unequal authority levels in collaborative forecasting tasks. Most current fuzzy collaborative forecasting methods address this problem by applying a (fuzzy) weighted average to aggregate experts’ fuzzy forecasts. However, the aggregation result may be unreasonable, hence fuzzy weighted intersection operators have been proposed for fuzzy collaborative forecasting. This paper proposes that unequal expert authority levels are considered when deriving the membership function rather than the aggregation value. Therefore, the membership of a value in the aggregation result cannot exceed those in experts’ fuzzy forecasts. The proposed approach was applied to forecast the yield of a dynamic random access memory product to validate its effectiveness. Experimental results showed that the proposed methodology outperformed all current best-practice methods considered in every aspect, and in particular achieving 65\% mean root mean square error reduction. Thus, a high expert authority level increased the likelihood for the forecast, which could not be satisfactorily addressed by simply applying a higher weight to the forecast.},
  archive      = {J_ASOC},
  author       = {Tin-Chih Toly Chen and Yu-Cheng Wang and Chi-Wei Lin},
  doi          = {10.1016/j.asoc.2020.106455},
  journal      = {Applied Soft Computing},
  pages        = {106455},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A fuzzy collaborative forecasting approach considering experts’ unequal levels of authority},
  volume       = {94},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Hypergraph membrane system based f2 fully convolutional
neural network for brain tumor segmentation. <em>ASOC</em>, <em>94</em>,
106454. (<a href="https://doi.org/10.1016/j.asoc.2020.106454">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate segmentation is a necessary step in the clinical management of brain tumors. However, the task remains challenging due to not only large variations in the sizes and shapes of brain tumors but also wide variations among individuals. In this paper, we develop a novel fully convolutional neural network with a feature reuse module and feature conformity module (F 2 2 FCN) to alleviate the above challenges and further improve the accuracy of segmentation. Specifically, to extract more valuable features, we present a feature reuse module to repeatedly utilize features from different layers. We also provide a feature conformity module to eliminate possible noise and enhance the fusion of different feature map levels. However, the difficult selection of multiple parameters and the long training time of a single model make CNNs less effective. To solve these problems, a new distributed and parallel computing model, a hypergraph membrane system, is designed to implement the F 2 2 FCN. In particular, we develop a hypergraph membrane structure with three new kinds of rules to implement several F 2 2 FCNs with different settings simultaneously to leverage the ensemble learning of F 2 2 FCNs and save time. Experimental results on two datasets show promotive performance in terms of the Dice similarity coefficient (DSC), positive predictive value (PPV) and sensitivity compared with the state-of-the-art methods.},
  archive      = {J_ASOC},
  author       = {Jie Xue and Jinyan Hu and Yuan Wang and Deting Kong and Shuo Yan and Rui Zhao and Dengwang Li and Yingchao Liu and Xiyu Liu},
  doi          = {10.1016/j.asoc.2020.106454},
  journal      = {Applied Soft Computing},
  pages        = {106454},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Hypergraph membrane system based f2 fully convolutional neural network for brain tumor segmentation},
  volume       = {94},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A nature-inspired feature selection approach based on
hypercomplex information. <em>ASOC</em>, <em>94</em>, 106453. (<a
href="https://doi.org/10.1016/j.asoc.2020.106453">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Feature selection for a given model can be transformed into an optimization task . The essential idea behind it is to find the most suitable subset of features according to some criterion. Nature-inspired optimization can mitigate this problem by producing compelling yet straightforward solutions when dealing with complicated fitness functions. Additionally, new mathematical representations, such as quaternions and octonions, are being used to handle higher-dimensional spaces. In this context, we are introducing a meta-heuristic optimization framework in a hypercomplex-based feature selection, where hypercomplex numbers are mapped to real-valued solutions and then transferred onto a boolean hypercube by a sigmoid function . The intended hypercomplex feature selection is tested for several meta-heuristic algorithms and hypercomplex representations, achieving results comparable to some state-of-the-art approaches. The good results achieved by the proposed approach make it a promising tool amongst feature selection research.},
  archive      = {J_ASOC},
  author       = {Gustavo H. de Rosa and João P. Papa and Xin-She Yang},
  doi          = {10.1016/j.asoc.2020.106453},
  journal      = {Applied Soft Computing},
  pages        = {106453},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A nature-inspired feature selection approach based on hypercomplex information},
  volume       = {94},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Fuzzy based image edge detection algorithm for blood vessel
detection in retinal images. <em>ASOC</em>, <em>94</em>, 106452. (<a
href="https://doi.org/10.1016/j.asoc.2020.106452">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We developed a contour detection based image processing algorithm based on Mamdani (Type-2) fuzzy rules for detection of blood vessels in retinal fundus images. The method uses the green channel data from eye fundus images as input, Contrast-Limited Adaptive Histogram Equalization (CLAHE) for contrast enhancement, and median filter for background exclusion. The Mamdani (Type-2) fuzzy rules applied on image gradient value are used for edge detection. The results of experiments on the Digital Retinal Images for Vessel Extraction (DRIVE), STructured Analysis of the Retina (STARE) and CHASEdb datasets show the applicability of the proposed method as a flexible approach which can be adapted to numerous edge detection/contour based applications. We achieved an accuracy of 0.865 for STARE dataset, an accuracy of 0.939 for the DRIVE dataset, and the accuracy of 0.950 for the ChaseDB dataset. In relation to works of other authors, our method offered a similar performance, but it offers an improved dynamics and flexibility in formulation of the linguistic threshold criteria, which can be a leading factor in design of image processing systems with dynamic and flexible rules, such as Type-2 fuzzy rules would allow, offering an interesting alternative to currently widespread deep learning applications.},
  archive      = {J_ASOC},
  author       = {F. Orujov and R. Maskeliūnas and R. Damaševičius and W. Wei},
  doi          = {10.1016/j.asoc.2020.106452},
  journal      = {Applied Soft Computing},
  pages        = {106452},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Fuzzy based image edge detection algorithm for blood vessel detection in retinal images},
  volume       = {94},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A case learning-based differential evolution algorithm for
global optimization of interplanetary trajectory design. <em>ASOC</em>,
<em>94</em>, 106451. (<a
href="https://doi.org/10.1016/j.asoc.2020.106451">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The problem of optimally designing an interplanetary trajectory for a space mission is considered in this paper. To tackle the extreme non-linearity of the search space, a case learning-based differential evolution algorithm , named CLDE, is proposed. It stores successful control parameters (scaling factor and crossover possibility) and retrieve the available reference information according to a geographic similarity in each generation. To depart from the basin of attraction of a local optimum, CLDE will give up learning from the successful cases once no better offsprings have been obtained within a certain number of generations and generate new control parameters. Two versions of CLDE have been developed, for global optimization (G-CLDE) and local optimization (L-CLDE), respectively. Their performance has been tested on GTOP benchmarks and real mission design. Experimental results show that G-CLDE performs better than related algorithms, including PYGMO algorithms and recently published L-SHADE variants. L-CLDE can improve upon the best known solution for the Messenger benchmark (full version). By connecting G-CLDE and L-CLDE together, CLDE finds promising results in acceptable computational time on the GTOP benchmark.},
  archive      = {J_ASOC},
  author       = {Mingcheng Zuo and Guangming Dai and Lei Peng and Maocai Wang and Zhengquan Liu and Changchun Chen},
  doi          = {10.1016/j.asoc.2020.106451},
  journal      = {Applied Soft Computing},
  pages        = {106451},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A case learning-based differential evolution algorithm for global optimization of interplanetary trajectory design},
  volume       = {94},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Scheduling different types of vehicles in distribution
centers with fixed due dates and packed shipments. <em>ASOC</em>,
<em>94</em>, 106450. (<a
href="https://doi.org/10.1016/j.asoc.2020.106450">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes a novel non-dominated sorting genetic algorithm-based method for scheduling simultaneously different types of vehicles with different capacities in distribution centers working as cross dock. In this paper, a three-objective model is proposed, which minimizes operational time, lateness and earliness of delivering products. In many of real life cases, packs of products with different number of products must be unloaded instead of arbitrary number of products because of products’ nature and/or physical limitations. It means that the optional amount of unloading is not prohibited in many of real life cases unlike to previous researches; therefore, scheduling vehicles by considering this limitation is different from previous works that allow optional amount of unloading. Another advantage of this paper is consideration of the frequent unloading pattern of inbound vehicles. This consideration better synchronizes the inbound and outbound vehicles compared to non-frequent pattern, and as a result, it helps to reduce the operational time of cross docking as well as the shipping cycle. This paper proposes a new scheduling method for considering the mentioned novelties. Furthermore, Taguchi design is used for regulating the proposed algorithm’s parameters. Several numerical examples are solved by the proposed method, and the obtained results are compared to multi-objective particle swarm optimization . The numerical results show the superiority of the proposed method.},
  archive      = {J_ASOC},
  author       = {Ali Mohtashami},
  doi          = {10.1016/j.asoc.2020.106450},
  journal      = {Applied Soft Computing},
  pages        = {106450},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Scheduling different types of vehicles in distribution centers with fixed due dates and packed shipments},
  volume       = {94},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Autonomous learning multiple-model zero-order classifier for
heart sound classification. <em>ASOC</em>, <em>94</em>, 106449. (<a
href="https://doi.org/10.1016/j.asoc.2020.106449">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes a new extended zero-order Autonomous Learning Multiple-Model (ALMMo-0*) neuro-fuzzy approach in order to classify different heart disorders through sounds. ALMMo-0* is build upon the recently introduced ALMMo-0. In this paper ALMMo-0 is extended by adding a pre-processing structure which improves the performance of the proposed method. ALMMo-0* has as a learning engine composed of hierarchical a massively parallel set of 0-order fuzzy rules, which are able to self-adapt and provide transparent and human understandable IF ... THEN representation. The heart sound recordings considered in the analysis were sourced from several contributors around the world. Data were collected from both clinical and nonclinical environment, and from healthy and pathological patients. Differently from mainstream machine learning approaches , ALMMo-0* is able to learn from unseen data. The main goal of the proposed method is to provide highly accurate models with high transparency, interpretability , and explainability for heart disorder diagnosis. Experiments demonstrated that the proposed neuro-fuzzy-based modeling is an efficient framework for these challenging classification tasks surpassing its state-of-the-art competitors in terms of classification accuracy . Additionally, ALMMo-0* produced transparent AnYa type fuzzy rules, which are human interpretable, and may help specialists to provide more accurate diagnosis. Medical doctors can easily identify abnormal heart sounds by comparing a patient’s sample with the identified prototypes from abnormal samples by ALMMo-0*.},
  archive      = {J_ASOC},
  author       = {Eduardo Soares and Plamen Angelov and Xiaowei Gu},
  doi          = {10.1016/j.asoc.2020.106449},
  journal      = {Applied Soft Computing},
  pages        = {106449},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Autonomous learning multiple-model zero-order classifier for heart sound classification},
  volume       = {94},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A decision making model based on the leading principal
submatrices of a reciprocal preference relation. <em>ASOC</em>,
<em>94</em>, 106448. (<a
href="https://doi.org/10.1016/j.asoc.2020.106448">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A completed comparison matrix is the basic tool in the analytic hierarchy process (AHP). In practice, the process of forming a comparison matrix is complex and it is worth being investigated carefully. In this study, by decomposing pairwise comparison process of alternatives, the leading principal submatrices (LPSMs) of a completed comparison matrix are used as the basis for decision analysis. Based on the particle swarm optimization (PSO), a novel method for improving consistency of inconsistent comparison matrices is proposed. The fitness function is constructed by considering the acceptable consistency of pairwise comparison matrices and the similarity degree between the initial and the adjusted decision-making information. A new algorithm is elaborated on for solving a decision making problem with multiplicative reciprocal matrices. Numerical results are reported to show the advantages of the proposed model by comparing with the other methods. The observations reveal that the proposed method and algorithm are effective when dealing with inconsistent comparison matrices and the corresponding decision making problems.},
  archive      = {J_ASOC},
  author       = {Fang Liu and Jia-Wei Zhang and Shu-Cai Zou},
  doi          = {10.1016/j.asoc.2020.106448},
  journal      = {Applied Soft Computing},
  pages        = {106448},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A decision making model based on the leading principal submatrices of a reciprocal preference relation},
  volume       = {94},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A hybrid type-2 fuzzy logic system and extreme learning
machine for low-cost INS/GPS in high-speed vehicular navigation system.
<em>ASOC</em>, <em>94</em>, 106447. (<a
href="https://doi.org/10.1016/j.asoc.2020.106447">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to the combined navigation system consisting of both Inertial Navigation System (INS) and Global Positioning System (GPS) in a complementary mode which assure a reliable, accurate, and continuous navigation system , we use a GPS/INS navigation system in our research. Because of the conditions of navigation system such as low-cost MEMS-based inertial sensors with considerable uncertainty in INS sensors, a highly noisy real data, and a long term outage of GPS signals during our flight tests, we enhance the positioning speed and accuracy by an Extreme Learning Machine (ELM) with the features of excellent generalization performance and fast learning speed. However, the generalization capability of ELM usually destabilizes with uncertainty existing in the dataset. In order to fix this limitation, first, a Type-2 Fuzzy Logic System (T2-FLS) handles the uncertainties in GPS/INS data, and then the final output ends up to the ELM to train and predict INS positioning error. We verify the efficiency of the suggested method in the estimation of speed and accuracy in INS sensors error during GPS satellites outage, particularly in real-time applications with a high-speed vehicle. Then, to evaluate the overall performance of the proposed method, the achieved results are discussed and compared to other methods like Extended Kalman Filter (EKF), wavelet-ELM, and Adaptive Neuro-Fuzzy Inference System (ANFIS). The results present considerable achievement and open the door to the application of T2-FLS and ELM in GPS/INS integration even in severe conditions.},
  archive      = {J_ASOC},
  author       = {E.S. Abdolkarimi and G. Abaei and A. Selamat and M.R. Mosavi},
  doi          = {10.1016/j.asoc.2020.106447},
  journal      = {Applied Soft Computing},
  pages        = {106447},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A hybrid type-2 fuzzy logic system and extreme learning machine for low-cost INS/GPS in high-speed vehicular navigation system},
  volume       = {94},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020a). A class of new support vector regression models.
<em>ASOC</em>, <em>94</em>, 106446. (<a
href="https://doi.org/10.1016/j.asoc.2020.106446">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a novel convex loss function termed as ‘ ϵ ϵ -penalty loss function’, to be used in Support Vector Regression (SVR) model. The proposed ϵ ϵ -penalty loss function is shown to be optimal for a more general noise distribution. The popular ϵ ϵ -insensitive loss function and the Laplace loss function are particular cases of the proposed loss function. Making the use of the proposed loss function, we have proposed two new Support Vector Regression models in this paper. The first model which we have termed with ‘ ϵ ϵ -Penalty Support Vector Regression’ ( ϵ ϵ -PSVR) model minimizes the proposed loss function with L 2 L2 -norm regularization . The second model minimizes the proposed loss function with L 1 L1 -Norm regularization and has been termed as ‘ L 1 L1 -Norm Penalty Support Vector Regression’ ( L 1 L1 - Norm PSVR) model. The proposed loss function can offer different rates of penalization inside and outside of the ϵ ϵ -tube. This strategy enables the proposed SVR models to use the full information of the training set which make them to generalize well. Further, the numerical results obtained from the experiments carried out on various artificial, benchmark datasets and financial time series datasets show that the proposed SVR models own better generalization ability than existing SVR models.},
  archive      = {J_ASOC},
  author       = {Pritam Anand and Reshma Rastogi nee Khemchandani and Suresh Chandra},
  doi          = {10.1016/j.asoc.2020.106446},
  journal      = {Applied Soft Computing},
  pages        = {106446},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A class of new support vector regression models},
  volume       = {94},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A modified salp swarm algorithm for task assignment problem.
<em>ASOC</em>, <em>94</em>, 106445. (<a
href="https://doi.org/10.1016/j.asoc.2020.106445">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The task assignment problem (TAP) is one of the standard combinatorial optimization problems (COPs) in the field of discrete optimization . TAP is known to be an NP-complete problem due to the difficulty to obtain the exact solution of it in polynomial time . Thus, it is essential to develop and/or propose an optimization algorithm to solve this problem. In TAPs, a set of tasks is assigned to a set of machines to both effectively minimize the communication and execution cost. This paper presents a modified Salp Swarm Algorithm (SSA), to solve not only task assignment problems but also, fundamental combinatorial optimization problems in engineering and real-world scientific domains. The modified salp algorithm takes place in the integration with the Local Refinement Heuristic (LRH) approach to enhance a given assignment along with operators. The modified algorithm is named Modified Salp Local Refinement Heuristic (MSLRH). To the best of our knowledge, this paper is the first of its kind to attempt using the SSA in task assignment problems. The MSLRH algorithm is tested on different benchmark datasets (tree structure and general graph), including various tasks and machines for each dataset. In addition, it compared with the most known meta-heuristic algorithms such as the Genetic Algorithm (GA), Particle Swarm Optimization (PSO) algorithm, and Jaya algorithm (JAYA) to investigate the effectiveness of the MSLRH algorithm in terms of average assignment allocation cost. From tree structure dataset (e.g., 200 tasks assigned to 8 machines), the proposed MSLRH algorithm has achieved a minimum average assignment cost better than GA by 62\% and better than PSO and JAYA by 42\%. From the general graph dataset (e.g., 209 tasks assigned to 16 machines), the proposed MSLRH algorithm has better results than other algorithms up to 60\%. From various and extensive experimental results, the proposed algorithm has proven the effectiveness in solving the task assignment problem.},
  archive      = {J_ASOC},
  author       = {Walaa H. El-Ashmawi and Ahmed F. Ali},
  doi          = {10.1016/j.asoc.2020.106445},
  journal      = {Applied Soft Computing},
  pages        = {106445},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A modified salp swarm algorithm for task assignment problem},
  volume       = {94},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). New peer effect-based approach for service matching in cloud
manufacturing under uncertain preferences. <em>ASOC</em>, <em>94</em>,
106444. (<a href="https://doi.org/10.1016/j.asoc.2020.106444">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cloud manufacturing is a kind of sharing manufacturing, and the supply–demand matching between manufacturing services and customers has become one of the most important issues for a platform. Because of the increasing complexity of customer personalization, the cognitive information from both sides becomes uncertain and fuzzy. Linguistics is used to describe uncertain preferences, especially in platforms. Also, the cloud model is adopted to convert the linguistics to reflect the randomness and fuzziness . Meanwhile, as the rapid development of communication techniques has strengthened the connections between different agents, the final matching results are affected by connections. Hence, the peer effect, which describes the mutual influences among individuals, is introduced in our study. In addition, considering the different strengths of the connections, the peer effect is improved by integrating grey relations, which are used to evaluate the diverse connections. Finally, the workload is introduced in the form of adjustment parameters. Consequently, we establish a bi-objective model that aims to maximize satisfaction and minimize the differences among individuals. To solve the mathematical model, an improved cuckoo algorithm is proposed. Additionally, the design of an air outlet grille for new-energy vehicles is taken as an example, and the effectiveness of the proposed method is verified.},
  archive      = {J_ASOC},
  author       = {Huagang Tong and Jianjun Zhu},
  doi          = {10.1016/j.asoc.2020.106444},
  journal      = {Applied Soft Computing},
  pages        = {106444},
  shortjournal = {Appl. Soft. Comput.},
  title        = {New peer effect-based approach for service matching in cloud manufacturing under uncertain preferences},
  volume       = {94},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A parallel compact cuckoo search algorithm for
three-dimensional path planning. <em>ASOC</em>, <em>94</em>, 106443. (<a
href="https://doi.org/10.1016/j.asoc.2020.106443">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The three-dimensional (3D) path planning of unmanned robots focuses on avoiding collisions with obstacles and finding an optimized path to the target location in a complex three-dimensional environment. An improved cuckoo search algorithm based on compact and parallel techniques for three-dimensional path planning problems is proposed. This paper implements the compact cuckoo search algorithm , and then, a new parallel communication strategy is proposed. The compact scheme can effectively save the memory of the unmanned robot. The parallel scheme can increase the accuracy and achieve faster convergence. The proposed algorithm is tested on several selected functions and three-dimensional path planning . Results compared with other methods show that the proposed algorithm can provide more competitive results and achieve more efficient execution.},
  archive      = {J_ASOC},
  author       = {Pei-Cheng Song and Jeng-Shyang Pan and Shu-Chuan Chu},
  doi          = {10.1016/j.asoc.2020.106443},
  journal      = {Applied Soft Computing},
  pages        = {106443},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A parallel compact cuckoo search algorithm for three-dimensional path planning},
  volume       = {94},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A multi-objective artificial butterfly optimization approach
for feature selection. <em>ASOC</em>, <em>94</em>, 106442. (<a
href="https://doi.org/10.1016/j.asoc.2020.106442">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Feature selection plays an essential role in machine learning since high dimensional real-world datasets are becoming more popular nowadays. The very basic idea consists in selecting a compact but representative set of features that reduce the computational cost and minimize the classification error . In this paper, the authors propose single, multi- and many-objective binary versions of the Artificial Butterfly Optimization (ABO) in the context of feature selection. The authors also propose two different approaches: (i) the first one (MO-I) aims at optimizing the classification accuracy of each class individually, while (ii) the second one (MO-II) considers the feature set minimization in the process either. The experiments were conducted over eight public datasets, and the proposed approaches are compared against the well-known Particle Swarm Optimization , Firefly Algorithm , Flower Pollination Algorithm , Brainstorm Optimization, and the Black Hole Algorithm. The results showed that the binary single-objective ABO performed better than the other meta-heuristic techniques, selecting fewer features and also figuring a lower computational burden. Concerning multi- and many-objective feature selection, both MO-I and MO-II approaches performed better than their single-objective meta-heuristic counterparts.},
  archive      = {J_ASOC},
  author       = {Douglas Rodrigues and Victor Hugo C. de Albuquerque and João Paulo Papa},
  doi          = {10.1016/j.asoc.2020.106442},
  journal      = {Applied Soft Computing},
  pages        = {106442},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A multi-objective artificial butterfly optimization approach for feature selection},
  volume       = {94},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). An extended pythagorean fuzzy complex proportional
assessment approach with new entropy and score function: Application in
pharmacological therapy selection for type 2 diabetes. <em>ASOC</em>,
<em>94</em>, 106441. (<a
href="https://doi.org/10.1016/j.asoc.2020.106441">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the context of medical decision making, the Type 2 Diabetes (T2D) pharmacological therapy selection problem involves several medications that can be stipulated to manage the blood glucose level of patients. The extensive range of hyperglycemia lowering agents with varying outcomes and several side effects makes the decision quite complicated and uncertain. Pythagorean Fuzzy Sets (PFSs) are proven as one of the valuable tools to handle ambiguous and ill-defined problems. This paper introduces an innovative Complex Proportional Assessment (COPRAS) to solve the T2D medication selection problem under the Pythagorean fuzzy environment. In this methodology, a new formula based on entropy measure and score function is introduced to evaluate the unknown criteria weights. To doing so, the new entropy measure and score function are developed under the PFSs context. Further, an illustrative case study of the T2D pharmacological therapy selection problem is taken to express the feasibility and usefulness of the proposed method in real-world decision-making problems. Finally, the obtained result is compared with some existing methods, which confirms the strength and stability of the proposed COPRAS method.},
  archive      = {J_ASOC},
  author       = {Pratibha Rani and Arunodaya Raj Mishra and Abbas Mardani},
  doi          = {10.1016/j.asoc.2020.106441},
  journal      = {Applied Soft Computing},
  pages        = {106441},
  shortjournal = {Appl. Soft. Comput.},
  title        = {An extended pythagorean fuzzy complex proportional assessment approach with new entropy and score function: Application in pharmacological therapy selection for type 2 diabetes},
  volume       = {94},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Memetic search for composing medical crews with equity and
efficiency. <em>ASOC</em>, <em>94</em>, 106440. (<a
href="https://doi.org/10.1016/j.asoc.2020.106440">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Composing medical crews with equity and efficiency is an important practical problem commonly arising from health care system management . This work presents the first hybrid memetic algorithm for this problem. The proposed approach integrates an original backbone-based crossover for generating promising offspring solutions and a tabu search based local optimization algorithm exploring both feasible and infeasible search regions. Computational experiments on two sets of benchmark instances in the literature are conducted to assess the proposed algorithm with reference to existing methods. This study advances the state-of-the-art of solving this relevant practical problem and is expected to inspire new solution methods to similar problems.},
  archive      = {J_ASOC},
  author       = {Qing Zhou and Jin-Kao Hao and Zhe Sun and Qinghua Wu},
  doi          = {10.1016/j.asoc.2020.106440},
  journal      = {Applied Soft Computing},
  pages        = {106440},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Memetic search for composing medical crews with equity and efficiency},
  volume       = {94},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Retinal vessel segmentation using multifractal
characterization. <em>ASOC</em>, <em>94</em>, 106439. (<a
href="https://doi.org/10.1016/j.asoc.2020.106439">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study presents a supervised classification method for the segmentation of retinal vessels using fundus images. This work proposes a novel retinal vasculature segmentation method based on multifractal characterization of the vessels to minimize the noise and enhance the vessels during segmentation. The Holder exponent, a multifractal measure is employed for the first time to segment the retinal vessels. The Holder exponent is used to quantify the local regularity of the vessels. The Holder exponents are computed from the Gabor wavelet responses for the effective segmentation of vessels, which is a novel feature of the method. The Gaussian mixture model (GMM) classifier is used for the classification of pixels. Different multifractal measures used to compute the Holder exponents are evaluated for the output quality. The effectiveness of the method is evaluated using three publicly available datasets for fundus images namely, DRIVE, STARE and CHASE_DB1. The proposed method provides robust segmentation of retinal vessels for both normal and abnormal images (i.e., images with pathologies) at a reasonable segmentation speed and the method is also simple to configure. It achieves an average accuracy and area under the receiver operating characteristic curve of 0.948 and 0.959 respectively on the DRIVE dataset; 0.9542 and 0.9711 respectively on the STARE dataset; 0.9459 and 0.9592 respectively on the CHASE_DB1 dataset; 0.9500 and 0.9623 respectively on the abnormal images.},
  archive      = {J_ASOC},
  author       = {Dhevendra Alagan Palanivel and Sivakumaran Natarajan and Sainarayanan Gopalakrishnan},
  doi          = {10.1016/j.asoc.2020.106439},
  journal      = {Applied Soft Computing},
  pages        = {106439},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Retinal vessel segmentation using multifractal characterization},
  volume       = {94},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Optimization of drop ejection frequency in EHD inkjet
printing system using an improved firefly algorithm. <em>ASOC</em>,
<em>94</em>, 106438. (<a
href="https://doi.org/10.1016/j.asoc.2020.106438">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Swarm intelligence approaches have been used to solve various optimization real-world applications in recent years. Firefly Algorithm (FA) is one of the popular stochastic swarm intelligence paradigms developed in the recent past. In order to solve the slow convergence speed of the standard FA, a new improved Firefly Algorithm (iFA) is proposed in this research. Instead of keeping a constant initial brightness coefficient, a new rule has been proposed for updating the brightness of the fireflies based on a selection probability during generations which leads to a better balance between exploration and exploitation. The efficiency of the iFA has been tested by solving benchmark mathematical functions as well as in a real-world engineering problems. In order to comprehensively compare the performance of the iFA, several other metaheuristic algorithms were used for solving the same benchmark functions and the real-world problems. The iFA shows its superiority in almost all the cases for finding better optima in terms of the objective function value. The droplet ejection speed of an electrohydrodynamic inkjet printing system has been significantly improved by the iFA, which hence fully demonstrates its potential to solve real-world problems. Additionally, the iFA proves its efficiency in solving some challenging, classic engineering design problems with unknown search space. The source codes of the iFA are publiclyavailable at https://www.amitball.com/projects/iFA .},
  archive      = {J_ASOC},
  author       = {Amit Kumar Ball and Shibendu Shekhar Roy and Dakshina Ranjan Kisku and Naresh Chandra Murmu and Leandro dos Santos Coelho},
  doi          = {10.1016/j.asoc.2020.106438},
  journal      = {Applied Soft Computing},
  pages        = {106438},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Optimization of drop ejection frequency in EHD inkjet printing system using an improved firefly algorithm},
  volume       = {94},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Incomplete data classification with view-based decision
tree. <em>ASOC</em>, <em>94</em>, 106437. (<a
href="https://doi.org/10.1016/j.asoc.2020.106437">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data quality issues may bring serious problems in data analysis. For instance, missing values could decrease the accuracy of the classification. As traditional classification approaches can only be applied to complete data sets, we present a generic classification model for incomplete data where existing classification methods can be effectively incorporated. Firstly, we generate complete views from the incomplete data by choosing proper subsets of attributes based on Information Gain measure. Then we use these selected views to obtain multiple base classifiers . Finally, the base classifies are effectively combined as a final classifier with a decision tree . Extensive experiments results on real data sets demonstrate that the proposed method outperforms existing approaches.},
  archive      = {J_ASOC},
  author       = {Hekai Huang and Hongzhi Wang and Ming Sun},
  doi          = {10.1016/j.asoc.2020.106437},
  journal      = {Applied Soft Computing},
  pages        = {106437},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Incomplete data classification with view-based decision tree},
  volume       = {94},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Identifying influential spreaders using multi-objective
artificial bee colony optimization. <em>ASOC</em>, <em>94</em>, 106436.
(<a href="https://doi.org/10.1016/j.asoc.2020.106436">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Advertisement over social networks is critical for many businesses, and selecting the initial set of influential nodes for which the advertising message is passed, is regarded as an important issue in this regard. Although various measures have been proposed for specifying the influentiality of a set, it is affected by different factors. In this paper, a multi-objective function is first defined as an influentiality measure, and finding such an initial is framed as an optimization problem . Then, using artificial bee colony optimization two approaches are proposed to solve the problem. In the first approach, influentiality of nodes is only taken into account, while in the second method a budget constraint is also considered. Different experiments on real networks are conducted to evaluate the proposed methods, where the obtained results show their outperformance over state-of-the-art influence maximization methods.},
  archive      = {J_ASOC},
  author       = {Amir Sheikhahmadi and Ahmad Zareie},
  doi          = {10.1016/j.asoc.2020.106436},
  journal      = {Applied Soft Computing},
  pages        = {106436},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Identifying influential spreaders using multi-objective artificial bee colony optimization},
  volume       = {94},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). User reviews: Sentiment analysis using lexicon integrated
two-channel CNN–LSTM​ family models. <em>ASOC</em>, <em>94</em>, 106435.
(<a href="https://doi.org/10.1016/j.asoc.2020.106435">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sentiment analysis , which refers to the task of detecting whether a textual item (e.g., a product review and a blog post) expresses a positive or negative opinion in general or about a given entity (e.g., a product, person, or policy), has received increasing attention in recent years. It serves as an important role in natural language processing . User generated content , like tourism reviews, developed dramatically during the past years, generating a large amount of unstructured data from which it is hard to obtain useful information. Due to the changes in textual order, sequence length and complicated logic, it is still a challenging task to predict the exact sentiment polarities of the user reviews, especially for fine-grained sentiment classification. In this paper, we first propose sentiment padding, a novel padding method compared with zero padding, making the input data sample of a consistent size and improving the proportion of sentiment information in each review. Inspired by the most recent studies with respect to neural networks , we propose deep learning based sentiment analysis models named lexicon integrated two-channel CNN–LSTM family models, combining CNN and LSTM/BiLSTM branches in a parallel manner. Experiments on several challenging datasets, like Stanford Sentiment Treebank, demonstrate that the proposed method outperforms many baseline methods .},
  archive      = {J_ASOC},
  author       = {Wei Li and Luyao Zhu and Yong Shi and Kun Guo and Erik Cambria},
  doi          = {10.1016/j.asoc.2020.106435},
  journal      = {Applied Soft Computing},
  pages        = {106435},
  shortjournal = {Appl. Soft. Comput.},
  title        = {User reviews: Sentiment analysis using lexicon integrated two-channel CNN–LSTM​ family models},
  volume       = {94},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). An ant colony hyperheuristic approach for matrix bandwidth
reduction. <em>ASOC</em>, <em>94</em>, 106434. (<a
href="https://doi.org/10.1016/j.asoc.2020.106434">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper considers the bandwidth reduction problem for large-scale matrices in serial computations. A heuristic for bandwidth reduction reorders the rows and columns of a given sparse matrix so that the method places entries with a nonzero value as close to the main diagonal as possible. Bandwidth optimization is a critical issue for many scientific and engineering applications . In this regard, this paper proposes an ant colony hyperheuristic approach for the bandwidth reduction of symmetric and nonsymmetric matrices. The ant colony hyperheuristic approach evolves and selects graph theory bandwidth reduction algorithms for application areas. This paper evaluates the resulting heuristics for bandwidth reduction in each application area against the most promising low-cost heuristics for bandwidth reduction. This paper also includes a numerical examination of the current state-of-the-art metaheuristic algorithms for matrix bandwidth reduction. The results yielded on a wide-ranging set of standard benchmark matrices showed that the proposed approach outperformed state-of-the-art low-cost heuristics for bandwidth reduction when applied to problem cases arising from several application areas, clearly indicating the promise of the proposal.},
  archive      = {J_ASOC},
  author       = {S.L. Gonzaga de Oliveira and L.M. Silva},
  doi          = {10.1016/j.asoc.2020.106434},
  journal      = {Applied Soft Computing},
  pages        = {106434},
  shortjournal = {Appl. Soft. Comput.},
  title        = {An ant colony hyperheuristic approach for matrix bandwidth reduction},
  volume       = {94},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A novel randomized machine learning approach: Reservoir
computing extreme learning machine. <em>ASOC</em>, <em>94</em>, 106433.
(<a href="https://doi.org/10.1016/j.asoc.2020.106433">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this study, a novel approach that is based on reservoir computing , which is a successful method in modeling sequential datasets, and extreme learning machines , which has a high generalization capacity, was proposed to model a non-sequential dataset or system. The proposed approach does not require any optimization stage ; each weight (except weights in the output layer), biases, the number of neurons in the reservoir, activation functions and the parameters of activation functions were determined arbitrarily and the weights in the output layer were calculated based on these arbitrarily assigned parameters. The proposed approach was evaluated and validated with 60 different benchmark datasets. Obtained results were compared with literature findings and results obtained by each of the extreme learning machine (ELM), randomized artificial neural network , random vector functional link, stochastic ELM, and pruned stochastic ELM methods. Achieved results are successful enough to be employed in classification and regression.},
  archive      = {J_ASOC},
  author       = {Ömer Faruk Ertuğrul},
  doi          = {10.1016/j.asoc.2020.106433},
  journal      = {Applied Soft Computing},
  pages        = {106433},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A novel randomized machine learning approach: Reservoir computing extreme learning machine},
  volume       = {94},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Constructing parsimonious analytic models for dynamic
systems via symbolic regression. <em>ASOC</em>, <em>94</em>, 106432. (<a
href="https://doi.org/10.1016/j.asoc.2020.106432">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Developing mathematical models of dynamic systems is central to many disciplines of engineering and science. Models facilitate simulations, analysis of the system’s behavior, decision making and design of automatic control algorithms. Even inherently model-free control techniques such as reinforcement learning (RL) have been shown to benefit from the use of models, typically learned online. Any model construction method must address the tradeoff between the accuracy of the model and its complexity, which is difficult to strike. In this paper, we propose to employ symbolic regression (SR) to construct parsimonious process models described by analytic equations. We have equipped our method with two different state-of-the-art SR algorithms which automatically search for equations that fit the measured data: Single Node Genetic Programming (SNGP) and Multi-Gene Genetic Programming (MGGP). In addition to the standard problem formulation in the state-space domain, we show how the method can also be applied to input–output models of the NARX (nonlinear autoregressive with exogenous input) type. We present the approach on three simulated examples with up to 14-dimensional state space: an inverted pendulum , a mobile robot, and a bipedal walking robot . A comparison with deep neural networks and local linear regression shows that SR in most cases outperforms these commonly used alternative methods. We demonstrate on a real pendulum system that the analytic model found enables a RL controller to successfully perform the swing-up task, based on a model constructed from only 100 data samples.},
  archive      = {J_ASOC},
  author       = {Erik Derner and Jiří Kubalík and Nicola Ancona and Robert Babuška},
  doi          = {10.1016/j.asoc.2020.106432},
  journal      = {Applied Soft Computing},
  pages        = {106432},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Constructing parsimonious analytic models for dynamic systems via symbolic regression},
  volume       = {94},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Solving green supplier selection problem using q-rung
orthopair fuzzy-based decision framework with unknown weight
information. <em>ASOC</em>, <em>94</em>, 106431. (<a
href="https://doi.org/10.1016/j.asoc.2020.106431">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As a powerful generalization to intuitionistic fuzzy set (IFS), q-rung orthopair fuzzy set (q-ROFS) is proposed by Yager, which can effectively mitigate the weakness of IFS and provide wider space for preference elicitation. Based on the literature analysis on q-ROFS, a comprehensive decision framework for promoting rational decision-making is lacking. Motivated by the superiority of q-ROFS and to circumvent the issue, in this paper, a new decision framework with minimum subjective randomness is proposed under q-ROFS context. Initially, decision makers’ (DMs’) relative importance is systematically calculated by extending evidence-based Bayes approximation to q-ROFS. Later, a new operator is proposed for aggregating DMs’ preferences by extending generalized Maclaurin symmetric mean (GMSM) to q-ROFS context. Attributes’ weight values are calculated by using newly proposed q-rung orthopair fuzzy statistical variance (q-ROFSV) method and objects are prioritized by extending the popular VIKOR method to q-ROFS context. Finally, the practical use of the proposed decision framework is validated by using a green supplier selection problem and the strengths and weaknesses of the framework are discussed by using comparative analysis with other methods.},
  archive      = {J_ASOC},
  author       = {R. Krishankumar and Y. Gowtham and Ifjaz Ahmed and K.S. Ravichandran and Samarjit Kar},
  doi          = {10.1016/j.asoc.2020.106431},
  journal      = {Applied Soft Computing},
  pages        = {106431},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Solving green supplier selection problem using q-rung orthopair fuzzy-based decision framework with unknown weight information},
  volume       = {94},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). An inverse-distance weighting genetic algorithm for
optimizing the wafer exposure pattern for enhancing OWE for smart
manufacturing. <em>ASOC</em>, <em>94</em>, 106430. (<a
href="https://doi.org/10.1016/j.asoc.2020.106430">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Wafer exposure pattern will determine the number of gross dies fabricated on the wafer and also affect the yield. Although a number of studies have addressed the wafer exposure pattern problem for maximizing the number of gross dies, little research has considered both the yield and gross dies simultaneously. To fill the gap, this study aims to develop an inverse distance weighting genetic algorithm (IDWGA) that simultaneously maximizes the total number of exposed gross dies and minimizes the deviation of die-estimated measurement from the target for yield enhancement and smart manufacturing. This study developed a novel approach for estimating the die yield from a few measurement points and a three-dimensional (3D) contour plot of die estimates for verifying the measurement pattern among the dies. The proposed IDWGA can detect the die yield pattern during the wafer exposure stage and thus optimize the exposure pattern to maximize the number of gross dies and minimize potential yield loss. On the basis of realistic data, experiments were designed to estimate the validity of the proposed approach. The results have shown practical viability of the proposed approach to optimize overall wafer effectiveness for total resource management.},
  archive      = {J_ASOC},
  author       = {Hung-Kai Wang and Chen-Fu Chien},
  doi          = {10.1016/j.asoc.2020.106430},
  journal      = {Applied Soft Computing},
  pages        = {106430},
  shortjournal = {Appl. Soft. Comput.},
  title        = {An inverse-distance weighting genetic algorithm for optimizing the wafer exposure pattern for enhancing OWE for smart manufacturing},
  volume       = {94},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Kriging-assisted discrete global optimization (KDGO) for
black-box problems with costly objective and constraints. <em>ASOC</em>,
<em>94</em>, 106429. (<a
href="https://doi.org/10.1016/j.asoc.2020.106429">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, a Kriging-assisted discrete global optimization method is presented for computationally expensive black-box problems. KDGO employs Kriging to approximate the landscape of a black-box model, and utilizes a novel infilling strategy to capture the promising discrete samples. In the infilling strategy, a multi-start knowledge mining approach is introduced, including Optimization, Projection, Sampling and Selection. Firstly, a multi-start optimization is used to capture the promising solutions in the continuous design range. Secondly, all these potential solutions are projected to a predefined matrix and a grid sampling method suitable for low and high-dimensional space is proposed to get the promising discrete samples. Thereafter, the k-nearest neighbors (KNN) search strategy and expected improvement (EI) criterion are jointly used to select the candidate samples. The algorithm keeps running to update Kriging and find the most promising samples until the satisfactory solution is obtained. KDGO is primarily developed to solve time-consuming black-box problems with various discrete cases including binary, integer, non-integer, uni/multimodal and box/inequality-constrained types. After the comparison tests on 20 representative benchmark cases, KDGO proves that it can build a reasonable balance between exploitation and exploration. Besides, compared with the existing 6 methods, KDGO has significant advantages on computational efficiency and robustness. Finally, KDGO is used for structure optimization of a blended-wing-body underwater glider, and gets the satisfactory design.},
  archive      = {J_ASOC},
  author       = {Huachao Dong and Peng Wang and Baowei Song and Yijin Zhang and Xiaoyi An},
  doi          = {10.1016/j.asoc.2020.106429},
  journal      = {Applied Soft Computing},
  pages        = {106429},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Kriging-assisted discrete global optimization (KDGO) for black-box problems with costly objective and constraints},
  volume       = {94},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Improving class noise detection and classification
performance: A new two-filter CNDC model. <em>ASOC</em>, <em>94</em>,
106428. (<a href="https://doi.org/10.1016/j.asoc.2020.106428">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Class noise is an important issue in classification with a lot of potential consequences. It can decrease the overall accuracy and increase the complexity of the induced model. This study investigates ensemble filtering, removing and relabeling noisy instances issues and proposes a new two-filter model for Class Noise Detection and Classification (CNDC). The proposed two-filter CNDC model comprises two major parts, which are noise detection and noise classification. The noise detection part involves ensemble and distance filtering to overcome ensemble issues. In latter part, a Removing-Relabeling (REM-REL) technique is proposed to enhance overall performance of noise classification. To evaluate the performance of the proposed model, several experiments were conducted on six real data sets . The proposed REM-REL technique was found to be successful to classify noisy instances. The final results showed that the proposed model led to a significant performance improvement compared with ensemble filtering.},
  archive      = {J_ASOC},
  author       = {Zahra Nematzadeh and Roliana Ibrahim and Ali Selamat},
  doi          = {10.1016/j.asoc.2020.106428},
  journal      = {Applied Soft Computing},
  pages        = {106428},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Improving class noise detection and classification performance: A new two-filter CNDC model},
  volume       = {94},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Strengthen EEG-based emotion recognition using firefly
integrated optimization algorithm. <em>ASOC</em>, <em>94</em>, 106426.
(<a href="https://doi.org/10.1016/j.asoc.2020.106426">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Emotion recognition is helpful for human to enhance self-awareness and respond appropriately towards the happenings around them. Due to the complexity and diversity of emotions, EEG-based emotion recognition is still a challenging task in pattern recognition. In order to recognize diverse emotions, we propose a novel firefly integrated optimization algorithm (FIOA) in this paper. It can simultaneously accomplish multiple tasks, i.e. the optimal feature selection, parameter setting and classifier selection according to different EEG-based emotion datasets. The FIOA utilizes a ranking probability objection function to guarantee the high accuracy recognition with less features. Moreover, the hybrid encoding expression and the dual updating strategy are developed in the FIOA so as to realize the optimal selection of feature subset and classifier without stagnating in the local optimum. In addition to the public DEAP datasets, we also conducted an EEG-based music emotion experiment involving 20 subjects for the validation of the proposed FIOA. After filtering and segmentation, three categories of features were extracted from every EEG signal. Then FIOA was applied to every subject dataset for two pattern recognition of emotions. The results show that the FIOA can automatically find the optimal features, parameter and classifier for different emotion datasets, which greatly reduces the artificial selection workload. Furthermore, comparing with the binary particle swarm optimization (PSObinary) and the binary firefly (FAbinary), the FIOA can achieve the higher accuracy with less features in the emotion recognition.},
  archive      = {J_ASOC},
  author       = {Hong He and Yonghong Tan and Jun Ying and Wuxiong Zhang},
  doi          = {10.1016/j.asoc.2020.106426},
  journal      = {Applied Soft Computing},
  pages        = {106426},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Strengthen EEG-based emotion recognition using firefly integrated optimization algorithm},
  volume       = {94},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). An information maximization multi-task clustering method for
egocentric temporal segmentation. <em>ASOC</em>, <em>94</em>, 106425.
(<a href="https://doi.org/10.1016/j.asoc.2020.106425">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the widespread application of vision-based wearable devices , temporal segmentation helps people search for and localize all occurrences quickly in egocentric videos. In the same scenario, the activities are similar to each other, e.g., people staying at home typically cook, clean and watch TV. These relations among videos of different individuals are regarded as auxiliary information to improve task performance. Inspired by this, we propose an Information Maximization Multi-task Clustering (IMMC) algorithm for egocentric temporal segmentation. The algorithm mainly includes two parts: (1) within-task clustering: clustering on each task based on an information maximization approach, and (2) cross-task information transferring: a novel strategy is presented to transfer correlation information between tasks, which balances the correlation among clusters in different tasks to improve the performance of the individual task. A draw-merge method is designed to address the optimization problem . Experiments are performed on three publicly available first-person vision data sets and a new data set we construct (Outdoor data set). The results show that IMMC consistently outperforms the state-of-the-art clustering methods in multiple evaluation metrics . Moreover, it achieves relatively good performance on runtime cost and convergence.},
  archive      = {J_ASOC},
  author       = {Mingming Zhang and Xiaoqiang Yan and Shizhe Hu and Yangdong Ye},
  doi          = {10.1016/j.asoc.2020.106425},
  journal      = {Applied Soft Computing},
  pages        = {106425},
  shortjournal = {Appl. Soft. Comput.},
  title        = {An information maximization multi-task clustering method for egocentric temporal segmentation},
  volume       = {94},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Continual learning classification method with new labeled
data based on the artificial immune system. <em>ASOC</em>, <em>94</em>,
106423. (<a href="https://doi.org/10.1016/j.asoc.2020.106423">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, a new supervised learning classification method, continual learning classification method with new labeled data based on the artificial immune system (CLCMNLD), is proposed as a new way to improve the classification performance in real-time by continually learning the new labeled data during the testing stage. It is inspired by the mechanism that vaccines can enhance immunity. New types of memory cells were continuously cultured by learning new labeled data during the testing stage. CLCMNLD will degenerate into a common supervised learning classification method when there is no new labeled data comes out during the testing stage. The effectiveness of the proposed CLCMNLD is tested on twenty well-known datasets from the UCI Machine Learning Repository that are commonly used in the domain of data classification . The experiments reveal that CLCMNLD has better classification performance when it degenerates into a common supervised learning classification method, and it outperforms the other methods when there are some new labeled data comes out during the testing stage. The more types of new labeled data, the more advantages it has.},
  archive      = {J_ASOC},
  author       = {Dong Li and Shulin Liu and Furong Gao and Xin Sun},
  doi          = {10.1016/j.asoc.2020.106423},
  journal      = {Applied Soft Computing},
  pages        = {106423},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Continual learning classification method with new labeled data based on the artificial immune system},
  volume       = {94},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). IECT: A methodology for identifying critical products using
purchase transactions. <em>ASOC</em>, <em>94</em>, 106420. (<a
href="https://doi.org/10.1016/j.asoc.2020.106420">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Identifying critical products and key customers to strengthen company performance is vitally important in the digital transformation era. Critical products are the itemsets that are preferred by vip customers and yet not popular among ordinary customers. As a result, critical products should be kept on the shelf despite its sales volumes may be lower than other popular items. However, few studies have considered identifying critical products or their potentially valuable patterns. Therefore an innovative algorithm taking advantage of vertical databases to identify critical products was designed. The proposed algorithm is applied to a transaction database of a midsize supermarket to verify the performance. The result showed that precision can reach 80.55\% and 82.15\% for two different filtering criteria. To the best of our knowledge, this study is the first to apply the concept of critical products to real retail industry transaction records.},
  archive      = {J_ASOC},
  author       = {Ping-Yu Hsu and Chen-Wan Huang},
  doi          = {10.1016/j.asoc.2020.106420},
  journal      = {Applied Soft Computing},
  pages        = {106420},
  shortjournal = {Appl. Soft. Comput.},
  title        = {IECT: A methodology for identifying critical products using purchase transactions},
  volume       = {94},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Z-number integrated weighted VIKOR technique for hazard
prioritization and its application in virtual prototype based EOT crane
operations. <em>ASOC</em>, <em>94</em>, 106419. (<a
href="https://doi.org/10.1016/j.asoc.2020.106419">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hazard identification and its ranking is one of the foundation steps of safety management. Nowadays, failure mode and effect analysis (FMEA), a reliability analysis tool, is widely used to identify, assess, and rank hazards/failure events related to products, processes, and services because of its simplistic nature. Traditional FMEA focuses on computing the risk priority number (RPN) to rank the identified hazards based on three risk factors, occurrence (O), severity (S), and detectability (D) of a failure mode (FM). However, the traditional RPN method has been criticized because of its limitations in the assessment of hazards/failure events, weighting of the risk factors, and prioritization of the FMs. Furthermore, experts usually provide rating on the risk factors based on their expertise, which introduces impreciseness, uncertainty, and vagueness in the subsequent assessments. In this study, Z-number is used to capture the uncertainty and unreliability associated with experts’ evaluation. Also, the concept of objectivity is deployed along with subjectivity in the weight calculation of the risk factors in the information assessment process. Classic VIKOR (ViseKriterijum-ska Optimizacija I Kompromisno Resenje) method is also extended in Z-environment to rank the identified hazards by incorporating the concepts of maximum group utility and minimum regret. For the illustration purpose, we have applied our proposed method in virtual prototyping based EOT crane operations. Sensitivity analysis and comparative evaluation with the existing methods are also presented to validate our proposed method.},
  archive      = {J_ASOC},
  author       = {Souvik Das and Krantiraditya Dhalmahapatra and J Maiti},
  doi          = {10.1016/j.asoc.2020.106419},
  journal      = {Applied Soft Computing},
  pages        = {106419},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Z-number integrated weighted VIKOR technique for hazard prioritization and its application in virtual prototype based EOT crane operations},
  volume       = {94},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Ameliorated moth-flame algorithm and its application for
modeling of silicon content in liquid iron of blast furnace based fast
learning network. <em>ASOC</em>, <em>94</em>, 106418. (<a
href="https://doi.org/10.1016/j.asoc.2020.106418">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Moth-Flame Optimization (MFO) algorithm is a widely used nature-inspired optimization algorithm. However, for some complex optimization problems , such as high dimensional and multimodal problems, the MFO may fall into the local optimal solution . Hence, in this paper an ameliorated Moth-Flame Optimization (AMFO) algorithm is presented to improve the solution quality and global optimization capability. The key features of the proposed algorithm are the Gaussian mutation produce flames and the modified position updating mechanism of moths, which can improve the ability of MFO to jump out of local optimum solutions. In addition, opposition-based learning is adopted to initialize the population. The AMFO algorithm is compared with 9 state-of-the-art algorithms (such as Lévy Moth-Flame Optimization (LMFO), Grey Wolf Optimization (GWO), Sine Cosine Algorithm (SCA), Heterogeneous Comprehensive Learning Particle Swarm Optimization (HCLPSO)) on 23 classical benchmark functions . The comparative results show that the AMFO is effective and has good performance in terms of jumping out of local optimum, balancing exploitation ability and exploration ability. Furthermore, the AMFO is adopted to optimize the parameters of fast learning network (FLN) to build the prediction model of silicon content in liquid iron for blast furnace , and simulation experiment results from field data show that the root mean square error of the AMFO-FLN model is 0.0542, hit ratio is 91 and the relative error is relatively stable, the main fluctuation is between -0.1 and 0.1; compared with other ten silicon content in liquid iron models, the AMFO-FLN model has better predictive performance .},
  archive      = {J_ASOC},
  author       = {Xiaodong Zhao and Yiming Fang and Le Liu and Miao Xu and Pan Zhang},
  doi          = {10.1016/j.asoc.2020.106418},
  journal      = {Applied Soft Computing},
  pages        = {106418},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Ameliorated moth-flame algorithm and its application for modeling of silicon content in liquid iron of blast furnace based fast learning network},
  volume       = {94},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Universal functions originator. <em>ASOC</em>, <em>94</em>,
106417. (<a href="https://doi.org/10.1016/j.asoc.2020.106417">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nowadays, couples of computing systems have been introduced to perform many applications, such as function approximation, pattern classification, categorization/clustering, forecasting/prediction, control, and optimization. Linear regression (LR) is commonly used for simple data where the relation between its coefficients is linear, while nonlinear regression (NLR) is used when that relation is nonlinear. Artificial neural networks (ANNs) and support vector machines (SVMs) are more efficient and they can be used for complex applications. However, each one of these approaches has its own strengths and weaknesses. This study introduces a new computing system called “universal functions originator (UFO)”. This system is a new symbolic regression (SR) technique that can generate mathematical models universally through two independent optimization algorithms . Different arithmetic operators can be entered into the search pool. Also, any analytic function can be dragged into that pool. UFO has been mathematically designed and practically tested with function approximation problems. However, UFO can also be used for the applications listed above, including anomaly detection , function complication, function simplification, dimension expansion, dimension reduction, and high -dimensional function visualization. This novel computing system shows an impressive performance with many promising uses and distinct capabilities. This study reveals the mechanism of UFO and solves some numerical problems via an advanced graphical user interface (GUI) designed just to validate the process of this computing system.},
  archive      = {J_ASOC},
  author       = {Ali R. Al-Roomi and Mohamed E. El-Hawary},
  doi          = {10.1016/j.asoc.2020.106417},
  journal      = {Applied Soft Computing},
  pages        = {106417},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Universal functions originator},
  volume       = {94},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Flexible job shop scheduling problem with reconfigurable
machine tools: An improved differential evolution algorithm.
<em>ASOC</em>, <em>94</em>, 106416. (<a
href="https://doi.org/10.1016/j.asoc.2020.106416">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Developing reconfigurable machine tools (RMTs) has attracted increasing attention recently. An RMT can be utilized as a group of machines, which can obtain different configurations to satisfy manufacturing requirements. This paper deals with a production scheduling problem in a shop-floor with RMTs as an extension of a flexible job shop scheduling problem (FJSSP). To begin with, two mixed-integer linear programming models with the position- and sequence-based decision variables are formulated to minimize the maximum completion time (i.e., makespan). The CPLEX solver is used to solve the small- and medium-sized instances. The computational experiments show that the sequence-based model significantly outperforms the other one. Since even the sequence-based model cannot optimally solve most of the medium-sized problems, a self-adaptive differential evolution (DE) algorithm is proposed to efficiently solve the given problem. Moreover, the effectiveness of the proposed algorithm is enhanced by introducing a new mutation strategy based on a searching approach hired from a Nelder–Mead method. The performance of the proposed method and three other well-known variants of the DE algorithm are first validated by comparing their results with the results of the sequence-based model. Additional experiments on another data set including large-sized problems also confirm that the proposed algorithm is extremely efficient and effective.},
  archive      = {J_ASOC},
  author       = {Mehdi Mahmoodjanloo and Reza Tavakkoli-Moghaddam and Armand Baboli and Ali Bozorgi-Amiri},
  doi          = {10.1016/j.asoc.2020.106416},
  journal      = {Applied Soft Computing},
  pages        = {106416},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Flexible job shop scheduling problem with reconfigurable machine tools: An improved differential evolution algorithm},
  volume       = {94},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A dynamic interval type-2 fuzzy customer segmentation model
and its application in e-commerce. <em>ASOC</em>, <em>94</em>, 106366.
(<a href="https://doi.org/10.1016/j.asoc.2020.106366">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Internet-based services and retail are growing rapidly. To manage online customer relationships with linguistic comments, we propose a dynamic interval type-2 fuzzy customer segmentation model. Interval type-2 fuzzy linguistic labels (IT2FLLs) are used to model customer comments. The similarity of IT2FLLs is computed based on an extended distance method. Customers are segmented dynamically according to the fuzzy equivalence relations of similarity. A case study in E-commerce shows the application of the proposed model, and a comparative analysis shows its effectiveness. The dynamic customer segmentation can help managers to have a deep understanding on customers’ purchasing behaviors and to make accurate recommendations.},
  archive      = {J_ASOC},
  author       = {Tong Wu and Xinwang Liu},
  doi          = {10.1016/j.asoc.2020.106366},
  journal      = {Applied Soft Computing},
  pages        = {106366},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A dynamic interval type-2 fuzzy customer segmentation model and its application in E-commerce},
  volume       = {94},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). An improved network structural balance approach based on
weighted node-to-node influence with evolutionary algorithm.
<em>ASOC</em>, <em>94</em>, 106323. (<a
href="https://doi.org/10.1016/j.asoc.2020.106323">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The network structural balance is a challenging task in social networks. The difficulty is how to determine the unbalanced degree of a network and make the network balanced with the least cost. Aiming at this issue, this paper proposes a new weighted node-to-node influence (W2NI) model, which integrates two important node-to-node influence factors, i.e., weights and influences between nodes. Then, an EA-based weighted influence structural balance (WISB) algorithm is devised deliberately to optimize W2NI model. In WISB algorithm, a neighbors-based initialization and a random greedy based local search strategies are proposed to enhance its convergence performance. Comprehensive experiments on a set of generated networks and real social networks demonstrate the effectiveness and efficiency of the proposed model.},
  archive      = {J_ASOC},
  author       = {Mingzhou Yang and Lianbo Ma and Xingwei Wang and Min Huang and Qiang He},
  doi          = {10.1016/j.asoc.2020.106323},
  journal      = {Applied Soft Computing},
  pages        = {106323},
  shortjournal = {Appl. Soft. Comput.},
  title        = {An improved network structural balance approach based on weighted node-to-node influence with evolutionary algorithm},
  volume       = {94},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Special issue on benchmarking of computational intelligence
algorithms in the applied soft computing journal. <em>ASOC</em>,
<em>93</em>, 106502. (<a
href="https://doi.org/10.1016/j.asoc.2020.106502">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_ASOC},
  author       = {Thomas Weise and Markus Wagner and Bin Li and Xingyi Zhang and Jörg Lässig},
  doi          = {10.1016/j.asoc.2020.106502},
  journal      = {Applied Soft Computing},
  pages        = {106502},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Special issue on benchmarking of computational intelligence algorithms in the applied soft computing journal},
  volume       = {93},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Multi-attribute dynamic two-sided matching method of talent
sharing market in incomplete preference ordinal environment.
<em>ASOC</em>, <em>93</em>, 106427. (<a
href="https://doi.org/10.1016/j.asoc.2020.106427">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the development of the mobile Internet, big data and cloud computing , talent sharing has emerged and developed in the labor market. As a new economy mode of the Internet, the talent sharing can match surplus labor with relevant needs companies and achieve the maximum interests for two sides with the help of the network platform. In this paper, we deeply investigate the matching problem of talent sharing in incomplete preference ordinal environment. Unlike the traditional two-sided matching method, which directly ignores the incomplete preference ordinal, we firstly fill the incomplete preference ordinal by using the collaborative filtering algorithm . In the two-sided matching of talent sharing, we pay full attention to the individual differences of the seekers and the solvers. Because of individual differences, the seekers and the solvers have different preferences for different decision attributes , i.e., the attribute priority matrices. At the same time, considering the psychological expectations of the seekers and the solvers, we construct the satisfaction degree matrices based on the prospect theory. Given the constraints on the attribute priority and the satisfaction degree, the bi-objective optimization model for multi-stage dynamic decision-making is established. Moreover, with the aid of dynamic decision-making process, we can obtain more matches when meeting the psychological expectations of the matching subjects. Finally, a case study of the talent sharing platform Upwork is given to illustrate the validity of our proposed method.},
  archive      = {J_ASOC},
  author       = {Decui Liang and Xin He and Zeshui Xu},
  doi          = {10.1016/j.asoc.2020.106427},
  journal      = {Applied Soft Computing},
  pages        = {106427},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Multi-attribute dynamic two-sided matching method of talent sharing market in incomplete preference ordinal environment},
  volume       = {93},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Optimized fuzzy self-tuning PID controller design based on
tribe-DE optimization algorithm and rule weight adjustment method for
load frequency control of interconnected multi-area power systems.
<em>ASOC</em>, <em>93</em>, 106424. (<a
href="https://doi.org/10.1016/j.asoc.2020.106424">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The reliable load frequency control (LFC) is addressed as one of the most important services in the modern electric power system operation and planning. Since the power systems have various structural and non-structural uncertainties, using control methods with fixed parameters may not yield the optimal performance of the system. Thus, in this research, a novel fuzzy PID controller is introduced to LFC for interconnected multi-area power systems in parametric uncertainties as well as external disturbances existence. The proposed algorithm adjusts the scaling factors and the modal parameters of input and output membership functions as well as the weights of fuzzy PID controller rule values. According to the transient response of the area control error (ACE) variable partitioning, the fuzzy rule weight values are obtained. Furthermore, in order to enhance the quality of the response, the scaling factors and the modal parameters of the input and output membership functions of fuzzy PID controllers are optimized by Tribe-DE (TDE) algorithm. The method is examined on the two and three area interconnected power systems at several conditions and an Integral of Time multiplied Absolute Error (ITAE) less than 0.0108 as well as an absolute maximum undershoot less than 0.0210 Hz for Δ f 1 Δf1 regulation are obtained in the two area interconnected power system. Good transient behavior , disturbance rejection capability and insensitivity to parameter changes are advantages of the proposed controller.},
  archive      = {J_ASOC},
  author       = {Neda Jalali and Hadi Razmi and Hasan Doagou-Mojarrad},
  doi          = {10.1016/j.asoc.2020.106424},
  journal      = {Applied Soft Computing},
  pages        = {106424},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Optimized fuzzy self-tuning PID controller design based on tribe-DE optimization algorithm and rule weight adjustment method for load frequency control of interconnected multi-area power systems},
  volume       = {93},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Predicting next day direction of stock price movement using
machine learning methods with persistent homology: Evidence from kuala
lumpur stock exchange. <em>ASOC</em>, <em>93</em>, 106422. (<a
href="https://doi.org/10.1016/j.asoc.2020.106422">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Predicting direction of stock price movement is notably important to provide a better guidance to assist market participants in making their investment decisions. This study presents a hybrid method combining machine learning methods with persistent homology to improve the prediction performance. Three stock prices namely Kuala Lumpur Composite Index, Kuala Lumpur Stock Exchange Industrial and Kuala Lumpur Stock Exchange Technology sampled from Kuala Lumpur Stock Exchange are selected for experimental evaluation. In particular, persistent homology was applied to obtain a new and useful input vectors of invariant topological features from returns of these stock prices for further classification task using machine learning methods such as logistic regression , artificial neural network , support vector machine and random forest to predict the next day movement direction of Kuala Lumpur Composite Index. For comparative analysis, we compare the proposed method with others, where the machine learning methods are applied independently on stock returns and also on technical indicators respectively. By using the average of prediction performances and pairwise model comparison method, these two evaluation measures revealed that machine learning methods with persistent homology produced better prediction performance. Our results also demonstrated that the combination of support vector machine with persistent homology generates the best outcome. In general, a combination of machine learning methods with persistent homology is an emerging and promising alternative tool for predicting direction of stock price movement.},
  archive      = {J_ASOC},
  author       = {Mohd Sabri Ismail and Mohd Salmi Md Noorani and Munira Ismail and Fatimah Abdul Razak and Mohd Almie Alias},
  doi          = {10.1016/j.asoc.2020.106422},
  journal      = {Applied Soft Computing},
  pages        = {106422},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Predicting next day direction of stock price movement using machine learning methods with persistent homology: Evidence from kuala lumpur stock exchange},
  volume       = {93},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Robust unsupervised dimensionality reduction based on
feature clustering for single-cell imaging data. <em>ASOC</em>,
<em>93</em>, 106421. (<a
href="https://doi.org/10.1016/j.asoc.2020.106421">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Biological data, and in particular imaging data, have experienced an exponential growth in terms of volume and complexity in the last few years, raising new challenges in the field of machine learning . Unsupervised problems are of particular relevance, as the generation of labels for the data is often labor-intensive, expensive or simply not possible. However, interpretability of the data and the results is key to extract new valuable knowledge from the large-scale datasets that are studied. This highlights the necessity of adequate unsupervised dimensionality reduction techniques that can lower the computational workload necessary to process the dataset, while at the same time providing information on its structure. This paper describes a framework that brings together previous proposals on unsupervised feature clustering , with the goal of providing a scalable, interpretable and robust dimensionality reduction on single-cell imaging data. The framework integrates several inter-feature dissimilarity measures , clustering algorithms , quality criteria to select the best feature clustering , and dimensionality reduction methods that are built on the clustering. For each of these components, several approaches proposed in previous works have been tested and evaluated on three use cases coming from two different imaging datasets, highlighting the best-performing components. Affinity clustering is applied for feature clustering for the first time. The results were validated using statistical tests, showing that many of the combinations tested lowered the complexity of the datasets while maintaining or improving the accuracy yielded by classifiers applied on them. The analysis highlighted affinity clustering as the best algorithm for feature clustering, with median differences of up to 8.9\% and 0.9\% in accuracy with respect to FSFS and hierarchical clustering . Representation entropy obtained a median difference of 13.0\% and 0.8\% with respect to class separability and silhouette index, respectively, as a robust unsupervised criterion to select the cluster set. Dissimilarities based on Pearson’s correlation performed slightly better than the alternatives, with a median improvement of 2.8\% with respect to the cosine distance.},
  archive      = {J_ASOC},
  author       = {Daniel Peralta and Yvan Saeys},
  doi          = {10.1016/j.asoc.2020.106421},
  journal      = {Applied Soft Computing},
  pages        = {106421},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Robust unsupervised dimensionality reduction based on feature clustering for single-cell imaging data},
  volume       = {93},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Performance risk assessment in public–private partnership
projects based on adaptive fuzzy cognitive map. <em>ASOC</em>,
<em>93</em>, 106413. (<a
href="https://doi.org/10.1016/j.asoc.2020.106413">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {High complexity exists underlying public–private partnership (PPP) projects due to their huge scale, large investment, and long-term relationships among various participants, leading to difficulty in managing PPP project performance risk. A robust model that integrates the structural equation model (SEM) and fuzzy cognitive map (FCM) is proposed to perceive and assess the performance risk in PPP projects. SEM is used to learn causal relationships among critical factors representing PPP project performance from the data given. Based on the well-verified SEM, an adaptive FCM model consisting of 14 observed variables and 5 latent variables is built. The proposed approach is capable of performing predictive, diagnostic, and hybrid analysis in various scenarios. Results indicate that variables, including project characteristics (A), project participants (B), project input (C), and project progress (D), all display positive correlations with the target performance (T). Particularly, variables C and D are identified to be more sensitive in ensuring the project satisfactory performance than variables A and B. The optimal risk mitigation strategy can be discovered when the project performance is under an unsatisfactory level. It is found that upgrading the variable with a higher priority would be more efficient to improve the target performance than the variable with a lower priority, which is helpful in both generic and specific situations. The novelty of this research lies in the development of an adaptive FCM model that is capable of learning casual relationships from observed data and assessing risk subjected to uncertainty, subjectivity, and interdependence. The developed model can be used to provide insights into a better understanding of risk mitigation strategies through what-if scenario analysis, enabling to enhance the likelihood of success in PPP projects.},
  archive      = {J_ASOC},
  author       = {Hongyu Chen and Limao Zhang and Xianguo Wu},
  doi          = {10.1016/j.asoc.2020.106413},
  journal      = {Applied Soft Computing},
  pages        = {106413},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Performance risk assessment in public–private partnership projects based on adaptive fuzzy cognitive map},
  volume       = {93},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A new fuzzy strategy for size and topology optimization of
truss structures. <em>ASOC</em>, <em>93</em>, 106412. (<a
href="https://doi.org/10.1016/j.asoc.2020.106412">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most of the metaheuristic optimization techniques are the general optimization tools with the ability of solving various class of problems. However, in the case of more complex engineering problems, adding extra customizing procedure(s) can improve their performance. In this regard, the current study deals with introducing a new auxiliary fuzzy decision mechanism to enhance these methods’ capability on handling the structural size and topology optimization problems. The proposed mechanism aims to reduce the complexity level of the structural size and topology optimization problems by converting their complex search spaces into simpler fuzzy domains. Introduced fuzzy mechanism, during the optimization process, permanently monitors the population updating process and emphasizes either size or topology search behavior of each agent. Proposed mechanism evaluates agents via two predefined concepts so-called Normalized Objective Function ( NOF i NOFi ) and Normalized Members Density ( NMD i NMDi ). Since the presented fuzzy strategy designed as an independent regulator module, it can be integrated with different optimization algorithms . In the current work, it is combined with the Interactive Search Algorithm (ISA) optimization method and the compound method is named as Fuzzy Tuned Interactive Search Algorithm (FTISA). Eventually, its performance is comparatively assessed on solving a number of structural size and topology optimization problems with dynamic and static constraints. Achieved results demonstrate that the introduced fuzzy strategy not only significantly enhances the computational cost of the process but also improves the accuracy of the solutions and stability of the algorithm.},
  archive      = {J_ASOC},
  author       = {Ali Mortazavi},
  doi          = {10.1016/j.asoc.2020.106412},
  journal      = {Applied Soft Computing},
  pages        = {106412},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A new fuzzy strategy for size and topology optimization of truss structures},
  volume       = {93},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Multi-objective scheduling strategy for scientific workflows
in cloud environment: A firefly-based approach. <em>ASOC</em>,
<em>93</em>, 106411. (<a
href="https://doi.org/10.1016/j.asoc.2020.106411">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cloud computing is a distributed computing paradigm, that provides infrastructure and services to the users using the pay-as-you-use billing model. With the increasing demands and diversity of the scientific workflows, the cloud providers face a fundamental issue of resource provisioning and load balancing. Although, the workflow scheduling in the cloud environment is extensively studied, however, most of the strategies ignore to consider the multiple conflicting objectives of the workflows for scheduling and resource provisioning . To address the above-mentioned issues, in the paper, we introduce a new workflow scheduling strategy using the Firefly algorithm (FA) by considering multiple conflicting objectives including workload of cloud servers, makespan, resource utilization, and reliability. The main purpose of the FA is to find a suitable cloud server for each workflow that can meet its requirements while balancing the loads and resource utilization of the cloud servers. In addition, a rule-based approach is designed to assign the tasks on the suitable VM instances for minimizing the makespan of the workflow while meeting the deadline. The proposed scheduling strategy is evaluated over Google cluster traces using various simulation runs. The control parameters of the FA are also thoroughly investigated for better performance. Through the experimental analysis, we prove that the proposed strategy performs better than the state-of-the-art-algorithms in terms of different Quality-of-Service (QoS) parameters including makespan, reliability, resource utilization and loads of the cloud servers.},
  archive      = {J_ASOC},
  author       = {Mainak Adhikari and Tarachand Amgoth and Satish Narayana Srirama},
  doi          = {10.1016/j.asoc.2020.106411},
  journal      = {Applied Soft Computing},
  pages        = {106411},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Multi-objective scheduling strategy for scientific workflows in cloud environment: A firefly-based approach},
  volume       = {93},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). SVM kernel based on particle swarm optimized vector and
bayesian optimized SVM in atmospheric particulate matter forecasting.
<em>ASOC</em>, <em>93</em>, 106410. (<a
href="https://doi.org/10.1016/j.asoc.2020.106410">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The application of Artificial Intelligence (AI) has been upgraded in many scientific fields the last years, with the development of new artificial intelligence-based technologies and techniques. Considering that in the literature there is a very limited number of studies proposing and testing new SVM kernels in regression problems , this research introduces a novel SVM Kernel by incorporating a transformed particle swarm optimized ANN weight vector in a Bayesian optimized SVM kernel in a time series problem for predicting the atmospheric pollutant factor Particulate Matter 10 (PM10). The proposed model introduces a new SVM kernel that illustrates an increased forecasting accuracy compared to the conventional optimized ANN and SVM models according to the experimental results. The findings of the proposed methodology illustrate that the new proposed SVM Kernel can be utilized as an improved forecasting technique.},
  archive      = {J_ASOC},
  author       = {Georgios N. Kouziokas},
  doi          = {10.1016/j.asoc.2020.106410},
  journal      = {Applied Soft Computing},
  pages        = {106410},
  shortjournal = {Appl. Soft. Comput.},
  title        = {SVM kernel based on particle swarm optimized vector and bayesian optimized SVM in atmospheric particulate matter forecasting},
  volume       = {93},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Feature selection based on improved binary global harmony
search for data classification. <em>ASOC</em>, <em>93</em>, 106402. (<a
href="https://doi.org/10.1016/j.asoc.2020.106402">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Harmony search (HS) is an effective meta-heuristic algorithm inspired by the music improvisation process, where musicians search for a pleasing harmony by adjusting their instruments’ pitches. The HS algorithm and its variants have been widely used to solve binary and continuous optimization problems . In this paper, we propose an improved binary global harmony search algorithm, called IBGHS, to undertake feature selection problems. A modified improvisation step is introduced to enhance the global search ability and increase the convergence speed of the algorithm. In addition, the K K -nearest neighbor (KNN) is used as an underlying learning model to evaluate the effectiveness of the selected feature subsets. The experimental results on eighteen benchmark problems indicate that the proposed IBGHS algorithm is able to produce comparable results as compared with other state-of-the-art population-based methods such as genetic algorithm (GA), particle swarm optimization (PSO), antlion optimizer (ALO), novel global harmony search (NGHS) and whale optimization algorithm (WOA) in solving feature selection problems.},
  archive      = {J_ASOC},
  author       = {Jafar Gholami and Farhad Pourpanah and Xizhao Wang},
  doi          = {10.1016/j.asoc.2020.106402},
  journal      = {Applied Soft Computing},
  pages        = {106402},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Feature selection based on improved binary global harmony search for data classification},
  volume       = {93},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Using deep learning for price prediction by exploiting
stationary limit order book features. <em>ASOC</em>, <em>93</em>,
106401. (<a href="https://doi.org/10.1016/j.asoc.2020.106401">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The recent surge in Deep Learning (DL) research of the past decade has successfully provided solution to many difficult problems. The field of Quantitative analysis has been slowly adapting the new methods to its problems, but due to problems such as the non-stationary nature of financial data, significant challenges must be overcome before DL is fully utilized. In this work a new method to construct stationary features is proposed such that allows DL models to be applied effectively. These features are thoroughly tested on the task of predicting mid price movements of the Limit Order Book. Several DL models are evaluated such as recurrent Long Short Term Memory (LSTM) networks and Convolutional Neural Networks (CNN). Finally a novel model that combines the ability of the CNN to extract useful features and the ability of LSTMs’ to analyse time series, is proposed and evaluated. The combined model is able to outperform the individual LSTM and CNN models in the prediction horizons that are tested.},
  archive      = {J_ASOC},
  author       = {Avraam Tsantekidis and Nikolaos Passalis and Anastasios Tefas and Juho Kanniainen and Moncef Gabbouj and Alexandros Iosifidis},
  doi          = {10.1016/j.asoc.2020.106401},
  journal      = {Applied Soft Computing},
  pages        = {106401},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Using deep learning for price prediction by exploiting stationary limit order book features},
  volume       = {93},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Neuro-fuzzy system dynamics technique for modeling
construction systems. <em>ASOC</em>, <em>93</em>, 106400. (<a
href="https://doi.org/10.1016/j.asoc.2020.106400">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The performance of construction systems (e.g., activities, operations, projects) is commonly measured using different indicators, such as productivity or production rate. The accurate prediction of performance, which is an important concern of construction researchers and practitioners, requires effective techniques for construction modeling. However, the complexity of construction systems creates three challenges for construction modeling: (1) construction systems are affected by numerous interacting factors, (2) the factors that affect construction systems often exhibit both probabilistic and non-probabilistic uncertainty, and (3) construction systems are dynamic. Fuzzy system dynamics (FSD) is a simulation technique that can be used for modeling construction systems with the potential to address these three challenges. However, the application of FSD in construction is still limited due to its low accuracy for modeling the non-linear, complex, and highly-dimensional relationships that exist between the system variables. Currently, these system relationships are most often defined in FSD by linear regression, due its computational simplicity. This paper introduces a new hybrid technique – neuro-fuzzy system dynamics (N-FSD) – by integrating FSD with hybrid neuro-fuzzy systems. In N-FSD, hybrid neuro-fuzzy systems are used to define the non-linear, complex and high-dimensional relationships between the system variables, which improves the accuracy of FSD models in construction applications. The applicability of the N-FSD technique is tested through a construction case study by modeling the production rate of earthmoving operations.},
  archive      = {J_ASOC},
  author       = {Nima Gerami Seresht and Aminah Robinson Fayek},
  doi          = {10.1016/j.asoc.2020.106400},
  journal      = {Applied Soft Computing},
  pages        = {106400},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Neuro-fuzzy system dynamics technique for modeling construction systems},
  volume       = {93},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). FUZZ-EQ: A data equalizer for boosting the discrimination
power of fuzzy classifiers. <em>ASOC</em>, <em>93</em>, 106399. (<a
href="https://doi.org/10.1016/j.asoc.2020.106399">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The definition of linguistic terms is a critical part of the construction of any fuzzy classifier. Fuzzy partitioning methods (FPMs) range from simple uniform partitioning to sophisticated optimization algorithms . In this paper we present FUZZ-EQ, a preprocessing algorithm that facilitates the construction of meaningful fuzzy partitions regardless of the FPM used. The proposed approach is radically different from any existing FPM: instead of adjusting the fuzzy sets to the training data, FUZZ-EQ adjusts the training data to a hypothetical uniform partition before applying any FPM. To do so, the original data distribution is transformed into a uniform distribution by applying the probability integral transform. FUZZ-EQ allows FPMs to provide classifiers with more granularity on high density regions, increasing the overall discrimination capability. Additionally, we describe the procedure to reverse this transformation and recover the interpretability of linguistic terms . To assess the effectiveness of our proposal, we conducted an extensive empirical study consisting of 41 classification tasks and 9 fuzzy classifiers with different FPMs, rule induction algorithms, and rule structures. We also tested the scalability of FUZZ-EQ in Big Data classification problems such as HIGGS, with 11 million examples. Experimental results reveal that FUZZ-EQ significantly boosted the classification performance of those classifiers using the same linguistic terms for all rules, including state-of-the-art classifiers such as FARC-HD or IVTURS.},
  archive      = {J_ASOC},
  author       = {Mikel Uriz and Mikel Elkano and Humberto Bustince and Mikel Galar},
  doi          = {10.1016/j.asoc.2020.106399},
  journal      = {Applied Soft Computing},
  pages        = {106399},
  shortjournal = {Appl. Soft. Comput.},
  title        = {FUZZ-EQ: A data equalizer for boosting the discrimination power of fuzzy classifiers},
  volume       = {93},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Constructing grey prediction models using grey relational
analysis and neural networks for magnesium material demand forecasting.
<em>ASOC</em>, <em>93</em>, 106398. (<a
href="https://doi.org/10.1016/j.asoc.2020.106398">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In terms of environmental protection, magnesium is a lightweight material that has been widely used to manufacture components for electronics. By forecasting the demand for magnesium materials, we can evaluate its prospects in the related industries. Grey prediction is appropriate for this study, because there is limited available data on the demand for magnesium, and it does not coincide with the statistical assumptions. Therefore, this study applies the GM(1, 1) model, which is the most frequently used grey prediction model, to forecast the demand for magnesium materials. To improve the accuracy of predictions with the GM(1, 1) model, its residual modification was established by the neural network . In particular, this study used grey relational analysis to estimate the weight of each sample that was required to avoid unreasonably treating each sample with equal importance in the traditional grey prediction. The forecasting ability of the proposed grey residual modification models was verified using real data regarding the demand for magnesium materials. The results showed that the proposed prediction model performed well compared with the other prediction models considered.},
  archive      = {J_ASOC},
  author       = {Yi-Chung Hu},
  doi          = {10.1016/j.asoc.2020.106398},
  journal      = {Applied Soft Computing},
  pages        = {106398},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Constructing grey prediction models using grey relational analysis and neural networks for magnesium material demand forecasting},
  volume       = {93},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Solution of an EPQ model for imperfect production process
under game and neutrosophic fuzzy approach. <em>ASOC</em>, <em>93</em>,
106397. (<a href="https://doi.org/10.1016/j.asoc.2020.106397">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article deals with an Economic Production Quantity (EPQ) deteriorating inventory model for non-random uncertain environment. It includes rework process, screening of imperfect items and partial backlogging. The items are partially serviceable, because at the time of production some items are found to be defective which cannot be recoverable or serviceable. At first, we develop a cost minimization problem under several assumptions related to imperfect items and rework process under certain linear constraints . We solve the crisp model (primal nonlinear problem) first, and then we convert this model into equivalent game problem taking the help of the theories related to strong and weak duality theorem. However, this game problem consists of the Lagrangian function that correspond a nonlinear objective function subject to some linear constraints. The main objective of the study is to develop a solution procedure of the problem associated to an imperfect process where all unit cost components might increase or decrease neutrosophically. Thus, according to the experiences gained by the decision maker (DM) we fuzzify all cost components as sub-neutrosophic offset. To defuzzify the model we have utilized the sine cuts of neutrosophic fuzzy numbers followed by a solution procedure developed in solving the matrix game exclusively. To validate the model, a numerical example is studied then we have compared the optimal results among the original problem, the equivalent game problem and the game problem under neutrosophic environment explicitly. Our findings reveal that under negative α α -cuts the value of the objective function assumes lower and higher values. Finally, sensitivity analysis, graphical illustrations , conclusions and scope of future works have been discussed.},
  archive      = {J_ASOC},
  author       = {Sujit Kumar De and Prasun Kumar Nayak and Anup Khan and Kousik Bhattacharya and Florentin Smarandache},
  doi          = {10.1016/j.asoc.2020.106397},
  journal      = {Applied Soft Computing},
  pages        = {106397},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Solution of an EPQ model for imperfect production process under game and neutrosophic fuzzy approach},
  volume       = {93},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Ensemble-learning based neural networks for novelty
detection in multi-class systems. <em>ASOC</em>, <em>93</em>, 106396.
(<a href="https://doi.org/10.1016/j.asoc.2020.106396">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In most real-world systems or processes, determining the complete set of classes during the training phase is generally impossible. There is a high chance that novelties or abnormal data can appear in future phases which might severely affect the performance of the machine learning system . Novelty detection is of great importance in many critical systems and domains, such as business intelligence , process monitoring, information security, clinical decision support etc. Most of the available methods for novelty detection use a one-class classification (OCC) criterion, i.e. treating multiple known classes as a single ”Normal” class, whose aim is to distinguish data samples between “Normal” and “Not Normal” classes. In this paper, the problem of novelty detection in multi-class systems is addressed through ensemble based learning of neural networks (EBNN), capable of both detecting novelties and classifying the known normal samples in future datasets. Moreover, the model is analogous to the semi-supervised learning system as it is trained using only the available normal classes. Evaluation of the proposed model (EBNN) on UCI machine learning datasets showed that the model not only outperforms other models in detecting novelties but also has a better multi-class classification accuracy for known normal classes. The proposed model implements a novel activation function in its framework and differs from the commonly available novelty detection models in three aspects. First, the model is much simpler to implement and does not need any initial assumptions about the model. Second, the model does not require any novel or abnormal data during training phase (semi-supervised learning). Third, it can be used as a two in one system to detect novelties and at the same time to classify data based on known classes.},
  archive      = {J_ASOC},
  author       = {Felix T.S. Chan and Z.X. Wang and S. Patnaik and M.K. Tiwari and X.P. Wang and J.H. Ruan},
  doi          = {10.1016/j.asoc.2020.106396},
  journal      = {Applied Soft Computing},
  pages        = {106396},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Ensemble-learning based neural networks for novelty detection in multi-class systems},
  volume       = {93},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Mathematical model and bee algorithms for mixed-model
assembly line balancing problem with physical human–robot collaboration.
<em>ASOC</em>, <em>93</em>, 106394. (<a
href="https://doi.org/10.1016/j.asoc.2020.106394">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The collaboration of human workers and robots draws increasing attention from the manufacturing enterprises to embrace the Industry 4.0 paradigm in a competitive way. Motivated by the requirements of collaboration between human workers and robots in assembly lines , this study investigates the mixed-model assembly line balancing (MMALB) problem with the collaboration between human workers and robots. A mixed-integer linear programming (MILP) model is formulated to tackle the small-size problems optimally to minimize the sum of cycle times of models. Also, bee algorithm (BA) and artificial bee colony (ABC) algorithm are implemented and improved to solve the large-size problems due to the NP-hardness of this problem. The proposed BA algorithm utilizes a new employed bee phase to accelerate the evolution of the swarm and new scout phase to escape from being trapped into local optima and produce a high-quality and diverse population. The developed ABC proposes a new onlooker phase to accelerate the evolution of the whole swarm by removing the poor-quality solutions, new scout phase to achieve high-quality solutions while preserving the diversity of the swarm, and local search to enhance exploitation capacity. Computational study on a set of generated instances indicates that the improvements enhance the BA and ABC algorithm by a significant margin, and the proposed BA and ABC algorithm achieve competing performance in comparison with nine other algorithms, including the late acceptance hill-climbing algorithm, simulated annealing algorithm , genetic algorithm , particle swarm optimization algorithm, discrete cuckoo search algorithm , the original bee algorithm, and three artificial bee colony algorithms.},
  archive      = {J_ASOC},
  author       = {Zeynel Abidin Çil and Zixiang Li and Suleyman Mete and Eren Özceylan},
  doi          = {10.1016/j.asoc.2020.106394},
  journal      = {Applied Soft Computing},
  pages        = {106394},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Mathematical model and bee algorithms for mixed-model assembly line balancing problem with physical human–robot collaboration},
  volume       = {93},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Crisscross differential evolution algorithm for constrained
hydrothermal scheduling. <em>ASOC</em>, <em>93</em>, 106393. (<a
href="https://doi.org/10.1016/j.asoc.2020.106393">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes a novel chaotic-crisscross differential evolution (CCDE) algorithm to realize an optimal generation schedule of multi-chain short-term hydrothermal system over 24 hours’ time-horizon in a multi-objective framework, considering conflicting economic-environmental aspects of thermal units. The equality constraints of active power balance and the amount of available water are independently handled using variable elimination method. However, the statistical uncertainties called residues arise due to infringements of equality constraints while adjusting the violated dependent variables within their boundaries. These residues are fuzzy quantified within their prescribed bounds, and are embedded as objectives to be optimized. An interactive unified fuzzy satisfying function is aimed to solve the conflict of three objectives. The global solution accuracy and convergence rate of stochastic algorithms are significantly affected by parameter-tuning, exploration and exploitation strategies. The proposed algorithm integrates dual crisscross mechanism orthogonally with chaotically tuned DE to balance exploration and exploitation. Information collected about non-dominated solutions from search space is processed using opposition-based learning for better accuracy of global solution in three-dimensional objective function hyperspace. The numerical results show improvement in unified satisfying objective function and convergence performance metrics over the existing methods. The competence of the proposed algorithm is confirmed through illustrations on benchmark functions and is substantiated through statistical significance tests.},
  archive      = {J_ASOC},
  author       = {Manbir Kaur and J.S. Dhillon and D.P. Kothari},
  doi          = {10.1016/j.asoc.2020.106393},
  journal      = {Applied Soft Computing},
  pages        = {106393},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Crisscross differential evolution algorithm for constrained hydrothermal scheduling},
  volume       = {93},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Dynamic differential annealed optimization: New
metaheuristic optimization algorithm for engineering applications.
<em>ASOC</em>, <em>93</em>, 106392. (<a
href="https://doi.org/10.1016/j.asoc.2020.106392">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This work proposes a novel optimization algorithm which can be used to solve a wide range of mathematical optimization problems where the global minimum or maximum is required. The new algorithm is based on random search and classical simulated annealing algorithm (it mimics the modern process of producing high-quality steel) and is designated dynamic differential annealed optimization (DDAO). The proposed algorithm was benchmarked for 51 test functions. The dynamic differential annealed optimization algorithm has been compared to a large number of highly cited optimization algorithms. Over numerical tests, DDAO has outperformed some of these algorithms in many cases and shown high performance. Constrained path planning and spring design problem were selected as a practical engineering optimization problem. DDAO converged to the global minimum of problems efficiently, and for spring design problem DDAO has found the best feasible solution than what is found by many algorithms.},
  archive      = {J_ASOC},
  author       = {Hazim Nasir Ghafil and Károly Jármai},
  doi          = {10.1016/j.asoc.2020.106392},
  journal      = {Applied Soft Computing},
  pages        = {106392},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Dynamic differential annealed optimization: New metaheuristic optimization algorithm for engineering applications},
  volume       = {93},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). An artificial bee colony algorithm with adaptive
heterogeneous competition for global optimization problems.
<em>ASOC</em>, <em>93</em>, 106391. (<a
href="https://doi.org/10.1016/j.asoc.2020.106391">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Artificial bee colony (ABC) algorithm is an efficient bio-inspired optimizer proposed recently. Though it has gained great popularity, ABC suffers from its slow convergence and poor generalization on various problem landscapes. To address the issues, an augmented ABC with adaptive heterogeneous competition (ABC-AHC) is proposed in this study. In ABC-AHC, two bee swarms with each conducting heterogeneous but complementary capabilities are implemented to improve the search capabilities on various problem spaces. To dynamically adjust the search behaviors , an adaptive mechanism is developed to trigger the competition and migration between the bee swarms. Comparative studies are conducted for parameter tuning and the heterogeneous searching (HST). Existing algorithms including ABC variants and non-ABC variants are adopted to validate the performance of ABC-AHC. Numerical comparisons are conducted on 30D and 100D benchmark functions , CEC 2014 test function, random function and the real-world problems. Numerical results demonstrate that the proposed strategies significantly enhance ABC’s search capability and convergence speed on the various benchmark functions .},
  archive      = {J_ASOC},
  author       = {Xianghua Chu and Fulin Cai and Da Gao and Li Li and Jianshuang Cui and Su Xiu Xu and Quande Qin},
  doi          = {10.1016/j.asoc.2020.106391},
  journal      = {Applied Soft Computing},
  pages        = {106391},
  shortjournal = {Appl. Soft. Comput.},
  title        = {An artificial bee colony algorithm with adaptive heterogeneous competition for global optimization problems},
  volume       = {93},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Energy load time-series forecast using decomposition and
autoencoder integrated memory network. <em>ASOC</em>, <em>93</em>,
106390. (<a href="https://doi.org/10.1016/j.asoc.2020.106390">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the increasing population and rising living standard, the demand for energy and materials have increased to a greater extent. The accurate estimation of increasing electricity demand is prerequisite for strategies planning, improving revenue, reducing power wastage and stable operation of the energy demand management system. Recent advancements in the field of electricity load forecasting provide powerful tools to capture non-linear energy demand trends and outperform conventional load prediction models. However, the existing demand prediction models suffer from some significant shortcomings that need to be addressed for improved prediction accuracy. In this context, the current research work proposes a deep learning based hybrid approach which firstly implements Variational Mode Decomposition (VMD) and Autoencoder models to extract meaningful sub-signals/features from the data. Subsequently, a Long Short-term Memory (LSTM) network model is trained for each sub-signal to forecast electricity demand by utilizing historical, seasonal and timestamp data dependencies . The support for incorporating seasonal and timestamp information to LSTM model is provided through the agglomerative clustering algorithm . Furthermore, an error variance modelling strategy is also employed to enhance the prediction accuracy of the proposed approach. The experiments are conducted on electricity consumption dataset of Himachal Pradesh, India. Performance assessment of the proposed approach is made by comparing prediction results with Support Vector Regression (SVR), Recurrent Neural Network (RNN), Deep Belief Network (DBN) and EMD+LSTM models. The experimental results demonstrate that the proposed model outperforms other state-of-the-art demand forecasting models and has the lowest MAPE (3.04\%).},
  archive      = {J_ASOC},
  author       = {Jatin Bedi and Durga Toshniwal},
  doi          = {10.1016/j.asoc.2020.106390},
  journal      = {Applied Soft Computing},
  pages        = {106390},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Energy load time-series forecast using decomposition and autoencoder integrated memory network},
  volume       = {93},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Short-term photovoltaic power generation forecasting based
on random forest feature selection and CEEMD: A case study.
<em>ASOC</em>, <em>93</em>, 106389. (<a
href="https://doi.org/10.1016/j.asoc.2020.106389">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To mitigate solar curtailment caused by large-scale development of photovoltaic (PV) power generation, accurate forecasting of PV power generation is important. A hybrid forecasting model was constructed that combines random forest (RF), improved grey ideal value approximation (IGIVA), complementary ensemble empirical mode decomposition (CEEMD), the particle swarm optimization algorithm based on dynamic inertia factor (DIFPSO), and backpropagation neural network (BPNN), called RF-CEEMD-DIFPSO-BPNN. PV power generation is affected by many factors. The RF method is used to calculate the importance degree and rank the factors, then eliminate the less important factors. Then, the importance degree calculated by RF is transferred as the weight values to the IGIVA model to screen the similar days of different weather types to improve the data quality of the training sets. Then, the original power sequence is decomposed into intrinsic mode functions (IMFs) at different frequencies and a residual component by CEEMD to weaken the fluctuation of the original sequence. We empirically analyzed a PV power plant to verify the effectiveness of the hybrid model, which proved that the RF-CEEMD-DIFPSO-BPNN is a promising approach in terms of PV power generation forecasting.},
  archive      = {J_ASOC},
  author       = {Dongxiao Niu and Keke Wang and Lijie Sun and Jing Wu and Xiaomin Xu},
  doi          = {10.1016/j.asoc.2020.106389},
  journal      = {Applied Soft Computing},
  pages        = {106389},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Short-term photovoltaic power generation forecasting based on random forest feature selection and CEEMD: A case study},
  volume       = {93},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Coupled application of deep learning model and quantile
regression for travel time and its interval estimation using data in
different dimensions. <em>ASOC</em>, <em>93</em>, 106387. (<a
href="https://doi.org/10.1016/j.asoc.2020.106387">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The rapid development of sensing and computing methods and their application to transportation engineering in recent years provide us data support to traffic flow prediction . However, the travel time prediction is still a complex and difficult task in the intelligent transportation system because of its nonlinear and nonstationary characteristics. In this study, a hybrid model coupling the deep learning model and the quantile regression (QR) has been proposed to achieve the deterministic and probabilistic travel time prediction. To consider multiple correlations of the traffic flow, a spatial–temporal state-space matrix has been developed. Then, a novel deep belief network stacked by several Gaussian Bernoulli Restricted Boltzmann Machine (GBRBM) to extract important features and a regression layer to finish the prediction were developed. Moreover, to strengthen the reliability of results, the QR was applied to generate a prediction interval. Using real-world data sets, the proposed hybrid model was evaluated and contrasted with several benchmark models . The results show the deep learning model outperform the shallow learning model. The prediction interval providing by QR is better than that provided by the traditional method. It indicates that our proposed hybrid model can obtain a more perfect and reliable prediction for travel time which is meaningful to the advanced traveler information system .},
  archive      = {J_ASOC},
  author       = {Linchao Li and Bin Ran and Jiasong Zhu and Bowen Du},
  doi          = {10.1016/j.asoc.2020.106387},
  journal      = {Applied Soft Computing},
  pages        = {106387},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Coupled application of deep learning model and quantile regression for travel time and its interval estimation using data in different dimensions},
  volume       = {93},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Unsupervised phase learning and extraction from
quasiperiodic multidimensional time-series data. <em>ASOC</em>,
<em>93</em>, 106386. (<a
href="https://doi.org/10.1016/j.asoc.2020.106386">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Automatic phase extraction is commonly done by digital markers or handcrafted feature detection that are designed specifically for one particular type of signal. A few more generalized ways are to apply Hilbert transform or complex wavelet convolution. However, they are limited to single-dimensional signals, and they require the input window to cover multiple signal periods to extract a smooth phase sequence. In this work, we propose a learning-based phase extraction method for multi-dimensional signals, consuming only unlabeled signal examples for training. A neural network architecture is designed to map a window of signal directly to a phase value under a key constraint that encourages two consecutive phase values to progress forward. The concept has been generalized to a broad range of applications with an adjustable input window, a flexible phase progression penalty design, and better training stability. The proposed method consistently outperforms the complex wavelet convolution in three synthetic signals by reducing the phase extraction error by 63\% while shortening the size of the observation window by over 90\%. An experiment with an augmented electrocardiogram (ECG) signal also produces a clean phase sequence with an input window that is over thirty times smaller. A qualitative test on eleven-dimensional kinematic signals from three repetitive upper limb movements has shown that the method can also capture the fluctuation in phase progression. With the flexibility, the labelless training, the accuracy, and the short observation window, this technique has a potential to be used in a variety of applications that need instantaneous phase extraction.},
  archive      = {J_ASOC},
  author       = {Prayook Jatesiktat and Guan Ming Lim and Christopher Wee Keong Kuah and Wei Tech Ang},
  doi          = {10.1016/j.asoc.2020.106386},
  journal      = {Applied Soft Computing},
  pages        = {106386},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Unsupervised phase learning and extraction from quasiperiodic multidimensional time-series data},
  volume       = {93},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A bi-objective home healthcare routing and scheduling
problem considering patients’ satisfaction in a fuzzy environment.
<em>ASOC</em>, <em>93</em>, 106385. (<a
href="https://doi.org/10.1016/j.asoc.2020.106385">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Home care services are an alternative answer to hospitalization, and play an important role in reducing the healthcare costs for governments and healthcare practitioners. To find a valid plan for these services, an optimization problem called the home healthcare routing and scheduling problem is motivated to perform the logistics of the home care services. Although most studies mainly focus on minimizing the total cost of logistics activities, no study, as far as we know, has treated the patients’ satisfaction as an objective function under uncertainty. To make this problem more practical, this study proposes a bi-objective optimization methodology to model a multi-period and multi-depot home healthcare routing and scheduling problem in a fuzzy environment. With regards to a group of uncertain parameters such as the time of travel and services as well as patients’ satisfaction, a fuzzy approach named as the Jimenez’s method, is also utilized. To address the proposed home healthcare problem, new and well-established metaheuristics are obtained. Although the social engineering optimizer (SEO) has been applied to several optimization problems, it has not yet been applied in the healthcare routing and scheduling area. Another innovation is to develop a new modified multi-objective version of SEO by using an adaptive memory strategy, so-called AMSEO. Finally, a comprehensive discussion is provided by comparing the algorithms based on multi-objective metrics and sensitivity analyses. The practicality and efficiency of the AMSEO in this context lends weight to the development and application of the approach more broadly.},
  archive      = {J_ASOC},
  author       = {Amir Mohammad Fathollahi-Fard and Abbas Ahmadi and Fariba Goodarzian and Naoufel Cheikhrouhou},
  doi          = {10.1016/j.asoc.2020.106385},
  journal      = {Applied Soft Computing},
  pages        = {106385},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A bi-objective home healthcare routing and scheduling problem considering patients’ satisfaction in a fuzzy environment},
  volume       = {93},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Deep learning for financial applications: A survey.
<em>ASOC</em>, <em>93</em>, 106384. (<a
href="https://doi.org/10.1016/j.asoc.2020.106384">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Computational intelligence in finance has been a very popular topic for both academia and financial industry in the last few decades. Numerous studies have been published resulting in various models. Meanwhile, within the Machine Learning (ML) field, Deep Learning (DL) started getting a lot of attention recently, mostly due to its outperformance over the classical models. Lots of different implementations of DL exist today, and the broad interest is continuing. Finance is one particular area where DL models started getting traction, however, the playfield is wide open, a lot of research opportunities still exist. In this paper, we tried to provide a state-of-the-art snapshot of the developed DL models for financial applications. We not only categorized the works according to their intended subfield in finance but also analyzed them based on their DL models. In addition, we also aimed at identifying possible future implementations and highlighted the pathway for the ongoing research within the field.},
  archive      = {J_ASOC},
  author       = {Ahmet Murat Ozbayoglu and Mehmet Ugur Gudelek and Omer Berat Sezer},
  doi          = {10.1016/j.asoc.2020.106384},
  journal      = {Applied Soft Computing},
  pages        = {106384},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Deep learning for financial applications: A survey},
  volume       = {93},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Automating detection and localization of myocardial
infarction using shallow and end-to-end deep neural networks.
<em>ASOC</em>, <em>93</em>, 106383. (<a
href="https://doi.org/10.1016/j.asoc.2020.106383">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Myocardial infarction (MI), also known as a heart attack, is one of the common cardiac disorders caused by prolonged myocardial ischemia. For MI patients, specifying the exact location of a heart muscle suffering from blood shortage or stoppage is of crucial importance. Automatic localization systems can support physicians for better decisions in emergency situations. Using 12-lead electrocardiogram, in this paper, two MI detection and localization methods are proposed with classic and end-to-end deep machine learning techniques . For the feature extraction phase, the classic approach performs a Discrete Wavelet Transform (DWT) and Principal Component Analysis (PCA) on the pre-processed signals followed by a shallow neural network (NN) for the classification phase . However, in the end-to-end residual deep learning technique, a Convolutional Neural Network (CNN) is directly employed on the pre-processed input signals. For specifying the infarcted region of myocardium, 6 classes of subdiagnosis are considered. Proposed models are verified with the Physikalisch-Technische Bundesanstalt (PTB) dataset, where the data of each patient is first grouped and then carefully partitioned to training, validation, and test datasets . The results of K-fold cross-validation indicate that the general model achieves over 98\% accuracy for both MI detection and localization with fewer number of feature sets compared to previous studies. Moreover, the end-to-end CNN model shows superior performance by achieving perfect results. Thus, with the larger size of CNN models, one may choose a perfect system that requires larger memory compared to another system that requires less computational power and accepts nearly 2\% of false positives and/or false negatives .},
  archive      = {J_ASOC},
  author       = {Kamal Jafarian and Vahab Vahdat and Seyedmohammad Salehi and Mohammadsadegh Mobin},
  doi          = {10.1016/j.asoc.2020.106383},
  journal      = {Applied Soft Computing},
  pages        = {106383},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Automating detection and localization of myocardial infarction using shallow and end-to-end deep neural networks},
  volume       = {93},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Multi-objective metaheuristics for discrete optimization
problems: A review of the state-of-the-art. <em>ASOC</em>, <em>93</em>,
106382. (<a href="https://doi.org/10.1016/j.asoc.2020.106382">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a state-of-the-art review on multi-objective metaheuristics for multi-objective discrete optimization problems (MODOPs). The relevant literature source and their distribution are presented firstly. We then review the literature from four perspectives, including existing multi-objective metaheuristics for MODOPs, application areas of MODOPs, performance metrics and test instances. Finally, some promising directions ranging from algorithms improvement to technical applications are outlined to inspire researchers to conduct research in related areas.},
  archive      = {J_ASOC},
  author       = {Qi Liu and Xiaofeng Li and Haitao Liu and Zhaoxia Guo},
  doi          = {10.1016/j.asoc.2020.106382},
  journal      = {Applied Soft Computing},
  pages        = {106382},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Multi-objective metaheuristics for discrete optimization problems: A review of the state-of-the-art},
  volume       = {93},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Extended TODIM method for CCUS storage site selection under
probabilistic hesitant fuzzy environment. <em>ASOC</em>, <em>93</em>,
106381. (<a href="https://doi.org/10.1016/j.asoc.2020.106381">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Carbon capture, utilization and storage (CCUS) technologies are effective for urgently dealing with climate change and reducing carbon dioxide (CO 2 ). The storage of CO 2 in deep strata often leads to CO 2 2 leakage due to geological and engineering reasons, which has a huge impact on humans and ecology. CO 2 storage site selection can be regarded as a multi-criteria decision-making (MCDM) problem. Decision makers do not always show completely rational and may have the preference of bounded rational behavior, which may affect the selection of CO 2 storage site. At the same time, criteria interaction is an interesting issue in multi-criteria decision-making. In this paper, we develop an extended novel TODIM method based on λ λ -fuzzy measure and Choquet integral to select the CO 2 storage site using evaluation information given by decision makers which can take the form of a probabilistic hesitant fuzzy set , whereby λ λ -fuzzy measure and Choquet integral are used to calculate the weights of criteria. The hamming distance measure between two probabilistic hesitant fuzzy element (P-HFE) is calculated and the gain and loss matrices for every criterion are obtained. Further, the overall values of all alternatives can be calculated to get the ranking order of CO 2 storage site. Decision makers can select a suitable CO 2 storage site according to ranking results. Finally, a CO 2 storage site selection example is used to describe the effectiveness of the proposed procedures. The sensitivity analysis also explores the influence of the loss aversion coefficient and the change of criteria weights on decision results.},
  archive      = {J_ASOC},
  author       = {Jian Guo and Jielin Yin and Ling Zhang and Zefu Lin and Xin Li},
  doi          = {10.1016/j.asoc.2020.106381},
  journal      = {Applied Soft Computing},
  pages        = {106381},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Extended TODIM method for CCUS storage site selection under probabilistic hesitant fuzzy environment},
  volume       = {93},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Local probabilistic model for bayesian classification: A
generalized local classification model. <em>ASOC</em>, <em>93</em>,
106379. (<a href="https://doi.org/10.1016/j.asoc.2020.106379">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In Bayesian classification , it is important to establish a probability distribution model, e.g., a Gaussian distribution for each class for probability estimation . Most of the previous methods modeled the probability distribution in the whole sample space. However, real-world problems are usually too complex to model in the whole sample space; some fundamental assumptions are required to simplify the global model, for example, the class conditional independence assumption for naive Bayesian classification . In this paper, with the insight that the distribution in a local sample space should be simpler than that in the whole sample space, a local probabilistic model established for a local region is expected much simpler and can relax the fundamental assumptions that may not be true in the whole sample space. Based on these advantages we propose establishing local probabilistic models for probability estimation in Bayesian classification. In addition, a Bayesian classifier adopting a local probabilistic model can even be viewed as a generalized local classification model ; by tuning the size of the local region and the corresponding local model assumption, a fitting model can be established for a particular classification problem. The experimental results on several real-world datasets demonstrate the effectiveness of local probabilistic models for Bayesian classification.},
  archive      = {J_ASOC},
  author       = {Chengsheng Mao and Lijuan Lu and Bin Hu},
  doi          = {10.1016/j.asoc.2020.106379},
  journal      = {Applied Soft Computing},
  pages        = {106379},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Local probabilistic model for bayesian classification: A generalized local classification model},
  volume       = {93},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A new game-theoretical multi-objective evolutionary approach
for cash-in-transit vehicle routing problem with time windows (a real
life case). <em>ASOC</em>, <em>93</em>, 106378. (<a
href="https://doi.org/10.1016/j.asoc.2020.106378">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cash transfer from a central treasury to bank branches, which is with high security, is one of the crucial processes in the banking system. In this paper, a new multi-objective game theory-based model is developed to increase the security of cash-in-transit. For this purpose and in order to reduce the transportation costs, a bi-objective vehicle routing problem with time window is developed where the risk of transfers (including armed robbers attack and theft) and the distance traveled by vehicles are minimized. In order to better estimate the robber’s performance, the probability of robber’s ambush is calculated by the game theory approach, in such a way that a two-player, zero-sum game is played between the robber and the cash carrier. The probability of theft success is also estimated in the proposed approach through a multiple-criteria decision-making and in order to be further representative of real-life situations. A periodic review is also added to the proposed model to increase the cash transport security in which the previously used links would enjoy less chance of choosing in the current period. Moreover, a new multi-objective hybrid genetic algorithm incorporated with a number of new heuristics and operators is developed to tackle the proposed model. The efficiency and effectiveness of the algorithm are examined through several standard data sets, and the results indicate the effectiveness of the proposed solution algorithm . The wide applicability of our proposed approach in real-life situations is examined with a real case study as well.},
  archive      = {J_ASOC},
  author       = {Seyed Farid Ghannadpour and Fatemeh Zandiyeh},
  doi          = {10.1016/j.asoc.2020.106378},
  journal      = {Applied Soft Computing},
  pages        = {106378},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A new game-theoretical multi-objective evolutionary approach for cash-in-transit vehicle routing problem with time windows (A real life case)},
  volume       = {93},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Cognitive fuzzy sets for decision making. <em>ASOC</em>,
<em>93</em>, 106374. (<a
href="https://doi.org/10.1016/j.asoc.2020.106374">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The intuitionistic fuzzy set (IFS) is useful in information expression but the cognitive overlap of people might cause the situation where the membership degree plus non-membership degree is greater than 1. The linear programming techniques for multidimensional analysis of preference (LINMAP) method is a well-known decision-making approach but little research focused on the interactive multiple criteria decision making (MCDM) problems within the LINMAP framework. This paper aims to propose a cognitive fuzzy set (CFS) to overcome the drawbacks of IFSs, and then develop an interactive cognitive fuzzy LINMAP (CF-LINMAP) method. To achieve these goals, the joint degree, which is caused by the cognitive overlap, of an IFS is interpreted, and then the concept of the CFS is proposed. We define the distance measure and comparison of CFSs, based on which, the cognitive fuzzy consistency and inconsistency indexes are calculated. The irrational index, which represents the irrational degree of experts, is introduced to improve the consistency and inconsistency indexes at each decision-making stage. Afterwards, an interactive CF-LINMAP method is proposed to deal with MCDM problems. An illustration concerning the hospital internal supply chain selection is given to show the applicability of the interactive CF-LINMAP method. Finally, the sensitivity analysis is done to demonstrate the reliability of the results.},
  archive      = {J_ASOC},
  author       = {Lisheng Jiang and Huchang Liao},
  doi          = {10.1016/j.asoc.2020.106374},
  journal      = {Applied Soft Computing},
  pages        = {106374},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Cognitive fuzzy sets for decision making},
  volume       = {93},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A fuzzy type-2 fault detection methodology to minimize false
alarm rate in induction motor monitoring applications. <em>ASOC</em>,
<em>93</em>, 106373. (<a
href="https://doi.org/10.1016/j.asoc.2020.106373">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Automatic routines for Fault Detection and Diagnosis (FDD) are very important in industrial monitoring systems. However, false alarms potentially occur. High false alarm rates may lead to outages and consequent losses in the production process. To address such problem, a new FDD strategy based on type-2 fuzzy systems is proposed herein to minimize the false alarm rate. By applying system identification techniques, parametric models are estimated in order to represent the operation of the system under several levels of fault severity. The test system is a detailed dynamic nonlinear model of induction motor drive. The faults considered were partial short-circuit in stator winding coils. A performance comparison was made by implementing the monitoring system with both a type-2 fuzzy system interval and a type-1 fuzzy system. The results obtained thereby showed the improved performance and robustness of type-2 fuzzy system-based monitoring system, which outdoes the performance obtained by a type-1 fuzzy system. Furthermore, the performance of the proposed type-2 fuzzy system-based monitoring system may be further improved by using a Genetic Algorithm for tuning the parameters of the fuzzy type-2 system.},
  archive      = {J_ASOC},
  author       = {Erick Melo Rocha and WalterBarra Junior and Kevin E. Lucas and Carlos Tavares da Costa Júnior and José Gracildo de Carvalho Júnior and Renan Landau Paiva de Medeiros and Fabrício Gonzalez Nogueira},
  doi          = {10.1016/j.asoc.2020.106373},
  journal      = {Applied Soft Computing},
  pages        = {106373},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A fuzzy type-2 fault detection methodology to minimize false alarm rate in induction motor monitoring applications},
  volume       = {93},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Fuzzy finite-time stable compensation control for a building
structural vibration system with actuator failures. <em>ASOC</em>,
<em>93</em>, 106372. (<a
href="https://doi.org/10.1016/j.asoc.2020.106372">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper investigates finite-time stability vibration control for a building structure system in the presence of actuator failure . The dynamic compensation approach for seismic waves improves the control performance by considering such waves as an unknown nonlinear item in the system. In the control design, this item is approximated via the adaptive fuzzy control method . In addition, it is considered that actuators can fail and therefore lessen, the effectiveness of the suppression of building structure vibration. An adaptive failure compensation method is proposed to address this problem. Moreover, to rapidly suppress vibration, a finite-time active vibration controller is designed in combination with a failure compensation method. Under the proposed control strategy, the building structure system with uncertain actuator failures can be guaranteed to be stable in finite time. Finally, examples of different failure cases are presented to demonstrate the anti-seismic effectiveness of the proposed method.},
  archive      = {J_ASOC},
  author       = {Jianhui Wang and Yunchang Huang and Tao Wang and Chunliang Zhang and Yan hui Liu},
  doi          = {10.1016/j.asoc.2020.106372},
  journal      = {Applied Soft Computing},
  pages        = {106372},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Fuzzy finite-time stable compensation control for a building structural vibration system with actuator failures},
  volume       = {93},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Modified two-phase fuzzy goal programming integrated with
IF-TOPSIS for green supplier selection. <em>ASOC</em>, <em>93</em>,
106371. (<a href="https://doi.org/10.1016/j.asoc.2020.106371">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The environmental consciousness of society and globally competitive market have considerably increased thanks to the scientific studies, media, governmental and non-governmental organizations. In this regard, environmental factors have been considered within the supplier selection process which is a major decision point in supply chains. Hence, in addition to the optimization of the traditional criteria, green criteria have also started to take its place in the supplier selection problem. In this study, an integrated methodology including the Intuitionistic Fuzzy Technique for Order Preference by Similarity to Ideal Solution (IF-TOPSIS) and a modified two-phase fuzzy goal programming model are proposed to better address this selection problem in a multi-item/multi-supplier/multi-period environment. The detailed steps are explicitly provided within the proposed methodology. In this respect, the criteria importance weights are determined via IF-TOPSIS which enables the opportunity to handle the vagueness within the evaluation process of decision-makers. Afterward, the obtained importance weights are used in the modified two-phase fuzzy goal programming model for selecting the best suppliers. An application in the air filter industry is performed to demonstrate the validation of the proposed methodology. Consequently, the proposed methodology successfully provides the best selection of suppliers by satisfying both classic and green criteria.},
  archive      = {J_ASOC},
  author       = {Huseyin Selcuk Kilic and Ahmet Selcuk Yalcin},
  doi          = {10.1016/j.asoc.2020.106371},
  journal      = {Applied Soft Computing},
  pages        = {106371},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Modified two-phase fuzzy goal programming integrated with IF-TOPSIS for green supplier selection},
  volume       = {93},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Online advertising assignment problem without free disposal.
<em>ASOC</em>, <em>93</em>, 106370. (<a
href="https://doi.org/10.1016/j.asoc.2020.106370">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents an online advertising assignment problem that generalizes the online version of the bipartite matching problem. Specifically, it focuses on the Display Ads problem, which is a generalization of the edge-weighted and capacitated matching problem. The display ads problem has been studied alongside the property of free disposal , in which an advertisement is allowed to be matched more times than its capacity. Although the problem with free disposal is tractable, the problem situation might be restricted and challenging to apply to other types of problems. The objective of this research on the display ads problem is to maximize the total weight of matched edges while considering a strict capacity constraint. This paper analyzes two online input orders (adversarial and probabilistic orders) to the problem. For the adversarial order, we design deterministic algorithms with worst-case guarantees and prove the competitive ratios of them. Upper bounds for the problem are also proposed. For the probabilistic order, stochastic online algorithms , consisting of scenario-based stochastic programming and Benders decomposition, are presented. We conduct numerical experiments of the stochastic online algorithm in two probabilistic order models (known IID and random permutation).},
  archive      = {J_ASOC},
  author       = {Gwang Kim and Ilkyeong Moon},
  doi          = {10.1016/j.asoc.2020.106370},
  journal      = {Applied Soft Computing},
  pages        = {106370},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Online advertising assignment problem without free disposal},
  volume       = {93},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Food package suggestion system based on multi-objective
optimization: A case study on a real-world restaurant. <em>ASOC</em>,
<em>93</em>, 106369. (<a
href="https://doi.org/10.1016/j.asoc.2020.106369">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ordering dishes in a restaurant is a significant task, which determines not only the customers’ dining experience, but also the restaurant’s reputation. However, assisting customers in ordering a satisfying food package (FP), i.e., a combination of dishes, remains a challenge. First, local restaurants usually have very limited information about their customers, except the number of customers and their budget. Thus, suggesting FPs that satisfy their budget as well as surprise their palate is very difficult. Second, as a real-world function, FPs are required to be generated in real time while addressing several realistic issues such as dynamic dish inventories. In this study, we first extract knowledge from the history of orders of a restaurant, such as correlations among dishes, to formulate the FP suggestion as a multi-objective optimization problem. Thereafter, we propose a knowledge-based multi-objective evolutionary algorithm (k-MOEA) to tackle the problem and generate the suggested FPs. In addition, we develop an intelligent dish-ordering system (iOrdering), including several designed online and offline mechanisms to meet the real-time requirements of the FP suggestion services. Finally, the effectiveness of the k-MOEA is evaluated quantitatively by comparing it with three categories of baselines. Moreover, we have deployed the iOrdering system in a hot pot restaurant chain, and a real-world experiment demonstrates the advanced user experience of the devised system, including more than 77\% acceptance rate of the suggested dishes and 66\% saving of ordering time.},
  archive      = {J_ASOC},
  author       = {Zhaoyuan Wang and Chuishi Meng and Shenggong Ji and Tianrui Li and Yu Zheng},
  doi          = {10.1016/j.asoc.2020.106369},
  journal      = {Applied Soft Computing},
  pages        = {106369},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Food package suggestion system based on multi-objective optimization: A case study on a real-world restaurant},
  volume       = {93},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Cultural coalitions detection approach using GPU based on
hybrid bat and cultural algorithms. <em>ASOC</em>, <em>93</em>, 106368.
(<a href="https://doi.org/10.1016/j.asoc.2020.106368">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Currently, robot technology plays a crucial role in our modern life. Its great favor over humanity was not limited to the industrial domain, but also on the social one. Social robots become equipped with an artificial culture that allows them to interact with humans. Although the great importance of this technology, some misdeeds can create a real catastrophe. Coalition creation is one of its most dangerous troubles, where a subset of robots cooperates to impose pernicious decisions. There are efficient coalitions detection methods, but their exponential computation time makes their use limited only to small data. This paper is an extended version of Kechid and Drias (2019) presented at the IEA/AIE 2019 conference. In this paper, we propose a Cultural coalitions detection approach using GPU based on hybrid Bat and Cultural Algorithms. Unlike the existing literature, We view the problem of finding coalitions as an optimization problem to get relevant solutions in a significantly reduced amount of time. The proposed approach can increase the population diversity and improve the searching ability for an optimal exploration–exploitation balance. Also, it can launch several cultural bats in GPU to make real parallelism . Experimental results on several datasets show that the proposed method will considerably reduce the runtime. These datasets represent the result of artificial cultural agents playing the colored trails (CT) game. Concerning the creation of profiles, we use real datasets generated based on the World Values survey.},
  archive      = {J_ASOC},
  author       = {Amine Kechid and Habiba Drias},
  doi          = {10.1016/j.asoc.2020.106368},
  journal      = {Applied Soft Computing},
  pages        = {106368},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Cultural coalitions detection approach using GPU based on hybrid bat and cultural algorithms},
  volume       = {93},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A memory-based grey wolf optimizer for global optimization
tasks. <em>ASOC</em>, <em>93</em>, 106367. (<a
href="https://doi.org/10.1016/j.asoc.2020.106367">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Grey Wolf Optimizer (GWO) is a new nature-inspired metaheuristic algorithm based on the leadership and social behaviour of grey wolves in nature. It has shown potential to solve several real-life applications, but still for some complex optimization tasks , it may face the problem of getting trapped at local optima and premature convergence. Therefore, in this study, to prevent from these drawbacks and to get a more stable sense of balance between exploitation and exploration, a new modified GWO called memory-based Grey Wolf Optimizer (mGWO) is proposed. In the mGWO, the search mechanism of the wolves is modified based on the personal best history of each individual wolves, crossover and greedy selection. These strategies help to enhance the global exploration, local exploitation and an appropriate balance between them during the search procedure. To investigate the effectiveness of the proposed mGWO, it has been tested on standard and complex benchmarks given in IEEE CEC 2014 and IEEE CEC 2017. Furthermore, some real engineering design problems and multilevel thresholding problem are also solved using the mGWO. The results analysis and its comparison with other algorithms demonstrate the better search-efficiency, solution accuracy and convergence rate of the proposed mGWO in performing the global optimization tasks .},
  archive      = {J_ASOC},
  author       = {Shubham Gupta and Kusum Deep},
  doi          = {10.1016/j.asoc.2020.106367},
  journal      = {Applied Soft Computing},
  pages        = {106367},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A memory-based grey wolf optimizer for global optimization tasks},
  volume       = {93},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Game theory based pixel approximation for remote sensing
imagery. <em>ASOC</em>, <em>93</em>, 106365. (<a
href="https://doi.org/10.1016/j.asoc.2020.106365">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Classification of remote sensing images faces several challenges due to mixed pixels. Such pixels that are wrongly classified are called mixed pixels. There is uncertainty about the class label of mixed pixels as they represent the average energy emitted from different objects present within their spatial extent . Rough set theory can address the vagueness in data. Therefore, in this work rough set concept is used to identify the mixed pixels. In a multi-player environment, the Game theory is a science of making rational decisions. We propose a game theory-based approach to approximate the mixed pixels to lower approximations of a class. For pixel approximation, we have applied the spatial information of neighbouring pixels . Experiment for the implementation of the proposed approach is carried on six Landsat 5 Thematic Mapper images with a different region of interest. These datasets vary in size and have different dominant classes. The impact of the proposed method on the quality of clusters is studied with the help of cluster quality parameters. The results of the percentage of approximation demonstrate the significant number of mixed pixels approximated to one of the class.},
  archive      = {J_ASOC},
  author       = {Aditya Raj and Sonajharia Minz},
  doi          = {10.1016/j.asoc.2020.106365},
  journal      = {Applied Soft Computing},
  pages        = {106365},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Game theory based pixel approximation for remote sensing imagery},
  volume       = {93},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Parameter-free fuzzy histogram equalisation with
illumination preserving characteristics dedicated for contrast
enhancement of magnetic resonance images. <em>ASOC</em>, <em>93</em>,
106364. (<a href="https://doi.org/10.1016/j.asoc.2020.106364">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Low-field MRI scanners do not offer sufficient image contrast. Hence, offline algorithms for improving image contrast are often needed. Even though modified versions of Histogram Equalisation (HE) are extensively used on panoramic images, they have serious limitations. Most of such modified algorithms have multiple operational parameters which need to be tuned manually. Parameter-free modifications lag in terms of illumination-preserving features. To address these issues, a novel formulation of Parameter-free Fuzzy Histogram Equalisation (PFHE) algorithm with good illumination-preserving characteristics, dedicated for contrast enhancement of MRI is introduced in this paper. In PFHE, a Homogeneity Fuzzy Sub-set (HFS) and its fuzzy complement, termed as Texture Fuzzy Sub-set (TFS) are computed based on the fuzzy similarity of the pixels in the input image with their eight-connected neighbours. Following this, an approximate output is estimated by applying a transformation similar to the histogram equalisation on the Fuzzy Textural Histogram (FTH) derived from TFS. The final output is computed as a nonlinear combination of the approximate output and the input image. The fuzzy weighting vectors used in the nonlinear combination are derived from the HFS. Both Qualitative and quantitative evaluations reveal that the PFHE is superior to Bi-Histogram Equalisation (BHE), Weighted Threshold Histogram Equalisation (WTHE), Contrast Limited Adaptive Histogram Equalisation (CLAHE), Non-parametric Modified Histogram Equalisation (NMHE), Exposure-based Sub-Image Histogram Equalisation (ESIHE), Median–Mean Based Sub-Image-Clipped Histogram Equalisation (MMSICHE) and Dominant Orientation-based Texture Histogram Equalisation (DOTHE), in terms of ability to preserve diagnostically significant features in the MR image.},
  archive      = {J_ASOC},
  author       = {Simi V.R. and Damodar Reddy Edla and Justin Joseph and Venkatanareshbabu Kuppili},
  doi          = {10.1016/j.asoc.2020.106364},
  journal      = {Applied Soft Computing},
  pages        = {106364},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Parameter-free fuzzy histogram equalisation with illumination preserving characteristics dedicated for contrast enhancement of magnetic resonance images},
  volume       = {93},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Water–energy–food nexus evaluation with a social network
group decision making approach based on hesitant fuzzy preference
relations. <em>ASOC</em>, <em>93</em>, 106363. (<a
href="https://doi.org/10.1016/j.asoc.2020.106363">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rapid increase and development of global population and economic, the Water–Energy–Food (WEF) nexus evaluation which is related to the sustainable development of human has become a hotspot. Whereas, the study of the WEF nexus evaluation from the perspectives of social network group decision making (SNGDM) is still a challenge. Hence, this paper aims to develop a trust-based SNGDM approach with hesitant fuzzy preference relations to the WEF nexus evaluation. In the proposed model, a new fuzzy adjacency matrix and trust score matrix based on expert’s self-confidence are defined to imply experts’ trust relationship and trust score, respectively. To improve the reliability of the final decision(s), an iterative algorithm is presented to improve the consistency of experts’ evaluations. Subsequently, the individual evaluation can be aggregated into a group one by using the trust score induced ordered weighted averaging operator while the trust scores of experts are the induced factors. Additionally, an algorithm is utilized to achieve a high level consensus in SNGDM of the WEF nexus evaluation. Finally, some comparison analyses and discussions show the feasibility and validity of the proposed method.},
  archive      = {J_ASOC},
  author       = {Nannan Wu and Yejun Xu and Xia Liu and Huimin Wang and Enrique Herrera-Viedma},
  doi          = {10.1016/j.asoc.2020.106363},
  journal      = {Applied Soft Computing},
  pages        = {106363},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Water–Energy–Food nexus evaluation with a social network group decision making approach based on hesitant fuzzy preference relations},
  volume       = {93},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A probabilistic linguistic-based deviation method for
multi-expert qualitative decision making with aspirations.
<em>ASOC</em>, <em>93</em>, 106362. (<a
href="https://doi.org/10.1016/j.asoc.2020.106362">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Probabilistic linguistic term set (PLTS) is a popular tool for modeling complex linguistic perceptions of decision-makers (DMs) and has gained successful applications in the field of multi-expert multi-criteria decision making (MEMCDM). In many probabilistic linguistic decision-making situations DMs are usually aspiration oriented in which the utilities of DMs do not depend on the absolute level of criteria values, but on the degree to which the criteria values match their aspirations levels . However, the aspirations of DMs are not considered in existing probabilistic linguistic decision making methods. One contribution of this paper is to introduce five probabilistic linguistic-based aspiration utility functions to take DMs’ aspirations into account. These functions can well describe the utility variation of DMs under different preference structures with the aspiration levels. Afterwards, the probabilistic linguistic-based indicator is defined to take into account the criteria weights represented by PLTSs, which greatly facilitates DMs to provide the weights of criteria, comparing with the use of crisp numbers. As the second contribution, we build a probabilistic linguistic-based deviation model to identify the decision results in MEMCDM. This model can achieve the goal that the decision results for group opinions are consistent with that for the individual DM’s opinions to the greatest extent. On the other hand, we also show with some counterexamples that the existing probabilistic linguistic distance measures are unreasonable. We present an improving distance measure for PLTSs and show its desirable properties . The biggest advantage of the developed distance measure is that it not only considers the deviation between the proportion information of linguistic terms but also takes into account linguistic terms themselves in PLTS.},
  archive      = {J_ASOC},
  author       = {Xiaolu Zhang and Huchang Liao and Bin Xu and Meifang Xiong},
  doi          = {10.1016/j.asoc.2020.106362},
  journal      = {Applied Soft Computing},
  pages        = {106362},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A probabilistic linguistic-based deviation method for multi-expert qualitative decision making with aspirations},
  volume       = {93},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Optimal driving based trip planning of electric vehicles
using evolutionary algorithms: A driving assistance system.
<em>ASOC</em>, <em>93</em>, 106361. (<a
href="https://doi.org/10.1016/j.asoc.2020.106361">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The existing driving assistance systems (DAS) are not capable to manage the electric vehicle (EV) problems namely insufficiency of charging stations and inadequate range. A novel DAS is presented here to extend the range and overcome other EV drawbacks by suggesting the driver an optimal driving strategy (ODS) continuously throughout trip performing. ODS is decided by solving a multi-objective optimization problem (MOOP), subsequently adopting a multi-criterion decision making technique. Implementation of the DAS in real application requires both better optimization results and low computational time. A study was carried out to investigate the DAS performance with four contending evolutionary algorithms (EAs), NSGAII (a non-dominated sorting multi-objective genetic algorithm), PESA (Pareto envelope-based selection algorithm), PAES (Pareto archived evolution strategy), and SPEA 2 (Strength Pareto evolutionary algorithm). After an initial investigation of EA performances based on different matrices, NSGAII and PESA were found to be most suitable. The natures of decision variables in the Pareto-optimal solutions were analyzed. After an extensive analysis based on different micro-trip structures, it was found that without considering the computational time, PESA solutions possess better convergence and diversity properties than NSGAII solutions. Various approaches were adopted to minimize DAS computation time considering both NSGAII and PESA without significantly compromising the solution’s optimality .},
  archive      = {J_ASOC},
  author       = {Mousumi Khanra and Arup Kr. Nandi},
  doi          = {10.1016/j.asoc.2020.106361},
  journal      = {Applied Soft Computing},
  pages        = {106361},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Optimal driving based trip planning of electric vehicles using evolutionary algorithms: A driving assistance system},
  volume       = {93},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A critical review on theoretical drawbacks and mathematical
incorrect assumptions in fuzzy OR methods: Review from 2010 to 2020.
<em>ASOC</em>, <em>93</em>, 106354. (<a
href="https://doi.org/10.1016/j.asoc.2020.106354">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As a practical approach for handling uncertainty, various fuzzy sets combined with traditional OR (operations research) methods have found huge applications. However, the fuzzy arithmetic is still a key challenge in this field. Moreover, some researchers have ignored the critical fact that traditional arithmetic operations will produce meaningless or questionable results when applied to fuzzy numbers in general. Thus, many papers in the field of OR have applied fuzzy numbers in a wrong way. Despite the very extensive use of combination of fuzzy sets with OR methods, literature shows that it has been less concentrated on the study of fallacy that may be generated using these methods. Motivated by some theoretical drawbacks and incorrect assumptions in fuzzy sets in conjunction with traditional methods in OR literature, the papers about these pitfalls are reviewed. The papers are included in this review, if they disclose directly a certain pitfall or incorrect assumption in the integration of fuzzy sets with OR methods. Because many other researchers may employ the same assumptions to solve real-life problems, the aim of this study is to make general users aware that many of these fuzzy arithmetic operations are incorrectly used and can lead to untrue and misleading consequences. In this paper, we review 102 papers published in 29 influential journals from 2010 to 25th, January, 2020. ISI Web of Science is adopted as the database of our review and the majority of selected journals (around 70\%) are ranked as Q1 determined by SJR. 1 To help scholars obtain quick information, the collected articles are summarized in a tabular layout. We hope that researchers who apply fuzzy-based methods in the field of OR would understand the serious problems and deep pitfalls associated with these models.},
  archive      = {J_ASOC},
  author       = {Alireza Sotoudeh-Anvari},
  doi          = {10.1016/j.asoc.2020.106354},
  journal      = {Applied Soft Computing},
  pages        = {106354},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A critical review on theoretical drawbacks and mathematical incorrect assumptions in fuzzy OR methods: Review from 2010 to 2020},
  volume       = {93},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Multi-scale channel importance sorting and spatial attention
mechanism for retinal vessels segmentation. <em>ASOC</em>, <em>93</em>,
106353. (<a href="https://doi.org/10.1016/j.asoc.2020.106353">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Retinal Vessels segmentation is an important procedure for detecting and diagnosing a variety of pathological diseases. However, the inherent complex properties around the disc make it challenging to improve the segmenting accuracy of capillaries and the retinal blood vessels at the ends. In this paper, we proposed a multi-scale channel importance sorting and important spatial information positioning (MSCS) encoder–decoder for segmentation in Retinal Vessels. Firstly, the fully convolutional encoder–decoder is formed to implement a series of linear and non-linear transformation and achieve end-to-end segmentation tasks . Then, the channel importance sorting module is employed to suppress useless feature responses during the process of encoding and to identify effective channels, whose information is utilized to recognize capillaries and the retinal vessels at the ends. Finally, in the decoding stage, the spatial attention mechanism module is designed to extract the positioning information of multi-scale feature maps. The spatial information of retinal vessels is collected to better locate the position of the vessels. In addition, aiming at taking fully advantage of the network, the multi-scale asymmetric cascade convolution module is proposed to reduce the parameters of the model and increase the operation rate. Experimental results on DRIVE, STARE datasets indicate that the proposed method outperforms other state-of-the-art strategies. This system, as demonstrated, can greatly decrease false positive rate of the blood vessels at the ends and enhance the sharpness of retinal vessels.},
  archive      = {J_ASOC},
  author       = {Xianlun Tang and Bing Zhong and Jiangping Peng and Bohui Hao and Jie Li},
  doi          = {10.1016/j.asoc.2020.106353},
  journal      = {Applied Soft Computing},
  pages        = {106353},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Multi-scale channel importance sorting and spatial attention mechanism for retinal vessels segmentation},
  volume       = {93},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Security analysis on dummy based side-channel
countermeasures—case study: AES with dummy and shuffling. <em>ASOC</em>,
<em>93</em>, 106352. (<a
href="https://doi.org/10.1016/j.asoc.2020.106352">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Side-channel analysis is a serious type of attack that can break mathematically secure cryptographic algorithms . Many studies have designed countermeasures against side-channel analysis, such as masking and hiding schemes. Frequently, designers employ combined countermeasures that use both a first-order masking scheme and a hiding scheme to provide sufficient security and efficiency. Random insertion of dummy operations scheme, which is one of the hiding schemes, randomly changes the execution time of the operation to be attacked by inserting dummy operations. However, if the dummy operations can be distinguished from real ones, attackers could extract secret information with lower complexity than the intended attack complexity with the designer inserting the dummy operations. In this paper, we present a novel vulnerability that can enable dummy and real operations to be distinguished for various implementation methods using C language on the XMEGA128D4 microprocessor . This novel vulnerability occurs regardless of the four methods of implementation of dummy operations and the compile levels. We also present a new countermeasure against this vulnerability and demonstrate the security through practical experimentation .},
  archive      = {J_ASOC},
  author       = {JongHyeok Lee and Dong-Guk Han},
  doi          = {10.1016/j.asoc.2020.106352},
  journal      = {Applied Soft Computing},
  pages        = {106352},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Security analysis on dummy based side-channel countermeasures—Case study: AES with dummy and shuffling},
  volume       = {93},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Data-driven prognosis method using hybrid deep recurrent
neural network. <em>ASOC</em>, <em>93</em>, 106351. (<a
href="https://doi.org/10.1016/j.asoc.2020.106351">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Prognostics and health management (PHM) has attracted increasing attention in modern manufacturing systems to achieve accurate predictive maintenance that reduces production downtime and enhances system safety. Remaining useful life (RUL) prediction plays a crucial role in PHM by providing direct evidence for a cost-effective maintenance decision. With the advances in sensing and communication technologies, data-driven approaches have achieved remarkable progress in machine prognostics. This paper develops a novel data-driven approach to precisely estimate the remaining useful life of machines using a hybrid deep recurrent neural network (RNN). The long short-term memory (LSTM) layers and classical neural networks are combined in the deep structure to capture the temporal information from the sequential data. The sequential sensory data from multiple sensors data can be fused and directly used as input of the model. The extraction of handcrafted features that relies heavily on prior knowledge and domain expertise as required by traditional approaches is avoided. The dropout technique and decaying learning rate are adopted in the training process of the hybrid deep RNN structure to increase the learning efficiency. A comprehensive experimental study on a widely used prognosis dataset is carried out to show the outstanding effectiveness and superior performance of the proposed approach in RUL prediction.},
  archive      = {J_ASOC},
  author       = {Min Xia and Xi Zheng and Muhammad Imran and Muhammad Shoaib},
  doi          = {10.1016/j.asoc.2020.106351},
  journal      = {Applied Soft Computing},
  pages        = {106351},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Data-driven prognosis method using hybrid deep recurrent neural network},
  volume       = {93},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Point and interval forecasting for wind speed based on
linear component extraction. <em>ASOC</em>, <em>93</em>, 106350. (<a
href="https://doi.org/10.1016/j.asoc.2020.106350">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As a renewable energy, wind power attracts more and more attention. However, the intermittence and randomness of wind speed make the utilization of wind power a challenging task. Therefore, it is essential to improve the ability of wind speed forecasting. This paper proposes a new hybrid system for wind speed forecasting, which includes three modules: data pre-processing module, deterministic point prediction module and probability interval prediction module. Empirical mode decomposition (EMD) and singular spectrum analysis (SSA) are conducted to extract the linear component of the initial wind speed series in data pre-processing module, autoregressive integrated moving average model (ARIMA) and back propagation neural network (BPNN) are employed to produce the prediction points (PPs) of the initial data in deterministic point prediction module, and ARIMA and improved First Order Markov Chain (IFOMC) model are provided to make the uncertainty analysis of wind speed in probability interval prediction module. A case study is selected to test the performance of the new system. The simulation results demonstrate that the new system can achieve better precision and higher efficiency than several benchmark methods.},
  archive      = {J_ASOC},
  author       = {Wen Ding and Fanyong Meng},
  doi          = {10.1016/j.asoc.2020.106350},
  journal      = {Applied Soft Computing},
  pages        = {106350},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Point and interval forecasting for wind speed based on linear component extraction},
  volume       = {93},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Energy-aware whale optimization algorithm for real-time task
scheduling in multiprocessor systems. <em>ASOC</em>, <em>93</em>,
106349. (<a href="https://doi.org/10.1016/j.asoc.2020.106349">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The growth of Multiprocessing Systems (MPS) has become a necessity for dealing with complex tasks and speeding up their execution. Increasing the number of processing cores on a single chip produces a vast processing power, but the biggest obstacle is the energy generated from these cores. The traditional techniques guarantee to get the optimal schedule, but they are costly in terms of time and memory storage. In this paper, we propose an Improved Whale Algorithm (IWA) to allocate the dependent tasks in MPS with two objectives minimizing the energy consumption and the makespan. The processing cores are assumed to support Dynamic Voltage and Frequency Scaling (DVFS) as an effective technique to reduce energy. The allocation of tasks in MPS is an NP-hard problem. Inadequate scheduling of tasks can result in consuming energy. Also, the failure to complete the tasks before their predetermined deadlines is a critical issue for real-time applications. We consider three different sources of energy coming from the communication, idle, and active states of the processing cores. We use an initialization procedure to produce a population of candidate schedules that respects the precedence among tasks. In IWA, we employ two different discretization methods to map the continuous values into discrete ones. Two specialized crossover operations are adopted to boost the quality of the candidate schedules while respecting the dependencies among the tasks. IWA implements the Load Balancing Improvement (LBI) strategy to alleviate the load of tasks from heavy cores. LBI plays a vital role in decreasing the static energy consumption. We compare IWA with the other algorithms, and the results show the superiority of IWA.},
  archive      = {J_ASOC},
  author       = {Mohamed Abdel-Basset and Doaa El-Shahat and Kalyanmoy Deb and Mohamed Abouhawwash},
  doi          = {10.1016/j.asoc.2020.106349},
  journal      = {Applied Soft Computing},
  pages        = {106349},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Energy-aware whale optimization algorithm for real-time task scheduling in multiprocessor systems},
  volume       = {93},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A quantum-inspired self-supervised network model for
automatic segmentation of brain MR images. <em>ASOC</em>, <em>93</em>,
106348. (<a href="https://doi.org/10.1016/j.asoc.2020.106348">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The classical self-supervised neural network architectures suffer from slow convergence problem and incorporation of quantum computing in classical self-supervised networks is a potential solution towards it. In this article, a fully self-supervised novel quantum-inspired neural network model referred to as Quantum-Inspired Self-Supervised Network (QIS-Net) is proposed and tailored for fully automatic segmentation of brain MR images to obviate the challenges faced by deeply supervised Convolutional Neural Network (CNN) architectures. The proposed QIS-Net architecture is composed of three layers of quantum neuron (input, intermediate and output) expressed as qbits . The intermediate and output layers of the QIS-Net architecture are inter-linked through bi-directional propagation of quantum states , wherein the image pixel intensities (quantum bits) are self-organized in between these two layers without any external supervision or training. Quantum observation allows to obtain the true output once the superimposed quantum states interact with the external environment. The proposed self-supervised quantum-inspired network model has been tailored for and tested on Dynamic Susceptibility Contrast (DSC) brain MR images from Nature data sets for detecting complete tumor and reported promising accuracy and reasonable dice similarity scores in comparison with the unsupervised Fuzzy C-Means clustering, self-trained QIBDS Net, Opti-QIBDS Net, deeply supervised U-Net and Fully Convolutional Neural Networks (FCNNs).},
  archive      = {J_ASOC},
  author       = {Debanjan Konar and Siddhartha Bhattacharyya and Tapan Kr. Gandhi and Bijaya Ketan Panigrahi},
  doi          = {10.1016/j.asoc.2020.106348},
  journal      = {Applied Soft Computing},
  pages        = {106348},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A quantum-inspired self-supervised network model for automatic segmentation of brain MR images},
  volume       = {93},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A novel RK4-hopfield neural network for power flow analysis
of power system. <em>ASOC</em>, <em>93</em>, 106346. (<a
href="https://doi.org/10.1016/j.asoc.2020.106346">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a novel Runge–Kutta (RK4) based modified hopfield neural network (MHNN) for solving a set of non-linear transcendental power flow equations of power system . The proffered method is a Lyapunov based energy function approach to minimize real and reactive power mismatches of the system. A set of non-linear differential equations derived from energy function, describing the dynamical behavior of HNN is framed for solving Power Flow equations. These dynamic equations of the network are solved by RK4 method to deduce the unknown variables of the system. The feasibility of proposed method is tested on 5-bus, IEEE 14-bus, 39-bus and 57-bus test system. The analytical equation describing the behavior of MHNN is coded in MATLAB software. The results obtained reveal that the suggested method gives accurate solution and reduces the computational complexity than conventional Newton Raphson (NR) method. The sensitivity analysis is also tested for change in R/X ratio of the system, initial conditions and loading of the system. The proposed method is robust for above specified changes and involves less computational effort. To prove the applicability and consistency of projected method, IEEE 118-bus system has been tested. The power flow solutions found through proffered method are compared with solutions obtained from numerical approaches in order to validate the proposed approach. Moreover, the stability of the system is studied in Lyapunov sense of notion which assures converged solution of proposed method.},
  archive      = {J_ASOC},
  author       = {Veerapandiyan Veerasamy and Noor Izzri Abdul Wahab and Rajeswari Ramachandran and Balasubramonian Madasamy and Muhammad Mansoor and Mohammad Lutfi Othman and Hashim Hizam},
  doi          = {10.1016/j.asoc.2020.106346},
  journal      = {Applied Soft Computing},
  pages        = {106346},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A novel RK4-hopfield neural network for power flow analysis of power system},
  volume       = {93},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Optimal generation scheduling of pumped storage
hydro-thermal system with wind energy sources. <em>ASOC</em>,
<em>93</em>, 106345. (<a
href="https://doi.org/10.1016/j.asoc.2020.106345">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, the pumped storage hydrothermal system with wind energy sources (PSHTS-WES) has been modelled. The generation scheduling problem consists of mixed decision variables. In order to search for an optimum generation schedule for the PSHTS-WES system, a solution methodology has been proposed. In the proposed solution methodology, a modified crisscross PSO (MCPSO) technique has been proposed to deal with continuous decision variables adaptively. Further, an improved binary PSO (BPSO) technique has been implemented to search the binary decision variables. The proposed methodology has been implemented for three different test systems. Test system-I and II are the hydrothermal system (HTS) and PSHTS, respectively. The test system-III is a coordinated PSHTS-WES energy source. The achieved results have been compared with the other state-of-art algorithms. It has been found that the proposed solution methodology is able to search better results with the least standard deviation and mean computational time. The presence of a pumped storage unit has reduced the total thermal power generation by 221.23 MW, and the total cost obtained has been reduced by 2.42\% for test system-II. The impact of WES is evident from the results of test system-III, which illustrated that WES able to reduce the total thermal power generation by 4528.85 MW and optimal cost by 19.86\%. The robustness of the proposed solution methodology has been verified by implementing the two-sample t-test. The analysis from the numerical results verifies that the proposed methodology can achieve a better solution in comparison with the established techniques.},
  archive      = {J_ASOC},
  author       = {Rituraj Singh Patwal and Nitin Narang},
  doi          = {10.1016/j.asoc.2020.106345},
  journal      = {Applied Soft Computing},
  pages        = {106345},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Optimal generation scheduling of pumped storage hydro-thermal system with wind energy sources},
  volume       = {93},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A remaining useful life prediction method with long-short
term feature processing for aircraft engines. <em>ASOC</em>,
<em>93</em>, 106344. (<a
href="https://doi.org/10.1016/j.asoc.2020.106344">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As one of the key components of aircraft, any failure of the engine can lead to serious accidents. The reliability and safety can be guaranteed by predicting the remaining useful life of the aircraft engine. The data-driven approaches are suitable for predicting the remaining useful life of the aircraft engine, but they generally suffer from the following challenges: (i) how to capture the real degradation trend of the engine; (ii) how to efficiently and fully utilize the temporal correlation between the sensor data; (iii) how to handle highly nonlinear data. In order to address these challenges, an effective data-driven remaining useful life prediction method is proposed in this paper. Firstly, a long-term differential technique is proposed to extract forward differential features, which fully reflects the actual degradation trend in the entire lifetime. Then, the Fibonacci window is proposed for short-term feature extension, which makes full use of the temporal correlation of the historical data and effectively reduces the extra computational load. Finally, the CatBoost algorithm is used to predict the remaining useful life on the highly nonlinear data, and superior prediction performance is obtained. In order to verify the effectiveness of the proposed method, the experiments are carried out on the aircraft engine dataset provided by NASA.},
  archive      = {J_ASOC},
  author       = {Kunyuan Deng and Xiaoyong Zhang and Yijun Cheng and Zhiyong Zheng and Fu Jiang and Weirong Liu and Jun Peng},
  doi          = {10.1016/j.asoc.2020.106344},
  journal      = {Applied Soft Computing},
  pages        = {106344},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A remaining useful life prediction method with long-short term feature processing for aircraft engines},
  volume       = {93},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Discrete evolutionary multi-objective optimization for
energy-efficient blocking flow shop scheduling with setup time.
<em>ASOC</em>, <em>93</em>, 106343. (<a
href="https://doi.org/10.1016/j.asoc.2020.106343">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sustainable scheduling problems have been attracted great attention from researchers. For the flow shop scheduling problems, researches mainly focus on reducing economic costs, and the energy consumption has not yet been well studied up to date especially in the blocking flow shop scheduling problem. Thus, we construct a multi-objective optimization model of the blocking flow shop scheduling problem with makespan and energy consumption criteria. Then a discrete evolutionary multi-objective optimization (DEMO) algorithm is proposed. The three contributions of DEMO are as follows. First, a variable single-objective heuristic is proposed to initialize the population. Second, the self-adaptive exploitation evolution and self-adaptive exploration evolution operators are proposed respectively to obtain high quality solutions. Third, a penalty-based boundary interstation based on the local search, called by PBI-based-local search, is designed to further improve the exploitation capability of the algorithm. Simulation results show that DEMO outperforms the three state-of-the-art algorithms with respect to hypervolume, coverage rate and distance metrics.},
  archive      = {J_ASOC},
  author       = {Yuyan Han and Junqing Li and Hongyan Sang and Yiping Liu and Kaizhou Gao and Quanke Pan},
  doi          = {10.1016/j.asoc.2020.106343},
  journal      = {Applied Soft Computing},
  pages        = {106343},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Discrete evolutionary multi-objective optimization for energy-efficient blocking flow shop scheduling with setup time},
  volume       = {93},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Multi-objective meta-heuristic optimization in intelligent
control: A survey on the controller tuning problem. <em>ASOC</em>,
<em>93</em>, 106342. (<a
href="https://doi.org/10.1016/j.asoc.2020.106342">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-objective optimization has been adopted in many engineering problems where a set of requirements must be met to generate successful applications. Among them, there are the tuning problems from control engineering , which are focused on the correct setting of the controller parameters to properly govern complex dynamic systems to satisfy desired behaviors such as high accuracy, efficient energy consumption, low cost, among others. These requirements are stated in a multi-objective optimization problem to find the most suitable controller parameters . Nevertheless, these parameters are tough to find because of the conflicting control performance requirements (i.e., a requirement cannot be met without harming the others). Hence, the use of techniques from computational intelligence and soft computing is necessary to solve multi-objective problems and handle the trade-offs among control performance objectives. Meta-heuristics have shown to obtain outstanding results when solving complex multi-objective problems at a reasonable computational cost. In this survey, the literature related to the use of multi-objective meta-heuristics in intelligent control focused on the controller tuning problem is reviewed and discussed.},
  archive      = {J_ASOC},
  author       = {Alejandro Rodríguez-Molina and Efrén Mezura-Montes and Miguel G. Villarreal-Cervantes and Mario Aldape-Pérez},
  doi          = {10.1016/j.asoc.2020.106342},
  journal      = {Applied Soft Computing},
  pages        = {106342},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Multi-objective meta-heuristic optimization in intelligent control: A survey on the controller tuning problem},
  volume       = {93},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Introducing clustering based population in binary
gravitational search algorithm for feature selection. <em>ASOC</em>,
<em>93</em>, 106341. (<a
href="https://doi.org/10.1016/j.asoc.2020.106341">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Feature Selection (FS) is an important aspect of knowledge extraction as it helps to reduce dimensionality of data. Among the numerous FS algorithms proposed over the years, Gravitational Search Algorithm (GSA) is a popular one which has been applied to various domains. However, GSA suffers from the problem of pre-mature convergence which affects exploration leading to performance degradation . To aid exploration, in the present work, we use a clustering technique in order to make the initial population distributed over the entire feature space and to increase the inclusion of features which are more promising. The proposed method is named Clustering based Population in Binary GSA (CPBGSA). To assess the performance of our proposed model, 20 standard UCI datasets are used, and the results are compared with some contemporary methods. It is observed that CPBGSA outperforms other methods in 12 out of 20 cases in terms of average classification accuracy . The relevant codes of the entire CPBGSA model can be found in the provided link: https://github.com/ManosijGhosh/Clustering-based-Population-in-Binary-GSA .},
  archive      = {J_ASOC},
  author       = {Ritam Guha and Manosij Ghosh and Akash Chakrabarti and Ram Sarkar and Seyedali Mirjalili},
  doi          = {10.1016/j.asoc.2020.106341},
  journal      = {Applied Soft Computing},
  pages        = {106341},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Introducing clustering based population in binary gravitational search algorithm for feature selection},
  volume       = {93},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Efficient design of wideband digital fractional order
differentiators and integrators using multi-verse optimizer.
<em>ASOC</em>, <em>93</em>, 106340. (<a
href="https://doi.org/10.1016/j.asoc.2020.106340">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, a novel method is proposed based on combining L 1 L1 -norm optimally criterion with a recently-proposed metaheuristic called multi-verse optimizer (MVO) to design 2nd–4th order stable, minimum phase and wideband infinite impulse response (IIR) digital fractional order differentiators (DFODs) for the fractional order differentiators (FODs) of one-half, one-third and one-fourth order. To confirm the superiority of the proposed approach, we conduct comparisons of the MVO-based designs with the real-coded genetic algorithm (RCGA) and particle swarm optimization (PSO)-based designs in terms of accuracy, robustness, consistency, and efficiency. The transfer functions of the proposed designs are inverted to obtain new models of digital fractional order integrators (DFOIs) of the same order. A comparative study of the frequency responses of the proposed digital fractional order differentiators and integrators with the ones of the existing models is then conducted. The results demonstrate that the proposed designs yield the optimal magnitude responses in terms of absolute magnitude error (AME) with flat response profiles.},
  archive      = {J_ASOC},
  author       = {Talal Ahmed Ali Ali and Zhu Xiao and Seyedali Mirjalili and Vincent Havyarimana},
  doi          = {10.1016/j.asoc.2020.106340},
  journal      = {Applied Soft Computing},
  pages        = {106340},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Efficient design of wideband digital fractional order differentiators and integrators using multi-verse optimizer},
  volume       = {93},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). FBI inspired meta-optimization. <em>ASOC</em>, <em>93</em>,
106339. (<a href="https://doi.org/10.1016/j.asoc.2020.106339">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study developed a novel optimization algorithm , called Forensic-Based Investigation (FBI), inspired by the suspect investigation–location–pursuit process that is used by police officers. Although numerous unwieldy optimization algorithms hamper their usability by requiring predefined operating parameters, FBI is a user-friendly algorithm that does not require predefined operating parameters. The performance of parameter-free FBI was validated using four experiments: (1) The robustness and efficiency of FBI were compared with those of 12 representations of the top leading metaphors by using 50 renowned multidimensional benchmark problems. The result indicated that FBI remarkably outperformed all other algorithms. (2) FBI was applied to solve a resource-constrained scheduling problem associated with a highway construction project. The experiment demonstrated that FBI yielded the shortest schedule with a success rate of 100\%, indicating its stability and robustness. (3) FBI was utilized to solve 30 benchmark functions that were most recently presented at the IEEE Congress on Evolutionary Computation (CEC) competition on bound-constrained problems. Its performance was compared with those of the three winners in CEC to validate its effectiveness. (4) FBI solved high-dimensional problems, by increasing the number of dimensions of benchmark functions to 1000. FBI is efficient because it requires a relatively short computational time for solving problems, it reaches the optimal solution more rapidly than other algorithms, and it efficaciously solves high-dimensional problems. Given that the experiments demonstrated FBI’s robustness, efficiency, stability, and user-friendliness, FBI is promising for solving various complex problems. Finally, this study provided the scientific community with a metaheuristic optimization platform for graphically and logically manipulating optimization algorithms.},
  archive      = {J_ASOC},
  author       = {Jui-Sheng Chou and Ngoc-Mai Nguyen},
  doi          = {10.1016/j.asoc.2020.106339},
  journal      = {Applied Soft Computing},
  pages        = {106339},
  shortjournal = {Appl. Soft. Comput.},
  title        = {FBI inspired meta-optimization},
  volume       = {93},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A novel hybrid feature selection method based on dynamic
feature importance. <em>ASOC</em>, <em>93</em>, 106337. (<a
href="https://doi.org/10.1016/j.asoc.2020.106337">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Feature selection aims to eliminate unimportant and redundant features or to select effective and interacting features. It is a challenging task to accurately measure the relationships of candidate features, the selected features and categories in the selection process, especially for high-dimensional and small-sample-size data. To this end, a new measure named Dynamic Feature Importance (DFI) is proposed, as well as its corresponding feature selection algorithm named Dynamic Feature Importance based Feature Selection (DFIFS). In order to obtain higher classification accuracy with smaller number of features, a newly Modified-Dynamic Feature Importance based Feature Selection (M-DFIFS) algorithm is developed by combining DFIFS with classical filters. Based on experiments with 14 public high-dimensional datasets, the lately M-DFIFS algorithm shows significantly better performance than five typical filter algorithms in terms of their average accuracy with acceptable computing time. When using random forest as the classifier, M-DFIFS brings a great advantage in the number decrease of selected features. Hence the new feature selection framework “Filter + + DFIFS” is verified very effective to solve problems of obtaining high accuracy with a few features.},
  archive      = {J_ASOC},
  author       = {Guangfen Wei and Jie Zhao and Yanli Feng and Aixiang He and Jun Yu},
  doi          = {10.1016/j.asoc.2020.106337},
  journal      = {Applied Soft Computing},
  pages        = {106337},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A novel hybrid feature selection method based on dynamic feature importance},
  volume       = {93},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Developing two heuristic algorithms with metaheuristic
algorithms to improve solutions of optimization problems with soft and
hard constraints: An application to nurse rostering problems.
<em>ASOC</em>, <em>93</em>, 106336. (<a
href="https://doi.org/10.1016/j.asoc.2020.106336">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many researchers have studied optimization problems with soft and hard constraints, such as school timetabling, nurse rostering, vehicle routing with soft time window, and job/machine scheduling. Nurse rostering problem (NRP) is the research problem in this paper. This study proposes two heuristic algorithms , which are the decision tree method and the greedy search algorithm, to integrate with metaheuristic algorithms in order to generate better initial solutions in less time and to improve solutions’ quality. This research examines the algorithms’ performance based on two scenarios and two metaheuristic algorithms : bat algorithm (BA) and particle swarm optimization (PSO). For the two scenarios, BA (or PSO) with the decision tree method outperforms BA (or PSO) without the decision tree method, and BA (or PSO) with the greedy search algorithm outperforms BA (or PSO) without the greedy search algorithm. Furthermore, the results show that BA (or PSO) with the decision tree method and the greedy search algorithm can generate better initial solutions in less time and improve solutions’ quality.},
  archive      = {J_ASOC},
  author       = {Ping-Shun Chen and Zhi-Yang Zeng},
  doi          = {10.1016/j.asoc.2020.106336},
  journal      = {Applied Soft Computing},
  pages        = {106336},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Developing two heuristic algorithms with metaheuristic algorithms to improve solutions of optimization problems with soft and hard constraints: An application to nurse rostering problems},
  volume       = {93},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A full migration BBO algorithm with enhanced population
quality bounds for multimodal biomedical image registration.
<em>ASOC</em>, <em>93</em>, 106335. (<a
href="https://doi.org/10.1016/j.asoc.2020.106335">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Medical images acquired from different modalities give rise to many practical problems in image registration. Intensity-based registration techniques have been increasingly used in multimodal image registration; these techniques integrate different images that have shared content into a single representation, by transformation. The estimation of the optimal transformation requires the optimization of a similarity metric between the images. Recently, many optimization methods have been proposed that focus on the development of the optimization component. However, there is still room for large amounts of improvement, from both an efficiency point of view and a quality perspective. In this paper we present a new Biogeography-based Optimization (BBO) algorithm, the Biogeography-based Optimization algorithm with Elite Learning (BBO-EL), for multimodal medical image registration. First, we propose a hybrid full migration operator in which each individual has the chance to perform the migration operation and the whole population has the chance to expand the search space. In this way, the search ability of the BBO algorithm is enhanced and matches well the characteristics of multimodal medical image registration. In addition, considering that the quality of some individuals could be deteriorated as caused by the migration operation, we propose an undo operator on the deteriorated individuals. Thus, the lower bound of the whole population’s quality can be maintained at a higher level. Furthermore, in the original BBO algorithm, a number of good individuals might be not involved in the migration operation, and we present an elite learning operator that is based on social comparison theory to improve the upper bound of the whole population’s quality. Therefore, after improving both the lower bound and the upper bound of the whole population’s quality, the accuracy and the convergence speed of the multimodal medical registration can be greatly enhanced. The BBO-EL has been tested in many experiments on benchmark datasets include six kind of different modality images, from up to eighteen different patients, which can make up 54 multimodal registration scenarios. The BBO-EL obtained 30 best performance scenarios while the state-of-the-art algorithm obtained 21 scenarios. The results demonstrated that BBO-EL outperforms the state-of-the-art algorithm in most cases for practical problems.},
  archive      = {J_ASOC},
  author       = {Yilin Chen and Fazhi He and Haoran Li and Dejun Zhang and Yiqi Wu},
  doi          = {10.1016/j.asoc.2020.106335},
  journal      = {Applied Soft Computing},
  pages        = {106335},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A full migration BBO algorithm with enhanced population quality bounds for multimodal biomedical image registration},
  volume       = {93},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Inception v3 based cervical cell classification combined
with artificially extracted features. <em>ASOC</em>, <em>93</em>,
106311. (<a href="https://doi.org/10.1016/j.asoc.2020.106311">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traditional cell classification methods generally extract multiple features of the cell manually. Moreover, the simple use of artificial feature extraction methods has low universality. For example, it is unsuitable for cervical cell recognition because of the complexity of the cervical cell texture and the large individual differences between cells. Using the convolutional neural network classification method is a good way to solve this problem. However, although the cell features can be extracted automatically, the cervical cell domain knowledge will be lost, and the corresponding features of different cell types will be missing; hence, the classification effect is not sufficiently accurate. Aiming at addressing the limitations of the two mentioned classification methods, this paper proposes a cell classification algorithm that combines Inception v3 and artificial features, which effectively improves the accuracy of cervical cell recognition. In addition, to address the under-fitting problem and carry out effective deep learning training with a relatively small amount of medical data, this paper inherits the strong learning ability from transfer learning , and achieves accurate and effective cervical cell image classification based on the Herlev dataset. Using this method, an accuracy of more than 98\% is achieved, providing an effective framework for computer aided diagnosis of cervical cancer. The proposed algorithm has good universality, low complexity, and high accuracy, rendering it suitable for further extension and application to the classification of other types of cancer cells.},
  archive      = {J_ASOC},
  author       = {N. Dong Ph.D. and L. Zhao and C.H. Wu Ph.D. and J.F. Chang},
  doi          = {10.1016/j.asoc.2020.106311},
  journal      = {Applied Soft Computing},
  pages        = {106311},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Inception v3 based cervical cell classification combined with artificially extracted features},
  volume       = {93},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). LSTSVM classifier with enhanced features from pre-trained
functional link network. <em>ASOC</em>, <em>93</em>, 106305. (<a
href="https://doi.org/10.1016/j.asoc.2020.106305">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose an improved model for the classification problems. We use least squares twin support vector machines (LSTSVM) and pre-trained functional link to enhance the feature space. LSTSVM algorithm is used in many real world classification problems as it has lower computational complexity and solves system of linear equations instead of solving quadratic programming problems (QPPs). Since neural network models provide implicit feature representation and is one of the reasons for the success of neural networks . Here, we propose a model wherein the input feature space is enhanced by the pre-trained functional link network. Weights are generated by LSTSVM, and a non-linear function is applied on the product between input features and the weights to get the enhanced features. These features are concatenated with the input features to get the extended feature space. Final classification is done by LSTSVM based on these extended features. Numerical experiments and statistical tests conducted show that the proposed model outperforms the baseline methods .},
  archive      = {J_ASOC},
  author       = {M.A. Ganaie and M. Tanveer},
  doi          = {10.1016/j.asoc.2020.106305},
  journal      = {Applied Soft Computing},
  pages        = {106305},
  shortjournal = {Appl. Soft. Comput.},
  title        = {LSTSVM classifier with enhanced features from pre-trained functional link network},
  volume       = {93},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Efficient hybrid local search heuristics for solving the
travelling thief problem. <em>ASOC</em>, <em>93</em>, 106284. (<a
href="https://doi.org/10.1016/j.asoc.2020.106284">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Real-world problems often consist of several interdependent subproblems . The degree of interaction of the subproblems is associated with the complexity of the problem and solving each subproblem optimally not ensure the optimal solution of the overall problem. The Travelling Thief Problem (TTP) integrates two well-known combinatorial optimization problems namely the classical Travelling Salesman Problem (TSP) and the 0–1 Knapsack Problem (KP). TTP was introduced to represent the complication of a real-world combinatorial optimization problem. The goal of this problem is to provide a tour to a thief over all the cities and a picking plan that determines which item should be taken from which city to achieve the maximum benefits. The KP component of the TTP is more efficient as compared to the TSP component for optimization. Our proposed method mainly focuses on constructing a picking plan for a near-optimal tour generated by Chained Lin–Kernighan Heuristic (CLKH). In this picking plan, items are picked up according to their scoring value which is calculated by our proposed formulation. Additionally, bit-flip is used for a better solution that can give a more profitable picking plan. The experimental results suggest that our proposed approach can meet or beat current state-of-the-art methods for a large number of TTP instances.},
  archive      = {J_ASOC},
  author       = {Alenrex Maity and Swagatam Das},
  doi          = {10.1016/j.asoc.2020.106284},
  journal      = {Applied Soft Computing},
  pages        = {106284},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Efficient hybrid local search heuristics for solving the travelling thief problem},
  volume       = {93},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Composite monte carlo decision making under high uncertainty
of novel coronavirus epidemic using hybridized deep learning and fuzzy
rule induction. <em>ASOC</em>, <em>93</em>, 106282. (<a
href="https://doi.org/10.1016/j.asoc.2020.106282">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the advent of the novel coronavirus epidemic since December 2019, governments and authorities have been struggling to make critical decisions under high uncertainty at their best efforts. In computer science, this represents a typical problem of machine learning over incomplete or limited data in early epidemic Composite Monte-Carlo (CMC) simulation is a forecasting method which extrapolates available data which are broken down from multiple correlated/casual micro-data sources into many possible future outcomes by drawing random samples from some probability distributions. For instance, the overall trend and propagation of the infested cases in China are influenced by the temporal–spatial data of the nearby cities around the Wuhan city (where the virus is originated from), in terms of the population density, travel mobility, medical resources such as hospital beds and the timeliness of quarantine control in each city etc. Hence a CMC is reliable only up to the closeness of the underlying statistical distribution of a CMC, that is supposed to represent the behaviour of the future events, and the correctness of the composite data relationships. In this paper, a case study of using CMC that is enhanced by deep learning network and fuzzy rule induction for gaining better stochastic insights about the epidemic development is experimented. Instead of applying simplistic and uniform assumptions for a MC which is a common practice, a deep learning-based CMC is used in conjunction of fuzzy rule induction techniques. As a result, decision makers are benefited from a better fitted MC outputs complemented by min–max rules that foretell about the extreme ranges of future possibilities with respect to the epidemic.},
  archive      = {J_ASOC},
  author       = {Simon James Fong and Gloria Li and Nilanjan Dey and Rubén González Crespo and Enrique Herrera-Viedma},
  doi          = {10.1016/j.asoc.2020.106282},
  journal      = {Applied Soft Computing},
  pages        = {106282},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Composite monte carlo decision making under high uncertainty of novel coronavirus epidemic using hybridized deep learning and fuzzy rule induction},
  volume       = {93},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A framework based on (probabilistic) soft logic and neural
network for NLP. <em>ASOC</em>, <em>93</em>, 106232. (<a
href="https://doi.org/10.1016/j.asoc.2020.106232">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep neural networks have emerged as a flexible framework that achieved state-of-the-art performance in many NLP applications such as machine translation, named entity recognition , sentiment analysis , and part-of-speech tagging. The main advantage of these neural models is their ability to learn useful representations without hand-engineering features. While this success, these models still suffer from the interpretability issue. More recently, probabilistic soft logic (PSL) is a promising framework based on first-order logic that achieves interesting results in both computer vision and NLP by capturing semantic relationships between entities. Moreover, unifying knowledge-driven modeling approaches and data-driven approaches is a promising framework that will have an exciting impact on structured prediction problems. In this paper, we developed NeuralGLogic a generalization framework of the previous model proposed by Huet al. (2016) that combines deep neural networks with logic rules built either using Soft Logic (SL) or Probabilistic Soft Logic (PSL). Furthermore, we evaluate our framework on different neural network architectures applied to two NLP tasks: sentiment classification and part-of-speech tagging. Experimental results showed that we were able to improve the results over the baselines and outperformed all the previous state-of-the-art systems emphasizing the utility of both SL and PSL rules in reducing the uninterpretability of the neural models thus validating our intuition.},
  archive      = {J_ASOC},
  author       = {Mourad Gridach},
  doi          = {10.1016/j.asoc.2020.106232},
  journal      = {Applied Soft Computing},
  pages        = {106232},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A framework based on (probabilistic) soft logic and neural network for NLP},
  volume       = {93},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Acceleration harmonics estimation and elimination with
MABC–RLS algorithm: Simulation and experimental analyses on shaking
table. <em>ASOC</em>, <em>92</em>, 106377. (<a
href="https://doi.org/10.1016/j.asoc.2020.106377">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, to estimate and eliminate acceleration harmonics in shaking table system, a novel approach combining modified artificial bee colony (MABC) algorithm and recursive least square (RLS) algorithm is proposed. MABC algorithm is employed in amplitude and phase estimation of acceleration harmonics while RLS algorithm is used to eliminate harmonics from the acceleration signal. In order to analyse the performance of the proposed algorithm, simulation and experimental studies are realized and the results are compared with the well-known algorithm reported in literature. In addition, harmonic frequencies and numbers are increased and robustness of proposed approach is investigated.},
  archive      = {J_ASOC},
  author       = {Serdar Kockanat},
  doi          = {10.1016/j.asoc.2020.106377},
  journal      = {Applied Soft Computing},
  pages        = {106377},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Acceleration harmonics estimation and elimination with MABC–RLS algorithm: Simulation and experimental analyses on shaking table},
  volume       = {92},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Load frequency regulation by de-loaded tidal turbine power
plant units using fractional fuzzy based PID droop controller.
<em>ASOC</em>, <em>92</em>, 106338. (<a
href="https://doi.org/10.1016/j.asoc.2020.106338">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study presents a model to analyse and simulate tidal power generation for hybrid power system in the presence of highly infiltrated tidal units. The response of the hybrid system may be at risk without suitable frequency enhancement techniques. The complete load frequency model is a combination of conventional automatic generation control (AGC) and automatic voltage regulator (AVR). AVR is employed to keep the output voltage magnitude of conventional generator at a particular level. The purpose of integrating AGC with AVR is to maintain the equilibrium between system’s generation and load as well as to keep the frequency of system within suitable range. Owing to inclusion of nonconventional sources, the total inertia of the power system is eventually diminished. Conventional PID droop controller demonstrates inefficacy in diminishing the frequency deviations due to slow controlling action. This research proposes a fractional order (FO) fuzzy PID droop in de-loaded area to improvise the frequency excursion over fixed/Fuzzy PID/PID droop controllers. Imperialist competitive algorithm (ICA) is employed to tune the parameters of the controllers. ICA optimized fractional order PID droop control strategy unveils best performance (settling time = = 11.65 s, undershoot amplitude = = 0.26pu, performance index = = 0.072e−6) over the integer order fuzzy PID control (settling time = = 12.02 s, undershoot amplitude = = 0.278 pu, performance index = = 0.189e−6) and PID droop control (settling time = = 30.68 s, undershoot amplitude = = 0.95 pu, performance index = = 0.215e−6). Examination of dynamic responses for abrupt changes in load request divulges the pre-eminence of proposed droop controller strategy with others controllers. The comprehensive study carried out in this paper implies that ICA optimized controller operates properly and has proven its robustness for ± ± 10\% variations in system parameters and physical constraints (Generation rate constraints, governor dead band, and time delay). This study also analyses the effect of tidal power plant contribution in the frequency regulation by inertia, primary and secondary frequency control.},
  archive      = {J_ASOC},
  author       = {Zaheeruddin and Kavita Singh},
  doi          = {10.1016/j.asoc.2020.106338},
  journal      = {Applied Soft Computing},
  pages        = {106338},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Load frequency regulation by de-loaded tidal turbine power plant units using fractional fuzzy based PID droop controller},
  volume       = {92},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Managing individual evaluator’s personalized semantic
environment of linguistic term with improved vector expression in
multi-granularity linguistic group decision making. <em>ASOC</em>,
<em>92</em>, 106334. (<a
href="https://doi.org/10.1016/j.asoc.2020.106334">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Group linguistic assessment with the vector symbolic of linguistic evaluation information has been recently proposed for qualitative group decision making. Due to various individualized characteristics and knowledge levels, evaluators in group assessment often provide linguistic terms based on different individual linguistic evaluation scales to express their preferences on alternatives. In some situations, decision maker needs to distinguish different meanings of the same linguistic term in different individual evaluators’ understandings. To further develop the resolution and the operational performance of the vector expression of linguistic term in multi-granularity linguistic group decision making (MGLGDM), in this study, we present the concept of improved vector expression of linguistic term. First, we present a method of rewriting the numerical symbolic of linguistic term into the improved vector expression based on the individual linguistic evaluation scale. Based on this, we introduce an approach to compare individual linguistic evaluation scales in MGLGDM. Then, an algorithm with improved vector expression is proposed for ranking alternatives in MGLGDM. Finally, a case illustration and some comparative studies have shown that the new proposed algorithm with improved vector expression of linguistic term is accurate and efficient in distinguishing and computing linguistic evaluation information in MGLGDM.},
  archive      = {J_ASOC},
  author       = {Yuling Zhai and Zeshui Xu},
  doi          = {10.1016/j.asoc.2020.106334},
  journal      = {Applied Soft Computing},
  pages        = {106334},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Managing individual evaluator’s personalized semantic environment of linguistic term with improved vector expression in multi-granularity linguistic group decision making},
  volume       = {92},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Imbalanced sample fault diagnosis of rotating machinery
using conditional variational auto-encoder generative adversarial
network. <em>ASOC</em>, <em>92</em>, 106333. (<a
href="https://doi.org/10.1016/j.asoc.2020.106333">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In many real applications of planetary gearbox fault diagnosis, the number of fault samples is much less than normal samples while fault samples are hard to collected in different working conditions, so many traditional diagnosis methods will get low accuracy. To solve this problem, a method based on conditional variational auto-encoder generative adversarial network (CVAE-GAN) is proposed for imbalanced fault diagnosis. Firstly, new method uses encoder network of conditional variational auto-encoder to obtain the distribution of fault samples, and then a large number of similar fault samples can be generated through decoder network. Secondly, the parameters of generator, discriminator and classifier may be continuously optimized using adversarial learning mechanism. Finally, the trained CVAE-GAN is applied for intelligent fault diagnosis of planetary gearbox. The experimental results show that CVAE-GAN can generate fault samples in different working conditions, which improve the fault diagnosis performance of planetary gearbox. The sample generating ability of CVAE-GAN is significantly higher than other methods in two cases of imbalanced dataset.},
  archive      = {J_ASOC},
  author       = {You-ren Wang and Guo-dong Sun and Qi Jin},
  doi          = {10.1016/j.asoc.2020.106333},
  journal      = {Applied Soft Computing},
  pages        = {106333},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Imbalanced sample fault diagnosis of rotating machinery using conditional variational auto-encoder generative adversarial network},
  volume       = {92},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Modeling energy flow in natural gas networks using time
series disaggregation and fuzzy systems tuned by particle swarm
optimization. <em>ASOC</em>, <em>92</em>, 106332. (<a
href="https://doi.org/10.1016/j.asoc.2020.106332">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Natural gas is widely used in industrial, residential, and commercial sectors and is delivered to consumption nodes via gas distribution networks . For efficient management and utilization of this nonrenewable energy resource, High Frequency (HF) response of gas networks to nodal consumption is needed that requires solution of the network governing equations. This high frequency response could either be measured with expensive high technology hardware at each consumption node or alternatively calculated from Low Frequency (LF) data collected by inexpensive low technology gas meters, which are usually preferred. Solution of the governing equations itself requires nodal gas consumption that is recorded by LF gas meters installed at consumption nodes. The recording frequency differs from one meter to another. Gas companies use these LF meter data just for billing. This paper presents a methodology for HF study of gas networks response to nodal consumption using these LF data. A Time Series Disaggregation (TSD) method is formulated that disaggregates the LF meter readings to HF gas consumption all with the same frequency by which the network governing equations are solved. HF gas consumption of each node is employed to train Takagi–Sugeno–Kang (TSK) fuzzy system to forecast HF consumption of that node in forthcoming days. The governing equations are then solved using this forecasted nodal consumption to predict the gas network behavior in the days ahead. This enables gas companies to recognize areas of the network with high pressure drop in cold days and manage the network accordingly. The paper also presents two techniques to prevent pressure drop in the areas of the network with high gas consumption. The proposed methods are applied to a gas network with 4258 customers using available LF data recorded during three years.},
  archive      = {J_ASOC},
  author       = {S. Askari and N. Montazerin and M.H. Fazel Zarandi},
  doi          = {10.1016/j.asoc.2020.106332},
  journal      = {Applied Soft Computing},
  pages        = {106332},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Modeling energy flow in natural gas networks using time series disaggregation and fuzzy systems tuned by particle swarm optimization},
  volume       = {92},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A multi-objective pharmaceutical supply chain network based
on a robust fuzzy model: A comparison of meta-heuristics. <em>ASOC</em>,
<em>92</em>, 106331. (<a
href="https://doi.org/10.1016/j.asoc.2020.106331">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The pharmaceutical supply chain has features that distinguish it from other supply chains. Medicine is considered a strategic commodity, and the smallest disruption in its supply chain may cause severe crises. This is why the distribution of pharmaceutical products needs to combine the minimization of costs with strong compliance with service standards while taking into account risks due to uncertainty. In this study, we present a new multi-objective multi-echelon multi-product multi-period pharmaceutical supply chain network (PSCN) along with the production–distribution–purchasing–ordering–inventory holding-allocation-routing problem under uncertainty. We formulate the problem as a Mixed-Integer Non-Linear Programming model and develop a novel robust fuzzy programming method to cope with uncertainty parameters. To find optimal solutions, several multi-objective metaheuristic algorithms , namely, MOSEO, MOSAM MOKA, and MOFFA considering different criteria and multi-objective assessment metrics are suggested. Since there are no benchmarks existing in the literature, 10 numerical instances in large and small sizes are generated and also the trapezoidal fuzzy numbers of the uncertain parameters were randomly generated based on a uniform distribution. The required parameters were set and also the simulated data were examined in an exact method and by metaheuristic algorithms . The results confirm the efficiency of the MOFFA algorithm to detect a near-optimal solution within a logical CPU time. The solution methods are complemented with several sensitivity analyses on the input parameters of the proposed model.},
  archive      = {J_ASOC},
  author       = {Fariba Goodarzian and Hasan Hosseini-Nasab and Jesús Muñuzuri and Mohammad-Bagher Fakhrzad},
  doi          = {10.1016/j.asoc.2020.106331},
  journal      = {Applied Soft Computing},
  pages        = {106331},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A multi-objective pharmaceutical supply chain network based on a robust fuzzy model: A comparison of meta-heuristics},
  volume       = {92},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Application of a genetic algorithm based model selection
algorithm for identification of carbide-based hot metal desulfurization.
<em>ASOC</em>, <em>92</em>, 106330. (<a
href="https://doi.org/10.1016/j.asoc.2020.106330">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sulfur is considered as one of the main impurities in hot metal. Hot metal desulfurization is often carried out with pneumatic injection of a fine-grade desulfurization reagent using a submerged lance. The aim of this study was to develop a data-driven model for the process. The model selection algorithm carries out a simultaneous variable selection and optimization of number of hidden neurons with a combination of binary and integer coded Genetic Algorithm . The objective function applied in the search is repeated Leave-Multiple-Out cross-validation. The model considered is a feedforward neural network with a single hidden layer. In the inner loop of the algorithm, the computational load is reduced by making use of Extreme Learning Machine (ELM) architecture. The final model is trained using the Bayesian regularization . The results show that a well-generalizing data-driven model with good prediction performance can be repeatedly selected based on noisy industrial data with the help of a Genetic Algorithm , provided that the model is validated comprehensively with internal and external data sets.},
  archive      = {J_ASOC},
  author       = {Tero Vuolio and Ville-Valtteri Visuri and Aki Sorsa and Seppo Ollila and Timo Fabritius},
  doi          = {10.1016/j.asoc.2020.106330},
  journal      = {Applied Soft Computing},
  pages        = {106330},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Application of a genetic algorithm based model selection algorithm for identification of carbide-based hot metal desulfurization},
  volume       = {92},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A two-stage fuzzy neural approach for credit risk assessment
in a brazilian credit card company. <em>ASOC</em>, <em>92</em>, 106329.
(<a href="https://doi.org/10.1016/j.asoc.2020.106329">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study explores and evaluates the use of soft computing systems for clients’ credit risk assessment in a Brazilian private credit card provider through the development of an innovative two-stage process, both involving soft computing techniques (fuzzy and neural networks). We use commercially available credit score ratings both in the development of our method and for benchmarking. After describing the development of our method, we present a discussion about the comparison of performances of our method and a number of other credit scoring methods described in literature (for e.g. statistical and soft computing-based). One of the analyzed existing methods for instance involves the use of a soft computing algorithm only – Artificial Neural Networks (ANN) – for client classification into solvent or non-solvent, having a market available credit score rating as input. One of the most relevant contributions of this study however is the development of what we consider an innovative approach for credit scoring. This is a two-stage process that involves the use of a fuzzy inference model as input for an ANN model (what we call a fuzzy-neural approach), using commercially available credit score ratings as response in order to conduct the fuzzy reasoning step of the analysis. The main conclusion of our research is that, in general, our fuzzy-neural method had better results than the pure application of some market available score rating method as input to a Multi-Layer Perceptron (MLP) since it was able to reduce uncertainty by improving predictability and reducing variability of the outcomes when compared to a model with no scores. The performance of a combination of a fuzzy and a neural method was very satisfactory; the vagueness usually present in the information of a company’ database was to a certain extent, incorporated by our method with good results. From the practical perspective, although our method has not proved to be substantially better than market available options, we demonstrated that it is possible for companies to develop credit score rating mechanisms internally based on past data by using fuzzy inference systems . Under certain circumstances, companies may find this option preferable than the usual option of paying high fees to large credit scoring agencies for the use of their proprietary systems .},
  archive      = {J_ASOC},
  author       = {Diego Paganoti Fonseca and Peter Fernandes Wanke and Henrique Luiz Correa},
  doi          = {10.1016/j.asoc.2020.106329},
  journal      = {Applied Soft Computing},
  pages        = {106329},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A two-stage fuzzy neural approach for credit risk assessment in a brazilian credit card company},
  volume       = {92},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Intelligent optic disc segmentation using improved particle
swarm optimization and evolving ensemble models. <em>ASOC</em>,
<em>92</em>, 106328. (<a
href="https://doi.org/10.1016/j.asoc.2020.106328">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this research, we propose Particle Swarm Optimization (PSO)-enhanced ensemble deep neural networks for optic disc (OD) segmentation using retinal images. An improved PSO algorithm with six search mechanisms to diversify the search process is introduced. It consists of an accelerated super-ellipse action, a refined super-ellipse operation, a modified PSO operation, a random leader-based search operation, an average leader-based search operation and a spherical random walk mechanism for swarm leader enhancement. Owing to the superior segmentation capabilities of Mask R-CNN, transfer learning with a PSO-based hyper-parameter identification method is employed to generate the fine-tuned segmenters for OD segmentation. Specifically, we optimize the learning parameters, which include the learning rate and momentum of the transfer learning process, using the proposed PSO algorithm. To overcome the bias of single networks, an ensemble segmentation model is constructed. It incorporates the results of distinctive base segmenters using a pixel-level majority voting mechanism to generate the final segmentation outcome. The proposed ensemble network is evaluated using the Messidor and Drions data sets and is found to significantly outperform other deep ensemble networks and hybrid ensemble clustering models that are incorporated with both the original and state-of-the-art PSO variants. Additionally, the proposed method statistically outperforms existing studies on OD segmentation and other search methods for solving diverse unimodal and multimodal benchmark optimization functions and the detection of Diabetic Macular Edema.},
  archive      = {J_ASOC},
  author       = {Li Zhang and Chee Peng Lim},
  doi          = {10.1016/j.asoc.2020.106328},
  journal      = {Applied Soft Computing},
  pages        = {106328},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Intelligent optic disc segmentation using improved particle swarm optimization and evolving ensemble models},
  volume       = {92},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A new wind power interval prediction approach based on
reservoir computing and a quality-driven loss function. <em>ASOC</em>,
<em>92</em>, 106327. (<a
href="https://doi.org/10.1016/j.asoc.2020.106327">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Understanding the uncertainty of wind power forecasting is crucial for its practical application. This paper proposes a new forecasting approach to estimate the wind power prediction intervals (PIs) to quantify the prediction uncertainty. This approach integrates the reservoir computing methodology into a three-layer neural network architecture , and outputs the final PIs by minimizing a quality-driven loss function. The reservoir computing methodology can help study the nonlinear relationship implicit in data as well as accelerate computational time, while the quality-driven loss function is assumption-free and can help improve the forecasting capability of the proposed model. The proposed model is applied to real wind power data to test its effectiveness. Case studies show that for the data used in this paper, the proposed model can reduce the mean prediction interval width (MPIW) by up to 16.69\%, reduce root mean square error (RMSE) by up to 7.36\%, and save up to 5 times computation time compared to the benchmark models , these indicate that the proposed model has strong predictive capability .},
  archive      = {J_ASOC},
  author       = {Jianming Hu and Yingying Lin and Jingwei Tang and Jing Zhao},
  doi          = {10.1016/j.asoc.2020.106327},
  journal      = {Applied Soft Computing},
  pages        = {106327},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A new wind power interval prediction approach based on reservoir computing and a quality-driven loss function},
  volume       = {92},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). PRRAT_AM—an advanced ant-miner to extract accurate and
comprehensible classification rules. <em>ASOC</em>, <em>92</em>, 106326.
(<a href="https://doi.org/10.1016/j.asoc.2020.106326">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ant-Miner, a rule-based classification algorithm , has been successfully applied for classification tasks but it has some limitations such as getting stuck in local optima, high selective pressure, fixed exploration and exploitation rate, and premature convergence. In this paper, we have proposed a novel Ant-Miner based technique based on new Pheromone update method, Rule Rejection threshold, Adaptive gamma, and altered Tournament selection (PRRAT_AM) that caters to these limitations. The proposed algorithm introduced an adaptive gamma parameter to avoid fixed exploration and exploitation rate. To decrease the selective pressure, pheromone is updated by weighted average of rule length, rule quality and heuristic of the path. Ants are selected using improved tournament selection strategy to update the pheromone. Rules that covered less than one percent of the training examples are rejected to generate generic rules. These improvements aid PRRAT_AM in avoiding premature convergence and high selective pressure. We have tested the proposed approach on eight publicly available data-sets on standard benchmark performance measures that include accuracy and F1-score. The proposed approach has been compared with state of the art versions of Ant-Miner and with various data mining algorithms . The experimental results showed that the proposed approach achieved better results when compared with other techniques in terms of standard performance measures and convergence speed.},
  archive      = {J_ASOC},
  author       = {Umair Ayub and Hammad Naveed and Waseem Shahzad},
  doi          = {10.1016/j.asoc.2020.106326},
  journal      = {Applied Soft Computing},
  pages        = {106326},
  shortjournal = {Appl. Soft. Comput.},
  title        = {PRRAT_AM—An advanced ant-miner to extract accurate and comprehensible classification rules},
  volume       = {92},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). New caledonian crow learning algorithm: A new metaheuristic
algorithm for solving continuous optimization problems. <em>ASOC</em>,
<em>92</em>, 106325. (<a
href="https://doi.org/10.1016/j.asoc.2020.106325">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Several metaheuristic algorithms have been introduced to solve different optimization problems . Such algorithms are inspired by a wide range of natural phenomena or behaviors. We introduced a new metaheuristic algorithm called New Caledonian (NC) crow learning algorithm (NCCLA), inspired by efficient social, asocial, and reinforcement mechanisms that NC-crows use to learn behaviors for developing tools from Pandanus trees to obtain food. Such mechanisms were modeled mathematically to develop NCCLA, whose performance was subsequently evaluated and statistically analyzed using 23 classical benchmark functions and 4 engineering problems. The results verify NCCLA’s performance efficiency and highlight its accelerated convergence and ability to escape from local minima. An extensive comparative study was conducted to demonstrate that the solution accuracy and convergence rate of NCCLA were better than those of other state-of-the-art metaheuristics . The results also indicate that NCCLA is a promising algorithm that can be applied to solve other optimization and real-world problems.},
  archive      = {J_ASOC},
  author       = {Wedad Al-Sorori and Abdulqader M. Mohsen},
  doi          = {10.1016/j.asoc.2020.106325},
  journal      = {Applied Soft Computing},
  pages        = {106325},
  shortjournal = {Appl. Soft. Comput.},
  title        = {New caledonian crow learning algorithm: A new metaheuristic algorithm for solving continuous optimization problems},
  volume       = {92},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Imprecise weighted extensions of random forests for
classification and regression. <em>ASOC</em>, <em>92</em>, 106324. (<a
href="https://doi.org/10.1016/j.asoc.2020.106324">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {One of the main problems of using the random forests (RF) in classification and regression tasks is a lack of sufficient data which fall into certain leaves of trees in order to estimate the tree predicted values. To cope with this problem, robust imprecise classification and regression RF models, called the imprecise RF, are proposed. They are based on the following ideas. First, imprecision of the tree estimates is taken into account by means of imprecise statistical inference models and confidence interval models. Secondly, we introduce weights assigned to trees or to groups of trees, which are computed in order to correct the RF estimates under condition of imprecise tree predicted values. In fact, the weights can be regarded as a robust meta-learner controlling the imprecision of estimates. Special modifications of loss functions to compute optimal weights for the classification and regression tasks are proposed in order to simplify maximin optimization problems . As a result, simple linear and quadratic optimization problems are obtained, whose solution does not meet any difficulties. Various numerical examples with real datasets illustrate the proposed robust models and show outperforming results when datasets are rather small or noisy.},
  archive      = {J_ASOC},
  author       = {Lev V. Utkin and Maxim S. Kovalev and Frank P.A. Coolen},
  doi          = {10.1016/j.asoc.2020.106324},
  journal      = {Applied Soft Computing},
  pages        = {106324},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Imprecise weighted extensions of random forests for classification and regression},
  volume       = {92},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Real-time location systems selection by using a fuzzy MCDM
approach: An application in humanitarian relief logistics.
<em>ASOC</em>, <em>92</em>, 106322. (<a
href="https://doi.org/10.1016/j.asoc.2020.106322">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The real-time location systems (RTLSs) with different positioning technologies allow real-time and high-precision localization of assets. Since the usage of RTLSs technologies and their population increase, RTLSs technology selection problem that addresses many factors should be considered. Therefore, it is important to determine the performance criteria and evaluation of these technologies should be investigated before RTLSs technology is applied to the system. This paper aims to select the most appropriate RTLSs technology by using a combined fuzzy based decision-making approach. Thus, the first paper for selection of RTLSs systems in a holistic approach by combining benefit and risk factors has been revealed. The developed approach is applied to humanitarian relief logistics warehouse with four alternatives which are given as Ultra-Wide Band, Wi-Fi, UHF RFID and Active RFID for the selection. The proposed approach has been integrated with interval-valued intuitionistic fuzzy (IVIF) sets that allow to deal with fuzziness inherent in decision making processes. For this aim, firstly IVIF DEMATEL is used to determine the inner and outer dependencies of the sub and main criteria; secondly, weights of the sub-criteria are obtained by using IVIF ANP. Finally, the best RTLSs technology to be used in humanitarian logistics warehouse is selected by using IVIF TOPSIS . As a result of the calculations, the best system is determined as the “Wi-Fi RTLS” system based on the given context. By the way, a sensitivity analysis has been also implemented to test and validate the developed methodology.},
  archive      = {J_ASOC},
  author       = {Ayşenur Budak and İhsan Kaya and Ali Karaşan and Melike Erdoğan},
  doi          = {10.1016/j.asoc.2020.106322},
  journal      = {Applied Soft Computing},
  pages        = {106322},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Real-time location systems selection by using a fuzzy MCDM approach: An application in humanitarian relief logistics},
  volume       = {92},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Application of modified pigeon-inspired optimization
algorithm and constraint-objective sorting rule on multi-objective
optimal power flow problem. <em>ASOC</em>, <em>92</em>, 106321. (<a
href="https://doi.org/10.1016/j.asoc.2020.106321">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To solve the non-differentiable optimal power flow (OPF) problems with multiple contradictory objectives, a modified pigeon-inspired optimization algorithm (MPIO) is put forward in this paper. Combining with the common-used penalty function method (PFM), the MPIO-PFM algorithm is proposed and applied to optimize the active power loss, emission and fuel cost (with valve-point loadings) of power system . Eight simulation trials carried out on MATLAB software validate MPIO-PFM algorithm can obtain superior Pareto Frontier (PF) comparing with the typical NSGA-II algorithm. Nevertheless, some Pareto solutions obtained by MPIO-PFM algorithm cannot satisfy all system constraints due to the difficulty in choosing the proper penalty coefficients. Thus, an innovative approach named as constraint-objective sorting rule (COSR) is presented in this paper. The bi-objective and tri-objective trials implemented on IEEE 30-node, 57-node and 118-node systems demonstrate that the Pareto optimal set (POS) obtained by MPIO-COSR algorithm realizes zero-violation of various system constraints. Furthermore, the generational-distance and hyper-volume indexes quantitatively illustrate that in contrast to NSGA-II and MPIO-PFM methods, the MPIO-COSR algorithm can determine the evenly-distributed PFs with satisfactory-diversity. The intelligent MPIO-COSR algorithm provides an effective way to handle the non-convex MOOPF problems.},
  archive      = {J_ASOC},
  author       = {Gonggui Chen and Jie Qian and Zhizhong Zhang and Shuaiyong Li},
  doi          = {10.1016/j.asoc.2020.106321},
  journal      = {Applied Soft Computing},
  pages        = {106321},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Application of modified pigeon-inspired optimization algorithm and constraint-objective sorting rule on multi-objective optimal power flow problem},
  volume       = {92},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Inbound tourism demand forecasting framework based on fuzzy
time series and advanced optimization algorithm. <em>ASOC</em>,
<em>92</em>, 106320. (<a
href="https://doi.org/10.1016/j.asoc.2020.106320">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The tourism industry has been integrated into the national strategic system in China. Thus, tourism demand forecasting has become a concern for the sustainable development of the tourism industry. Unfortunately, the sample size for tourism in China is always small and cannot satisfy the hypothesis test of an economic model or the data volume for a traditional time series model. In this study, a novel hybrid forecasting framework combining fuzzy time series (FTS) and an atom search optimization (ASO) algorithm is proposed for inbound tourism demand forecasting; this forecasting framework is particularly suitable for small sample sizes. Specifically, information optimization technology is applied in the FTS to improve the recognition ability of the system and effectively identify small sample information. The ASO algorithm is applied to search the optimal parameters of FTS that can further improve forecasting performance. All comparison experiments and tests verify the effectiveness and superiority of our proposed model, which provides excellent forecasting results for tourism demand and a basis for policymakers and managers to plan appropriately for the tourism market.},
  archive      = {J_ASOC},
  author       = {Ping Jiang and Hufang Yang and Ranran Li and Chen Li},
  doi          = {10.1016/j.asoc.2020.106320},
  journal      = {Applied Soft Computing},
  pages        = {106320},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Inbound tourism demand forecasting framework based on fuzzy time series and advanced optimization algorithm},
  volume       = {92},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Robust fuzzy c-means clustering algorithm with adaptive
spatial &amp; intensity constraint and membership linking for noise
image segmentation. <em>ASOC</em>, <em>92</em>, 106318. (<a
href="https://doi.org/10.1016/j.asoc.2020.106318">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The fuzzy C-means (FCM) clustering method is proven to be an efficient method to segment images. However, the FCM method is not robustness and less accurate for noise images. In this paper, a modified FCM method named FCM_SICM for noise image segmentation is proposed. Firstly, fast bilateral filter is used to acquire local spatial &amp; intensity information; secondly, absolute difference image between the original image and the bilateral filtered image is employed and the reciprocal of the difference image and the difference image itself constrain conventional FCM as well as the local spatial &amp; intensity information respectively; finally, membership linking is achieved by summing all membership degrees calculated from previous iteration within every cluster in squared logarithmic form as the denominator of objective function. Experiments show that this proposed method achieves superior segmentation performance in terms of segmentation accuracy (SA), average intersection-over-union (mIoU), E-measure and number of iteration steps on mixed noise images compared with several state-of-the-art methods.},
  archive      = {J_ASOC},
  author       = {Qingsheng Wang ( Master’s candidate ) and Xiaopeng Wang Ph.D. and Chao Fang ( Ph.D. candidate ) and Wenting Yang ( Master’s candidate )},
  doi          = {10.1016/j.asoc.2020.106318},
  journal      = {Applied Soft Computing},
  pages        = {106318},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Robust fuzzy c-means clustering algorithm with adaptive spatial &amp; intensity constraint and membership linking for noise image segmentation},
  volume       = {92},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A multi-objective fuzzy robust optimization approach for
designing sustainable and reliable power systems under uncertainty.
<em>ASOC</em>, <em>92</em>, 106317. (<a
href="https://doi.org/10.1016/j.asoc.2020.106317">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A sustainable and reliable power system is extremely important to ensure the prosperity of a country and its society. Traditional power systems are facing serious environmental and social issues while renewable energy systems possess low reliability due to the intermittent nature of energy sources. This paper addresses the sustainable and reliable power system design problem in an uncertain environment by using an approach called multi-objective fuzzy robust programming. The proposed approach, which is an integration of robust programming and two main branches of fuzzy programming (possibilistic and flexible programming), solves the presented multi-objective problem by simultaneously improving both sustainability and reliability, as well as by capturing uncertain factors. The objective is to determine the optimal number, location, capacity, and technology of the generation units as well as the electricity generated and transmitted through the network while minimizing the sustainability and reliability costs of the system. The proposed model considers uncertainties in the demand, the intermittent nature of renewable energy resources , and cost parameters. A case study in Vietnam was conducted to demonstrate the efficacy and efficiency of the proposed model. Results show that the proposed model improves the total cost of the power system, including sustainability and reliability costs, by approximately 4.2\% and reduces the computational time by 20\% compared to the scenario-based stochastic programming approach. Our findings also show that due to risk disruption, the reliability cost of the power system increases to 56.72\% when more electric power is generated.},
  archive      = {J_ASOC},
  author       = {Yu-Chung Tsao and Vo-Van Thanh},
  doi          = {10.1016/j.asoc.2020.106317},
  journal      = {Applied Soft Computing},
  pages        = {106317},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A multi-objective fuzzy robust optimization approach for designing sustainable and reliable power systems under uncertainty},
  volume       = {92},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Simultaneous use of two normalization methods in
decomposition-based multi-objective evolutionary algorithms.
<em>ASOC</em>, <em>92</em>, 106316. (<a
href="https://doi.org/10.1016/j.asoc.2020.106316">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In real-world applications, the order of magnitude in each objective varies, whereas most of fitness evaluation methods in many-objective solvers are scaling dependent. Objective space normalization has a large effect on the performance of each algorithm (i.e., on the practical applicability of each algorithm to real-world problems). In order to put equal emphasis on each objective, a normalization mechanism is always encouraged to be employed in the framework of the algorithm. Decomposition-based algorithms have become more and more popular in many-objective optimization. MOEA/D is a representative decomposition-based algorithm. Recently, some negative effects of normalization have been reported, which may deteriorate the practical applicability of MOEA/D to real-world problems. In this paper, to remedy the performance deterioration introduced by normalization in MOEA/D, we propose an idea of using two types of normalization methods in MOEA/D simultaneously (denoted as MOEA/D-2N). The proposed idea is compared with the standard MOEA/D and MOEA/D with normalization (denoted as MOEA/D-N) via two widely-used test suites (as well as their variants) and a real-world optimization problem . Experimental results show that MOEA/D-2N can effectively evolve a more diverse set of solutions and achieve robust and comparable performance compared with the standard MOEA/D and MOEA/D-N.},
  archive      = {J_ASOC},
  author       = {Linjun He and Ke Shang and Hisao Ishibuchi},
  doi          = {10.1016/j.asoc.2020.106316},
  journal      = {Applied Soft Computing},
  pages        = {106316},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Simultaneous use of two normalization methods in decomposition-based multi-objective evolutionary algorithms},
  volume       = {92},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Improving the reliability of test functions generators.
<em>ASOC</em>, <em>92</em>, 106315. (<a
href="https://doi.org/10.1016/j.asoc.2020.106315">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Computational intelligence methods have gained importance in several real-world domains such as process optimization, system identification, data mining, or statistical quality control . Tools are missing, which determine the performance of computational intelligence methods in these application domains in an objective manner. Statistics provide methods for comparing algorithms on certain data sets. In the past, several test suites were presented and considered as state of the art. However, there are several drawbacks of these test suites, namely: (i) problem instances are somehow artificial and have no direct link to real-world settings; (ii) since there is a fixed number of test instances, algorithms can be fitted or tuned to this specific and very limited set of test functions; (iii) statistical tools for comparisons of several algorithms on several test problem instances are relatively complex and not easily to analyze. We propose a methodology to overcome these difficulties. It is based on standard ideas from statistics: analysis of variance and its extension to mixed models. This paper combines essential ideas from two approaches: problem generation and statistical analysis of computer experiments.},
  archive      = {J_ASOC},
  author       = {Andreas Fischbach and Thomas Bartz-Beielstein},
  doi          = {10.1016/j.asoc.2020.106315},
  journal      = {Applied Soft Computing},
  pages        = {106315},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Improving the reliability of test functions generators},
  volume       = {92},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Enhancing tree-seed algorithm via feed-back mechanism for
optimizing continuous problems. <em>ASOC</em>, <em>92</em>, 106314. (<a
href="https://doi.org/10.1016/j.asoc.2020.106314">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Tree-Seed Algorithm (TSA) is a novel population-based random search algorithm with its advantages in continuous optimization problems . However, there are some problems in its searching procedure. Problem (1) : its balance mechanism of exploration and exploitation is implemented with a constant ST , and this fixed value is unreasonable in the random search procedure; Problem (2) : the seed generation mechanism is achieved randomly without considering different searching phases based on function evaluations. To overcome these two problems, the feedback mechanism should be enhanced. Firstly, the st_TSA is proposed to solve the Problem (1); secondly, the ns_TSA is proposed to further solve the Problem (2); finally, in order to inherit these feedback mechanisms, a novel fb_TSA has been proposed and verified by standard 30 test benchmark functions from IEEE CEC 2014 with the basic TSA and its variants, such as STSA. In addition, GWO, ABC , SCA, DE, PSO and CLPSO are adopted for some comparative experiments with different dimensions. The computational results demonstrate that the enhanced feedback mechanism on ST and ns parameters can improve the optimization capability of the basic TSA significantly, especially in global optimum. The applicability of the proposed fb_TSA is proved by the 4 real engineering problems when compared with TSA, SCA, ABC and PSO .},
  archive      = {J_ASOC},
  author       = {Jianhua Jiang and Xianqiu Meng and Yunjun Chen and Chunyan Qiu and Yang Liu and Keqin Li},
  doi          = {10.1016/j.asoc.2020.106314},
  journal      = {Applied Soft Computing},
  pages        = {106314},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Enhancing tree-seed algorithm via feed-back mechanism for optimizing continuous problems},
  volume       = {92},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Hybrid FORM-sampling simulation method for finding design
point and importance vector in structural reliability. <em>ASOC</em>,
<em>92</em>, 106313. (<a
href="https://doi.org/10.1016/j.asoc.2020.106313">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {It is still a big challenge to calculate the structural reliability index. Although first order reliability method (FORM) is effective in calculating the reliability index, it encounters many obstacles due to the need for differentiation of the limit state function (LSF) and using an optimization method, particularly when the LSF is nonlinear and non-differentiable. On the other hand, although simulation methods do not suffer from none of these problems, they require a large number of random samples. Moreover, simulation methods can only calculate the failure probability ( ρ f ρf ) directly, and they are not capable of calculating the design point. In the present paper, a new hybrid FORM-sampling simulation algorithm has been proposed to calculate the reliability index, design point, and importance vector. The proposed algorithm is viable to analysis the structural reliability with few random samples by using superior capabilities of importance sampling and step-by-step correction of the standard deviation (SD) of variables associated with the sampling density function. Furthermore, the LSF has been well approximated using the artificial neural network (ANN), leading to a significant reduction in the computation time. The efficiency of the present algorithm is illustrated through some examples in comparison to conventional methods.},
  archive      = {J_ASOC},
  author       = {Kiyanoosh Malakzadeh and Maryam Daei},
  doi          = {10.1016/j.asoc.2020.106313},
  journal      = {Applied Soft Computing},
  pages        = {106313},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Hybrid FORM-sampling simulation method for finding design point and importance vector in structural reliability},
  volume       = {92},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Multi-robot path planning using improved particle swarm
optimization algorithm through novel evolutionary operators.
<em>ASOC</em>, <em>92</em>, 106312. (<a
href="https://doi.org/10.1016/j.asoc.2020.106312">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The highlight of this paper is to propose an innovative approach to compute an optimal collision free trajectory path for each robot in a known and complex environment. The problem under consideration has been solved by employing an improved version of particle swarm optimization (IPSO) with evolutionary operators (EOPs). In the present context, PSO is improved with the concept of governance in human society and two evolutionary operators such as multi-crossover inherited from the genetic algorithm , and bee colony operator to enhance the intensification capability of the IPSO algorithm. The algorithm proposed to compute the deadlock free subsequent coordinate of an individual robot from their present coordinate, in addition, to minimize the path length for each robot by maintaining a good balance between intensification and diversification. Results obtained from the proposed IPSO-EOPs have been compared with competitors such as DE and IPSO in a similar environment to substantiate the robustness and usefulness of the algorithm. It perceives from the result obtained from simulation and experimentation that IPSO-EOPs is succeeding IPSO, and DE in terms of arrival time, generating a safe optimal path, and energy utilization during the travel.},
  archive      = {J_ASOC},
  author       = {P.K. Das and P.K. Jena},
  doi          = {10.1016/j.asoc.2020.106312},
  journal      = {Applied Soft Computing},
  pages        = {106312},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Multi-robot path planning using improved particle swarm optimization algorithm through novel evolutionary operators},
  volume       = {92},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Highly interpretable hierarchical deep rule-based
classifier. <em>ASOC</em>, <em>92</em>, 106310. (<a
href="https://doi.org/10.1016/j.asoc.2020.106310">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Pioneering the traditional fuzzy rule-based (FRB) systems, deep rule-based (DRB) classifiers are able to offer both human-level performance and transparent system structure on image classification problems by integrating zero-order fuzzy rule base with a multi-layer image-processing architecture that is typical for deep learning . Nonetheless, it is frequently observed that the inner structure of DRB can become over sophisticated and not interpretable for humans when applied to large-scale, complex problems. To tackle the issue, one feasible solution is to construct a tree structural classification model by aggregating the possibly huge number of prototypes identified from data into a much smaller number of more descriptive and highly abstract ones. Therefore, in this paper, we present a novel hierarchical deep rule-based (H-DRB) approach that is capable of summarizing the less descriptive raw prototypes into highly generalized ones and self-arranging them into a hierarchical prototype-based structure according to their descriptive abilities. By doing so, H-DRB can offer high-level performance and, most importantly, full transparency and human-interpretability on various problems including large-scale ones. The proposed concept and generical principles are verified through numerical experiments based on a wide variety of popular benchmark image sets. Numerical results demonstrate that the promise of H-DRB.},
  archive      = {J_ASOC},
  author       = {Xiaowei Gu and Plamen P. Angelov},
  doi          = {10.1016/j.asoc.2020.106310},
  journal      = {Applied Soft Computing},
  pages        = {106310},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Highly interpretable hierarchical deep rule-based classifier},
  volume       = {92},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Generalised fuzzy cognitive maps: Considering the time
dynamics between a cause and an effect. <em>ASOC</em>, <em>92</em>,
106309. (<a href="https://doi.org/10.1016/j.asoc.2020.106309">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fuzzy Cognitive Maps (FCMs) have been used to quantitatively model the dynamics of complex systems and predict their behaviours. However, they are usually unable to address the issues arising from time lags between causes and effects. Accordingly, Generalised Fuzzy Cognitive Maps (GFCMs) have been introduced to overcome this problem. This article deals with a breed of GFCMs that addresses time lags between cause(s) and effect(s), demonstrated by a case-study that deals with the social, economic and technological consequences of heavy rainfall in Kampala, Uganda. The results show that the inclusion of time lags alters both, the final steady-state values of the social, economic and technological consequences of heavy rainfall and the time taken to stabilise. Thus, the inclusion of time lags increases the reliability of GFCMs as a means to quantitatively model the dynamics of complex systems.},
  archive      = {J_ASOC},
  author       = {Abhishek Nair and Diana Reckien and M.F.A.M van Maarseveen},
  doi          = {10.1016/j.asoc.2020.106309},
  journal      = {Applied Soft Computing},
  pages        = {106309},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Generalised fuzzy cognitive maps: Considering the time dynamics between a cause and an effect},
  volume       = {92},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A novel case adaptation method based on differential
evolution algorithm for disaster emergency. <em>ASOC</em>, <em>92</em>,
106306. (<a href="https://doi.org/10.1016/j.asoc.2020.106306">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {When disasters happen, time is often very urgent. Case-based reasoning (CBR) is one of the most effective approaches to support disaster emergency management. CBR takes good use of historical case data, which is one of the typical data-driven decision-making methods. Among the steps of CBR, adaptation is the core. To improve the adaptation, a hybrid mutation operator is implemented, and a new differential evolution (DE) algorithm is developed. An adaptation method based on the proposed algorithm is put forward to achieve case adaptation in the CBR system . The comparison results have shown that the proposed algorithm is superior compared with the state-of-art algorithms. Then, experiments of CBR have revealed that the adaptation method can effectively generate appropriate solutions with the help of the proposed algorithm.},
  archive      = {J_ASOC},
  author       = {Xiaobing Yu and Chenliang Li and Wen-Xuan Zhao and Hong Chen},
  doi          = {10.1016/j.asoc.2020.106306},
  journal      = {Applied Soft Computing},
  pages        = {106306},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A novel case adaptation method based on differential evolution algorithm for disaster emergency},
  volume       = {92},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). RBFNN based terminal sliding mode adaptive control for
electric ground vehicles after tire blowout on expressway.
<em>ASOC</em>, <em>92</em>, 106304. (<a
href="https://doi.org/10.1016/j.asoc.2020.106304">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes a radial basis function neural network (RBFNN) based terminal sliding mode control scheme for electric ground vehicles subject to tire blowout on expressway in presence of tire nonlinearities , unmodeled dynamics and external disturbances . For enhancing the longitudinal and lateral stability of the vehicle after tire blowout, a saturated velocity planner is firstly constructed for tracking the original motion trajectory, by which the longitudinal velocity and yaw rate saturation constraints can be effectively handled. Afterwards, a terminal sliding mode controller (TSMC) is designed for tracking the planned velocity signals because of its inherent finite time convergence rate and superior steady-state property, by which the adverse dynamic behaviors can be timely suppressed. Further, to strengthen the adaptability and robustness of the control scheme, a RBFNN approximator is developed for identifying the lumped uncertainty, such as tire nonlinearities , unmodeled dynamics and external disturbances , etc., and then compensated into the controller. Lastly, simulations with front-right tire blowout on expressway are performed to validate the effectiveness and efficiency of presented control scheme and methods, and the comprehensive performance of TSMC+RBFNN and TSMC schemes in maintaining original trajectory tracking capacity is evaluated and discussed.},
  archive      = {J_ASOC},
  author       = {Lu Yang and Ming Yue and Yuanchang Liu and Lie Guo},
  doi          = {10.1016/j.asoc.2020.106304},
  journal      = {Applied Soft Computing},
  pages        = {106304},
  shortjournal = {Appl. Soft. Comput.},
  title        = {RBFNN based terminal sliding mode adaptive control for electric ground vehicles after tire blowout on expressway},
  volume       = {92},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A fast surrogate-assisted particle swarm optimization
algorithm for computationally expensive problems. <em>ASOC</em>,
<em>92</em>, 106303. (<a
href="https://doi.org/10.1016/j.asoc.2020.106303">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Although many surrogate-assisted evolutionary algorithms (SAEAs) have been proposed to solve computationally expensive problems, they usually need to consume plenty of expensive evaluations to obtain an acceptable solution. In this paper, we proposed a fast surrogate-assisted particle swarm optimization (FSAPSO) algorithm to solve medium scaled computationally expensive problems through a small number of function evaluations (FEs). Two criteria are applied in tandem to select candidates for exact evaluations. The performance-based criterion is used to exploit the current global best and accelerate the convergence rate, while the uncertainty-based criterion is used to enhance the exploration of the algorithm. The distance-based uncertainty criterion in SAEAs does not consider the fitness landscape of different problems. Therefore, we developed a criterion to estimate uncertainty by considering the distance and fitness value information simultaneously. This criterion can make up for the disadvantage of the conventional distance-based uncertainty criterion by considering the fitness landscape of a problem. In addition, it can be applied in any surrogate-assisted evolutionary algorithm irrespective of the used surrogate model . Twenty-three benchmark functions widely adopted in the literature and a 10-dimension propeller design problem are used to test the proposed approach. Experimental results demonstrate the superiority of the proposed FSAPSO algorithm over seven state-of-the-art algorithms.},
  archive      = {J_ASOC},
  author       = {Fan Li and Weiming Shen and Xiwen Cai and Liang Gao and G. Gary Wang},
  doi          = {10.1016/j.asoc.2020.106303},
  journal      = {Applied Soft Computing},
  pages        = {106303},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A fast surrogate-assisted particle swarm optimization algorithm for computationally expensive problems},
  volume       = {92},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Robust fusion for RGB-d tracking using CNN features.
<em>ASOC</em>, <em>92</em>, 106302. (<a
href="https://doi.org/10.1016/j.asoc.2020.106302">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, RGB-D sensors have become popular. Many computer vision problems can be better dealt with depth data. It is a challenging problem to integrate depth data into a visual object tracker to address the problems such as scale change and occlusion. In this paper, we propose a robust fusion based RGB-D tracking method. Specifically, hierarchical convolutional neural network (CNN) features are first adopted to encode RGB and depth images separately. Next, target is tracked based on correlation filter tracking framework. Then the results of each CNN feature are localized according to the tracking results in a short period of time. Finally, the target is localized by jointly fusing the results of RGB and depth images. Model updating is finally carried out according to the differences between RGB and depth images. Experiments on the University of Birmingham RGB-D Tracking Benchmark (BTB) and the Princeton RGB-D Tracking Benchmark (PTB) achieve comparable results to state-of-the-art methods.},
  archive      = {J_ASOC},
  author       = {Yong Wang and Xian Wei and Hao Shen and Lu Ding and Jiuqing Wan},
  doi          = {10.1016/j.asoc.2020.106302},
  journal      = {Applied Soft Computing},
  pages        = {106302},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Robust fusion for RGB-D tracking using CNN features},
  volume       = {92},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A survey and taxonomy of the fuzzy signature-based intrusion
detection systems. <em>ASOC</em>, <em>92</em>, 106301. (<a
href="https://doi.org/10.1016/j.asoc.2020.106301">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Organizations that benefit from information technologies are vulnerable to various attacks and malicious behaviors . Intrusion Detection Systems (IDS) are one of the main lines of defense which in conjunction with firewalls and other security components are applied to deal with intrusions and unauthorized misbehaviors. Misuse detection is one of the main branches of the intrusion detection which intends to prevent known security attacks regarding their previously known signatures. This paper presents a comprehensive investigation of the fuzzy misuse detection schemes designed using various machine learning and data mining techniques to deal with different kinds of intrusions. For this purpose, it first presents the key points and knowledge about intrusion detection and then classifies the fuzzy misuse detection approaches regarding their applied fuzzy techniques and algorithms. Then, it illustrates the major contributions of the fuzzy IDS schemes and illuminates their merits and limitations. Besides, in each section, the comparison of their applied datasets, performance evaluation factors, feature extraction methods as well as the type of fuzzy logic controller (FLC) and membership functions are provided. Finally, the concluding issues and the directions for future researches are highlighted.},
  archive      = {J_ASOC},
  author       = {Mohammad Masdari and Hemn Khezri},
  doi          = {10.1016/j.asoc.2020.106301},
  journal      = {Applied Soft Computing},
  pages        = {106301},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A survey and taxonomy of the fuzzy signature-based intrusion detection systems},
  volume       = {92},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Deep belief network and linear perceptron based cognitive
computing for collaborative robots. <em>ASOC</em>, <em>92</em>, 106300.
(<a href="https://doi.org/10.1016/j.asoc.2020.106300">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Objective: This paper is to analyze the performance of the control system of collaborative robots based on cognitive computing technology. Methods: This study combines cognitive computing and deep belief network algorithms with collaborative robots to construct a cognitive computing system model based on deep belief networks, which is applied to the control system of collaborative robots. Further, the simulation is used to compare and analyze the algorithm performance of deep belief network (DBN), multilayer perceptron (MLP) and the cognitive computing system model of deep belief network and linear perceptron (DBNLP) proposed in this study. Results: The results show that compared with the DBN and MLP algorithms, the DBNLP algorithm model has a significantly lower error rate in the number of repetitions of the training set, the number of hidden neurons , and the number of network layers. And the number of task backlog, the number of resources to be allocated and the time consumption are less, as well as the accuracy is high. After comparing and analyzing the changes in the estimated value of Ex (expected value), En (entropy value) and He (hyper entropy value), it is found that the estimated value of the DBNLP algorithm model is closer to the true value than that of the DBN and MLP algorithms. Conclusion: The application of the DBNLP algorithm model to collaborative robots can significantly improve its accuracy and safety, providing an experimental basis for the performance improvement of later collaborative robots.},
  archive      = {J_ASOC},
  author       = {Zhihan Lv and Liang Qiao},
  doi          = {10.1016/j.asoc.2020.106300},
  journal      = {Applied Soft Computing},
  pages        = {106300},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Deep belief network and linear perceptron based cognitive computing for collaborative robots},
  volume       = {92},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Feature selection via normative fuzzy information weight
with application into tumor classification. <em>ASOC</em>, <em>92</em>,
106299. (<a href="https://doi.org/10.1016/j.asoc.2020.106299">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Feature selection via mutual information has been widely used in data analysing. Mutual information with monotonous is an effective tool to analyse the correlation and redundancy of features. However, the mutual information, which adopted in most of existing feature selection criterions, can’t explain the correlation and redundancy of features in the fuzzy situation well. Therefore, we propose feature selection strategy via normative fuzzy information weight based on fuzzy conditional mutual information in this paper. Firstly, the monotone fuzzy metric structure is defined, and some theoretical properties are proved. Secondly, we put forward the concept of fuzzy independent classification information based on fuzzy conditional mutual information, and propose a feature selection method via fuzzy independent classification information. Thirdly, considering the proportion of new classification information provided by the selected feature in its own information, we introduce the concept of normative fuzzy information weight and propose an improved feature selection method. Finally, the availability of the two proposed methods is tested by comparative experiments , and the improved feature selection method is applied to tumor classification. This work provides an alternative strategy for feature selection in real-world data applications.},
  archive      = {J_ASOC},
  author       = {Jianhua Dai and Jiaolong Chen},
  doi          = {10.1016/j.asoc.2020.106299},
  journal      = {Applied Soft Computing},
  pages        = {106299},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Feature selection via normative fuzzy information weight with application into tumor classification},
  volume       = {92},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Applying deep learning algorithms to enhance simulations of
large-scale groundwater flow in IoTs. <em>ASOC</em>, <em>92</em>,
106298. (<a href="https://doi.org/10.1016/j.asoc.2020.106298">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning for enhancing simulation IoTs groundwater flow is a good solution for gaining insights into the behavior of aquifer systems. In previous studies, corresponding results give a basis for the rational management of groundwater resources. The users generally require special skills or knowledge and massive observations in representing the field reality to perform the deep learning algorithms and simulations. To simplify the procedures for performing the numerical and large-scale groundwater flow simulations, we apply the deep learning algorithms which combine both the numerical groundwater model and large-scale IoTs, groundwater flow measuring equipment and various complex groundwater numerical models. The mechanism has the capability to show spatial distributions of in-situ data, analyze the spatial relationships of observed data, generate meshes , update users’ databases with in-situ observed data, and create professional reports. According to the numerical simulation results, we revealed that the deep learning algorithms are high computational efficiency, and we can enhance precise variance estimations for large-scale groundwater flow problems. The findings help users to best apply the deep learning algorithms in an easier way, get more accurate simulation results, and manage the groundwater resources rationally.},
  archive      = {J_ASOC},
  author       = {Yu-Sen Su and Chuen-Fa Ni and Wei-Ci Li and I-Hsien Lee and Chi-Ping Lin},
  doi          = {10.1016/j.asoc.2020.106298},
  journal      = {Applied Soft Computing},
  pages        = {106298},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Applying deep learning algorithms to enhance simulations of large-scale groundwater flow in IoTs},
  volume       = {92},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). DenseAttentionSeg: Segment hands from interacted objects
using depth input. <em>ASOC</em>, <em>92</em>, 106297. (<a
href="https://doi.org/10.1016/j.asoc.2020.106297">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hand segmentation is an important task in computer vision , which is usually the foundation of hand pose recognition, hand tracking, and reconstruction. For hand segmentation, it is more challenging when the hand is interacting with objects, but handling interacting motions is more important for applications like HCI and VR. In this paper, we propose a real-time DNN-based technique to segment hand and object in interacting motions from a single depth input. Our model is called DenseAttentionSeg, which contains a dense attention mechanism which effectively fuses information in different scales and improves the quality of result with skip-connections. Besides, we introduce a contour loss in model training, which helps to generate accurate hand and object boundaries. Finally, we propose our InterSegHands dataset, a fine-scale hand segmentation dataset containing about 52k depth maps of hand-object interactions, with the ground truth segmentation masks. Our experiments evaluate the effectiveness of our techniques and datasets, and indicate that our method outperforms the current state-of-the-art deep segmentation methods in handling hand-object interactions.},
  archive      = {J_ASOC},
  author       = {Zi-Hao Bo and Hao Zhang and Jun-Hai Yong and Hao Gao and Feng Xu},
  doi          = {10.1016/j.asoc.2020.106297},
  journal      = {Applied Soft Computing},
  pages        = {106297},
  shortjournal = {Appl. Soft. Comput.},
  title        = {DenseAttentionSeg: Segment hands from interacted objects using depth input},
  volume       = {92},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Multi-scale single image rain removal using a
squeeze-and-excitation residual network. <em>ASOC</em>, <em>92</em>,
106296. (<a href="https://doi.org/10.1016/j.asoc.2020.106296">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Rain adversely affects the performance of collaborative robots in outdoor applications. In machine vision, single image rain removal is an extremely difficult problem due to the disordered and irregular rain streaks in the image. Existing methods either fail to achieve satisfactory rain removal results or destroy image details. In this paper, we propose a novel multi-scale rain removal model to address these problems by decomposing images into base layers and detail layers. The proposed method adapts a two-branch squeeze-and-excitation residual network architecture that learns the basic structure and texture details of the corresponding clean image. By decomposing the image into multiple layers and merging these layers, the network can effectively remove rain streaks from an image to restore its structural information and details. Extensive experiments on synthetic and real datasets demonstrate that the proposed method significantly outperforms recent state-of-the-art algorithms in terms of both qualitative and quantitative measures.},
  archive      = {J_ASOC},
  author       = {Rushi Lan and Xipu Hu and Cheng Pang and Zhenbing Liu and Xiaonan Luo},
  doi          = {10.1016/j.asoc.2020.106296},
  journal      = {Applied Soft Computing},
  pages        = {106296},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Multi-scale single image rain removal using a squeeze-and-excitation residual network},
  volume       = {92},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A topology-based single-pool decomposition framework for
large-scale global optimization. <em>ASOC</em>, <em>92</em>, 106295. (<a
href="https://doi.org/10.1016/j.asoc.2020.106295">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Identification of variable interaction plays a crucial role in applying a divide-and-conquer algorithm for large-scale black-box optimization. However, most of the existing decomposition methods are less efficient in decomposing the overlapping problems. This drawback diminishes the practicality of the existing methods. In this paper, we propose an efficient single-pool decomposition framework (SPDF). The interactions of decision variables are identified in an ordinal fashion. The unbalanced grouping efficiency of the existing decomposition methods can be significantly alleviated. Furthermore, we find that the grouping efficiency can be further improved by integrating the topological information into the decomposition process . In many real-world problems, this information can be 1-, 2- or 3-dimensional coordinates, which represent the geometric structure of the large-scale systems. Based on this, we propose a topology-based decomposition method, which we call Topology-based Single-Pool Differential Grouping (TSPDG). The efficacy of our proposed methods is demonstrated on the CEC’2010 and the CEC’2013 large-scale benchmark suites, as well as a practical case study in production optimization.},
  archive      = {J_ASOC},
  author       = {Xiaoming Xue and Kai Zhang and Rupeng Li and Liming Zhang and Chuanjin Yao and Jian Wang and Jun Yao},
  doi          = {10.1016/j.asoc.2020.106295},
  journal      = {Applied Soft Computing},
  pages        = {106295},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A topology-based single-pool decomposition framework for large-scale global optimization},
  volume       = {92},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A new combined model based on multi-objective salp swarm
optimization for wind speed forecasting. <em>ASOC</em>, <em>92</em>,
106294. (<a href="https://doi.org/10.1016/j.asoc.2020.106294">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Wind energy as the representative renewable energy sources attracted the global attention and wind power plays a significant role in power system . Thus, wind speed forecasting is highly critical in wind power grid management. The short-term wind speed prediction can effectively support power grid-management to reduce wind curtailments . In the past, lots of researches had often considered how to enhance the accuracy or stability in short wind speed forecasting. Nevertheless, just focus on one criterion is the inability to build an effective predictive system. In this paper, a novel combined forecasting system was proposed and effectively applied to address the issue of wind speed prediction while obtaining high precision and strong stability simultaneously at the same time. Four ANNs (artificial neural networks) were combined by the optimal weighting coefficients determined by MSSO (multi-objective salp swarm optimizer) in this system and data decomposition and denoising are included in the data preprocessing stage. The multi-objective optimization algorithm overcomes the weakness of the single-objective optimization algorithm that can only achieve one criterion. It can simultaneously optimize accuracy and stability. The 10-minute wind speed data of three data sets of Penglai, China were selected for multi-step forecasting to evaluate the effectiveness of the proposed combined model. And experimental results show that the proposed model not only achieves excellent precision and stability but also outperforms other proposed combined models.},
  archive      = {J_ASOC},
  author       = {Zishu Cheng and Jiyang Wang},
  doi          = {10.1016/j.asoc.2020.106294},
  journal      = {Applied Soft Computing},
  pages        = {106294},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A new combined model based on multi-objective salp swarm optimization for wind speed forecasting},
  volume       = {92},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). An improved sine–cosine algorithm for simultaneous network
reconfiguration and DG allocation in power distribution systems.
<em>ASOC</em>, <em>92</em>, 106293. (<a
href="https://doi.org/10.1016/j.asoc.2020.106293">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a recently proposed meta-heuristic sine–cosine algorithm combined with levy flights to reconfigure the distribution network with simultaneous allocation (placement and size) of multiple distributed generators (DGs). The algorithm is proposed to be adaptive with an exponentially decreasing conversion parameter and a self-controlled levy mutation in order to explore the solution space more efficiently during the course of iterations. The effectiveness of the algorithm is verified on 10 standard benchmark functions . Later, it is used to address the issues of a real combinatorial optimization , such as network reconfiguration (NR) in the presence of DGs. In order to enhance the effectiveness of the system, a multi-objective function is developed considering total active power loss and overall voltage stability of the network with suitable weights without violating the system limitations. To evaluate the objective function, a depth fast search integrated forward–backward sweep based load flow technique that is capable of managing any topological alterations owing to the NR and DG integration is developed. In order to demonstrate the efficiency of the system, four distinct cases of NR and DG installation are investigated. The proposed algorithm is contrasted with other well-known algorithms that exist in the literature, namely, harmony search algorithm (HSA), fireworks algorithm (FWA), genetic algorithm (GA), refined genetic algorithm (RGA) and firefly (FF) algorithm considering 33 and 69-bus distribution systems at three different load levels and its superiority is established.},
  archive      = {J_ASOC},
  author       = {Usharani Raut and Sivkumar Mishra},
  doi          = {10.1016/j.asoc.2020.106293},
  journal      = {Applied Soft Computing},
  pages        = {106293},
  shortjournal = {Appl. Soft. Comput.},
  title        = {An improved sine–cosine algorithm for simultaneous network reconfiguration and DG allocation in power distribution systems},
  volume       = {92},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Soft computing models for predicting blast-induced air
over-pressure: A novel artificial intelligence approach. <em>ASOC</em>,
<em>92</em>, 106292. (<a
href="https://doi.org/10.1016/j.asoc.2020.106292">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Applying soft computing models for solving real-life problems has yielded many significant benefits, especially in the mining industry. This study proposed a novel soft computing model for estimating blast-induced air over-pressure (AOp) with high accuracy. Accordingly, the boosted smoothing spline (BSTSM) and genetic algorithm (GA) were considered and combined, namely GA-BSTSM model. One hundred twenty-one blasts were collected at the Coc Sau open-pit coal mine (Vietnam) for this aim. The explosive capacity used (W) and distance (R) were considered as the primary input variables for the aiming of AOp prediction. Also, the meteorological conditions such as temperature (T), relative humidity (RH), atmospheric pressure (AP), wind speed (WS), and wind direction (WD) were taken into account to predict AOp in this study. To confirm the performance of the proposed GA-BSTSM model, an empirical model and six other artificial intelligence models were also developed to predict AOp based on the same dataset, including CART (classification and regression tree), KNN (k-nearest neighbors), ANN (artificial neural network), BRR (Bayesian ridge regression), SVR (support vector regression), and Gaussian process (GP). The developed models were then evaluated through three performance indices (i.e., RMSE , R 2 , and VAF) using the testing dataset . In addition, a Taylor diagram was also developed to evaluate the quality of the models. The evaluation results showed that the proposed GA-BSTSM model yielded a robust performance with high accuracy for AOp prediction herein. The findings also disclosed that meteorological factors have a strong influence on the accuracy of AOp predictive models , especially RH and WS.},
  archive      = {J_ASOC},
  author       = {Hoang Nguyen and Xuan-Nam Bui},
  doi          = {10.1016/j.asoc.2020.106292},
  journal      = {Applied Soft Computing},
  pages        = {106292},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Soft computing models for predicting blast-induced air over-pressure: A novel artificial intelligence approach},
  volume       = {92},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). The automatic segmentation of residential solar panels based
on satellite images: A cross learning driven u-net method.
<em>ASOC</em>, <em>92</em>, 106283. (<a
href="https://doi.org/10.1016/j.asoc.2020.106283">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Segmenting small-scale residential solar panels (RSPs) based on satellite images is an emerging data science problem in the renewable energy field. In this paper, we develop a cross learning driven U-Net (CrossNets) method and its extension, adaptive CrossNets, to automatically segment RSPs in satellite images. Proposed methods employ a group of generic U-Nets as a community and target to enhance the RSP segmentation performance . First, parameters of each generic U-Net in the community of CrossNets are initialized individually via the initialization with transfer learning and the classical initialization methods . Next, a novel training mechanism, cross learning, is developed to serve as a constraint for better optimizing CrossNets. Based on cross learning, each generic U-Net in the community first individually updates parameters at every epoch and next learns parameters from the best individual at specific epochs. Cross learning relieves the reliance of generic U-Nets on a careful initialization and better optimizes U-Nets. In testing, the result of the best performed generic U-Net in the community is selected as the final segmentation result of CrossNets. Adaptive CrossNets, a variant of CrossNets, is developed by applying an additional threshold to reduce the possibility of over-learning caused by cross learning. Satellite images collected from one city in U.S. are utilized to validate the performance of proposed methods. These images cover a large area of 135 km 2 with 2794 RSPs. Compared with two generic U-Nets based benchmarks, our method can enhance the overall segmentation IoU by around 34\% and 1.5\%. Moreover, the segmentation robustness is improved from 1.191e−2 and 1.286e−4 to 2.481e−5. In addition, two new image datasets collected from other two cities in U.S. are applied to further examine the applicability of proposed methods.},
  archive      = {J_ASOC},
  author       = {Li Zhuang and Zijun Zhang and Long Wang},
  doi          = {10.1016/j.asoc.2020.106283},
  journal      = {Applied Soft Computing},
  pages        = {106283},
  shortjournal = {Appl. Soft. Comput.},
  title        = {The automatic segmentation of residential solar panels based on satellite images: A cross learning driven U-net method},
  volume       = {92},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Convolutional descriptors aggregation via cross-net for skin
lesion recognition. <em>ASOC</em>, <em>92</em>, 106281. (<a
href="https://doi.org/10.1016/j.asoc.2020.106281">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Malignant melanoma is one of the rare but deadliest types of skin cancers. Clinically, the early diagnosis of this disease is based on human visual inspection with dermoscopy imaging. However, human observations are subjective and prone to errors due to huge variations within dermoscopy images. To address it, we propose a framework for automatic skin lesion recognition using cross-net based aggregation of multiple convolutional networks . The output activation maps of each network are extracted as indicator maps to select the local deep convolutional descriptors (i.e., local patterns and color) in dermoscopy images. Also, this map of a convolutional layer captures the semantic regions of the input image and localizes the target object. These selected features are aggregated into an informative feature map, which are potentially better preserved in the convolutional feature maps. Finally, we use Fisher vector (FV) to encode the selected features. Extensive experiments demonstrate the effectiveness of our proposed method. Comparing with aggregation strategy using pooling approaches, the proposed method learns more robust and discriminative representations based on the publicly available skin lesion challenge datasets from the International Symposium on Biomedical Imaging (ISBI) 2016 and 2017.},
  archive      = {J_ASOC},
  author       = {Zhen Yu and Feng Jiang and Feng Zhou and Xinzi He and Dong Ni and Siping Chen and Tianfu Wang and Baiying Lei},
  doi          = {10.1016/j.asoc.2020.106281},
  journal      = {Applied Soft Computing},
  pages        = {106281},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Convolutional descriptors aggregation via cross-net for skin lesion recognition},
  volume       = {92},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A learnheuristic approach for the team orienteering problem
with aerial drone motion constraints. <em>ASOC</em>, <em>92</em>,
106280. (<a href="https://doi.org/10.1016/j.asoc.2020.106280">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This work proposes a learnheuristic approach (combination of heuristics with machine learning) to solve an aerial-drone team orienteering problem. The goal is to maximise the total reward collected from information gathering or surveillance observations of a set of known targets within a fixed amount of time. The aerial drone team orienteering problem has the complicating feature that the travel times between targets depend on a drone’s flight path between previous targets. This path-dependence is caused by the aerial surveillance drones flying under the influence of air-resistance, gravity, and the laws of motion. Sharp turns slow drones down and the angle of ascent and air-resistance influence the acceleration a drone is capable of. The route dependence of inter-target travel times motivates the consideration of a learnheuristic approach, in which the prediction of travel times is outsourced to a machine learning algorithm. This work proposes an instance-based learning algorithm with interpolated predictions as the learning module. We show that a learnheuristic approach can lead to higher quality solutions in a shorter amount of time than those generated from an equivalent metaheuristic algorithm , an effect attributed to the search-diversity enhancing consequence of the online learning process.},
  archive      = {J_ASOC},
  author       = {Christopher Bayliss and Angel A. Juan and Christine S.M. Currie and Javier Panadero},
  doi          = {10.1016/j.asoc.2020.106280},
  journal      = {Applied Soft Computing},
  pages        = {106280},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A learnheuristic approach for the team orienteering problem with aerial drone motion constraints},
  volume       = {92},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A new fuzzy approach based on BWM and fuzzy preference
programming for hospital performance evaluation: A case study.
<em>ASOC</em>, <em>92</em>, 106279. (<a
href="https://doi.org/10.1016/j.asoc.2020.106279">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hospital performance evaluation (HPE) has a major role in improving the quality, safety, and effectiveness of health care services , so it is indispensable for proper and continuous operation of hospitals. Although several studies have been performed on HPE, few have used group decision-making (GDM). This study presents a comprehensive multi-criteria GDM model for HPE under uncertain conditions. In this model, we have combined the group best–worst​ method (GBWM) and fuzzy preference programming (FPP) method to create an applicable framework for GDM in which members of a decision-making group including decision-makers (DMs) have different expertise and the importance of criteria and DMs opinions are determined by a supervisor. The advantages of the proposed method include the integration of the GDM process in the form of a single model and there is no need to calculate separately the consistency of the decisions of the decision-making team members. Finally, a case study conducted on 5 hospitals in Tehran is presented to demonstrate the applicability and effectiveness of the proposed method. The results show that Sina Hospital, Baharloo Hospital, and Tehran Heart Center were ranked first to third, respectively. Also, we can conclude from this study that the proposed integrated framework is capable to address the HPE problem by using a GDM and considering the uncertainty of the comparisons made by decision-making team members.},
  archive      = {J_ASOC},
  author       = {Maghsoud Amiri and Mohammad Hashemi-Tabatabaei and Mohammad Ghahremanloo and Mehdi Keshavarz-Ghorabaee and Edmundas Kazimieras Zavadskas and Jurgita Antucheviciene},
  doi          = {10.1016/j.asoc.2020.106279},
  journal      = {Applied Soft Computing},
  pages        = {106279},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A new fuzzy approach based on BWM and fuzzy preference programming for hospital performance evaluation: A case study},
  volume       = {92},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Online RBM: Growing restricted boltzmann machine on the fly
for unsupervised representation. <em>ASOC</em>, <em>92</em>, 106278. (<a
href="https://doi.org/10.1016/j.asoc.2020.106278">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this work, we endeavor to investigate and propose a novel unsupervised online learning algorithm , namely the Online Restricted Boltzmann Machine (O-RBM). The O-RBM is able to construct and adapt the architecture of a Restricted Boltzmann Machine (RBM) artificial neural network , according to the statistics of the streaming input data. Specifically, for a training data that is not fully available at the onset of training, the proposed O-RBM begins with a single neuron in the hidden layer of the RBM, progressively adds and suitably adapts the network to account for the variations in streaming data distributions. Such an unsupervised learning helps to effectively model the probability distribution of the entire data stream, and generates robust features. We will demonstrate that such unsupervised representations can be used for discriminative classifications on a set of multi-category and binary classification problems for unstructured image and structured signal data sets, having varying degrees of class-imbalance. We first demonstrate the O-RBM algorithm and characterize the network evolution using the simple and conventional multi-class MNIST image dataset, aimed at recognizing hand-written digit. We then benchmark O-RBM performance to other machine learning , neural network and Class RBM techniques using a number of public non-stationary datasets. Finally, we study the performance of the O-RBM on a real-world problem involving predictive maintenance of an aircraft component using time series data . In all these studies, it is observed that the O-RBM converges to a stable, concise network architecture , wherein individual neurons are inherently discriminative to the class labels despite unsupervised training. It can be observed from the performance results that on an average O-RBM improves accuracy by 2.5\%–3\% over conventional offline batch learning techniques while requiring at least 24\%–70\% fewer neurons.},
  archive      = {J_ASOC},
  author       = {Ramasamy Savitha and ArulMurugan Ambikapathi and Kanagasabai Rajaraman},
  doi          = {10.1016/j.asoc.2020.106278},
  journal      = {Applied Soft Computing},
  pages        = {106278},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Online RBM: Growing restricted boltzmann machine on the fly for unsupervised representation},
  volume       = {92},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Local gradient full-scale transform patterns based off-line
text-independent writer identification. <em>ASOC</em>, <em>92</em>,
106277. (<a href="https://doi.org/10.1016/j.asoc.2020.106277">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Handwriting based writer identification is one of the reliable components of behavioral biometrics . A huge effort has been done in recent years to improve the writer identification performance. Our paper presents a new and effective off-line text-independent system for writer identification. Extracting features from handwriting substantially impacts the ability of the classification process to identify the query writers. With the use of suitable classifier, a well-designed and discriminative feature extraction improves the classification performance. For that, we introduce a discriminative yet simple feature method, referred to as Local gradient full-Scale Transform Patterns (LSTP). The proposed LSTP algorithm captures salient local writing structure at small regions of interest of the writing. These writing regions are termed as connected components. In the classification stage, we perform Hamming distance based NN classifier to compare and match LSTP feature vectors. The proposed framework is evaluated on 9 well-known handwritten benchmarks. Experimental results show high identification performance against the current state-of-the-art.},
  archive      = {J_ASOC},
  author       = {Abderrazak Chahi and Youssef El merabet and Yassine Ruichek and Raja Touahni},
  doi          = {10.1016/j.asoc.2020.106277},
  journal      = {Applied Soft Computing},
  pages        = {106277},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Local gradient full-scale transform patterns based off-line text-independent writer identification},
  volume       = {92},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Transfer stacking from low-to high-fidelity: A
surrogate-assisted bi-fidelity evolutionary algorithm. <em>ASOC</em>,
<em>92</em>, 106276. (<a
href="https://doi.org/10.1016/j.asoc.2020.106276">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Optimization of many real-world optimization problems relies on numerical simulations for function evaluations. In some cases, both high- and low-fidelity simulations are available, where the high fidelity evaluation is accurate but time-consuming, whereas the low-fidelity evaluation is less accurate but computationally cheap. To find an acceptable optimum within a limited budget, it is economical for evolutionary algorithms to use both high- and low-fidelity evaluations in a single optimization search. This paper proposes a novel surrogate-assisted evolutionary algorithm using the transfer stacking technique for bi-fidelity optimization. To this end, a radial basis function network is firstly built to approximate the high-fidelity fitness function as additional low-fidelity evaluation, then a surrogate model transferring the original and additional low-fidelity evaluations to the expensive high-fidelity evaluation is adapted to guide the search. The simulation results on a series of bi-fidelity optimization benchmark problems with resolution, stochastic, and instability errors and a beneficiation processes optimization problem show that the proposed algorithm is both effective and efficient for solving bi-fidelity optimization problems , when their low-fidelity evaluations have resolution and stochastic errors.},
  archive      = {J_ASOC},
  author       = {Handing Wang and Yaochu Jin and Cuie Yang and Licheng Jiao},
  doi          = {10.1016/j.asoc.2020.106276},
  journal      = {Applied Soft Computing},
  pages        = {106276},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Transfer stacking from low-to high-fidelity: A surrogate-assisted bi-fidelity evolutionary algorithm},
  volume       = {92},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Fuzzy neural networks and neuro-fuzzy networks: A review the
main techniques and applications used in the literature. <em>ASOC</em>,
<em>92</em>, 106275. (<a
href="https://doi.org/10.1016/j.asoc.2020.106275">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a review of the central theories involved in hybrid models based on fuzzy systems and artificial neural networks , mainly focused on supervised methods for training hybrid models. The basic concepts regarding the history of hybrid models, from the first proposed model to the current advances, the composition and the functionalities in their architecture, the data treatment and the training methods of these intelligent models are presented to the reader so that the evolution of this category of intelligent systems can be evidenced. Finally, the features of the leading models and their applications are presented to the reader. We conclude that the fuzzy neural network models and their derivations are efficient in constructing a system with a high degree of accuracy and an appropriate level of interpretability working in a wide range of areas of economics and science.},
  archive      = {J_ASOC},
  author       = {Paulo Vitor de Campos Souza},
  doi          = {10.1016/j.asoc.2020.106275},
  journal      = {Applied Soft Computing},
  pages        = {106275},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Fuzzy neural networks and neuro-fuzzy networks: A review the main techniques and applications used in the literature},
  volume       = {92},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Bayesian optimization algorithm for multi-objective
scheduling of time and precedence constrained tasks in heterogeneous
multiprocessor systems. <em>ASOC</em>, <em>92</em>, 106274. (<a
href="https://doi.org/10.1016/j.asoc.2020.106274">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a Bayesian optimization based novel approach for multi-objective scheduling of time and precedence constrained tasks in heterogeneous multiprocessing environments. The proposed approach, termed as multi-objective Bayesian optimization algorithm (moBOA) for real-time scheduling, can suitably produce optimal task schedules without any violation of the timing and precedence constraints. The moBOA utilizes Bayesian networks to learn the task graphs that represent the precedence relationships among the tasks. It first allocates tasks to individual processors and then decides the order of execution on each processor based on the latest deadline first policy. The proposed moBOA may be applied to both homogeneous and heterogeneous multiprocessor systems . Extensive comparative analysis has been made by considering two other existing evolutionary algorithms , namely the multi-objective genetic algorithm (moGA) and multi-objective hybrid genetic algorithm (mohGA) through experimental simulations with benchmark datasets. From the results of the simulation experiments, it is observed that moBOA outperforms both moGA and mohGA in terms of quality of solutions, Pareto-optimalilty and standard performance measures . We demonstrate that the task schedules produced by moBOA are optimal and comply with the timing as well as the precedence constraints. Statistical significant analyses of the results are conducted.},
  archive      = {J_ASOC},
  author       = {Pranab K. Muhuri and Sajib K. Biswas},
  doi          = {10.1016/j.asoc.2020.106274},
  journal      = {Applied Soft Computing},
  pages        = {106274},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Bayesian optimization algorithm for multi-objective scheduling of time and precedence constrained tasks in heterogeneous multiprocessor systems},
  volume       = {92},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A novel harmony search algorithm and its application to data
clustering. <em>ASOC</em>, <em>92</em>, 106273. (<a
href="https://doi.org/10.1016/j.asoc.2020.106273">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a variant of harmony search algorithm (HS), called best–worst-mean harmony search (BWM_HS). The main difference between the proposed algorithm and the canonical HS is that it employs a modified memory consideration procedure to utilize more efficiently the accumulated knowledge and experience in harmony memory (HM). To this aim, the random harmony selection scheme of this procedure is replaced with three novel pitch selection and production rules. These rules use the information of the current best and worst harmonies as well as the mean of all harmonies to guide the search process. To further utilize the valuable information of HM, two new harmonies are generated at each iteration where the better one will compete with the current worst harmony. The mean of all harmonies is always employed to produce a new harmony. On the other hand, each pitch of the second one is obtained by the rules that consider the information of the best and worst harmonies. These rules can present either explorative or exploitative search behaviors at different stages of search. Thus, a probabilistic self-adaptive selection scheme decides to choose between them to properly balance the exploration and exploitation abilities. The general performance of BWM_HS for solving optimization problems is evaluated against CEC 2017 benchmark functions and its results are compared with HS and eight state-of-the-art variants of HS. The comparison indicates that the performance of BWM_HS is better than or equal to the compared algorithms with respect to the accuracy, robustness, and convergence speed criteria. Moreover, the performance of BWM_HS in solving clustering problems is investigated by applying it for clustering several well-known benchmark datasets. The experimental results show that, in general, the BWM_HS outperforms other well-known algorithms in the literature and in particular, it significantly improves the statistical results for one dataset.},
  archive      = {J_ASOC},
  author       = {Kazem Talaei and Amin Rahati and Lhassane Idoumghar},
  doi          = {10.1016/j.asoc.2020.106273},
  journal      = {Applied Soft Computing},
  pages        = {106273},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A novel harmony search algorithm and its application to data clustering},
  volume       = {92},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Semi-supervised image depth prediction with deep learning
and binocular algorithms. <em>ASOC</em>, <em>92</em>, 106272. (<a
href="https://doi.org/10.1016/j.asoc.2020.106272">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Combining the advantages and disadvantages of supervised learning and unsupervised learning strategies in convolution neural networks, this paper proposes a semi-supervised single-image depth prediction model based on binocular information and sparse laser data. The model improves the depth prediction accuracy by introducing sparse depth monitoring information, which provides a better convergence of the model with a local optimal solution . In the experiment, we validate the effectiveness of the model on the KITTI data set. Compared to the supervised algorithm, the root mean square error is reduced by 41.6\% and, compared to the unsupervised algorithm, the root mean square error is reduced by 26.9\%.},
  archive      = {J_ASOC},
  author       = {Kuo-Kun Tseng and Yaqi Zhang and Qinglin Zhu and K.L. Yung and W.H. Ip},
  doi          = {10.1016/j.asoc.2020.106272},
  journal      = {Applied Soft Computing},
  pages        = {106272},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Semi-supervised image depth prediction with deep learning and binocular algorithms},
  volume       = {92},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A robust dynamic scheduling approach based on release time
series forecasting for the steelmaking-continuous casting production.
<em>ASOC</em>, <em>92</em>, 106271. (<a
href="https://doi.org/10.1016/j.asoc.2020.106271">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this work, the dynamic scheduling problem is investigated considering the uncertainty of the job release time in steelmaking-continuous casting production processes. In contrast to existing dynamic scheduling strategies, a novel robust dynamic scheduling approach based on release time series forecasting (RDSA_RTSF) is proposed. The proposed RDSA_RTSF consists of two stages, i.e., offline preparation and online robust dynamic scheduling. In the offline preparation stage, a release time series forecasting model is established using historical data, and the forecasting accuracy of the model is calculated. In the online robust dynamic scheduling stage, a chance constrained programming (CCP) model is built first for rescheduling based on the predicted release time series and the statistical information of the forecasting accuracy . A robust schedule is subsequently generated by using a Monte Carlo simulation and an evolutionary algorithm to solve the CCP model. The evolutionary algorithm is formulated by combining a genetic algorithm with a local search strategy based on the problem characteristics. Computational experiments based on real data from a steel plant in China show that the proposed RDSA_RTSF strategy performs better than classical approaches in obtaining robust schedule results under a dynamic environment with uncertain release times.},
  archive      = {J_ASOC},
  author       = {Jianyu Long and Zhenzhong Sun and Panos M. Pardalos and Yun Bai and Shaohui Zhang and Chuan Li},
  doi          = {10.1016/j.asoc.2020.106271},
  journal      = {Applied Soft Computing},
  pages        = {106271},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A robust dynamic scheduling approach based on release time series forecasting for the steelmaking-continuous casting production},
  volume       = {92},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). An optimal service selection approach for service-oriented
business collaboration using crowd-based cooperative computing.
<em>ASOC</em>, <em>92</em>, 106270. (<a
href="https://doi.org/10.1016/j.asoc.2020.106270">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Crowd-based cooperative computing (CBCC) emerges as a new computing paradigm , the core issue of which is the effective management and the coordinated use of crowd resources, including Internet users, application services, and smart devices. The service-oriented architecture (SOA) provides interoperability among crowd resources to support service-oriented business collaboration (SOBC). To address such a common issue of the coordinated use of crowd resources for SOBC, this paper studies a collaborative service computing model by considering the competition and cooperation among crowd resources. Then, a multi-objective optimization mathematical model is established for optimal service selection (OSS). Specifically, the methodology is resorted to an improved particle swarm optimization (IPSO) algorithm to find suitable collaborative services that optimally balance the quality of service (QoS) and synergy effect (SE). Furthermore, a flexible rescheduling strategy is presented for faulty services. The experimental results show that the proposed methodology is effective and feasible to obtain better-quality solutions for fulfilling the SOBC.},
  archive      = {J_ASOC},
  author       = {Lu Zhao and Wenan Tan and Na Xie and Li Huang},
  doi          = {10.1016/j.asoc.2020.106270},
  journal      = {Applied Soft Computing},
  pages        = {106270},
  shortjournal = {Appl. Soft. Comput.},
  title        = {An optimal service selection approach for service-oriented business collaboration using crowd-based cooperative computing},
  volume       = {92},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Selecting a diverse set of benchmark instances from a
tunable model problem for black-box discrete optimization algorithms.
<em>ASOC</em>, <em>92</em>, 106269. (<a
href="https://doi.org/10.1016/j.asoc.2020.106269">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As the number of practical applications of discrete black-box metaheuristics is growing faster and faster, the benchmarking of these algorithms is rapidly gaining importance. While new algorithms are often introduced for specific problem domains, researchers are also interested in which general problem characteristics are hard for which type of algorithm. The W-Model is a benchmark function for discrete black-box optimization, which allows for the easy, fast, and reproducible generation of problem instances exhibiting characteristics such as ruggedness, deceptiveness, epistasis, and neutrality in a tunable way. We conduct the first large-scale study with the W-Model in its fixed-length single-objective form, investigating 17 algorithm configurations (including Evolutionary Algorithms and local searches) and 8372 problem instances. We develop and apply a machine learning methodology to automatically discover several clusters of optimization process runtime behaviors as well as their reasons grounded in the algorithm and model parameters. Both a detailed statistical evaluation and our methodology confirm that the different model parameters allow us to generate problem instances of different hardness, but also find that the investigated algorithms struggle with different problem characteristics. With our methodology, we select a set of 19 diverse problem instances with which researchers can conduct a fast but still in-depth analysis of algorithm performance. The best-performing algorithms in our experiment were Evolutionary Algorithms applying Frequency Fitness Assignment, which turned out to be robust over a wide range of problem settings and solved more instances than the other tested algorithms.},
  archive      = {J_ASOC},
  author       = {Thomas Weise and Yan Chen and Xinlu Li and Zhize Wu},
  doi          = {10.1016/j.asoc.2020.106269},
  journal      = {Applied Soft Computing},
  pages        = {106269},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Selecting a diverse set of benchmark instances from a tunable model problem for black-box discrete optimization algorithms},
  volume       = {92},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Improved metaheuristics for the two-dimensional strip
packing problem. <em>ASOC</em>, <em>92</em>, 106268. (<a
href="https://doi.org/10.1016/j.asoc.2020.106268">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Given a fixed set of rectangular items and a single rectangular object of fixed width and unlimited height, the two-dimensional strip packing problem consists of packing all the items into the object in a non-overlapping manner, such that the resulting packing height is a minimum. Two improved strip packing metaheuristics are proposed in this paper. The first algorithm is a hybrid approach in which the method of simulated annealing is combined with a heuristic construction algorithm , while the second algorithm involves application of the method of simulated annealing directly in the space of completely defined packing layouts, without an encoding of solutions. These two algorithms are compared with a representative sample of metaheuristics from the literature in terms of solution quality achieved in the context of a large set of 1 718 benchmark instances, clustered into four sets of test problems, each with differing characteristics. It is found that the new algorithms indeed compare favourably with, and in some cases outperform, existing strip packing metaheuristics in the literature.},
  archive      = {J_ASOC},
  author       = {Rosephine G. Rakotonirainy and Jan H. van Vuuren},
  doi          = {10.1016/j.asoc.2020.106268},
  journal      = {Applied Soft Computing},
  pages        = {106268},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Improved metaheuristics for the two-dimensional strip packing problem},
  volume       = {92},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Integrated fabric procurement and multi-site apparel
production planning with cross-docking: A hybrid fuzzy-robust stochastic
programming approach. <em>ASOC</em>, <em>92</em>, 106267. (<a
href="https://doi.org/10.1016/j.asoc.2020.106267">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we study the integrated inbound logistics decisions and multi-site aggregate production planning (APP) over the tactical planning horizon in a textile industry. In this regard, supplier selection, order allocation, inbound transportation logistics, and multi-site APP problems are addressed in a multi-period, multi-product and multiple transportation modes under uncertainty. To make procurement and production decisions simultaneously, a mixed-integer nonlinear mathematical programming model is proposed in a hybrid fuzzy-stochastic environment. To solve the proposed model, an efficient multi-stage algorithm is developed by re-formulating with a linearization scheme and employing a novel defuzzification process to cope with the proposed possibilistic programming. Then, a robust two-stage stochastic programming method is applied. Finally, the application of the model and the effectiveness of the solution method are examined through comprehensive numerical studies for the apparel industry.},
  archive      = {J_ASOC},
  author       = {Fateme Darvishi and R. Ghasemy Yaghin and Abdolhossein Sadeghi},
  doi          = {10.1016/j.asoc.2020.106267},
  journal      = {Applied Soft Computing},
  pages        = {106267},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Integrated fabric procurement and multi-site apparel production planning with cross-docking: A hybrid fuzzy-robust stochastic programming approach},
  volume       = {92},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Modeling and analysis of factors affecting repair
effectiveness of repairable systems using bayesian network.
<em>ASOC</em>, <em>92</em>, 106261. (<a
href="https://doi.org/10.1016/j.asoc.2020.106261">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Imperfect maintenance modeling and analysis of repairable systems involves an additional parameter called repair effectiveness index (REI) ‘q’, along with shape and scale parameters in generalized renewal process (GRP), which is a measure of the quality of repair or repair effectiveness (RE). The quantitative measure of this parameter as proposed by Kijima through his virtual age models attracted enough attention and is extensively used by the researchers. But, RE could be dependent on many subjective factors and also requires qualitative analysis as well for better understanding of repair effects and performance. Quantitative assessment of RE is necessary but not sufficient to analyze it completely. The paper brings out the limitations of the quantitative assessment of RE and highlights the need for further examining it qualitatively. This paper conducts an extensive study with the help of field experts on selection and analysis of various factors affecting RE and proposes eleven primary factors and their sub-factors (total 55 factors/sub-factors) which affect it the most. After due selection of the factors and sub-factors, the paper then proposes a Bayesian network (BN) to model their dependency on each other and with RE. As a result, the proposed BN model provides the measurable effect of all the selected factors and sub factors on RE in percentage form. The results are demonstrated with the help of two examples inspired by practical industrial applications. The presented work could be extremely useful for industries in undertaking reliability improvement of repairable systems by analyzing their repair quality in detail. The proposed methodology can also be used as fundamental guidelines to develop basic understanding of the repair effectiveness and how it can be improved for a particular system leading to an overall improvement in the reliability of the system.},
  archive      = {J_ASOC},
  author       = {Garima Sharma and Rajiv Nandan Rai},
  doi          = {10.1016/j.asoc.2020.106261},
  journal      = {Applied Soft Computing},
  pages        = {106261},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Modeling and analysis of factors affecting repair effectiveness of repairable systems using bayesian network},
  volume       = {92},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Discrete artificial electric field algorithm for high-order
graph matching. <em>ASOC</em>, <em>92</em>, 106260. (<a
href="https://doi.org/10.1016/j.asoc.2020.106260">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {High-order graph matching is a problem of establishing the correspondences between two sets of visual features subject to high-order matching constraints. This is an NP-hard combinatorial optimization problem and formulated as a maximization problem of matching score over all permutations of features. Artificial electric field algorithm (AEFA) (Yadav et al., 2019) is a proven optimization algorithm in the family of meta-heuristic and performed well for continuous optimization problems . In this article, we extended the AEFA algorithm for combinatorial high-order graph matching problems and introduced a discrete artificial electric field algorithm (DAEFA). This framework incorporates the redefine position and velocity representation scheme, addition–subtraction operation, velocity and position update rules, and a problem specific initialization by using heuristic information . The efficiency of the proposed algorithm is tested over three well-known datasets: synthetic, CMU house and real-world datasets. The computational results measured the matching score, accuracy of matching and established the correspondences between two graphs. The computational results show the outperformance of the proposed algorithm over the other state-of-art algorithms in terms of good matching score and accuracy both.},
  archive      = {J_ASOC},
  author       = {Anita and Anupam Yadav},
  doi          = {10.1016/j.asoc.2020.106260},
  journal      = {Applied Soft Computing},
  pages        = {106260},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Discrete artificial electric field algorithm for high-order graph matching},
  volume       = {92},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Prediction of bank telephone marketing results based on
improved whale algorithms optimizing s_kohonen network. <em>ASOC</em>,
<em>92</em>, 106259. (<a
href="https://doi.org/10.1016/j.asoc.2020.106259">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Time deposit has the characteristics of strong stability and low cost. It is a stable source of funds for banks. In this paper, S_Kohonen network is used to predict the success rate of fixed deposit in bank telephone marketing. Firstly, the output layer is added after the competition layer of unsupervised Kohonen network, which makes Kohonen network become S_Kohonen network with supervised learning. Because the improved S_Kohonen network is similar to other feedforward neural networks , each adjacent layer is connected by weights, and the initial weights are random, which easily leads to the unstable output of the network, and still has the disadvantage of relatively low prediction accuracy. Therefore, an improved whale optimization algorithm (IWOA) is proposed to optimize the weights between the input layer and the competition layer of S_Kohonen network. In this paper, the inertia weight of whale optimization algorithm is introduced into random factor on the basis of non-linear decline, and then the random search pattern of Levy flight is introduced into whale algorithm. Finally, the empirical results show that the improved S_Kohonen network can more intuitively represent the classification results of the network, and the classification accuracy of S_Kohonen network optimized by IWOA is significantly higher than that of S_Kohonen network optimized by GA , WOA and LWOA algorithm.},
  archive      = {J_ASOC},
  author       = {Chun Yan and Meixuan Li and Wei Liu},
  doi          = {10.1016/j.asoc.2020.106259},
  journal      = {Applied Soft Computing},
  pages        = {106259},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Prediction of bank telephone marketing results based on improved whale algorithms optimizing S_Kohonen network},
  volume       = {92},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Noise gradient strategy for an enhanced hybrid
convolutional-recurrent deep network to control a self-driving vehicle.
<em>ASOC</em>, <em>92</em>, 106258. (<a
href="https://doi.org/10.1016/j.asoc.2020.106258">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper a noise gradient strategy on the Adam optimizer is introduced, in order to reduce the training time of our enhanced Chauffeur hybrid deep model. This neural network was modified to take into account the time dependence of the input visual information from a time-distributed convolution, with the aim of increasing the autonomy of a self-driving vehicle. The effectiveness of the proposed optimizer and model was evaluated and quantified during training and validation with a higher performance than the original Chauffeur model in combination with the comparative optimizers. In terms of the autonomy, it can be seen that our enhanced Hybrid Convolutional-Recurrent Deep Network was better trained, achieving autonomy greater than 95\% with a minimum number of human interventions.},
  archive      = {J_ASOC},
  author       = {Dante Mújica-Vargas and Antonio Luna-Álvarez and José de Jesús Rubio and Blanca Carvajal-Gámez},
  doi          = {10.1016/j.asoc.2020.106258},
  journal      = {Applied Soft Computing},
  pages        = {106258},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Noise gradient strategy for an enhanced hybrid convolutional-recurrent deep network to control a self-driving vehicle},
  volume       = {92},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A novel image steganographic method based on integer wavelet
transformation and particle swarm optimization. <em>ASOC</em>,
<em>92</em>, 106257. (<a
href="https://doi.org/10.1016/j.asoc.2020.106257">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image steganography is a technique of hiding secret data into a cover image and so as to prevent the intruders from accessing the secret data . The efficiency of image steganography techniques are usually evaluated based on perceptual transparency, payload capacity , security, temper resistance and computational costs. Though there has been significant improvement in related research over the decades, available steganographic techniques usually satisfy only a subset of these criteria. This paper presents a novel IWT (Integer Wavelet Transform) based steganography method using PSO (Particle Swarm Optimization) to find the optimal substitution matrix for converting secret data into their substituted forms. In the proposed method, optimal pixel adjustment process is used to improve perceptual transparency so that the obtained stego image has low distortion. The proposed method improves the security, imperceptibility , and robustness of the secret data by hiding them into the wavelet coefficients of an image. Thus, the paper provides a detailed study of the use of PSO in three different image steganographic methods based on (i) LSB (Least Significant Bit) substitution, (ii) DWT (Discrete Wavelet Transform), and (iii) IWT. Experiments are conducted using well-known benchmark images and results are comparatively analyzed. It is found that our proposed approach of PSO based IWT outperforms both PSO based LSB and PSO based DWT from the context of standard quality metrics, statistical analysis, security level estimation, payload capacity , imperceptibility etc. We have also provided a comparative overview of the existing data hiding methods including the proposed approach.},
  archive      = {J_ASOC},
  author       = {Pranab K. Muhuri and Zubair Ashraf and Swati Goel},
  doi          = {10.1016/j.asoc.2020.106257},
  journal      = {Applied Soft Computing},
  pages        = {106257},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A novel image steganographic method based on integer wavelet transformation and particle swarm optimization},
  volume       = {92},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Optimizing the DNA fragment assembly using
metaheuristic-based overlap layout consensus approach. <em>ASOC</em>,
<em>92</em>, 106256. (<a
href="https://doi.org/10.1016/j.asoc.2020.106256">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nucleotide sequencing finds the exact order of nucleotides present in a DNA molecule. The correct DNA sequence is required to obtain the desired information about the complete genetic makeup of an organism. The DNA fragment assembly correctly combines the DNA information present in the form of fragments as a sequence. Reconstruction of the original DNA sequence from large fragments is a challenging task due to the limitations of the available technologies that reads the DNA sequence. Objective of the DNA fragment assembly is to find the correct order of the fragments which is further used in the generation of a consensus sequence that represents the original DNA sequence. Power Aware Local Search (PALS) algorithm proposed for the DNA fragment assembly is an efficient method that orders the fragments in a correct sequence by minimizing the number of contigs. This work presents a hybrid approach on the basis of Overlap Layout Consensus for the DNA fragment assembly, where Restarting and Recentering Genetic Algorithm (RRGA) with integrated PALS is utilized as an evolutionary operator. Quality of the current proposal is quantified using overlap scores and the number of contigs. This work is evaluated using 25 benchmark datasets with three types of experiments. The results are compared with four state-of-the-art methods for the same task, namely, Recentering–Restarting Genetic Algorithm variation for DNA fragment assembly, PALS, Genetic Algorithm, and Hybrid Genetic Algorithm. Results show better average performance of the proposed solution.},
  archive      = {J_ASOC},
  author       = {Uzma and Zahid Halim},
  doi          = {10.1016/j.asoc.2020.106256},
  journal      = {Applied Soft Computing},
  pages        = {106256},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Optimizing the DNA fragment assembly using metaheuristic-based overlap layout consensus approach},
  volume       = {92},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Preference-driven pareto front exploitation for bloat
control in genetic programming. <em>ASOC</em>, <em>92</em>, 106254. (<a
href="https://doi.org/10.1016/j.asoc.2020.106254">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As one of evolutionary algorithms (EAs), genetic programming (GP) has been applied in a wide range of areas, e.g. bioinformatics and robotics. Different from other EAs, GP can represent problems with variable length (e.g. trees), which makes it more flexible in evolving solutions, yet leads to a serious problem, bloat. It can cause evolving redundant parts and slowing down search. Multi-objective techniques are popularly used for reducing bloat in GP (termed as MOGP). Specifically, MOGP methods evolve trade-off solutions of all objectives, which constitute the so-called Pareto front . Then users select solutions on the front based on their preference for specific tasks. However, existing MOGP methods rarely consider users’ preference during evolution, which wastes computation power and time to search for useless solutions and cannot generate fine-grained interested regions on the Pareto front . Therefore, this paper investigates introducing users’ preference to guide multi-objective techniques to focus on the interested regions on Pareto front during evolution. Specifically, Pareto dominance is an important notion in multi-objective techniques for comparing two solutions. We design two preference-driven Pareto dominance mechanisms, scPd (static constraint Pareto dominance) and dcPd (dynamic constraint Pareto dominance), which are introduced in a base multi-objective technique and then are incorporated with GP respectively to form two new bloat control MOGP methods, i.e. scPd_MOGP and dcPd_MOGP. They are tested on benchmark symbolic regression tasks comparing with GP, two existing bloat control methods (i.e. a parsimony GP method (pGP) and a standard multi-objective GP method (sMOGP)), and four popularly-used symbolic regression methods . Results show that the proposed methods can reduce bloat in GP and outperform pGP in bloat control, and comparison with sMOGP shows that they can search front regions based on users’ preference where the solutions have better functionality, yet relatively larger sizes. In addition, compared with four popularly-used symbolic regression methods , scPd_MOGP is generally better; while dcPd_MOGP achieves varied results, yet it performs better or similar to the reference methods on the majority of the given test functions. Moreover, comparison between the two proposed methods suggests that the constraint in the Pareto dominance of scPd_MOGP is more relaxed than that of dcPd_MOGP.},
  archive      = {J_ASOC},
  author       = {Jiayu Liang and Yuxin Liu and Yu Xue},
  doi          = {10.1016/j.asoc.2020.106254},
  journal      = {Applied Soft Computing},
  pages        = {106254},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Preference-driven pareto front exploitation for bloat control in genetic programming},
  volume       = {92},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Research on feature selection for rotating machinery based
on supervision kernel entropy component analysis with whale optimization
algorithm. <em>ASOC</em>, <em>92</em>, 106245. (<a
href="https://doi.org/10.1016/j.asoc.2020.106245">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Aimed at finding a scientific and effective method to diagnose faults in rotating machinery , the algorithm based on Supervision Kernel Entropy Component Analysis with Whale Optimization Algorithm (WOSKECA) for feature selection has been proposed in this paper. Firstly, for ensure sufficient information gathering from the rotating machinery , multi-features parameters of the time domain, frequency domain, time-frequency domain and entropy domain are extracted, and a high-dimensional feature matrix is constructed from these features. Secondly, the WOSKECA for feature selection is applied to eliminate any redundant information. The algorithm takes the class information as the supervised information to improve the recognition accuracy of Kernel Entropy Component Analysis (KECA) and can extract low-dimensional features with discriminative ability from the high-dimensional feature space. Meanwhile, the Whale Optimization Algorithm (WOA) as a new meta-heuristic optimization algorithm is applied to optimize the kernel parameters in KECA, which reduces the interference of subjective factors and reduces the professionalism of obtaining fault information. Finally, Support Vector Machine based on the Particle Swarm Optimization (PSOSVM) is used to classify the fault type as well as assess the severity of the faults. The feature extraction algorithm is entirely evaluated through experimentation and comparative. The results show that the proposed method is able to detect and classify the faults of rotating machinery more successfully and more accurately than traditional manifold learning.},
  archive      = {J_ASOC},
  author       = {Lili Bai and Zhennan Han and Jiajun Ren and Xiaofeng Qin},
  doi          = {10.1016/j.asoc.2020.106245},
  journal      = {Applied Soft Computing},
  pages        = {106245},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Research on feature selection for rotating machinery based on supervision kernel entropy component analysis with whale optimization algorithm},
  volume       = {92},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Pareto optimality and game theory approach for optimal
deployment of DG in radial distribution system to improve
techno-economic benefits. <em>ASOC</em>, <em>92</em>, 106234. (<a
href="https://doi.org/10.1016/j.asoc.2020.106234">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper discusses the application of nature-inspired swarm intelligence methods for optimal allocation and sizing of distributed generation (DG) in the radial distribution system (RDS). Introducing DG units in the RDS will enhance the technical and economic benefits if they are optimally deployed. The objective functions considered are to improve the technical aspects and net economical saving cost with DG units integration on RDS. In this paper, a weighted multi-objective index considers a wide range of technical issues such as active and reactive power losses of the system, voltage profile, line loading, and the voltage stability, these are assumed as technical improvement aspects in the RDS. A recent optimization method, i.e. improved raven roosting optimization (IRRO) algorithm has been implemented for optimal deployment of DG in RDS. The state of the art of IRRO algorithm parameters will improve the ability for exploration and prevent premature convergence. Pareto optimality is used in making a set of the best solutions between two conflicting objectives considered, i.e. technical and economical aspects. The main contribution in this paper is to utilize a game theory based (minimax) algorithm in taking the best decision from a set of non-dominated solutions obtained by Pareto optimality criteria. IEEE 33-bus and 69-bus RDS’s are considered as the test systems for verifying the effectiveness of the IRRO algorithm. A comparative analysis with other nature-inspired swarm optimization techniques such as particle swarm optimization (PSO), modified teaching learning based optimization (MTLBO), Jaya algorithm (JAYA), and grey wolf optimizer (GWO) is also presented in this work. The simulation results of IRRO are compared with similar existing papers. It is observed that the IRRO algorithm can produce better results for the considered multi-objective functions. The MATLAB software is employed for the purpose. The novelty of the paper lies in the use of Pareto optimal and game theory in obtaining better results to the problem of optimal deployment of DG in RDS to improve technical as well as economic benefits.},
  archive      = {J_ASOC},
  author       = {Srinivas Nagaballi and Vijay S. Kale},
  doi          = {10.1016/j.asoc.2020.106234},
  journal      = {Applied Soft Computing},
  pages        = {106234},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Pareto optimality and game theory approach for optimal deployment of DG in radial distribution system to improve techno-economic benefits},
  volume       = {92},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). An improved scheme for digital mammogram classification
using weighted chaotic salp swarm algorithm-based kernel extreme
learning machine. <em>ASOC</em>, <em>91</em>, 106266. (<a
href="https://doi.org/10.1016/j.asoc.2020.106266">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Over the past years, the surge in the necessity for early detection and diagnosis of breast cancer has resulted in many innovative research directions. According to the World Health Organization, an early and accurate detection of breast cancer successfully leads to a correct decision towards its treatment. Development of computer-aided diagnosis (CAD) system is considered to be a major stead in current research practice to abet medical practitioners in decision-making. This paper proposes an improved CAD framework to correctly classify the digital mammograms into normal or abnormal, and further, benign or malignant. The proposed scheme employs a block-based discrete wavelet packet transform (BDWPT) to extract the features, namely, the Shannon entropy , Tsallis entropy, Renyi entropy, and energy. Then, principal component analysis (PCA) technique is utilized to extract the discriminating features from the original feature vector. Subsequently, an optimized wrapper-based kernel extreme learning machine (KELM) using a weighted chaotic salp swarm algorithm (WC-SSA) is proposed as classifier to classify the digital mammograms. Since the efficacy of KELM algorithm depends on its two important parameters, namely, the penalty parameter and the kernel parameter , the prime objective of the proposed work is to obtain the optimized value of the aforementioned parameters as well as to select the most relevant features from the reduced feature vector simultaneously. The proposed scheme is evaluated on three publicly available standard datasets, namely, MIAS, DDSM, and BCDR to validate the efficacy of the proposed BDWPT+PCA+WC-SSA-KELM scheme. The performance of the proposed model is evaluated in terms of different metrics, namely, classification accuracy , sensitivity, specificity, area under curve (AUC), Matthew’s correlation coefficient (MCC), and F-measure via a 5 × 5 stratified cross-validation approach. From the experimental results and their analysis, it is observed that for the normal–abnormal category, the proposed technique results in an accuracy of 99.62\% and 99.92\% for MIAS and DDSM, respectively, whereas in the case of benign–malignant classification, the proposed method yields an accuracy of 99.28\%, 99.63\%, 99.60\% for MIAS, DDSM, and BCDR datasets, respectively. Further, it is also observed that the proposed WC-SSA-KELM scheme exhibits superior performance as compared to that of its counterparts. Additionally, two well-known statistical analyses, namely, ANOVA and Friedman tests are performed to demonstrate that the performance of the proposed scheme is significantly better than that of the other existing schemes.},
  archive      = {J_ASOC},
  author       = {Figlu Mohanty and Suvendu Rup and Bodhisattva Dash and Banshidhar Majhi and M.N.S. Swamy},
  doi          = {10.1016/j.asoc.2020.106266},
  journal      = {Applied Soft Computing},
  pages        = {106266},
  shortjournal = {Appl. Soft. Comput.},
  title        = {An improved scheme for digital mammogram classification using weighted chaotic salp swarm algorithm-based kernel extreme learning machine},
  volume       = {91},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Adaptive fuzzy-wavelet neural network identification core
for reinforced control of general arbitrarily switched nonlinear multi
input-multi output dynamic systems. <em>ASOC</em>, <em>91</em>, 106265.
(<a href="https://doi.org/10.1016/j.asoc.2020.106265">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In control of switched systems with undetectable switching signals, robustness and precision are on two different sides of a spectrum. In one hand, robustness to significant modeling uncertainties arising from undetectability of switching modes can be attained by designing the control scheme for worst-case switching configurations. On the other hand, such a control scheme would potentially be overly conservative and imprecise. A natural solution to this problem is pinpointing active switched dynamics at any given moment. However, this is no trivial task. In this study, we propose that the aforementioned problems can be overcome by designing a control scheme that prioritizes appropriate objectives according to operating conditions. In other words, the control scheme adjusts itself such that either of robustness to unknowable switching or increased tracking precision is selected as the primary control objective. As a result, the control model can be considered as dual-mode featuring a safe control mode and a precise control mode . In precise control mode, a model generation scheme using a modified Fuzzy-Wavelet Neural Network (FWNN) for Multi Input-Multi Output (MIMO) systems is incorporated for precise estimation of active dynamics which potentially features unknowable switching dynamics, external disturbances and parametric modeling uncertainty. However, this approximate model cannot be used immediately since convergence of the FWNN-based model to actual system dynamics takes place after a limited interval. In such periods (which often correspond to discontinuities in switching dynamics and references), using the FWNN scheme is perilous. Therefore, a robust discrete-time sliding mode control (DSMC) is used to ensure stabilization of closed-loop system in all potential modes of switched dynamics at the cost of reduced tracking precision. In combination, the dual-mode scheme ensures robust stabilization in safe control modes corresponding to transient-state stage and accurate tracking in steady-state stage of system response based on the proposed precise mode scheme. Numerical and experimental examples highlight the key features and improvements of the presented control algorithm.},
  archive      = {J_ASOC},
  author       = {M.R. Homaeinezhad and S. Yaqubi},
  doi          = {10.1016/j.asoc.2020.106265},
  journal      = {Applied Soft Computing},
  pages        = {106265},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Adaptive fuzzy-wavelet neural network identification core for reinforced control of general arbitrarily switched nonlinear multi input-multi output dynamic systems},
  volume       = {91},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A development on multimodal optimization technique and its
application in structural damage detection. <em>ASOC</em>, <em>91</em>,
106264. (<a href="https://doi.org/10.1016/j.asoc.2020.106264">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, a heuristic algorithm fusing with niche identification (NIT) and Artificial Bee Colony (ABC) technique is developed to solve multimodal optimization problems , and is then applied for structural damage detection. In order to improve the detection accuracy of the proposed algorithm, the Depth First Search (DFS) is adopted, and a new particle update scheme is proposed to maintain the diversity of particle populations. The effectiveness and robustness of the algorithm for multimodal optimization are demonstrated by the well-known benchmark functions . Case studies on structural damage detection are carried out using ANSYS-powered data. Simulation results show that, even for the contaminated data or extreme damage scenarios (e.g., the adjacent damages), the DFS-based nNIT with ABC technique can lead to a satisfactory result.},
  archive      = {J_ASOC},
  author       = {Diancheng Chen and Yiyang Li},
  doi          = {10.1016/j.asoc.2020.106264},
  journal      = {Applied Soft Computing},
  pages        = {106264},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A development on multimodal optimization technique and its application in structural damage detection},
  volume       = {91},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Statistical and machine learning models in credit scoring: A
systematic literature survey. <em>ASOC</em>, <em>91</em>, 106263. (<a
href="https://doi.org/10.1016/j.asoc.2020.106263">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In practice, as a well-known statistical method, the logistic regression model is used to evaluate the credit-worthiness of borrowers due to its simplicity and transparency in predictions. However, in literature, sophisticated machine learning models can be found that can replace the logistic regression model . Despite the advances and applications of machine learning models in credit scoring, there are still two major issues: the incapability of some of the machine learning models to explain predictions; and the issue of imbalanced datasets. As such, there is a need for a thorough survey of recent literature in credit scoring. This article employs a systematic literature survey approach to systematically review statistical and machine learning models in credit scoring, to identify limitations in literature, to propose a guiding machine learning framework, and to point to emerging directions. This literature survey is based on 74 primary studies, such as journal and conference articles, that were published between 2010 and 2018. According to the meta-analysis of this literature survey, we found that in general, an ensemble of classifiers performs better than single classifiers. Although deep learning models have not been applied extensively in credit scoring literature, they show promising results.},
  archive      = {J_ASOC},
  author       = {Xolani Dastile and Turgay Celik and Moshe Potsane},
  doi          = {10.1016/j.asoc.2020.106263},
  journal      = {Applied Soft Computing},
  pages        = {106263},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Statistical and machine learning models in credit scoring: A systematic literature survey},
  volume       = {91},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Imbalanced credit risk evaluation based on multiple
sampling, multiple kernel fuzzy self-organizing map and local accuracy
ensemble. <em>ASOC</em>, <em>91</em>, 106262. (<a
href="https://doi.org/10.1016/j.asoc.2020.106262">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Credit risk evaluation model is generally regarded as a valid method for business risk management . Although the most of literatures about credit risk evaluation always use class-balanced data as sample sets, the study on class-imbalanced datasets is more suitable for actual situation. This paper proposes a new ensemble model to evaluate class-imbalanced credit risk, which integrates multiple sampling, multiple kernel fuzzy self-organizing map and local accuracy ensemble. To preprocess imbalanced sample sets of credit risk evaluation, multiple sampling approaches (synthetic minority over-sampling technique, under sampling and hybrid sampling) are improved and integrated to acquire balanced datasets. To construct more suitable base classifiers , multiple kernel functions (Gaussian, Polynomial and Sigmoid) respectively are used to improve fuzzy self-organizing map. Then, the balanced sample sets are respectively processed by the improved base classifiers to acquire different prediction results. The local accuracy ensemble method is employed to dynamically synthesize these prediction results to obtain final result. The new ensemble model can further avoid over-fitting and information loss, be more suitable to handle the dataset including different financial indicators, and acquire the stable and satisfactory prediction result for imbalanced credit risk evaluation In the empirical research, this paper adopts the financial data from Chinese listed companies, and makes the comparative analysis with the relative models step by step. The results can prove that the new ensemble model presented by this article has better performance than other methods in terms of evaluating the imbalanced credit risk.},
  archive      = {J_ASOC},
  author       = {Lu Wang and Yuangao Chen and Hui Jiang and Jianrong Yao},
  doi          = {10.1016/j.asoc.2020.106262},
  journal      = {Applied Soft Computing},
  pages        = {106262},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Imbalanced credit risk evaluation based on multiple sampling, multiple kernel fuzzy self-organizing map and local accuracy ensemble},
  volume       = {91},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Feature selection based on regularization of sparsity based
regression models by hesitant fuzzy correlation. <em>ASOC</em>,
<em>91</em>, 106255. (<a
href="https://doi.org/10.1016/j.asoc.2020.106255">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, the Ridge, LASSO and Elastic Net regression methods are adapted for the task of selecting feature. In order to enhance the feature selection performance via these methods, a Hesitant Fuzzy Correlation Matrix (HFCM) is added to the objective functions of these models for addressing the minimum redundancy of features. To this end, the fuzzy C-means clustering is utilized, and the obtained fuzzy clusters are projected on the features in a way that the number of fuzzy Membership Functions (MF) for each feature is equal to the number of clusters. Then, the projected MFs on each feature are considered as a Hesitant Fuzzy Set (HFS), and thereby the hesitant fuzzy correlation between features is calculated. Afterward, the obtained HFCM is employed in the regression methods for securing the minimum redundancy of features. Eventually, the accuracies of the selected features, achieved by these methods, are determined by three different classification models such as Naive Bayes, SVM and Decision Tree . A large number of experiments are conducted over twenty-four classification datasets to demonstrate the efficiency and applicability of using HFCM in some classical regression methods.},
  archive      = {J_ASOC},
  author       = {Mahla Mokhtia and Mahdi Eftekhari and Farid Saberi-Movahed},
  doi          = {10.1016/j.asoc.2020.106255},
  journal      = {Applied Soft Computing},
  pages        = {106255},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Feature selection based on regularization of sparsity based regression models by hesitant fuzzy correlation},
  volume       = {91},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A novel multi-focus image fusion by combining simplified
very deep convolutional networks and patch-based sequential
reconstruction strategy. <em>ASOC</em>, <em>91</em>, 106253. (<a
href="https://doi.org/10.1016/j.asoc.2020.106253">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-focus image fusion is an important approach to obtain the composite image with all objects in focus, and it can be treated as an image segmentation problem, which is solved by convolutional neural networks (CNN). For CNN-based multi-focus image fusion methods, public training dataset does not exist, and the network model determines the recognition accuracy of the focused and defocused pixels. Considering these problems, we proposed a novel CNN-based multi-focus image fusion method by combining simplified very deep convolutional networks and patch-based sequential reconstruction strategy in this study. Firstly, the defocused images with five blurred levels were simulated by the Gaussian filter , and a novel training dataset was constructed for multi-focus image fusion. Secondly, the very deep convolutional networks model was simplified to design a Siamese CNN model, and this model was used to recognize the focused and defocused pixels. Thirdly, the focused and defocused regions were detected by the patch-based sequential reconstruction strategy, and the final decision map was refined by the morphological operator . Finally, the multi-focus image fusion was performed. Lytro dataset as a public multi-focus image dataset was used to prove the validation of the proposed method. Information entropy, mutual information, universal image quality index, visual information fidelity , and edge retention were adopted as evaluation metrics , and the proposed method was compared with state-of-the-art methods. Experimental results demonstrated that the proposed method can achieve state-of-the-art fusion results in terms of visual quality and objective assessment.},
  archive      = {J_ASOC},
  author       = {Chang Wang and Zongya Zhao and Qiongqiong Ren and Yongtao Xu and Yi Yu},
  doi          = {10.1016/j.asoc.2020.106253},
  journal      = {Applied Soft Computing},
  pages        = {106253},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A novel multi-focus image fusion by combining simplified very deep convolutional networks and patch-based sequential reconstruction strategy},
  volume       = {91},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Optimal power flow using the AMTPG-jaya algorithm.
<em>ASOC</em>, <em>91</em>, 106252. (<a
href="https://doi.org/10.1016/j.asoc.2020.106252">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This work proposes the implementation of a recently invented meta-heuristic optimization solver namely, an adaptive multiple teams perturbation-guiding Jaya (AMTPG-Jaya) technique to tackle with diverse single goal optimum power flow (OPF) forms. The AMTPG-Jaya solver employs numerous populations named as teams to investigate the search domain. Each team is guided by a number of movement equations (exploration pathways). The algorithm adjusts the number of teams along with the approaching to the finest so-far nominee solution. In this study, an original AMTPG-Jaya inspired approach to handle the OPF formulation is suggested. The efficacy of the AMTPG-Jaya solver is scrutinized and tested on two well-known standard power systems with different goal functions. The optimization outcomes reveal that the AMTPG-Jaya is able to reach an optimal solution with brilliant convergence speed. In addition, a robustness examination is implemented to evaluate the reliability of the AMTPG-Jaya solver. The simulation results disclose the dominance and potential of the AMTPG-Jaya over many solvers recently stated in the previous publications with regard to solution quality and validity.},
  archive      = {J_ASOC},
  author       = {Warid Warid},
  doi          = {10.1016/j.asoc.2020.106252},
  journal      = {Applied Soft Computing},
  pages        = {106252},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Optimal power flow using the AMTPG-jaya algorithm},
  volume       = {91},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A decomposition-based memetic algorithm using helper
objectives for shortwave radio broadcast resource allocation problem in
china. <em>ASOC</em>, <em>91</em>, 106251. (<a
href="https://doi.org/10.1016/j.asoc.2020.106251">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Shortwave radio broadcast resource allocation (SRBRA) is an NP-hard combinatorial optimization problem with practical significance in many countries. The aim of SRBRA is to allocate radio programs to transmission devices so as to broadcast all radio programs felicitously with a maximized objective of total qualified monitoring sites. To solve such an issue presented by the State Administration of Press, Publication, Radio, Film and Television (SAPPRFT) in China, the authors propose a parallel multi-objective memetic algorithm based on helper objective assistance and decomposition technique, called pMMA-HD. Specifically, a multi-objective evolutionary optimization framework is used to maintain the diversity in a single objective problem, where the authors add a helper objective function on the diversity metric. Then, the decomposition method is performed to settle this transformational multi-objective problem effectively, and a diversity matrix is preserved in order to provide sufficient candidates for a decision maker to select from. To approach the pareto front , an efficient local search with a guided perturbation is integrated after the evolutionary process. Considering the natural characteristics of evolutionary algorithms (EAs), a thread-based parallel version of MMA-HD is carefully designed to improve the computational efficiency. Experiments are performed on real-world benchmarks to compare pMMA-HD with three categories of algorithms: one exact solver clasp , two canonical multi-objective algorithms and three local search methods . Afterwards, the experiments on parameters tuning are conducted based on the Taguchi method of design-of-experiment. Besides, the validation of strategies are investigated and the robustness of solutions is further analyzed. The experimental results on the real-world dataset validate the efficiency of pMMA-HD by updating 33 best-known solutions.},
  archive      = {J_ASOC},
  author       = {Yupeng Zhou and Mingjie Fan and Feifei Ma and Xin Xu and Minghao Yin},
  doi          = {10.1016/j.asoc.2020.106251},
  journal      = {Applied Soft Computing},
  pages        = {106251},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A decomposition-based memetic algorithm using helper objectives for shortwave radio broadcast resource allocation problem in china},
  volume       = {91},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A one-class classification decision tree based on kernel
density estimation. <em>ASOC</em>, <em>91</em>, 106250. (<a
href="https://doi.org/10.1016/j.asoc.2020.106250">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {One-class Classification (OCC) is an important field of machine learning which aims at predicting a single class on the basis of its lonely representatives and potentially some additional counter-examples. OCC is thus opposed to traditional classification problems involving two or more classes, and addresses the issue of class unbalance. There is a wide range of one-class models which give satisfaction in terms of performance. But at the time of explainable artificial intelligence , there is an increasing need for interpretable models. The present work advocates a novel one-class model which tackles this challenge. Within a greedy and recursive approach, our proposal for an explainable One-Class decision Tree (OC-Tree) rests on kernel density estimation to split a data subset on the basis of one or several intervals of interest. Thus, the OC-Tree encloses data within hyper-rectangles of interest which can be described by a set of rules. Against state-of-the-art methods such as Cluster Support Vector Data Description (ClusterSVDD), One-Class Support Vector Machine (OCSVM) and isolation Forest (iForest), the OC-Tree performs favorably on a range of benchmark datasets. Furthermore, we propose a real medical application for which the OC-Tree has demonstrated effectiveness, through the ability to tackle interpretable medical diagnosis aid based on unbalanced datasets.},
  archive      = {J_ASOC},
  author       = {Sarah Itani and Fabian Lecron and Philippe Fortemps},
  doi          = {10.1016/j.asoc.2020.106250},
  journal      = {Applied Soft Computing},
  pages        = {106250},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A one-class classification decision tree based on kernel density estimation},
  volume       = {91},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Estimating missing data using novel correlation maximization
based methods. <em>ASOC</em>, <em>91</em>, 106249. (<a
href="https://doi.org/10.1016/j.asoc.2020.106249">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The accurate estimation of missing data plays a vital role in ensuring a high level of data quality. The missing values should be imputed before performing data mining, machine learning , and other data processing tasks. Ten correlation-based imputation methods are proposed in this paper. All of these methods try to maximize the correlation between a missing feature and other features. The maximization is achieved by selecting segments of data that have strong correlations. The proposed approach involves the following main steps to impute each missing instance. First, a base set is selected from complete instances. Second, data segments with strong correlations are generated using the base set and the rest of the complete instances. Finally, each missing value is imputed by applying linear models to the discovered segments of data. This study considers seven real datasets from different fields with different missing rates. The imputation quality of the proposed methods is compared to those of seven other imputation approaches in terms of three well-known evaluation criteria. The experimental results reveal that the proposed approach has better imputation performance than competing imputation techniques in most cases.},
  archive      = {J_ASOC},
  author       = {Amir Masoud Sefidian and Negin Daneshpour},
  doi          = {10.1016/j.asoc.2020.106249},
  journal      = {Applied Soft Computing},
  pages        = {106249},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Estimating missing data using novel correlation maximization based methods},
  volume       = {91},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A new weighted distance-based approximation methodology for
flow shop scheduling group decisions under the interval-valued fuzzy
processing time. <em>ASOC</em>, <em>91</em>, 106248. (<a
href="https://doi.org/10.1016/j.asoc.2020.106248">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Scheduling plays a significant role in production planning. This paper introduces a new extension of a weighted distance-based approximation (WBDA) methodology to determine the sequence of jobs in flow shop scheduling problems. Furthermore, a new version of WDBA is used to specify the decision-makers’ weights. In reality, there are many inherent uncertainties in the processing time owing to the batch loading, the capacity of processing unit, operator skills, transformation quality of raw materials in the production systems, imperfect information regarding systems, transportation lag, traffic jam, machine disablements, arrival of new jobs, and resources deficiencies. In this situation, interval-valued fuzzy sets (IVFSs) are employed for considering the uncertainty of practical conditions. Finally, an illustrative example of the literature under different weight schemes is adopted and solved to address the strengths of the introduced methodology better.},
  archive      = {J_ASOC},
  author       = {Y. Dorfeshan and R. Tavakkoli-Moghaddam and S.M. Mousavi and B. Vahedi-Nouri},
  doi          = {10.1016/j.asoc.2020.106248},
  journal      = {Applied Soft Computing},
  pages        = {106248},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A new weighted distance-based approximation methodology for flow shop scheduling group decisions under the interval-valued fuzzy processing time},
  volume       = {91},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). On the feasibility of using evolvable hardware for hardware
trojan detection and prevention. <em>ASOC</em>, <em>91</em>, 106247. (<a
href="https://doi.org/10.1016/j.asoc.2020.106247">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Evolvable hardware (EH) architectures are capable of changing their configuration and behavior dynamically based on inputs from the environment. In this paper, we investigate the feasibility of using EH to prevent Hardware Trojan Horses (HTHs) from being inserted, activated, or propagated in a digital electronic chip. HTHs are malicious hardware components that intend to leak secret information or cause malfunctioning at run-time in the chip in which they are integrated. We hypothesize that EH can detect internal circuit errors at run-time and reconfigure to a state in which the errors are no longer present. We implement a Virtual Reconfigurable Circuit (VRC) on a Field-Programmable Gate Array (FPGA) that autonomously and periodically reconfigures itself based on an Evolutionary Algorithm (EA). New VRC configurations are generated with an on-chip EA engine. We show that the presented approach is applicable in a scenario in which (1) the HTH-critical areas in the circuit are known in advance, and (2) the VRC is a purely combinatorial circuit , as opposed to the on-chip memory holding the golden reference, which requires one or more cycles to be read/written. We compare two different approaches for protecting the system against HTHs: Genetic Programming (GP) and Cartesian Genetic Programming (CGP). The paper reports on experiments on four benchmark circuits and gives an overview of both the limitations and the added value of the presented approaches.},
  archive      = {J_ASOC},
  author       = {Mansoureh Labafniya and Stjepan Picek and Shahram Etemadi Borujeni and Nele Mentens},
  doi          = {10.1016/j.asoc.2020.106247},
  journal      = {Applied Soft Computing},
  pages        = {106247},
  shortjournal = {Appl. Soft. Comput.},
  title        = {On the feasibility of using evolvable hardware for hardware trojan detection and prevention},
  volume       = {91},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A benchmark suite for designing combinational logic circuits
via metaheuristics. <em>ASOC</em>, <em>91</em>, 106246. (<a
href="https://doi.org/10.1016/j.asoc.2020.106246">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The evolvable hardware literature reports several methods for the evolution of digital circuits . However, there is a large variability in the set of problems and the appropriate metrics used for the evaluation of the results. In this paper, we propose a set of representative problems to comparatively evaluate metaheuristics when designing Combinational Logic Circuits (CLCs). We also define a set of performance measurements and descriptive statistics to analyze the results found by the search techniques. As a case study, we compare Cartesian Genetic Programming variants applied to this domain. The results highlight the benefits of the proposed heterogeneous benchmark suite in the analysis of metaheuristics when designing CLCs.},
  archive      = {J_ASOC},
  author       = {Lucas Augusto Müller de Souza and José Eduardo Henriques da Silva and Luciano Jerez Chaves and Heder Soares Bernardino},
  doi          = {10.1016/j.asoc.2020.106246},
  journal      = {Applied Soft Computing},
  pages        = {106246},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A benchmark suite for designing combinational logic circuits via metaheuristics},
  volume       = {91},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Ultra-high reliable optimization based on monte carlo tree
search over nakagami-m fading. <em>ASOC</em>, <em>91</em>, 106244. (<a
href="https://doi.org/10.1016/j.asoc.2020.106244">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Supporting the ultra-reliable and low-latency communications (URLLCs) has become one of the major goals for future wireless networks. In this paper, we present an analytical reliability model for user equipment (UE) connecting multiple base stations (BSs) with multi-stream carrier aggregation (MSCA) technology. We first derive a closed-form expression for reliability characterization using signal-to-interference-plus-noise (SINR) model over Nakagami- m m fading. We then formulate a joint resource allocation problem to maximize reliability, considering UE association, sub-carrier assignment and discrete power allocation . With distributed decision making (DDM) theory, we decouple it into two sub-problems, and each sub-problem is formulated as a separate Markov Decision Problem (MDP). We further propose the Monte Carlo Tree Search (MCTS) method to find the optimal solution for each sub-problem. The iterative joint optimization method based on DDM is also proposed. Our simulation results show that the reliability with MSCA increases with increasing the number of sub-carriers, as well as increasing the power allocation . We also show that our algorithm is an effective way in finding the optimal resource allocation for reliability improvement.},
  archive      = {J_ASOC},
  author       = {Jie Jia and Jian Chen and Xingwei Wang},
  doi          = {10.1016/j.asoc.2020.106244},
  journal      = {Applied Soft Computing},
  pages        = {106244},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Ultra-high reliable optimization based on monte carlo tree search over nakagami-m fading},
  volume       = {91},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Lightning search algorithm-based contextually fused
multilevel image segmentation. <em>ASOC</em>, <em>91</em>, 106243. (<a
href="https://doi.org/10.1016/j.asoc.2020.106243">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, a new well-organized fusion-based 3D Otsu energy thresholding prototypical for multilevel color image segmentation by means of lightning search algorithm (LSA) has been projected. Although, 3D Otsu works by means of between-class variances over the help of 3-D histogram but the performance is not satisfactory. As a result, the energy curve model is implemented, that works on contextual information of an image. However, using the concept of energy curve for 3D Otsu produces better result, but at the cost of complexity and also correspondingly the complication level for pick out appropriate thresholds is high. To overcome this limitation, the concept of LSA optimization algorithm is introduced. LSA is a fresh optimization process encouraged by the winding characteristics of lightening through a thunderstorm. In this paper, LSA is used to shorten the delinquent of comprehensive exploration for finding the finest thresholds. In addition to this, to enhance the quality of multi-level segmented image the concept of fusion based on local contrast is introduced. In this paper, 1D, 2D and 3D-Otsu methods using numerous optimization algorithms are implemented with energy curve and fusion based approach and compared with proposed fusion based energy 3D Otsu method using LSA algorithm. Experimental outputs demonstrate that the proposed Fusion-Energy-3D Otsu-LSA algorithm is outperforms and it can be established by comparing the well-known fidelity constraints of an image.},
  archive      = {J_ASOC},
  author       = {Ashish Kumar Bhandari and Neha Singh and Immadisetty Vinod Kumar},
  doi          = {10.1016/j.asoc.2020.106243},
  journal      = {Applied Soft Computing},
  pages        = {106243},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Lightning search algorithm-based contextually fused multilevel image segmentation},
  volume       = {91},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A surrogate-assisted particle swarm optimization using
ensemble learning for expensive problems with small sample datasets.
<em>ASOC</em>, <em>91</em>, 106242. (<a
href="https://doi.org/10.1016/j.asoc.2020.106242">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Solving the real-world optimization problems often needs a large number of expensive function evaluations (FEs) by using evolutionary algorithms (EAs). To alleviate this difficulty, surrogate-assisted EAs (SAEAs) have attracted more and more attention from academia and industry. However, the existing SAEAs need a large amount of sample points to construct surrogate model within the expected times of FEs, otherwise it cannot achieve satisfactory prediction accuracy. Few SAEAs can reduce the times of expensive FEs while a high-quality surrogate model is constructed using a small number of sample points. In this paper, a novel SAEAs inspired from ensemble learning is proposed. In the proposed algorithm, the small sample date set is divided into multiple subsets, and the surrogate model is trained on each subset. Two new model management strategies based on ensemble learning are applied to global search and local search respectively. Two search methods are cleverly combined to form a high precision surrogate ensemble. In order to verify the performance of the proposed method, we performed comprehensive tests on eight benchmark functions from 10 to 50 dimensions, and compared their result with the five state-of-the-art SAEAs. Experimental results demonstrate that the proposed method shows superior performance in a majority of benchmarks when only a limited computational budget is available. In addition, we apply the proposed algorithm to three real-time optimization problems. The results of each problem are compared with the solutions to verify the effectiveness of the algorithm in solving engineering application problems.},
  archive      = {J_ASOC},
  author       = {Chaodong Fan and Bo Hou and Jinhua Zheng and Leyi Xiao and Lingzhi Yi},
  doi          = {10.1016/j.asoc.2020.106242},
  journal      = {Applied Soft Computing},
  pages        = {106242},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A surrogate-assisted particle swarm optimization using ensemble learning for expensive problems with small sample datasets},
  volume       = {91},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A new fractional-order general type-2 fuzzy predictive
control system and its application for glucose level regulation.
<em>ASOC</em>, <em>91</em>, 106241. (<a
href="https://doi.org/10.1016/j.asoc.2020.106241">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper a new robust fractional-order predictive controller is presented and employed to regulate the glucose level in type-1 diabetes. The dynamics of the system is fully unknown an it is online estimated by a fractional-order model using interval Type-2 (T2) fuzzy logic system . The proposed control system is composed of two main controllers which are the predictive General T2 Fuzzy Logic Controller (GT2-FLC) and compensator controller. In this structure, the main controller is the GT2-FLC which is optimized via the Biogeography-based Optimization (BBO) algorithm such that to minimize a cost function in a fixed prediction horizon. The compensator controller is designed to guarantee the closed-loop asymptotic stability . The performance of proposed control strategy is examined on the modified Bergman’s model of some patients with time-varying parameters, external noise perturbation and meal disturbances. The effectiveness of the proposed control scheme is verified and is compared with the other T2 fuzzy and well-known model predictive controllers. The results of the paper clearly show the superiority of the proposed T2 fuzzy logic control system.},
  archive      = {J_ASOC},
  author       = {Ardashir Mohammadzadeh and Tufan Kumbasar},
  doi          = {10.1016/j.asoc.2020.106241},
  journal      = {Applied Soft Computing},
  pages        = {106241},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A new fractional-order general type-2 fuzzy predictive control system and its application for glucose level regulation},
  volume       = {91},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A cascaded deep convolution neural network based CADx system
for psoriasis lesion segmentation and severity assessment.
<em>ASOC</em>, <em>91</em>, 106240. (<a
href="https://doi.org/10.1016/j.asoc.2020.106240">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The design of an efficient computer-aided diagnosis (CADx) system for psoriasis severity assessment demands both accurate segmentation and classification of psoriasis lesions. Recently, few studies have been conducted to design automatic CADx systems for psoriasis severity assessment using traditional machine learning approaches . However, these approaches are highly featured dependent and require extensive and careful feature extraction. Among a large number of features extracted, assessing the features which contribute significantly to the classifier performing is a difficult and time-consuming task. Large features lead to poor generalization, due to high inter and intra-class variation of psoriasis skin lesions. This makes the task of implementing a reliable CADx system challenging. In such similar cases, Deep learning-based approaches have been proven better because of their ability to learn and make intelligent decisions automatically. In this study, a fully automated deep learning-based CADx system for psoriasis has been proposed. The system combines three modules in a single framework for achieving different objectives namely; recognition of psoriasis and non-psoriasis disease, automatic segmentation of psoriatic lesion, and its severity assessment. The modified U-Net and modified VGG-16 model have been implemented and trained for the segmentation and classification task respectively. The severity assessment module is capable of extracting discriminative features specifically related to the psoriatic lesion, which is automatically segmented by the segmentation module. The performance of the proposed CADx framework has been extensively evaluated on an extensive psoriasis dataset using k -fold cross-validation procedure. The appropriateness of the proposed system has been justified in terms of its performance at each of the three stages along with benchmarking against previously reported systems. Further, the system accuracy and reliability index has been evaluated for a dataset of varying size to validate the consistency of the proposed system.},
  archive      = {J_ASOC},
  author       = {Manoranjan Dash and Narendra D. Londhe and Subhojit Ghosh and Ritesh Raj and Rajendra S. Sonawane},
  doi          = {10.1016/j.asoc.2020.106240},
  journal      = {Applied Soft Computing},
  pages        = {106240},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A cascaded deep convolution neural network based CADx system for psoriasis lesion segmentation and severity assessment},
  volume       = {91},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Design of modeling error PDF based fuzzy neural network for
effluent ammonia nitrogen prediction. <em>ASOC</em>, <em>91</em>,
106239. (<a href="https://doi.org/10.1016/j.asoc.2020.106239">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To predict the effluent ammonia nitrogen (NH 4 -N) of wastewater treatment process (WWTP), the soft computing methods are widely used, in which the mean square error (MSE) is usually adopted as the performance criterion. However, the MSE based methods cannot fully utilize the statistic information of data and are vulnerable to the nonzero-mean noise. To address these issues, the modeling-error probability density function based fuzzy neural network (PDF-FNN) is proposed in this paper. Firstly, the modeling error PDF criterion is generated to minimize the spatial deviation between the modeling error distribution and the predefined target. Then, a gradient descent method with adaptive learning rate is presented to update the parameters of PDF-FNN. Furthermore, the convergence of PDF-FNN is analyzed from a mathematical point of view. Finally, a nonlinear system modeling and the effluent NH 4 -N prediction in WWTP are applied to prove the effectiveness of the proposed PDF-FNN. The results indicate that the PDF-FNN has better prediction accuracy and model stability than other methods, especially in the noisy environment .},
  archive      = {J_ASOC},
  author       = {Junfei Qiao and Limin Quan and Cuili Yang},
  doi          = {10.1016/j.asoc.2020.106239},
  journal      = {Applied Soft Computing},
  pages        = {106239},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Design of modeling error PDF based fuzzy neural network for effluent ammonia nitrogen prediction},
  volume       = {91},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A feature-fusion framework of clinical, genomics, and
histopathological data for METABRIC breast cancer subtype
classification. <em>ASOC</em>, <em>91</em>, 106238. (<a
href="https://doi.org/10.1016/j.asoc.2020.106238">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Breast cancer is the most common cancer type attacking women worldwide. Also, breast cancer has been phenotypically classified into five subtypes. Each subtype group has unique characteristics that demonstrate the heterogeneity present within the breast cancer tumour. In 2012, the American Association for Cancer Research provided a population based molecular integrative clusters for the METABRIC (Molecular Taxonomy of Breast Cancer International Consortium) dataset, resulting in ten subtypes. Previous work on the METABRIC dataset used only gene expression data to figure out the effective genes for each subtype, without applying integration to benefit from all data sources. The objective of this paper is to present a breast cancer subtype classification model that applies feature fusion on the METABRIC datasets, namely clinical, gene expression, Copy Number Aberrations (CNA), Copy Number Variations (CNV), and histopathological images . State-of-the-art machine learning classifiers were applied on different data profiles, including Linear-SVM, Radial-SVM, Random Forests (RF), Ensemble SVM (E-SVM), and Boosting. The highest accuracy achieved for IntClust subtyping was 88.36\% using Linear-SVM, applied on the data profile with features fused from the clinical, gene expression, CNA, and CNV datasets, with a Jaccard and Dice scores of 0.802 and 0.8835, respectively. On the other hand, for the Pam50 subtyping, an accuracy of 97.1\% was achieved, Jaccard score ranging from 0.9439 to 0.9472, and Dice score of 0.971, using Linear-SVM and E-SVM classifiers, with several data profiles that include features from histopathological images. Conclusively, the significance of our study is to validate that using feature fusion from various METABRIC datasets improves breast cancer subtypes classification performance. Moreover, histopathological images give promising results on Pam50 subtypes, and it is expected to improve the accuracy for IntClust subtyping when applied on a higher population.},
  archive      = {J_ASOC},
  author       = {Ala’a El-Nabawy and Nashwa El-Bendary and Nahla A. Belal},
  doi          = {10.1016/j.asoc.2020.106238},
  journal      = {Applied Soft Computing},
  pages        = {106238},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A feature-fusion framework of clinical, genomics, and histopathological data for METABRIC breast cancer subtype classification},
  volume       = {91},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A new multi-criteria decision model based on incomplete dual
probabilistic linguistic preference relations. <em>ASOC</em>,
<em>91</em>, 106237. (<a
href="https://doi.org/10.1016/j.asoc.2020.106237">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The use of dual probabilistic linguistic term sets (DPLTSs) to represent the use’s preferences in decision making can reflect the decision maker’s cognitive certainty and uncertainty. Additionally, the appearance of incomplete preferences is a recurring phenomenon that must be taken into account if you want to make a successful decision. This paper presents a new multi-criteria decision model based on the incomplete dual probabilistic linguistic preference relations (IDPLPRs). We first propose a step-by-step repairing method to repair the linguistic section and probabilistic section of IDPLPRs separately. The superiority is that this step-by-step method conforms to the principle of element generation . After that, the consistency index based on the distance measure between the dual probabilistic linguistic preference relations (DPLPRs) is defined to check and improve the consistency of DPLPRs. Then the weights of criteria can be obtained by information fusion. Moreover, we construct optimistic and pessimistic data envelopment analysis models under the dual probabilistic linguistic environment to do the sorting process. Optimistic and pessimistic data envelopment analysis models can demonstrate the efficiency of each decision-making unit (DMU) from the perspective of the most and least favorable. Finally, we simulate a cased of 5G industry market to help enterprises choose appropriate 5G partners by using proposed methods.},
  archive      = {J_ASOC},
  author       = {Wanying Xie and Zeshui Xu and Zhiliang Ren and Enrique Herrera-Viedma},
  doi          = {10.1016/j.asoc.2020.106237},
  journal      = {Applied Soft Computing},
  pages        = {106237},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A new multi-criteria decision model based on incomplete dual probabilistic linguistic preference relations},
  volume       = {91},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Evaluating deep models for absenteeism prediction of public
security agents. <em>ASOC</em>, <em>91</em>, 106236. (<a
href="https://doi.org/10.1016/j.asoc.2020.106236">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Absenteeism is a complex phenomenon characterized by the physical absence of the individual, usually at his workplace. Such absences generally lead to innumerable personal, social, and economic losses, particularly in public security institutions, where incidence is higher than the one verified in other occupational categories. Identifying preponderant absenteeism factors and allowing preventive actions to be carried out effectively may be beneficial to these institutions and their agents. Such knowledge could be acquired hypothetically by exploiting large human resources data sets. In this paper, we investigate the potential of machine learning classifiers to identify security workers prone to long-term absenteeism. Such predictors shall make decisions based on the professional history of each agent, which is extracted from databases of public security institutions. In our study, we performed experiments on a database comprised of 6 years of professional data from workers of the Military Police of Alagoas, Brazil . We evaluated deep models, including variations of Multilayer Perceptrons (MLP), Recurrent Neural Networks (RNN) and Long Short-Term Memory (LSTM), and compared with baseline Support-Vector Machines (SVM) classifiers. We show results revealing that the best architectures achieve up to 78\% of accuracy. Also, experiments indicated that the use of data accumulated over several years improves the accuracy of the prediction of absenteeism. Finally, we conclude that such results encourage the usage of deep learning techniques to predict absenteeism and support the implementation of effective prevention measures in these institutions.},
  archive      = {J_ASOC},
  author       = {Edival Lima and Thales Vieira and Evandro de Barros Costa},
  doi          = {10.1016/j.asoc.2020.106236},
  journal      = {Applied Soft Computing},
  pages        = {106236},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Evaluating deep models for absenteeism prediction of public security agents},
  volume       = {91},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Structural improved regular simplex support vector machine
for multiclass classification. <em>ASOC</em>, <em>91</em>, 106235. (<a
href="https://doi.org/10.1016/j.asoc.2020.106235">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Although the structural regularized support vector machine (SRSVM) can enhance the generalization capability of the standard support vector machine (SVM), its current version is used only for binary classification . To make SRSVM adapt to the K -class classification, the most direct approach is combining it with partitioning strategies, which may however lead to the following shortcomings: (1) Extracting structural information repeatedly for individual classifiers based on different class partitions increases the computational complexity . (2) Individual classifiers can hardly utilize complete data structural information. Under the basic framework of regular simplex support vector machine (RSSVM), we developed a novel structural improved regular simplex support vector machine (SIRSSVM). SIRSSVM generates only a single primal optimization problem , into which the data structural information within all classes is embedded, rather than using only partial structural information to construct individual classifiers as partitioning strategies do. Additionally, we modified the sequential minimization optimization (SMO)-type solver for RSSVM to adapt the proposed SIRSSVM model. Experimental results verified that our SIRSSVM could achieve excellent performance on both generalization capability and training efficiency.},
  archive      = {J_ASOC},
  author       = {Long Tang and Yingjie Tian and Wenjun Li and Panos M. Pardalos},
  doi          = {10.1016/j.asoc.2020.106235},
  journal      = {Applied Soft Computing},
  pages        = {106235},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Structural improved regular simplex support vector machine for multiclass classification},
  volume       = {91},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A compact neuromorphic architecture with dynamic routing to
efficiently simulate the FXECAP-l algorithm for real-time active noise
control. <em>ASOC</em>, <em>91</em>, 106233. (<a
href="https://doi.org/10.1016/j.asoc.2020.106233">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this work, we introduce, for the first time, the design of a compact neuromorphic architecture to efficiently support a filtered-x error-coded affine projection-like (FXECAP-L) algorithm that is based on affine projection (AP) algorithms for active noise cancellation (ANC) in an acoustic duct. To date, few practical ANC implementations have used AP algorithms because of their high computational complexity , despite providing fast convergence speeds. One of the main factors that increases their computational complexity is linked to the dimensions of the matrix used in the AP algorithm’s computations. Evidently, the largest dimensions of the matrix increase the convergence speed of the AP algorithms by paying a penalty in terms of area consumption. However, convergence speed is crucial in ANC applications since this factor determines the speed at which the noise is canceled. Recently, an FXECAP-L algorithm with evolving order has been proposed to dynamically reduce the dimensions of the matrix by maintaining the convergence speed of AP algorithms. Here, we propose a compact neuromorphic architecture with a dynamic routing mechanism to efficiently implement the evolutionary method of the FXECAP-L algorithm by creating a virtual matrix, whose dimensions can be modified over the filter processing. In this way, we avoid spending a large amount of memory to save the largest matrix elements. In addition, the inclusion of the dynamic routing mechanism in the proposed neuromorphic architecture has allowed us to guarantee low area consumption since the neuromorphic architecture is capable of simulating different adaptive structures without modifying its structure. Here, the neuromorphic architecture has been configured as the system identification and ANC controller for practical noise cancellation in an acoustic duct. Our results have demonstrated that the combination of the properties of the FXECAP-L algorithm and the implementation techniques generate a versatile signal processing development tool that can be used in practical real-time ANC applications.},
  archive      = {J_ASOC},
  author       = {Giovanny Sanchez and Juan-Gerardo Avalos and Angel Vazquez and Luis Garcia and Thania Frias and Karina Toscano and Gonzalo Duchen and Hector Perez},
  doi          = {10.1016/j.asoc.2020.106233},
  journal      = {Applied Soft Computing},
  pages        = {106233},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A compact neuromorphic architecture with dynamic routing to efficiently simulate the FXECAP-L algorithm for real-time active noise control},
  volume       = {91},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A decomposition-based multi-objective optimization approach
for extractive multi-document text summarization. <em>ASOC</em>,
<em>91</em>, 106231. (<a
href="https://doi.org/10.1016/j.asoc.2020.106231">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Currently, due to the overflow of textual information on the Internet, automatic text summarization methods are becoming increasingly important in many fields of knowledge. Extractive multi-document text summarization approaches are intended to automatically generate summaries from a document collection, covering the main content and avoiding redundant information. These approaches can be addressed through optimization techniques. In the scientific literature, most of them are single-objective optimization approaches, but recently multi-objective approaches have been developed and they have improved the single-objective existing results. In addition, in the field of multi-objective optimization, decomposition-based approaches are being successfully applied increasingly. For this reason, a Multi-Objective Artificial Bee Colony algorithm based on Decomposition (MOABC/D) is proposed to solve the extractive multi-document text summarization problem. An asynchronous parallel design of MOABC/D algorithm has been implemented in order to take advantage of multi-core architectures. Experiments have been carried out with Document Understanding Conferences (DUC) datasets, and the results have been evaluated with Recall-Oriented Understudy for Gisting Evaluation (ROUGE) metrics. The obtained results have improved the existing ones in the scientific literature for ROUGE-1, ROUGE-2, and ROUGE-L scores, also reporting a very good speedup.},
  archive      = {J_ASOC},
  author       = {Jesus M. Sanchez-Gomez and Miguel A. Vega-Rodríguez and Carlos J. Pérez},
  doi          = {10.1016/j.asoc.2020.106231},
  journal      = {Applied Soft Computing},
  pages        = {106231},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A decomposition-based multi-objective optimization approach for extractive multi-document text summarization},
  volume       = {91},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Fuzzy-rough assisted refinement of image processing
procedure for mammographic risk assessment. <em>ASOC</em>, <em>91</em>,
106230. (<a href="https://doi.org/10.1016/j.asoc.2020.106230">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The use of computer aided diagnosis (CAD) systems, which are computer based tools for the automatic analysis of medical images such as mammogram and prostate MRI, can assist in the early detection and diagnosis of developing cancer. In the process of CAD for mammogram, the task of image processing (IP) plays a fundamental role in providing promising diagnostic results, by exploiting high-quality features extracted from the mammographic images. Normally, an IP procedure for mammographic images involves three mechanisms: region of interest (ROI) extraction, image enhancement (IE) and feature extraction (FE). However, an improper utilisation of IE may lead to an inferior composition of the features due to unexpected enhancement of any irrelevant or useless information in ROI. In order to overcome this problem, a fuzzy-rough refined IP (FRIP) framework is presented in this paper to improve the quality of mammographic image features hierarchically. Following the proposed framework, the ROI of each mammographic image is segmented and enhanced locally in the area of the block which is of the highest value of fuzzy positive region (FPR). Here, FPR implies a positive dependency relationship between the block and the decision with regard to the given feature set. The higher a block’s FPR value the more certain its underlying image category. To attain a high quality of the image enhancement procedure, the winner block will be further improved by a multi-round strategy to create a pool of IE results. As such, for a mammographic image, after embedding the candidate enhanced blocks into the original ROI, the respectively extracted features from the locally enhanced ROI are compared against each other on the basis of the value of FPR. A given image is therefore represented by a set of features which are supported by the premier FPR among all of the resulting extracted features. The quality of the extracted features by FRIP is compared against that of those directly extracted from the original images, from the globally enhanced images or from the randomly locally enhanced images in performing classification tasks . The experimental results demonstrate that the mammographic risk assessment results based on the features achieved by the proposed framework are much improved over those by the alternatives.},
  archive      = {J_ASOC},
  author       = {Yanpeng Qu and Qilin Fu and Changjing Shang and Ansheng Deng and Reyer Zwiggelaar and Minu George and Qiang Shen},
  doi          = {10.1016/j.asoc.2020.106230},
  journal      = {Applied Soft Computing},
  pages        = {106230},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Fuzzy-rough assisted refinement of image processing procedure for mammographic risk assessment},
  volume       = {91},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Spam filtering using a logistic regression model trained by
an artificial bee colony algorithm. <em>ASOC</em>, <em>91</em>, 106229.
(<a href="https://doi.org/10.1016/j.asoc.2020.106229">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Email spam is a serious problem that annoys recipients and wastes their time. Machine-learning methods have been prevalent in spam detection systems owing to their efficiency in classifying mail as solicited or unsolicited. However, existing spam detection techniques usually suffer from low detection rates and cannot efficiently handle high-dimensional data. Therefore, we propose a novel spam detection method that combines the artificial bee colony algorithm with a logistic regression classification model . The empirical results on three publicly available datasets (Enron, CSDMC2010, and TurkishEmail) show that the proposed model can handle high-dimensional data thanks to its highly effective local and global search abilities. We compare the proposed model’s spam detection performance to those of support vector machine , logistic regression , and naive Bayes classifiers , in addition to the performance of the state-of-the-art methods reported by previous studies. We observe that the proposed method outperforms other spam detection techniques considered in this study in terms of classification accuracy .},
  archive      = {J_ASOC},
  author       = {Bilge Kagan Dedeturk and Bahriye Akay},
  doi          = {10.1016/j.asoc.2020.106229},
  journal      = {Applied Soft Computing},
  pages        = {106229},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Spam filtering using a logistic regression model trained by an artificial bee colony algorithm},
  volume       = {91},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A fuzzy rough number-based AHP-TOPSIS for design concept
evaluation under uncertain environments. <em>ASOC</em>, <em>91</em>,
106228. (<a href="https://doi.org/10.1016/j.asoc.2020.106228">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Design concept evaluation in the early phase of product design plays a crucial role in new product development as it considerably determines the direction of subsequent design activities. However, it is a process involving uncertainty and subjectivity. The evaluation information mainly relies on expert’s subjective judgment, which is imprecise and uncertain. How to effectively and objectively evaluate the design concept under such subjective and uncertain environments remains an open question. To fill this gap, this paper proposes a fuzzy rough number-enhanced group decision-making framework for design concept evaluation by integrating a fuzzy rough number-based AHP (analytic hierarchy process) and a fuzzy rough number-based TOPSIS (technique for order preference by similarity to ideal solution). First of all, a fuzzy rough number is presented to aggregate personal risk assessment information and to manipulate the uncertainty and subjectivity during the decision-making. Then a fuzzy rough number-based AHP is developed to determine the criteria weights. A fuzzy rough number-based TOPSIS is proposed to conduct the alternative ranking. A practical case study is put forward to illustrate the applicability of the proposed decision-making framework. Experimental results and comparative studies demonstrate the superiority of the fuzzy rough number-based method in dealing with the uncertainty and subjectivity in design concept evaluation under group decision-making environment.},
  archive      = {J_ASOC},
  author       = {Guo-Niu Zhu and Jie Hu and Hongliang Ren},
  doi          = {10.1016/j.asoc.2020.106228},
  journal      = {Applied Soft Computing},
  pages        = {106228},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A fuzzy rough number-based AHP-TOPSIS for design concept evaluation under uncertain environments},
  volume       = {91},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Extracting combinatorial test parameters and their values
using model checking and evolutionary algorithms. <em>ASOC</em>,
<em>91</em>, 106219. (<a
href="https://doi.org/10.1016/j.asoc.2020.106219">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Combinatorial Testing (CT) is one of the popular testing approaches for generating a minimum test suite to detect defects caused by interactions between subsystems. One of the most practical CT methods is the Covering Array (CA). While generating CA, there are at least two main groups of challenges. The first one is extracting information about parameters, identifying constraints and detecting interactions between subsystems automatically. In most of the existing approaches, this information is fed to the system manually which makes it difficult or even impossible for testing modern software systems. The second one is the speed and the array size. Even though most of the existing approaches are concentrated on this challenge, their results show that there is still room for improvement. In this paper, we propose an idea to cope with both challenges. At first, we represent a method to extract information about the system under test (SUT) from its model using model checking (MC) techniques. MC is a method that scans all possible states of the system for detecting errors. After that, we propose another new approach using genetic algorithm to generate the optimal CA in terms of speed and size. To evaluate the results, we implemented the proposed strategy along with several other metaheuristic algorithms in the GROOVE tool, an open toolset for designing and model checking graph transformation specifications. The results represent that the proposed strategy performs better than others.},
  archive      = {J_ASOC},
  author       = {Sajad Esfandyari and Vahid Rafe},
  doi          = {10.1016/j.asoc.2020.106219},
  journal      = {Applied Soft Computing},
  pages        = {106219},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Extracting combinatorial test parameters and their values using model checking and evolutionary algorithms},
  volume       = {91},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). An efficient long short-term memory model based on laplacian
eigenmap in artificial neural networks. <em>ASOC</em>, <em>91</em>,
106218. (<a href="https://doi.org/10.1016/j.asoc.2020.106218">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A new algorithm for data prediction based on the Laplacian Eigenmap (LE) is presented. We construct the Long Short-Term Memory model with the application of the LE in artificial neural networks . The new Long Short-Term Memory model based on Laplacian Eigenmap (LE-LSTM) reserves the characteristics of original data using the eigenvectors derived from the Laplacian matrix of the data matrix. LE-LSTM introduces the projection layer embedding data into a lower dimension space so that it improves the efficiency. With the implementation of LE, LE-LSTM provides higher accuracy and less running time on various simulated data sets with characteristics of multivariate, sequential, and time-series. In comparison with previously reported algorithms such as stochastic gradient descent and artificial neural network with three layers, LE-LSTM leads to many more successful runs and learns much faster. The algorithm provides a computationally efficient approach to most of the artificial neural network data sets.},
  archive      = {J_ASOC},
  author       = {Fang Hu and Yanhui Zhu and Jia Liu and Liuhuan Li},
  doi          = {10.1016/j.asoc.2020.106218},
  journal      = {Applied Soft Computing},
  pages        = {106218},
  shortjournal = {Appl. Soft. Comput.},
  title        = {An efficient long short-term memory model based on laplacian eigenmap in artificial neural networks},
  volume       = {91},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Particle filter and levy flight-based decomposed
multi-objective evolution hybridized particle swarm for flexible job
shop greening scheduling with crane transportation. <em>ASOC</em>,
<em>91</em>, 106217. (<a
href="https://doi.org/10.1016/j.asoc.2020.106217">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Since greening scheduling is arousing increasing attention from many manufacturing enterprises, this paper focuses on a flexible job shop greening scheduling problem with crane transportation (FJSGSP-CT). Distinguished from the traditional scheduling model which merely concentrates on machining processes, FJSGSP-CT takes the comprehensive effect of machining and crane transportation processes into consideration. Due to the NP-hard nature of the problem, an efficient hybrid algorithm, particle filter and Levy flight-based decomposed multi-objective evolution hybridized with particle swarm (PLMEAPS), is developed to find feasible solutions. The proposed PLMEAPS benefits from the synergy of decomposed multi-objective evolutionary algorithm (MOEA/D) and particle swarm optimization (PSO). Particle filter and Levy flights are then creatively fused into the framework of PLMEAPS to enhance the computational performance of the algorithm. The introduction of particle filter enriches the diversity of the population and makes it possible to predict the near optimal solutions at each iteration, and the combination of Levy flights has beneficial effect on escaping from local optimum and accelerating convergence speed. The performance of the proposed PLMEAPS is evaluated by comparing with two other high-performing intelligent optimization algorithms , the multi-objective genetic local search (MOGLS) and the multi-objective grey wolf optimizer (MOGWO). The computational results reveal that the proposed PLMEAPS outperforms the other two algorithms both in solutions’ quality and convergence rate when solving FJSGSP-CT.},
  archive      = {J_ASOC},
  author       = {Binghai Zhou and Xiumei Liao},
  doi          = {10.1016/j.asoc.2020.106217},
  journal      = {Applied Soft Computing},
  pages        = {106217},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Particle filter and levy flight-based decomposed multi-objective evolution hybridized particle swarm for flexible job shop greening scheduling with crane transportation},
  volume       = {91},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Autonomic performance prediction framework for data
warehouse queries using lazy learning approach. <em>ASOC</em>,
<em>91</em>, 106216. (<a
href="https://doi.org/10.1016/j.asoc.2020.106216">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Information is one of the most important assets of an organization. In recent years, the volume of data stored in organizations, varying user requirements, time constraints, and query management complexities have grown exponentially. Due to these problems, the performance modeling of queries in data warehouses (DWs) has assumed a key role in organizations. DWs make relevant information available to decision-makers; however, DW administration is becoming increasingly difficult and time-consuming. DW administrators spend too much time managing queries, which also affects data warehouse performance. To enhance the performance of overloaded data warehouses with varying queries, a prediction-based framework is required that forecasts the behavior of query performance metrics in a DW. In this study, we propose a cluster-based autonomic performance prediction framework using a case-based reasoning approach that determines the performance metrics of the data warehouse in advance by incorporating autonomic computing characteristics. This prediction is helpful for query monitoring and management. For evaluation, we used metrics for precision, recall, accuracy, and relative error rate. The proposed approach is also compared with existing lazy learning techniques. We used the standard TPC-H dataset. Experiments show that our proposed approach produce better results compared to existing techniques.},
  archive      = {J_ASOC},
  author       = {Basit Raza and Adeel Aslam and Asma Sher and Ahmad Kamran Malik and Muhammad Faheem},
  doi          = {10.1016/j.asoc.2020.106216},
  journal      = {Applied Soft Computing},
  pages        = {106216},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Autonomic performance prediction framework for data warehouse queries using lazy learning approach},
  volume       = {91},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). DSTARS: A multi-target deep structure for tracking
asynchronous regressor stacking. <em>ASOC</em>, <em>91</em>, 106215. (<a
href="https://doi.org/10.1016/j.asoc.2020.106215">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Several applications of supervised learning involve the prediction of multiple continuous target variables from a dataset. When the target variables exhibit statistical dependencies among them, a multi-target regression (MTR) modelling permits to improve the predictive performance in comparison to induce a separate model for each target. Apart from describing the dependencies among the targets, the MTR methods could offer better performance and less overfitting than traditional single-target (ST) methods. A group of MTR methods have addressed this demand, but there are still many possibilities for further improvements. This paper presents a novel MTR method called Deep Structure for Tracking Asynchronous Regressor Stacking (DSTARS), which overcomes some existing gaps in the current solutions. DSTARS extends the Stacked Single-Target (SST) approach by combining multiple stacked regressors into a deep structure. In this sense, it is able to boost the predictive performance by successively improving the predictions for the targets. Besides, DSTARS exploits the dependency of each target individually by tracking an asynchronous number of stacked regressors. Additionally, our proposal explores the inter-targets dependencies by exposing and measuring them through a nonlinear metric of variable importance. We compared DSTARS to SST, Ensemble of Regressor Chains (ERC) and Multi-objective Random Forest (MORF). Also, the ST strategy with different algorithms was used to compute independent regressions for each target. We used Random Forest (RF) and Support Vector Machine (SVM) as base-learners to investigate the prediction capability of algorithms belonging to different machine learning paradigms. The experiments carried out on eighteen diverse datasets showed that the proposed method was significantly better than the other compared approaches.},
  archive      = {J_ASOC},
  author       = {Saulo Martiello Mastelini and Everton Jose Santana and Ricardo Cerri and Sylvio Barbon Jr.},
  doi          = {10.1016/j.asoc.2020.106215},
  journal      = {Applied Soft Computing},
  pages        = {106215},
  shortjournal = {Appl. Soft. Comput.},
  title        = {DSTARS: A multi-target deep structure for tracking asynchronous regressor stacking},
  volume       = {91},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A minimum centre distance rule activation method for
extended belief rule-based classification systems. <em>ASOC</em>,
<em>91</em>, 106214. (<a
href="https://doi.org/10.1016/j.asoc.2020.106214">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Originating from the belief-rule-based (BRB) system, the extended belief rule-based (EBRB) system combined the advantages of the rule-based method and those of data-driven methods. By transforming the data set into extended belief rules and using evidential reasoning (ER), the EBRB system has expanded the application of BRB systems and demonstrated their capability in addressing classification problems. Nevertheless, the problem of activating nearly the entire rule base in every classification process is embedded in the EBRB scheme. There have been advances in rule activation for the EBRB system; however, the introduction of subjective information into the classification, high computational costs and long response times are common problems facing existing rule activation methods. To solve the problems facing rule activation for EBRB systems, a minimum centre distance rule activation (MCDRA) method for EBRB systems is proposed. In MCDRA, no subjective information is required, and no time-consuming iteration procedure is necessary. Two components of the proposed MCDRA, i.e., the filtering procedure and the selection procedure, are designed to eliminate unrelated samples of input query data and to select and activate the highly related samples to the input query data. A total of 12 benchmark data sets are used to test the performance of EBRB with MCDRA (M-EBRB). The experimental results show that compared with other rule activation methods, the proposed method obtains satisfactory rule activation ratios, accuracies and response times. Additionally, M-EBRB performs well on noisy data and comparatively with both the fuzzy-rule-based classification system (FRBCS) and several machine learning classification algorithms . In addition, MCDRA can be utilized as a generic rule activation method and can be used to optimize other rule-based classification systems.},
  archive      = {J_ASOC},
  author       = {Haizhen Zhu and Mingqing Xiao and Longhao Yang and Xilang Tang and Yajun Liang and Jianfeng Li},
  doi          = {10.1016/j.asoc.2020.106214},
  journal      = {Applied Soft Computing},
  pages        = {106214},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A minimum centre distance rule activation method for extended belief rule-based classification systems},
  volume       = {91},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Numerical sensitive data recognition based on hybrid gene
expression programming for active distribution networks. <em>ASOC</em>,
<em>91</em>, 106213. (<a
href="https://doi.org/10.1016/j.asoc.2020.106213">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Complex and flexible access mode, and frequent data interaction bring about large security risks to data transmission for active distribution networks . How to ensure data security is critical to the safe and stable operation of active distribution networks . Traditional methods, like access control, data encryption , and text filtering based on intelligent algorithms, are difficult to ensure the security of dynamically increased and high-dimensional numerical data transmission in active distribution networks. In this paper, we first propose a rough feature selection algorithm based on the average importance measurement (RFS-AIM) to simplify the complexity of data recognition. Then, we propose a sensitive data recognition function mining algorithm based on RFS-AIM and improved gene expression programming (SDR-IGEP) where population update operation is constructed by chromosome similarity based on the Jaccard coefficient . The operation avoids local convergence of the gene express programming by increasing individual diversity in the new population. Finally, we present a new incremental mining algorithm for a sensitive data recognition function based on global function fitting (ISDR-GFF) by using a grain granulation model for incremental datasets. The experimental results on IEEE benchmark datasets and real datasets show that the algorithms proposed in this paper outperform the state-of-the-art algorithms in terms of the average running time, precision, recall, F 1 F1 index, accuracy, specificity and speedup on all experimental datasets .},
  archive      = {J_ASOC},
  author       = {Song Deng and Xiangpeng Xie and Changan Yuan and Lechan Yang and Xindong Wu},
  doi          = {10.1016/j.asoc.2020.106213},
  journal      = {Applied Soft Computing},
  pages        = {106213},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Numerical sensitive data recognition based on hybrid gene expression programming for active distribution networks},
  volume       = {91},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A decision-theoretic rough set model with q-rung orthopair
fuzzy information and its application in stock investment evaluation.
<em>ASOC</em>, <em>91</em>, 106212. (<a
href="https://doi.org/10.1016/j.asoc.2020.106212">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Stock investment is characterized by high risk and massive profit, so it is necessary to propose a scientific and accurate stock assessment and selection method for avoiding investment risks and obtaining high returns. Stock investment evaluation and selection can be regarded as a three-way decision (3WD) problem. Decision-theoretic rough sets (DTRSs) are an excellent tool to cope with 3WDs under risks and uncertainty. Due to the increasing complexity and high uncertainty of decision environments, the loss functions involved in DTRSs are not always expressed with real numbers. As a novel generalized form of Pythagorean fuzzy sets (PFSs) and intuitionistic fuzzy sets (IFSs), q -rung orthopair fuzzy sets ( q -ROFSs) depict uncertain information more widely and flexibly. Thus, it is a significant innovation to combine q -ROFSs with DTRSs and construct a new 3WD model for stock investment evaluation. More specifically, we first extend q -rung orthopair fuzzy numbers ( q -ROFNs) to DTRSs, which can offer a novel illustration for loss functions. Then, we establish a novel q -rung orthopair fuzzy DTRS ( q -ROFDTRS) model and explore some fundamental properties of the expected losses. Additionally, we propose two methods to handle q -ROFNs and obtain 3WDs. These two methods are compared, and their characteristics and applicability are analysed. Finally, a practical case concerning stock investment evaluation is supplied to illustrate the effectiveness and the superiority of the developed approaches over existing methods.},
  archive      = {J_ASOC},
  author       = {Guolin Tang and Francisco Chiclana and Peide Liu},
  doi          = {10.1016/j.asoc.2020.106212},
  journal      = {Applied Soft Computing},
  pages        = {106212},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A decision-theoretic rough set model with q-rung orthopair fuzzy information and its application in stock investment evaluation},
  volume       = {91},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Cleaning decision model of MBR membrane based on bandelet
neural network optimized by improved bat algorithm. <em>ASOC</em>,
<em>91</em>, 106211. (<a
href="https://doi.org/10.1016/j.asoc.2020.106211">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The membrane fouling is an important factor of restricting wide application of MBR (Membrane Bio-Reactor), which causes the fall of membrane flux and reduces the membrane cleaning period. So the Bandelet neural network is proposed through combining Bandelet transform and neural network, which predicts membrane flux and its recovery rate for making proper membrane cleaning decision. Firstly, the main affecting factors of membrane fouling are discussed. Secondly, the architecture of Bandelet neural network is designed with Bandelet function and its scale function as activation functions of hidden and output layers respectively. Thirdly, the improved Bat algorithm is established, which is applied to improve the optimization effect of parameters of Bandelet neural network. Finally, the simulation analysis is carried out, the improved bat algorithm has higher performance than the traditional bat algorithm through analyzing the single objective optimization problem from 2018 CEC competition, the optimal number of nodes in hidden layer is confirmed based on comparison analysis and statistical tests. The proposed BNN-IBA has obvious superiority in prediction accuracy and speed according to prediction simulation results of membrane fouling of MBR, which has better prediction results than other state-of-art prediction models optimized by the novel optimal algorithms. In addition, the proper membrane cleaning period and method are confirmed according to the prediction results of membrane flux and its recovery rate.},
  archive      = {J_ASOC},
  author       = {Bin Zhao and Hao Chen and Diankui Gao and Lizhi Xu and Yuanyuan Zhang},
  doi          = {10.1016/j.asoc.2020.106211},
  journal      = {Applied Soft Computing},
  pages        = {106211},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Cleaning decision model of MBR membrane based on bandelet neural network optimized by improved bat algorithm},
  volume       = {91},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Learning the retinal anatomy from scarce annotated data
using self-supervised multimodal reconstruction. <em>ASOC</em>,
<em>91</em>, 106210. (<a
href="https://doi.org/10.1016/j.asoc.2020.106210">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning is becoming the reference paradigm for approaching many computer vision problems. Nevertheless, the training of deep neural networks typically requires a significantly large amount of annotated data, which is not always available. A proven approach to alleviate the scarcity of annotated data is transfer learning . However, in practice, the use of this technique typically relies on the availability of additional annotations, either from the same or natural domain. We propose a novel alternative that allows to apply transfer learning from unlabelled data of the same domain, which consists in the use of a multimodal reconstruction task. A neural network trained to generate one image modality from another must learn relevant patterns from the images to successfully solve the task. These learned patterns can then be used to solve additional tasks in the same domain, reducing the necessity of a large amount of annotated data. In this work, we apply the described idea to the localization and segmentation of the most important anatomical structures of the eye fundus in retinography. The objective is to reduce the amount of annotated data that is required to solve the different tasks using deep neural networks. For that purpose, a neural network is pre-trained using the self-supervised multimodal reconstruction of fluorescein angiography from retinography. Then, the network is fine-tuned on the different target tasks performed on the retinography. The obtained results demonstrate that the proposed self-supervised transfer learning strategy leads to state-of-the-art performance in all the studied tasks with a significant reduction of the required annotations.},
  archive      = {J_ASOC},
  author       = {Álvaro S. Hervella and José Rouco and Jorge Novo and Marcos Ortega},
  doi          = {10.1016/j.asoc.2020.106210},
  journal      = {Applied Soft Computing},
  pages        = {106210},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Learning the retinal anatomy from scarce annotated data using self-supervised multimodal reconstruction},
  volume       = {91},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Mixed spatial pyramid pooling for semantic segmentation.
<em>ASOC</em>, <em>91</em>, 106209. (<a
href="https://doi.org/10.1016/j.asoc.2020.106209">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Semantic segmentation is a challenging task as each pixel should be labeled accurately in the image. To improve the performance of semantic segmentation, some Fully Convolutional Network (FCN) based semantic segmentation methods adopt a spatial pyramid pooling structure to enrich contextual information. Others employ an encoder–decoder architecture to recover object details gradually. In this paper, we propose a semantic segmentation framework which combines the benefits of these approaches. Specifically, we propose a Mixed Spatial Pyramid Pooling (MSPP) module based on region-based average pooling and dilated convolution to obtain dense multi-level contextual priors. To further refine the details of objects more effectively, we also propose a Global-Attention Fusion (GAF) module to provide global context as guidance for low-level features. Our proposed method achieves mIoU of 84.1\% on PASCAL VOC 2012 dataset and 80.4\% on Cityscapes dataset without using any post-processing or additional datasets for pretrained model.},
  archive      = {J_ASOC},
  author       = {Zhengyu Xia and Joohee Kim},
  doi          = {10.1016/j.asoc.2020.106209},
  journal      = {Applied Soft Computing},
  pages        = {106209},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Mixed spatial pyramid pooling for semantic segmentation},
  volume       = {91},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Dynamic scheduling for flexible job shop with new job
insertions by deep reinforcement learning. <em>ASOC</em>, <em>91</em>,
106208. (<a href="https://doi.org/10.1016/j.asoc.2020.106208">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In modern manufacturing industry, dynamic scheduling methods are urgently needed with the sharp increase of uncertainty and complexity in production process. To this end, this paper addresses the dynamic flexible job shop scheduling problem (DFJSP) under new job insertions aiming at minimizing the total tardiness. Without lose of generality, the DFJSP can be modeled as a Markov decision process (MDP) where an intelligent agent should successively determine which operation to process next and which machine to assign it on according to the production status of current decision point, making it particularly feasible to be solved by reinforcement learning (RL) methods. In order to cope with continuous production states and learn the most suitable action (i.e. dispatching rule) at each rescheduling point, a deep Q-network (DQN) is developed to address this problem. Six composite dispatching rules are proposed to simultaneously select an operation and assign it on a feasible machine every time an operation is completed or a new job arrives. Seven generic state features are extracted to represent the production status at a rescheduling point. By taking the continuous state features as input to the DQN, the state–action value (Q-value) of each dispatching rule can be obtained. The proposed DQN is trained using deep Q-learning (DQL) enhanced by two improvements namely double DQN and soft target weight update. Moreover, a “softmax” action selection policy is utilized in real implementation of the trained DQN so as to promote the rules with higher Q-values while maintaining the policy entropy. Numerical experiments are conducted on a large number of instances with different production configurations. The results have confirmed both the superiority and generality of DQN compared to each composite rule, other well-known dispatching rules as well as the stand Q-learning-based agent.},
  archive      = {J_ASOC},
  author       = {Shu Luo},
  doi          = {10.1016/j.asoc.2020.106208},
  journal      = {Applied Soft Computing},
  pages        = {106208},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Dynamic scheduling for flexible job shop with new job insertions by deep reinforcement learning},
  volume       = {91},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A novel hybrid MCDM model for machine tool selection using
fuzzy DEMATEL, entropy weighting and later defuzzification VIKOR.
<em>ASOC</em>, <em>91</em>, 106207. (<a
href="https://doi.org/10.1016/j.asoc.2020.106207">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Machine tool selection has been an important issue in the manufacturing industry because improper machine tool selection can have a negative effect on productivity, accuracy, flexibility, and the responsive manufacturing capabilities of a company. The current multi-criteria decision making (MCDM) approach of machine tool selection mostly focuses on the subjective perspective. However, as the objective evaluation represents the actual performance of machine tools, both subjective and objective perspectives need to be considered when choosing an appropriate machining tool. Therefore, this study proposes a machine tool selection method based on a novel hybrid MCDM model. Firstly, the presented method employs a comprehensive weight technique integrating subjective weights obtained using fuzzy decision-making trial and evaluation laboratory (FDEMATEL) with objective weights obtained using entropy weighting (EW). Secondly, later defuzzification VIKOR (LDVIKOR) is put forward to rank the optional alternatives. Finally, a case application verifies the effectiveness of the proposed method. The evaluation results indicate that the best and worst selected machine tool of the proposed method keeps high conformance with the actual ranking in real factory. Additionally, sensitivity analysis results of the effect of parameters φ φ on the decision outcome show that irrespective of the variations in this parameter, the best decision outcome will be not influenced. These indicate that the presented hybrid model has advantages in granting flexibility to the preferences of decision makers .},
  archive      = {J_ASOC},
  author       = {Hai Li and Wei Wang and Lei Fan and Qingzhao Li and Xuezhen Chen},
  doi          = {10.1016/j.asoc.2020.106207},
  journal      = {Applied Soft Computing},
  pages        = {106207},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A novel hybrid MCDM model for machine tool selection using fuzzy DEMATEL, entropy weighting and later defuzzification VIKOR},
  volume       = {91},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Iris anti-spoofing through score-level fusion of handcrafted
and data-driven features. <em>ASOC</em>, <em>91</em>, 106206. (<a
href="https://doi.org/10.1016/j.asoc.2020.106206">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the past two decads, iris spoofing detection has occupied an ample space in the literature of iris biometics. The textured lens may be used to spoof the Iris Recognition (IR) system by exploiting its external texture. Besides, the soft lens may cause an upsurge in the false rejection rate as it blurs the iris texture. Therefore, it is foremost to identify contact lens in human eyes before accessing an IR system. This paper proposes a novel fusion-based approach to discriminate live iris from contact lens images that combines handcrafted and data-driven features. It also demonstrates a Densely-connected Contact-lens Classification Network (DCCNet) as a data-driven model that is basically a customized Densenet121 framework. The DCCNet features are -pooled with handcrafted counterparts to create a combined feature set. However, the optimal features are identified by top-k feature selection using the Friedman test and are fused through score-level fusion. The assessment of the proposed approach includes several experiments simulated on three iris databases, i . e . Notre Dame (ND) Contact Lens 2013, IIIT-Delhi Contact Lens (IIITD), and Clarkson Databases. The equal error rate (EER) and the detection error tradeoff (DET) curve are used as performance metrics. Further, the statistical analysis is performed using Nemenyi and Bonferroni-Dunn tests, where the proposed approach significantly improves the state of the arts.},
  archive      = {J_ASOC},
  author       = {Meenakshi Choudhary and Vivek Tiwari and Venkanna U.},
  doi          = {10.1016/j.asoc.2020.106206},
  journal      = {Applied Soft Computing},
  pages        = {106206},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Iris anti-spoofing through score-level fusion of handcrafted and data-driven features},
  volume       = {91},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). An integrated framework of deep learning and knowledge graph
for prediction of stock price trend: An application in chinese stock
exchange market. <em>ASOC</em>, <em>91</em>, 106205. (<a
href="https://doi.org/10.1016/j.asoc.2020.106205">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many studies have been carried out on stock price trend prediction, but most of them focused on the public market data and did not utilize the trading behaviors owing to the unavailability of real transaction records data. In fact, trading behaviors can better reflect the market movements, and the fusion of trading information and market information can further improve the prediction accuracy. In this paper, we propose a deep neural network model using the desensitized transaction records and public market information to predict stock price trend. Considering the correlation between stocks, our method utilizes the knowledge graph and graph embeddings techniques to select the relevant stocks of the target for constructing the market and trading information. Given the considerable number of investors and the complexity of transaction records data, the investors are clustered to reduce the dimensions of the trading feature matrices, and then the matrices are fed into the convolutional neural network to unearth the investment patterns. Eventually, the attention-based bidirectional long short-term memory network can predict the stock price trends for financial decision support. The experiments on the price movement direction and trend prediction show that our method achieves the best performance in comparison with other prediction baselines.},
  archive      = {J_ASOC},
  author       = {Jiawei Long and Zhaopeng Chen and Weibing He and Taiyu Wu and Jiangtao Ren},
  doi          = {10.1016/j.asoc.2020.106205},
  journal      = {Applied Soft Computing},
  pages        = {106205},
  shortjournal = {Appl. Soft. Comput.},
  title        = {An integrated framework of deep learning and knowledge graph for prediction of stock price trend: An application in chinese stock exchange market},
  volume       = {91},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A multi-objective methodology for multi-criteria engineering
design. <em>ASOC</em>, <em>91</em>, 106204. (<a
href="https://doi.org/10.1016/j.asoc.2020.106204">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Optimization is more and more significant due to its application in the real engineering problems. The recently proposed imperialist competitive algorithm (ICA) is a successful method in mono-objective optimization. Nevertheless, ICA cannot handle simultaneously the conflicting objectives in multi-objective design problem. In addition, the ICA has the drawback of trapping in local optimum solutions when used for high-dimensional or complex multimodal functions . In order to deal with these situations, in this work, an improved ICA, named modified multi-objective imperialist competitive algorithm (MOMICA) is proposed. In MOMICA, an attraction and repulsion (AR) concept is implemented in the assimilation phase to improve the performances of the algorithm to reach the global optimal position. Moreover, in contrast to ICA, the proposed algorithm integrates the sorting non-dominated strategy (SND) to store the Pareto optimal solutions of multiple conflicting functions. Three performance metrics are used to evaluate the performance of the proposed algorithm: (a) convergence to the true Pareto-optimal set, (b) solutions diversity and (c) robustness, characterized by the variance over 10 runs. The results presented in this paper show that the MOMICA algorithm outperforms the other popular techniques in terms of convergence characteristics and global search ability, for both benchmark functions optimization and multi-objective engineering optimization problems .},
  archive      = {J_ASOC},
  author       = {Nejlaoui Mohamed and Najlawi Bilel and Ali Sulaiman Alsagri},
  doi          = {10.1016/j.asoc.2020.106204},
  journal      = {Applied Soft Computing},
  pages        = {106204},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A multi-objective methodology for multi-criteria engineering design},
  volume       = {91},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Co-reconfiguration of product family and supply chain using
leader–follower stackelberg game theory: Bi-level multi-objective
optimization. <em>ASOC</em>, <em>91</em>, 106203. (<a
href="https://doi.org/10.1016/j.asoc.2020.106203">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article deals with product Family : a group of products with similar modules produced based on assembling to order (ATO) approach to cover diverse customer needs. The demand level and the customer requirements of these products are dynamically changing, which necessitate a novel model for co-reconfiguration of the product family (PF) and the supply chain (SC). Therefore, this article aims to apply Leader–follower Stackelberg Game Theory in order to present a co-reconfiguration of PF and SC based on three objectives: maximizing the total profit, maximizing customer utility and minimizing the supply chain cost in a bi-level structure. Maximizing the total profit and maximizing customer utility are the two objectives of the Leader problem for product family reconfiguration, which results in the optimal selection of components, modules, and product variants. The upper-level problem is considered as a multi-objective problem with the aforementioned two objectives. The lower level of this problem intends to reconfigure the supply chain with the objective of minimizing the supply chain costs, and therefore, to reach the optimal selection of suppliers, manufacturers, assembly plant, distribution centers, and retailers. A bi-level multi-objective linear programming problem (B-MOLP) is used to model the game of the leader–follower. A new particle swarm optimization algorithm, called bi-level multi-objective PSO (B-MOPSO), is developed to solve the proposed bi-level multi-objective model. To show the validity of the proposed model and the efficiency of our algorithm, a case study at a mountain bike industry is investigated. Finally, results in some managerial implications are obtained through sensitivity analysis.},
  archive      = {J_ASOC},
  author       = {Milad Pakseresht and Iraj Mahdavi and Babak Shirazi and Nezam Mahdavi-Amiri},
  doi          = {10.1016/j.asoc.2020.106203},
  journal      = {Applied Soft Computing},
  pages        = {106203},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Co-reconfiguration of product family and supply chain using leader–follower stackelberg game theory: Bi-level multi-objective optimization},
  volume       = {91},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A hybrid algorithm for task scheduling on heterogeneous
multiprocessor embedded systems. <em>ASOC</em>, <em>91</em>, 106202. (<a
href="https://doi.org/10.1016/j.asoc.2020.106202">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most of the scheduling algorithms proposed for real-time embedded systems , with energy constraints, try to reduce power consumption . However, reducing the power consumption may decrease the computation speed and impact the makespan. Therefore, for real-time embedded systems , makespan and power consumption need to be considered simultaneously. Since task scheduling is an NP-hard problem, most of the proposed scheduling algorithms are not able to find the multi-objective optimal solution. In this paper, we propose a two-phase hybrid task scheduling algorithm based on decomposition of the input task graph, by applying spectral partitioning. The proposed algorithm, called G-SP, assigns each part of the task graph to a low power processor in order to minimize power consumption. Through experiments, we compare the makespan and power consumption of the G-SP against well-known algorithms of this area for a large set of randomly generated and real-world task graphs with different characteristics. The obtained results show that the G-SP outperforms other algorithms in both metrics, under various conditions, involving different numbers of processors and considering several system configurations.},
  archive      = {J_ASOC},
  author       = {Golnaz Taheri and Ahmad Khonsari and Reza Entezari-Maleki and Leonel Sousa},
  doi          = {10.1016/j.asoc.2020.106202},
  journal      = {Applied Soft Computing},
  pages        = {106202},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A hybrid algorithm for task scheduling on heterogeneous multiprocessor embedded systems},
  volume       = {91},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A soft computing approach for group decision making: A
supply chain management application. <em>ASOC</em>, <em>91</em>, 106201.
(<a href="https://doi.org/10.1016/j.asoc.2020.106201">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a novel Soft Computing Approach called “Neuro-Fuzzy Analytical Network Process (NFANP)” for the group decision-making problems based on the conventional Analytic Network Process (ANP) method. The proposed approach deals with the interval values of judgments in a fuzzy environment using mobile, not fixed, trapezoidal and triangular membership functions, as well as the interval numerical ratio defined by alpha-cuts and the decision maker’s confidence levels. The consistency problem of the fuzzy reciprocal matrices is addressed in the proposed paper by allowing a certain tolerance deviation to be less than 0.20. Furthermore, trained Artificial Neural Networks (ANNs) are included in the proposed approach to reduce the large number of computations of the arithmetic operations required to correlate decision factors with the alternatives. In the proposed implementation, the selection problem is defined into three main decision groups: Supplier Characteristics, On-Going Performance, and Project Management Capabilities. The supplier alternatives are classified by the decision makers corresponding to company size, quality system implementation, and cost management. The application of the proposed approach shows a great accuracy in the final utility values and a significant reduction in the calculation requirements.},
  archive      = {J_ASOC},
  author       = {Diego A. Carrera and Rene V. Mayorga and Wei Peng},
  doi          = {10.1016/j.asoc.2020.106201},
  journal      = {Applied Soft Computing},
  pages        = {106201},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A soft computing approach for group decision making: A supply chain management application},
  volume       = {91},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Local segmentation of images using an improved fuzzy c-means
clustering algorithm based on self-adaptive dictionary learning.
<em>ASOC</em>, <em>91</em>, 106200. (<a
href="https://doi.org/10.1016/j.asoc.2020.106200">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image segmentation is an active research topic in image processing . The Fuzzy C-means (FCM) clustering analysis has been widely used in image segmentation. As there is a large amount of delicate tissues such as blood vessels and nerves in medical images, noise generated during imaging process can easily affect successful segmentation of these tissues. The traditional FCM algorithm is not ideal for segmentation of images containing strong noise. In this study, we proposed an improved FCM algorithm with anti-noise capability. We first discussed the algorithm of dictionary learning for noise reduction. Then we developed a new image segmentation algorithm as a combination of the dictionary learning for noise reduction and the improved fuzzy C-means clustering. Lastly we used the algorithm of the improved FCM to segment images, during which we removed the non-target areas making use of the grayscale features of images and extracted accurately the areas of interests. The algorithm was tested using synthetic Shepp-Logan images and real medical magnetic resonance imaging (MRI) and computed tomography (CT) images. Compared to the synthetic data and real medical images segmented by the fuzzy C-means (FCM) clustering algorithm, the Kernel Fuzzy C-mean (KFCM) clustering algorithm, spectral clustering algorithm, the sparse learning based fuzzy C-means (SL_FCM) clustering algorithm, and the modified spatial KFCM (MSFCM) algorithm, the images segmented by the dictionary learning Fuzzy C-mean clustering (DLFCM) algorithm have higher partition coefficient, lower partition entropy, better visual perception, better clustering accuracy, and clustering purity.},
  archive      = {J_ASOC},
  author       = {Jiaqing Miao and Xiaobing Zhou and Ting-Zhu Huang},
  doi          = {10.1016/j.asoc.2020.106200},
  journal      = {Applied Soft Computing},
  pages        = {106200},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Local segmentation of images using an improved fuzzy C-means clustering algorithm based on self-adaptive dictionary learning},
  volume       = {91},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A universal strengthened searching module for
multi-objective optimization based on variable properties.
<em>ASOC</em>, <em>91</em>, 106199. (<a
href="https://doi.org/10.1016/j.asoc.2020.106199">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Numerous algorithms have been introduced in solving multi-/many-objective optimization problems . However, some rigorous elite definitions may result in the side-effect that new solutions are hard to be admitted by the elite archive, which will restrain the searching ability. In this paper, a universal strengthened searching (SS) module is proposed as an accessorial searching procedure to improve the performance of existing algorithms. By concentrating part of the computational resources, this strategy can enhance the convergence and diversity searching strengths in separate areas. These areas are determined by classifying decision variables according to a dynamic spread-based procedure. Moreover, incorporating SS with existing algorithms can improve the overall performance of original algorithms while persisting their inherent advantages. In this paper, the structure of an SS-embedded algorithm is illustrated and the comparison experiments have been established among several couples of algorithms. The results demonstrate that the algorithms with the SS module have better overall performance compared to the original algorithms towards different problem properties. Meanwhile, the proposed strategy can accelerate the searching procedure for the time-consuming algorithms.},
  archive      = {J_ASOC},
  author       = {Anqi Pan and Lei Wang and Weian Guo},
  doi          = {10.1016/j.asoc.2020.106199},
  journal      = {Applied Soft Computing},
  pages        = {106199},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A universal strengthened searching module for multi-objective optimization based on variable properties},
  volume       = {91},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Sarcasm detection in mash-up language using soft-attention
based bi-directional LSTM and feature-rich CNN. <em>ASOC</em>,
<em>91</em>, 106198. (<a
href="https://doi.org/10.1016/j.asoc.2020.106198">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Analyzing explicit and clear sentiment is challenging owing to the growing use of emblematic and multilingual language constructs. This research proposes sarcasm detection using deep learning in code-switch tweets, specifically the mash-up of English with Indian native language, Hindi. The proposed model is a hybrid of bidirectional long short-term memory with a softmax attention layer and convolution neural network for real-time sarcasm detection. To evaluate the performance of the proposed model, real-time mash-up tweets are extracted on the trending political (#government) and entertainment (#cricket, #bollywood) posts on Twitter. The randomly sampled dataset contains 3000 sarcastic and 3000 non-sarcastic bilingual Hinglish (Hindi + + English) tweets. Feature engineering is done using pre-trained GloVe word embeddings to extract English semantic context vector, hand-crafted features using subjective lexicon Hindi-SentiWordNet to generate the SentiHindi feature vector and an auxiliary pragmatic feature vector depicting the count of pragmatic markers in tweet. Performance analysis is done to compare and validate the proposed softAtt softAtt BiLSTM- feature-rich feature-rich CNN model. The model outperforms the baseline deep learning models with a superior classification accuracy of 92.71\% and F-measure of 89.05\%.},
  archive      = {J_ASOC},
  author       = {Deepak Jain and Akshi Kumar and Geetanjali Garg},
  doi          = {10.1016/j.asoc.2020.106198},
  journal      = {Applied Soft Computing},
  pages        = {106198},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Sarcasm detection in mash-up language using soft-attention based bi-directional LSTM and feature-rich CNN},
  volume       = {91},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Cooperative localization for multiple AUVs based on the
rough estimation of the measurements. <em>ASOC</em>, <em>91</em>,
106197. (<a href="https://doi.org/10.1016/j.asoc.2020.106197">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The accuracy of cooperative localization for multiple autonomous underwater vehicles (AUVs) equipped with low precise proprioceptive localization sensors can be improved by using relative location information between individuals and Bayesian filtering . However, when the relative location measurement errors are high, its accuracy will be reduced. Two measurement for rough estimation algorithms under the constraint environment of the cooperative structure are developed in this paper: the first algorithm is based on the underwater acoustic isotropic transmission. And the second algorithm is based on the common observation environment. In the first algorithm, it builds under the assumption that the distance errors calculated from the simultaneous omnidirectional response signals from the same transmitting source have correlated. Similarly, in the second algorithm, the assumption that “common observation environment” is correlated is made. First, the correlation between the errors is used roughly to estimate the measurement of information. Then, a suitable filter is applied to fuse the rough estimation measurement with dead-reckoning estimation that improves the location estimation accuracy. The final simulation, by changing the AUV formation navigation paths and the sensor observation noises, shows the proposed processing methods have effectiveness and consistency compared to the traditional algorithm.},
  archive      = {J_ASOC},
  author       = {Jian Lu and Xu Chen and Maoxin Luo and Yanran Zhou},
  doi          = {10.1016/j.asoc.2020.106197},
  journal      = {Applied Soft Computing},
  pages        = {106197},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Cooperative localization for multiple AUVs based on the rough estimation of the measurements},
  volume       = {91},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). On the rectangular fuzzy complex linear systems.
<em>ASOC</em>, <em>91</em>, 106196. (<a
href="https://doi.org/10.1016/j.asoc.2020.106196">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we focus on generalized fuzzy complex numbers and provide several examples of membership functions and graphical representations of these numbers. Then, a specific type of fuzzy complex linear systems, called the rectangular fuzzy complex linear system, is considered and its algebraic and general solutions are defined. Finally an approach based on restricting the general solution is presented to solve a rectangular fuzzy complex linear system. It is illustrated that the proposed method gives a unique algebraic solution to a rectangular fuzzy complex linear system if it exists. To show the ability and efficiency of the method and for more illustration, three numerical examples including a real application in electrical engineering are modeled and solved.},
  archive      = {J_ASOC},
  author       = {M. Ghanbari and T. Allahviranloo and W. Pedrycz},
  doi          = {10.1016/j.asoc.2020.106196},
  journal      = {Applied Soft Computing},
  pages        = {106196},
  shortjournal = {Appl. Soft. Comput.},
  title        = {On the rectangular fuzzy complex linear systems},
  volume       = {91},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Multiobjective multi-verse optimization algorithm to solve
combined economic, heat and power emission dispatch problems.
<em>ASOC</em>, <em>91</em>, 106195. (<a
href="https://doi.org/10.1016/j.asoc.2020.106195">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study implements a potent Multiobjective Multi-Verse Optimization algorithm to solve the highly complicated combined economic emission dispatch and combined heat and power economic emission dispatch problems. Solving these problems operates the power system integrated with cogeneration plants economically and reduces the environmental impacts caused by the pollutants of fossil fuel-fired power plants. A chaotic opposition based strategy is proposed to explore the search space extensively and to generate the initial populations for the multiobjective optimization algorithm. An effective constraint handling mechanism is also proposed to enable the population to remain within the bounds and in the feasible operating region of the cogeneration plants . The algorithm is applied to standard test functions, four test systems including a large 140 bus system considering valve-point effects, ramp limits, transmission power losses, and the feasible operating region of cogeneration units . The Pareto Optimal solutions obtained by the algorithm are well spread and diverse when compared with other optimization algorithms. The statistical analysis and various performance metrics used indicate the algorithm converges to true POF and is a viable alternative to solve the highly complicated combined economic emission dispatch and combined heat and power economic emission dispatch problems.},
  archive      = {J_ASOC},
  author       = {Arunachalam Sundaram},
  doi          = {10.1016/j.asoc.2020.106195},
  journal      = {Applied Soft Computing},
  pages        = {106195},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Multiobjective multi-verse optimization algorithm to solve combined economic, heat and power emission dispatch problems},
  volume       = {91},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A novel bilateral impedance controls for underwater
tele-operation systems. <em>ASOC</em>, <em>91</em>, 106194. (<a
href="https://doi.org/10.1016/j.asoc.2020.106194">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Owing to characteristics of the flow and the variability, it is extremely difficult to achieve the stability and the transparency of the underwater tele-operation system. In practice, the accurate force may not easily be acquired due to model uncertainties, the time delay and external disturbances . In order to enhance the stability and the transparency of the underwater tele-operation, an adaptive neural fuzzy inference system disturbance observer-based impedance control is proposed to both the master side and slave side. The learning algorithm of the adaptive neural fuzzy inference system network and the disturbance observer may simultaneously suppress model uncertainties of the nonlinear system and disturbances of external underwater environment. Concerning the time delay , the stability is analyzed by Lyapunov theorem. Numerical simulations are performed and results demonstrate the effective performance of the proposed method.},
  archive      = {J_ASOC},
  author       = {Ting Wang and Yujie Li and Jianjun Zhang and Yuan Zhang},
  doi          = {10.1016/j.asoc.2020.106194},
  journal      = {Applied Soft Computing},
  pages        = {106194},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A novel bilateral impedance controls for underwater tele-operation systems},
  volume       = {91},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Influence of initialization on the performance of
metaheuristic optimizers. <em>ASOC</em>, <em>91</em>, 106193. (<a
href="https://doi.org/10.1016/j.asoc.2020.106193">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {All metaheuristic optimization algorithms require some initialization, and the initialization for such optimizers is usually carried out randomly. However, initialization can have some significant influence on the performance of such algorithms. This paper presents a systematic comparison of 22 different initialization methods on the convergence and accuracy of five optimizers: differential evolution (DE), particle swarm optimization (PSO), cuckoo search (CS), artificial bee colony (ABC) and genetic algorithm (GA). We have used 19 different test functions with different properties and modalities to compare the possible effects of initialization, population sizes and the numbers of iterations. Rigorous statistical ranking tests indicate that 43.37\% of the functions using the DE algorithm show significant differences for different initialization methods , while 73.68\% of the functions using both PSO and CS algorithms are significantly affected by different initialization methods. The simulations show that DE is less sensitive to initialization, while both PSO and CS are more sensitive to initialization. In addition, under the condition of the same maximum number of fitness evaluations (FEs), the population size can also have a strong effect. Particle swarm optimization usually requires a larger population, while the cuckoo search needs only a small population size. Differential evolution depends more heavily on the number of iterations, a relatively small population with more iterations can lead to better results. Furthermore, ABC is more sensitive to initialization, while such initialization has little effect on GA. Some probability distributions such as the beta distribution , exponential distribution and Rayleigh distribution can usually lead to better performance. The implications of this study and further research topics are also discussed in detail.},
  archive      = {J_ASOC},
  author       = {Qian Li and San-Yang Liu and Xin-She Yang},
  doi          = {10.1016/j.asoc.2020.106193},
  journal      = {Applied Soft Computing},
  pages        = {106193},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Influence of initialization on the performance of metaheuristic optimizers},
  volume       = {91},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A hybrid algorithm based optimal placement of DG units for
loss reduction in the distribution system. <em>ASOC</em>, <em>91</em>,
106191. (<a href="https://doi.org/10.1016/j.asoc.2020.106191">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Distributed generation (DG) has been utilized in some electric power networks. Power loss reduction, environmental friendliness, voltage improvement, postponement of system upgrading, and increasing reliability are some advantages of DG-unit application This paper uses a hybrid technique to optimize the position and size of DG units to reduce losses in the distribution system. The hybrid technique is the joined execution of both the Grasshopper Optimization Algorithm (GOA) and Cuckoo Search (CS) technique. Here, the GOA optimization behavior is upgraded by utilizing the CS technique. Here, the perfect position of the DG unit is settled with respect to the power loss, line power flow and voltage profile using the proposed system. For improving the dynamic execution, the limit of DG is directed by the proposed technique with respect to the cost work. The motivation behind the proposed system is to produce optimal capacity to lessen the aggregate power loss and enhance the voltage profiles of power distribution networks . The proposed hybrid technique is executed in MATLAB/Simulink working platform and the dynamic dependability execution is tested and considered with IEEE 33-bus distribution networks and IEEE 69-bus system. The stability by diminishing loss of the distribution system is investigated by executed different load state of the system. The execution of the proposed system is analyzed and compared with different existing techniques.},
  archive      = {J_ASOC},
  author       = {M.C.V. Suresh and J. Belwin Edward},
  doi          = {10.1016/j.asoc.2020.106191},
  journal      = {Applied Soft Computing},
  pages        = {106191},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A hybrid algorithm based optimal placement of DG units for loss reduction in the distribution system},
  volume       = {91},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Developing the seismic fragility analysis with fuzzy random
variables using mouth brooding fish algorithm. <em>ASOC</em>,
<em>91</em>, 106190. (<a
href="https://doi.org/10.1016/j.asoc.2020.106190">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The present work proposes a fuzzy-random fragility assessment framework for evaluating 2D reinforced concrete moment frame buildings in the presence of various sources of uncertainty, most notably aleatory and epistemic. Such uncertainties exert a powerful influence on the nonlinear behavior of structural systems and consequently affect the seismic response of these structures. For this reason, a number of effective techniques including Latin Hypercube Sampling (LHS) simulation, fuzzy set theory , and a well-known α α -cut approach have been used to quantify the median of the collapse fragility curve as the fuzzy-random response. As a major step in the α α -cut approach, metaheuristic evolutionary algorithms including modified genetic algorithm (MGA), and a novel global optimization algorithm inspired by Mouth Brooding Fish (MBF) in nature have been adopted to explore the maximum and minimum of such median in each membership degree, α α . The results demonstrate that the merit of new MBF algorithm is its greater efficiency in specifying the α α -cut boundaries compared with MGA. Herein, for the sake of more simplicity and efficiency, a new equation is proposed for the prediction of the median of collapse fragility curve of the case-study building, using the gene expression programming (GEP) methodology. This median is formulated in terms of several effective parameters such as steel modulus of elasticity E s , steel yield stress f y , and concrete strength f ′ ′ c , which regarded as the input fuzzy-random variables. The performance and validity of the GEP model are further tested using several criteria. The results indicate that analyzing the proposed GEP model in terms of fuzzy-random variables thru the MBF algorithm significantly improves efficiency and reduces computational time by 75\%.},
  archive      = {J_ASOC},
  author       = {Elaheh Ebrahimi and Gholamreza Abdollahzadeh and Ehsan Jahani},
  doi          = {10.1016/j.asoc.2020.106190},
  journal      = {Applied Soft Computing},
  pages        = {106190},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Developing the seismic fragility analysis with fuzzy random variables using mouth brooding fish algorithm},
  volume       = {91},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Frame-by-frame wi-fi attack detection algorithm with
scalable and modular machine-learning design. <em>ASOC</em>,
<em>91</em>, 106188. (<a
href="https://doi.org/10.1016/j.asoc.2020.106188">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The popularity of Wi-Fi networks coupled with the intrinsic vulnerability of wireless interfaces has promoted the investigation and proposal of traffic analysis and anomaly detection algorithms targeted to that application. We propose a scalable and modular algorithm architecture to set up a lightweight classifier, able to detect malicious frames with high reliability, allowing a simple implementation and suitable for real-time operations. We compare two design alternatives, based on either an optimized neuro-fuzzy classifier or a k k -Nearest Neighbor classifier wrapped into a genetic optimization procedure . Both designs exploit a dissimilarity measure able to handle both numerical and non-numerical features. Scalability and modularity are obtained by considering an array of binary classifiers tuned to identify one specific attack against any other type of traffic. We exploit the Aegean Wi-Fi Intrusion Detection (AWID) dataset to assess the accuracy of the proposed algorithm, finding up to twelve out of the fourteen attack classes of the dataset can be identified with high reliability based just on the inspection of a single frame, provided the right features are observed.},
  archive      = {J_ASOC},
  author       = {Antonello Rizzi and Giuseppe Granato and Andrea Baiocchi},
  doi          = {10.1016/j.asoc.2020.106188},
  journal      = {Applied Soft Computing},
  pages        = {106188},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Frame-by-frame wi-fi attack detection algorithm with scalable and modular machine-learning design},
  volume       = {91},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). BG-SAC: Entity relationship classification model based on
self-attention supported capsule networks. <em>ASOC</em>, <em>91</em>,
106186. (<a href="https://doi.org/10.1016/j.asoc.2020.106186">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To date, deep learning techniques, especially the combination of convolutional neural networks and recurrent neural networks with the attention mechanism , have been the state-of-the-art solutions for processing relation extraction and classification tasks . However, the neural network model constructed by this method cannot make full use of the labeled entities and their positional information in the relation classification, or even performs poorly on the small sample dataset . To address these issues, this paper proposes an entity relationship classification model BG-SAC, which combines BiGRU , Self-Attention mechanism and Capsule Networks. BG-SAC primarily uses BiGRU to obtain sentence sequential information and context-based semantic information , and then is coupled with the Self-Attention mechanism to get the correlation between words. Capsule Networks are used to acquire the positional information of entities. Eventually the probability that entities belong to a certain relationship category is calculated through the length of a capsule, so as to determine the relationship between entities and realize the classification of entity relationship. The experimental results show that the proposed model can effectively capture the word positional information and improve the classification effect with small sample datasets.},
  archive      = {J_ASOC},
  author       = {Dunlu Peng and Dongdong Zhang and Cong Liu and Jing Lu},
  doi          = {10.1016/j.asoc.2020.106186},
  journal      = {Applied Soft Computing},
  pages        = {106186},
  shortjournal = {Appl. Soft. Comput.},
  title        = {BG-SAC: Entity relationship classification model based on self-attention supported capsule networks},
  volume       = {91},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). OHDA: An opposition based high dimensional optimization
algorithm. <em>ASOC</em>, <em>91</em>, 106185. (<a
href="https://doi.org/10.1016/j.asoc.2020.106185">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {One of the challenging problems to-date is to deal with high dimensional data . This problem is getting more severe as the data gathering tools are progressing. This paper proposes an opposition-based optimization algorithm suitable for high dimensions. Its novelty is the angular movement according to a few selected dimensions that makes it effective to search in high dimensions. Accordingly, the Opposition-based High Dimensional optimization Algorithm (OHDA) is proposed. Its performance is studied using functions from CEC2005 for high dimensional data including 1000D and 2000D. In addition, the performance of the proposed algorithm is tested with CEC2014, which is more complicated than CEC2005 and CEC2013. The performance of OHDA is also examined using CEC2017 constraint optimization test suit. The comparing algorithms are CA, ICA, AAA, ABC, KH, MVO, WOA, RW-GWO, B-BBO, LX-BBO and LSHADE44-IEpsilon. The results verify that the proposed algorithm outperforms some conventional optimization algorithms in terms of their accuracies. The efficiency of employing opposite points in optimization is also validated in this paper.},
  archive      = {J_ASOC},
  author       = {Manizheh GhaemiDizaji and Chitra Dadkhah and Henry Leung},
  doi          = {10.1016/j.asoc.2020.106185},
  journal      = {Applied Soft Computing},
  pages        = {106185},
  shortjournal = {Appl. Soft. Comput.},
  title        = {OHDA: An opposition based high dimensional optimization algorithm},
  volume       = {91},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Application of surrogate models in estimation of storm
surge: A comparative assessment. <em>ASOC</em>, <em>91</em>, 106184. (<a
href="https://doi.org/10.1016/j.asoc.2020.106184">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Coastal storm surge hazard assessment has received increased attention due to major hurricane events in the last two decades. Robust hazard assessment requires accurate and efficient storm surge prediction models; however, existing numerical models are either high-fidelity but computationally demanding or low-fidelity but with real-time forecasting application. This dichotomy has prompted the development of surrogate models that leverage available synthetic/historical storm databases to build prediction models intended to better balance efficiency and accuracy. Despite numerous studies that have examined the application of various surrogate modeling methods in coastal response prediction, no study is available that compares all of the frequently used methods for storm surge prediction. Furthermore, in most of these studies, the discussion of the performance of these models is bounded to aggregated error metrics (e.g., RMSE, R). This study aims to provide a comprehensive framework for comparison and assessment of the performance of surrogate models based on Artificial Neural Network (ANN), Gaussian Process Regression (GPR) and Support Vector Regression (SVR) for predicting storm surge. The United States Army Corp of Engineers’ North Atlantic Coast Comprehensive Study (NACCS) database is used for developing the models at representative coastal locations. In this study, the performance of the models is explored by investigating the stability of performance across training sample sizes, identifying systematic trends in errors, assessing performance in predicting large target response quantities, and characterizing the distribution of error. The results indicate that the performance of surrogate models may be improved by use of physically-motivated parameter scaling and that the selection of a surrogate modeling method should be informed by factors such as consistency in performance under a range of target surge elevations. In particular, the results suggest that accuracy of the tested surrogate models may be a function of the target surge elevation. Furthermore, results suggest that model performance should be assessed using factors beyond aggregate error metrics because such measures (particularly when used without modification to focus on risk-significant events) may give incomplete information about performance of surrogate models.},
  archive      = {J_ASOC},
  author       = {Azin Al Kajbaf and Michelle Bensi},
  doi          = {10.1016/j.asoc.2020.106184},
  journal      = {Applied Soft Computing},
  pages        = {106184},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Application of surrogate models in estimation of storm surge: A comparative assessment},
  volume       = {91},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Dynamic ensemble mechanisms to improve particulate matter
forecasting. <em>ASOC</em>, <em>91</em>, 106123. (<a
href="https://doi.org/10.1016/j.asoc.2020.106123">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Respirable solid particles and liquid droplets suspended in the air, known as particulate matter (PM), may have a significant impact on human health, urban infrastructure, and natural and agricultural systems. The adverse effects of PM have raised public concern, especially in heavily polluted areas in the world, making it imperative the development of strategies to keep the concentration levels of these pollutants below harmful thresholds. Traditional machine learning approaches have been used to forecast PM concentrations. However, complex chemical processes may be involved in the composition of PM in the atmosphere and influenced by many meteorological parameters. Thus, underlying data distributions of PM data, uninterruptedly collected, may evolve over time. This phenomenon, known as concept drift, implies an important challenge for traditional machine learning techniques since they do not have mechanisms to handle changes on data distribution at the running time, thus limiting their forecasting capabilities. The overall goal of this work is to evaluate whether the incorporation of mechanisms to deal with concept drift, together with online sequential learning approaches, can improve the accuracy of PM forecasting. To do so, new mechanisms that enable online dynamic ensembles to handle and retain knowledge from different concepts for more time were proposed and adapted to EOS and DOER algorithms, resulting in three approaches: EOS-rank, EOS-D and DOER-rank. These ensemble strategies, which were based on Online Sequential Extreme Learning Machines (OS-ELM), were compared with five algorithms from the literature. To evaluate their performance, real-world and artificial datasets, with known dynamic behaviors, and PM concentration datasets from different cities of the State of São Paulo , Brazil , were used in the experiments. The obtained results showed that the proposed approaches can handle dynamic environments with different rates of drift and that EOS-rank was capable of outperforming most approaches from the literature in scenarios with higher rates of drift. The results also indicate that PM data distributions slowly evolve over time and, consequently, the proposed mechanisms that keep information of past concepts and slowly adapt the ensemble tend to present better results when applied to forecast PM concentration.},
  archive      = {J_ASOC},
  author       = {Andrés Bueno and Guilherme Palermo Coelho and João Roberto Bertini Junior},
  doi          = {10.1016/j.asoc.2020.106123},
  journal      = {Applied Soft Computing},
  pages        = {106123},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Dynamic ensemble mechanisms to improve particulate matter forecasting},
  volume       = {91},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Ra-dominance: A new dominance relationship for
preference-based evolutionary multiobjective optimization.
<em>ASOC</em>, <em>90</em>, 106192. (<a
href="https://doi.org/10.1016/j.asoc.2020.106192">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {While traditional Pareto-based evolutionary multi-objective optimization (EMO) algorithms have shown an excellent balance between convergence and diversity on a wide range of practical problems with two or three objectives in real applications, the decision maker (DM) is interested in a unique set of solutions rather than the whole population on Pareto optimal front (POF). In addition, Pareto-based EMO algorithms have some shortcomings in dealing with many-objective problems because of insufficient selection pressure toward trade-off solutions. Due to the above, it is crucial to incorporate DM preference information into EMO and seek a representative subset of Pareto optimal solutions with an increase in the number of objectives. This paper proposes a new dominance relationship, called Ra-dominance, which can improve diversity among the Pareto-equivalent solutions increase the selection pressure in evolutionary process. It has the ability to guide the population toward areas more responsive to the needs of the DM according to a reference point and preference angle. We use the new dominance relationship in the NSGA-II algorithm, and the efficacy and usefulness of the modified procedure are assessed through two- to ten-objective problems. Experimental results show that the algorithm applying this new dominance relationship is highly competitive when compared with four state-of-the-art preference-based EMO methods.},
  archive      = {J_ASOC},
  author       = {Juan Zou and Qite Yang and Shengxiang Yang and Jinhua Zheng},
  doi          = {10.1016/j.asoc.2020.106192},
  journal      = {Applied Soft Computing},
  pages        = {106192},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Ra-dominance: A new dominance relationship for preference-based evolutionary multiobjective optimization},
  volume       = {90},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A novel hybrid particle swarm optimization and gravitational
search algorithm for multi-objective optimization of text mining.
<em>ASOC</em>, <em>90</em>, 106189. (<a
href="https://doi.org/10.1016/j.asoc.2020.106189">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Big-data is one of the milestones on the web especially on social media (SM). Due to the widespread popularity of SM on the web, it is a painful task to capture the essence of SM. In this study, mining big social media data is re-formulated into a multi-objective optimization (MOO) task for an extractive summary. A Gravitational Search Algorithm (GSA) is utilized for optimizing several expressive objectives for generating a concise summary of SM. Moreover, particle swarm optimization (PSO) is mixed with GSA in a new shape to strengthen a local search ability and slow convergence speed in standard GSA. Whereas some users may demand the brief at any moment, several groups are constituted for incremental updating process during real-time based on naïve Bayes algorithm. From experimental results, the proposed approach outperformed other notable and state-of-art comparative methods .},
  archive      = {J_ASOC},
  author       = {Mohamed Atef Mosa},
  doi          = {10.1016/j.asoc.2020.106189},
  journal      = {Applied Soft Computing},
  pages        = {106189},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A novel hybrid particle swarm optimization and gravitational search algorithm for multi-objective optimization of text mining},
  volume       = {90},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Ensemble of machine learning algorithms for cryptocurrency
investment with different data resampling methods. <em>ASOC</em>,
<em>90</em>, 106187. (<a
href="https://doi.org/10.1016/j.asoc.2020.106187">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This work proposes a system based on machine learning aimed at creating an investment strategy capable of trading on the cryptocurrency exchange markets. Additionally, with the goal of generating investments with higher returns and lower risk, rather than investing on predictions based on time sampled financial series, a novel method for resampling financial series was developed and employed in this work. For this purpose, the originally time sampled financial series are resampled according to a closing value threshold, thus creating a series prone to obtaining higher returns and lower risk than the original series. Out of these resampled series as well as the original, technical indicators are calculated and fed as inputs to four machine learning algorithms : Logistic Regression , Random Forest , Support Vector Classifier, and Gradient Tree Boosting. Each of these algorithms is responsible for generating a transaction signal. Afterwards, a fifth transaction signal is generated by simply calculating the unweighted average of the four trading signals outputted from the previous algorithms, to improve on their results. In the end, the investment results obtained with the resampled series are compared to the commonly utilized fixed time interval sampling. This work demonstrates that independently of using or not a resampling method, all learning algorithms outperform the Buy and Hold (B&amp;H) strategy in the overwhelming majority of the 100 markets tested. Nevertheless, out of the learning algorithms, the unweighted average obtains the best overall results, namely accuracies up to 59.26\% for time resampled series. But most importantly, it is concluded that both alternative resampling methods tested are capable of generating far greater returns and with lower risk relatively to time resampled data.},
  archive      = {J_ASOC},
  author       = {Tomé Almeida Borges and Rui Ferreira Neves},
  doi          = {10.1016/j.asoc.2020.106187},
  journal      = {Applied Soft Computing},
  pages        = {106187},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Ensemble of machine learning algorithms for cryptocurrency investment with different data resampling methods},
  volume       = {90},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Multi-scale patch based representation feature learning for
low-resolution face recognition. <em>ASOC</em>, <em>90</em>, 106183. (<a
href="https://doi.org/10.1016/j.asoc.2020.106183">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In practical video surveillance, the quality of facial regions of interest is usually affected by the large distances between the objects and surveillance cameras, which undoubtedly degrade the recognition performance. Existing methods usually consider the holistic representations, while neglecting the complementary information from different patch scales. To tackle this problem, this paper proposes a multi-scale patch based representation feature learning (MSPRFL) scheme for low-resolution face recognition problem. Specifically, the proposed MSPRFL approach first exploits multi-level information to learn more accurate resolution-robust representation features of each patch with the help of a training dataset. Then, we exploit these learned resolution-robust representation features to reduce the resolution discrepancy by integrating the recognition results from all patches. Finally, by considering the complementary discriminative ability from different patch scales, we try to fuse the multi-scale outputs by learning scale weights via an ensemble optimization model. We further verify the efficiency of the proposed MSPRFL on low-resolution face recognition by the comparison experiments on several commonly used face datasets.},
  archive      = {J_ASOC},
  author       = {Guangwei Gao and Yi Yu and Meng Yang and Pu Huang and Qi Ge and Dong Yue},
  doi          = {10.1016/j.asoc.2020.106183},
  journal      = {Applied Soft Computing},
  pages        = {106183},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Multi-scale patch based representation feature learning for low-resolution face recognition},
  volume       = {90},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A multi-objective evolutionary approach for planning and
optimal condition restoration of secondary distribution networks.
<em>ASOC</em>, <em>90</em>, 106182. (<a
href="https://doi.org/10.1016/j.asoc.2020.106182">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A secondary distribution network (SDN), corresponding to the final user low voltage distribution circuit, is continuously growing due to a persistent increase in load demand. Consequently, the performance of any optimized design will inevitably degrade over time. To avoid the associated repercussions such as faults, congestion, voltage drops, and other major quality issues, we are eventually prompted to redesign this part of the grid. To do so, we propose a Two-Stage Multi-Objective Evolutionary Approach (TS-MOEAP), which is able to find a new optimal network configuration , circumventing the associated quality issues. The proposed approach is oriented to improve the performance of SDNs by combining the concepts of network reconfiguration (NR) and optimal placement of distribution transformers (DTs). Due to the large and complex topology of SDNs, we deal with a hard combinatorial, non-convex, and nonlinear optimization problem . Consequently, to facilitate the resolution of the problem, the proposal is divided into two stages: (1) optimal placement and sizing of distribution transformers, as well as conductor sizing and branch routing, and (2) optimal network reconfiguration. For the first stage, an improved particle swarm optimization technique (IPSO) combined with a greedy algorithm is used, and for the second stage, an improved nondominated sorting genetic algorithm with a heuristic mutation operator (NSGA-HO) is implemented. The approach redesigns SDNs by minimizing total power loss and investment costs while satisfying quality issues and technical constraints. The proposed approach is validated by improving a real-life SDN with critical quality and technical issues. We also compare the results with respect to other state-of-the-art algorithms.},
  archive      = {J_ASOC},
  author       = {J.P. Avilés and J.C. Mayo-Maldonado and O. Micheloud},
  doi          = {10.1016/j.asoc.2020.106182},
  journal      = {Applied Soft Computing},
  pages        = {106182},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A multi-objective evolutionary approach for planning and optimal condition restoration of secondary distribution networks},
  volume       = {90},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Financial time series forecasting with deep learning : A
systematic literature review: 2005–2019. <em>ASOC</em>, <em>90</em>,
106181. (<a href="https://doi.org/10.1016/j.asoc.2020.106181">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Financial time series forecasting is undoubtedly the top choice of computational intelligence for finance researchers in both academia and the finance industry due to its broad implementation areas and substantial impact. Machine Learning (ML) researchers have created various models, and a vast number of studies have been published accordingly. As such, a significant number of surveys exist covering ML studies on financial time series forecasting. Lately, Deep Learning (DL) models have appeared within the field, with results that significantly outperform their traditional ML counterparts. Even though there is a growing interest in developing models for financial time series forecasting, there is a lack of review papers that solely focus on DL for finance. Hence, the motivation of this paper is to provide a comprehensive literature review of DL studies on financial time series forecasting implementation. We not only categorized the studies according to their intended forecasting implementation areas, such as index, forex, and commodity forecasting, but we also grouped them based on their DL model choices, such as Convolutional Neural Networks (CNNs), Deep Belief Networks (DBNs), and Long-Short Term Memory (LSTM). We also tried to envision the future of the field by highlighting its possible setbacks and opportunities for the benefit of interested researchers.},
  archive      = {J_ASOC},
  author       = {Omer Berat Sezer and Mehmet Ugur Gudelek and Ahmet Murat Ozbayoglu},
  doi          = {10.1016/j.asoc.2020.106181},
  journal      = {Applied Soft Computing},
  pages        = {106181},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Financial time series forecasting with deep learning : a systematic literature review: 2005–2019},
  volume       = {90},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A case-based reasoning system for recommendation of data
cleaning algorithms in classification and regression tasks.
<em>ASOC</em>, <em>90</em>, 106180. (<a
href="https://doi.org/10.1016/j.asoc.2020.106180">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, advances in Information Technologies (social networks, mobile applications, Internet of Things , etc.) generate a deluge of digital data; but to convert these data into useful information for business decisions is a growing challenge. Exploiting the massive amount of data through knowledge discovery (KD) process includes identifying valid, novel, potentially useful and understandable patterns from a huge volume of data. However, to prepare the data is a non-trivial refinement task that requires technical expertise in methods and algorithms for data cleaning. Consequently, the use of a suitable data analysis technique is a headache for inexpert users. To address these problems, we propose a case-based reasoning system (CBR) to recommend data cleaning algorithms for classification and regression tasks . In our approach, we represent the problem space by the meta-features of the dataset, its attributes, and the target variable. The solution space contains the algorithms of data cleaning used for each dataset. We represent the cases through a Data Cleaning Ontology. The case retrieval mechanism is composed of a filter and similarity phases. In the first phase, we defined two filter approaches based on clustering and quartile analysis. These filters retrieve a reduced number of relevant cases. The second phase computes a ranking of the retrieved cases by filter approaches, and it scores a similarity between a new case and the retrieved cases. The retrieval mechanism proposed was evaluated through a set of judges. The panel of judges scores the similarity between a query case against all cases of the case-base (ground truth). The results of the retrieval mechanism reach an average precision on judges ranking of 94.5\% in top 3 (P@3), for top 7 (P@7) 84.55\%, while in top 10 (P@10) 78.35\%.},
  archive      = {J_ASOC},
  author       = {David Camilo Corrales and Agapito Ledezma and Juan Carlos Corrales},
  doi          = {10.1016/j.asoc.2020.106180},
  journal      = {Applied Soft Computing},
  pages        = {106180},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A case-based reasoning system for recommendation of data cleaning algorithms in classification and regression tasks},
  volume       = {90},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Markov network versus recurrent neural network in forming
herd behavior based on sight and simple sound communication.
<em>ASOC</em>, <em>90</em>, 106177. (<a
href="https://doi.org/10.1016/j.asoc.2020.106177">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sound emission based on information received from the environment, including messages made by other individuals, enables communication between organisms of a given type (e.g., victims). Sound is the main form of communication for animals that they can incorporate into the decision-making process. In this paper, we describe conducted experiments to observe the role of sound communication in forming herd behavior. During the simulation, we investigated prey and predator organisms steered by a controller in the virtual world. We consider two types of agent controllers. The first one is developed using a Markov Network , the second one – a Recurrent Neural Network . The controller, based on information received in the form of environmental stimuli or states of own memory, makes decisions to change the position or, optionally, to make a sound that can then be picked up by nearby individuals. To find the parameters of the controllers, they are evolved by a genetic algorithm . In each generation, genotypes are decoded to the recurrent neural network or Markov Network , then some steps of simulations in a unique artificial environment, modeling the real world, are performed. On this basis, the evaluation of individuals is calculated. The main research element in this work was examining the impact of simple sound communication on forming herd behavior under the predator pressure. A comparison of controllers, i.e., Markov Network and Recurrent Neural Network , was the second goal of our research.},
  archive      = {J_ASOC},
  author       = {Urszula Markowska-Kaczmar and Tomasz Marcinkowski},
  doi          = {10.1016/j.asoc.2020.106177},
  journal      = {Applied Soft Computing},
  pages        = {106177},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Markov network versus recurrent neural network in forming herd behavior based on sight and simple sound communication},
  volume       = {90},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A new metaheuristic optimization algorithm inspired by human
dynasties with an application to the wind turbine micrositing problem.
<em>ASOC</em>, <em>90</em>, 106176. (<a
href="https://doi.org/10.1016/j.asoc.2020.106176">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Optimization is an art that is best performed by a well-tuned algorithm. Nature – instead of being fully deterministic – is evolutionary, vibrant and resourceful. The nature-inspired algorithms use the best combination and evolution strategy in a given situation. In this work, a new metaheuristic algorithm is developed by using social behavior in human dynasties. The motivation, conceptual framework, mathematical model, pseudocode and working of the algorithm are described in this paper and the adjoining papers. The proposed dynastic optimization algorithm (DOA) has evolved with the wind turbine micrositing (WTM) problem in mind. The proposed DOA has been successfully applied to the traditional WTM and encouraging results have been obtained. It is demonstrated that the proposed approach is equally viable as other existing algorithms, like the Genetic algorithm (GA) and Differential evolution algorithm (DEA). The main advantage of the proposed DOA is that it is simple, unique, fast, unbiased and versatile in comparison with others. The validation of results has been made with respect to a few other mainstream algorithms in the literature, besides statistical sensitivity analysis is also performed. The 95\% confidence interval forecasts for the power enhancement and cost reduction by using DOA against GA and DEA are encouraging and guarantee an adequate amount of mean increase in power output and a considerable average cost reduction.},
  archive      = {J_ASOC},
  author       = {Shafiq-ur-Rehman Massan and Asim Imdad Wagan and Muhammad Mujtaba Shaikh},
  doi          = {10.1016/j.asoc.2020.106176},
  journal      = {Applied Soft Computing},
  pages        = {106176},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A new metaheuristic optimization algorithm inspired by human dynasties with an application to the wind turbine micrositing problem},
  volume       = {90},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A new incomplete pattern belief classification method with
multiple estimations based on KNN. <em>ASOC</em>, <em>90</em>, 106175.
(<a href="https://doi.org/10.1016/j.asoc.2020.106175">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The classification of missing data is a challenging task, because the lack of pattern attributes may bring uncertainty to the classification results and most classification methods produce only one estimation, which may have a risk of misclassification . A new incomplete pattern belief classification (PBC) method with multiple estimations based on K K -nearest neighbors (KNNs) is proposed to deal with missing data. PBC preliminarily classifies the incomplete pattern using its KNNs obtained by the known attributes. The pattern whose KNNs contain only one class information can be directly divided into this class. If not, the p p ( p ≤ c p≤c ) estimations will be computed according to the different KNNs in different classes when p p classes are included in the KNNs of the pattern and it will yield p p pieces of classification results by the chosen classifier. Then, a weighted possibility distance method is used to further divide the p p classification results with their KNNs’ classification information. The pattern with similar possibility distances in different classes will be reasonably classified into a proper meta-class under the framework of belief functions theory, which truly reflects the uncertainty of the pattern caused by missing values and effectively reduces the error rate. Experiments on both artificial and real data sets show that PBC is effective for dealing with missing data.},
  archive      = {J_ASOC},
  author       = {Zong-fang Ma and Hong-peng Tian and Ze-chao Liu and Zuo-wei Zhang},
  doi          = {10.1016/j.asoc.2020.106175},
  journal      = {Applied Soft Computing},
  pages        = {106175},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A new incomplete pattern belief classification method with multiple estimations based on KNN},
  volume       = {90},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Momentum method powered by swarm approaches for topology
optimization. <em>ASOC</em>, <em>90</em>, 106174. (<a
href="https://doi.org/10.1016/j.asoc.2020.106174">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents the momentum algorithm for topology optimization problems. Since topology optimization problems are multimodal, the momentum may be trapped into a local optimum. To achieve a better solution, this approach is combined with an enhanced particle swarm optimization (PSOG). The constrained problems are converted into unconstrained ones by an external penalty function. To illustrate the effectiveness of the proposed method, five examples with static, multiple and self-weight loadings are examined. The problems are investigated by the proposed method and the results are compared with optimality criteria algorithm (OC), method of moving asymptotes (MMA), sequential linear programming (SLP), momentum, PSO and PSOG. The numerical results show the superiority of the proposed approach.},
  archive      = {J_ASOC},
  author       = {Farzad Salajegheh and Mohammad Kamalodini and Eysa Salajegheh},
  doi          = {10.1016/j.asoc.2020.106174},
  journal      = {Applied Soft Computing},
  pages        = {106174},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Momentum method powered by swarm approaches for topology optimization},
  volume       = {90},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Constrained evolutionary algorithms for epidemic spreading
curing policy. <em>ASOC</em>, <em>90</em>, 106173. (<a
href="https://doi.org/10.1016/j.asoc.2020.106173">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The design and developments of policies aiming to control and contain spreading processes when resources are limited is an important problem in many application domains dealing with resource allocation, such as public health and network security . This problem, referred as Optimal Curing Policy (OCP) problem, can be formalized as a constrained minimization problem by relying on the approximated heterogeneous N-Intertwined Mean-Field Approximation (NIMFA) model of the S I S SIS spreading process. In this paper, an approach which combines Differential Evolution and Genetic Algorithms is proposed to solve the O C P OCP problem. The hybridization leverages the best characteristics of the two methods to produce high quality solutions in an efficient and effective way. An extensive experimentation on both real-world and synthetic networks shows that the approach is able to outperform a standard solver for semidefinite programming .},
  archive      = {J_ASOC},
  author       = {Clara Pizzuti and Annalisa Socievole},
  doi          = {10.1016/j.asoc.2020.106173},
  journal      = {Applied Soft Computing},
  pages        = {106173},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Constrained evolutionary algorithms for epidemic spreading curing policy},
  volume       = {90},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Emended salp swarm algorithm for multiobjective electric
power dispatch problem. <em>ASOC</em>, <em>90</em>, 106172. (<a
href="https://doi.org/10.1016/j.asoc.2020.106172">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents an emended salp swarm algorithm (ESSA) which is basically the extension of the basic salp swarm algorithm (SSA) to solve multiobjective electric power load dispatch problem. The main inspiration behind ESSA is the swarming behavior and reproduction cycle of salps. Salps, in the chain, move across a multi-dimensional search space to aim for the food source (global solution). Owing to the searching behavior of SSA that makes the algorithm prone to premature convergence, the solitary and colonial reproduction phase of salp has been introduced to improve the convergence behavior along with their swarming behavior The multiobjective optimization problem is firstly converted into scalar objective exploiting fuzzy set theory and the conflicting nature of objectives is resolved by cardinal priority ranking. The variable elimination method with exterior penalty is used to handle the physical and operational constraints of generating units. The validation of the proposed ESSA has been examined on the standard benchmark functions and seven EcLD test systems including both scalar and multiple objectives. The statistical analysis based on Wilcoxon sign rank test, supports that results achieved by the algorithm are superior to the other competing algorithms. So, the proposed algorithm (ESSA) is found a promising algorithm.},
  archive      = {J_ASOC},
  author       = {Veenus Kansal and J.S. Dhillon},
  doi          = {10.1016/j.asoc.2020.106172},
  journal      = {Applied Soft Computing},
  pages        = {106172},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Emended salp swarm algorithm for multiobjective electric power dispatch problem},
  volume       = {90},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A novel fuzzy clustering algorithm by minimizing global and
spatially constrained likelihood-based local entropies for noisy 3D
brain MR image segmentation. <em>ASOC</em>, <em>90</em>, 106171. (<a
href="https://doi.org/10.1016/j.asoc.2020.106171">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose a novel fuzzy clustering algorithm by minimizing global and spatially constrained likelihood-based local entropies (FCMGsLE) for segmenting noisy 3D brain magnetic resonance (MR) image volumes. For each voxel, in order to measure uncertainties that arise while identifying its class, two different entropies are defined. In particular, they measure the amount of uncertainties in terms of global entropy using fuzzifier weighted global membership function and spatially constrained likelihood-based local entropy using fuzzifier weighted local membership function. To mitigate the effect of noise and intensity inhomogeneity (IIH) or radio frequency (RF) inhomogeneity, the local membership function is induced by spatially constrained likelihood measure. These entropies are minimized through a fuzzy objective function to obtain the cluster prototypes and membership functions. The final membership function is obtained by integrating these global and local membership functions using weighted parameters. The algorithm is assessed both qualitatively and quantitatively on ten 3D volumes of simulated and clinical brain MR image data having high levels of noise and intensity inhomogeneity and a synthetic 3D image volume with Rician noise. The simulation results reveal that the proposed algorithm outperforms several state-of-the-art algorithms devised in recent past when evaluated in terms of segmentation accuracy , Dice similarity coefficient, partition coefficient, and partition entropy},
  archive      = {J_ASOC},
  author       = {Nabanita Mahata and Jamuna Kanta Sing},
  doi          = {10.1016/j.asoc.2020.106171},
  journal      = {Applied Soft Computing},
  pages        = {106171},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A novel fuzzy clustering algorithm by minimizing global and spatially constrained likelihood-based local entropies for noisy 3D brain MR image segmentation},
  volume       = {90},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Island-based crow search algorithm for solving optimal
control problems. <em>ASOC</em>, <em>90</em>, 106170. (<a
href="https://doi.org/10.1016/j.asoc.2020.106170">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Crow Search Algorithm (CROW) is one of the members of recently developed swarm-based metaheuristic algorithms . Literature includes different applications of this algorithm on engineering design problems . However, this optimization method suffers from some drawbacks such as premature convergence and trapping into local optima at the early phase of iterations. In order to conquer this algorithm specific inabilities, many research studies have been conducted in the literature dealing with the improvements and enhancements on the search mechanism of CROW. Structured population mechanism plays a vital role in preserving and controlling diversity, and thus increases the solution efficiency in evolutionary algorithms . Among the different types of methods used in structured algorithms, the island model is one of the widely applied solution strategies, in which the population individuals are subdivided into a predefined number of subpopulations. Migration mechanism is the key factor increasing population diversity, which takes place between independently running subpopulations during iterations to exchange valuable and useful solution information. This study embeds the fundamentals of the island model concepts into the Crow Search Algorithm to improve its probing capabilities of the search domain, by means of the periodically interacting subpopulations on the course of iterations. In addition, four different hierarchical migration topologies have been proposed, and their search effectiveness have been evaluated and compared over 45 optimization test functions. The optimization function test set includes classic benchmark optimization problems and CEC 2015 benchmark functions . Furthermore, each hierarchical island model is applied for solving six different optimal control problems in order to investigate their efficiencies on multi-dimensional real world optimization problems. The investigated optimal control problems are parallel reaction, continuous stirred tank reactor , batch reactor consecutive reaction, nonlinear constrained mathematical system, nonlinear continuous stirred tank reactor and nonlinear crane container problems. It is found out that the island model concepts improved the optimization performance of CROW. The proposed island models outperformed or showed similar performance compared to the six selected literature optimizers for 27–29 classic benchmark optimization problems. Moreover, incorporating the master sub-population to the island model improved the optimization capability of the algorithm further in most cases. The island models that employ the master sub-population came up with more favorable results compared to their non-master sub-population peers in all optimal control problems. The island model that includes the master sub-population and has the migration topology entitled “82” found the most desirable solutions for 4–6 optimal control problems.},
  archive      = {J_ASOC},
  author       = {Mert Sinan Turgut and Oguz Emrah Turgut and Deniz Türsel Eliiyi},
  doi          = {10.1016/j.asoc.2020.106170},
  journal      = {Applied Soft Computing},
  pages        = {106170},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Island-based crow search algorithm for solving optimal control problems},
  volume       = {90},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Strongly-typed genetic programming and fuzzy inference
system: An embedded approach to model and generate trading rules.
<em>ASOC</em>, <em>90</em>, 106169. (<a
href="https://doi.org/10.1016/j.asoc.2020.106169">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Generating trading signals is an interesting topic and a hard problem to solve. This work uses fuzzy inference system (FIS) and strongly typed genetic programming (STGP) to generate trading rules for the US stock market, a framework that we call FISTGP. The two embedded models have not been widely evaluated in financial applications, and according to the literature, their combination could improve forecasting performance. The fitness function used to train the STGP model is based on accuracy, optimizing the buy and sell signals, taking a different approach to the classic optimization of return–risk ratio. The rules are generated in a FIS framework, and the final signal depends on the amount of information that the investor relies on. The model is suited to each investor as a recommendation of when to change portfolio composition according to his or her particular criteria. Ternary rules are generated based on an economic interpretation, considering the risk-free rate as a part of more demanding rules. The model is applied to 90 of the most traded and active stocks in the US stock market. This approach generates important recommendations and delivers useful information to investors. The results show that the proposed model outperforms the Buy and Hold (B&amp;H) strategy by 28.62\% in the test period, considering excesses of return, with almost the same risk (1.28\% higher). The other base models underperform in comparison to the B&amp;H, with the proposed model also outperforming them.},
  archive      = {J_ASOC},
  author       = {Kevin Michell and Werner Kristjanpoller},
  doi          = {10.1016/j.asoc.2020.106169},
  journal      = {Applied Soft Computing},
  pages        = {106169},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Strongly-typed genetic programming and fuzzy inference system: An embedded approach to model and generate trading rules},
  volume       = {90},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Implementation of optimized dynamic trajectory modification
algorithm to avoid obstacles for secure navigation of UAV.
<em>ASOC</em>, <em>90</em>, 106168. (<a
href="https://doi.org/10.1016/j.asoc.2020.106168">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To develop escape manoeuvre from obstacles and to find new waypoints for dynamic trajectory modification of UAV , a novel Particle Swarm Optimization based Collision Avoidance algorithm (PSO-CA) is presented in this paper. The proposed “obstacle sense and avoid” algorithm and the logical decision-making system aids the Unmanned Aerial Vehicle (UAV) to re-route its current path to a safer flight course when an obstacle pops up along its intended path. A radar with 10 km range identifies obstacles and the UAV manoeuvres based on radar data, making it suitable for any unknown environment. The proposed system would manoeuvre the UAV autonomously along optimized alternate path to avoid the conflicting traffic. New waypoints are identified and the waypoint list is modified dynamically to avoid collision with stationary threats like hill, tree and moving intruders like other UAVs. The proposed algorithm steers the vehicle safely along alternate path with less change in intended trajectory while avoiding all potential threats. As the PSO-CA algorithm detects obstacles and identifies alternate path well in advance for unknown pop up threats, the UAV is safe and is suitable for real time environment . The proposed algorithm has considered obstacles with different positions, different sizes and random motion. Experimental results conducted on 6-DOF model UAV with different obstacles clearly justify the efficiency of the proposed method in comparison with other planners.},
  archive      = {J_ASOC},
  author       = {P.S. Krishnan and K. Manimala},
  doi          = {10.1016/j.asoc.2020.106168},
  journal      = {Applied Soft Computing},
  pages        = {106168},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Implementation of optimized dynamic trajectory modification algorithm to avoid obstacles for secure navigation of UAV},
  volume       = {90},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Multi-label feature selection based on label distribution
and feature complementarity. <em>ASOC</em>, <em>90</em>, 106167. (<a
href="https://doi.org/10.1016/j.asoc.2020.106167">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the real-world, data in various domains usually tend to be high-dimensional, which may result in considerable time complexity and poor performance for multi-label classification problems. Multi-label feature selection is an important preprocessing step in machine learning , which can effectively solve the so-called “curse of dimensionality” by removing irrelevant and redundant features. Nevertheless, the significance of related labels for each instance is generally different, which is an issue that most of the existing multi-label feature selection algorithms have not addressed. Hence, in this paper, we integrate label-distribution learning into multi-label feature selection from the perspective of granular computing with considering multiple feature correlations. Then, a novel multi-label feature selection algorithm based on label distribution and feature complementarity is developed. In addition, the proposed algorithm consists of two primary parts: first, the different significances of related labels for each instance in the multi-label data are obtained based on granular computing ; second, the feature complementarity is estimated based on neighborhood mutual information without discretization . Moreover, the superiority of our proposed method over other state-of-the-art methods is demonstrated by conducting comprehensive experiments with 10 publicly available multi-label datasets on six widely-used metrics. Finally, the proposed method can significantly improve the performance of the classifier while reducing the dimension of the original data.},
  archive      = {J_ASOC},
  author       = {Wenbin Qian and Xuandong Long and Yinglong Wang and Yonghong Xie},
  doi          = {10.1016/j.asoc.2020.106167},
  journal      = {Applied Soft Computing},
  pages        = {106167},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Multi-label feature selection based on label distribution and feature complementarity},
  volume       = {90},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). An integrated multi-criteria decision making model with
self-organizing maps for the assessment of the performance of publicly
traded banks in borsa istanbul. <em>ASOC</em>, <em>90</em>, 106166. (<a
href="https://doi.org/10.1016/j.asoc.2020.106166">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The major role of banks is to play financial intermediation function between funding needs and funding surpluses. Any problem in the banking system directly influences not only the stakeholders but also the general economy. There have been many attempts to measure the performance of banks by using Multi-Criteria Decision Making Tools (MCDM). This study aims to assess the performance of the publicly traded banks in Borsa Istanbul operating in the Turkish banking sector for the four quarter of 2018. Contribution of this study can be summarized as follow. Dataset is consisted of three dimensions: (i) financial ratios , (ii) branch and personnel network, (iii) daily stock market returns and standard deviation of daily returns. Using a multi-dimensional dataset compiled from three different data sources enabled an objective assessment of bank performance. Features are selected from the financial ratio dataset by using Self-Organizing Maps technique. Instead of using single weight set, a million weight combinations are calculated to monitor the score distributions of MCDM tools. EDAS, MOORA , OCRA and TOPSIS techniques are selected due to their similarity in calculation steps. It is found that OCRA technique produced consistent rankings for different periods. Highest correlation coefficients observed between OCRA and TOPSIS techniques.},
  archive      = {J_ASOC},
  author       = {Mehmet Ozcalici and Mete Bumin},
  doi          = {10.1016/j.asoc.2020.106166},
  journal      = {Applied Soft Computing},
  pages        = {106166},
  shortjournal = {Appl. Soft. Comput.},
  title        = {An integrated multi-criteria decision making model with self-organizing maps for the assessment of the performance of publicly traded banks in borsa istanbul},
  volume       = {90},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Generative adversarial network and texture features applied
to automatic glaucoma detection. <em>ASOC</em>, <em>90</em>, 106165. (<a
href="https://doi.org/10.1016/j.asoc.2020.106165">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Glaucoma is a neurodegenerative disease that has a multifactorial etiology. The main characteristic of this illness is the progressive lesion of the optic nerve. This disease is chronic and causes permanent blindness at an advanced stage. Early diagnosis is essential to ensure a favorable prognosis and improve the patient’s quality of life. Digital Image Processing together with computational techniques of machine learning allow the creation of methods for automatic detection of glaucoma. In this context, this work aims at the early diagnosis of glaucoma through a Generative Adversarial Network allied to texture attributes defined from indexes of taxonomic diversity. The method we propose can be divided into: (i) image acquisition through the RIM-ONE and Drishti-GS public databases; (ii) training of a conditional Generative Adversarial Network for segmentation of the optical discs into retinal images; (iii) pre-processing through enhancement and hole fill-in techniques; (iv) extraction of texture attributes using the index of taxonomic diversity; and (v) validation of the proposal through three classifiers evaluated according to four performance metrics. The results are promising and indicate that the method is robust, initially reaching 77.9\% accuracy. However, as we apply improvements and adjustments in the method employed, we reach 100\% accuracy and a ROC curve of 1. Therefore, we propose a second opinion on the diagnosis of glaucoma, assisting the specialist precisely.},
  archive      = {J_ASOC},
  author       = {Tomaz Ribeiro Viana Bisneto and Antonio Oseas de Carvalho Filho and Deborah Maria Vieira Magalhães},
  doi          = {10.1016/j.asoc.2020.106165},
  journal      = {Applied Soft Computing},
  pages        = {106165},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Generative adversarial network and texture features applied to automatic glaucoma detection},
  volume       = {90},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). From big data to business analytics: The case study of churn
prediction. <em>ASOC</em>, <em>90</em>, 106164. (<a
href="https://doi.org/10.1016/j.asoc.2020.106164">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The success of companies hugely depends on how well they can analyze the available data and extract meaningful knowledge. The Extract-Transform-Load (ETL) process is instrumental in accomplishing these goals, but requires significant effort, especially for Big Data. Previous works have failed to formalize, integrate, and evaluate the ETL process for Big Data problems in a scalable and cost-effective way. In this paper, we propose a cloud-based ETL framework for data fusion and aggregation from a variety of sources. Next, we define three scenarios regarding data aggregation during ETL: (i) ETL with no aggregation; (ii) aggregation based on predefined columns or time intervals; and (iii) aggregation within single user sessions spanning over arbitrary time intervals. The third scenario is very valuable in the context of feature engineering, making it possible to define features as “the time since the last occurrence of event X”. The scalability was evaluated on Amazon AWS Hadoop clusters by processing user logs collected with Kinesis streams with datasets ranging from 30 GB to 2.6 TB. The business value of the architecture was demonstrated with applications in churn prediction, service-outage prediction, fraud detection, and more generally — decision support and recommendation systems. In the churn prediction case, we showed that over 98\% of churners could be detected, while identifying the individual reason. This allowed support and sales teams to perform targeted retention measures.},
  archive      = {J_ASOC},
  author       = {Eftim Zdravevski and Petre Lameski and Cas Apanowicz and Dominik Ślȩzak},
  doi          = {10.1016/j.asoc.2020.106164},
  journal      = {Applied Soft Computing},
  pages        = {106164},
  shortjournal = {Appl. Soft. Comput.},
  title        = {From big data to business analytics: The case study of churn prediction},
  volume       = {90},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Collaborative filtering based recommendation of sampling
methods for software defect prediction. <em>ASOC</em>, <em>90</em>,
106163. (<a href="https://doi.org/10.1016/j.asoc.2020.106163">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The performance of software defect prediction have been hindered by the imbalanced nature of software defect data. Fortunately, a variety of sampling methods have been employed to improve defect prediction performance. However, researchers and practitioners are usually burdened with selecting the optimal sampling methods for the defect data at hand. In practice, no sampling method has been found to perform best in theory and practice. Therefore it is necessary and valuable to study how to select applicable sampling methods according to the current data characteristics. This paper presents a collaborative filtering based sampling methods recommendation algorithm (CFSR) for automatically recommending applicable sampling methods for the new defect data. CFSR firstly ranks existing sampling methods with historical defect data, and then mines the data similarity between the new and historical defect data with meta-features. Finally, all the information of ranked sampling methods and data similarity are combined to build a recommendation network, with which the user-based collaborative filtering algorithm is employed to recommend appropriate sampling methods for the new defect data. A thorough experiment with five classification algorithms , two prediction performance, five recommendation performance and 12 popular sampling methods was conducted over 20 imbalanced software defect data. The experimental results firstly demonstrate the importance and necessity of present study, and then show that the proposed CFSR method is feasible and effective.},
  archive      = {J_ASOC},
  author       = {Zhongbin Sun and Jingqi Zhang and Heli Sun and Xiaoyan Zhu},
  doi          = {10.1016/j.asoc.2020.106163},
  journal      = {Applied Soft Computing},
  pages        = {106163},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Collaborative filtering based recommendation of sampling methods for software defect prediction},
  volume       = {90},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A novel image encryption scheme using both pixel level and
bit level permutation with chaotic map. <em>ASOC</em>, <em>90</em>,
106162. (<a href="https://doi.org/10.1016/j.asoc.2020.106162">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In cryptography, chaotic cryptosystem is one of the methods to carry out encryption and decryption of images. This paper introduces a new symmetric key encryption technique based on chaotic map , scan method and cyclic shift operation. The confusion and diffusion techniques are implemented using Hilbert curve and Henon map. To ensure image scrambling, both pixel level and bit level permutations are performed. A novel method is adopted for bit level permutation using cyclic shift operation. The key streams for cyclic shift and diffusion operations are generated from the Henon map. Final encrypted image is generated from the double scrambled image. The performance of the proposed method has been analyzed using various analyses like statistical analysis, entropy analysis, differential attack analysis, key sensitivity analysis and known plain text attack analysis. Experimental results show that the proposed image encryption technique resists various attacks and ensures high security. It also provides better performance when compared with several traditional and state-of-the-art image encryption methods.},
  archive      = {J_ASOC},
  author       = {Shahna K.U. and Anuj Mohamed},
  doi          = {10.1016/j.asoc.2020.106162},
  journal      = {Applied Soft Computing},
  pages        = {106162},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A novel image encryption scheme using both pixel level and bit level permutation with chaotic map},
  volume       = {90},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A new secondary decomposition-ensemble approach with cuckoo
search optimization for air cargo forecasting. <em>ASOC</em>,
<em>90</em>, 106161. (<a
href="https://doi.org/10.1016/j.asoc.2020.106161">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The accurate forecast of air cargo demand is essential for infrastructure construction planning and daily operation management. Evidently, it is extremely difficult to capture the dynamics of time series impacted by distinct sources. To reduce the complexity of data, the current popular method is to decompose the original data into several modal branches with different characteristic attributes. But the new problem is that the components generated by decomposition are still irregular and unstable, and there is no unified method to predict them. In this paper, a new secondary decomposition-ensemble (SDE) approach with a cuckoo search algorithm (CSA) is proposed for air cargo forecasting. More specifically, the original air cargo time series is decomposed into several components by an enhanced decomposition formwork , which consists of variational mode decomposition (VMD), sample entropy (SE) and empirical mode decomposition (EMD). Subsequently, the ARIMA and the Elman neural networks (ENN) optimized by CSA are respectively applied to forecast the trend component and the low-frequency components, during which the phase space reconstruction (PSR) is conducted to determine the input structure of neural networks . The final forecasting results are obtained by integrating the predicted values of each component. Besides, the air cargo series from three different airports in China are adopted to validate the performance of our proposed approach and the empirical results show that it is superior to all other benchmark models in terms of the robustness and accuracy.},
  archive      = {J_ASOC},
  author       = {Hongtao Li and Juncheng Bai and Xiang Cui and Yongwu Li and Shaolong Sun},
  doi          = {10.1016/j.asoc.2020.106161},
  journal      = {Applied Soft Computing},
  pages        = {106161},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A new secondary decomposition-ensemble approach with cuckoo search optimization for air cargo forecasting},
  volume       = {90},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). An adaptive dual-population evolutionary paradigm with
adversarial search: Case study on many-objective service consolidation.
<em>ASOC</em>, <em>90</em>, 106160. (<a
href="https://doi.org/10.1016/j.asoc.2020.106160">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Optimizing many conflicting objectives simultaneously is one of the most challenging topics in the multi-criterion decision-making. This paper develops a dual-population co-evolutionary paradigm for solving many-objective service selection problems. It evolves two co-evolving populations separately with different scalarizing functions (SFs) and adversarial search orientations in parallel. In particular, one population, driven by convergence-oriented SF with ideal point, pulls the solutions toward the Pareto front ; the other one, driven by diversity-oriented SF with nadir point, pushes the solutions backward from the nadir point. Accordingly, the search behaviors of the two populations are arguably complement to each other. Moreover, corner solutions and angle-based similarity are employed to enhance the coverage of population as widely as possible, the interaction and collaboration among populations are leveraged by a carefully crafted elitism pairing strategy. A series of experimental studies have been performed on challenging real-world service composition problems . Empirical results have demonstrated the competitiveness of our proposal against the state-of-the-art peers.},
  archive      = {J_ASOC},
  author       = {Jiajun Zhou and Liang Gao and Xifan Yao and Chunjiang Zhang and Felix T.S. Chan and Yingzi Lin},
  doi          = {10.1016/j.asoc.2020.106160},
  journal      = {Applied Soft Computing},
  pages        = {106160},
  shortjournal = {Appl. Soft. Comput.},
  title        = {An adaptive dual-population evolutionary paradigm with adversarial search: Case study on many-objective service consolidation},
  volume       = {90},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Adaptive inertia weight bat algorithm with sugeno-function
fuzzy search. <em>ASOC</em>, <em>90</em>, 106159. (<a
href="https://doi.org/10.1016/j.asoc.2020.106159">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Bat algorithm (BA) turns into the most generally utilized meta-heuristic algorithm to solve the different sort of global optimization problems . In the optimization of continuous data, BA experiences one of the prominent difficulties called premature convergence. In order to tackle premature convergence, this study exhibits a new version of BA called Adaptive inertia weight Bat algorithm with Sugeno-Function Fuzzy Search (ASF-BA). The proposed algorithm ASF-BA brings two major adjustments in the standard BA. Firstly, we incorporated an adaptive inertia weight to boost up the velocity rate of bats effectively. Secondly, we replaced the random searching method of standard BA with Sugeno-Function fuzzy search, which used Sugeno-Function decline curves to dynamically adjust the fitness of each bat according to their own experience and experience of their neighbour bats. We compared ASF-BA with several old fashioned and new fashioned optimization algorithms . ASF-BA is also tested against top hybridized and enhanced version of DE algorithms . The CEC 2017 benchmark (30 real parameter numerical optimization problems ), CEC 2017 ( 28 constrained optimization problems) and 19 additional benchmark problems have been used to examine and compare the performance of ASF-BA with other state of the art variants. Contrasted with the existing BA and other leading variants of BA, DE, and PSO on CEC 2017 constrained and numerical benchmarks, the ASF-BA is excellent to the state-of-art variants of BA, DE, and PSO in terms of stability, convergence speed and solution quality. The ASF-BA sets stable support for resolving optimization problems of intelligent and expert systems. Furthermore, we also examined the performance of proposed ASF-BA for the weight optimization of Feed Forward Neural Networks (FFNN) and compared ASF-BA with Back Propagation Algorithm (BPA), BA and PSO. ASF-BA achieved 94\% of maximum accuracy. The experimental outcomes reveal that the suggested algorithm executed especially reliable and effective than the existing state of the art variants.},
  archive      = {J_ASOC},
  author       = {Hafiz Tayyab Rauf and Sumbal Malik and Umar Shoaib and Muhammad Naeem Irfan and M. Ikramullah Lali},
  doi          = {10.1016/j.asoc.2020.106159},
  journal      = {Applied Soft Computing},
  pages        = {106159},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Adaptive inertia weight bat algorithm with sugeno-function fuzzy search},
  volume       = {90},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Indicator &amp; crowding distance-based evolutionary
algorithm for combined heat and power economic emission dispatch.
<em>ASOC</em>, <em>90</em>, 106158. (<a
href="https://doi.org/10.1016/j.asoc.2020.106158">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Heat and power have become the most indispensable resources. However, the traditional ways of generating power and heat are inefficient and cause high pollution; a CHP (Combined Heat and Power) unit can solve these problems well. In recent years, more attention has been paid to energy conservation and environmental protection, and Combined Heat and Power Economic Emission Dispatch (CHPEED) has become an important multi-objective optimization problem. In this paper, an Indicator &amp; crowding Distance-based Evolutionary Algorithm (IDBEA) is put forward for handling this non-convex and non-linear problem. With consideration of the valve-point effects and power transmission loss, IDBEA is tested on three standard test systems with different types, including four units, five units and seven units. In the experiment, IDBEA is compared with several evolutionary algorithms, the simulation results demonstrate that IDBEA has strong stability and superiority, while the solutions show better convergence and diversity than several typical algorithms.},
  archive      = {J_ASOC},
  author       = {Jiaze Sun and Jiahui Deng and Yang Li},
  doi          = {10.1016/j.asoc.2020.106158},
  journal      = {Applied Soft Computing},
  pages        = {106158},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Indicator &amp; crowding distance-based evolutionary algorithm for combined heat and power economic emission dispatch},
  volume       = {90},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Modified hybrid bat algorithm with genetic crossover
operation and smart inertia weight for multilevel image segmentation.
<em>ASOC</em>, <em>90</em>, 106157. (<a
href="https://doi.org/10.1016/j.asoc.2020.106157">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multilevel thresholding is one of the most commonly used methods in image segmentation . However, the exhaustive search method is computationally expensive for selecting the optimal thresholds. Therefore, a hybrid bat algorithm with genetic crossover operation and smart inertia weight (SGA-BA) is proposed to choose the optimal thresholds. Furthermore, between-class variance (the Otsu method) and Kapur’s entropy are used as objective functions. In the novel SGA-BA, the smart inertia weight balances the SGA-BA’s exploration and exploitation based on the number of iterations and fitness values. Moreover, the local search capability of SGA-BA is strengthened by the crossover operation of the genetic algorithm . Meanwhile, the random vector is replaced by the beta distribution , which updates the frequency of bat in a smart way. The proposed SGA-BA was evaluated by a set of benchmark images with various levels of thresholds. Additionally, SGA-BA was compared with some well-known and recent heuristic algorithms , such as the genetic algorithm (GA), gravitational search algorithm (GSA), particle swarm optimization (PSO), whale optimization algorithm (WOA), improved salp swarm algorithm (LSSA) and basic bat algorithm (BA). The experimental results show that the proposed SGA-BA provides better outcomes than the other algorithms.},
  archive      = {J_ASOC},
  author       = {Xiaofeng Yue and Hongbo Zhang},
  doi          = {10.1016/j.asoc.2020.106157},
  journal      = {Applied Soft Computing},
  pages        = {106157},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Modified hybrid bat algorithm with genetic crossover operation and smart inertia weight for multilevel image segmentation},
  volume       = {90},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A novel foraging algorithm for swarm robotics based on
virtual pheromones and neural network. <em>ASOC</em>, <em>90</em>,
106156. (<a href="https://doi.org/10.1016/j.asoc.2020.106156">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Swarm robotics is an emerging interdisciplinary field that has many potential real-world applications. Swarm robotics aims to produce robust, scalable, and flexible self-organizing behaviors through local interactions from a large number of simple robots. In this paper, a novel pheromone model of swarm foraging behavior is developed based on a neural network . The output of a single neuron corresponds to the density of a pheromone, which diffuses to neighboring neurons through their local connections. A neural network is updated based on the proposed evaporation model. Neural networks can often mimic the dynamics and features of pheromones. Therefore, in this work, we develop an optimization method to determine the key parameters of cooperative foraging based on mathematical modeling . The differential equation variables represent the number of foraging robots assigned different tasks. The solutions of the differential equations represent the dynamics of the foraging behavior. The key parameters that affect task allocation are determined to make optimal decision rules. Simulation experiments are conducted under different foraging scenarios. The experimental results demonstrate the effectiveness of the proposed pheromone model.},
  archive      = {J_ASOC},
  author       = {Yong Song and Xing Fang and Bing Liu and Caihong Li and Yibin Li and Simon X. Yang},
  doi          = {10.1016/j.asoc.2020.106156},
  journal      = {Applied Soft Computing},
  pages        = {106156},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A novel foraging algorithm for swarm robotics based on virtual pheromones and neural network},
  volume       = {90},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Investigating classification supervised learning approaches
for the identification of critical patients’ posts in a healthcare
social network. <em>ASOC</em>, <em>90</em>, 106155. (<a
href="https://doi.org/10.1016/j.asoc.2020.106155">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nowadays, Healthcare Social Networks (HSNs) offer the possibility to enhance patient care and education. However, they also present potential risks for patients due to the possible distribution of poor-quality or wrong information along with their bad interpretation. On one hand doctors and practitioners want to promote the exchange of information among patients about a specific disease, but on the other hand they do not have enough time to read patients’ posts and moderate them when required. In this paper, we investigate and compare different supervised learning classifiers that we adopted for the classification of critical patients’ posts who can trigger the intervention of the medical personnel. In particular, by considering different Bayesian, Linear and Support Vector Machine (SVM) classifiers we analyze their accuracy considering different n-grams datasets preparation approaches in order to identify the best approach for the identification of critical patients’ posts in a Healthcare Social Network.},
  archive      = {J_ASOC},
  author       = {Lorenzo Carnevale and Antonio Celesti and Giacomo Fiumara and Antonino Galletta and Massimo Villari},
  doi          = {10.1016/j.asoc.2020.106155},
  journal      = {Applied Soft Computing},
  pages        = {106155},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Investigating classification supervised learning approaches for the identification of critical patients’ posts in a healthcare social network},
  volume       = {90},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). KASRA: A kriging-based adaptive space reduction algorithm
for global optimization of computationally expensive black-box
constrained problems. <em>ASOC</em>, <em>90</em>, 106154. (<a
href="https://doi.org/10.1016/j.asoc.2020.106154">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Efficient Global Optimization (EGO) methodology over the entire design space can be considerably time-consuming as much as the expensive simulation computer codes on High-multimodal and computationally Expensive Black-box (HEB) constrained problems. This paper introduces a strategy specifically, the Kriging-based Adaptive Space Reduction Algorithm , named KASRA, to enhance the performance of EGO for HEB constrained optimization problems . A new measure is proposed according to the activity of the decision variables to adaptively reduce the size of design intervals centered at the current best solution. The shrunken intervals gradually are expanded to decrease the risk of missing the desirable region. The design sub-spaces are explored based on the weighed constrained expected improvement criterion. The weighting coefficients of exploration and exploitation dynamically are regulated according to the volume ratio of the current hyper-box-shaped region and the original one. The sequential quadratic programming and exponential tunneling algorithms as two local and global optimizers are employed on Kriging-based functions to achieve a more accurate solution at the end of the procedure if necessary. The genetic algorithm with different tuning strategies is used to defeat the extreme time challenge of constructing Kriging-based surrogates. The proposed algorithm is applicable even if there is no feasible point in the initial samples. The efficiency of KASRA is demonstrated on twenty-two mathematical and ten classical engineering benchmark problems. Experimental results and comparative studies confirm that the proposed approach has a promising performance to deal with HEB constrained optimization problems and generally performs better than the competitor methods on most of the benchmark problems.},
  archive      = {J_ASOC},
  author       = {Hossein Akbari and Afshin Kazerooni},
  doi          = {10.1016/j.asoc.2020.106154},
  journal      = {Applied Soft Computing},
  pages        = {106154},
  shortjournal = {Appl. Soft. Comput.},
  title        = {KASRA: A kriging-based adaptive space reduction algorithm for global optimization of computationally expensive black-box constrained problems},
  volume       = {90},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A novel vSLAM framework with unsupervised semantic
segmentation based on adversarial transfer learning. <em>ASOC</em>,
<em>90</em>, 106153. (<a
href="https://doi.org/10.1016/j.asoc.2020.106153">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Significant progress has been made in the field of visual Simultaneous Localization and Mapping (vSLAM) systems. However, the localization accuracy of vSLAM can be significantly reduced in dynamic applications with mobile robots or passengers. In this paper, a novel semantic SLAM framework in dynamic environments is proposed to improve the localization accuracy . We incorporate a semantic segmentation model into the Oriented FAST and Rotated BRIEF-SLAM2 (ORB-SLAM2) system to filter out dynamic feature points, but we encounter one main challenge, i.e. the performance of a segmentation network well-trained with labeled datasets may decrease seriously in a real application without any labeled data due to the inconsistency between the source domain and the target domain. Therefore, we proposed an unsupervised semantic segmentation model with a Residual Neural Network (ResNet) structure, which is trained by the adversarial transfer learning method in the multi-level feature spaces. This work may be the first to perform multi-level feature space adversarial transfer learning for the semantic SLAM task in dynamic environments. In order to evaluate our method, images of indoor scenes from three datasets are used as the source domain, and the dynamic sequences of the TUM dataset are used as the target domain. The extensive experimental results show favorable performance against the state-of-the-art methods in terms of the absolute trajectory accuracy and image semantic segmentation quality.},
  archive      = {J_ASOC},
  author       = {Sheng Jin and Liang Chen and Rongchuan Sun and Seán McLoone},
  doi          = {10.1016/j.asoc.2020.106153},
  journal      = {Applied Soft Computing},
  pages        = {106153},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A novel vSLAM framework with unsupervised semantic segmentation based on adversarial transfer learning},
  volume       = {90},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Financial distress prediction: Regularized sparse-based
random subspace with ER aggregation rule incorporating textual
disclosures. <em>ASOC</em>, <em>90</em>, 106152. (<a
href="https://doi.org/10.1016/j.asoc.2020.106152">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For the sake of risks management , losses reduction, and costs saving, financial distress prediction (FDP) has attracted extensive attention from various communities including academic researchers, industrial practitioners, and government regulators. In addition to the conventional financial information, the textual disclosures regarding companies have received especial concern nowadays and are demonstrated to be effective for FDP. Ensemble methods have become a prevalent research line in the field of FDP incorporating financial and non-financial features. Feature quality is an important factor determining the accuracy in ensemble, however, traditional ensemble methods integrate these different types of features directly and ignore their grouping structures, hence weakening the feature quality and ultimately deteriorating the prediction accuracy. Moreover, although diversity can be obtained by virtue of the randomness of feature sampling in ensemble, the problem is that such randomness leads to the ambiguities among base classifiers , resulting in that the prediction accuracy of each classifier could not be ensured. Having noted these deficiencies, we propose a novel and robust meta FDP framework, which incorporates the feature regularizing module for identifying discriminatory predictive power of multiple features and the probabilistic fusion module for enhancing the aggregation over base classifiers . To validate our proposed regularized sparse-based Random Subspace with Evidential Reasoning rule (RS 2 _ER), we conducted extensive experiments on the datasets collected from the China Security Market Accounting Research Database (CSMARD), and the experimental results indicate that the proposed RS 2 _ER method enables the prediction effectiveness on FDP to be significantly facilitated by dealing with the features grouping property and the ambiguities among base classifiers.},
  archive      = {J_ASOC},
  author       = {Gang Wang and Jingling Ma and Gang Chen and Ying Yang},
  doi          = {10.1016/j.asoc.2020.106152},
  journal      = {Applied Soft Computing},
  pages        = {106152},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Financial distress prediction: Regularized sparse-based random subspace with ER aggregation rule incorporating textual disclosures},
  volume       = {90},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Wind power prediction using a three stage genetic ensemble
and auxiliary predictor. <em>ASOC</em>, <em>90</em>, 106151. (<a
href="https://doi.org/10.1016/j.asoc.2020.106151">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a novel method for accurate wind power prediction by applying computational intelligence approaches while exploiting Auxiliary Predictor (AxP) and Genetic Programming (GP) based ensemble of Neural Networks (AxP-GPNN). The inherent fluctuations in the power generated by wind mills may affect their optimal integration in the electric grid and therefore, accurate prediction is highly desired. To cater these fluctuations and highly nonlinear mapping , we present an ensemble approach, where the auxiliary predictor is constructed with Radial Basis Function (RBF) network and Relevance Vector Machine (RVM) and various neural networks are then employed as base regressors . Use of RVM is based on its established advantages for robust prediction on unseen data to address the overfitting issue in training phase. AxP is used for suitable weight initialization to base predictors and provides initial decision space to base learners. Further, an ensemble of neural networks based on GP is developed which utilizes the base predictions of neural networks as well as the auxiliary information generated by AxP. The GP ensemble based forecasting engine is thus robust to minor variations in the data as compared to the individual base regressors . We also employ information-theoretic feature selection on physical measurements of the wind mills. Results have been extracted in the form of statistical performance indices including mean absolute error , standard deviation error and mean square error . These error measures are compared with the other existing wind power prediction techniques. These results present better wind power estimates and reduced prediction error. Paired t-test for the proposed model with other machine learning based models is carried out for further evaluation. Overall, these comparisons validate the importance of auxiliary predictor in ensemble model of GP and ANNs .},
  archive      = {J_ASOC},
  author       = {Farah Shahid and Asifullah Khan and Aneela Zameer and Junaid Arshad and Kamran Safdar},
  doi          = {10.1016/j.asoc.2020.106151},
  journal      = {Applied Soft Computing},
  pages        = {106151},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Wind power prediction using a three stage genetic ensemble and auxiliary predictor},
  volume       = {90},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Tracking a dynamic invading target by UAV in oilfield
inspection via an improved bat algorithm. <em>ASOC</em>, <em>90</em>,
106150. (<a href="https://doi.org/10.1016/j.asoc.2020.106150">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A novel dynamic invading target tracking method for the oilfield inspection by unmanned aerial vehicle (UAV) is presented in this paper. In this study the quad-rotor UAV is used to track an invading target, because the traditional manual inspection method and fixed-points video monitoring method has some drawbacks such as low efficiency, high cost, blind spot, and so on. A trajectory prediction method for the ground dynamic invading target is firstly proposed to predict the moving trajectory of the invading target. Then, the swarm intelligence based optimization algorithm is used to optimize the tracking trajectory of UAV, which in order to keep the distance between the UAV and the target closing to the desired distance during tracking process. In order to overcome some drawbacks such as easily being fallen into the local optimal solution and poor stability of the optimization, an improved bat algorithm (named FOBA) is proposed to improve the local searching ability of the bat algorithm (BA), which uses a food searching mechanism in the fruit fly optimization algorithm (FOA). Case studies are conducted with the desired distance is 50m between the UAV and the target, and experimental results show that the FOBA algorithm can effectively keep the tracking distance between the UAV and the target being about 55m, which is better than some other methods.},
  archive      = {J_ASOC},
  author       = {Yi’an Wang and Kun Li and Ying Han and Fawei Ge and Wensu Xu and Liang Liu},
  doi          = {10.1016/j.asoc.2020.106150},
  journal      = {Applied Soft Computing},
  pages        = {106150},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Tracking a dynamic invading target by UAV in oilfield inspection via an improved bat algorithm},
  volume       = {90},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Soft sensor modeling of industrial process data using kernel
latent variables-based relevance vector machine. <em>ASOC</em>,
<em>90</em>, 106149. (<a
href="https://doi.org/10.1016/j.asoc.2020.106149">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A composite model integrating latent variables of kernel partial least squares with relevance vector machine (KPLS-RVM) has been proposed to improve the prediction performance of conventional soft sensors when facing industrial processes. First, the latent variables are extracted to cope with the high dimensionality and complex collinearity of nonlinear process data by using KPLS projection. Then, the probabilistic method RVM is used to develop predictive function between latent variables and the output variable. The performance of the proposed method is evaluated through two case studies based on subway indoor air quality (IAQ) data and wastewater treatment processes (WWTP) data, respectively. The results show the superiority of KPLS-RVM in prediction performance over the other counterparts including least squares support vector machine (LSSVM), PLS-LSSVM, PLS-RVM, and KPLS-LSSVM. For the prediction of effluent chemical oxygen demand in WWTP data, the coefficient of determination value of KPLS-RVM has been improved by approximately 7.30-19.65\% in comparison with the other methods.},
  archive      = {J_ASOC},
  author       = {Hongbin Liu and Chong Yang and Mingzhi Huang and ChangKyoo Yoo},
  doi          = {10.1016/j.asoc.2020.106149},
  journal      = {Applied Soft Computing},
  pages        = {106149},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Soft sensor modeling of industrial process data using kernel latent variables-based relevance vector machine},
  volume       = {90},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Requirement ambiguity and fuzziness in large-scale projects:
The problem and potential solutions. <em>ASOC</em>, <em>90</em>, 106148.
(<a href="https://doi.org/10.1016/j.asoc.2020.106148">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In large-scale projects, it often occurs that the product the purchaser ends up receiving – possibly from projects extending over many years – differs from what they expected. The provider usually defends its delivered product and may blame the imprecision and ambiguity of the requirements, defined by the purchaser, as the primary reason for misinterpretation of requirements and resulting deficiencies. This letter relies on game theory to explain this problem including both intentional and unintentional misinterpretation of requirements. The letter also highlights the practical and scientific significance of the problem using two real-world cases and suggests potential tools and techniques from soft computing in order to develop decision support systems to address the problem.},
  archive      = {J_ASOC},
  author       = {Mehdi Rajabi Asadabadi and Elizabeth Chang and Keiran Sharpe},
  doi          = {10.1016/j.asoc.2020.106148},
  journal      = {Applied Soft Computing},
  pages        = {106148},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Requirement ambiguity and fuzziness in large-scale projects: The problem and potential solutions},
  volume       = {90},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Combining information from thresholding techniques through
an evolutionary bayesian network algorithm. <em>ASOC</em>, <em>90</em>,
106147. (<a href="https://doi.org/10.1016/j.asoc.2020.106147">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Segmentation is an important task in image processing because it could affect the performance of other steps in image analysis. One of the most used methods for segmentation is thresholding which can be formulated as an optimization problem , and evolutionary algorithms (EAs) are alternatives commonly applied to solve it. Estimation of Distribution Algorithms (EDAs) is a branch of EAs that explores the search space by building a probabilistic model, such as Bayesian Networks (BNs). In this article is proposed a BN-based EDA for multilevel image segmentation called BNMTH . The proposed approach iteratively selects the combination of thresholding techniques that permits to find the best configuration of thresholds for a digital image, exploring the inter-dependencies between the decision variables (thresholds) and the different techniques. BNMTH is applied over a set of benchmark images and the results of the segmentation are qualitatively analyzed by using the Peak Signal-to-Noise Ratio (PSNR), the Structure Similarity Index (SSIM) and the Feature Similarity Index (FSIM). Besides, a statistical analysis is provided to compare BNMTH with other state-of-the-art optimization algorithms . The results show that BNMTH is a competitive approach for image segmentation, providing accurate results in almost all the cases. Moreover, the segmented images and the histograms show that the classes are accurately generated even in complex conditions.},
  archive      = {J_ASOC},
  author       = {Diego Oliva and Marcella S.R. Martins and Valentín Osuna-Enciso and Erikson Freitas de Morais},
  doi          = {10.1016/j.asoc.2020.106147},
  journal      = {Applied Soft Computing},
  pages        = {106147},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Combining information from thresholding techniques through an evolutionary bayesian network algorithm},
  volume       = {90},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Dynamic and multi-objective reconfiguration of distribution
network using a novel hybrid algorithm with parallel processing
capability. <em>ASOC</em>, <em>90</em>, 106146. (<a
href="https://doi.org/10.1016/j.asoc.2020.106146">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The configuration of the networks leads to significant effect on the power quality factors like power loss, voltage profile, reliability, and networks resiliency. Due to intermittent nature of system parameters, the network configuration should be adjusted adaptively and dynamically; this process needs a fast and precise reconfiguration algorithm . The main challenge of existing algorithms is the efficient tradeoff between speed and accuracy of reconfiguration. Most of methods use wide-area searching algorithms . Hence, their response to dynamic deviations is low in the term of computational speed. In this regard, the reconfiguration methods that have been presented for dynamic purposes, have weak optimization structure and low accuracy. In this paper, a novel hybrid algorithm is proposed for dynamic and multi-objective reconfiguration of the distribution networks by using the parallel processing method and adaptive population approach. The combination of the exchange market algorithm (EMA) and wild goats algorithm (WGA) is implemented, in parallel pools, for enhancing the accuracy and speed of the reconfiguration simultaneously. The adaptive updating of the population size of parallel algorithms increases the convergence speed of the hybrid method and also offers a fast responding approach for dynamic reconfiguration of network. The objective functions intended for reconfiguration are active power loss and reliability indexes. The conducted research proposes an applicable architecture called as improved loop matrix for eliminating defects of the conventional loop matrix method which leads to ensuring the radial structure of network. The proposed method is tested on IEEE 15, 33, 69 and 85-bus standard test systems and the results are compared with literature and base mode of network. The analysis of the comparisons illuminates the superiority of the proposed method in terms of convergence speed, accuracy and processing time.},
  archive      = {J_ASOC},
  author       = {Amirreza Jafari and Hamed Ganjeh Ganjehlou and Farzad Baghal Darbandi and Behnam Mohammadi-Ivatloo and Mehdi Abapour},
  doi          = {10.1016/j.asoc.2020.106146},
  journal      = {Applied Soft Computing},
  pages        = {106146},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Dynamic and multi-objective reconfiguration of distribution network using a novel hybrid algorithm with parallel processing capability},
  volume       = {90},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Optimising convolutional neural networks using a hybrid
statistically-driven coral reef optimisation algorithm. <em>ASOC</em>,
<em>90</em>, 106144. (<a
href="https://doi.org/10.1016/j.asoc.2020.106144">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Convolutional Neural Networks stands at the front of many solutions which deal with computer vision related tasks. The use and the applications of these models are growing unceasingly, as well as the complexity required to deal with bigger and highly complex problems. However, hitting the most suitable model for solving a specific task is not trivial. A very manually intensive and time consuming trial-and-error experimentation is needed in order to find an architecture, hyperparameters and parameters which reach a certain level of performance. Moreover, this process leads to oversized models, diminishing their generalisation capacity. In this paper, we leverage a metaheuristic and a hybridisation process to optimise the reasoning block of CNN models, composed by fully connected and dropout layers, conducting a full reconstruction that leads to lighter models with better performance. Our approach is architecture-independent and operates at the topology, hyperparameters and parameters (connection weights) levels. For that purpose, we have implemented the Hybrid Statistically-driven Coral Reef Optimisation (HSCRO) algorithm as an extension of SCRO, a metaheuristic which does not require to adjust any parameter since they are automatically and dynamically chosen based on the statistical characteristics of the evolution. In addition, a hybridisation process employs the backpropagation algorithm to make a final fine-grained weights adjustment. In the experiments, the VGG-16 model is successfully optimised in two different scenarios (the CIFAR-10 and the CINIC-10 datasets), resulting in a lighter architecture, with an 88\% reduction of the connection weights, but without losing its generalisation performance .},
  archive      = {J_ASOC},
  author       = {Alejandro Martín and Víctor Manuel Vargas and Pedro Antonio Gutiérrez and David Camacho and César Hervás-Martínez},
  doi          = {10.1016/j.asoc.2020.106144},
  journal      = {Applied Soft Computing},
  pages        = {106144},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Optimising convolutional neural networks using a hybrid statistically-driven coral reef optimisation algorithm},
  volume       = {90},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Adaptive repair method for constraint handling in
multi-objective genetic algorithm based on relationship between
constraints and variables. <em>ASOC</em>, <em>90</em>, 106143. (<a
href="https://doi.org/10.1016/j.asoc.2020.106143">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {While evolutionary algorithms are known among the best methods for solving both theoretical and real-world optimization problems, constraint handling is still one of the major concerns. Common constraint handling methods reject or devalue infeasible solutions depending on their distance from the feasible space, even if they dominate feasible solutions. Alternatively, repair methods aim to overcome infeasibility, but they are currently limited to specific types of problems. In this paper, we propose a more generic repair approach to improve efficiency of constraint handling in non-dominance based genetic algorithm . We start by identifying variables which influence each constraint. This information is used to replace variable values that caused constraint violation, using other solutions in the current generation. Repairing is carried out on the solutions that dominate all feasible members of the population, or have the smallest constraint violation. The repair approach is implemented into NSGA-II and tested on one optimization test case and an engineering optimization problem . The latter focuses on structural design of a ship hull girder , involving two conflicting objectives, 94 decision variables and 376 nonlinear constraints. The proposed repairing approach reduces drastically the number of function evaluations needed to find the feasible space, and it leads to faster convergence and better spread of the non-dominated front. Starting from different random populations, the new algorithm finds feasible solutions within one generation, while the original algorithm takes between 7 and 72 generations. Effectiveness of the optimization is analyzed in terms of the hypervolume performance metric. The repairing algorithm obtains significantly better hypervolume values throughout the optimization run. The highest improvements are achieved in the initial phase of the optimization, which is important for the practical design. The new algorithm performs better than two constraint handling approaches from the literature. It also outperforms MOEA/D algorithm in the engineering problem.},
  archive      = {J_ASOC},
  author       = {Faezeh Samanipour and Jasmin Jelovica},
  doi          = {10.1016/j.asoc.2020.106143},
  journal      = {Applied Soft Computing},
  pages        = {106143},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Adaptive repair method for constraint handling in multi-objective genetic algorithm based on relationship between constraints and variables},
  volume       = {90},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). NPrSVM: Nonparallel sparse projection support vector machine
with efficient algorithm. <em>ASOC</em>, <em>90</em>, 106142. (<a
href="https://doi.org/10.1016/j.asoc.2020.106142">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The recently proposed projection twin support vector machine (PTSVM) is an excellent nonparallel classifier. However, PTSVM employs the least-squares loss function to measure its within-class empirical risk, resulting in several drawbacks, such as non-sparseness for decision, sensitivity to outliers, expensive matrix inversion, and inconsistency in the linear and nonlinear models . To alleviate these issues, in this paper, we propose a novel nonparallel sparse projection support vector machine (NPrSVM). Different from the original PTSVM that squeezes the projected values of within-class instances to its own class center, NPrSVM aims to cluster them as much as possible within an insensitive tube. Specifically, our NPrSVM owns the following attractive merits: (i) Benefiting from the L 1 L1 -norm symmetric Hinge loss function , NPrSVM not only enjoys sparseness for decision but also improves robustness to outliers. (ii) The elegant formulation of dual problems in NPrSVM no longer involves the matrix inversion during the training procedure. This greatly saves the computing time compared to PTSVM. (iii) While the nonlinear formulation of PTSVM is not the direct extension of linear PTSVM, the linear and nonlinear versions of our NPrSVM are consistent. (iv) An efficient dual coordinate descent algorithm is further designed for NPrSVM to handle large-scale classification. Finally, the feasibility and effectiveness of NPrSVM are validated by extensive experiments on both synthetic and real-world datasets.},
  archive      = {J_ASOC},
  author       = {Wei-Jie Chen and Yuan-Hai Shao and Chun-Na Li and Yu-Qing Wang and Ming-Zeng Liu and Zhen Wang},
  doi          = {10.1016/j.asoc.2020.106142},
  journal      = {Applied Soft Computing},
  pages        = {106142},
  shortjournal = {Appl. Soft. Comput.},
  title        = {NPrSVM: Nonparallel sparse projection support vector machine with efficient algorithm},
  volume       = {90},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Learning to recommend third-party library migration
opportunities at the API level. <em>ASOC</em>, <em>90</em>, 106140. (<a
href="https://doi.org/10.1016/j.asoc.2020.106140">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The manual migration between different third-party libraries represents a challenge for software developers. Developers typically need to explore both libraries Application Programming Interfaces , along with reading their documentation, in order to locate the suitable mappings between replacing and replaced methods. In this paper, we introduce RAPIM, a machine learning model that recommends mappings between methods from two different libraries. Our model learns from previous migrations, manually performed in mined software systems, and extracts a set of features related to the similarity between method signatures and method textual documentations. We evaluate our model using 8 popular migrations, collected from 57, 447 open-source Java projects. Results show that RAPIM is able to recommend relevant library API mappings with an average accuracy score of 87\%. Finally, we provide the community with an API recommendation web service that could be used to support the migration process.},
  archive      = {J_ASOC},
  author       = {Hussein Alrubaye and Mohamed Wiem Mkaouer and Igor Khokhlov and Leon Reznik and Ali Ouni and Jason Mcgoff},
  doi          = {10.1016/j.asoc.2020.106140},
  journal      = {Applied Soft Computing},
  pages        = {106140},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Learning to recommend third-party library migration opportunities at the API level},
  volume       = {90},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Scalable and customizable benchmark problems for
many-objective optimization. <em>ASOC</em>, <em>90</em>, 106139. (<a
href="https://doi.org/10.1016/j.asoc.2020.106139">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Solving many-objective problems (MaOPs) is still a significant challenge in the multi-objective optimization (MOO) field. One way to measure algorithm performance is through the use of benchmark functions (also called test functions or test suites), which are artificial problems with a well-defined mathematical formulation, known solutions and a variety of features and difficulties. In this paper we propose a parameterized generator of scalable and customizable benchmark problems for MaOPs. It is able to generate problems that reproduce features present in other benchmarks and also problems with some new features. We propose here the concept of generative benchmarking, in which one can generate an infinite number of MOO problems, by varying parameters that control specific features that the problem should have: scalability in the number of variables and objectives, bias, deceptiveness, multimodality, robust and non-robust solutions, shape of the Pareto front , and constraints. The proposed Generalized Position-Distance (GPD) tunable benchmark generator uses the position-distance paradigm, a basic approach to building test functions, used in other benchmarks such as Deb, Thiele, Laumanns and Zitzler (DTLZ), Walking Fish Group (WFG) and others. It includes scalable problems in any number of variables and objectives and it presents Pareto fronts with different characteristics. The resulting functions are easy to understand and visualize, easy to implement, fast to compute and their Pareto optimal solutions are known.},
  archive      = {J_ASOC},
  author       = {Ivan Reinaldo Meneghini and Marcos Antonio Alves and António Gaspar-Cunha and Frederico Gadelha Guimarães},
  doi          = {10.1016/j.asoc.2020.106139},
  journal      = {Applied Soft Computing},
  pages        = {106139},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Scalable and customizable benchmark problems for many-objective optimization},
  volume       = {90},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Understanding the problem space in single-objective
numerical optimization using exploratory landscape analysis.
<em>ASOC</em>, <em>90</em>, 106138. (<a
href="https://doi.org/10.1016/j.asoc.2020.106138">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In benchmarking theory, creating a comprehensive and uniformly distributed set of problems is a crucial first step to designing a good benchmark. However, this step is also one of the hardest, as it can be difficult to determine how to evaluate the quality of the chosen problem set. In this article, we evaluate if the field of exploratory landscape analysis can be used to develop a generalized method of visualizing a set of arbitrary optimization functions. We present a method for visually determining the distribution of problems within a benchmark set using exploratory landscape analysis combined with clustering and t-sne visualization, and evaluate and explain the visualization this methodology produces. The proposed method is evaluated on a set of benchmark problems taken from two well known state-of-the-art real-parameter single objective optimization benchmarks: the CEC Special Sessions and Competitions on Real-Parameter Single Objective optimization, and the GECCO Black-Box Optimization Benchmark workshops. The main goal of this paper is to present an analysis of how exploratory landscape analysis can be used to visualize a benchmark problem set. We show that this method can provide a clear visualization of a benchmark problem set and shows the similarities of the problems in it by placing similar problems visually close together. We also show that the problem sets of the above benchmarks have a somewhat distinct set of problems that do not overlap. In addition, by applying feature selection approaches we show that a number of landscape features provided by state-of-the-art exploratory landscape analysis libraries are redundant and that a large amount of them are not invariant to simple transforms like scaling and shifting, at least when analyzing these two datasets.},
  archive      = {J_ASOC},
  author       = {Urban Škvorc and Tome Eftimov and Peter Korošec},
  doi          = {10.1016/j.asoc.2020.106138},
  journal      = {Applied Soft Computing},
  pages        = {106138},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Understanding the problem space in single-objective numerical optimization using exploratory landscape analysis},
  volume       = {90},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). An empirical assessment of autonomicity for autonomic query
optimizers using fuzzy-AHP technique. <em>ASOC</em>, <em>90</em>,
106137. (<a href="https://doi.org/10.1016/j.asoc.2020.106137">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Quality assurance and evaluation has always been a key cause of concern for software developers. This problem has been further aggravated by the complete dependence of business enterprises, financial institutions and stock markets on computer hardware and software. It is therefore needed to propose and develop such software evaluation and quality assurance techniques that can fit into the business model’s domain and satisfy the customer needs and aspirations. Autonomic computation is an artificial intelligent based approach used to design and develop software systems which can fit into business model and also satisfy customer needs. These systems are built with self-managed policy system. To guarantee their customers a Total Quality Assurance on the business applications being developed, the paper presents some key aspects of domain-specific software and its quality estimation parameter. In this paper, the authors have analyzed the various aspects of quality metrics of autonomic computation suggested by enhanced ISO 9126 quality model . A universally acceptable approach to assure quality for autonomic computing system would be to measure the Autonomicity of a system to determine whether it is autonomic or not. If it is autonomic then “to what extent” is the next question? Autonomicity is an excellent indicator to assure quality of the autonomic software. The approach taken to measure the subjective attribute of Autonomicity is fuzzy theory with Analytic Hierarchy Process (AHP) integrated in it. Human assessment is qualitative and fuzzy technique is best candidate to quantify their opinions. For empirical analysis, three different query optimizers are examined to measure autonomicity. The result of the empirical analysis will be validated using the already proposed results of the research studies. The present study will provide a base for further research in terms of development of applications with autonomic features. It will also help in proposing new metrics for quality characteristics to estimate the overall quality of such application.},
  archive      = {J_ASOC},
  author       = {Pooja Dehraj and Arun Sharma},
  doi          = {10.1016/j.asoc.2020.106137},
  journal      = {Applied Soft Computing},
  pages        = {106137},
  shortjournal = {Appl. Soft. Comput.},
  title        = {An empirical assessment of autonomicity for autonomic query optimizers using fuzzy-AHP technique},
  volume       = {90},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). An integration of a QFD model with fuzzy-ANP approach for
determining the importance weights for engineering characteristics of
the proposed wheelchair design. <em>ASOC</em>, <em>90</em>, 106136. (<a
href="https://doi.org/10.1016/j.asoc.2020.106136">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A wheelchair design with a nested seat back and hand-rest was proposed and ergonomically analyzed, with the objective of decreasing the likelihood of poor and awkward body postures for both the disabled user and his/her companion. The proposed design was validated by integrating a Quality Function Deployment (QFD) framework with a Fuzzy Analytic Network Process (FANP) to determine the degree of importance of the engineering characteristics. In this study, the influence of this integration on determining weights that prioritize engineering characteristics (ECs) was highlighted by taking into consideration the mutual dependence between customer needs (CNs) and ECs and the inner dependence amongst them. This study focused on utilizing FANP methodology, in which triangular fuzzy numbers (TFNs) were used to represent the degree of importance of CNs and ECs, since human judgments of the intensity of preference for these attributes are subjective, vague, and uncertain. Regarding the importance weights for engineering characteristics of the proposed wheelchair design, it has been found that the quality of material scores the highest weight compared to other ECs, with an overall importance weight of 0.43. This result differed from that one obtained using the QFD model without integration, in which the method of design came in first position. The integrated approach proved to be a promising tool in solving fuzzy decision-making problems in different fields and in several applications such as product development and design for ergonomics . As the FANP approach is not popular in the product development/selection field, this study will expand its employment by decision makers in facilitating the evaluation process where there are interrelated factors under an uncertain environment.},
  archive      = {J_ASOC},
  author       = {Mahmoud Z. Mistarihi and Rasha A. Okour and Ahmad A. Mumani},
  doi          = {10.1016/j.asoc.2020.106136},
  journal      = {Applied Soft Computing},
  pages        = {106136},
  shortjournal = {Appl. Soft. Comput.},
  title        = {An integration of a QFD model with fuzzy-ANP approach for determining the importance weights for engineering characteristics of the proposed wheelchair design},
  volume       = {90},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Surrogate models for high performance control systems in
wind-excited tall buildings. <em>ASOC</em>, <em>90</em>, 106133. (<a
href="https://doi.org/10.1016/j.asoc.2020.106133">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {High-performance control systems (HPCSs), including active, semi-active, and hybrid systems, have been demonstrated as promising methods to mitigate a variety of excitations. However, their deployment in the field is still very limited, attributable to reliability concerns in the closed loop configuration. A solution to promote their applicability is the development of an uncertainty-based design procedure, but such solution comes at a high computational cost due to the large number of possible scenarios to consider on both the closed-loop configuration and external load sides. To alleviate the computational demand of such analysis, this paper investigates the use of data-driven surrogate assisted techniques for uncertainty quantification of HPCSs deployed in wind-excited tall buildings . Both a Kriging surrogate and an adaptive wavelet network (AWN) are investigated and compared to map the unknown relationship between structural inputs and responses. The Kriging model exploits an offline batch learning process while the AWN uses an online sequential process. The surrogate models are applied to a 39-story building equipped with semi-active friction devices exposed to wind load and are compared in terms of accuracy and computational time. Two applications of the surrogate models for uncertainty analysis of the case study building are presented. One is for uncertainty quantification and the other for identification of the most influential uncertain variables. Results show that Kriging provides a more accurate representation to map uncertainties to the system response and to quantify the uncertain performance of HPCSs, but that the AWN provides a significantly faster computational alternative. In particular, for a case containing 17 uncertain variables, Kriging found a representation in 3h20, while the AWN converged in 37 min. Under 41 uncertain variables, these metrics increased to 16h20 and 3h22 for Kriging and the AWN, respectively. These representations were leveraged to identify and remove the uncertainties from three key variables yielding high variance in structural response. Results showed that variables identified under Kriging yielded a 34.9\% decrease in variance under 17 uncertain inputs and a 22.9\% decrease in variance under 41 uncertain inputs, while AWN yielded a 29.0\% and 19.8\% decrease, respectively.},
  archive      = {J_ASOC},
  author       = {Laura Micheli and Jonathan Hong and Simon Laflamme and Alice Alipour},
  doi          = {10.1016/j.asoc.2020.106133},
  journal      = {Applied Soft Computing},
  pages        = {106133},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Surrogate models for high performance control systems in wind-excited tall buildings},
  volume       = {90},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A spatially explicit evolutionary algorithm for the spatial
partitioning problem. <em>ASOC</em>, <em>90</em>, 106129. (<a
href="https://doi.org/10.1016/j.asoc.2020.106129">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Spatial optimization seeks optimal allocation or arrangement of spatial units under constraints such as distance, adjacency, contiguity, and pattern. Evolutionary Algorithms (EAs) are well-known optimization heuristics. However, classic EAs, based on a binary problem encoding and bit-operation-based offspring operators, are spatially unaware and do not capture topological and geometric relationships. Unsurprisingly when spatial characteristics are not explicitly considered in the design of EA operators, that EA becomes ineffective because satisfying spatial constraints is computationally expensive. We design and develop novel spatially explicit EA recombination operators , inspired by the path relinking and ejection chain heuristic strategies, that implement crossover and mutation using intelligently guided strategies in a spatially constrained decision space. Our spatial EA approach is general and slots well into the foundational theory of evolutionary algorithms for spatial optimization. We demonstrate improved solution quality and computational performance with a large-scale spatial partitioning application.},
  archive      = {J_ASOC},
  author       = {Yan Y. Liu and Wendy K. Tam Cho},
  doi          = {10.1016/j.asoc.2020.106129},
  journal      = {Applied Soft Computing},
  pages        = {106129},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A spatially explicit evolutionary algorithm for the spatial partitioning problem},
  volume       = {90},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Extracting core answers using the grey wolf optimizer in
community question answering. <em>ASOC</em>, <em>90</em>, 106125. (<a
href="https://doi.org/10.1016/j.asoc.2020.106125">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A rapidly increasing number of users in community question answering has led to an explosive growth of answers. Thus, it is becoming increasingly more difficult to browse all the answers. Choosing a subset of answers arbitrarily will likely lead to cognitive bias and even poor decisions. Reading a set of core answers that can cover most topics of all the answers is a novel method to overcome information overload and facilitates information retrieval. In this paper, the method named AnsExt for extracting core answers using a kind of swarm intelligence algorithm , named the grey wolf optimizer (GWO), is proposed. First, answers are modeled with the biterm topic model, which fits both short and long texts. Then, factors including quality, coverage, redundancy and number in extracting core answers are defined. Two scenarios are modeled with the requirements of quality, coverage and redundancy: extracting the least number of answers and extracting a predefined number of answers. To extract the minimum number of core questions, a binary GWO is used to resolve the single-objective optimal model. The binary multi-objective GWO is constructed to resolve the optimal model, which is used to extract a predefined number of core answers. Extensive experiments are conducted on real datasets. The results show that the proposed method is feasible and performs well.},
  archive      = {J_ASOC},
  author       = {Ming Li and Lisheng Chen and Yueyun Chen and Jun Wang},
  doi          = {10.1016/j.asoc.2020.106125},
  journal      = {Applied Soft Computing},
  pages        = {106125},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Extracting core answers using the grey wolf optimizer in community question answering},
  volume       = {90},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Taxonomical classification of barriers for scaling agile
methods in global software development environment using fuzzy analytic
hierarchy process. <em>ASOC</em>, <em>90</em>, 106122. (<a
href="https://doi.org/10.1016/j.asoc.2020.106122">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Increasingly, software development organizations are scaling agile practices in the global software development (GSD) environment in order to meet the requirements of the quickly changing and regularly developing business environment. The main objectives of this study are to investigate the key barriers and develop a prioritization-based taxonomy of the barriers for scaling agile development in the GSD environment. Total twenty-two barriers were extracted from the available literature and categorized into five categories, i.e. “human resources management”, ‘coordination”, “technology”, “project management”, and “software methodology”. In the next phase, the identified barriers and their categories were further validated using the questionnaire survey. In the final phase, fuzzy-AHP method, a multi-criterion decision making (MCDM) technique, was applied to prioritize and taxonomy of identified barriers and their related categories was designed. The contribution of this study is not limited to investigate the barriers, but it also provides the roadmap to tackle the issues related to the scaling agile methods in the GSD environment.},
  archive      = {J_ASOC},
  author       = {Mohammad Shameem and Rakesh Ranjan Kumar and Mohammad Nadeem and Arif Ali Khan},
  doi          = {10.1016/j.asoc.2020.106122},
  journal      = {Applied Soft Computing},
  pages        = {106122},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Taxonomical classification of barriers for scaling agile methods in global software development environment using fuzzy analytic hierarchy process},
  volume       = {90},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Masked conditional neural networks for sound classification.
<em>ASOC</em>, <em>90</em>, 106073. (<a
href="https://doi.org/10.1016/j.asoc.2020.106073">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The remarkable success of deep convolutional neural networks in image-related applications has led to their adoption also for sound processing. Typically the input is a time–frequency representation such as a spectrogram , and in some cases this is treated as a two-dimensional image. However, spectrogram properties are very different to those of natural images. Instead of an object occupying a contiguous region in a natural image, frequencies of a sound are scattered about the frequency axis of a spectrogram in a pattern unique to that particular sound. Applying conventional convolution neural networks has therefore required extensive hand-tuning, and presented the need to find an architecture better suited to the time–frequency properties of audio. We introduce the ConditionaL Neural Network (CLNN) 1 and its extension, the Masked ConditionaL Neural Network (MCLNN) designed to exploit the nature of sound in a time–frequency representation. The CLNN is, broadly speaking, linear across frequencies but non-linear across time: it conditions its inference at a particular time based on preceding and succeeding time slices, and the MCLNN use a controlled systematic sparseness that embeds a filterbank-like behavior within the network. Additionally, the MCLNN automates the concurrent exploration of several feature combinations analogous to hand-crafting the optimum combination of features for a recognition task. We have applied the MCLNN to the problem of music genre classification, and environmental sound recognition on several music (Ballroom, GTZAN, ISMIR2004, and Homburg), and environmental sound (Urbansound8K, ESC-10, and ESC-50) datasets. The classification accuracy of the MCLNN surpasses neural networks based architectures including state-of-the-art Convolutional Neural Networks and several hand-crafted attempts.},
  archive      = {J_ASOC},
  author       = {Fady Medhat and David Chesmore and John Robinson},
  doi          = {10.1016/j.asoc.2020.106073},
  journal      = {Applied Soft Computing},
  pages        = {106073},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Masked conditional neural networks for sound classification},
  volume       = {90},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Tensor alternating least squares grey model and its
application to short-term traffic flows. <em>ASOC</em>, <em>89</em>,
106145. (<a href="https://doi.org/10.1016/j.asoc.2020.106145">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traffic flow data, as an important data source for the research and development of intelligent transportation systems , contain abundant multi-mode features. In this paper, a high-dimensional multi-mode tensor is used to represent traffic flow data. The Tucker tensor decomposition least squares algorithm is used to establish the tensor alternating least squares GM (1, 1) model by combining the modelling mechanism of the grey classical model GM (1, 1) with the algorithm, and the modelling steps are obtained. To demonstrate the effectiveness of the new model, first, the multi-mode traffic flow data are represented by the tensor model, and the correlation of the traffic flow data is analysed. Second, two short-term traffic flow prediction cases are analysed, and the results show that the performance of the GM (1, 1) model based on the tensor alternating least squares algorithm is obviously better than that of the other models. Finally, the original tensor data and the approximate tensor data during the peak period from 8:00 to 8:30 a.m. for six consecutive Mondays are selected as the experimental data, and the effect of the new model is much better than that of the GM (1, 1) model of the original tensor data.},
  archive      = {J_ASOC},
  author       = {Huiming Duan and Xinping Xiao and Jie Long and Yongzhi Liu},
  doi          = {10.1016/j.asoc.2020.106145},
  journal      = {Applied Soft Computing},
  pages        = {106145},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Tensor alternating least squares grey model and its application to short-term traffic flows},
  volume       = {89},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Constrained design optimization of selected mechanical
system components using rao algorithms. <em>ASOC</em>, <em>89</em>,
106141. (<a href="https://doi.org/10.1016/j.asoc.2020.106141">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The design optimization of mechanical system components, like bearings, pulleys, springs, etc. is an essential issue due to the current competitive market. The performance of any mechanical system depends on the design of their mechanical components. The design optimization of these mechanical components is a difficult task due to the intricate design constraints and mixed type design variables (i.e., continuous, discrete, and integer). This paper explores the performance of Rao algorithms on the design optimization of selected mechanical system components. The designs obtained using Rao algorithms are compared with the designs obtained using other optimization algorithms in previous studies. The comparison of results shows the ability and the efficiency of Rao algorithms for solving complex design optimization problems of mechanical components.},
  archive      = {J_ASOC},
  author       = {R.V. Rao and R.B. Pawar},
  doi          = {10.1016/j.asoc.2020.106141},
  journal      = {Applied Soft Computing},
  pages        = {106141},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Constrained design optimization of selected mechanical system components using rao algorithms},
  volume       = {89},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). An enhanced decentralized artificial immune-based strategy
formulation algorithm for swarms of autonomous vehicles. <em>ASOC</em>,
<em>89</em>, 106135. (<a
href="https://doi.org/10.1016/j.asoc.2020.106135">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This work presents an algorithmic approach to the problem of strategy assignment to the members of a swarm of autonomous vehicles. The proposed methodology draws inspiration from the artificial immune system (AIS), where a large number of antibodies cooperate in order to protect an organism from foreign threats by local exchange of information. The decentralized nature of the methodology does not suffer from problems like the need of a central control unit, the high maintenance costs and the risks associated with having a single point of system failure, which are common to centralized control techniques. Decentralized and distributed optimization schemes employ simple algorithms, which are fast, robust and can run locally on an autonomous unit due to their low processing power requirements. In contrast to standard AIS-based decentralized schemes, the proposed methodology makes use of a dynamic formulation of the available strategies and avoids the possibility of choosing an invalid strategy, which may lead to inferior swarm performance. The methodology is further enhanced by a dual strategy activation decay technique and a blind threat-follow rule. Statistical testing on different case studies based on “enemy search and engage” type scenarios in a simulated environment demonstrates the superior performance of the proposed algorithm against the standard AIS, an enhanced AIS version and a centralized particle swarm optimization (PSO) based methodology.},
  archive      = {J_ASOC},
  author       = {Marios Stogiannos and Alex Alexandridis and Haralambos Sarimveis},
  doi          = {10.1016/j.asoc.2020.106135},
  journal      = {Applied Soft Computing},
  pages        = {106135},
  shortjournal = {Appl. Soft. Comput.},
  title        = {An enhanced decentralized artificial immune-based strategy formulation algorithm for swarms of autonomous vehicles},
  volume       = {89},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Failure mode and effects analysis (FMEA) for risk assessment
based on interval type-2 fuzzy evidential reasoning method.
<em>ASOC</em>, <em>89</em>, 106134. (<a
href="https://doi.org/10.1016/j.asoc.2020.106134">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Failure mode and effect analysis (FMEA) has been widely adopted to define, identity, and remove potential and recognized hazards. As an indicator in traditional FMEA , the risk priority number (RPN) is an effective tool for measuring risk and the calculation of RPN is also very simple. Nevertheless, there are many drawbacks in the conventional FMEA method. It is necessary to seek approaches that can make up for the deficiency of traditional FMEA method and strengthen assessment capability of ranking failure modes according to three relevant risk factors. This paper presents a way to combine interval type-2 fuzzy sets (IT2FSs) with evidential reasoning (ER) method, which is able to overcome some disadvantages of the conventional FMEA approach and deal with uncertainties more efficiently. First, we give a more precise expression of the risk factors in the form of IT2FSs and gain the relative weight of three risk factors. Second, one can judge the failure modes in relation to each risk factors with belief structures. Finally, the ER method is used to combine the belief structures under the weight of the three risk factors. To verify the feasibility of the method, an application for steam valve system is performed and the obtained results show the effectiveness of the method.},
  archive      = {J_ASOC},
  author       = {Jindong Qin and Yan Xi and Witold Pedrycz},
  doi          = {10.1016/j.asoc.2020.106134},
  journal      = {Applied Soft Computing},
  pages        = {106134},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Failure mode and effects analysis (FMEA) for risk assessment based on interval type-2 fuzzy evidential reasoning method},
  volume       = {89},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A benchmark data set for aircraft type recognition from
remote sensing images. <em>ASOC</em>, <em>89</em>, 106132. (<a
href="https://doi.org/10.1016/j.asoc.2020.106132">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Aircraft type recognition from remote sensing images has many civil and military applications. In images obtained with modern technologies such as high spatial resolution remote sensing, even details of aircraft can become visible. With this, the identification of aircraft types from remote sensing images becomes possible. However, the existing methods for this purpose have mostly been evaluated on different data sets and under different experimental settings. This makes it hard to compare their results and judge the progress in the field. Moreover, the data sets used are often not publicly available, which brings difficulties to reproduce the works for fair comparison. This severely limits the progress of research and the state of the art is not entirely clear. To address this problem, we introduce a new benchmark data set for aircraft type recognition from remote sensing images. This data set is called Multi-Type Aircraft Remote Sensing Images (MTARSI), which contains 9’385 images of 20 aircraft types, with complex backgrounds, different spatial resolutions, and complicated variations in pose, spatial location , illumination, and time period. The publicly available MTARSI data set allows researchers to develop more accurate and robust methods for both remote sensing image processing and interpretation analysis of remote sensing object. We also provide a performance analysis of state-of-the-art aircraft type recognition and deep learning approaches on MTARSI, which serves as baseline result on this benchmark.},
  archive      = {J_ASOC},
  author       = {Zhi-Ze Wu and Shou-Hong Wan and Xiao-Feng Wang and Ming Tan and Le Zou and Xin-Lu Li and Yan Chen},
  doi          = {10.1016/j.asoc.2020.106132},
  journal      = {Applied Soft Computing},
  pages        = {106132},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A benchmark data set for aircraft type recognition from remote sensing images},
  volume       = {89},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Blind de-convolution of images degraded by atmospheric
turbulence. <em>ASOC</em>, <em>89</em>, 106131. (<a
href="https://doi.org/10.1016/j.asoc.2020.106131">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Atmospheric turbulence can change the path and direction of light during the imaging of a target in space due to the random motion of the turbulent medium, resulting in severe image distortion. To correct geometric distortion , and reduce spatially and temporally varying blur, this paper proposes a convolutional network for blind deblurring atmospheric turbulence (BDATNet) that includes a feature extraction noise suppression block (FENSB), an asymmetric U-net, and an image reconstruction subnetwork (IRSubnetwork). A deblurring noise suppression block (DNSB) is used instead of the traditional convolution layer for the U-net. The core principle of this model is to suppress noise before deblurring. During convolutional encoding, the FENSB and DNSB can suppress noise and capture rich feature maps. To fuse information obtained from low-level and high-level features, the FENSB and IRSubnetwork are skip-connected to ensure the integrity of the former during image reconstruction. Moreover, the method of gradually increasing the difficulty of data to train the network is used to cause it to gradually converge from simple to complex, so that it can deal with images severely degraded by turbulence. The experimental results of real data and simulation data show that the BDATNet can restore details of the image and sharpen its edges, and can suppress noise.},
  archive      = {J_ASOC},
  author       = {Gongping Chen and Zhisheng Gao and Qiaolu Wang and Qingqing Luo},
  doi          = {10.1016/j.asoc.2020.106131},
  journal      = {Applied Soft Computing},
  pages        = {106131},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Blind de-convolution of images degraded by atmospheric turbulence},
  volume       = {89},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Multi-objective cartesian genetic programming optimization
of morphological filters in navigation systems for visually impaired
people. <em>ASOC</em>, <em>89</em>, 106130. (<a
href="https://doi.org/10.1016/j.asoc.2020.106130">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Navigation systems for Visually Impaired People (VIP) have improved in the last decade, incorporating many features to ensure navigation safety. Such systems often use grayscale depth images to segment obstacles and paths according to distances. However, this approach has the common problem of unknown distances. While this can be solved with good quality morphological filters , these might be too complex and power demanding. Considering navigation systems for VIP rely on limited energy sources that have to run multiple tasks, fixing unknown distance areas without major impacts on power consumption is a definite concern. Multi-objective optimization algorithms might improve filters’ energy efficiency and output quality, which can be accomplished by means of different quality vs. complexity trade-offs. This study presents NSGA2CGP, a multi-objective optimization method that employs the NSGA-II algorithm on top of Cartesian Genetic Programming to optimize morphological filters for incomplete depth images used by navigation systems for VIP. Its goal is to minimize output errors and structuring element complexity, presenting several feasible alternatives combining different levels of filter quality and complexity—both of which affect power consumption . NSGA2CGP-optimized filters were deployed into an actual embedded platform, so as to experimentally measure power consumption and execution time. We also propose two new fitness functions based on existing approaches from literature. Results showed improvements in visual quality, performance, speed and power consumption, thanks to our proposed error function, proving NSGA2CGP as a solid method for developing and evolving efficient morphological filters.},
  archive      = {J_ASOC},
  author       = {Antonio Miguel Batista Dourado and Emerson Carlos Pedrino},
  doi          = {10.1016/j.asoc.2020.106130},
  journal      = {Applied Soft Computing},
  pages        = {106130},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Multi-objective cartesian genetic programming optimization of morphological filters in navigation systems for visually impaired people},
  volume       = {89},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). An effective automatic system deployed in agricultural
internet of things using multi-context fusion network towards crop
disease recognition in the wild. <em>ASOC</em>, <em>89</em>, 106128. (<a
href="https://doi.org/10.1016/j.asoc.2020.106128">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Automatic crop disease recognition in the wild is a challenging topic in modern intelligent agriculture due to the appearance variances and cluttered background among crop diseases. To overcome these obstacles, the popular methods are to design a Convolutional Neural Network (CNN) model that extracts visual features and identifies crop disease images based on these features. These methods work well on laboratory environment under simple background but achieve low accuracy and poor robustness in processing the raw images captured from practical fields that contain inevitable noises. In this case, Internet of Things (IoT) is attracting increasing attention, with many alternatives to collect high-level contextual information that helps modern recognition system to effectively identify crop diseases in the wild. Motivated by the usefulness of agricultural IoT, a deep learning system using a novel approach named Multi-Context Fusion Network (MCFN), is developed to be deployed in agricultural IoT towards practical crop disease recognition in the wild. Our MCFN firstly adopts a standard CNN backbone to extract highly discriminative and robust visual features from over 50, 000 in-field crop disease samples. Next, we exploit contextual features collected from image acquisition sensors as prior information to assist crop disease classification and reduce false positives in our presented ContextNet. Finally, a deep fully connected network is designed to fuse visual features as well as contextual features and output the crop disease prediction. Experimental results on 77 common crop diseases captured in our newly built domain specific dataset show that MCFN with the deep fusion model outperforms the state-of-the-art methods in wild crop disease recognition, and achieves a good identification accuracy of 97.5\%.},
  archive      = {J_ASOC},
  author       = {Yushan Zhao and Liu Liu and Chengjun Xie and Rujing Wang and Fangyuan Wang and Yingqiao Bu and Shunxiang Zhang},
  doi          = {10.1016/j.asoc.2020.106128},
  journal      = {Applied Soft Computing},
  pages        = {106128},
  shortjournal = {Appl. Soft. Comput.},
  title        = {An effective automatic system deployed in agricultural internet of things using multi-context fusion network towards crop disease recognition in the wild},
  volume       = {89},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Texture selection for automatic music genre classification.
<em>ASOC</em>, <em>89</em>, 106127. (<a
href="https://doi.org/10.1016/j.asoc.2020.106127">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Music Genre Classification is the problem of associating genre-related labels to digitised music tracks. It has applications in the organisation of commercial and personal music collections. Often, music tracks are described as a set of timbre-inspired sound textures. A subset of the sound textures is often selected to represent the entire track. In this paper, we evaluate the impact of texture selection on automatic music genre classification. Although previous work has selected textures by linear downsampling, no extensive work has been done to evaluate how texture selection benefits music genre classification. We also present a novel texture selector based on K-Means aimed to identify diverse sound textures within each track. Our results show that capturing texture diversity within tracks is important towards improving classification performance. Our results also indicate that our K-Means based texture selector is able to achieve significant improvements over the baseline with fewer textures per track than the other texture selectors evaluated. We also show that using multiple texture representations allows further opportunities for feature selection to improve classification performance.},
  archive      = {J_ASOC},
  author       = {Juliano Henrique Foleis and Tiago Fernandes Tavares},
  doi          = {10.1016/j.asoc.2020.106127},
  journal      = {Applied Soft Computing},
  pages        = {106127},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Texture selection for automatic music genre classification},
  volume       = {89},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Reconstruction method with the learned regularizer for
imaging problems in electrical capacitance tomography. <em>ASOC</em>,
<em>89</em>, 106126. (<a
href="https://doi.org/10.1016/j.asoc.2020.106126">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The electrical capacitance tomography (ECT) is an attractive tomography method for process monitoring applications across different tasks and domains, but low quality images deteriorate its reliability and applicability. In order to change this situation, a potent method is developed to reduce reconstruction artifacts in this study. The learned regularization (LR) from a new ensemble learning method that integrates the advantageous properties of the least square support vector machine (LSSVM) method, the random projection (RP) method and the sparse matrix regression (SMR) solved by the differential evolution (DE) algorithm is proposed to increase the elasticity in mining and utilizing the prior knowledge. A potent model for imaging is devised by simultaneously taking advantage of the domain knowledge of the reconstruction targets (RTs) and the LR. The iterative split Bregman (ISB) is extended into a simple but powerful solver for the devised model by leveraging the forward backward splitting (FBS) algorithm and the soft thresholding (ST) algorithm to solve sub-problems efficiently. The imaging method proposed in the study is validated to successfully work on a series of testing tasks with the significant improvement of the reconstruction quality (RQ) over the popular imaging methods.},
  archive      = {J_ASOC},
  author       = {J. Lei and Q.B. Liu},
  doi          = {10.1016/j.asoc.2020.106126},
  journal      = {Applied Soft Computing},
  pages        = {106126},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Reconstruction method with the learned regularizer for imaging problems in electrical capacitance tomography},
  volume       = {89},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Multiple scale self-adaptive cooperation mutation
strategy-based particle swarm optimization. <em>ASOC</em>, <em>89</em>,
106124. (<a href="https://doi.org/10.1016/j.asoc.2020.106124">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Particle Swarm Optimization (PSO) algorithm has lately received great attention due to its powerful search capacity and simplicity in implementation. However, previous studies have demonstrated that PSO still suffers from two key drawbacks of premature convergence and slow convergence, especially when dealing with multi-modal optimization problems . In order to address these two issues, we propose a multiple scale self-adaptive cooperative mutation strategy-based particle swarm optimization algorithm (MSCPSO) in this paper. In the proposed approach, we adopt multi-scale Gaussian mutations with different standard deviations to promote the capacity of sufficiently searching the whole solution space. In the adopted multi-scale mutation strategy, large-scale mutation can make populations explore the global solution space and rapidly locate the better solution area at the early stage, thus avoiding the premature convergence and simultaneously speeding up the convergence, while small-scale mutation can allow the populations to more accurately exploit the local best solution area during the later stage, thus improving the accuracy of final solution. In order to guarantee the convergence speed while avoiding premature convergence, the standard deviations for multi-scale Gaussian mutations would be reduced with the increase of iterations, which can make populations pay more attention to local accurate solution exploitation during the later evolution stage and consequently speed up the convergence. In addition, the threshold for each dimension to execute mutation is also dynamically adjusted according to its previous mutation frequency, which can allow MSCPSO to better balance the global and local search capacities, thus avoiding premature convergence without reducing convergence speed. The extensive experimental results on various benchmark optimization problems demonstrate that the proposed approach is superior to other existing PSO techniques with good robustness.},
  archive      = {J_ASOC},
  author       = {Xinmin Tao and Wenjie Guo and Qing Li and Chao Ren and Rui Liu},
  doi          = {10.1016/j.asoc.2020.106124},
  journal      = {Applied Soft Computing},
  pages        = {106124},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Multiple scale self-adaptive cooperation mutation strategy-based particle swarm optimization},
  volume       = {89},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Generating kranok patterns with an interactive evolutionary
algorithm. <em>ASOC</em>, <em>89</em>, 106121. (<a
href="https://doi.org/10.1016/j.asoc.2020.106121">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The kranok pattern is a classical Thai pattern, often seen in the ornamentation of Thai religious artifacts such as Tripiṭaka cabinets, temple doors and coffins. All Thai trainee artists must practice drawing the kranok pattern because it is a fundamental motif in Thai traditional decorative art. Individual skilled artists and instructors have their own preferred ways of drawing the pattern, and their styles differ in ornamental details. Nevertheless, all make use of similar basic structures. Most trainees learn a range of styles by observing experienced artists. After exploring the methods of drawing used by experts and published in textbooks, we designed an algorithm for automatically generating kranok patterns. We used an interactive evolutionary algorithm (IEA) to improve the aesthetic appeal of the generated patterns in response to users’ feedback. The aim of our work was not to replace artists with machines, but to enhance human artistic expression. Specifically, the work aimed to help users without artistic skills to create kranok patterns in their own style. The algorithm facilitated the creation of a variety of personalized kranok patterns – diverse in their expressive curvature, refinement and proportions – that were satisfying to the varying preferences of a range of users. We also analyzed the proposed algorithm’s behavior in terms of its convergence to generate specific shapes. The proposed method was examined by 28 respondents (27 Thai and 1 foreign) who were selected to include representatives of both sexes as well as experts in both Thai drawing and evolutionary algorithms. The results from our questionnaires showed that all respondents were satisfied with the generated kranok patterns: one respondent was ‘completely satisfied’, seventeen ‘very satisfied’, seven ‘moderately satisfied’, and three ‘slightly satisfied’.},
  archive      = {J_ASOC},
  author       = {Nutthanon Leelathakul and Sunisa Rimcharoen},
  doi          = {10.1016/j.asoc.2020.106121},
  journal      = {Applied Soft Computing},
  pages        = {106121},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Generating kranok patterns with an interactive evolutionary algorithm},
  volume       = {89},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A clustering and dimensionality reduction based evolutionary
algorithm for large-scale multi-objective problems. <em>ASOC</em>,
<em>89</em>, 106120. (<a
href="https://doi.org/10.1016/j.asoc.2020.106120">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {When solving multi-objective problems (MOPs) with a large number of variables, analysis of the linkage between decision variables is maybe useful for avoiding “the curse of dimensionality”. In this work, a clustering and dimensionality reduction based evolutionary algorithm for large-scale multi-objective problems is suggested, which focuses on clustering decision variables into two categories and then utilizes a dimensionality reduction approach to get a lower dimensional representation for those variables that affect the convergence of the evolution. The interdependence analysis is carried out next aiming to decompose the convergence variables into a number of subcomponents that are easier to be tackled. The algorithm presented in this article is promising on a series of test functions, and the outcome of these experiments reveal that our suggested algorithm is able to prominently enhance the performance; meanwhile it can save computing costs to a large extent compared with some latest evolutionary algorithms (EAs). In addition, the proposed algorithm can be extended to solve MOPs with dimensions up to 5000, with a good performance obtained.},
  archive      = {J_ASOC},
  author       = {Ruochen Liu and Rui Ren and Jin Liu and Jing Liu},
  doi          = {10.1016/j.asoc.2020.106120},
  journal      = {Applied Soft Computing},
  pages        = {106120},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A clustering and dimensionality reduction based evolutionary algorithm for large-scale multi-objective problems},
  volume       = {89},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Constructing a health indicator for roller bearings by using
a stacked auto-encoder with an exponential function to eliminate
concussion. <em>ASOC</em>, <em>89</em>, 106119. (<a
href="https://doi.org/10.1016/j.asoc.2020.106119">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most deep-learning models, especially stacked auto-encoders (SAEs), have been used in recent years for the diagnosis of faults in rotating machinery . However, very few studies have reported on health indicator (HI) construction by using SAEs in deep learning . SAEs have a good feature-extraction ability when several hidden layers are used to reconstruct the original input. In this study, we first introduce a method to reduce dependence on prior knowledge that is based on SAEs and enables extraction of the preliminary degradation trend from the bearing’s frequency domain directly. Second, to construct the final HI and improve the monotonicity of the indicators, an exponential function is used to eliminate global severe vibration after an SAE has extracted the preliminary degradation trend. To prove the effect of our presented method, some other HI construction models, such as root mean square , kurtosis, approximate entropy, permutations entropy, empirical mode decomposition-singular value decomposition, K-means/K-medoids, and various time–frequency fusion indicators are used for comparison. Moreover, to prove that the exponential-function effect exceeds other severe vibration-eliminating methods, examples of the latter methods such as exponentially weighted moving-average and outlier detection are used for comparative analysis. Finally, the results shows that our proposed model is better than the above-mentioned existing models.},
  archive      = {J_ASOC},
  author       = {Fan Xu and Zhelin Huang and Fangfang Yang and Dong Wang and Kwok Leung Tsui},
  doi          = {10.1016/j.asoc.2020.106119},
  journal      = {Applied Soft Computing},
  pages        = {106119},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Constructing a health indicator for roller bearings by using a stacked auto-encoder with an exponential function to eliminate concussion},
  volume       = {89},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Preventing epidemic spreading in networks by community
detection and memetic algorithm. <em>ASOC</em>, <em>89</em>, 106118. (<a
href="https://doi.org/10.1016/j.asoc.2020.106118">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Targeted immunization is a commonly used strategy in preventing epidemic spreading. Traditional methods immunize targeted nodes based on specific global or local network structures instead of optimization. In this paper, we propose a novel community-based immunization strategy to select targeted immunization nodes based on optimization. The proposed algorithm consists of three steps. First, community structures are discovered by community detection algorithm . Second, possible candidates are narrowed down based on the structure properties of community. Finally, a novel memetic algorithm is designed to select immunization nodes from the candidate set. In the final step, epidemic threshold is adopted as objective function and then targeted immunization is formulated as an optimization problem . To solve this optimization problem, a novel memetic algorithm is designed. Experimental results demonstrate that the proposed algorithm outperforms some state-of-the-art immunization algorithms in optimizing epidemic threshold.},
  archive      = {J_ASOC},
  author       = {Shanfeng Wang and Maoguo Gong and Wenfeng Liu and Yue Wu},
  doi          = {10.1016/j.asoc.2020.106118},
  journal      = {Applied Soft Computing},
  pages        = {106118},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Preventing epidemic spreading in networks by community detection and memetic algorithm},
  volume       = {89},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Non linear system identification using kernel based
exponentially extended random vector functional link network.
<em>ASOC</em>, <em>89</em>, 106117. (<a
href="https://doi.org/10.1016/j.asoc.2020.106117">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Identification of nonlinear systems finds extensive applications in control design and stability analysis. To identify complex nonlinear systems , the neural network has drawn the attention of many researchers due to its broad application area. In this paper, an improved identification method based on Kernel Exponentially Extended Random Vector Functional Link Network (KERVFLN) has been proposed for nonlinear system identification . Good generalization capability, fast learning speed, simple architecture and the direct connection between input and output nodes along with non linear enhancement nodes with random weights of traditional Random Vector Functional Link Network (RVFLN) are very essential to industrial applications. To avoid the selection of the number of hidden nodes and hidden mapping function, kernel function has been used in this paper to increase the stability. The input is extended using trigonometric expansion which increases the accuracy of the algorithm when ever there is a sudden random change. In case of KERVFLN the number of enhancement nodes and its corresponding activation function need not to be known if its corresponding kernel function is given. To verify the accuracy of the proposed model, some benchmark Monte Carlo simulations and one SISO system are carried out through simulation study and the obtained results are compared with some established techniques such as original RVFLN, Extreme Learning Machine (ELM), and Least Mean Square (LMS). The efficiency of the proposed technique has been tested with the real time data set as well. The prediction accuracy of the proposed method KERVFLN is higher than the normal RVFLN for different nonlinear systems which is clear from the performance evaluation section.},
  archive      = {J_ASOC},
  author       = {Tatiana Chakravorti and Penke Satyanarayana},
  doi          = {10.1016/j.asoc.2020.106117},
  journal      = {Applied Soft Computing},
  pages        = {106117},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Non linear system identification using kernel based exponentially extended random vector functional link network},
  volume       = {89},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A long-term prediction approach based on long short-term
memory neural networks with automatic parameter optimization by
tree-structured parzen estimator and applied to time-series data of NPP
steam generators. <em>ASOC</em>, <em>89</em>, 106116. (<a
href="https://doi.org/10.1016/j.asoc.2020.106116">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Developing an accurate and reliable multi-step ahead prediction model is a key problem in many Prognostics and Health Management (PHM) applications. Inevitably, the further one attempts to predict into the future, the harder it is to achieve an accurate and stable prediction due to increasing uncertainty and error accumulation. In this paper, we address this problem by proposing a prediction model based on Long Short-Term Memory (LSTM), a deep neural network developed for dealing with the long-term dependencies in time-series data. Our proposed prediction model also tackles two additional issues. Firstly, the hyperparameters of the proposed model are automatically tuned by a Bayesian optimization algorithm , called Tree-structured Parzen Estimator (TPE). Secondly, the proposed model allows assessing the uncertainty on the prediction. To validate the performance of the proposed model, a case study considering steam generator data acquired from different French nuclear power plants (NPPs) is carried out. Alternative prediction models are also considered for comparison purposes.},
  archive      = {J_ASOC},
  author       = {Hoang-Phuong Nguyen and Jie Liu and Enrico Zio},
  doi          = {10.1016/j.asoc.2020.106116},
  journal      = {Applied Soft Computing},
  pages        = {106116},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A long-term prediction approach based on long short-term memory neural networks with automatic parameter optimization by tree-structured parzen estimator and applied to time-series data of NPP steam generators},
  volume       = {89},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A new fuzzy multi-hop clustering protocol with automatic
rule tuning for wireless sensor networks. <em>ASOC</em>, <em>89</em>,
106115. (<a href="https://doi.org/10.1016/j.asoc.2020.106115">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In today’s world, a major challenge is to conserve and make optimal use of energy. This is a critical matter in wireless sensor networks due to their wide application in different areas. More importantly, scant attention has been paid to the use of node energy for certain applications in such networks. This study used the Shuffled Frog Leaping Algorithm (SFLA) to propose a Fuzzy Multi-hop clustering protocol (FMSFLA). The SFLA is used for automated configuration and optimization of the rule-base table in a fuzzy inference system and five adjustable parameters in two phases, i.e. Cluster Head (CH) selection and parent selection, based on application features. The proposed protocol (FMSFLA) considers effective parameters including energy, distance from the base station (BS), the number of neighboring nodes, real node distance from the BS, mean route load, delay, overlap, and the problem of hot spots , to achieve the best application-based performance. The FMSFLA includes rounds, in each round the phases of CH selection, parent selection, cluster formation, and steady state are performed. In the CH selection phase, CHs are selected from candidate nodes based on the fuzzy output and energy threshold (i.e. a control parameter) with respect to the overlap rate of adjacent CHs. In our protocol, the parent selection phase began by determining the levels of CHs in the network. At the end of this phase, the parent of each CH is determined on the basis of the greatest fuzzy output based on application. In the cluster formation phase, the clusters are formed on the basis of the determined CHs. Finally, the information received by CHs is sent through their parents to the BS in the steady state phase. The FMSFLA is evaluated against the LEACH, LEACH-EP, LEACH-FL, ASLPR, SIF, and ERA protocols in terms of the number of alive nodes, received packets, and cluster heads in addition to their appropriate distribution rates and other parameters pertaining to the network lifetime and protocol scalability using three application-oriented scenarios. According to the simulation results, the FMSFLA functioned far better than the other protocols in all scenarios with respect to goals and application features.},
  archive      = {J_ASOC},
  author       = {Fakhrosadat Fanian and Marjan Kuchaki Rafsanjani},
  doi          = {10.1016/j.asoc.2020.106115},
  journal      = {Applied Soft Computing},
  pages        = {106115},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A new fuzzy multi-hop clustering protocol with automatic rule tuning for wireless sensor networks},
  volume       = {89},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Dynamic histogram equalization for contrast enhancement for
digital images. <em>ASOC</em>, <em>89</em>, 106114. (<a
href="https://doi.org/10.1016/j.asoc.2020.106114">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Magnetic Resonance Imaging (MRI) is an efficient tool, produced by applying radio waves and magnetic fields which is being useful in the diagnosis of various diseases like cancer, epilepsy and stroke etc. The quality of the resulting image is needed to be enhanced because it is challenging for the specialists to investigate. Modified Histogram Equalization on Fuzzy based Improved Particle Swarm Optimization (FIPSO) is proposed for Dynamic Histogram Equalization which resolves this problem through image contrast enhancement. The details of an images are captured by smoothing and it uses Gaussian function to distribute pixel intensity to nearest pixel. It uses normal distribution and here blur is removed by applying Non subsampled Contourlet Transform. Then local maxima are calculated to extract dark and bright pixel values. The smoothed images are fuzzified with TSK ( Takagi-Sugeno-Kang ) model and it provides importance to all the local maxima intervals. An Improved particle swarm optimization (IPSO) algorithm is obtained by combining Galactic Swarm Optimization (GSO) with PSO which equalizes histogram of an image. FIPSO algorithm is used to the minimum contrast images of MRI brain images. Non-subsampled Contourlet transform (NSCT) based modified histogram equalization enhances image contrast. Here IPSO generates optimum values and these value are used to calculate cumulative distribution function in histogram equalization. The quality measures demonstrate that the current equalization technique attains highest performance against existing techniques in terms of brightness and contrast.},
  archive      = {J_ASOC},
  author       = {Boyina Subrahmanyeswara Rao},
  doi          = {10.1016/j.asoc.2020.106114},
  journal      = {Applied Soft Computing},
  pages        = {106114},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Dynamic histogram equalization for contrast enhancement for digital images},
  volume       = {89},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Remaining useful life prediction using multi-scale deep
convolutional neural network. <em>ASOC</em>, <em>89</em>, 106113. (<a
href="https://doi.org/10.1016/j.asoc.2020.106113">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate and reliable remaining useful life (RUL) assessment result provides decision-makers valuable information to take suitable maintenance strategy to maximize the equipment usage and avoid costly failure. The conventional RUL prediction methods include model-based and data-driven. However, with the rapid development of modern industries, the physical model is becoming less capable of describing sophisticated systems, and the traditional data-driven methods have limited ability to learn sophisticated features. To overcome these problems, a multi-scale deep convolutional neural network (MS-DCNN) which have powerful feature extraction capability due to its multi-scale structure is proposed in this paper. This network constructs a direct relationship between Condition Monitoring (CM) data and ground-RUL without using any prior information. The MS-DCNN has three multi-scale blocks (MS-BLOCKs), where three different sizes of convolution operations are put on each block in parallel. This structure improves the network’s ability to learn complex features by extracting features of different scales. The developed algorithm includes three stages: data pre-processing, model training, and RUL prediction. After the min–max normalization pre-processing, the data is sent to the MS-DCNN network for parameter training directly, and the associated RUL value can be estimated base on the learned representations. Regularization helps to improve prediction accuracy and alleviate the overfitting problem. We evaluate the method on the available modular aero-propulsion system simulation data (C-MAPSS dataset) from NASA. The results show that the proposed method achieves good prognostics performance compared with other network architectures and state-of-the-art methods. RUL prediction result is obtained precisely without increasing the calculation burden.},
  archive      = {J_ASOC},
  author       = {Han Li and Wei Zhao and Yuxi Zhang and Enrico Zio},
  doi          = {10.1016/j.asoc.2020.106113},
  journal      = {Applied Soft Computing},
  pages        = {106113},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Remaining useful life prediction using multi-scale deep convolutional neural network},
  volume       = {89},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). An infinite-resolution grid snapping technique based on
fuzzy theory. <em>ASOC</em>, <em>89</em>, 106112. (<a
href="https://doi.org/10.1016/j.asoc.2020.106112">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In ordinary CAD systems with pointing devices, users input geometric objects by inputting their feature points one by one through pointing and dragging operations. Thus, each of the feature points is snapped into place interactively, and the geometric objects are aligned on the specified grid as a result. In sketch-based CAD systems with pen input devices, users simply draw a freehand stroke that the system recognizes as a geometric object, and then the system must automatically snap all of the feature points to the grid by a batch process . In this case, it is not easy for the user to set up the grid resolution in advance, because the appropriate resolution changes according to the drawing. Therefore, a multi-resolution fuzzy grid snapping (MFGS) technique has been proposed. In MFGS, the appropriate snapping resolution is dynamically selected in a multi-resolution grid system according to the roughness of the drawing manner. However, the multiplicity of the grid resolutions in MFGS is limited to a finite number, and grid snapping outside this range of resolutions cannot be appropriately handled. In this paper, we propose infinite-resolution fuzzy grid snapping (IFGS), in which there is an infinite number of grid resolutions, and experimentally demonstrate that IFGS effectively resolves the problems of MFGS.},
  archive      = {J_ASOC},
  author       = {Ten Watanabe and Tomohito Yoshikawa and Tomohiko Ito and Yuto Miwa and Takeshi Shibata and Sato Saga},
  doi          = {10.1016/j.asoc.2020.106112},
  journal      = {Applied Soft Computing},
  pages        = {106112},
  shortjournal = {Appl. Soft. Comput.},
  title        = {An infinite-resolution grid snapping technique based on fuzzy theory},
  volume       = {89},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Multi-objective railway alignment optimization considering
costs and environmental impacts. <em>ASOC</em>, <em>89</em>, 106105. (<a
href="https://doi.org/10.1016/j.asoc.2020.106105">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With increasing transportation requirements in mountainous regions , railways are encroaching ever more on environmentally-sensitive areas in those regions. Selecting an economical and eco-friendly railway alignment can effectively minimize negative impacts on mountain environments while also reducing costs. To this end, this paper formulates the alignment design problem as a multi-objective optimization model, which includes both economic and environmental objectives. Two new quantitative indexes for measuring environmental impacts are proposed to reflect the degree of vegetation destruction and soil erosion. A multi-objective optimization method based on the particle swarm optimization (PSO) algorithm is proposed for seeking non-dominated solutions. New update mechanisms for dealing with the multi-objective optimization problem are devised. A local repair algorithm based on a customized crossover operator is designed to save promising alignment alternatives during the search process. Two real-world cases are used to demonstrate the effectiveness of the proposed method. The results show that it can trade off the economic and environmental objectives and bypass all the pre-specified forbidden zones , thus providing designers a set of non-dominated alignment alternatives.},
  archive      = {J_ASOC},
  author       = {Hong Zhang and Hao Pu and Paul Schonfeld and Taoran Song and Wei Li and Jie Wang and Xianbao Peng and Jianping Hu},
  doi          = {10.1016/j.asoc.2020.106105},
  journal      = {Applied Soft Computing},
  pages        = {106105},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Multi-objective railway alignment optimization considering costs and environmental impacts},
  volume       = {89},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A constrained multi-objective evolutionary algorithm based
on decomposition and dynamic constraint-handling mechanism.
<em>ASOC</em>, <em>89</em>, 106104. (<a
href="https://doi.org/10.1016/j.asoc.2020.106104">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Constrained multi-objective optimization problems (CMOPs) are common in real-world engineering application , and are difficult to solve because of the conflicting nature of the objectives and many constraints. Some constrained multi-objective evolutionary algorithms (CMOEAs) have been developed for CMOPs, but they still suffer from the problems of easily getting trapped into local optimal solutions and low convergence. This paper introduces a multi-objective evolutionary algorithm based on decomposition and dynamic constraint-handling mechanism (MOEA/D-DCH) to tackle this issue. Firstly, the dynamic constraint-handling mechanism divides the search modes into the unconstrained search mode and the constrained search mode, which are dynamically adjusted by the generation number and the proportion of feasible solutions in the population. This mechanism could lead to a faster convergence than the traditional constraint-handling mechanisms. For the constrained search mode, an improved epsilon constraint-handling method is used to enhance the diversity of the population. Then, an individual update mechanism based on the best feasible solution of each sub-problem is designed to update the feasible individuals for maintaining the convergence of the feasible solutions. Finally, MOEA/D-DCH dynamically regulates the parameters of the differential evolution operator to enhance the local search ability. Experiments on 21 benchmark test functions are conducted to test MOEA/D-DCH and five other typical CMOEAs. Meanwhile, a real-world problem is employed to evaluate the practical performance of MOEA/D-DCH. MOEA/D-DCH achieves significantly better results than the other five algorithms on most of the test problems. The results indicate the effectiveness and competitiveness of MOEA/D-DCH for solving CMOPs.},
  archive      = {J_ASOC},
  author       = {Yongkuan Yang and Jianchang Liu and Shubin Tan},
  doi          = {10.1016/j.asoc.2020.106104},
  journal      = {Applied Soft Computing},
  pages        = {106104},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A constrained multi-objective evolutionary algorithm based on decomposition and dynamic constraint-handling mechanism},
  volume       = {89},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A general framework and guidelines for benchmarking
computational intelligence algorithms applied to forecasting problems
derived from an application domain-oriented survey. <em>ASOC</em>,
<em>89</em>, 106103. (<a
href="https://doi.org/10.1016/j.asoc.2020.106103">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Benchmarking computational intelligence algorithms provides valuable knowledge for selecting the best or, at least, the proper algorithm for a certain problem. The experimental results of the computational intelligence techniques applications in various domains, as well as the comparative studies that were reported in the literature can be analyzed and synthesized as development strategies for new successful applications of CI algorithms. Starting from an application domain-oriented survey of selected recently reported research work, the paper presents a general benchmarking framework applicable to computational intelligence algorithms and a set of guidelines for the selection of the best or more suitable CI algorithm for solving forecasting problems. Our approach proposes the integration of software and knowledge engineering best practice towards CI benchmarking, being a computational intelligence engineering methodology. The framework uses two knowledge bases, one for the application domain and one for the CI algorithms, providing heuristic knowledge for a more informed and efficient benchmarking, a case base in which solved problems are recorded with their solution and lessons that were learned, and a knowledge-based problem instance features selection. Some examples of how to apply the framework for problems of forecasting in seismology, environmental protection, hydrology and energy are also discussed. We point out that the framework might be implemented as a software tool (e.g. a decision support system) or as a tool suite. The main conclusion of our research work is that the integration of the derived knowledge from an application domain-oriented survey into the general benchmarking framework along with the set of guidelines for best or proper CI algorithms selection can improve significantly the forecasting accuracy and the response time, in case of real time forecasters.},
  archive      = {J_ASOC},
  author       = {Mihaela Oprea},
  doi          = {10.1016/j.asoc.2020.106103},
  journal      = {Applied Soft Computing},
  pages        = {106103},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A general framework and guidelines for benchmarking computational intelligence algorithms applied to forecasting problems derived from an application domain-oriented survey},
  volume       = {89},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Two cluster validity indices for the LAMDA clustering
method. <em>ASOC</em>, <em>89</em>, 106102. (<a
href="https://doi.org/10.1016/j.asoc.2020.106102">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The learning algorithm and multivariable data analysis (LAMDA) is an algorithm to group quantitative and qualitative data, applying self-learning and/or directed learning . Usually, LAMDA automatically generates classes by assigning the best data partition to a class. To evaluate the data partitions generated by LAMDA, the internal evaluation is used to find the optimal number of clusters. For the LAMDA algorithm, the cluster validity (CV) is the most popular index which is based on inter-class contrast (ICC). However, other indices have not been defined for LAMDA and a comparative analysis is required to evaluate its performance. In this paper, two metrics called cluster validity index based on granulation error and the ratio of the distance (CVGED) and cluster validity index based on the ratios of covariance and distance (CVCOD) are proposed. Such indices are compared with the CV and ICC indices for two experiments: using a databases repository and selected open data and experimental laboratory data. According to the main results, CVGED and CVCOD have a better performance in compactness, separation, and coefficient of variation than ICC and CV for most of the selected repository databases but the accuracy is limited for the four indices. Nevertheless, CVCOD improves the quality of data partition when the open data and experimental laboratory data are used.},
  archive      = {J_ASOC},
  author       = {Javier Fernando Botía Valderrama and Diego José Luis Botía Valderrama},
  doi          = {10.1016/j.asoc.2020.106102},
  journal      = {Applied Soft Computing},
  pages        = {106102},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Two cluster validity indices for the LAMDA clustering method},
  volume       = {89},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Hybrid change point detection for time series via support
vector regression and CUSUM method. <em>ASOC</em>, <em>89</em>, 106101.
(<a href="https://doi.org/10.1016/j.asoc.2020.106101">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study considers the change point testing problem regarding time series based on the location and scale-based cumulative sum (LSCUSUM) test constructed with the residuals obtained from support vector regression (SVR)-autoregressive moving average (ARMA) models. For this, we first estimate the model parameters in SVR–ARMA models from a training time series sample, in which a long AR model is fitted to the data to obtain residuals. We then use these as initial values of the error terms in SVR–ARMA ( p , q p, q ) models and obtain the forecasting values recursively until the updated error terms converge to a certain limit. Finally, we select an optimal order of p , q p, q with the root mean square error (RMSE) and use the forecasting errors from this selected model as the residuals for constructing the LSCUSUM test. Monte Carlo simulations are performed to evaluate the validity of the test. A real data example is provided for illustration.},
  archive      = {J_ASOC},
  author       = {Sangyeol Lee and Sangjo Lee and Miteum Moon},
  doi          = {10.1016/j.asoc.2020.106101},
  journal      = {Applied Soft Computing},
  pages        = {106101},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Hybrid change point detection for time series via support vector regression and CUSUM method},
  volume       = {89},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A FE model updating technique based on SAP2000-OAPI and
enhanced SOS algorithm for damage assessment of full-scale structures.
<em>ASOC</em>, <em>89</em>, 106100. (<a
href="https://doi.org/10.1016/j.asoc.2020.106100">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Although many existing damage diagnosis techniques based on the combination of optimization algorithms and finite element model updating have been studied and demonstrated to be promising, there are still some limitations that need to be improved to enhance their performance for the large and complex structures. In this regard, the present article proposes a FE model updating technique based on the existing commercial software SAP2000-OAPI and an enhanced symbiotic organisms search (ESOS) algorithm for damage assessment of full-scale structures. First, to overcome the complexities of FE simulation , the FE model of monitored structure is built in SAP2000 software for analyzing the dynamic behavior of the structure. Then, the damage assessment of the structure is set up in the form of an optimization problem in which the objective function is established based on a combination of flexibility matrix and modal assurance criterion (MAC). An improved version of SOS algorithm, called ESOS algorithm, is adopted to solve this optimization problem for detecting and quantifying any stiffness degradation induced by damage. To perform the iterative optimization task automatically, a link between MATLAB and SAP2000 is created by using the OAPI feature of SAP2000. Finally, the numerical investigations on two full-scale structures with considering measurement noise and sparse measured data demonstrate the feasibility of the proposed technique in predicting the actual damage sites and their severities.},
  archive      = {J_ASOC},
  author       = {D. Dinh-Cong and T. Nguyen-Thoi and Duc T. Nguyen},
  doi          = {10.1016/j.asoc.2020.106100},
  journal      = {Applied Soft Computing},
  pages        = {106100},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A FE model updating technique based on SAP2000-OAPI and enhanced SOS algorithm for damage assessment of full-scale structures},
  volume       = {89},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A novel reinforcement learning based grey wolf optimizer
algorithm for unmanned aerial vehicles (UAVs) path planning.
<em>ASOC</em>, <em>89</em>, 106099. (<a
href="https://doi.org/10.1016/j.asoc.2020.106099">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unmanned aerial vehicles (UAVs) have been used in wide range of areas, and a high-quality path planning method is needed for UAVs to satisfy their applications. However, many algorithms reported in the literature may not feasible or efficient, especially in the face of three-dimensional complex flight environment. In this paper, a novel reinforcement learning based grey wolf optimizer algorithm called RLGWO has been presented for solving this problem. In the proposed algorithm, the reinforcement learning is inserted that the individual is controlled to switch operations adaptively according to the accumulated performance. Considering that the proposed algorithm is designed to serve for UAVs path planning , four operations have been introduced for each individual: exploration, exploitation, geometric adjustment, and optimal adjustment. In addition, the cubic B-spline curve is used to smooth the generated flight route and make the planning path be suitable for the UAVs. The simulation experimental results show that the RLGWO algorithm can acquire a feasible and effective route successfully in complicated environment.},
  archive      = {J_ASOC},
  author       = {Chengzhi Qu and Wendong Gai and Maiying Zhong and Jing Zhang},
  doi          = {10.1016/j.asoc.2020.106099},
  journal      = {Applied Soft Computing},
  pages        = {106099},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A novel reinforcement learning based grey wolf optimizer algorithm for unmanned aerial vehicles (UAVs) path planning},
  volume       = {89},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Cooperative coevolution of real predator robots and virtual
robots in the pursuit domain. <em>ASOC</em>, <em>89</em>, 106098. (<a
href="https://doi.org/10.1016/j.asoc.2020.106098">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The pursuit domain, or predator–prey problem is a standard testbed for the study of coordination techniques. In spite that its problem setup is apparently simple, it is challenging for the research of the emerged swarm intelligence . This paper presents a particle swarm optimization (PSO) based cooperative coevolutionary algorithm for the (predator) robots, called CCPSO-R, where real and virtual robots coexist in an evolutionary algorithm (EA). Virtual robots sample and explore the vicinity of the corresponding real robots and act as their action spaces, while the real robots consist of the real predators who actually pursue the prey robot without fixed behavior rules under the immediate guidance of the fitness function, which is designed in a modular manner with very limited domain knowledge. In addition, kinematic limits and collision avoidance considerations are integrated into the update rules of robots. Experiments are conducted on a scalable swarm of predator robots with 4 types of preys, the results of which show the reliability, generality, and scalability of the proposed CCPSO-R. Comparison with a representative dynamic path planning based algorithm Multi-Agent Real-Time Pursuit (MAPS) further shows the effectiveness of CCPSO-R. Finally, the codes of this paper are public available at: https://github.com/LijunSun90/pursuitCCPSOR .},
  archive      = {J_ASOC},
  author       = {Lijun Sun and Chao Lyu and Yuhui Shi},
  doi          = {10.1016/j.asoc.2020.106098},
  journal      = {Applied Soft Computing},
  pages        = {106098},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Cooperative coevolution of real predator robots and virtual robots in the pursuit domain},
  volume       = {89},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Towards the use of vector based GP to predict physiological
time series. <em>ASOC</em>, <em>89</em>, 106097. (<a
href="https://doi.org/10.1016/j.asoc.2020.106097">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Prediction of physiological time series is frequently approached by means of machine learning (ML) algorithms. However, most ML techniques are not able to directly manage time series, thus they do not exploit all the useful information such as patterns, peaks and regularities provided by the time dimension. Besides advanced ML methods such as recurrent neural network that preserve the ordered nature of time series, a recently developed approach of genetic programming , VE-GP, looks promising on the problem in analysis. VE-GP allows time series as terminals in the form of a vector, including new strategies to exploit this representation. In this paper we compare different ML techniques on the real problem of predicting ventilation flow from physiological variables with the aim of highlighting the potential of VE-GP. Experimental results show the advantage of applying this technique in the problem and we ascribe the good performances to the ability of properly catching meaningful information from time series.},
  archive      = {J_ASOC},
  author       = {Irene Azzali and Leonardo Vanneschi and Illya Bakurov and Sara Silva and Marco Ivaldi and Mario Giacobini},
  doi          = {10.1016/j.asoc.2020.106097},
  journal      = {Applied Soft Computing},
  pages        = {106097},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Towards the use of vector based GP to predict physiological time series},
  volume       = {89},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). InOvIn: A fuzzy-rough approach for detecting overlapping
communities with intrinsic structures in evolving networks.
<em>ASOC</em>, <em>89</em>, 106096. (<a
href="https://doi.org/10.1016/j.asoc.2020.106096">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Real-world networks, such as biological, biomedical and social networks, often contain overlapping and intrinsic communities. More significantly, such networks are growing or evolving over time, which leads to a continuous alteration of community structures. Detecting overlapping community together with intrinsic structures in evolving scenarios is one of the challenging tasks. Prior researches are limited in handling all the events together while designing a community detector. We propose an integrated solution, InOvIn ( In trinsic Ov erlapping Community Detection in In cremental Networks), for detecting overlapping, non-overlapping and intrinsic communities in evolving networks. Herein, we have explored a rough-fuzzy clustering approach for overlapping community detection. Fuzzy membership helps in soft decision making for deciding membership of a node towards a target community. While rough boundary of the communities decides the shared membership of a node in multiple communities. The node degree density variation measure is used to discover the existence of intrinsic community within a community. We assess the performance of InOvIn in light of twelve (12) popular real-world social networks. It may be noted that available real-world networks are lacking in labeled overlapping and intrinsic communities. Hence, we synthetically generate six (06) networks with both overlapping and intrinsic communities. We demonstrate the superiority of InOvIn over contemporary community detection methods using ten (10) different statistical assessment parameters. Interestingly, for the first time, our method detects intrinsic communities in PolBooks and Word Adjacencies networks.},
  archive      = {J_ASOC},
  author       = {Keshab Nath and Swarup Roy and Sukumar Nandi},
  doi          = {10.1016/j.asoc.2020.106096},
  journal      = {Applied Soft Computing},
  pages        = {106096},
  shortjournal = {Appl. Soft. Comput.},
  title        = {InOvIn: A fuzzy-rough approach for detecting overlapping communities with intrinsic structures in evolving networks},
  volume       = {89},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Learning rules for sugeno ANFIS with parametric conjunction
operations. <em>ASOC</em>, <em>89</em>, 106095. (<a
href="https://doi.org/10.1016/j.asoc.2020.106095">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The paper presents a Sugeno Adaptive Neuro-Fuzzy Inference System with parametric conjunction operations architecture , ANFIS-CX. The advantages of using parametric conjunction operations in fuzzy models are discussed, and learning rules for system identification with such operations are proposed. These learning strategies can include steepest descent gradient , differential evolution and least square estimation algorithms for tuning antecedent, conjunction, and consequent parameters, respectively. The results of system identification by parameter tuning of conjunction operations in addition to or instead of parameter tuning of the input membership functions are presented. Simulation results show that parameter training in conjunction operations, composed of four basic t-norms, significantly improves the approximation capability of fuzzy models.},
  archive      = {J_ASOC},
  author       = {Prometeo Cortés-Antonio and Ildar Batyrshin and Alfonso Martínez-Cruz and Luis A. Villa-Vargas and Marco A. Ramírez-Salinas and Imre Rudas and Oscar Castillo and Herón Molina-Lozano},
  doi          = {10.1016/j.asoc.2020.106095},
  journal      = {Applied Soft Computing},
  pages        = {106095},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Learning rules for sugeno ANFIS with parametric conjunction operations},
  volume       = {89},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Quantum based whale optimization algorithm for wrapper
feature selection. <em>ASOC</em>, <em>89</em>, 106092. (<a
href="https://doi.org/10.1016/j.asoc.2020.106092">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose the Quantum Whale Optimization Algorithm (QWOA) for feature selection, which is an amalgamation of the Quantum Concepts and the Whale Optimization Algorithm (WOA). The proposed method enhances the exploratory and exploitation power of the classical WOA, with the use of quantum bit representation of the individuals of the population and the quantum rotation gate operator as a variation operator. Modified mutation and crossover operators are also introduced for quantum-based exploration, shrinking and spiral movement of the whales in the proposed QWOA. The efficacy of the proposed method is compared with that of the conventional WOA and with well-known evolutionary, swarm and quantum algorithms with fourteen datasets from diversified domains. Experimental results demonstrate the superior performance of the proposed QWOA method. Statistical tests also demonstrate the significantly better performance of the QWOA in comparison to eight well-known meta-heuristic algorithms.},
  archive      = {J_ASOC},
  author       = {R.K. Agrawal and Baljeet Kaur and Surbhi Sharma},
  doi          = {10.1016/j.asoc.2020.106092},
  journal      = {Applied Soft Computing},
  pages        = {106092},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Quantum based whale optimization algorithm for wrapper feature selection},
  volume       = {89},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A granular deep learning approach for predicting energy
consumption. <em>ASOC</em>, <em>89</em>, 106091. (<a
href="https://doi.org/10.1016/j.asoc.2020.106091">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes a granular deep learning approach consisting of maximal overlap discrete wavelet transformation (MODWT) and long short-term memory (LSTM) network for predicting the energy consumption of different sectors at macro levels. Input features are first evaluated using Boruta algorithm-based feature selection model. MODWT is then used to decompose the energy consumption time series to alienate the linear and nonlinear components. The LSTM network, a deep learning tool, is used to make predictions on individual sub-series at a granular level . The final prediction is obtained by aggregating the forecasts obtained on decomposed components. Statistical analyses rationalize the efficacy and superiority of the proposed hybrid framework over six other well-known prediction algorithms. Monthly data for residential, commercial, industrial and transportation sectors of the USA have been taken for analyses. It is observed that energy consumption in commercial and transportation sectors are easier to predict than residential and industrial sectors.},
  archive      = {J_ASOC},
  author       = {Rabin K. Jana and Indranil Ghosh and Manas K. Sanyal},
  doi          = {10.1016/j.asoc.2020.106091},
  journal      = {Applied Soft Computing},
  pages        = {106091},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A granular deep learning approach for predicting energy consumption},
  volume       = {89},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Selection of eco-friendly cities in turkey via a hybrid
hesitant fuzzy decision making approach. <em>ASOC</em>, <em>89</em>,
106090. (<a href="https://doi.org/10.1016/j.asoc.2020.106090">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Environmental pollution can be defined as the alteration and deterioration of the natural structure and composition of the environment. Industrialization and population density in cities increase environmental pollution. Today, environmental pollution, a common problem in all countries, has reached dimensions that threaten nature and human health. In this context, this study focuses on the selection of eco-friendly cities in Turkey according to criteria such as average PM 10 values at air quality measurement stations, forest area per km 2 , and percentage of population receiving waste services, using the hesitant fuzzy linguistic term set (HFLTS)-based additive ratio assessment (ARAS) method. The multi-criteria HFLTS method is used to determine the weights assigned to environmental criteria. The ARAS method is used to obtain the final ranking of 81 cities in Turkey. Empirical results demonstrate that the proposed approach is viable in selecting eco-friendly cities.},
  archive      = {J_ASOC},
  author       = {Aslı Çalış Boyacı},
  doi          = {10.1016/j.asoc.2020.106090},
  journal      = {Applied Soft Computing},
  pages        = {106090},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Selection of eco-friendly cities in turkey via a hybrid hesitant fuzzy decision making approach},
  volume       = {89},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). An adaptive framework against android privilege escalation
threats using deep learning and semi-supervised approaches.
<em>ASOC</em>, <em>89</em>, 106089. (<a
href="https://doi.org/10.1016/j.asoc.2020.106089">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The immense popularity of Android makes it a primary target of malicious attackers and developers which brings a significant threat from malicious applications for android users through the escalation of the abuse of android permissions and inter-component communication (ICC) mechanism. Therefore, protecting android users from malicious developers and applications is crucial for Android market and communities. As malicious applications can hide their malicious behavior and change the behaviors frequently by abusing the android’s ICC mechanism and related vulnerabilities, it is a challenging task to identify them accurately before it becomes a prevalent reason for users’ privacy and data breach. Therefore, it is essential to develop such a malware detection engine that will ensure zero-day detection. In this research, we propose an adaptive framework which can learn the behavior of malware from the usage of permissions and their escalations. For our adaptive framework, we proposed two different detection models using deep learning and semi-supervised approaches. The proposed detection models can extract knowledge from unlabeled apps to identify the new malicious behavior using the unsupervised training nature of deep learning and clustering techniques and their integration to the supervised detection engine. Thus, our adaptive framework learns about new malicious apps and their behavior without supervised labeling by manual expert and can ensure zero-day protection. The proposed detection models have been tested on a real mobile malware test-bed and data set. The Experimental results show that the deep learning and semi-supervised based models achieve 99.024\% of accuracies, more effective for zero-day protection and outperform other existing supervised detection engines.},
  archive      = {J_ASOC},
  author       = {Shaila Sharmeen and Shamsul Huda and Jemal Abawajy and Mohammad Mehedi Hassan},
  doi          = {10.1016/j.asoc.2020.106089},
  journal      = {Applied Soft Computing},
  pages        = {106089},
  shortjournal = {Appl. Soft. Comput.},
  title        = {An adaptive framework against android privilege escalation threats using deep learning and semi-supervised approaches},
  volume       = {89},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Navigational analysis of multiple humanoids using a hybrid
regression-fuzzy logic control approach in complex terrains.
<em>ASOC</em>, <em>89</em>, 106088. (<a
href="https://doi.org/10.1016/j.asoc.2020.106088">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the current work, a hybrid navigational control architecture combining regression analysis with fuzzy logic control has been proposed for smooth and hassle-free motion planning of humanoids. In the proposed hybrid scheme, sensory information regarding obstacle distances are initially supplied to the regression controller, and an interim turning angle is obtained as the preliminary output based on the preloaded training pattern of the regression model. In the next phase, interim turning angle is again supplied to the fuzzy controller to generate the ultimate turning angle which eventually guides the humanoid to take a safe direction of turn while avoiding any obstacle present in the work environment. The working of the developed hybrid model is validated through simulation and real-time environments, and satisfactory results have been obtained from comparisons of selected navigational parameters along with a minimal percentage of deviations. To avoid possible chances of inter-collision for navigation of multiple humanoids in a common platform, a Petri-Net model has been integrated with the developed hybrid control scheme. Finally, the developed motion planning model is also assessed against another existing navigational controller, and significant performance enhancement is obtained.},
  archive      = {J_ASOC},
  author       = {Priyadarshi Biplab Kumar and Manoj Kumar Muni and Dayal R. Parhi},
  doi          = {10.1016/j.asoc.2020.106088},
  journal      = {Applied Soft Computing},
  pages        = {106088},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Navigational analysis of multiple humanoids using a hybrid regression-fuzzy logic control approach in complex terrains},
  volume       = {89},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Dynamic granularity selection based on local weighted
accuracy and local likelihood ratio. <em>ASOC</em>, <em>89</em>, 106087.
(<a href="https://doi.org/10.1016/j.asoc.2020.106087">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Granular computing aims to develop a granular view for interpreting and solving problems, in which granularity selection is a key problem and has received extensive attention in recent years. Existing studies select the same granularity for all samples. In fact, different samples may prefer to different granularities. To address this issue, dynamic granularity selection is proposed in this paper. Namely, granularity selection is considered with respect to specific sample. Two indices, denoted as local weighted accuracy and local likelihood ratio, are introduced to compute the weight of granularity. Subsequently, an algorithm called DGS − − LWA-LLS is given for dynamic granularity selection, in which the granularity with the largest weight is considered to be optimal The weight of granularity is related to specific sample, thus the weights of a granularity may be different with different samples. Consequently, different granularities will be selected with respect to different samples. Experiments were carried out based on neighborhood granularity to explain the necessity of granularity selection and to validate the rationality and effectiveness of DGS − − LWA-LLS.},
  archive      = {J_ASOC},
  author       = {Lei-Jun Li and Mei-Zheng Li and Ju-Sheng Mi and Bin Xie},
  doi          = {10.1016/j.asoc.2020.106087},
  journal      = {Applied Soft Computing},
  pages        = {106087},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Dynamic granularity selection based on local weighted accuracy and local likelihood ratio},
  volume       = {89},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Multivariable grey prediction evolution algorithm: A new
metaheuristic. <em>ASOC</em>, <em>89</em>, 106086. (<a
href="https://doi.org/10.1016/j.asoc.2020.106086">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The theoretical foundation of the grey prediction system, proposed by Deng J. in 1982, is built on the fact that appropriate conversion can transform unordered data to series data with an approximate exponential law under certain conditions. Inspired by the grey prediction theory, this paper introduces a novel evolutionary algorithm based on the multivariable grey prediction model MGM(1, n), called MGPEA. The proposed MGPEA considers the population series of an evolutionary algorithm as a time series. It first transforms the population data to series data with an approximate exponential law and then forecasts its next population using MGM(1, n). Philosophically, MGPEA implements the optimizing process by forecasting the development trend of the genetic information chain of a population sequence. The performance of MGPEA is validated on CEC2005 benchmark functions , CEC2014 benchmark functions and a test suite composed of five engineering constrained design problems. The comparative experiments show the effectiveness and superiority of MGPEA. The proposed MGPEA could be regard as a case of constructing metaheuristics by using the grey prediction model. It is hoped that this design idea leads to more metaheuristics inspired by other prediction models.},
  archive      = {J_ASOC},
  author       = {Xinlin Xu and Zhongbo Hu and Qinghua Su and Yuanxiang Li and Jianhua Dai},
  doi          = {10.1016/j.asoc.2020.106086},
  journal      = {Applied Soft Computing},
  pages        = {106086},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Multivariable grey prediction evolution algorithm: A new metaheuristic},
  volume       = {89},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). An easy-to-use real-world multi-objective optimization
problem suite. <em>ASOC</em>, <em>89</em>, 106078. (<a
href="https://doi.org/10.1016/j.asoc.2020.106078">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Although synthetic test problems are widely used for the performance assessment of evolutionary multi-objective optimization algorithms, they are likely to include unrealistic properties which may lead to overestimation/underestimation. To address this issue, we present a multi-objective optimization problem suite consisting of 16 bound-constrained real-world problems. The problem suite includes various problems in terms of the number of objectives, the shape of the Pareto front , and the type of design variables. 4 out of the 16 problems are multi-objective mixed-integer optimization problems. We provide Java, C, and Matlab source codes of the 16 problems so that they are available in an off-the-shelf manner. We examine an approximated Pareto front of each test problem. We also analyze the performance of six representative evolutionary multi-objective optimization algorithms on the 16 problems. In addition to the 16 problems, we present 8 constrained multi-objective real-world problems.},
  archive      = {J_ASOC},
  author       = {Ryoji Tanabe and Hisao Ishibuchi},
  doi          = {10.1016/j.asoc.2020.106078},
  journal      = {Applied Soft Computing},
  pages        = {106078},
  shortjournal = {Appl. Soft. Comput.},
  title        = {An easy-to-use real-world multi-objective optimization problem suite},
  volume       = {89},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Fuzzy dissimilarity color histogram equalization for
contrast enhancement and color correction. <em>ASOC</em>, <em>89</em>,
106077. (<a href="https://doi.org/10.1016/j.asoc.2020.106077">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many statistical histogram-based methods perform intensity transformation on gray levels in the statistical histogram. This may cause over-enhancement due to dominating portions of the histogram. Various methods tackle this problem by overwhelming the dominating portions and improving the inferior components. Though, this may change the natural appearances of the image and results in degraded visual quality. In order to attenuate such limitations, an efficient method called Fuzzy Dissimilarity Adaptive Histogram Equalization with Gamma Correction (FDAHE-GC) algorithm is proposed. In this work, a Fuzzy Dissimilarity Histogram (FDH) is obtained from the neighborhood characteristics of an intensity. An intensity mapping function, constructed from FDH is applied to enhance the contrast and natural characteristics of an image. Finally, the gamma correction is employed to enhance the dark regions. In order to tune the fine details and to improve visual appearance of an image, the proposed FDAHE-GC algorithm is applied to the intensity value of HSI space. The performance of the presented method is evaluated with different existing methods using image quality assessment tools such as entropy, Colorfulness (C), Hue Deviation Index (HDI), Saturation, Contrast Enhancement Factor (CEF) and Gradient (G). The investigational results tested on standard benchmark test images with visual inspection shows the superiority of the proposed FDAHE-GC algorithm.},
  archive      = {J_ASOC},
  author       = {Magudeeswaran Veluchamy and Bharath Subramani},
  doi          = {10.1016/j.asoc.2020.106077},
  journal      = {Applied Soft Computing},
  pages        = {106077},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Fuzzy dissimilarity color histogram equalization for contrast enhancement and color correction},
  volume       = {89},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Multi-objective path planning of an autonomous mobile robot
using hybrid PSO-MFB optimization algorithm. <em>ASOC</em>, <em>89</em>,
106076. (<a href="https://doi.org/10.1016/j.asoc.2020.106076">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The main aim of this paper is to solve a path planning problem for an autonomous mobile robot in static and dynamic environments. The problem is solved by determining the collision-free path that satisfies the chosen criteria for shortest distance and path smoothness. The proposed path planning algorithm mimics the real world by adding the actual size of the mobile robot to that of the obstacles and formulating the problem as a moving point in the free-space. The proposed algorithm consists of three modules. The first module forms an optimized path by conducting a hybridized Particle Swarm Optimization-Modified Frequency Bat (PSO-MFB) algorithm that minimizes distance and follows path smoothness criteria. The second module detects any infeasible points generated by the proposed hybrid PSO-MFB Algorithm by a novel Local Search (LS) algorithm integrated with the hybrid PSO-MFB algorithm to be converted into feasible solutions. The third module features obstacle detection and avoidance (ODA), which is triggered when the mobile robot detects obstacles within its sensing region, allowing it to avoid collision with obstacles. The simulation results indicate that this method generates an optimal feasible path even in complex dynamic environments and thus overcomes the shortcomings of conventional approaches such as grid methods. Moreover, compared to recent path planning techniques, simulation results show that the proposed hybrid PSO-MFB algorithm is highly competitive in terms of path optimality .},
  archive      = {J_ASOC},
  author       = {Fatin H. Ajeil and Ibraheem Kasim Ibraheem and Mouayad A. Sahib and Amjad J. Humaidi},
  doi          = {10.1016/j.asoc.2020.106076},
  journal      = {Applied Soft Computing},
  pages        = {106076},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Multi-objective path planning of an autonomous mobile robot using hybrid PSO-MFB optimization algorithm},
  volume       = {89},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A wildfire warning system applied to the state of acre in
the brazilian amazon. <em>ASOC</em>, <em>89</em>, 106075. (<a
href="https://doi.org/10.1016/j.asoc.2020.106075">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we present a dynamic wildfire warning map that combines both spatial and weather information. In particular, our wildfire early warning model is obtained by aggregating two indexes called wildfire risk and wildfire danger. The wildfire risk index, which is based on georeferenced features such as altitude and forest type, measures the fuel necessary for a wildfire to start at a certain location on a map. The wildfire danger uses weather conditions to yield temporal information concerning the possibility of a wildfire to spread. Machine learning techniques and fuzzy logic operations are used to determine the wildfire risk and danger indexes from available data. Although both wildfire risk and wildfire danger indexes can be used separately, using concepts from fuzzy logic, they can be combined to yield a wildfire warning system that takes into account both weather and static information. We illustrate the wildfire early warning model by considering weather and geographical data for the state of Acre.},
  archive      = {J_ASOC},
  author       = {I.D.B. Silva and M.E. Valle and L.C. Barros and J.F.C.A. Meyer},
  doi          = {10.1016/j.asoc.2020.106075},
  journal      = {Applied Soft Computing},
  pages        = {106075},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A wildfire warning system applied to the state of acre in the brazilian amazon},
  volume       = {89},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Chaos-based vortex search algorithm for solving inverse
kinematics problem of serial robot manipulators with offset wrist.
<em>ASOC</em>, <em>89</em>, 106074. (<a
href="https://doi.org/10.1016/j.asoc.2020.106074">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Vortex Search (VS) algorithm is a single-solution-based optimization algorithm that requires the high maximum number of iterations (NOI) to solve optimization problems . In this study, two methods were proposed to reduce the required maximum NOI of the VS algorithm. These methods are based on using ten chaos maps with the VS algorithm and provide improvements in the exploration and exploitation abilities of the algorithm for reducing the required maximum NOI. Ten chaos-based VS algorithms (CVSs) were obtained by combining these methods with the VS algorithm. The performances of the CVS algorithms were tested by fifty benchmark functions . The results were evaluated in terms of some statistical values and a pairwise statistical test, Wilcoxon Signed-Rank Test. According to the results, it was found that the CVS algorithm obtained by using the Gauss–Mouse chaos map was the best algorithm. And also, it was shown that the proposed CVS algorithm performs better than the classical VS algorithm, even when its maximum NOI was ten times less than the maximum NOI of the VS algorithm. Additionally, the effects of the proposed methods in the exploration and the exploitation abilities of the VS algorithm were visually shown and a comparison about algorithm processing time was presented. In order to test the performance of the proposed CVS algorithm in solving the real-world optimization problems , the inverse kinematics problem of a six Degrees Of Freedom (DOF) serial robot manipulator with offset wrist was solved with both the proposed CVS algorithm and the VS algorithm for two different types of trajectories. The results showed that the proposed algorithm outperforms the VS algorithm in terms of the objective function values and position errors of the end-effector of the serial robot manipulator .},
  archive      = {J_ASOC},
  author       = {Metin Toz},
  doi          = {10.1016/j.asoc.2020.106074},
  journal      = {Applied Soft Computing},
  pages        = {106074},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Chaos-based vortex search algorithm for solving inverse kinematics problem of serial robot manipulators with offset wrist},
  volume       = {89},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Semi-supervised learning quantization algorithm with deep
features for motor imagery EEG recognition in smart healthcare
application. <em>ASOC</em>, <em>89</em>, 106071. (<a
href="https://doi.org/10.1016/j.asoc.2020.106071">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper depicts a novel semi-supervised classification model with convolutional neural networks (CNN) for EEG Recognition. The performance of popular machine learning algorithm usually rely on the number of labeled training samples, such as the deep learning approaches, sparse classification approaches and supervised learning approaches. However, the labeled samples are very difficulty to get for electroencephalography(EEG) data. In addition, most deep learning algorithms are usually time-consuming in the process of training. Considering these problems, in this article, a novel semi-supervised quantization algorithm based on the cartesian K K -means algorithm is proposed, which named it as the semi-supervised cartesian K K -means (SSCK), we use the CNN models pre-trained on motor imagery samples to create deep features, and then we applied it for motor imagery (MI) data classification . Unlike the traditional semi-supervised learning models that labeled information can be directly casted into the model training, label information can only be implicitly used in the semi-supervised learning strategy, in the semi-supervised learning algorithm, supervised information is integrated into the quantization algorithm by resorting a supervised constructed laplacian regularizer. Experimental results over four popular EEG datasets substantiate the efficiency and effectiveness of our proposed semi-supervised cartesian K K -means.},
  archive      = {J_ASOC},
  author       = {Minjie Liu and Mingming Zhou and Tao Zhang and Naixue Xiong},
  doi          = {10.1016/j.asoc.2020.106071},
  journal      = {Applied Soft Computing},
  pages        = {106071},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Semi-supervised learning quantization algorithm with deep features for motor imagery EEG recognition in smart healthcare application},
  volume       = {89},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Adaptive LSSVM based iterative prediction method for NOx
concentration prediction in coal-fired power plant considering system
delay. <em>ASOC</em>, <em>89</em>, 106070. (<a
href="https://doi.org/10.1016/j.asoc.2020.106070">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Obtaining accurate and real-time value of the pollution concentration is fundamental to effective and energy-saving operation for pollution controlling in coal-fired power plants. However, accurate measurements for NO x x concentration cannot be guaranteed, due to the intrinsic hardware and software design in sensors. In this paper, a prediction method, including variables processing and model regression, is proposed for NO x x concentration measurement. Specifically, a set of variables is firstly selected adaptively as an input set by using modified transfer entropy (TE), while the relationships among them are guaranteed to be as weak as possible. Then, the input set can cover features without introducing redundant information to the prediction model , and the system delay is reduced based on the TE and sequential displacement. After the variables are processed, a forgetting factor online least square support vector machine (FFOLSSVM) is constructed to predict NO x x concentration timely and accurately. The proposed method is the first work that takes the system delay into consideration for NO x x prediction model, without mechanism analysis. The simulation indicates that the computational time and prediction accuracy requirements are sufficiently guaranteed by the proposed model.},
  archive      = {J_ASOC},
  author       = {Yongjie Zhai and Xuda Ding and Xiuzhang Jin and Lihui Zhao},
  doi          = {10.1016/j.asoc.2020.106070},
  journal      = {Applied Soft Computing},
  pages        = {106070},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Adaptive LSSVM based iterative prediction method for NOx concentration prediction in coal-fired power plant considering system delay},
  volume       = {89},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Using IoT technology for computer-integrated manufacturing
systems in the semiconductor industry. <em>ASOC</em>, <em>89</em>,
106065. (<a href="https://doi.org/10.1016/j.asoc.2020.106065">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The evolution of semiconductor manufacturing between lot and equipment has an increasingly complex relationship in the track-in/track-out process mechanism. Failure of reading or updating barcodes may cause some lots to scrap with mis-operation. In this paper, it proposed an Internet of Things (IoT) based Computer-Integrated Manufacturing (CIM) system used in Feature Advantage Benefit (FAB), that semiconductor company used Radio Frequency Identification (RFID) into 300-mm (FAB), and according to the information provided by the semiconductor company MTB (Manufacturing Technical Board). This solution to change the operation process flow to fit the CIM system characteristics of Fab in Manufacturing Execution System (MES). It also use IoT design strategy and the system architecture of this new IoT solution. The result of this research is to incorporate deep learning method in the IoT system into the current CIM system to reveal the benefits in FAB. It estimate to save about US$ 2.8M by adopting IoT RFID instead of tagging to identify a work in process lot in the initiation phase. The company can gain greater asset visibility, reduce the costs of man-power requirements and connect with the worldwide trend.},
  archive      = {J_ASOC},
  author       = {Yu-Qiang Chen and Biao Zhou and Mingming Zhang and Chien-Ming Chen},
  doi          = {10.1016/j.asoc.2020.106065},
  journal      = {Applied Soft Computing},
  pages        = {106065},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Using IoT technology for computer-integrated manufacturing systems in the semiconductor industry},
  volume       = {89},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). An efficient krill herd algorithm for color image multilevel
thresholding segmentation problem. <em>ASOC</em>, <em>89</em>, 106063.
(<a href="https://doi.org/10.1016/j.asoc.2020.106063">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The conventional thresholding methods are very efficient for bi-level thresholding, but the computational complexity may be excessively high for color image multilevel thresholding. Color image multilevel thresholding segmentation can be considered as a constrained optimization problem , therefore swarm intelligence algorithms are widely used to reduce the complexity. In this paper, an efficient krill herd (EKH) algorithm is proposed to search optimal thresholding values at different level for color images and the Otsu’s method, Kapur’s entropy and Tsallis entropy are employed as objective functions. Seven different algorithms, KH without any genetic operators (KH I), KH with crossover operator (KH II), KH with crossover and mutation operators (KH IV), modified firefly algorithm (MFA), modified grasshopper optimization algorithm (MGOA), bat algorithm (BA) and water cycle algorithm (WCA), are compared with the EKH algorithm. Experiments are performed on ten color benchmark images in terms of optimal threshold values, objective values, PSNR , SSIM and standard deviation of the objective values at different levels. The experimental results show that the presented EKH algorithm is superior to the other algorithms for color image multilevel thresholding segmentation. On the other hand, Kapur’s entropy is found to be more accurate and robust for color image multilevel thresholding segmentation.},
  archive      = {J_ASOC},
  author       = {Lifang He and Songwei Huang},
  doi          = {10.1016/j.asoc.2020.106063},
  journal      = {Applied Soft Computing},
  pages        = {106063},
  shortjournal = {Appl. Soft. Comput.},
  title        = {An efficient krill herd algorithm for color image multilevel thresholding segmentation problem},
  volume       = {89},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Configuration space evolutionary algorithm for
multi-objective unequal-area facility layout problems with flexible
bays. <em>ASOC</em>, <em>89</em>, 106052. (<a
href="https://doi.org/10.1016/j.asoc.2019.106052">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Facility layout problem (FLP) which deals with the layout of facilities within a given plant floor is an NP-hard combinatorial optimization problem . This paper studies multi-objective unequal-area facility layout problems (UA-FLPs) with the flexible bay structure (FBS), whose objectives refer to the material handling cost, the closeness relationship, the distance requirement and the aspect ratio of facilities. In recent years, some successes have been achieved by multi-objective evolutionary algorithms (MOEAs) for solving various kinds of optimization problems with multiple conflicting objectives. However, traditional MOEAs face a great challenge in the convergence and diversity of solutions for UA-FLPs. In this paper, a novel MOEA called the configuration space evolutionary (CSE) algorithm is developed to solve the UA-FLPs with multiple objectives. We consider a mating pool called a configuration (solution) bank in the CSE, and use evolutionary operations (selection, novel crossover and mutation) to produce new configurations of the pool. By introducing a measure of the radius d s p a c e dspace of the configuration bank, whose value is gradually reduced to narrow the search space, the convergence of solutions in the CSE is controlled. A method of the nearest and farthest candidate solution based on objective function normalization is combined with the fast non-dominated sorting to choose the Pareto-optimal solutions, which is good for the algorithm to keep diversity of the obtained solutions. The main contributions of this study lie in the use of a mechanism of evolution of population in the algorithm based on a configuration bank, and the use of a selection strategy based on the nearest and farthest candidate solution method, in order to improve the convergence and diversity of solutions. Experiments are carried out on eight different representative instances and performance metrics from the literature. Compared with the existing MOEAs, the CSE is able to find the better results and show better performance. The numerical experiments confirm the effectiveness of the CSE for solving multi-objective UA-FLPs.},
  archive      = {J_ASOC},
  author       = {Jingfa Liu and Siyu Liu and Zhaoxia Liu and Bi Li},
  doi          = {10.1016/j.asoc.2019.106052},
  journal      = {Applied Soft Computing},
  pages        = {106052},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Configuration space evolutionary algorithm for multi-objective unequal-area facility layout problems with flexible bays},
  volume       = {89},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Hybridization of cognitive computing for food services.
<em>ASOC</em>, <em>89</em>, 106051. (<a
href="https://doi.org/10.1016/j.asoc.2019.106051">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The application of data mining technology to food services and the restaurant industry has certain social value. By predicting customer traffic and needs, a restaurant can prepare a reasonable amount of meals for customers according to predicted needs which is conducive to improving the dining experience of customers and also improving the quality of food preparation and making the restaurant itself operate more efficiently. In recent years, we have seen the use of collaborative robots for use in the fast food industry. In Asia and more specifically in Japan , we have seen many fast-food chains implement the use of robots to better serve their customers. By studying the linear regression algorithm and the random forest algorithm, this paper proposes a new interwoven novel fusion approach of combining both algorithms and applies the new model to restaurant data to assist in the prediction of customer traffic in the restaurant industry. This predictive algorithm using cognitive techniques can assist these newly place robots in the food industry better serve their client base and in doing so make the industry more efficient. Experimental, comparison, and analysis are reported in the paper. The error rate of the fusion solution is reduced by approximately 5.503\% compared with the linear regression algorithm and is approximately 3.719\% lower than the error rate of the random forest algorithm. Results show that the new fusion algorithm can achieve better prediction results of customer traffic prediction for the restaurant industry. Furthermore, we also provide a new take on the application of data mining technology in the restaurant industry itself.},
  archive      = {J_ASOC},
  author       = {Xiaobo Zhang and Senbin Yang and Gautam Srivastava and Mu-Yen Chen and Xiaochun Cheng},
  doi          = {10.1016/j.asoc.2019.106051},
  journal      = {Applied Soft Computing},
  pages        = {106051},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Hybridization of cognitive computing for food services},
  volume       = {89},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A two-stage robust optimisation for terminal traffic flow
problem. <em>ASOC</em>, <em>89</em>, 106048. (<a
href="https://doi.org/10.1016/j.asoc.2019.106048">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Airport congestion witnesses potential conflicts: insufficient terminal airspace and delay propagation within scrambled the competition in the terminal manoeuvring area. Re-scheduling of flights is needed in numerous situations, heavy traffic in air segments, holding patterns, runway schedules and airport surface operations. Robust optimisation for terminal traffic flow problem, providing a practical point of view in hedging uncertainty, can leverage the adverse effect of uncertainty and schedule intervention. To avoid delay propagation throughout the air traffic flow network and reduce the vulnerability to disruption, this research adopts a two-stage robust optimisation approach in terminal traffic flow. It further enhances the quality of Pareto-optimality Benders-dual cutting plane based on core point approximation in the second stage recourse decision. The efficiency of the cutting plane algorithm is evaluated by a set of medium sized real-life scenarios. The numerical results show that the proposed scheme outperforms the well-known Pareto-optimal cuts in Benders-dual method from the literature.},
  archive      = {J_ASOC},
  author       = {K.K.H. Ng and C.K.M. Lee and Felix T.S. Chan and Chun-Hsien Chen and Yichen Qin},
  doi          = {10.1016/j.asoc.2019.106048},
  journal      = {Applied Soft Computing},
  pages        = {106048},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A two-stage robust optimisation for terminal traffic flow problem},
  volume       = {89},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). An intensify harris hawks optimizer for numerical and
engineering optimization problems. <em>ASOC</em>, <em>89</em>, 106018.
(<a href="https://doi.org/10.1016/j.asoc.2019.106018">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently developed Harris Hawks Optimization has virtuous behavior for finding optimum solution in search space. However, it easily get trapped into local search space for constrained engineering optimization problems . In order to accelerate the global search phase of existing Harris Hawks optimizer and to stuck it out of local search space, the proposed research aims to explore the exploration phase of the existing optimizer, the hybrid variant of Harris Hawks optimizer has been developed using sine–cosinealgorithm and named as Hybrid Harris Hawks-Sine Cosine Algorithm (hHHO-SCA) The effectiveness of the proposed optimizer has been tested for various nonlinear, non-convex and highly constrained engineering design problem . In order to validate the results of the proposed algorithm, 65 standard benchmark problems including CEC2017, CEC2018 and eleven multidisciplinary engineering design optimization problems has been taken into consideration. After verification it has been observed that the outcomes of the proposed hHHO-SCA optimization algorithm is much better than standard sine–cosine optimization algorithm , Harris Hawks Optimizer, Ant Lion Optimizer algorithm, Moth Flame Optimization algorithm, grey wolf optimizer algorithm, and others recently described meta-heuristics, heuristics and hybrid type optimization search algorithm and proposed algorithm endorses its effectiveness in multi-disciplinary design and engineering optimization problems.},
  archive      = {J_ASOC},
  author       = {Vikram Kumar Kamboj and Ayani Nandi and Ashutosh Bhadoria and Shivani Sehgal},
  doi          = {10.1016/j.asoc.2019.106018},
  journal      = {Applied Soft Computing},
  pages        = {106018},
  shortjournal = {Appl. Soft. Comput.},
  title        = {An intensify harris hawks optimizer for numerical and engineering optimization problems},
  volume       = {89},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Energy-efficient face detection and recognition scheme for
wireless visual sensor networks. <em>ASOC</em>, <em>89</em>, 106014. (<a
href="https://doi.org/10.1016/j.asoc.2019.106014">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Energy-efficient and robust face detection and recognition scheme can be useful for many application fields such as security and surveillance in multimedia and visual sensor network (VSN). VSN consists of wireless resources-constrained nodes that are equipped with low-energy CMOS cameras for monitoring. On the one hand, captured images are meaningful multimedia-data that impose high energy consumption to be processed and transmitted. On the other hand, visual sensor (VS) is a battery-powered node with limited life-time. This situation leads to a trade-off between detection-accuracy and power-consumption. This trade-off is considered as the most major challenge for applications using multimedia data in wireless environments such as VSN. For optimizing this trade-off, a novel face detection and recognition scheme has been proposed in this paper based on VSN. In this scheme, detection phase is performed at VS and recognition phase is accomplished at the base station (sink). The contributions of this paper are in three folds: 1. Fast and energy-aware face-detection algorithm is proposed based on omitting non-human blobs and feature-based face detection in the considered human-blobs. 2. A novel energy-aware and secure algorithm for extracting light-weight discriminative vector of detected face-sequence to be sent to sink with low transmission-cost and high security level . 3. An efficient face recognition algorithm has been performed on the received vectors at the sink. The performance of our proposed scheme has been evaluated in terms of energy-consumption, detection and recognition accuracy. Experimental results, performed on standard datasets (FERET, Yale and CDnet) and on personal datasets, demonstrate the superiority of our scheme over the recent state-of-the-art methods.},
  archive      = {J_ASOC},
  author       = {Abdulaziz Zam and Mohammad Reza Khayyambashi and Ali Bohlooli},
  doi          = {10.1016/j.asoc.2019.106014},
  journal      = {Applied Soft Computing},
  pages        = {106014},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Energy-efficient face detection and recognition scheme for wireless visual sensor networks},
  volume       = {89},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Grid based clustering for satisfiability solving.
<em>ASOC</em>, <em>88</em>, 106069. (<a
href="https://doi.org/10.1016/j.asoc.2020.106069">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The originality of this work resides into the exploitation of data mining techniques for problem solving. Two major phases define this work. The first one is to determine the clustering technique that best suits each SAT instance based on the distribution of the later. The clustering technique is then applied to reduce the complexity of each instance by creating sub-instances that can be solved independently in the second phase. The latter consists into a resolution step where the DPLL or BSO algorithms are executed depending on the number of variables to be assigned in each cluster. This two-phase resolution strategy provides more efficient problem solving. The Boolean Satisfiability problem (SAT) is considered in this study because of its importance for the Artificial Intelligence (AI) community and the impact of its solving on other complex problems. Three different distributions of the problem were observed. The first distribution defines a space where the variables are dispersed forming regions of considerable density interspersed with regions of lower density or empty regions. On the other hand, the other two distributions do not show any significant shape, as the variables are randomly scattered, with one of these two dispersions having the particularity that almost all its variables are of high occurrence. To each of the three distributions, a clustering technique is associated. Density-based clustering techniques are the most appropriate type of clustering for the first distribution. Meanwhile, grid-based clustering and frequent patterns mining seem to be the most suitable clustering techniques for the second and third distributions. Investigations are undertaken on these latter issues and contributions are presented in this paper. Experiments were conducted on public benchmarks and the results showed the importance of the pre-processing step of data mining to solve the SAT problem.},
  archive      = {J_ASOC},
  author       = {Celia Hireche and Habiba Drias and Hadjer Moulai},
  doi          = {10.1016/j.asoc.2020.106069},
  journal      = {Applied Soft Computing},
  pages        = {106069},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Grid based clustering for satisfiability solving},
  volume       = {88},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Optimizing hyperparameters of deep learning in predicting
bus passengers based on simulated annealing. <em>ASOC</em>, <em>88</em>,
106068. (<a href="https://doi.org/10.1016/j.asoc.2020.106068">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Bus is certainly one of the most widely used public transportation systems in a modern city because it provides an inexpensive solution to public transportation users, such as commuters and tourists. Most people would like to avoid taking a crowded bus on the way. That is why forecasting the number of bus passengers has been a critical problem for years. The proposed method is inspired by the fact that there is no easy way to know the suitable parameters for most of the deep learning methods in solving the optimization problem of forecasting the number of passengers on a bus. To address this issue, the proposed algorithm uses a simulated annealing (SA) to find out a suitable number of neurons for each layer of a fully connected deep neural network (DNN) to enhance the accuracy rate in solving this particular optimization problem . The proposed method is compared with support vector machine , random forest , eXtreme gradient boosting, deep neural network , and deep neural network with dropout for the data provided by the Taichung city smart transportation big data research center, Taiwan (TSTBDRC). Our simulation results indicate that the proposed method outperforms all the other forecasting methods for forecasting the number of bus passengers in terms of the accuracy rate and the prediction time.},
  archive      = {J_ASOC},
  author       = {Chun-Wei Tsai and Chien-Hui Hsia and Shuang-Jie Yang and Shih-Jui Liu and Zhi-Yan Fang},
  doi          = {10.1016/j.asoc.2020.106068},
  journal      = {Applied Soft Computing},
  pages        = {106068},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Optimizing hyperparameters of deep learning in predicting bus passengers based on simulated annealing},
  volume       = {88},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A quasi-oppositional-chaotic symbiotic organisms search
algorithm for optimal allocation of DG in radial distribution networks.
<em>ASOC</em>, <em>88</em>, 106067. (<a
href="https://doi.org/10.1016/j.asoc.2020.106067">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper aims to apply an improved meta-heuristic method to optimize the allocation of distributed generation (DG) units in radial distribution networks (RDNs). The proposed method, namely the Quasi-Oppositional Chaotic Symbiotic Organisms Search (QOCSOS) algorithm, is the improved version of the original SOS algorithm. QOCSOS incorporates the Quasi-Opposition-Based Learning (QOBL) and Chaotic Local Search (CLS) strategies into SOS to improve the global search capacity. In this study, the objective of the optimal DG allocation (OGDA) problem is to optimally reduce the real power loss, improve the voltage profile, and increase the voltage stability in RDNs. The proposed QOCSOS algorithm was applied to find the optimal locations and sizes of DG units with different DG power factors (unity and non-unity) in the RDNs including 33, 69, and 118-bus. It was found that the operation of DG units with optimal power factor significantly improved the performance of RDNs in terms of voltage deviation minimization, and voltage stability maximization, especially for power loss reduction. After the DG integration, for the case of DG units operating with unity power factor, the power loss reduction was reduced by 65.50\%, 69.14\%, and 60.23\% for the 33, 69, and 118-bus RDNs, respectively. In addition, it should be emphasized that for the cases of DG units operating with optimal power factor, the power loss reduction was reduced up to 94.44\%, 98.10\%, and 90.28\% for these RDNs, respectively. The obtained results from QOCSOS were evaluated by comparing to those from SOS and other optimization methods in the literature. The results showed that the proposed QOCSOS method performed greater than SOS, and offered better quality solutions than many other compared methods, suggesting the feasibility of QOCSOS in solving the ODGA problem, especially for a complex and large-scale system.},
  archive      = {J_ASOC},
  author       = {Khoa H. Truong and Perumal Nallagownden and Irraivan Elamvazuthi and Dieu N. Vo},
  doi          = {10.1016/j.asoc.2020.106067},
  journal      = {Applied Soft Computing},
  pages        = {106067},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A quasi-oppositional-chaotic symbiotic organisms search algorithm for optimal allocation of DG in radial distribution networks},
  volume       = {88},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Oil project selection in iran: A hybrid MADM approach in an
uncertain environment. <em>ASOC</em>, <em>88</em>, 106066. (<a
href="https://doi.org/10.1016/j.asoc.2020.106066">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This research focusses on the selection of oil projects by using Multi-Attribute Decision Making (MADM) methods in an uncertain environment. Oil production plays a crucial role in the economy of Iran, a country with many opportunities for onshore oil exploration. Differently from other countries, however, oil producers in Iran face some constraints with respect to oil extraction because some areas are shared with other producers. Additionally, oil producers in Iran must also allocate scarce physical, human, and monetary resources among different projects. Inaccurate decision-making may not only yield sub-optimal revenue generation, but also adversely affect the national economy For these reasons, priority oil projects must observe a sequence of steps. In the first step, critical factors for selecting oil projects are collected from previous related studies and experts are interviewed. These factors are subsequently filtered using the Delphi method. The oil projects are then ranked using a comprehensive approach involving novel alternative MADM methods. The best-worst method (BWM) is a new MADM stream that relies on pairwise comparison . It presents several distinct advantages with respect to fewer computational steps and higher discriminatory power among alternatives. Differently from previous research, this paper couples BWM with Weighted Aggregated Sum–Product Assessment (WASPAS) to improve result sensitivity under uncertain decision-making environments as modelled by Z-numbers. A robustness cross-check against other MADM models is also presented. Results indicate that quality has the highest priority and that production technology has the lowest priority among ten factors for oil project selection, thus reflecting the impact of US sanctions on oil production in Iran. Managerial implications and future avenues of research are derived.},
  archive      = {J_ASOC},
  author       = {Amir Karbassi Yazdi and Alireza Rashidi Komijan and Peter Fernandes Wanke and Soheila Sardar},
  doi          = {10.1016/j.asoc.2020.106066},
  journal      = {Applied Soft Computing},
  pages        = {106066},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Oil project selection in iran: A hybrid MADM approach in an uncertain environment},
  volume       = {88},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Multi-criteria decision making techniques for the management
of public procurement tenders: A case study. <em>ASOC</em>, <em>88</em>,
106064. (<a href="https://doi.org/10.1016/j.asoc.2020.106064">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-Criteria Decision Making (MCDM) techniques are mathematical tools that help decision makers evaluating and ranking in an automatic way many possible alternatives over multiple conflicting criteria in highly complex situations. Several MCDM approaches exist, and their application fields are numerous, including the Supplier Selection Problem (SSP), which is an important problem in the management field. The aim of this paper is to perform a comparative analysis among some selected well-known MCDM techniques to show how they can properly support the specific decision making process of Public Procurement (PP) tenders, which is a particular type of the SSP, characterized by very stringent rules, thus requiring a specific assessment. Indeed, PP is a field characterized by the need for transparency, objectivity, and non-discrimination, which requires tendering organizations to explicitly state the adopted awarding method, the chosen decision criteria, and their relative importance in the call for proposals. However, this field has been seldomly investigated in the pertinent literature and thus the aim of this paper is to overcome such a limitation. In particular, this work focuses on the most commonly adopted methods in the field of supplier selection, namely the Analytic Hierarchy Process (AHP), the Preference Ranking Organization METHod for Enrichment of Evaluations (PROMETHEE), the Multi Attribute Utility Theory (MAUT), and the Data Envelopment Analysis (DEA). First, we adapt these techniques to the PP problem and its requirements. Then, by means of some real tenders at a European Institution, the selected techniques are compared with each other and with the currently adopted methodology in their classical deterministic setting, to identify which method best suits the specific requirements of PP tenders. Hence, since nowadays uncertainty is inherent in data from real applications, and can be modelled by expert evaluations through fuzzy logic, the comparison is extended to the fuzzy counterparts of two of the most promising selected approaches, i.e., the Fuzzy AHP and the Fuzzy DEA, showing that these methods can be effectively applied to the PP sector also in the presence of uncertainty on the tenders data.},
  archive      = {J_ASOC},
  author       = {Mariagrazia Dotoli and Nicola Epicoco and Marco Falagario},
  doi          = {10.1016/j.asoc.2020.106064},
  journal      = {Applied Soft Computing},
  pages        = {106064},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Multi-criteria decision making techniques for the management of public procurement tenders: A case study},
  volume       = {88},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Adaptive protection scheme for microgrids based on SOM
clustering technique. <em>ASOC</em>, <em>88</em>, 106062. (<a
href="https://doi.org/10.1016/j.asoc.2020.106062">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Microgrids are penetrating into the power systems at an unprecedented rate. The reason is the mutual economic and environmental benefits of microgrids, both for power grid utility and the consumers. Some special features of microgrids such as, the two main operational conditions called, islanded and grid-connected modes, and being composed of various types of distributed energy resources along with different uncertainties cause some tough challenges to protection and control systems. From the protection aspect, the coordination of overcurrent relays protection will become a difficulty, due to the extensive changes in the fault current levels sensed by these devices. In this paper, a new adaptive protection coordination scheme based on Self-Organizing Map (SOM) clustering algorithm is proposed for digital overcurrent relays equipped with several setting groups. Considering the similarity of mis-coordinated relay pairs for the clustering purpose, the proposed protection scheme focuses on solving the mis-coordination between main/backup relay pairs. As a case study, a modified IEEE 33-bus test system is used as a microgrid. In the case study, a synchronous distributed generation and two electric vehicle charging stations are installed. The results suggest that not only the proposed method is fully capable and flexible to significantly improve the mis-coordination of overcurrent relay pairs, but it can also ameliorate the operating time of relay.},
  archive      = {J_ASOC},
  author       = {Seyyed Mohammad Ebrahim Ghadiri and Kazem Mazlumi},
  doi          = {10.1016/j.asoc.2020.106062},
  journal      = {Applied Soft Computing},
  pages        = {106062},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Adaptive protection scheme for microgrids based on SOM clustering technique},
  volume       = {88},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Application of fuzzy reference ideal method (FRIM) to the
military advanced training aircraft selection. <em>ASOC</em>,
<em>88</em>, 106061. (<a
href="https://doi.org/10.1016/j.asoc.2020.106061">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {When a nation needs to acquire a new military training aircraft for its Air Force, many factors must be taken into account. This requires a good command of conflicting factors, which can benefit from the domain of Multi-Criteria Decision Making (MCDM). However, some criteria involved in the assessment process are often imprecise or vague and the use of linguistic terms characterized by fuzzy numbers could be advisable. The aim of this research is thus to extract the best of a combination of Fuzzy MCDM approaches with the aim of solving a real decision problem of interest for the Spanish Air Force, specifically, the selection of the best military advanced training aircraft , based on a set of criteria of differing natures. This decision problem involves, on the one hand, quantitative or technical criteria (combat ceiling, operational speed, take-off race, etc.) and, on the other hand, qualitative criteria (maneuverability, ergonomics , etc.) based on the experience of a set of flight instructors of the 23rd Fighter and Attack Training Wing, collected via questionnaires. The Analytic Hierarchy Process (AHP) is applied to obtain the weights of the criteria, whereas the Reference Ideal Method (RIM) and its Fuzzy version (FRIM) are used to evaluate the alternatives based on a reference ideal alternative defined by the flight instructors mentioned above. As a result, the Italian Alenia Aermacchi M-346 Master aircraft was selected as the best option.},
  archive      = {J_ASOC},
  author       = {J.M. Sánchez-Lozano and O. Naranjo Rodríguez},
  doi          = {10.1016/j.asoc.2020.106061},
  journal      = {Applied Soft Computing},
  pages        = {106061},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Application of fuzzy reference ideal method (FRIM) to the military advanced training aircraft selection},
  volume       = {88},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Stacked pruning sparse denoising autoencoder based
intelligent fault diagnosis of rolling bearings. <em>ASOC</em>,
<em>88</em>, 106060. (<a
href="https://doi.org/10.1016/j.asoc.2019.106060">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes a new stacked pruning sparse denoising autoencoder (sPSDAE) model for intelligent fault diagnosis of rolling bearings . Different from the traditional autoencoder , the proposed sPSDAE model, including a fully connected autoencoder network, uses the superior features extracted in all the previous layers to participate in the subsequent layers. This means that some new channels are created to connect the front layers and the back layers, which reduces information loss. To improve the training efficiency and precision of the sPSDAE model, a pruning operation is added into the sPSDAE model so as to prohibit non-superior units from participating in all the subsequent layers. Meanwhile, a feature fusion mechanism is introduced to ensure the uniqueness of the feature dimensions. After that, the sparse expression of the sPSDAE model is strengthened, thereby improving the generalization ability . The proposed method is evaluated by using a public bearing dataset and is compared with other popular fault diagnosis models. The results show that the ability of the sPSDAE model to extract features is significantly enhanced and the phenomenon of gradient disappearance is further reduced. The proposed model achieves higher diagnostic accuracy than other popular fault diagnosis models.},
  archive      = {J_ASOC},
  author       = {Haiping Zhu and Jiaxin Cheng and Cong Zhang and Jun Wu and Xinyu Shao},
  doi          = {10.1016/j.asoc.2019.106060},
  journal      = {Applied Soft Computing},
  pages        = {106060},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Stacked pruning sparse denoising autoencoder based intelligent fault diagnosis of rolling bearings},
  volume       = {88},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Cooperative coevolution with an improved resource allocation
for large-scale multi-objective software project scheduling.
<em>ASOC</em>, <em>88</em>, 106059. (<a
href="https://doi.org/10.1016/j.asoc.2019.106059">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The existing literature of search-based software project scheduling merely studied to schedule a small to medium-scale project in static scenarios, while little work has considered to schedule a large-scale software project with uncertainties. However, many real-world software projects involve a large number of tasks and employees. Meanwhile, they are confronted with uncertain environments. To tackle such problems, this paper constructs a mathematical model for the large-scale multi-objective software project scheduling problem, and proposes a cooperative coevolutionary multi-objective genetic algorithm to solve the established model. In our model, more practical features of human resources and tasks are captured in the context of large-scale projects than the previous studies. Two efficiency related objectives of duration and cost are considered together with robustness to uncertainties and employees’ satisfaction to allocations subject to various realistic constraints. Three novel strategies are incorporated in the proposed algorithm, which include the problem feature-based variable decomposition method , the improved computational resource allocation mechanism and the problem-specific subcomponent optimizer. To evaluate the performance of the proposed algorithm, empirical experiments have been performed on 15 randomly generated large-scale software project scheduling instances with up to 2048 decision variables, and three instances derived from real-world software projects. Experimental results indicate that on most of the 15 random instances and three real-world instances, the proposed algorithm achieves significantly better convergence performance than several state-of-the-art evolutionary algorithms , while maintaining a set of well-distributed solutions. Thus, it can be concluded that the proposed algorithm has a promising scalability to decision variables on software project scheduling problems. We also demonstrate how different compromises among the four objectives can offer software managers a deeper insight into various trade-offs among many objectives, and enabling them to make an informed decision.},
  archive      = {J_ASOC},
  author       = {Xiaoning Shen and Yinan Guo and Aimin Li},
  doi          = {10.1016/j.asoc.2019.106059},
  journal      = {Applied Soft Computing},
  pages        = {106059},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Cooperative coevolution with an improved resource allocation for large-scale multi-objective software project scheduling},
  volume       = {88},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Picture fuzzy normalized projection and extended VIKOR
approach to software reliability assessment. <em>ASOC</em>, <em>88</em>,
106056. (<a href="https://doi.org/10.1016/j.asoc.2019.106056">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {An extended VIsekriterijumska optimizacija i KOmpromisno Resenje (VIKOR) method in group decision-making (GDM) setting is developed in this paper. The decision information is characterized by picture fuzzy number. A new GDM model is established and applied to software reliability assessment. First, this research finds that the current projection measure is not always reasonable in picture fuzzy setting. To solve this problem, a new normalization projection measure is developed in picture fuzzy setting. In addition, this research also finds that the most VIKOR-based GDM method requires a collective decision as a carrier to get an optimal solution. However, if a collective decision is aggregated by all the individual decisions, the positive and negative information might be canceled each other out in the decision process. To solve this problem, this paper proposes a direct VIKOR-based GDM method. The values of three VIKOR indexes are determined by the original decision matrices . The feasibility and practicability developed method in this work are illustrated by an example of software reliability assessment and some experimental analyses.},
  archive      = {J_ASOC},
  author       = {Chuan Yue},
  doi          = {10.1016/j.asoc.2019.106056},
  journal      = {Applied Soft Computing},
  pages        = {106056},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Picture fuzzy normalized projection and extended VIKOR approach to software reliability assessment},
  volume       = {88},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A genetic artificial bee colony algorithm for signal
reconstruction based big data optimization. <em>ASOC</em>, <em>88</em>,
106053. (<a href="https://doi.org/10.1016/j.asoc.2019.106053">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, the researchers have witnessed the changes or transformations driven by the existence of the big data on the definitions, complexities and future directions of the real world optimization problems . Analyzing the capabilities of the previously introduced techniques, determining possible drawbacks of them and developing new methods by taking into consideration of the unique properties related with the big data are nowadays in urgent demands. Artificial Bee Colony (ABC) algorithm inspired by the clever foraging behaviors of the real honey bees is one of the most successful swarm intelligence based optimization algorithms . In this study, a novel ABC algorithm based big data optimization technique was proposed. For exploring the solving abilities of the proposed technique, a set of experimental studies has been carried out by using different signal decomposition based big data optimization problems presented at the Congress on Evolutionary Computation (CEC) 2015 Big Data Optimization Competition. The results obtained from the experimental studies first were compared with the well-known variants of the standard ABC algorithm named gbest-guided ABC (GABC), ABC/best/1, ABC/best/2, crossover ABC (CABC), converge-onlookers ABC (COABC) and quick ABC (qABC). The results of the proposed ABC algorithm were also compared with the Differential Evolution (DE) algorithm, Genetic algorithm (GA), Firefly algorithm (FA), Fireworks algorithm (FW), Phase Base Optimization (PBO) algorithm, Particle Swarm Optimization (PSO) algorithm and Dragonfly algorithm (DA) based big data optimization techniques. From the experimental studies, it was understood that the newly introduced ABC algorithm based technique is capable of producing better or at least promising results compared to the mentioned big data optimization techniques for all of the benchmark instances.},
  archive      = {J_ASOC},
  author       = {Selcuk Aslan and Dervis Karaboga},
  doi          = {10.1016/j.asoc.2019.106053},
  journal      = {Applied Soft Computing},
  pages        = {106053},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A genetic artificial bee colony algorithm for signal reconstruction based big data optimization},
  volume       = {88},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Surrogate-based optimisation using adaptively scaled radial
basis functions. <em>ASOC</em>, <em>88</em>, 106050. (<a
href="https://doi.org/10.1016/j.asoc.2019.106050">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Aerodynamic shape optimisation is widely used in several applications, such as road vehicles, aircraft and trains. This paper investigates the performance of two surrogate-based optimisation methods; a Proper Orthogonal Decomposition-based method and a force-based surrogate model . The generic passenger vehicle DrivAer is used as a test case where the predictive capability of the surrogate in terms of aerodynamic drag is presented. The Proper Orthogonal Decomposition-based method uses simulation results from topologically different meshes by interpolating all solutions to a common mesh for which the decomposition is calculated. Both the Proper Orthogonal Decomposition- and force-based approaches make use of Radial Basis Function interpolation. The Radial Basis Function hyperparameters are optimised using differential evolution. Additionally, the axis scaling is treated as a hyperparameter, which reduces the interpolation error by more than 50\% for the investigated test case. It is shown that the force-based approach performs better than the Proper Orthogonal Decomposition method, especially at low sample counts, both with and without adaptive scaling. The sample points, from which the surrogate model is built, are determined using an optimised Latin Hypercube sampling plan. The Latin Hypercube sampling plan is extended to include both continuous and categorical values, which further improve the surrogate’s predictive capability when categorical design parameters, such as on/off parameters, are included in the design space. The performance of the force-based surrogate model is compared with four other gradient-free optimisation techniques: Random Sample, Differential Evolution, Nelder–Mead and Bayesian Optimisation. The surrogate model performed as good as, or better than these algorithms, for 17 out of the 18 investigated benchmark problems.},
  archive      = {J_ASOC},
  author       = {Magnus Urquhart and Emil Ljungskog and Simone Sebben},
  doi          = {10.1016/j.asoc.2019.106050},
  journal      = {Applied Soft Computing},
  pages        = {106050},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Surrogate-based optimisation using adaptively scaled radial basis functions},
  volume       = {88},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Unified model for interpreting multi-view echocardiographic
sequences without temporal information. <em>ASOC</em>, <em>88</em>,
106049. (<a href="https://doi.org/10.1016/j.asoc.2019.106049">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The robust and fully automatic interpretation of multi-view echocardiographic sequences across multi-vendor and multi-center is a challenging task due to abounding artifacts, low signal-to-noise ratio, large shape variations among different views, and large gaps across different centers and vendors. In this paper, a dense pyramid and deep supervision network (DPSN) is proposed to tackle this challenging task. DPSN incorporates the advantages of the densely connected network, feature pyramid network, and deeply supervised network, which help to extract and fuse multi-level and multi-scale holistic semantic information . This capability endows DPSN with prominent generalization and robustness, enabling it to yield a precise interpretation. To reduce the computational complexity and avoid the frequent information loss in temporal modeling , DPSN processes all frames independently (i.e., without utilizing temporal information) but can still obtain stable and coherent performance in the sequence. Adequate experiments on the heterogeneous (multi-view, multi-center, and multi-vendor) dataset (10858 labeled images) corroborate that DPSN achieves not only superior segmentation results but also prominent computational efficiency and stable performance. Estimation of the ejection fraction also shows good clinical correlation, revealing the clinical potential of DPSN.},
  archive      = {J_ASOC},
  author       = {Ming Li and Shizhou Dong and Zhifan Gao and Cheng Feng and Huahua Xiong and Wei Zheng and Dhanjoo Ghista and Heye Zhang and Victor Hugo C. de Albuquerque},
  doi          = {10.1016/j.asoc.2019.106049},
  journal      = {Applied Soft Computing},
  pages        = {106049},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Unified model for interpreting multi-view echocardiographic sequences without temporal information},
  volume       = {88},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A GPU fully vectorized approach to accelerate performance of
NSGA-2 based on stochastic non-domination sorting and grid-crowding.
<em>ASOC</em>, <em>88</em>, 106047. (<a
href="https://doi.org/10.1016/j.asoc.2019.106047">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This work introduces an accelerated implementation of NSGA-2 on a graphics processing unit (GPU) to reduce execution time. Parallelism is achieved in the population level using vectorization . All the components of the algorithm are run on the device, minimizing communication overhead . New stochastic versions of both non-domination sorting and crowding are introduced in the article. They are designed to be efficiently vectorized on GPU, therefore, the proposed approach is finally limited by the sorting procedure O ( n log ( n ) ) O(nlog(n)) , while the original algorithm was of order O ( n 2 ) O(n2) . This improvement is reflected on the speed-ups attained in the experiments. The results include metrics regarding solution quality to show there is not a significant trade-off between acceleration and performance. The possibilities to apply multi-objective evolutionary algorithms to real-time applications are discussed in the conclusions, where the possibility of devising restricted multi-objective evolutionary algorithms is concluded a possibility to implement faster methods.},
  archive      = {J_ASOC},
  author       = {Anton Aguilar-Rivera},
  doi          = {10.1016/j.asoc.2019.106047},
  journal      = {Applied Soft Computing},
  pages        = {106047},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A GPU fully vectorized approach to accelerate performance of NSGA-2 based on stochastic non-domination sorting and grid-crowding},
  volume       = {88},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Fuzzy forecasting for long-term time series based on
time-variant fuzzy information granules. <em>ASOC</em>, <em>88</em>,
106046. (<a href="https://doi.org/10.1016/j.asoc.2019.106046">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As to the long-term time series forecasting, it is more challenging and practical to obtain the trend information and fluctuation range of sequence data than single-step prediction values. In this article, by means of fuzzy information granules (FIGs) and recurrent fuzzy neural networks , a novel long-term prediction model for time series is proposed. Based on a variable-length division method, generalized zonary time-variant fuzzy information granule (GZT-FIG) is constructed, which can express the variation trend, fluctuation range and dispersion degree of sequence data. Furthermore, in order to improve the anti-noise ability and memory ability, type-2 fuzzy sets and long short-term memory mechanism are introduced into the prediction scheme, based on which a self-evolving interval type-2 LSTM fuzzy neural network (eIT2FNN-LSTM) is provided. Compared with the existing works related to fuzzy-neural models, the involvement of long short-term memory mechanism effectively improves the memory ability to achieve the long-term prediction. In order to verify the validation and effectiveness of the proposed scheme, several groups of experiments, including synthetic sequences and real-life time series, are carried out. The experimental results reveal the better predictive performance and rich semantic information .},
  archive      = {J_ASOC},
  author       = {Chao Luo and Haiyue Wang},
  doi          = {10.1016/j.asoc.2019.106046},
  journal      = {Applied Soft Computing},
  pages        = {106046},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Fuzzy forecasting for long-term time series based on time-variant fuzzy information granules},
  volume       = {88},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Distance-based consensus reaching process for group decision
making with intuitionistic multiplicative preference relations.
<em>ASOC</em>, <em>88</em>, 106045. (<a
href="https://doi.org/10.1016/j.asoc.2019.106045">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The intuitionistic multiplicative preference relation uses Saaty’s 1 ∕ 9 1∕9 - 9 9 scale to depict people’s opinions from the prior and not prior aspects. Due to its objectivity in representing people’s cognition, the intuitionistic multiplicative preference relation has attracted extensive attentions of scholars. In this paper, we study the distance-based consensus measures in the context of group decision-making with intuitionistic multiplicative preference relations. First of all, some new distance measures between intuitionistic multiplicative numbers/sets are proposed, which contains the improved Hamming distance , the improved Euclidean distance , and their weighted forms. Then, we investigate their desirable properties . To aid the group decision-making process, we further develop a new consensus measures regarding intuitionistic multiplicative preference relations based on the proposed distance measures. Afterwards, a new group decision-making method is proposed to solve the complex group decision-making problems with intuitionistic multiplicative preference relations. Finally, an example concerning the project investment selection is given to demonstrate the proposed group decision-making method, and then we compare the proposed method with other existing group decision-making methods with intuitionistic multiplicative preference relations in detail.},
  archive      = {J_ASOC},
  author       = {Cheng Zhang and Huchang Liao and Li Luo and Zeshui Xu},
  doi          = {10.1016/j.asoc.2019.106045},
  journal      = {Applied Soft Computing},
  pages        = {106045},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Distance-based consensus reaching process for group decision making with intuitionistic multiplicative preference relations},
  volume       = {88},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Solution to economic emission dispatch problem including
wind farms using exchange market algorithm method. <em>ASOC</em>,
<em>88</em>, 106044. (<a
href="https://doi.org/10.1016/j.asoc.2019.106044">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents an Exchange Market Algorithm (stocktickerEMA) method for solving the Economic Emission Dispatch (EED) problem including wind farms in the power systems . The stocktickerEMA algorithm is a powerful and useful method for finding the optimal value of an optimization problem with high accuracy. In recent years, because of the emission of harmful gases from fossil fuels and global warming issues, the penetration level of cleaner energies such as the wind and solar energy has been increased in order to produce the desired electrical energy. Therefore, it is vital to consider the wind turbines and wind farms in the EED optimization problem . Due to the probabilistic nature of wind speed in wind turbines, the generated power by wind turbines and wind farms has uncertain nature. Hence, the Weibull probability distribution function is used to model the wind power in the EED problem. The proposed method is tested on the IEEE 40-units test system. The analysis shows that, compared to other algorithms the EMA method has faster convergence and better ability in finding the optimal solution for the EED problem.},
  archive      = {J_ASOC},
  author       = {Mehrdad Tarafdar Hagh and Seyed Mohammad Sajjadi Kalajahi and Naser Ghorbani},
  doi          = {10.1016/j.asoc.2019.106044},
  journal      = {Applied Soft Computing},
  pages        = {106044},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Solution to economic emission dispatch problem including wind farms using exchange market algorithm method},
  volume       = {88},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A novel fractional-order fuzzy control method based on
immersion and invariance approach. <em>ASOC</em>, <em>88</em>, 106043.
(<a href="https://doi.org/10.1016/j.asoc.2019.106043">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In most of industrial applications, the dynamics of the system in hand are perturbed by a number of operational conditions. Also the outputs of the sensors always include noise. To alleviate these common problems, this paper presents a novel fuzzy control approach based on the immersion and invariance (I&amp;I) approach under the conditions of unknown dynamics and measurement errors. The adaptation laws for the parameters of the proposed non-singleton type-2 fuzzy neural network (NT2FNN) are derived through a stability analysis based on I&amp;I method. The effectiveness of the proposed membership function (MF) and non-singleton fuzzification is verified by comparison with the conventional Gaussian MF in the presence of measurement errors. The performance of the proposed control method is compared with other techniques and an experimental study is provided to show the capability of the proposed control scheme in real-time applications.},
  archive      = {J_ASOC},
  author       = {Ardashir Mohammadzadeh and Okyay Kaynak},
  doi          = {10.1016/j.asoc.2019.106043},
  journal      = {Applied Soft Computing},
  pages        = {106043},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A novel fractional-order fuzzy control method based on immersion and invariance approach},
  volume       = {88},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Automatic vessel lumen segmentation in optical coherence
tomography (OCT) images. <em>ASOC</em>, <em>88</em>, 106042. (<a
href="https://doi.org/10.1016/j.asoc.2019.106042">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a graph based method to automatically and accurately segment the lumen borders from optical coherence tomography (OCT) images. The proposed method unravels the OCT images from the Cartesian coordinates to polar coordinates so that the segmentation is transferred into a height field delineation problem. In effect, the method imposes a simplistic star shape prior but without the bias towards narrower lumen size. The lumen border is identified as the solution to finding the minimum closed set on a node-weighed, directed graph . In order to cope with both the variability in imaging condition and different forms of image artefacts, we adopt an image feature that relies on very little assumption on the appearance of lumen border but is resilient to image noise and so on. This feature is derived from a convolution of the image gradient field and thus it takes into account gradient vector interactions at a much more global scale compared to conventional gradient based approaches. The proposed method is fully automatic without the need for an initialisation. We compare this method with a number of techniques, including both conventional methods and data driven models.},
  archive      = {J_ASOC},
  author       = {Huaizhong Zhang and Ehab Essa and Xianghua Xie},
  doi          = {10.1016/j.asoc.2019.106042},
  journal      = {Applied Soft Computing},
  pages        = {106042},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Automatic vessel lumen segmentation in optical coherence tomography (OCT) images},
  volume       = {88},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Multi-objective feature selection based on artificial bee
colony: An acceleration approach with variable sample size.
<em>ASOC</em>, <em>88</em>, 106041. (<a
href="https://doi.org/10.1016/j.asoc.2019.106041">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to the need to repeatedly call a classifier to evaluate individuals in the population, existing evolutionary feature selection algorithms have the disadvantage of high computational cost. In view of it, this paper studies a multi-objective feature selection framework based on sample reduction strategy and evolutionary algorithm , significantly reducing the computational cost of algorithm without affecting optimal results. In the framework, a selection strategy of representative samples, called K-means clustering based differential selection, and a ladder-like sample utilization strategy are proposed to reduce the size of samples used in the evolutionary process. Moreover, a fast multi-objective evolutionary feature selection algorithm, called FMABC-FS, is proposed by embedding an improved artificial bee colony algorithm based on the particle update model into the framework. By applying FMABC-FS to several typical UCI datasets, and comparing with three multi-objective feature selection algorithms , experimental results show that the proposed variable sample size strategy is more suitable to FMABC-FS, and FMABC-FS can obtain better feature subsets with much less running time than those comparison algorithms.},
  archive      = {J_ASOC},
  author       = {Xiao-han Wang and Yong Zhang and Xiao-yan Sun and Yong-li Wang and Chang-he Du},
  doi          = {10.1016/j.asoc.2019.106041},
  journal      = {Applied Soft Computing},
  pages        = {106041},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Multi-objective feature selection based on artificial bee colony: An acceleration approach with variable sample size},
  volume       = {88},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Novel quantum inspired approaches for automatic clustering
of gray level images using particle swarm optimization, spider monkey
optimization and ageist spider monkey optimization algorithms.
<em>ASOC</em>, <em>88</em>, 106040. (<a
href="https://doi.org/10.1016/j.asoc.2019.106040">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper is intended to identify the optimal number of clusters automatically from an image dataset using some quantum behaved nature inspired meta-heuristic algorithms. Due to the lack of sufficient information, it is difficult to identify the appropriate number of clusters from a dataset, which has enthused the researchers to solve the problem of automatic clustering and to open up a new era of cluster analysis with the help of several natures inspired meta-heuristic algorithms. In this paper, three quantum inspired meta-heuristic techniques, viz., Quantum Inspired Particle Swarm Optimization (QIPSO), Quantum Inspired Spider Monkey Optimization (QISMO) and Quantum Inspired Ageist Spider Monkey Optimization (QIASMO), have been proposed. A comparison has been outlined between the quantum inspired algorithms with their corresponding classical counterparts. The efficiency of the quantum inspired algorithms has been established over their corresponding classical counterparts with regards to fitness, mean, standard deviation, standard errors of fitness, convergence curves (for benchmarked mathematical functions) and computational time. Finally, the results of two statistical superiority tests, viz., t- test and Friedman test have been provided to prove the superiority of the proposed methods. The superiority of the proposed methods has been established on five publicly available real life image datasets, five Berkeley image datasets of different dimensions and four benchmark mathematical functions both visually and quantitatively.},
  archive      = {J_ASOC},
  author       = {Alokananda Dey and Sandip Dey and Siddhartha Bhattacharyya and Jan Platos and Vaclav Snasel},
  doi          = {10.1016/j.asoc.2019.106040},
  journal      = {Applied Soft Computing},
  pages        = {106040},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Novel quantum inspired approaches for automatic clustering of gray level images using particle swarm optimization, spider monkey optimization and ageist spider monkey optimization algorithms},
  volume       = {88},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Side-blotched lizard algorithm: A polymorphic population
approach. <em>ASOC</em>, <em>88</em>, 106039. (<a
href="https://doi.org/10.1016/j.asoc.2019.106039">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In metaheuristic algorithms , finding the optimal balance between exploration and exploitation is a key research topic that remains open. In the nature, a reptile called Side-Blotched Lizard has achieved an interesting dynamic balance over its population. Such lizards evolved with three morphs associated with distinctive mating strategies. The synergy between the morphs generates a polymorphic population, able to self-balance the subpopulations of each morph without depleting the weakest morph. This equilibrium is achieved as the most common morph becomes the weakest, and the smaller subpopulations increase their chances of mating. In this paper, the Side-Blotched Lizard Algorithm (SBLA) is proposed to emulate the polymorphic population of the lizard. For this purpose, three operators are used to guarantee a dynamic over the population that allows the coexistence of multiple morphs. From the computational point, SBLA uses a subpopulation managing strategy which emulates the sinusoidal distribution of the lizard population over time. Even more, the mating behavior of each morph is modeled with three concepts, defensive, expansive, and sneaky. The performance of SBLA is tested on a set of five unimodal, eighteen multimodal, four composite benchmark functions , and engineering problems like; the welded beam, FM synthesizer, and rolling element bearing. To validate the results, we compared them to ten well-established algorithms and using the Wilcoxon test and the Bonferroni correction . The examination of the experimental results exhibits the accuracy, robustness and unique problem-solving method of the proposed algorithm.},
  archive      = {J_ASOC},
  author       = {Oscar Maciel C. and Erik Cuevas and Mario A. Navarro and Daniel Zaldívar and Salvador Hinojosa},
  doi          = {10.1016/j.asoc.2019.106039},
  journal      = {Applied Soft Computing},
  pages        = {106039},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Side-blotched lizard algorithm: A polymorphic population approach},
  volume       = {88},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Non-inertial opposition-based particle swarm optimization
and its theoretical analysis for deep learning applications.
<em>ASOC</em>, <em>88</em>, 106038. (<a
href="https://doi.org/10.1016/j.asoc.2019.106038">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Particle Swarm Optimization (PSO) and its variants are often used to train and optimize the structure and parameters of deep learning models to improve accuracy of learning results in the reasonable time-consuming. The performance of PSO is completed determined by its kinetic equation of particles. To accelerate convergence rate, a novel kinetic equation without inertial term is devised and applied to PSO, and then a non-inertial opposition-based particle swarm optimization (NOPSO) is generated combined with a adaptive elite mutation strategy and generalized opposition-based learning strategy. Simulation Experimental results show that the new kinetic equation has effectively accelerated convergence rate of PSO. Meanwhile, Theoretical analysis of the new kinetic equation is carried out by order-2 difference recurrence equation, the inference conclusions of which are consistent with the results of simulation experiments. NOPSO algorithm with a new kinetic equation is a highly competitive algorithm compared with some state-of-art PSOs and is suitable for deep learning applications.},
  archive      = {J_ASOC},
  author       = {Lanlan Kang and Ruey-Shun Chen and Wenliang Cao and Yeh-Cheng Chen},
  doi          = {10.1016/j.asoc.2019.106038},
  journal      = {Applied Soft Computing},
  pages        = {106038},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Non-inertial opposition-based particle swarm optimization and its theoretical analysis for deep learning applications},
  volume       = {88},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020a). A new global best guided artificial bee colony algorithm
with application in robot path planning. <em>ASOC</em>, <em>88</em>,
106037. (<a href="https://doi.org/10.1016/j.asoc.2019.106037">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Artificial bee colony has received much attention in recent years as a competitive population-based optimization algorithm . However, its slow convergence speed and one-dimensional search strategy limit it from demonstrating advantage in separable functions. To address these concerning issues, this paper introduces a coevolution framework into ABC and designs a global best leading artificial bee colony algorithm with an improved strategy to accelerate its convergence and conquer the dependency of dimension separately. A set of classical and Congress on Evolutionary Computation 2015 benchmark functions are adopted for validating the efficiency of our algorithm. In addition, in order to show the practicality of our algorithm, a robot path-planning problem is tested, and our algorithm still achieves superior results.},
  archive      = {J_ASOC},
  author       = {Feiyi Xu and Haolun Li and Chi-Man Pun and Haidong Hu and Yujie Li and Yurong Song and Hao Gao},
  doi          = {10.1016/j.asoc.2019.106037},
  journal      = {Applied Soft Computing},
  pages        = {106037},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A new global best guided artificial bee colony algorithm with application in robot path planning},
  volume       = {88},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Gradient immune-based sparse signal reconstruction algorithm
for compressive sensing. <em>ASOC</em>, <em>88</em>, 106032. (<a
href="https://doi.org/10.1016/j.asoc.2019.106032">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The reconstruction aspect is the main core of the compressive sensing theory, in which the sparse signal is reconstructed from an incomplete set of random measurements. The constraint of spare signal reconstruction is the minimization of l 0 l0 -norm, especially under noise condition. Thus, this paper proposes a new method called Gradient Immune-based Sparse Signal Reconstruction Algorithm for Compressive Sensing (GISSRA-CS) to optimize the trade-off between the reconstruction error and the sparsity requirements. The principle of the GISSRA-CS method is embedding the Gradient Local Search (GLS) method in the evolutionary process of the Immune Algorithm (IA) for solving the sparsity problem. Here, the sparsity problem is formulated as a multi-objective problem (MOP) by combining l 0 l0 and l 1 l1 - norms of a solution and l 2 l2 -norm of a residual error in the same criterion to optimize the trade-off between the sparsity requirements and the error. This MOP problem is solved in a several subproblems manner by assigning different weights for each subproblem to increase the population diversity. For a long-term sparse signal , the window method is used to divide it into multiple short signals to improve the performance and computational complexity of the proposed method. Mathematical analysis and simulation experiments are presented to validate the performance and complexity of the GISSRA-CS method. Results of different simulation scenarios based on the benchmark and simulated signals show that the GISSRA-CS method outperforms the other methods in recovering the sparse signals with a small reconstruction error from noiseless and noisy measurements. Furthermore, the convergence of GISSRA-CS is faster than the other evolutionary recovery methods, but it is slower than the traditional recovery methods.},
  archive      = {J_ASOC},
  author       = {Nabil Sabor},
  doi          = {10.1016/j.asoc.2019.106032},
  journal      = {Applied Soft Computing},
  pages        = {106032},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Gradient immune-based sparse signal reconstruction algorithm for compressive sensing},
  volume       = {88},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Self-adaptive parameter and strategy based particle swarm
optimization for large-scale feature selection problems with multiple
classifiers. <em>ASOC</em>, <em>88</em>, 106031. (<a
href="https://doi.org/10.1016/j.asoc.2019.106031">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Feature selection has been widely used in classification for improving classification accuracy and reducing computational complexity . Recently, evolutionary computation (EC) has become an important approach for solving feature selection problems. However, firstly, as the datasets processed by classifiers become increasingly large and complex, more and more irrelevant and redundant features may exist and there may be more local optima in the large-scale feature space. Therefore, traditional EC algorithms which have only one candidate solution generation strategy (CSGS) with fixed parameter values may not perform well in searching for the optimal feature subsets for large-scale feature selection problems. Secondly, many existing studies usually use only one classifier to evaluate feature subsets. To show the effectiveness of evolutionary algorithms for feature selection problems, more classifiers should be tested. Thus, in order to efficiently solve large-scale feature selection problems and to show whether the EC-based feature selection method is efficient for more classifiers, a self-adaptive parameter and strategy based particle swarm optimization (SPS-PSO) algorithm is proposed in this paper using multiple classifiers . In SPS-PSO, a representation scheme of solutions and five CSGSs have been used. To automatically adjust the CSGSs and their parameter values during the evolutionary process, a strategy self-adaptive mechanism and a parameter self-adaptive mechanism are employed in the framework of particle swarm optimization (PSO). By using the self-adaptive mechanisms, the SPS-PSO can adjust both CSGSs and their parameter values when solving different large-scale feature selection problems. Therefore, SPS-PSO has good global and local search ability when dealing with these large-scale problems. Moreover, four classifiers, i.e., k-nearest neighbor (KNN), linear discriminant analysis (LDA), extreme learning machine (ELM), and support vector machine (SVM), are individually used as the evaluation functions for testing the effectiveness of feature subsets generated by SPS-PSO. Nine datasets from the UCI Machine Learning Repository and Causality Workbench are used in the experiments. All the nine datasets have more than 600 dimensions, and two of them have more than 5, 000 dimensions. The experimental results show that the strategy and parameter self-adaptive mechanisms can improve the performance of the evolutionary algorithms , and that SPS-PSO can achieve higher classification accuracy and obtain more concise solutions than those of the other algorithms on the large-scale feature problems selected in this research. In addition, feature selection can improve the classification accuracy and reduce computational time for various classifiers. Furthermore, KNN is a better surrogate model compared with the other classifiers used in these experiments.},
  archive      = {J_ASOC},
  author       = {Yu Xue and Tao Tang and Wei Pang and Alex X. Liu},
  doi          = {10.1016/j.asoc.2019.106031},
  journal      = {Applied Soft Computing},
  pages        = {106031},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Self-adaptive parameter and strategy based particle swarm optimization for large-scale feature selection problems with multiple classifiers},
  volume       = {88},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A nonlinear method of learning neuro-fuzzy models for
dynamic control systems. <em>ASOC</em>, <em>88</em>, 106030. (<a
href="https://doi.org/10.1016/j.asoc.2019.106030">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The paper describes a new learning algorithm of adaptive neuro-fuzzy inference systems that is based on the method of areas’ ratio (MAR-ANFIS). Using linear and nonlinear functions we obtain a generalized model for fuzzy inference. Considering various implication methods, different t- or s- norms and equations for fuzzy inference composition we can change the properties of the resulting output variable. As an example, we illustrate the proposed learning algorithm and show its distinctive characteristics. Firstly, MAR-ANFIS learning algorithm is additive. Secondly, soft operators provide symmetry for the output variable. Also, the proposed algorithm that allows improving accuracy when learning fuzzy system and speed of its learning. Using detailed numerically calculated RMSE and MAPE we evaluate the proposed algorithm. High accuracy of the proposed MAR-ANFIS is confirmed through the calculation of the learning time of neuro-fuzzy network RMSE and MAPE.},
  archive      = {J_ASOC},
  author       = {Maxim V. Bobyr and Sergey G. Emelyanov},
  doi          = {10.1016/j.asoc.2019.106030},
  journal      = {Applied Soft Computing},
  pages        = {106030},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A nonlinear method of learning neuro-fuzzy models for dynamic control systems},
  volume       = {88},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020a). A novel system for multi-step electricity price forecasting
for electricity market management. <em>ASOC</em>, <em>88</em>, 106029.
(<a href="https://doi.org/10.1016/j.asoc.2019.106029">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Electricity price forecasting is an important and challenging issue for all participants in the power market because of the wide application of electricity in our society and its inherent features. In this context, some current forecasting systems use data preprocessing and optimization for theoretical and practical achievements. However, some limitations to these systems exist which need to be urgently solved. First, future information is overdrawn in the data preprocessing stage of these forecasting systems, which is actually unknown in practical applications. The crucial question, therefore, is how to develop a forecasting system without using any future information. Second, the complex features of original nonlinear and nonstationary electricity price have a negative influence on the generalization ability of these previously developed models. To decrease the negative effects on management, a method to develop a forecasting system to improve the model’s generalization ability is required. Therefore, in this study, we developed an adaptive deterministic and probabilistic interval forecasting system for multi-step electricity price forecasting, which can present more valuable information to power market decision makers . Two cases and one comparative study are provided and analyzed to validate the performance of the developed system in multi-step electricity price forecasting. Furthermore, further discussions are presented to illustrate the significance of this study, thus proving that the results of the present study fill the present knowledge gap and provide some new future directions for related studies.},
  archive      = {J_ASOC},
  author       = {Wendong Yang and Jianzhou Wang and Tong Niu and Pei Du},
  doi          = {10.1016/j.asoc.2019.106029},
  journal      = {Applied Soft Computing},
  pages        = {106029},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A novel system for multi-step electricity price forecasting for electricity market management},
  volume       = {88},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Laplacian least learning machine with dynamic updating for
imbalanced classification. <em>ASOC</em>, <em>88</em>, 106028. (<a
href="https://doi.org/10.1016/j.asoc.2019.106028">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {When handling imbalanced datasets, the existing least learning machine ignores both the interrelationship between instances and the prior knowledge about the classes and hence tends to provide unfavorable accuracies across the classes. With the Laplacian matrix based loss function, Laplacian least learning machine (L 2 MM) is developed to address the infeasibility of the existing least learning machine for imbalanced data classification in this study. In order to find out the optimal number of hidden nodes in L 2 MM, by means of the derived update equations without any re-calculation of inverse matrix , its two incremental versions (i.e., with and without regularization) are invented for dynamically updating the hidden nodes in one-by-one way, thereby saving much training time. L 2 MM and its incremental versions inherit fast learning and good generalization capability of least learning machine. Experimental results on 19 benchmarking imbalanced datasets indicate the effectiveness of the proposed incremental L 2 MM for imbalanced classification tasks .},
  archive      = {J_ASOC},
  author       = {Jie Zhou and Zhibin Jiang and Shitong Wang},
  doi          = {10.1016/j.asoc.2019.106028},
  journal      = {Applied Soft Computing},
  pages        = {106028},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Laplacian least learning machine with dynamic updating for imbalanced classification},
  volume       = {88},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Benchmarking discrete optimization heuristics with
IOHprofiler. <em>ASOC</em>, <em>88</em>, 106027. (<a
href="https://doi.org/10.1016/j.asoc.2019.106027">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Automated benchmarking environments aim to support researchers in understanding how different algorithms perform on different types of optimization problems . Such comparisons provide insights into the strengths and weaknesses of different approaches, which can be leveraged into designing new algorithms and into the automation of algorithm selection and configuration. With the ultimate goal to create a meaningful benchmark set for iterative optimization heuristics, we have recently released IOHprofiler, a software built to create detailed performance comparisons between iterative optimization heuristics. With this present work we demonstrate that IOHprofiler provides a suitable environment for automated benchmarking. We compile and assess a selection of 23 discrete optimization problems that subscribe to different types of fitness landscapes. For each selected problem we compare performances of twelve different heuristics, which are as of now available as baseline algorithms in IOHprofiler. We also provide a new module for IOHprofiler which extents the fixed-target and fixed-budget results for the individual problems by ECDF results, which allows one to derive aggregated performance statistics for groups of problems.},
  archive      = {J_ASOC},
  author       = {Carola Doerr and Furong Ye and Naama Horesh and Hao Wang and Ofer M. Shir and Thomas Bäck},
  doi          = {10.1016/j.asoc.2019.106027},
  journal      = {Applied Soft Computing},
  pages        = {106027},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Benchmarking discrete optimization heuristics with IOHprofiler},
  volume       = {88},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A priority-based intuitionistic multiplicative UTASTAR
method and its application in low-carbon tourism destination selection.
<em>ASOC</em>, <em>88</em>, 106026. (<a
href="https://doi.org/10.1016/j.asoc.2019.106026">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The intuitionistic multiplicative preference relation extends Saaty’s multiplicative preference relation by membership and non-membership functions to describe decision-makers’ preferences on objects (alternatives or criteria), and the product of pairwise membership and non-membership degrees is less than or equal to one. In recent years, the intuitionistic multiplicative preference relation has attracted increasing attention and many methods have been proposed to handle multiple criteria decision making (MCDM) problems. However, few of the existing methods can be used to tackle the MCDM problems with both quantitative and qualitative criteria. To overcome this challenge, in this paper, a novel MCDM method called the priority-based intuitionistic multiplicative UTASTAR method is proposed, which combines a new intuitionistic multiplicative prioritization method and the UTASTAR method within the context of intuitionistic multiplicative sets. The consistency checking and improving processes are considered in the weight-determining method. A case study regarding the low-carbon tourism destination selection is adopted to illustrate the applicability of the proposed method. The effectiveness and superiority of the proposed method are further verified by comparative analyses and discussions.},
  archive      = {J_ASOC},
  author       = {Cheng Zhang and Li Luo and Huchang Liao and Abbas Mardani and Dalia Streimikiene and Abdullah Al-Barakati},
  doi          = {10.1016/j.asoc.2019.106026},
  journal      = {Applied Soft Computing},
  pages        = {106026},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A priority-based intuitionistic multiplicative UTASTAR method and its application in low-carbon tourism destination selection},
  volume       = {88},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). APAD: Autoencoder-based payload anomaly detection for
industrial IoE. <em>ASOC</em>, <em>88</em>, 106017. (<a
href="https://doi.org/10.1016/j.asoc.2019.106017">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Internet of Things era is being replaced by the Internet of Everything (IoE) era, where everything can communicate with everything else. With the advent of the fourth industrial revolution and the IoE era, industrial control systems (ICSs) are transitioning into industrial IoE (IIoE). The ICS, which is no longer a closed network, suffers from various cybersecurity threats. Since the attack of Stuxnet in 2010, the malware used to attack critical infrastructure (CI) has become increasingly sophisticated. Furthermore, recent attacks have mimicked normal network traffic. Therefore, investigating intrusion detection systems is necessary to detect these advanced cyberattacks. However, detecting advanced attacks, such as false data injection, is difficult because the traditional detection methods focus only on the protocol header fields. Most studies have not considered low-performance field devices, which are vulnerable to threats. Therefore, we have classified the extended reference model RAMI 4.0, which is the new ICS reference model in the fourth industrial revolution, into two levels: an operative level and a product process management level. A fast and lightweight algorithm is required at the operative level because the low-performance devices communicate with each other in real time. In addition, efficient data processing is important because considerable amount of data is concentrated at the product process management level. Based on these characteristics, a payload-based abnormal behavior detection method, i.e., the autoencoder-based payload anomaly detection (APAD), is proposed for each level. APAD uses an autoencoder to distinguish between normal and abnormal behaviors in low-performance devices. Furthermore, traffic analysis and a considerable amount of time are required to apply a traditional detection method at the product process management level. However, the proposed method does not require long-time traffic analysis. It exhibits a higher detection rate compared with those exhibited by other methods based on verification using the open secure water treatment dataset called SWaT.},
  archive      = {J_ASOC},
  author       = {SungJin Kim and WooYeon Jo and Taeshik Shon},
  doi          = {10.1016/j.asoc.2019.106017},
  journal      = {Applied Soft Computing},
  pages        = {106017},
  shortjournal = {Appl. Soft. Comput.},
  title        = {APAD: Autoencoder-based payload anomaly detection for industrial IoE},
  volume       = {88},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Fuzzy c-means clustering using jeffreys-divergence based
similarity measure. <em>ASOC</em>, <em>88</em>, 106016. (<a
href="https://doi.org/10.1016/j.asoc.2019.106016">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In clustering, similarity measure has been one of the major factors for discovering the natural grouping of a given dataset by identifying hidden patterns. To determine a suitable similarity measure is an open problem in clustering analysis for several years. The purpose of this study is to make known a divergence based similarity measure. The notion of the proposed similarity measure is derived from Jeffrey-divergence. Various features of the proposed similarity measure are explained. Afterwards we develop fuzzy c-means (FCM) by making use of the proposed similarity measure, which guarantees to converge to local minima. The various characteristics of the modified FCM algorithm are also addressed. Some well known real-world and synthetic datasets are considered for the experiments. In addition to that two remote sensing image datasets are also adopted in this work to illustrate the effectiveness of the proposed FCM over some existing methods. All the obtained results demonstrate that FCM with divergence based proposed similarity measure outperforms three latest FCM algorithms.},
  archive      = {J_ASOC},
  author       = {Ayan Seal and Aditya Karlekar and Ondrej Krejcar and Consuelo Gonzalo-Martin},
  doi          = {10.1016/j.asoc.2019.106016},
  journal      = {Applied Soft Computing},
  pages        = {106016},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Fuzzy c-means clustering using jeffreys-divergence based similarity measure},
  volume       = {88},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A hybrid ANN-based imperial competitive algorithm
methodology for structural damage identification of slab-on-girder
bridge using data mining. <em>ASOC</em>, <em>88</em>, 106013. (<a
href="https://doi.org/10.1016/j.asoc.2019.106013">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Implementation of data mining (DM) techniques in different areas of civil engineering has recently given very good results. However, application of DM in structural health monitoring (SHM) is not used as much as expected, thus, many challenges are still ahead. Therefore, it seems a vital need is required to develop the applicability of DM in SHM. To this end, the current study attempts to present a DM-based damage detection methodology using modal parameter data, which trained by means of a hybrid artificial neural network-based imperial competitive algorithm (ANN-ICA). Likewise, the hybrid ANN is optimized by a new optimization-based evolutionary algorithm , called ICA, to predict the severity and location of multiple damage cases obtained from experimental modal analysis of intact and damaged slab-on-girder bridge structures. Furthermore, the applicability of DM approach was developed to detect the hidden patterns in vibration data using Cross Industry Standard Process for DM (CRISP-DM) tool. The performance of the model was carried out using comparison of a pre-developed ANN and ANN-ICA model.},
  archive      = {J_ASOC},
  author       = {Meisam Gordan and Hashim Abdul Razak and Zubaidah Ismail and Khaled Ghaedi and Zhi Xin Tan and Haider Hamad Ghayeb},
  doi          = {10.1016/j.asoc.2019.106013},
  journal      = {Applied Soft Computing},
  pages        = {106013},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A hybrid ANN-based imperial competitive algorithm methodology for structural damage identification of slab-on-girder bridge using data mining},
  volume       = {88},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A three-dimensional group search optimization approach for
simultaneous planning of distributed generation units and distribution
network reconfiguration. <em>ASOC</em>, <em>88</em>, 106012. (<a
href="https://doi.org/10.1016/j.asoc.2019.106012">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper deals with the simultaneous distributed generation (DG) planning and distribution network reconfiguration (SDGNR) issue. The problem is formulized as an optimization model which includes three types of variables, i.e. DGs location as the integer variables, DGs operating point as the continuous ones and switches open\close state as the binary variables. A new approach entitled three-dimensional group search optimization (3D-GSO) method is also introduced to cope with such a problem. The proposed method is a general optimization scheme applicable to all types of optimization problems which deal with an integer, continuous, and binary variables at the same time. The revised approach is successfully applied to the SDGNR problem with the objective of total loss reduction in power distribution systems. Power flow criteria, as well as operation constraints, are all together accommodated in the process of optimization. Five different scenarios at three load levels are also considered to cover all possible conditions. The validity of the proposed 3D-GSO approach in handling SDGNR problem is assured through comprehensive simulation studies on 33-bus and 69-bus test systems.},
  archive      = {J_ASOC},
  author       = {Hamid Teimourzadeh and Behnam Mohammadi-Ivatloo},
  doi          = {10.1016/j.asoc.2019.106012},
  journal      = {Applied Soft Computing},
  pages        = {106012},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A three-dimensional group search optimization approach for simultaneous planning of distributed generation units and distribution network reconfiguration},
  volume       = {88},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Non-parametric predictive inference for solving multi-label
classification. <em>ASOC</em>, <em>88</em>, 106011. (<a
href="https://doi.org/10.1016/j.asoc.2019.106011">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Decision Trees (DTs) have been adapted to Multi-Label Classification (MLC). These adaptations are known as Multi-Label Decision Trees (ML-DT). In this research, a new ML-DT based on the Nonparametric Predictive Inference Model on Multinomial data (NPI-M) is proposed. The NPI-M is an imprecise probabilities model that provides good results when it is applied to DTs in standard classification. Unlike other models based on imprecise probabilities, the NPI-M is a nonparametric approach and it does not make unjustified assumptions before observing data. It is shown that the new ML-DT based on the NPI-M is more robust to noise than the ML-DT based on precise probabilities. As the intrinsic noise in MLC might be higher than in traditional classification, it is expected that the new ML-DT based on the NPI-M outperforms the already existing ML-DT. This fact is validated with an exhaustive experimentation carried out in this work on different MLC datasets with several levels of added noise. In it, many MLC evaluation metrics are employed in order to measure the performance of the algorithms. The experimental analysis shows that the proposed ML-DT based on NPI-M obtains better results than the ML-DT that uses precise probabilities, especially when we work on data with noise.},
  archive      = {J_ASOC},
  author       = {Serafín Moral-García and Carlos J. Mantas and Javier G. Castellano and Joaquín Abellán},
  doi          = {10.1016/j.asoc.2019.106011},
  journal      = {Applied Soft Computing},
  pages        = {106011},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Non-parametric predictive inference for solving multi-label classification},
  volume       = {88},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A multi-population differential evolution algorithm based on
cellular learning automata and evolutionary context information for
optimization in dynamic environments. <em>ASOC</em>, <em>88</em>,
106009. (<a href="https://doi.org/10.1016/j.asoc.2019.106009">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a multi-population differential evolution algorithm to address dynamic optimization problems. In the proposed approach, a cellular learning automaton adjusts the behavior of each subpopulation by adaptively controlling its updating schemes. As the environment changes over time, an evolving population may go through a quite number of state transitions. Each state demands specific characteristics from an optimizer; hence, an adapted evolutionary scheme for one state may be unsuitable for the upcoming ones. Additionally, a learning approach may have limited time to adapt to a newly encountered state due to the frequentness of the environmental changes. Hence, it is infeasible for a dynamic optimizer to unlearn its existing beliefs to accommodate the practices required to embrace newly encountered states. In order to address this issue, we introduce a context dependent learning approach, which can adapt the behavior of each subpopulation according to the contexts of its different states. The performance of the proposed approach is compared with several state-of-the-art dynamic optimizers over the GDBG benchmark set. Comparison results indicate that the proposed method can achieve statistically superior performances on a wide range of tested instances.},
  archive      = {J_ASOC},
  author       = {Reza Vafashoar and Mohammad Reza Meybodi},
  doi          = {10.1016/j.asoc.2019.106009},
  journal      = {Applied Soft Computing},
  pages        = {106009},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A multi-population differential evolution algorithm based on cellular learning automata and evolutionary context information for optimization in dynamic environments},
  volume       = {88},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A two-step method for damage identification in moment frame
connections using support vector machine and differential evolution
algorithm. <em>ASOC</em>, <em>88</em>, 106008. (<a
href="https://doi.org/10.1016/j.asoc.2019.106008">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The main aim of this study is to introduce a two-step method for damage identification in moment frame connections using a support vector machine (SVM) and differential evolution algorithm (DEA). In the first step, the potential location of damage in connections is determined through SVM leading to reducing the dimension of the search space. Then, the accurate location and precise amount of damage in connections are determined in the second step via DEA with a high speed. In order to simulate damage in connections, a moment frame is modeled through semi-rigid beam to column connections and the analytical model is used to randomly generate structures with damaged connections as data. Then, SVM is trained and tested using this data, to facilitate natural frequencies are considered as input data and the characteristics of damage in beam to column connections are considered as output data of the network. Now, the possible location of the damage in connections can be determined using the SVM trained. The accurate location and severity of damage are determined by DEA based on the prediction of SVM in the first step. In order to assess the efficiency of the proposed method, two numerical examples are considered with different damage cases and considering noise. A comparative study is also made to judge the performance of the method with that of a work available in the literature. The outcome shows the high efficiency of the proposed method to identify the location and severity of the damage in moment frame connections.},
  archive      = {J_ASOC},
  author       = {Seyed Mohammad Seyedpoor and Mohammad Hossein Nopour},
  doi          = {10.1016/j.asoc.2019.106008},
  journal      = {Applied Soft Computing},
  pages        = {106008},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A two-step method for damage identification in moment frame connections using support vector machine and differential evolution algorithm},
  volume       = {88},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Distribution linguistic preference relations with incomplete
symbolic proportions for group decision making. <em>ASOC</em>,
<em>88</em>, 106005. (<a
href="https://doi.org/10.1016/j.asoc.2019.106005">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Distribution linguistic preference relations (DLPRs) with complete symbolic proportions have been recently investigated to record the comparison information coming from decision makers (DMs) in the context of linguistic decisions. Due to various reasons such as a lack of experience and partial knowledge about the pairs of decision alternatives, it is not always easy for DMs to provide complete symbolic proportions in DLPRs. In this paper, we propose a new style of pairwise comparison called DLPR with incomplete symbolic proportions to represent DMs’ comparison information. Two aggregation operators for DLPRs with incomplete symbolic proportions and their desirable properties are presented. An expectation-based numerical preference relation (EBNPR) is deduced from a DLPR with incomplete symbolic proportions using numerical scale models. The consistency of DLPR with incomplete symbolic proportions is defined via its associated EBNPR. On the other hand, solving linguistic decision problems implies the need for invoking the principles of computing with words (CW). The key point about CW is that words might exhibit different meaning for different people. Hence, another aim of this paper is to deal with the point about CW by setting personalized numerical scales of linguistic terms for different DMs in group decision making (GDM) with the newly introduced preference relations. Several numerical scale computation models are developed to personalize numerical scales for each DM to show their individual difference in understanding the meaning of words. Finally, we present the applications of the aforesaid theoretical results to GDM situations, which are demonstrated by solving a GDM problem of evaluating and selecting research projects.},
  archive      = {J_ASOC},
  author       = {Xiaoan Tang and Qiang Zhang and Zhanglin Peng and Witold Pedrycz and Shanlin Yang},
  doi          = {10.1016/j.asoc.2019.106005},
  journal      = {Applied Soft Computing},
  pages        = {106005},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Distribution linguistic preference relations with incomplete symbolic proportions for group decision making},
  volume       = {88},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Quantifying reusability of software components using hybrid
fuzzy analytical hierarchy process (FAHP)-metrics approach.
<em>ASOC</em>, <em>88</em>, 105997. (<a
href="https://doi.org/10.1016/j.asoc.2019.105997">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The role of reusability cannot be neglected in the software component selection since it determines the worth of a component for its potential (re)use. The reusability is a qualitative feature and it is arduous to measure it directly. Typically, it is assessed subjectively without paying much heed to the involvement of befitting stakeholders and the use of a quantitative approach. We aim to quantify the reusability of a software component based on its quality through multi-criteria decision making (MCDM) solution to rank alternatives. To do this, the quality preferences of stakeholders about the reuse of components are determined. The reusability is quantified using a hybrid fuzzy analytic hierarchy process (FAHP) and quality metrics approach. In this approach, the weights of both FAHP and quality metrics are integrated to get the final ranking. FAHP-Metrics approach is applied to payment gateway (software) components, and the ranking of given components is obtained based on their reusability value. Finally, the comparative analysis of our results with respect to other recent studies reveals that the Spearman rank correlation result is highly significant and acceptable, and the obtained weights have a good association with other studies.},
  archive      = {J_ASOC},
  author       = {Simrandeep Singh Thapar and Himali Sarangal},
  doi          = {10.1016/j.asoc.2019.105997},
  journal      = {Applied Soft Computing},
  pages        = {105997},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Quantifying reusability of software components using hybrid fuzzy analytical hierarchy process (FAHP)-metrics approach},
  volume       = {88},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Application of neural network and weighted improved PSO for
uncertainty modeling and optimal allocating of renewable energies along
with battery energy storage. <em>ASOC</em>, <em>88</em>, 105979. (<a
href="https://doi.org/10.1016/j.asoc.2019.105979">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The power uncertainty of renewable energy encounters their optimal allocation with obstacle in an on-grid microgrids . Hence, power prediction is a logical tactic. Besides, optimal performance of battery energy storage for compensating electrical loads is another strategy. However, these two solutions must meet the economic expectations as well as reliability of customers. In this paper, generating section includes wind, solar and wave energies in which wind speed and solar irradiance are uncertain parameters and information of Hormoz Island, Iran is the input of problem. Hence, artificial neural network (DANN) is trained by three adaptive techniques to minimize the prediction error dynamically. Then, eco-statistic objective function, reliability criterion and efficient battery strategy are modeled thoroughly. After introducing five feasible scenarios, a heuristic algorithm which is weighted improved PSO , searches the global answers of optimization to test the efficiency of hybrid architecture . Moreover, forecasting results are compared with five adaptive neural network based fuzzy inference system (ANFIS) and one conventional Levenberg Marquardt (LM) based ANN for addressing the functionality of proposed DANN against the uncertainty of renewables. Consequently, simulation results of optimization are tested with heuristic algorithms to demonstrate the optimal answers of problem.},
  archive      = {J_ASOC},
  author       = {Amin Masoumi and Saeid Ghassem-zadeh and Seyed Hossein Hosseini and Behnam Zamanzad Ghavidel},
  doi          = {10.1016/j.asoc.2019.105979},
  journal      = {Applied Soft Computing},
  pages        = {105979},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Application of neural network and weighted improved PSO for uncertainty modeling and optimal allocating of renewable energies along with battery energy storage},
  volume       = {88},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Maintaining filter structure: A gabor-based convolutional
neural network for image analysis. <em>ASOC</em>, <em>88</em>, 105960.
(<a href="https://doi.org/10.1016/j.asoc.2019.105960">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In image segmentation and classification tasks, utilizing filters based on the target object improves performance and requires less training data. We use the Gabor filter as initialization to gain more discriminative power . Considering the mechanism of the error backpropagation procedure to learn the data, after a few updates, filters will lose their initial structure. In this paper, we modify the updating rule in Gradient Descent to maintain the properties of Gabor filters. We use the Left Ventricle (LV) segmentation task and handwritten digit classification task to evaluate our proposed method. We compare Gabor initialization with random initialization and transfer learning initialization using convolutional autoencoders and convolutional networks . We experimented with noisy data and we reduced the amount of training data to compare how different methods of initialization can deal with these matters. The results show that the pixel predictions for the segmentation task are highly correlated with the ground truth. In the classification task , in addition to Gabor and random initialization, we initialized the network using pre-trained weights obtained from a convolutional Autoencoder using two different data sets and pre-trained weights obtained from a convolutional neural network . The experiments confirm the out-performance of Gabor filters comparing to the other initialization method even when using noisy inputs and a lesser amount of training data.},
  archive      = {J_ASOC},
  author       = {Somayeh Molaei and Mohammad Ebrahim Shiri Ahmad Abadi},
  doi          = {10.1016/j.asoc.2019.105960},
  journal      = {Applied Soft Computing},
  pages        = {105960},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Maintaining filter structure: A gabor-based convolutional neural network for image analysis},
  volume       = {88},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Knowledge distilling based model compression and feature
learning in fault diagnosis. <em>ASOC</em>, <em>88</em>, 105958. (<a
href="https://doi.org/10.1016/j.asoc.2019.105958">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, there has been interest in developing diagnosis methods that combine model-based and data-driven diagnosis. In both approaches, selecting the relevant measurements or extracting important features from historical data is a key determiner of the success of the algorithm. Recently, deep learning methods have been effective in automating the feature selection process. Autoencoders have been shown to be an effective neural network configuration for extracting features from complex data, however, they may also learn irrelevant features. In addition, end-to-end classification neural networks have also been used for diagnosis, but like autoencoders, this method may also learn unimportant features thus making the diagnostic inference scheme inefficient. To rapidly extract significant fault features, this paper employs end-to-end networks and develops a new feature extraction method based on importance analysis and knowledge distilling. First, a set of cumbersome neural network models are trained to predict faults and some of their internal values are defined as features. Then an occlusion-based importance analysis method is developed to select the most relevant input variables and learned features. Finally, a simple student neural network model is designed based on the previous analysis results and an improved knowledge distilling method is proposed to train the student model. Because of the way the cumbersome networks are trained, only fault features are learned, with the importance analysis further pruning the relevant feature set. These features can be rapidly generated by the student model. We discuss the algorithms, and then apply our method to two typical dynamic systems, a communication system and a 10-tank system employed to demonstrate the proposed approach.},
  archive      = {J_ASOC},
  author       = {Wenfeng Zhang and Gautam Biswas and Qi Zhao and Hongbo Zhao and Wenquan Feng},
  doi          = {10.1016/j.asoc.2019.105958},
  journal      = {Applied Soft Computing},
  pages        = {105958},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Knowledge distilling based model compression and feature learning in fault diagnosis},
  volume       = {88},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A two phase removing algorithm for minimum independent
dominating set problem. <em>ASOC</em>, <em>88</em>, 105949. (<a
href="https://doi.org/10.1016/j.asoc.2019.105949">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The minimum independent dominating set (MIDS) problem is a famous combinatorial optimization problem and is widely used in real-world domains. In this paper, we design a novel local search algorithm with tabu method and two phase removing strategies including double-checked removing strategy and random diversity removing strategy to solve the MIDS problem. The first removing strategy checks and then removes the second-level neighbourhood of the just removal vertex to break the limitation of the independence property. When the quality of candidate solution has not been improved after some steps, the second removing strategy dynamically and greedily removes lots of vertices so that the current candidate solution can escape from suboptimal search space, and then we introduce the random walk into the repair process. Experiments are carried out on two classical benchmarks named DIMACS and BHOSLIB, and the results show that the proposed algorithm significantly outperforms the previous state-of-the-art MIDS heuristic algorithms .},
  archive      = {J_ASOC},
  author       = {Yiyuan Wang and Chenxi Li and Minghao Yin},
  doi          = {10.1016/j.asoc.2019.105949},
  journal      = {Applied Soft Computing},
  pages        = {105949},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A two phase removing algorithm for minimum independent dominating set problem},
  volume       = {88},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A cooperative advertising collaboration policy in supply
chain management under uncertain conditions. <em>ASOC</em>, <em>88</em>,
105948. (<a href="https://doi.org/10.1016/j.asoc.2019.105948">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many industries are facing big challenges to design supply chains in a way to maximize the profit and meet the heightened expectations of the customer. This new era entirely relies on the dynamic advantages of competition and the role played by the collaboration policy. A global economy and increasing demand have put a huge pressure on supply chain partners to build a collaboration policy based on price, order quantity, and advertising. Companies are adopting the idea of ”shaking hands” to obtain more profit instead of taking risks through competition. Cooperative (co-op) advertising is a significant policy of centralized supply chain management (SCM) to boost the revenues generated by the supplier, manufacturer, and retailers. The uncertain costs associated with the supply chain management also create obstacles in economic analysis and feasibility. These uncertainties are associated with the basic costs of all supply chain partners, which are represented using a signed distance formula. This paper develops the concept of co-op advertising among the supplier, manufacturer, and retailers with a variable demand driven by selling price and advertising costs, where all basic costs are considered as fuzzy. The profit is optimized by considering variable cycle time, shipments, pricing and advertising costs for the decision support system of the supply chain management. The optimal results of the co-op advertisement ensured an increase in the revenue of whole supply chain.},
  archive      = {J_ASOC},
  author       = {Biswajit Sarkar and Muhammad Omair and Namhun Kim},
  doi          = {10.1016/j.asoc.2019.105948},
  journal      = {Applied Soft Computing},
  pages        = {105948},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A cooperative advertising collaboration policy in supply chain management under uncertain conditions},
  volume       = {88},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Chaotic multi-swarm whale optimizer boosted support vector
machine for medical diagnosis. <em>ASOC</em>, <em>88</em>, 105946. (<a
href="https://doi.org/10.1016/j.asoc.2019.105946">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Support vector machine ( SVM ) is a widely used pattern classification method that its classification accuracy is greatly influenced by both kernel parameter setting and feature selection. Therefore, in this study, to perform parameter optimization and feature selection simultaneously for SVM , we propose an improved whale optimization algorithm ( CMWOA ), which combines chaotic and multi-swarm strategies. Using several well-known medical diagnosis problems of breast cancer, diabetes, and erythemato-squamous, the proposed SVM model, termed CMWOAFS-SVM , was compared with multiple competitive SVM models based on other optimization algorithms including the original algorithm, particle swarm optimization , bacterial foraging optimization, and genetic algorithms . The experimental results demonstrate that CMWOAFS-SVM significantly outperformed all the other competitors in terms of classification performance and feature subset size.},
  archive      = {J_ASOC},
  author       = {Mingjing Wang and Huiling Chen},
  doi          = {10.1016/j.asoc.2019.105946},
  journal      = {Applied Soft Computing},
  pages        = {105946},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Chaotic multi-swarm whale optimizer boosted support vector machine for medical diagnosis},
  volume       = {88},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). IBGSS: An improved binary gravitational search algorithm
based search strategy for QoS and ranking prediction in cloud
environments. <em>ASOC</em>, <em>88</em>, 105945. (<a
href="https://doi.org/10.1016/j.asoc.2019.105945">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Quality of Service (QoS) value prediction and QoS ranking prediction have their significance in optimal service selection and service composition problems . QoS based service ranking prediction is an NP-Complete problem which examines the order of ranked service sequence with respect to the unique QoS requirements. To address the NP-Complete problem, greedy and optimization-based strategies such as CloudRank and PSO have been widely employed in service oriented environments. However, they pose several challenges with respect to the similarity measure based QoS prediction, trap at local optima, and near optimal solution. Hence, this paper presents Improved Binary Gravitational Search Strategy (IBGSS), an optimization based search strategy to address the challenges in the state-of-the-art QoS value prediction and service ranking prediction techniques. IBGSS employs improved cosine similarity measure, and Newton–Raphson inspired Binary Gravitational Search Algorithm (NR-BGSA) for accurate QoS value prediction and optimal service ranking prediction respectively. The effectiveness of IBGSS over the state-of-the-art QoS value prediction and ranking prediction techniques was validated using two real world QoS datasets, namely WSDream#1 and web service QoS dataset in terms of various statistical measures (Mean Absolute Error (MAE), Root Mean Square Error (RMSE), and Average Precision Correlation (APC)).},
  archive      = {J_ASOC},
  author       = {Nivethitha Somu and Gauthama Raman M.R. and Akshya Kaveri and Akshay Rahul K. and Kannan Krithivasan and Shankar Sriram V.S.},
  doi          = {10.1016/j.asoc.2019.105945},
  journal      = {Applied Soft Computing},
  pages        = {105945},
  shortjournal = {Appl. Soft. Comput.},
  title        = {IBGSS: An improved binary gravitational search algorithm based search strategy for QoS and ranking prediction in cloud environments},
  volume       = {88},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A hybrid wavelet decomposer and GMDH-ELM ensemble model for
network function virtualization workload forecasting in cloud computing.
<em>ASOC</em>, <em>88</em>, 105940. (<a
href="https://doi.org/10.1016/j.asoc.2019.105940">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nowadays Network function virtualization (NFV) has drawn immense attention from many cloud providers because of its benefits. NFV enables networks to virtualize node functions such as firewalls, load balancers, and WAN accelerators, conventionally running on dedicated hardware, and instead implements them as virtual software components on standard servers, switches, and storages. In order to provide NFV resources and meet Service Level Agreement (SLA) conditions, minimize energy consumption and utilize physical resources efficiently, resource allocation in the cloud is an essential task. Since network traffic is changing rapidly, an optimized resource allocation strategy should consider resource auto-scaling property for NFV services. In order to scale cloud resources, we should forecast the NFV workload. Existing forecasting methods are providing poor results for highly volatile and fluctuating time series such as cloud workloads. Therefore, we propose a novel hybrid wavelet time series decomposer and GMDH-ELM ensemble method named Wavelet-GMDH-ELM (WGE) for NFV workload forecasting which predicts and ensembles workload in different time-frequency scales. We evaluate the WGE model with three real cloud workload traces to verify its prediction accuracy and compare it with state of the art methods . The results show the proposed method provides better average prediction accuracy. Especially it improves Mean Absolute Percentage Error (MAPE) at least 8\% compared to the rival forecasting methods such as support vector regression (SVR) and Long short term memory (LSTM).},
  archive      = {J_ASOC},
  author       = {Sima Jeddi and Saeed Sharifian},
  doi          = {10.1016/j.asoc.2019.105940},
  journal      = {Applied Soft Computing},
  pages        = {105940},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A hybrid wavelet decomposer and GMDH-ELM ensemble model for network function virtualization workload forecasting in cloud computing},
  volume       = {88},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Glucose forecasting combining markov chain based enrichment
of data, random grammatical evolution and bagging. <em>ASOC</em>,
<em>88</em>, 105923. (<a
href="https://doi.org/10.1016/j.asoc.2019.105923">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Diabetes Mellitus is a disease affecting more and more people every year. Depending on the kind of diabetes and sometimes on the stage of the illness, diabetic patients have to inject some amount of artificial insulin, namely bolus, before the meals, to make up the absence or malfunctioning of their natural insulin. This decision is a difficult task since they need to estimate the number of carbohydrates they are going to ingest, take into account the past and future circumstances, know the past values of glucose, evaluate if the effect of previously injected insulin has already finished and any other relevant information. In this paper, we present and compare a set of methodologies to automate the decision of the insulin bolus, which reduces the number of dangerous predictions. We combine two different data enrichment techniques based on Markov chains with grammatical evolution engines to generate models of blood glucose , and univariate marginal distribution algorithms and bagging techniques to select the set of models to assemble. In particular, we propose the Random-GE procedure, an adaptation of Random Forests to Grammatical Evolution, which leads to excellent prediction models, with a simple configuration and a reduced execution time. The ensemble gives the prediction of glucose for a duple of food and insulins, helping patients in the selection of the appropriate bolus to maintain healthy glucose levels after the meals. Experimental results show that our models get more accurate and robust predictions than previous approaches.},
  archive      = {J_ASOC},
  author       = {J. Ignacio Hidalgo and Marta Botella and J. Manuel Velasco and Oscar Garnica and Carlos Cervigón and Remedios Martínez and Aranzazu Aramendi and Esther Maqueda and Juan Lanchares},
  doi          = {10.1016/j.asoc.2019.105923},
  journal      = {Applied Soft Computing},
  pages        = {105923},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Glucose forecasting combining markov chain based enrichment of data, random grammatical evolution and bagging},
  volume       = {88},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Identifying good algorithm parameters in evolutionary multi-
and many-objective optimisation: A visualisation approach.
<em>ASOC</em>, <em>88</em>, 105902. (<a
href="https://doi.org/10.1016/j.asoc.2019.105902">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Evolutionary algorithms are often highly dependent on the correct setting of their parameters, and benchmarking different parametrisations allows a user to identify which parameters offer the best performance on their given problem. Visualisation offers a way of presenting the results of such benchmarking so that a non-expert user can understand how their algorithm is performing. By examining the characteristics of their algorithm, such as convergence and diversity, the user can learn how effective their chosen algorithm parametrisation is. This paper presents a technique intended to offer this insight, by presenting the relative performance of a set of EAs optimising the same multi-objective problem in a simple visualisation. The visualisation characterises the behaviour of the algorithm in terms of known performance indicators drawn from the literature, and is capable of visualising the optimisation of many-objective problems also. The method is demonstrated with benchmark test problems from the popular DTLZ and CEC 2009 problem suites, optimising them with different parametrisations of both NSGA-II and NSGA-III, and it is shown that known characteristics of optimisers solving these problems can be observed in the visualisations resulting.},
  archive      = {J_ASOC},
  author       = {David J. Walker and Matthew J. Craven},
  doi          = {10.1016/j.asoc.2019.105902},
  journal      = {Applied Soft Computing},
  pages        = {105902},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Identifying good algorithm parameters in evolutionary multi- and many-objective optimisation: A visualisation approach},
  volume       = {88},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A multi-objective perspective on performance assessment and
automated selection of single-objective optimization algorithms.
<em>ASOC</em>, <em>88</em>, 105901. (<a
href="https://doi.org/10.1016/j.asoc.2019.105901">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We build upon a recently proposed multi-objective view onto performance measurement of single-objective stochastic solvers. The trade-off between the fraction of failed runs and the mean runtime of successful runs – both to be minimized – is directly analyzed based on a study on algorithm selection of inexact state-of-the-art solvers for the famous Traveling Salesperson Problem (TSP). Moreover, we adopt the hypervolume indicator (HV) commonly used in multi-objective optimization for simultaneously assessing both conflicting objectives and investigate relations to commonly used performance indicators, both theoretically and empirically. Next to Penalized Average Runtime (PAR) and Penalized Quantile Runtime (PQR), the HV measure is used as a core concept within the construction of per-instance algorithm selection models offering interesting insights into complementary behavior of inexact TSP solvers.},
  archive      = {J_ASOC},
  author       = {Jakob Bossek and Pascal Kerschke and Heike Trautmann},
  doi          = {10.1016/j.asoc.2019.105901},
  journal      = {Applied Soft Computing},
  pages        = {105901},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A multi-objective perspective on performance assessment and automated selection of single-objective optimization algorithms},
  volume       = {88},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Extending social responsibility to small and medium-sized
suppliers in supply chains: A fuzzy-set qualitative comparative
analysis. <em>ASOC</em>, <em>88</em>, 105899. (<a
href="https://doi.org/10.1016/j.asoc.2019.105899">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Buying firms’ sales and reputation will be greatly damaged by any non-responsible behaviors on the part of suppliers, especially when those suppliers, like small and medium-sized enterprises (SMEs), have restricted resources and capabilities. To eradicate these risks, a growing number of buying firms have introduced socially responsible supplier development (SRSD). SRSD, including monitoring and evaluating suppliers, can provide them with incentives and assistance. Based on configuration theory and contingency theory, fuzzy-set qualitative comparative analysis (fsQCA) is adopted in this study to examine how SRSD practices adopted by buying firms, supply chain partnership, and market turbulence affect the corporate social responsibility (CSR) performance of their SME suppliers. We find that, as core factors, supplier monitoring, supplier assistance, and supply chain partnership can work together with peripheral conditions to achieve superior CSR performance. In addition, even at different levels of market turbulence, superior CSR performance can be realized through different causal configurations.},
  archive      = {J_ASOC},
  author       = {Guangyu Huang and Yang Tong and Fei Ye and Jinhua Li},
  doi          = {10.1016/j.asoc.2019.105899},
  journal      = {Applied Soft Computing},
  pages        = {105899},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Extending social responsibility to small and medium-sized suppliers in supply chains: A fuzzy-set qualitative comparative analysis},
  volume       = {88},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Immune/neural approach to characterize salivary gland
neoplasms (SGN). <em>ASOC</em>, <em>88</em>, 105877. (<a
href="https://doi.org/10.1016/j.asoc.2019.105877">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The purpose of the current study was to use immune-inspired algorithm ClonALG whose performance is increased by using the Kohonen neural network training algorithm (Immune/Neural approach), to characterize the nature of salivary gland neoplasms (SGNs). The leader gene approach in order to identify biomarkers for SGNs. Extensive data were obtained for each of the 35 types of neoplasms. The gene leaders for each type of SGN were identified in a table and then divided according to the two different methods: K-means clustering and Immune/Neural approach. Genes related to SGNs were identified using PubMed, OMIM and Genecards databases. A bioinformatics algorithm was then applied, and the STRING database was employed to build networks of protein–protein interactions for each nature of an SGNs. The weighted number of links (WNL) and total interactions score (TIS) values were then obtained. Finally, the genes were clustered, and the gene leaders were identified using the K-means clustering method and the Immune/Neural approach.},
  archive      = {J_ASOC},
  author       = {Carlos Rafael Lima Monção and Eloa Mangabeira Santos and Thiago Silva Prates and Alfredo Maurício Batista de Paula and Claudio Marcelo Cardoso and Lucyana Conceição Farias and Sérgio Henrique Sousa Santos and Marcos Flávio Silveira Vasconcelos D’Angelo and André Luiz Sena Guimarães},
  doi          = {10.1016/j.asoc.2019.105877},
  journal      = {Applied Soft Computing},
  pages        = {105877},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Immune/Neural approach to characterize salivary gland neoplasms (SGN)},
  volume       = {88},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020d). Fibonacci multi-modal optimization algorithm in noisy
environment. <em>ASOC</em>, <em>88</em>, 105874. (<a
href="https://doi.org/10.1016/j.asoc.2019.105874">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Noises are very common in practical optimization problems . It will cause interference on optimization algorithms and thus makes the algorithms difficult to find a true global extreme point and multiple local extreme points. For the problem, this paper proposes a Fibonacci multi-modal optimization (FMO) algorithm. Firstly, the proposed algorithm alternates between global search and local optimization in order not to fall into local optimum points and to retain multiple optimum points. And then, a Fibonacci regional scaling criterion is proposed in the FMO algorithm to alleviate the effects of noise, and the position of optimum point is determined according to its probability distribution under noise interference. In experiments, we evaluate the performance of the proposed FMO algorithm through 35 benchmark functions . The experimental results show that compared with Particle Swarm Optimization (PSO) algorithm, three improved versions of PSO, and Genetic algorithm (GA), the proposed FMO algorithm can gain more accurate location of optimum point and more global and local extreme points under noisy environment . Finally, an example of practical optimization in radio spectrum monitoring is used to show the performance of the FMO algorithm.},
  archive      = {J_ASOC},
  author       = {Xia Wang and Yaomin Wang and Haifeng Wu and Lian Gao and Li Luo and Peng Li and Xinling Shi},
  doi          = {10.1016/j.asoc.2019.105874},
  journal      = {Applied Soft Computing},
  pages        = {105874},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Fibonacci multi-modal optimization algorithm in noisy environment},
  volume       = {88},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Type II fuzzy set-based data analytics to explore amino acid
associations in protein sequences of swine influenza virus.
<em>ASOC</em>, <em>88</em>, 105856. (<a
href="https://doi.org/10.1016/j.asoc.2019.105856">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The veracity present in molecular data available in biological databases possesses new challenges for data analytics . The analysis of molecular data of various diseases can provide vital information for developing better understanding of the molecular mechanism of a disease. In this paper, an attempt has been made to propose a model that addresses the issue of veracity in data analytics for amino acid association patterns in protein sequences of Swine Influenza Virus. The veracity is caused by intra-sequential and inter-sequential biases present in the sequences due to varying degrees of relationships among amino acids. A complete dataset of 63, 682 protein sequences is downloaded from NCBI and is refined. The refined dataset consists of 26, 594 sequences which are employed in the present study. The type I fuzzy set is employed to explore amino acid association patterns in the dataset. The type I fuzzy support is refined to partially remove the inter-sequential biases causing veracity in data. The remaining inter-sequential biases present in refined fuzzy support are evaluated and eliminated using type II fuzzy set. Hence, it is concluded that a combination of type II fuzzy &amp; refined fuzzy approach is the optimal approach for extracting a better picture of amino acid association patterns in the molecular dataset.},
  archive      = {J_ASOC},
  author       = {Alekh Gour and K.R. Pardasani},
  doi          = {10.1016/j.asoc.2019.105856},
  journal      = {Applied Soft Computing},
  pages        = {105856},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Type II fuzzy set-based data analytics to explore amino acid associations in protein sequences of swine influenza virus},
  volume       = {88},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Research on circular area search algorithm of multi-robot
service based on SOA cloud platform. <em>ASOC</em>, <em>88</em>, 105816.
(<a href="https://doi.org/10.1016/j.asoc.2019.105816">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Considering autonomous mobile robots with a variety of specific functions as a kind of service, when there are many types and quantities of services and the density of regional services is large, proposing an algorithm of Circular Area Search (CAS) because of the problem of multi-robot service scheduling in various areas. Firstly, Django is used as the web framework to build the Service-Oriented Architecture (SOA) multi-robot service cloud platform, which is the basic platform for multi-service combination. Then, the service type, the latitude and longitude and the scoring parameters of the service are selected as the service search metrics to design the CAS algorithm that based on the existing service information registered in MySQL and the Gaode Map for screening optimal service, and then providing the service applicant with the best service. Finally, the service applicant applies for the self-driving tour service as an example to perform performance simulation test on the proposed CAS algorithm. The results show that the CAS algorithm of the multi-robot service cloud platform proposed in this paper is practical compared to the global search. And compared with the Greedy Algorithm experiment, the service search time is reduced about 58\% compared with the Greedy Algorithm, which verifies the efficiency of CAS algorithm.},
  archive      = {J_ASOC},
  author       = {HaiBo Zhou and JianJun Zhang and ZhenZhong Liu and Dong Nie and WanQing Wu and Victor Hugo C. de Albuquerque},
  doi          = {10.1016/j.asoc.2019.105816},
  journal      = {Applied Soft Computing},
  pages        = {105816},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Research on circular area search algorithm of multi-robot service based on SOA cloud platform},
  volume       = {88},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). An improved feature extraction method using texture analysis
with LBP for bearing fault diagnosis. <em>ASOC</em>, <em>87</em>,
106019. (<a href="https://doi.org/10.1016/j.asoc.2019.106019">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Bearings are one of the most widespread components used for energy transformation in machines. Mechanical wear and faulty bearings reduce the efficiency of rotating machines and thus increase energy consumption. The feature extraction process is an essential part of fault diagnosis in bearings. In order to diagnose the fault caused by the bearing correctly, it is necessary to determine an effective feature extraction method that best describes the fault. In this study, a new approach based on texture analysis is proposed for diagnosing bearing vibration signals . Bearing vibration signals were first converted to gray scale images . It can be understood from the images that the signals of different bearing failures form different textures. Then, using these images, LBP (Local Binary Pattern) and texture features were obtained. Using these features, different machine learning models and bearing vibration signals are classified. Three different data sets were created to test the proposed approach. For the first data set, the signals composed of very close velocities were classified. 95.9\% success rate was observed for the first data set. The second data set consists of faulty signals at different parts of the bearing (inner ring, outer ring and ball) measured in the same RPM. The type of fault has been determined, and a 100\% success rate was obtained for this data set. The final data set is composed of the fault size dimensions (mm) of different ratios. With the proposed approach, a 100\% success rate was obtained in the classification of these signals. As a result, it was observed that the obtained feature had promising results for three different data types and was more successful than the traditional methods.},
  archive      = {J_ASOC},
  author       = {Kaplan Kaplan and Yılmaz Kaya and Melih Kuncan and Mehmet Recep Mi̇naz and H. Metin Ertunç},
  doi          = {10.1016/j.asoc.2019.106019},
  journal      = {Applied Soft Computing},
  pages        = {106019},
  shortjournal = {Appl. Soft. Comput.},
  title        = {An improved feature extraction method using texture analysis with LBP for bearing fault diagnosis},
  volume       = {87},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). The laser-induced damage change detection for optical
elements using siamese convolutional neural networks. <em>ASOC</em>,
<em>87</em>, 106015. (<a
href="https://doi.org/10.1016/j.asoc.2019.106015">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to the fact that weak and fake laser-induced damages may occur in the surface of optical elements in high-energy laser facilities, it is still a challenging issue to effectively detect the real laser-induced damage changes of optical elements in optical images. Different from the traditional methods, in this paper, we put forward a similarity metric optimization driven supervised learning model to perform the laser-induced damage change detection task. In the proposed model, an end-to-end siamese convolutional neural network is designed and trained which can integrate the difference image generating and difference image analysis into a whole network. Thus, the damage changes can be highlighted by the pre-trained siamese network that classifies the central pixel between input multi-temporal image patches into changed and unchanged classes. To address the problem of unbalanced distribution between positive and negative samples, a modified average frequency balancing based weighted softmax loss is used to train the proposed network. Experiments conducted on two real datasets demonstrate the effectiveness and superiority of the proposed model.},
  archive      = {J_ASOC},
  author       = {Jingwei Kou and Tao Zhan and Deyun Zhou and Wei Wang and Zhengshang Da and Maoguo Gong},
  doi          = {10.1016/j.asoc.2019.106015},
  journal      = {Applied Soft Computing},
  pages        = {106015},
  shortjournal = {Appl. Soft. Comput.},
  title        = {The laser-induced damage change detection for optical elements using siamese convolutional neural networks},
  volume       = {87},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Community detection in networks using bio-inspired
optimization: Latest developments, new results and perspectives with a
selection of recent meta-heuristics. <em>ASOC</em>, <em>87</em>, 106010.
(<a href="https://doi.org/10.1016/j.asoc.2019.106010">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Detecting groups within a set of interconnected nodes is a widely addressed problem that can model a diversity of applications. Unfortunately, detecting the optimal partition of a network is a computationally demanding task, usually conducted by means of optimization methods. Among them, randomized search heuristics have been proven to be efficient approaches. This manuscript is devoted to providing an overview of community detection problems from the perspective of bio-inspired computation. To this end, we first review the recent history of this research area, placing emphasis on milestone studies contributed in the last five years. Next, we present an extensive experimental study to assess the performance of a selection of modern heuristics over weighted directed network instances. Specifically, we combine seven global search heuristics based on two different similarity metrics and eight heterogeneous search operators designed ad-hoc. We compare our methods with six different community detection techniques over a benchmark of 17 Lancichinetti–Fortunato–Radicchi network instances. Ranking statistics of the tested algorithms reveal that the proposed methods perform competitively, but the high variability of the rankings leads to the main conclusion: no clear winner can be declared. This finding aligns with community detection tools available in the literature that hinge on a sequential application of different algorithms in search for the best performing counterpart. We end our research by sharing our envisioned status of this area, for which we identify challenges and opportunities which should stimulate research efforts in years to come.},
  archive      = {J_ASOC},
  author       = {Eneko Osaba and Javier Del Ser and David Camacho and Miren Nekane Bilbao and Xin-She Yang},
  doi          = {10.1016/j.asoc.2019.106010},
  journal      = {Applied Soft Computing},
  pages        = {106010},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Community detection in networks using bio-inspired optimization: Latest developments, new results and perspectives with a selection of recent meta-heuristics},
  volume       = {87},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Applied improved RBF neural network model for predicting the
broiler output energies. <em>ASOC</em>, <em>87</em>, 106006. (<a
href="https://doi.org/10.1016/j.asoc.2019.106006">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The aim of this study is to optimize the Radial Base Function (RBF) parameters by combining the Response Surface Method (RSM) and Genetic Algorithm (GA) for modeling the output energies of broiler farms. For this purpose, data were collected from 210 broiler farms in Mazandaran province, Iran. The results showed that L 2 L2 and σ σ (RBF parameters) have a significant effect on RBF performance ( p p -value L2 and σ σ . The R 2 -adj and R 2 indexes for RSM coefficients at the test stage by using Trainbr algorithm for broiler meat and manure outputs were 97.13, 96.76 and 98.70, 98.53 and by using Trainlm algorithm were, 87.95, 86.41 and 96.90, 96.50, respectively. The mean and standard deviation of RMSE and MAPE in both two training algorithms (Trainbr and Trainlm) for manure and broiler meat outputs were used to compare them based on 100 random data sets of k-fold cross validation method. The results showed that Trainbr has the lowest error and can use for high accuracy modeling. The results of sensitivity analysis showed that one-day chicks ( x c xc ), human labor ( x l xl ), food ( x f xf ), electricity ( x e xe ), diesel fuel ( x d xd ) and machinery ( x m xm ) have the highest effect on chicken energy modeling and x f xf , x l xl , x m xm , x c xc , x d xd and x e xe have the most impact on manure energy modeling.},
  archive      = {J_ASOC},
  author       = {Sherwin Amini and Morteza Taki and Abbas Rohani},
  doi          = {10.1016/j.asoc.2019.106006},
  journal      = {Applied Soft Computing},
  pages        = {106006},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Applied improved RBF neural network model for predicting the broiler output energies},
  volume       = {87},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Sustainable supplier selection for smart supply chain
considering internal and external uncertainty: An integrated rough-fuzzy
approach. <em>ASOC</em>, <em>87</em>, 106004. (<a
href="https://doi.org/10.1016/j.asoc.2019.106004">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study proposes a novel framework to identify smart-sustainable SCMP (supply chain management practices) as supplier selection criteria for a smart supply chain. Supplier selection consists of two parts: criteria weights determination and suppliers ranking. DEMATEL (Decision Making Trial and Evaluation Laboratory) has been acknowledged as a relatively feasible method for determining the criteria weights due to its effectiveness in acquiring the interrelationships between criteria. TOPSIS (Technique for Order of Preference by Similarity to Ideal Solution) has been identified as the most frequently used method for supplier ranking due to its superiority in quickly finding the best alternatives. However, most existing research contains scant study of the simultaneous manipulation of internal uncertainty (individual linguistic vagueness) and external uncertainty (group preference diversity), which are involved in the supplier selection process. Therefore, this study proposes a hybrid rough-fuzzy DEMATEL-TOPSIS approach to sustainable supplier selection for a smart supply chain. The proposed method combines the strength of the fuzzy set in handling internal uncertainty and the advantages of the rough set in manipulating external uncertainty. The effectiveness and accuracy of the proposed methodology are illustrated through its application in sustainable vehicle transmission supplier selection and through comparisons with other methods.},
  archive      = {J_ASOC},
  author       = {Zhihua Chen and Xinguo Ming and Tongtong Zhou and Yuan Chang},
  doi          = {10.1016/j.asoc.2019.106004},
  journal      = {Applied Soft Computing},
  pages        = {106004},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Sustainable supplier selection for smart supply chain considering internal and external uncertainty: An integrated rough-fuzzy approach},
  volume       = {87},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). An enhanced multi-objective grey wolf optimizer for service
composition in cloud manufacturing. <em>ASOC</em>, <em>87</em>, 106003.
(<a href="https://doi.org/10.1016/j.asoc.2019.106003">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents an enhanced multi-objective grey wolf optimizer (EMOGWO) for multi-objective service composition and optimal selection (MO-SCOS) problem in cloud manufacturing, wherein both the quality of service and the energy consumption are considered from the perspectives of sustainable manufacturing. Given that there are still deficiencies in local optimum and diversity for the original multi-objective grey wolf optimizer (MOGWO). in response, the backward learning strategy is employed to heighten the exploration of initial population, so as to increase the search efficiency. Then, in order to improve the diversity, a nonlinear adjustment strategy for control parameter is proposed to enhance the global exploration of the algorithm. Besides, an enhanced search strategy, which strengthens the exploration of leaders, is designed to avoid the local optimum. Finally, the comparative study with typical multi-objective algorithms for MO-SCOS problems and benchmark problems are carried out to verify the proposed approach. simulation results suggest that the improvements for MOGWO are effective, and the proposed EMOGWO obtains better performance over other compared algorithms.},
  archive      = {J_ASOC},
  author       = {Yefeng Yang and Bo Yang and Shilong Wang and Tianguo Jin and Shi Li},
  doi          = {10.1016/j.asoc.2019.106003},
  journal      = {Applied Soft Computing},
  pages        = {106003},
  shortjournal = {Appl. Soft. Comput.},
  title        = {An enhanced multi-objective grey wolf optimizer for service composition in cloud manufacturing},
  volume       = {87},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Link-based multi-verse optimizer for text documents
clustering. <em>ASOC</em>, <em>87</em>, 106002. (<a
href="https://doi.org/10.1016/j.asoc.2019.106002">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Text document clustering (TDC) represents a key task in text mining and unsupervised machine learning , which partitions a specific documents’ collection into varied K-groups according to certain similarity/dissimilarity criterion. There exists a considerable amount of knowledge in the text clustering field and many attempts were carried out to resolve the TDC problem and improve the learning performance. The multi-verse optimizer algorithm (MVO) is a stochastic population-based algorithm, which was recently introduced and successfully utilized to tackle many optimization problems that are complex. The original MVO performance is limited to the utilization of only the best solution in the exploitation phase (local search capability), which makes it suffer from entrapment in local optima and low convergence rate. This paper aims to propose a novel method of modifying the MVO algorithm called link-based Multi-verse optimizer algorithm (LBMVO) to enhance the exploitation phase in the original MVO . The enhancement involves adding a neighbor operator to the MVO algorithm to enhance the search capability via a novel probability factor , namely neighborhood selection strategy (NSS). The proposed LBMVO’s effectiveness was tested on six standard datasets, which are used in the text clustering domain in addition to five standard datasets, which are utilized in the data clustering domain. The experiments revealed that the modified MVO with NSS has boosted the results in terms of error rate, accuracy, recall, precision, F-measure, purity, entropy criteria, and high convergence rate. Generally, LBMVO has outperformed or at least showed that it is profoundly competitive compared with the original MVO algorithm and with widely known clustering techniques like Spectral, Agglomerative, Density-based spatial clustering of applications with noise (DBSCAN), K-means, K-means++ clustering techniques and the optimization algorithms like harmony search (HS), genetic algorithm (GA), particle swarm optimization (PSO), krill herd algorithm (KHA), covariance matrix adaptation evolution strategy (CMAES), coyote optimization algorithm (COA), as well as original MVO.},
  archive      = {J_ASOC},
  author       = {Ammar Kamal Abasi and Ahamad Tajudin Khader and Mohammed Azmi Al-Betar and Syibrah Naim and Sharif Naser Makhadmeh and Zaid Abdi Alkareem Alyasseri},
  doi          = {10.1016/j.asoc.2019.106002},
  journal      = {Applied Soft Computing},
  pages        = {106002},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Link-based multi-verse optimizer for text documents clustering},
  volume       = {87},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). The design of a new hybrid controller for fractional-order
uncertain chaotic systems with unknown time-varying delays.
<em>ASOC</em>, <em>87</em>, 106000. (<a
href="https://doi.org/10.1016/j.asoc.2019.106000">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, the combination of a soft computing technique , i.e., adaptive neuro-fuzzy inference system (ANFIS) with fractional-order robust adaptive control is used to control a class of fractional order uncertain chaotic nonlinear systems with uncertainty, external disturbances and unknown time-varying delays. At first, the fractional-order hyperbolic tangential robust adaptive intelligent controller (FHRAIC) is designed for the system, when the unknown time-varying heterogeneous delays and uncertainties and disturbances exist in system states. Next, the controller design is extended for the case of existence of unknown time-varying delay in the inputs of the system. The hyperbolic tangential sliding surface enables the closed-loop system to safely and effectively avoid large errors in tracking control . Adaptive control parameters are adjusted based on Lyapunov stability analysis ANFIS is used to approximate unknown functions. The stability analysis of this controller has been carried out based on Lyapunov–Krasovskii method and Barbalat’s Lemma. Various simulation examples show the effectiveness of the proposed method for a vast range of systems. To demonstrate the effectiveness of ANFIS on FHRAIC controller, its performance has been compared with that of fractional-order hyperbolic tangential robust adaptive controller (FHRAC).},
  archive      = {J_ASOC},
  author       = {Mehdi Dalir and Nooshin Bigdeli},
  doi          = {10.1016/j.asoc.2019.106000},
  journal      = {Applied Soft Computing},
  pages        = {106000},
  shortjournal = {Appl. Soft. Comput.},
  title        = {The design of a new hybrid controller for fractional-order uncertain chaotic systems with unknown time-varying delays},
  volume       = {87},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Using pairwise precedences for solving the linear ordering
problem. <em>ASOC</em>, <em>87</em>, 105998. (<a
href="https://doi.org/10.1016/j.asoc.2019.105998">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {It is an old claim that, in order to design a (meta)heuristic algorithm for solving a given optimization problem , algorithm designers need first to gain a deep insight into the structure of the problem. Nevertheless, in recent years, we have seen an incredible rise of “new” meta-heuristic paradigms that have been applied to any type of optimization problem without even considering the features of these problems. In this work, we put this initial claim into practice and try to solve a classical permutation problem : the Linear Ordering Problem (LOP). To that end, first, we study the structure of the LOP by focusing on the relation between the pairwise precedences of items in the solution and its objective value. In a second step, we design a new meta-heuristic scheme, namely CD-RVNS, that incorporates critical information about the problem in its three key algorithmic components : a variable neighborhood search algorithm, a construction heuristic , and a destruction procedure. Conducted experiments, on the most challenging LOP instances available in the literature, reveal an outstanding performance when compared to existing algorithms. Moreover, we also demonstrate (experimentally) that the developed heuristic procedures perform individually better than their state-of-the-art counterparts.},
  archive      = {J_ASOC},
  author       = {Valentino Santucci and Josu Ceberio},
  doi          = {10.1016/j.asoc.2019.105998},
  journal      = {Applied Soft Computing},
  pages        = {105998},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Using pairwise precedences for solving the linear ordering problem},
  volume       = {87},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020b). A bi-projection model based on linguistic terms with
weakened hedges and its application in risk allocation. <em>ASOC</em>,
<em>87</em>, 105996. (<a
href="https://doi.org/10.1016/j.asoc.2019.105996">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Risk allocation is a key point to improve the efficiency of the public private partnership (PPP) project. During the process of risk allocation, qualitative information cannot be avoided. A bi-projection model is proposed to resolve the qualitative decision-making (QDM) problem in the process of risk allocation when qualitative information is represented by linguistic terms with weakened hedges (LTWHs). The semantics and syntax of LTWHs are discussed firstly, then the basic conceptions of the bi-projection model are defined afterwards. Secondly, we develop the process of bi-projection model based on LTWHs. Then, a case study of risk allocation is used to illustrate the availability and effectiveness of the proposed model. Moreover, the proposed method is compared with the TOPSIS and VIKOR methods. The result shows that the proposed method not only takes the weakened hedges as a component, but also reduces the problem of lacking information, along with increasing the objectivity of the decision-making process.},
  archive      = {J_ASOC},
  author       = {Lina Wang and Hai Wang and Zeshui Xu and Zhiliang Ren},
  doi          = {10.1016/j.asoc.2019.105996},
  journal      = {Applied Soft Computing},
  pages        = {105996},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A bi-projection model based on linguistic terms with weakened hedges and its application in risk allocation},
  volume       = {87},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A reversible privacy-preserving clustering technique based
on k-means algorithm. <em>ASOC</em>, <em>87</em>, 105995. (<a
href="https://doi.org/10.1016/j.asoc.2019.105995">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data mining techniques can efficiently extract valuable knowledge from the data. However, data publishing and mining may result in a potential threat: privacy invasion. Privacy-Preserving Data Mining (PPDM) aims at effectively protecting the privacy while retaining the knowledge contained in the original data, which has received much attention in recent years. PPDM techniques often use methods such as swap, modification, and deletion to protect the original data so that no correlation exists between the original data and the resultant protected data. As a result, it cannot recover the original data from the protected data. However, in certain applications, a reliance on the original data to perform precision analysis is necessary, and consequently the storage of the original data is of great importance. In this paper, we use the concept of the k -means algorithm and propose a Reversible Privacy-Preserving k-means Clustering ( kRPP ) algorithm for protecting the clustering knowledge of a dataset. The experimental results show that adding noises to a cluster with a minimum total distance in the k RPP algorithm, there is no significant effect on the movement of centroids , Precision, Recall, and F1 when the noise ratio or noise offset ratio increases.},
  archive      = {J_ASOC},
  author       = {Chen-Yi Lin},
  doi          = {10.1016/j.asoc.2019.105995},
  journal      = {Applied Soft Computing},
  pages        = {105995},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A reversible privacy-preserving clustering technique based on k-means algorithm},
  volume       = {87},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Short-term forecasting of renewable energy consumption:
Augmentation of a modified grey model with a kalman filter.
<em>ASOC</em>, <em>87</em>, 105994. (<a
href="https://doi.org/10.1016/j.asoc.2019.105994">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {One of the important global trends in near future is to replace fossil-fuel energy with sustainable energy. The accurate predictions of the renewable energy consumption are seemingly crucial in both national and international levels. In the context of a limited number of historical data, grey prediction system of single variable is one of primary choices for such prediction. Nonetheless, this seems rather sceptical when the dynamics of a system relies on solely one variable. This paper presents a novel approach based on a modification of multivariable grey prediction model whereby the influences of exogenous variables are taken into account. Furthermore, instead of employing the least square method for parameter estimation, states and parameters in our proposed method are sequentially estimated by means of the traditional Kalman filtering . The genetic algorithm is additionally supplemented in the Kalman filter step in order to justify some unknown noise statistics. To validate the effectiveness of the proposed scheme, it is employed to estimate and predict the renewable energy consumption in Thailand along with its associated factors using the data from 1990 to 2015. Compared with the multivariable grey model using the least square method for estimation of model parameters, the results show that the hybrid approach provides a better estimation and prediction performance.},
  archive      = {J_ASOC},
  author       = {Sompop Moonchai and Nawinda Chutsagulprom},
  doi          = {10.1016/j.asoc.2019.105994},
  journal      = {Applied Soft Computing},
  pages        = {105994},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Short-term forecasting of renewable energy consumption: Augmentation of a modified grey model with a kalman filter},
  volume       = {87},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Uncertain demand estimation with optimization of time and
cost using facebook disaster map in emergency relief operation.
<em>ASOC</em>, <em>87</em>, 105992. (<a
href="https://doi.org/10.1016/j.asoc.2019.105992">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {When a disaster disrupts a society within a moment without a little warning, the living people of those areas face the deprivation due to demolition. Facebook, one of the most popular social media plays a vital role in response to the victims. The authority of Facebook has launched safety check feature to know about the requirement after the disruption of disaster and based on the information, a intensity factor is defined for requirement of relief products in this research work. Estimating the amount of relief products through intensity measure, our research work has introduced a mathematical model for initiation of humanitarian logistic operation plan. The research has focused on two objective functions through the mathematical model which are minimization of total cost and response time. The model with two objective functions is converted to a equivalent compromise model with neutrosophic compromise programming approach. Deterministic and non-deterministic both algorithms are implemented in the solution process of the compromise mathematical model. In deterministic approach , mathematical model is varified with different methods to obtain compromise results. For non-deterministic approach, a genetic algorithm is proposed to solve the model. The model is experienced with a numerical example and hereby statistical investigation is performed considering different dimension varying the parameters of the model.},
  archive      = {J_ASOC},
  author       = {Deepshikha Sarma and Amrit Das and Uttam Kumar Bera},
  doi          = {10.1016/j.asoc.2019.105992},
  journal      = {Applied Soft Computing},
  pages        = {105992},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Uncertain demand estimation with optimization of time and cost using facebook disaster map in emergency relief operation},
  volume       = {87},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Hybrid multi-objective evolutionary algorithm based on
search manager framework for big data optimization problems.
<em>ASOC</em>, <em>87</em>, 105991. (<a
href="https://doi.org/10.1016/j.asoc.2019.105991">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Big Data optimization (Big-Opt) refers to optimization problems which require to manage the properties of big data analytics . In the present paper, the Search Manager (SM), a recently proposed framework for hybridizing metaheuristics to improve the performance of optimization algorithms , is extended for multi-objective problems (MOSM), and then five configurations of it by combination of different search strategies are proposed to solve the EEG signal analysis problem which is a member of the big data optimization problems class. Experimental results demonstrate that the proposed configurations of MOSM are efficient in this kind of problems. The configurations are also compared with NSGA-III with uniform crossover and adaptive mutation operators (NSGA-III UCAM), which is a recently proposed method for Big-Opt problems.},
  archive      = {J_ASOC},
  author       = {Yousef Abdi and Mohammad-Reza Feizi-Derakhshi},
  doi          = {10.1016/j.asoc.2019.105991},
  journal      = {Applied Soft Computing},
  pages        = {105991},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Hybrid multi-objective evolutionary algorithm based on search manager framework for big data optimization problems},
  volume       = {87},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Nonlinear black-box system identification through
coevolutionary algorithms and radial basis function artificial neural
networks. <em>ASOC</em>, <em>87</em>, 105990. (<a
href="https://doi.org/10.1016/j.asoc.2019.105990">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The present work deals with the application of coevolutionary algorithms and artificial neural networks to perform input selection and related parameter estimation for nonlinear black-box models in system identification. In order to decouple the resolution of the input selection and parameter estimation, we propose a problem decomposition formulation and solve it by a coevolutionary algorithm strategy. The novel methodology is successfully applied to identify a magnetorheological damper , a continuous polymerization reactor and a piezoelectric robotic micromanipulator. The results show that the method provides valid models in terms of accuracy and statistical properties. The main advantage of the method is the joint input and parameter estimation, towards automating a tedious and error prone procedure with global optimization algorithms .},
  archive      = {J_ASOC},
  author       = {Helon Vicente Hultmann Ayala and Didace Habineza and Micky Rakotondrabe and Leandro dos Santos Coelho},
  doi          = {10.1016/j.asoc.2019.105990},
  journal      = {Applied Soft Computing},
  pages        = {105990},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Nonlinear black-box system identification through coevolutionary algorithms and radial basis function artificial neural networks},
  volume       = {87},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Local feature selection based on artificial immune system
for classification. <em>ASOC</em>, <em>87</em>, 105989. (<a
href="https://doi.org/10.1016/j.asoc.2019.105989">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Conventional feature selection algorithms select a global feature subset for the entire sample space. In contrast, in this paper we propose an efficient filter local feature selection algorithm based on artificial immune system , which assigns a locally relevant feature subset for each neighboring region of the sample space. This algorithm introduces a clonal selection algorithm to explore the search space for the optimal feature subsets, and adopts local clustering idea as an evaluation criterion that maximizes the inter-class distance and minimizes the intra-class distance in the small region of each sample. Experimental results on a wide variety of synthetic and UCI datasets demonstrates that our proposed method achieves better performance than both state-of-the-art global feature selection algorithms and local feature selection algorithms. In addition, a main parameter analysis of the proposed method is carried out.},
  archive      = {J_ASOC},
  author       = {Yi Wang and Tao Li},
  doi          = {10.1016/j.asoc.2019.105989},
  journal      = {Applied Soft Computing},
  pages        = {105989},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Local feature selection based on artificial immune system for classification},
  volume       = {87},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Cooperative particle swarm optimization with
reference-point-based prediction strategy for dynamic multiobjective
optimization. <em>ASOC</em>, <em>87</em>, 105988. (<a
href="https://doi.org/10.1016/j.asoc.2019.105988">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dynamic multiobjective optimization problems (DMOPs) have received increasing attention in the evolutionary community in recent years. The problem environment of a DMOP dynamically changes over time, causing the movement of the Pareto front (PF). It is critical but challenging to find the new PF in a new environment by reusing historical information of past environments since the successive environments are often relevant. Thus, we propose a new cooperative particle swarm optimization with a reference-point-based prediction strategy to solve DMOPs. In the proposed method, multiple swarms cooperate to approximate the whole PF with a new learning strategy in dynamic environments. Specially, when the environment is changed, the outdated particles are relocated based on the PF subparts they belong to using the novel reference-point-based prediction strategy. The proposed algorithm has been evaluated on the very recent scalable dynamic problem test suite with different numbers of objectives and different change severity. Experimental results show that the proposed algorithm is competitive to other typical state-of-the-art dynamic multiobjective algorithms and can find well-diversified and well-converged solution sets in dynamic environments.},
  archive      = {J_ASOC},
  author       = {Xiao-Fang Liu and Yu-Ren Zhou and Xue Yu},
  doi          = {10.1016/j.asoc.2019.105988},
  journal      = {Applied Soft Computing},
  pages        = {105988},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Cooperative particle swarm optimization with reference-point-based prediction strategy for dynamic multiobjective optimization},
  volume       = {87},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). SPCM: Image quality assessment based on symmetry phase
congruency. <em>ASOC</em>, <em>87</em>, 105987. (<a
href="https://doi.org/10.1016/j.asoc.2019.105987">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Phase congruency (PC) algorithm is a frequency based algorithm. Instead of processing image spatially, the PC algorithm calculates the phase and amplitude of individual frequency components in the frequency domain. As one of the successful algorithm of image feature detection, PC has some advantages in the image quality assessment , however it has some inherent limitations. This paper studies the applications of symmetry phase in image quality assessment (IQA) and proposes a metric based on symmetry phase congruency (SPC). Symmetry phase congruency overcomes the limitations of phase congruency during the feature detection of image. This paper proposes a new IQA metric which is named as the symmetry phase congruency metric (SPCM). The sign responses of neighboring pixels are used to find the location of symmetry phases and then the symmetry phase congruency is used to detect image features and assess image quality. The experimental results show that SPCM is more sensitive to structural features of image and more robust to noises, and SPCM can achieve a higher consistency with the subjective evaluation of image quality.},
  archive      = {J_ASOC},
  author       = {Fan Zhang and Boyan Zhang and Ruoya Zhang and Xinhong Zhang},
  doi          = {10.1016/j.asoc.2019.105987},
  journal      = {Applied Soft Computing},
  pages        = {105987},
  shortjournal = {Appl. Soft. Comput.},
  title        = {SPCM: Image quality assessment based on symmetry phase congruency},
  volume       = {87},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Hand-crafted and deep convolutional neural network features
fusion and selection strategy: An application to intelligent human
action recognition. <em>ASOC</em>, <em>87</em>, 105986. (<a
href="https://doi.org/10.1016/j.asoc.2019.105986">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Human action recognition (HAR) has gained much attention in the last few years due to its enormous applications including human activity monitoring, robotics, visual surveillance, to name but a few. Most of the previously proposed HAR systems have focused on using hand-crafted images features . However, these features cover limited aspects of the problem and show performance degradation on a large and complex datasets. Therefore, in this work, we propose a novel HAR system which is based on the fusion of conventional hand-crafted features using histogram of oriented gradients (HoG) and deep features. Initially, human silhouette is extracted with the help of saliency-based method - implemented in two phases. In the first phase, motion and geometric features are extracted from the selected channel, whilst, second phase calculates the Chi-square distance between the extracted and threshold-based minimum distance features. Afterwards, extracted deep CNN and hand-crafted features are fused to generate a resultant vector. Moreover, to cope with the curse of dimensionality, an entropy-based feature selection technique is also proposed to identify the most discriminant features for classification using multi-class support vector machine (M-SVM). All the simulations are performed on five publicly available benchmark datasets including Weizmann, UCF11 (YouTube), UCF Sports, IXMAS, and UT-Interaction. A comparative evaluation is also presented to show that our proposed model achieves superior performances in comparison to a few exiting methods.},
  archive      = {J_ASOC},
  author       = {Muhammad Attique Khan and Muhammad Sharif and Tallha Akram and Mudassar Raza and Tanzila Saba and Amjad Rehman},
  doi          = {10.1016/j.asoc.2019.105986},
  journal      = {Applied Soft Computing},
  pages        = {105986},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Hand-crafted and deep convolutional neural network features fusion and selection strategy: An application to intelligent human action recognition},
  volume       = {87},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Likelihood-based hybrid ORESTE method for evaluating the
thermal comfort in underground mines. <em>ASOC</em>, <em>87</em>,
105983. (<a href="https://doi.org/10.1016/j.asoc.2019.105983">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A thermal environment has many adverse effects on the safety and health of workers. Especially in an underground mining context, thermal hazards become more serious as mining depth increases. This study aims to find a suitable method for evaluating the thermal comfort in underground mines. First, considering the ambiguity of human thinking, picture fuzzy numbers (PFNs) are adopted to indicate the subjective evaluation information. Then, the likelihood is defined to measure the priority degree of two PFNs. Subsequently, the Organísation, rangement et Synthèse de données relarionnelles (in French) (ORESTE) is extended with hybrid evaluation information to solve the non-compensation problems of the indexes. Finally, the likelihood-based hybrid decision making framework is successfully implemented in a case study that assesses the thermal comfort in a copper mine in China. The evaluation results are reasonable and consistent with the field conditions. Additionally, the strengths of this methodology are validated through comparison analyses. The results show that the proposed decision-making framework is reliable and stable for evaluating the thermal comfort in underground mines and can provide references for the prevention and management of thermal hazards.},
  archive      = {J_ASOC},
  author       = {Suizhi Luo and Weizhang Liang and Guoyan Zhao},
  doi          = {10.1016/j.asoc.2019.105983},
  journal      = {Applied Soft Computing},
  pages        = {105983},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Likelihood-based hybrid ORESTE method for evaluating the thermal comfort in underground mines},
  volume       = {87},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Artificial bee colony directive for continuous optimization.
<em>ASOC</em>, <em>87</em>, 105982. (<a
href="https://doi.org/10.1016/j.asoc.2019.105982">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The artificial bee colony (ABC) algorithm, a relatively new swarm intelligence optimization technique, has been shown to be a competitive alternative to other population-based algorithms. This paper fundamentally modifies the solution search equations of the ABC in a manner that sends bee agents in search of three types of search regions that improve convergence speeds and proposes an innovative artificial bee colony directive (ABCD) algorithm. Moreover, this paper validates the ABCD algorithm by showing better performance by improving two familiar ABC variants in experimental tests. In addition, 10 applicable search strategies that adopt the proposed three search-region types are presented. The proposed ABCD not only improves the original ABC and its subsequently improved versions but is also useful for setting the search regions of other swarm intelligence algorithms.},
  archive      = {J_ASOC},
  author       = {Hsing-Chih Tsai},
  doi          = {10.1016/j.asoc.2019.105982},
  journal      = {Applied Soft Computing},
  pages        = {105982},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Artificial bee colony directive for continuous optimization},
  volume       = {87},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). MOEA/d-based participant selection method for crowdsensing
with social awareness. <em>ASOC</em>, <em>87</em>, 105981. (<a
href="https://doi.org/10.1016/j.asoc.2019.105981">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the explosive popularity of mobile terminal devices, crowdsensing has become a novel paradigm for thoroughly sensing the environment. A crucial issue in crowdsensing system involves that the selection of appropriate participants from a number of mobile users to guarantee completing the sensing tasks. The traditional participant selection methods only consider the profit of the task publisher, resulting in the loss of potential users. Base on this, a novel multi-objective participant selection model is built, with the purpose of guaranteeing the interests of the mobile users by maximizing the total reward of participants and the profit of task publisher by maximizing the overall sensing quality. To solve this combinatorial optimization problem , an improved multi-objective evolutionary algorithm based on decomposition (MOEA/D) is proposed. It utilizes a balance factor to uniform various scales between two objectives. The optimal solution for each objective obtained by greedy algorithm is incorporated into initial population, with the purpose of avoiding falling into the local optima. In addition, a novel mutation operator is developed to enhance the convergence of solutions. The simulation results show that the improved MOEA/D has a significant better performance than the other algorithms for our problem, and the proposed multi-objective participant selection model more fits for the crowdsensing based on social awareness.},
  archive      = {J_ASOC},
  author       = {Jianjiao Ji and Yinan Guo and Dunwei Gong and Wanbao Tang},
  doi          = {10.1016/j.asoc.2019.105981},
  journal      = {Applied Soft Computing},
  pages        = {105981},
  shortjournal = {Appl. Soft. Comput.},
  title        = {MOEA/D-based participant selection method for crowdsensing with social awareness},
  volume       = {87},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). An efficient feature selection based bayesian and rough set
approach for intrusion detection. <em>ASOC</em>, <em>87</em>, 105980.
(<a href="https://doi.org/10.1016/j.asoc.2019.105980">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The exponential growth of network size leads to increase attacks and intrusions. Detection of these attacks from the network has turned into a noteworthy issue of security. An intrusion detection system is an important approach to achieves high detection rate. A high dimensional dataset increase complexities of detection systems. In this paper, we have designed a novel intelligent system that comprises the feature selection with a hybrid approach of the Rough set theory and the Bayes theorem . The proposed feature selection computed core features and ranked them based on estimated probability . In a decision system, an object may belong to a single or multiple decision, and a feature contains a set of objects that occurrences compute an estimated probability . The rough set theory is being applied to classify information into lower and upper approximations . Uncertain information is distinguished using rough set approximations and solved by the Bayes theorem . In this research work, it has also been highlighted the quantitative realism of recently generated dataset and compared to publicly available datasets. This approach reduces false alarm rate , computational complexity , training complexity and increases detection rate. Comparisons with relevant classifiers are also tabled that show proposed method performs better than existing classifiers.},
  archive      = {J_ASOC},
  author       = {Mahendra Prasad and Sachin Tripathi and Keshav Dahal},
  doi          = {10.1016/j.asoc.2019.105980},
  journal      = {Applied Soft Computing},
  pages        = {105980},
  shortjournal = {Appl. Soft. Comput.},
  title        = {An efficient feature selection based bayesian and rough set approach for intrusion detection},
  volume       = {87},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Data-driven two-stage distributionally robust optimization
with risk aversion. <em>ASOC</em>, <em>87</em>, 105978. (<a
href="https://doi.org/10.1016/j.asoc.2019.105978">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper studies a two-stage distributionally robust optimization problem with risk aversion. We define an ambiguity set containing the true distribution function with L 1 L1 distance. Taking a data-driven approach, we use a product kernel density to estimate the nominal distribution and provide the bounds of the estimation. For tractability, we transform the worst-case risk aversion problem into a linear programming problem . To handle the risk measure in the objective function, we propose a modified decomposition method . Numerical tests are utilized to validate the proposed method.},
  archive      = {J_ASOC},
  author       = {Ripeng Huang and Shaojian Qu and Zaiwu Gong and Mark Goh and Ying Ji},
  doi          = {10.1016/j.asoc.2019.105978},
  journal      = {Applied Soft Computing},
  pages        = {105978},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Data-driven two-stage distributionally robust optimization with risk aversion},
  volume       = {87},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). DSCTool: A web-service-based framework for statistical
comparison of stochastic optimization algorithms. <em>ASOC</em>,
<em>87</em>, 105977. (<a
href="https://doi.org/10.1016/j.asoc.2019.105977">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {DSCTool is a statistical tool for comparing performance of stochastic optimization algorithms on a single benchmark function (i.e. single-problem analysis) or a set of benchmark functions (i.e., multiple-problem analysis). DSCTool implements a recently proposed approach, called Deep Statistical Comparison (DSC), and its variants. DSC ranks optimization algorithms by comparing distributions of obtained solutions for a problem instead of using a simple descriptive statistic such as the mean or the median. The rankings obtained for an individual problem give the relations between the performance of the applied algorithms. To compare optimization algorithms in the multiple-problem scenario, an appropriate statistical test must be applied to the rankings obtained for a set of problems. The main advantage of DSCTool are its REST web services, which means all its functionalities can be accessed from any programming language . In this paper, we present the DSCTool in detail with examples for its usage.},
  archive      = {J_ASOC},
  author       = {Tome Eftimov and Gašper Petelin and Peter Korošec},
  doi          = {10.1016/j.asoc.2019.105977},
  journal      = {Applied Soft Computing},
  pages        = {105977},
  shortjournal = {Appl. Soft. Comput.},
  title        = {DSCTool: A web-service-based framework for statistical comparison of stochastic optimization algorithms},
  volume       = {87},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Data-driven symbolic ensemble models for wind speed
forecasting through evolutionary algorithms. <em>ASOC</em>, <em>87</em>,
105976. (<a href="https://doi.org/10.1016/j.asoc.2019.105976">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Non-linear data-driven symbolic models have been gaining traction in many fields due to their distinctive combination of modeling expressiveness and interpretability . Despite that, they are still rather unexplored for ensemble wind speed forecasting, leaving behind new promising avenues for advancing the development of more accurate models which impact the efficiency of energy production. In this work, we develop a methodology based on the evolutionary algorithm known as grammatical evolution, and apply it to build forecasting models of near-surface wind speed over five locations in northeastern Brazil . Taking advantage of the symbolic nature of the models built, we conducted an extensive series of post-analyses. Overall, our models reduced the forecasting errors by 7\%–56\% when compared with other techniques, including a real-world operational ensemble model used in Brazil .},
  archive      = {J_ASOC},
  author       = {Amanda S. Dufek and Douglas A. Augusto and Pedro L.S. Dias and Helio J.C. Barbosa},
  doi          = {10.1016/j.asoc.2019.105976},
  journal      = {Applied Soft Computing},
  pages        = {105976},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Data-driven symbolic ensemble models for wind speed forecasting through evolutionary algorithms},
  volume       = {87},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Designing a composite deep learning based differential
protection scheme of power transformers. <em>ASOC</em>, <em>87</em>,
105975. (<a href="https://doi.org/10.1016/j.asoc.2019.105975">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes a novel differential protection scheme based on deep neural networks (DNN). The goal is to propose a fast, reliable, and independent protection scheme in distinguishing inrush current from internal fault in power transformers , as the most challenging issue in power transformers protection. Shallow-based techniques require spectral analysis and handcraft feature extraction to be proper methods in this major. However, they require a significant computational cost. In order to address this issue, in this paper, a novel DNN-based approach is proposed based on combining convolutional neural network (CNN) and light-gated recurrent unit (LGRU), namely CLGNN. The results show a more accurate and more reliable performance than three different shallow and three state-of-the-art DNN based techniques. Adaptability and robustness of the proposed scheme are evaluated considering CT saturation, superconducting fault current limiter (SFCL), and series compensation impacts. The obtained results prove the effectiveness and validity of the proposed DNN-based protection scheme in this paper.},
  archive      = {J_ASOC},
  author       = {Shahabodin Afrasiabi and Mousa Afrasiabi and Benyamin Parang and Mohammad Mohammadi},
  doi          = {10.1016/j.asoc.2019.105975},
  journal      = {Applied Soft Computing},
  pages        = {105975},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Designing a composite deep learning based differential protection scheme of power transformers},
  volume       = {87},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Lion swarm optimization algorithm for comparative study with
application to optimal dispatch of cascade hydropower stations.
<em>ASOC</em>, <em>87</em>, 105974. (<a
href="https://doi.org/10.1016/j.asoc.2019.105974">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Lion swarm optimization (LSO) algorithm that based on the natural division of labor among lion king, lionesses and lion cubs in a pack of lions is recently introduced. To evaluate the exploration and the exploitation of the LSO algorithm comprehensively, an intensive study based on optimization problems is necessary. In this work, we firstly present the revised version of the LSO algorithm in detail. Secondly, the efficiency of LSO is evaluating using quantitative analysis , convergence analysis , statistical analysis, and robustness analysis on 60 classical numerical test problems, encompassing the Uni-modal, the Multi-modal, the Separable, the Non-separable, and the Multi-dimension problems. For comparison purposes, the results obtained by the LSO algorithm are compared against a large set of state-of-the-art optimization methods. The comparative results show that the LSO can provide significantly superior results for the US, the UN, and the MS problems regarding convergence speed, robustness, success rate, time complexity, and optimization accuracy compared with the other optimizers, and present very competitive results in terms of those indicators compared with the other optimizers. Finally, to check the applicability and robustness of the LSO algorithm, a case study on optimal dispatch problem of China’s Wujiang cascade hydropower stations shows that the LSO can obtain well and reliable optimal results with average generation of 122.421180 10 8 kW ⋅ ⋅ h, 103.463636 10 8 kW ⋅ ⋅ h, and 99.3826340 10 8 kW ⋅ ⋅ h for three different scenarios (i.e. the wet year, the normal year and the dry year), which are satisfying compared with that of the GA , the improved CS , and the PSO in terms of optimization accuracy. Besides, regarding the convergence speed, the results are also competitive. Therefore, we can conclude that the LSO is an efficient method for solving complex problems with correlative decision variables with simple structure and excellent convergence speed.},
  archive      = {J_ASOC},
  author       = {Junfeng Liu and Dingfang Li and Yun Wu and Dedi Liu},
  doi          = {10.1016/j.asoc.2019.105974},
  journal      = {Applied Soft Computing},
  pages        = {105974},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Lion swarm optimization algorithm for comparative study with application to optimal dispatch of cascade hydropower stations},
  volume       = {87},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Clustering-based method for large group decision making with
hesitant fuzzy linguistic information: Integrating correlation and
consensus. <em>ASOC</em>, <em>87</em>, 105973. (<a
href="https://doi.org/10.1016/j.asoc.2019.105973">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Aiming at the large-scale experts and the lower consensus in large group decision making, a novel clustering-based method integrating correlation and consensus of hesitant fuzzy linguistic information is proposed. Firstly, develop a new hesitant degree function for hesitant fuzzy linguistic element considering its scale. Secondly, put forward the correlation measure and consensus measure models combining the hesitant degree. And then present a clustering method integrating the correlation and consensus to divide the large-scale experts into several clusters. The clustering method simultaneously ensures the cohesion of clusters and the gradual increasing of the collective consensus level. After clustering, activate the selection process to update the weights of clusters combining the number of experts in clusters and the consensus level of clusters and use the score function considering the hesitant degree to rank the alternatives. Finally, a case and some comparisons are studied and analyzed to verify the rationality and effectiveness of the method.},
  archive      = {J_ASOC},
  author       = {Xiangyu Zhong and Xuanhua Xu},
  doi          = {10.1016/j.asoc.2019.105973},
  journal      = {Applied Soft Computing},
  pages        = {105973},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Clustering-based method for large group decision making with hesitant fuzzy linguistic information: Integrating correlation and consensus},
  volume       = {87},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A novel combined forecasting system for air pollutants
concentration based on fuzzy theory and optimization of aggregation
weight. <em>ASOC</em>, <em>87</em>, 105972. (<a
href="https://doi.org/10.1016/j.asoc.2019.105972">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Effective forecasting of the air pollutant concentration is crucial for a robust air quality early-warning system and has both theoretical and practical significance. However, the accidental and cognitive uncertainty in the model selection or parameter setting of a single system will result in inaccurate and unstable forecasting results. Thus, in this paper, a novel fuzzy combination forecasting system based on the data preprocessing , fuzzy theory, and advanced optimization algorithm is proposed to improve the accuracy and stability of forecasting results. Based on the fuzzy theory and decorrelation maximization method, our proposed forecasting system can considering more information and maintaining the diversity of models. Moreover, Cuckoo Search algorithm applied in the system can determine the optimal weights for models aggregation. Several experiments based on PM 2.5 2.5 and PM 10 datasets in three cities are analyzed and discussed to verify the excellent performance of our proposed forecasting system, and the results indicate that the forecasting system outperforms others with respect to the accuracy, stability and generalization capabilities which are the basis of a robust air quality early-warning system in practice.},
  archive      = {J_ASOC},
  author       = {Hufang Yang and Zhijie Zhu and Chen Li and Ranran Li},
  doi          = {10.1016/j.asoc.2019.105972},
  journal      = {Applied Soft Computing},
  pages        = {105972},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A novel combined forecasting system for air pollutants concentration based on fuzzy theory and optimization of aggregation weight},
  volume       = {87},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Hybrid strategy for selecting compact set of clustering
partitions. <em>ASOC</em>, <em>87</em>, 105971. (<a
href="https://doi.org/10.1016/j.asoc.2019.105971">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The selection of the most appropriate clustering algorithm is not a straightforward task, given that there is no clustering algorithm capable of determining the actual groups present in any dataset. A potential solution is to use different clustering algorithms to produce a set of partitions (solutions) and then select the best partition produced according to a specified validation measure; these measures are generally biased toward one or more clustering algorithms. Nevertheless, in several real cases, it is important to have more than one solution as the output. To address these problems, we present a hybrid partition selection algorithm, HSS , which accepts as input a set of base partitions potentially generated from clustering algorithms with different biases and aims, to return a reduced and yet diverse set of partitions (solutions). HSS comprises three steps: (i) the application of a multiobjective algorithm to a set of base partitions to generate a Pareto Front (PF) approximation; (ii) the division of the solutions from the PF approximation into a certain number of regions; and (iii) the selection of a solution per region by applying the Adjusted Rand Index. We compare the results of our algorithm with those of another selection strategy, ASA . Furthermore, we test HSS as a post-processing tool for two clustering algorithms based on multiobjective evolutionary computing: MOCK and MOCLE . The experiments revealed the effectiveness of HSS in selecting a reduced number of partitions while maintaining their quality.},
  archive      = {J_ASOC},
  author       = {Vanessa Antunes and Tiemi C. Sakata and Katti Faceli and Marcilio C.P. de Souto},
  doi          = {10.1016/j.asoc.2019.105971},
  journal      = {Applied Soft Computing},
  pages        = {105971},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Hybrid strategy for selecting compact set of clustering partitions},
  volume       = {87},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A hybrid optimized error correction system for time series
forecasting. <em>ASOC</em>, <em>87</em>, 105970. (<a
href="https://doi.org/10.1016/j.asoc.2019.105970">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Time series forecasting is a challenging task in machine learning . Real world time series are often composed by linear and nonlinear structures which need to be mapped by some forecasting method. Linear methods such as autoregressive integrated moving average (ARIMA) and nonlinear methods such as artificial neural networks (ANNs) could be employed to handle such problems, however model misspecification hinders the forecasting process producing inaccurate models. Hybrid models based on error forecasting and combination can reduce the misspecification of single models and improve the accuracy of the system. This work proposes a hybrid system that is composed of three parts: a) linear modeling of the time series, b) nonlinear modeling of the error series, and c) combination of the forecasts using three distinct approaches. The system performs a search for the best parameters of the linear and nonlinear components, and of the combination approaches. Particle swarm optimization is used to find suitable architecture and weights. Experiments show that the proposed technique achieved promising results in time series forecasting.},
  archive      = {J_ASOC},
  author       = {João Fausto Lorenzato de Oliveira and Luciano Demetrio Santos Pacífico and Paulo Salgado Gomes de Mattos Neto and Emanoel Francisco Spósito Barreiros and Cleyton Mário de Oliveira Rodrigues and Adauto Trigueiro de Almeida Filho},
  doi          = {10.1016/j.asoc.2019.105970},
  journal      = {Applied Soft Computing},
  pages        = {105970},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A hybrid optimized error correction system for time series forecasting},
  volume       = {87},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A spatio-temporal decomposition based deep neural network
for time series forecasting. <em>ASOC</em>, <em>87</em>, 105963. (<a
href="https://doi.org/10.1016/j.asoc.2019.105963">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Spatio-temporal problems arise in a broad range of applications, such as climate science and transportation systems. These problems are challenging because of unique spatial, short-term and long-term patterns, as well as the curse of dimensionality. In this paper, we propose a deep learning framework for spatio-temporal forecasting problems. We explicitly design the neural network architecture for capturing various types of spatial and temporal patterns, and the model is robust to missing data. In a preprocessing step , a time series decomposition method is applied to separately feed short-term, long-term and spatial patterns into different components of the neural network . A fuzzy clustering method finds clusters of neighboring time series residuals, as these contain short-term spatial patterns. The first component of the neural network consists of multi-kernel convolutional layers which are designed to extract short-term features from clusters of time series data . Each convolutional kernel receives a single cluster of input time series. The output of convolutional layers is concatenated by trends and followed by convolutional-LSTM layers to capture long-term spatial patterns. To have a robust forecasting model when faced with missing data, a pretrained denoising autoencoder reconstructs the model’s output in a fine-tuning step. In experimental results, we evaluate the performance of the proposed model for the traffic flow prediction . The results show that the proposed model outperforms baseline and state-of-the-art neural network models .},
  archive      = {J_ASOC},
  author       = {Reza Asadi and Amelia C. Regan},
  doi          = {10.1016/j.asoc.2019.105963},
  journal      = {Applied Soft Computing},
  pages        = {105963},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A spatio-temporal decomposition based deep neural network for time series forecasting},
  volume       = {87},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A novel method for constructing the optimal hierarchical
structure based on fuzzy granular space. <em>ASOC</em>, <em>87</em>,
105962. (<a href="https://doi.org/10.1016/j.asoc.2019.105962">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Granular computing serves as a general framework for complex problem solving in broad scopes and at various levels. The granularity was constructed via many ways, however, for complex systems there remain two challenges including determining a reasonable granularity and extracting the hierarchical information. In this paper, a new method is presented for constructing the optimal hierarchical structure based on fuzzy granular space. Firstly, the inter-class deviations and intra-class deviations were introduced, whose properties were investigated in depth and approved mathematically. Secondly, the fuzzy hierarchical evaluation index is developed, followed with a novel model for extracting the global optimal hierarchical structure established. An algorithm is then proposed, which reliably constructs the multi-level structure of complex system. Finally, to reduce the complexity, the granular signatures are extracted according to the nearest-to-center principle; with the use of the signatures, a classifier is designed for verifying our method. The validation of this method is approved by an application to the H1N1 influenza virus system. The theories and methodologies on granular computing presented here are helpful for capturing the structural information of complex system, especially for data mining and knowledge discovery.},
  archive      = {J_ASOC},
  author       = {Xu-Qing Tang and Yang Li and Wei-Wei Li and Wanqiang Shen},
  doi          = {10.1016/j.asoc.2019.105962},
  journal      = {Applied Soft Computing},
  pages        = {105962},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A novel method for constructing the optimal hierarchical structure based on fuzzy granular space},
  volume       = {87},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Representing complex intuitionistic fuzzy set by quaternion
numbers and applications to decision making. <em>ASOC</em>, <em>87</em>,
105961. (<a href="https://doi.org/10.1016/j.asoc.2019.105961">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Intuitionistic fuzzy sets are useful for modeling uncertain data of realistic problems. In this paper, we generalize and expand the utility of complex intuitionistic fuzzy sets using the space of quaternion numbers. The proposed representation can capture composite features and convey multi-dimensional fuzzy information via the functions of real membership, imaginary membership, real non-membership, and imaginary non-membership. We analyze the order relations and logic operations of the complex intuitionistic fuzzy set theory and introduce new operations based on quaternion numbers. We also present two quaternion distance measures in algebraic and polar forms and analyze their properties. We apply the quaternion representations and measures to decision-making models. The proposed model is experimentally validated in medical diagnosis, which is an emerging application for tackling patient’s symptoms and attributes of diseases.},
  archive      = {J_ASOC},
  author       = {Roan Thi Ngan and Le Hoang Son and Mumtaz Ali and Dan E. Tamir and Naphtali D. Rishe and Abraham Kandel},
  doi          = {10.1016/j.asoc.2019.105961},
  journal      = {Applied Soft Computing},
  pages        = {105961},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Representing complex intuitionistic fuzzy set by quaternion numbers and applications to decision making},
  volume       = {87},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Classification of power loads based on an improved denoising
deconvolutional auto-encoder. <em>ASOC</em>, <em>87</em>, 105959. (<a
href="https://doi.org/10.1016/j.asoc.2019.105959">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Machine learning has a wide range of applications in the recognition of power loads (PLs). In the light of the problems, such as poor generalization and the ease of falling into the local optima existing in the current PL classification algorithms , an improved algorithm based on the denoising deconvolutional auto-encoder was proposed to classify the field PL data. With the mirror symmetric structure of the network, the convolutional module can extract the distinctive features, while the deconvolutional module can reduce data redundancy and maintain high activation pixels. The data preprocessing accomplishes data dimensionality reduction. In order to accelerate convergence and improve classification accuracy , the unsupervised pre-training and ℓ 2 ℓ2 , regularization were used. The experimental results in the field data of a provincial power grid demonstrate that the proposed algorithm has a better generalization performance and a higher recognition rate than other algorithms, thus providing an efficient and objective way for PLs recognition.},
  archive      = {J_ASOC},
  author       = {Jianhua Wu and Jiahan Liu and Jian Ma and Kexu Chen and Chunhua Xu},
  doi          = {10.1016/j.asoc.2019.105959},
  journal      = {Applied Soft Computing},
  pages        = {105959},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Classification of power loads based on an improved denoising deconvolutional auto-encoder},
  volume       = {87},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Strong approximate markov blanket and its application on
filter-based feature selection. <em>ASOC</em>, <em>87</em>, 105957. (<a
href="https://doi.org/10.1016/j.asoc.2019.105957">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In feature selection problems, strong relevant features may be misjudged as redundant by the approximate Markov blanket . To avoid this, a new concept called strong approximate Markov blanket is proposed. It is theoretically proved that no strong relevant feature will be misjudged as redundant by the proposed concept. To reduce computation time, we propose the concept of modified strong approximate Markov blanket, which still performs better than the approximate Markov blanket in avoiding misjudgment of strong relevant features. A new filter-based feature selection method that is applicable to high-dimensional datasets is further developed. It first groups features to remove redundant features, and then uses a sequential forward selection method to remove irrelevant features. Numerical results on four benchmark and seven real datasets suggest that it is a competitive feature selection method with high classification accuracy , moderate number of selected features, and above-average robustness.},
  archive      = {J_ASOC},
  author       = {Zhongsheng Hua and Jian Zhou and Ye Hua and Wei Zhang},
  doi          = {10.1016/j.asoc.2019.105957},
  journal      = {Applied Soft Computing},
  pages        = {105957},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Strong approximate markov blanket and its application on filter-based feature selection},
  volume       = {87},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Sparse feature selection: Relevance, redundancy and locality
structure preserving guided by pairwise constraints. <em>ASOC</em>,
<em>87</em>, 105956. (<a
href="https://doi.org/10.1016/j.asoc.2019.105956">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Selection of features as a pre-processing stage is an essential issue in many machine learning tasks (such as classification) to reduce data dimensionality as there are many irrelevant and redundant features that can mislead the learning process. Graph-based sparse feature selection is developed to overcome this issue. In this paper, a novel graph-based sparse feature selection method is proposed that take into account both issues: relevancy and redundancy analysis . An empirical loss function joining with ℓ 1 ℓ1 -norm regularization term is proposed to overcome the relevancy issue and the redundancy issue is overcome by introducing a regularization term that prefers uncorrelated features. Furthermore, the proposed learning procedure is guided by two different sets of supervision information as pairs of must-linked (positive) and cannot-linked (negative) constraint sets to select a discriminative feature subset. These guiding information besides the whole data points are encoded in the graph Laplacian matrix that preserves the locality structure of the original data. The graph Laplacian matrix is constructed by two different approaches. Our first approach tries to preserve the structure of the original data guided just by the positive data points (unique samples in the must-linked constraints), and our second approach applies a normalized adapted affinity matrix to embed the pairwise must-linked and cannot-linked constraints as well as the neighborhood relationships information, all together. The experimental results on a number of several datasets from the University of California-Irvine machine learning repository, in addition to several high dimensional gene expression datasets show the efficacy of the proposed methods in the classification tasks compared to several powerful feature selection methods.},
  archive      = {J_ASOC},
  author       = {Zahir Noorie and Fatemeh Afsari},
  doi          = {10.1016/j.asoc.2019.105956},
  journal      = {Applied Soft Computing},
  pages        = {105956},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Sparse feature selection: Relevance, redundancy and locality structure preserving guided by pairwise constraints},
  volume       = {87},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Hybrid whale optimization algorithm enhanced with lévy
flight and differential evolution for job shop scheduling problems.
<em>ASOC</em>, <em>87</em>, 105954. (<a
href="https://doi.org/10.1016/j.asoc.2019.105954">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The job shop scheduling problem (JSSP) has been a hot issue in manufacturing. For the past few decades, scholars have been attracted to research JSSP and proposed many novel meta-heuristic algorithms to solve it. Whale optimization algorithm (WOA) is such a novel meta-heuristic algorithm and has been proven to be efficient in solving real-world optimization problems in the literature. This paper proposes a hybrid WOA enhanced with Lévy flight and differential evolution (WOA-LFDE) to solve JSSP. By changing the expression of Lévy flight and DE search strategy, Lévy flight enhances the abilities of global search and convergence of WOA in iteration, while DE algorithm improves the exploitation and local search capabilities of WOA and keeps the diversity of solutions to escape local optima. It is then applied to solve 88 JSSP benchmark instances and compared with other state-of-art algorithms. The experimental results and statistical analysis show that the proposed algorithm has superior performance over contesting algorithms.},
  archive      = {J_ASOC},
  author       = {Min Liu and Xifan Yao and Yongxiang Li},
  doi          = {10.1016/j.asoc.2019.105954},
  journal      = {Applied Soft Computing},
  pages        = {105954},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Hybrid whale optimization algorithm enhanced with lévy flight and differential evolution for job shop scheduling problems},
  volume       = {87},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A fuzzy full consistency method-dombi-bonferroni model for
prioritizing transportation demand management measures. <em>ASOC</em>,
<em>87</em>, 105952. (<a
href="https://doi.org/10.1016/j.asoc.2019.105952">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The selection and prioritization of appropriate Transportation Demand Management (TDM) measures is a common problem faced by transport planners and decision makers . The problem involves many uncertainties due to changing economic conditions, uncertainty in project success, changes in mobility and population characteristics etc. In this study, the multi-criteria decision making (MCDM) based fuzzy Full Consistency Method-Dombi-Bonferroni (fuzzy FUCOM-D’Bonferroni) model is proposed for a case study in Istanbul’s urban mobility system. Istanbul’s historical peninsula is considered to be a pilot area for the implementation of TDM projects by the local government. The proposed model is compared with other well-known four MCDM methods in order to show its validity and consistency. The results show that public transport capacity improvements is the best alternative among the other TDM measures.},
  archive      = {J_ASOC},
  author       = {Dragan Pamucar and Muhammet Deveci and Fatih Canıtez and Darko Bozanic},
  doi          = {10.1016/j.asoc.2019.105952},
  journal      = {Applied Soft Computing},
  pages        = {105952},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A fuzzy full consistency method-dombi-bonferroni model for prioritizing transportation demand management measures},
  volume       = {87},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A novel learning cloud bayesian network for risk
measurement. <em>ASOC</em>, <em>87</em>, 105947. (<a
href="https://doi.org/10.1016/j.asoc.2019.105947">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Bayesian network (BN) is a popularly used approach for risk analysis . Because it is a graphic model being able to deal with randomness yet unable to model ambiguity, the fuzzy set theory is often combined with it to create a so-called fuzzy BN. Instead of using the classical fuzzy set theory , this paper intends to combine a normal Cloud model with the BN. In the normal Cloud model, an element belonging to a certain qualitative concept is not certain and precise as well. The Cloud BN is a generalization of the fuzzy BN. It is more adaptive for the uncertainty description of linguistic concepts, for example, the risks. Using the normal Cloud model, the following numerical characteristics of the variables can be estimated: the expectation, the dispersion degree compared with the expectation, and the dispersion degree of entropy. Consequently, the risk assessment contains a richer set of analytical information. Cloud BNs attract growing research interests. Compared to its precedents, the Cloud BN in this paper has a learning capability. Since the risk factors may have a combined effect, the causal relationships among the variables can be very complex, and hidden variables may exist. The learning mechanism allows for automatic structure discovery from data, giving rise to a dynamically evolving network. The proposed learning Cloud BN is able to represent the real risk situation better than its precedents. Its effectiveness and applicability are demonstrated by an illustrative case for risk prediction of the face instability in an underground tunnel construction project.},
  archive      = {J_ASOC},
  author       = {Chen Chen and Limao Zhang and Robert Lee Kong Tiong},
  doi          = {10.1016/j.asoc.2019.105947},
  journal      = {Applied Soft Computing},
  pages        = {105947},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A novel learning cloud bayesian network for risk measurement},
  volume       = {87},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A new multi-stable fractional-order four-dimensional system
with self-excited and hidden chaotic attractors: Dynamic analysis and
adaptive synchronization using a novel fuzzy adaptive sliding mode
control method. <em>ASOC</em>, <em>87</em>, 105943. (<a
href="https://doi.org/10.1016/j.asoc.2019.105943">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Four-dimensional chaotic systems are a very interesting topic for researchers, given their special features. This paper presents a novel fractional-order four-dimensional chaotic system with self-excited and hidden attractors, which includes only one constant term. The proposed system presents the phenomenon of multi-stability, which means that two or more different dynamics are generated from different initial conditions. It is one of few published works in the last five years belonging to the aforementioned category. Using Lyapunov exponents , the chaotic behavior of the dynamical system is characterized, and the sensitivity of the system to initial conditions is determined. Also, systematic studies of the hidden chaotic behavior in the proposed system are performed using phase portraits and bifurcation transition diagrams . Moreover, a design technique of a new fuzzy adaptive sliding mode control (FASMC) for synchronization of the fractional-order systems has been offered. This control technique combines an adaptive regulation scheme and a fuzzy logic controller with conventional sliding mode control for the synchronization of fractional-order systems. Applying Lyapunov stability theorem , the proposed control technique ensures that the master and slave chaotic systems are synchronized in the presence of dynamic uncertainties and external disturbances . The proposed control technique not only provides high performance in the presence of the dynamic uncertainties and external disturbances, but also avoids the phenomenon of chattering. Simulation results have been presented to illustrate the effectiveness of the presented control scheme.},
  archive      = {J_ASOC},
  author       = {Hadi Jahanshahi and Amin Yousefpour and Jesus M. Munoz-Pacheco and Irene Moroz and Zhouchao Wei and Oscar Castillo},
  doi          = {10.1016/j.asoc.2019.105943},
  journal      = {Applied Soft Computing},
  pages        = {105943},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A new multi-stable fractional-order four-dimensional system with self-excited and hidden chaotic attractors: Dynamic analysis and adaptive synchronization using a novel fuzzy adaptive sliding mode control method},
  volume       = {87},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Fuzzy c-means clustering through SSIM and patch for image
segmentation. <em>ASOC</em>, <em>87</em>, 105928. (<a
href="https://doi.org/10.1016/j.asoc.2019.105928">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this study, we propose a new robust Fuzzy C-Means (FCM) algorithm for image segmentation called the patch-based fuzzy local similarity c-means (PFLSCM). First of all, the weighted sum distance of image patch is employed to determine the distance of the image pixel and the cluster center, where the comprehensive image features are considered instead of a simple level of brightness (gray value). Second, the structural similarity (SSIM) index takes into account similar degrees of luminance, contrast, and structure of image. The DSSIM (distance for structural similarity) metric is developed on a basis of SSIM in order to characterize the distance between two pixels in the whole image. Next a new similarity measure is proposed. Furthermore, a new fuzzy coefficient is proposed via the new similarity measure together with the weighted sum distance of image patch, and then the PFLSCM algorithm is put forward based on the idea of image patch and this coefficient. Through a collection of experimental studies using synthetic and publicly available images, we demonstrate that the proposed PFLSCM algorithm achieves improved segmentation performance in comparison with the results produced by some related FCM-based algorithms.},
  archive      = {J_ASOC},
  author       = {Yiming Tang and Fuji Ren and Witold Pedrycz},
  doi          = {10.1016/j.asoc.2019.105928},
  journal      = {Applied Soft Computing},
  pages        = {105928},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Fuzzy C-means clustering through SSIM and patch for image segmentation},
  volume       = {87},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A hierarchical gamma mixture model-based method for
estimating the number of clusters in complex data. <em>ASOC</em>,
<em>87</em>, 105891. (<a
href="https://doi.org/10.1016/j.asoc.2019.105891">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes a new method for estimating the true number of clusters and initial cluster centers in a dataset with many clusters. The observation points are assigned to the data space to observe the clusters through the distributions of the distances between the observation points and the objects in the dataset. A Gamma Mixture Model (GMM) is built from a distance distribution to partition the dataset into subsets, and a GMM tree is obtained by recursively partitioning the dataset. From the leaves of the GMM tree, a set of initial cluster centers are identified and the true number of clusters is estimated. This method is implemented in the new GMM-Tree algorithm. Two GMM forest algorithms are further proposed to ensemble multiple GMM trees to handle high dimensional data with many clusters. The GMM-P-Forest algorithm builds GMM trees in parallel, whereas the GMM-S-Forest algorithm uses a sequential process to build a GMM forest. Experiments were conducted on 32 synthetic datasets and 15 real datasets to evaluate the performance of the new algorithms. The results have shown that the proposed algorithms outperformed the existing popular methods: Silhouette, Elbow and Gap Statistic, and the recent method I-nice in estimating the true number of clusters from high dimensional complex data.},
  archive      = {J_ASOC},
  author       = {Muhammad Azhar and Joshua Zhexue Huang and Md Abdul Masud and Mark Junjie Li and Laizhong Cui},
  doi          = {10.1016/j.asoc.2019.105891},
  journal      = {Applied Soft Computing},
  pages        = {105891},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A hierarchical gamma mixture model-based method for estimating the number of clusters in complex data},
  volume       = {87},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Algorithm for improving additive consistency of linguistic
preference relations with an integer optimization model. <em>ASOC</em>,
<em>86</em>, 105955. (<a
href="https://doi.org/10.1016/j.asoc.2019.105955">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Linguistic preference relation (LPR) composed by linguistic terms can well express decision makers’ (DMs’) qualitative preference opinion by comparing alternatives with each other. The investigation of its consistency becomes an important issue to guarantee the rationality of the decision making solutions. Therefore, it is significant to investigate the consistency measure and the consistency improving approach for LPRs. In this paper we present a new method for group decision making (GDM) with LPRs. First, an additive consistency index is introduced on the basis of the information of the original LPR to check whether a LPR is acceptably additive consistency. For unacceptably additively consistent LPR, an integer optimization model is further developed to obtain the acceptably additively consistent LPR. Moreover, the optimization model can guarantee the integrity of the information of the LPR with acceptably additive consistency. Then, with respect to GDM with LPRs, an entropy weight method is proposed to determine the weights of DMs. Finally, the proposed methods are implemented in two numerical examples including a GDM problem. Meanwhile, the comparative analysis with existing methods are discussed in detail to demonstrate the validity of the proposed methods.},
  archive      = {J_ASOC},
  author       = {Peng Wu and Jinpei Liu and Ligang Zhou and Huayou Chen},
  doi          = {10.1016/j.asoc.2019.105955},
  journal      = {Applied Soft Computing},
  pages        = {105955},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Algorithm for improving additive consistency of linguistic preference relations with an integer optimization model},
  volume       = {86},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Advanced backtracking search optimization algorithm for a
new joint replenishment problem under trade credit with grouping
constraint. <em>ASOC</em>, <em>86</em>, 105953. (<a
href="https://doi.org/10.1016/j.asoc.2019.105953">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the real business situation, suppliers usually provide retailers with forward financing to decrease inventory or increase demand. Moreover, some heterogeneous goods are not allowed to transport together, or a penalty cost is incurred when heterogeneous goods are transported at the same time. This research proposes a practical multi-item joint replenishment problem (JRP) by considering trade credit and grouping constraint in accordance with the practical situation. The JRP aims to find reasonable item replenishment frequencies and each group’s basic replenishment cycle time so that the overall cost can be minimized. Four intelligent algorithms, which include an advanced backtracking search optimization algorithm (ABSA), genetic algorithm (GA), differential evolution (DE) and backtracking search optimization algorithm (BSA), are provided to solve this problem. Findings of contrastive example verify that ABSA is superior to GA, DE, and BSA, which have been validated to be effective algorithms. Randomly generated problems are used to test the performance of ABSA. Results indicate ABSA is more effective and stable to resolve the proposed JRP than the other algorithms. ABSA is a good solution for the proposed JRP with heterogeneous items under trade credits.},
  archive      = {J_ASOC},
  author       = {Lin Wang and Lu Peng and Sirui Wang and Shan Liu},
  doi          = {10.1016/j.asoc.2019.105953},
  journal      = {Applied Soft Computing},
  pages        = {105953},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Advanced backtracking search optimization algorithm for a new joint replenishment problem under trade credit with grouping constraint},
  volume       = {86},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Probabilistic tree-based representation for solving minimum
cost integer flow problems with nonlinear non-convex cost functions.
<em>ASOC</em>, <em>86</em>, 105951. (<a
href="https://doi.org/10.1016/j.asoc.2019.105951">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The minimum cost flow problem (MCFP) is the most generic variation of the network flow problem which aims to transfer a commodity throughout the network to satisfy demands. The problem size (in terms of the number of nodes and arcs) and the shape of the cost function are the most critical factors when considering MCFPs. Existing mathematical programming techniques often assume the cost functions to be linear or convex. Unfortunately, the linearity and convexity assumptions are too restrictive for modelling many real-world scenarios. In addition, many real-world MCFPs are large-scale, with networks having a large number of nodes and arcs. In this paper, we propose a probabilistic tree-based genetic algorithm (PTbGA) for solving large-scale minimum cost integer flow problems with nonlinear non-convex cost functions. We first compare this probabilistic tree-based representation scheme with the priority-based representation scheme, which is the most commonly-used representation for solving MCFPs. We then compare the performance of PTbGA with that of the priority-based genetic algorithm (PrGA), and two state-of-the-art mathematical solvers on a set of MCFP instances. Our experimental results demonstrate the superiority and efficiency of PTbGA in dealing with large-sized MCFPs, as compared to the PrGA method and the mathematical solvers.},
  archive      = {J_ASOC},
  author       = {Behrooz Ghasemishabankareh and Xiaodong Li and Melih Ozlen and Frank Neumann},
  doi          = {10.1016/j.asoc.2019.105951},
  journal      = {Applied Soft Computing},
  pages        = {105951},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Probabilistic tree-based representation for solving minimum cost integer flow problems with nonlinear non-convex cost functions},
  volume       = {86},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Fault diagnostics between different type of components: A
transfer learning approach. <em>ASOC</em>, <em>86</em>, 105950. (<a
href="https://doi.org/10.1016/j.asoc.2019.105950">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Transfer learning methods have been successfully applied into many fields for solving the problem of performance degradation in evolving working conditions or environments. This paper expands the range of transfer learning application by designing an integrated approach for fault diagnostics with different kinds of components. We use two deep learning methods, Convolutional Neural Network (CNN) and Multi-layer Perceptron (MLP), to train several base models with a mount of source data. Then the base models are transferred to target data with different level of variations, including the variations of working load and component type. Case Western Reserve University bearing dataset and 2009 PHM Data Challenge gearbox dataset are used to validate the performance of proposed approach. Experimental results show that proposed approach can improve the diagnostic accuracy not only between the working conditions from the same component but also different components.},
  archive      = {J_ASOC},
  author       = {Xudong Li and Yang Hu and Mingtao Li and Jianhua Zheng},
  doi          = {10.1016/j.asoc.2019.105950},
  journal      = {Applied Soft Computing},
  pages        = {105950},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Fault diagnostics between different type of components: A transfer learning approach},
  volume       = {86},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A gradient boosting decision tree based GPS signal reception
classification algorithm. <em>ASOC</em>, <em>86</em>, 105942. (<a
href="https://doi.org/10.1016/j.asoc.2019.105942">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In urban areas, GPS signals are often reflected or blocked by buildings, which causes multipath effects and non-line-of-sight (NLOS) reception respectively consequently degrading GPS positioning performance. While improved receiver design can reduce the effect of multipath to some extent, it cannot deal with NLOS. Modelling methods based on measurements have shown promise to reduce the effect of NLOS signal reception. However, this depends on their ability to accurately and reliably classify line-of-sight (LOS), multipath and NLOS signals. The traditional method is based on one feature using signal strength as measured by the carrier to noise ratio, C/N 0 . However, this feature is ineffective in capturing the characteristics of multipath and NLOS in all environments. In this paper, to improve the accuracy of signal reception classification, we are using the three features of C/N 0 , pseudorange residuals and satellite elevation angle with a gradient boosting decision tree (GBDT) based classification algorithm . Experiments are carried out to compare the proposed algorithm with classifiers based on decision tree , distance weighted k-nearest neighbour (KNN) and the adaptive network-based fuzzy inference system (ANFIS). Test results from static receivers in urban environments, show that the GBDT based algorithm achieves a classification accuracy of 100\%, 82\% and 86\% for LOS, multipath and NLOS signals, respectively. This is superior to the other three algorithms with the corresponding results of 100\%, 82\% and 84\% for the Distance-Weighted KNN, 99\%, 70\% and 65\% for the ANFIS and 98\%, 35\% and 95\% for the traditional decision tree. With the NLOS detection and exclusion, the proposed GBDT with multi-feature based method can provide a positioning accuracy improvement of 34.1\% compared to the traditional C/N 0 based method.},
  archive      = {J_ASOC},
  author       = {Rui Sun and Guanyu Wang and Wenyu Zhang and Li-Ta Hsu and Washington Y. Ochieng},
  doi          = {10.1016/j.asoc.2019.105942},
  journal      = {Applied Soft Computing},
  pages        = {105942},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A gradient boosting decision tree based GPS signal reception classification algorithm},
  volume       = {86},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020c). An improved random forest-based rule extraction method for
breast cancer diagnosis. <em>ASOC</em>, <em>86</em>, 105941. (<a
href="https://doi.org/10.1016/j.asoc.2019.105941">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Breast cancer has been becoming the main cause of death in women all around the world. An accurate and interpretable method is necessary for diagnosing patients with breast cancer for well-performed treatment. Nowadays, a great many of ensemble methods have been widely applied to breast cancer diagnosis, capable of achieving high accuracy, such as Random Forest . However, they are black-box methods which are unable to explain the reasons behind the diagnosis. To surmount this limitation, a rule extraction method named improved Random Forest (RF)-based rule extraction (IRFRE) method is developed to derive accurate and interpretable classification rules from a decision tree ensemble for breast cancer diagnosis. Firstly, numbers of decision tree models are constructed using Random Forest to generate abundant decision rules available. And then a rule extraction approach is devised to detach decision rules from the trained trees. Finally, an improved multi-objective evolutionary algorithm (MOEA) is employed to seek for an optimal rule predictor where the constituent rule set is the best trade-off between accuracy and interpretability . The developed method is evaluated on three breast cancer data sets, i.e., the Wisconsin Diagnostic Breast Cancer (WDBC) dataset, Wisconsin Original Breast Cancer (WOBC) dataset, and Surveillance, Epidemiology and End Results (SEER) breast cancer dataset. The experimental results demonstrate that the developed method can primely explain the black-box methods and outperform several popular single algorithms, ensemble learning methods, and rule extraction methods from the view of accuracy and interpretability . What is more, the proposed method can be popularized to other cancer diagnoses in practice, which provides an option to a more interpretable, more accurate cancer diagnosis process.},
  archive      = {J_ASOC},
  author       = {Sutong Wang and Yuyan Wang and Dujuan Wang and Yunqiang Yin and Yanzhang Wang and Yaochu Jin},
  doi          = {10.1016/j.asoc.2019.105941},
  journal      = {Applied Soft Computing},
  pages        = {105941},
  shortjournal = {Appl. Soft. Comput.},
  title        = {An improved random forest-based rule extraction method for breast cancer diagnosis},
  volume       = {86},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Metaheuristics for maximization of obstacles constrained
area coverage in heterogeneous wireless sensor networks. <em>ASOC</em>,
<em>86</em>, 105939. (<a
href="https://doi.org/10.1016/j.asoc.2019.105939">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Wireless Sensor Networks (WSNs) collect and transfer environmental data from a predefined field to a base station to be processed and analyzed. A major problem in designing WSNs is coverage maximization, in which a given number of sensor nodes must be deployed in a way that maximizes area coverage of a given network, without violating practical constraints. This is a known NP-hard problem and thus requires metaheuristic approaches for practical problem sizes. Two metaheuristics, namely Genetic Algorithm and Particle Swarm Optimization are proposed to tackle this problem. Our new contributions include a partial use of heuristic initialization, new fitness function, modified virtual force algorithm, addition of a uniform deceleration to the calculation of inertia weight and addition of the influence of sub-populations’ head individuals. The proposed algorithms are comprehensively experimented and compared with the current state-of-the-art for the equivalent problem without obstacles. Experimental results not only suggest which algorithms should be applied to which cases, but also provide insights into parameter settings, effects of heuristic initialization and effects of virtual force algorithm in each case. These conclusions are meaningful for our future research on obstacles constrained area coverage problems related to connectivity and lifetime of WSNs.},
  archive      = {J_ASOC},
  author       = {Huynh Thi Thanh Binh and Nguyen Thi Hanh and La Van Quan and Nguyen Duc Nghia and Nilanjan Dey},
  doi          = {10.1016/j.asoc.2019.105939},
  journal      = {Applied Soft Computing},
  pages        = {105939},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Metaheuristics for maximization of obstacles constrained area coverage in heterogeneous wireless sensor networks},
  volume       = {86},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Efficient optimization technique for multiple DG allocation
in distribution networks. <em>ASOC</em>, <em>86</em>, 105938. (<a
href="https://doi.org/10.1016/j.asoc.2019.105938">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the last few decades, interest in the integration of Distributed Generators (DGs) into distribution networks has been increased due to their benefits such as enhance power system reliability, reduce the power losses and improve the voltage profile. These benefits can be increased by determining the optimal DGs allocation (location and size) into distribution networks . This paper proposes an efficient optimization technique to optimally allocate the multiple DG units in distribution networks. This technique is based on Sine Cosine Algorithm (SCA) and chaos map theory. As any random search-based optimization algorithm , SCA faces some issues such as low convergence rate and trapping in local solutions during the exploration and exploitation phases. This issue can be addressed by developing Chaotic SCA (CSCA). CSCA is mainly based on the iterative chaotic map which used to update the random parameters of SCA instead of using the random probability distribution. The iterative chaotic map is applied for single and multi-objective SCA. The proposed technique is validated using two stranded IEEE radial distribution feeders; 33 and 69-nodes. Comprehensive comparison among the proposed technique, the original SCA, and other competitive optimization techniques are carried out to prove the effectiveness of CSCA. Finally, a complete study is performed to address the impact of the intermittent nature of renewable energy resource on the distribution system. Hence, typical loads and generation (represented in PV power) profiles are applied. The result proves that the CSCA is more efficient to solve the optimal multiple DGs allocation with minimum power loss and high convergence rate.},
  archive      = {J_ASOC},
  author       = {Ali Selim and Salah Kamel and Francisco Jurado},
  doi          = {10.1016/j.asoc.2019.105938},
  journal      = {Applied Soft Computing},
  pages        = {105938},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Efficient optimization technique for multiple DG allocation in distribution networks},
  volume       = {86},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Enhanced whale optimization algorithm for maximum power
point tracking of variable-speed wind generators. <em>ASOC</em>,
<em>86</em>, 105937. (<a
href="https://doi.org/10.1016/j.asoc.2019.105937">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes an enhancement of the meta-heuristic whale optimization algorithm (WOA) for maximum power point tracking (MPPT) of variable-speed wind generators. First of all, twenty-three benchmark functions tested the enhanced whale optimization algorithm (EWOA). Then the statistical results of EWOA compared with the results of other algorithms (WOA, salp swarm algorithm (SSA), enhanced SSA (ESSA), grey wolf optimizer (GWO), augmented GWO (AGWO), and particle swarm optimization (PSO). Also, the non-parametric statistical test and convergence curves proved the superiority and the speed of the EWOA. After that, the EWOA and WOA are implemented to design optimal Takagi–Sugeno fuzzy logic controllers (FLCs) to enhance the MPPT control of variable-speed wind generators. Moreover, real wind speed data has confirmed the robustness of optimal EWOA-MPPT. In conclusion, the simulation results revealed that the EWOA is a promising algorithm to be applied for solving different engineering problems.},
  archive      = {J_ASOC},
  author       = {Mohammed H. Qais and Hany M. Hasanien and Saad Alghuwainem},
  doi          = {10.1016/j.asoc.2019.105937},
  journal      = {Applied Soft Computing},
  pages        = {105937},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Enhanced whale optimization algorithm for maximum power point tracking of variable-speed wind generators},
  volume       = {86},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A bolasso based consistent feature selection enabled random
forest classification algorithm: An application to credit risk
assessment. <em>ASOC</em>, <em>86</em>, 105936. (<a
href="https://doi.org/10.1016/j.asoc.2019.105936">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Credit risk assessment has been a crucial issue as it forecasts whether an individual will default on loan or not. Classifying an applicant as good or bad debtor helps lender to make a wise decision. The modern data mining and machine learning techniques have been found to be very useful and accurate in credit risk predictive capability and correct decision making. Classification is one of the most widely used techniques in machine learning . To increase prediction accuracy of standalone classifiers while keeping overall cost to a minimum, feature selection techniques have been utilized, as feature selection removes redundant and irrelevant attributes from dataset. This paper initially introduces Bolasso (Bootstrap-Lasso) which selects consistent and relevant features from pool of features. The consistent feature selection is defined as robustness of selected features with respect to changes in dataset Bolasso generated shortlisted features are then applied to various classification algorithms like Random Forest (RF), Support Vector Machine (SVM), Naïve Bayes (NB) and K-Nearest Neighbors (K-NN) to test its predictive accuracy . It is observed that Bolasso enabled Random Forest algorithm (BS-RF) provides best results forcredit risk evaluation. The classifiers are built on training and test data partition (70:30) of three datasets (Lending Club’s peer to peer dataset, Kaggle’s Bank loan status dataset and German credit dataset obtained from UCI). The performance of Bolasso enabled various classification algorithms is then compared with that of other baseline feature selection methods like Chi Square, Gain Ratio, ReliefF and stand-alone classifiers (no feature selection method applied). The experimental results shows that Bolasso provides phenomenal stability of features when compared with stability of other algorithms. Jaccard Stability Measure (JSM) is used to assess stability of feature selection methods. Moreover BS-RF have good classification accuracy and is better than other methods in terms of AUC and Accuracy resulting in effectively improving the decision making process of lenders.},
  archive      = {J_ASOC},
  author       = {Nisha Arora and Pankaj Deep Kaur},
  doi          = {10.1016/j.asoc.2019.105936},
  journal      = {Applied Soft Computing},
  pages        = {105936},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A bolasso based consistent feature selection enabled random forest classification algorithm: An application to credit risk assessment},
  volume       = {86},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Hierarchical learning water cycle algorithm. <em>ASOC</em>,
<em>86</em>, 105935. (<a
href="https://doi.org/10.1016/j.asoc.2019.105935">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In order to improve the global searching ability of Water Cycle Algorithm (WCA), the hierarchical learning concept is introduced and the Hierarchical Learning WCA (HLWCA) is proposed in this paper. The underlying idea of HLWCA is to divide the solutions into collections and give these collections with hierarchy differences. One of the collections has a higher hierarchy than others and utilizes an exploration-inclined updating mechanism. The solutions in this high hierarchy collection are the exemplars of other collections. The other collections are sorted according to the exemplars’ function value and the solutions in these collections actively choose whether to follow their own exemplar or not. Through different updating mechanisms of collections, the global searching ability is improved while the fast convergence and strong local search ability of WCA are retained. The proposed HLWCA is firstly experimented on IEEE CEC 2017 benchmark suite to testify its performance on complex numerical optimization tasks. Then, it is tested on four practical design benchmark problems to verify its ability of solving real-world problems. The experimental results illustrate the efficiency of the proposed algorithm.},
  archive      = {J_ASOC},
  author       = {Caihua Chen and Peng Wang and Huachao Dong and Xinjing Wang},
  doi          = {10.1016/j.asoc.2019.105935},
  journal      = {Applied Soft Computing},
  pages        = {105935},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Hierarchical learning water cycle algorithm},
  volume       = {86},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Dual-branch residual network for lung nodule segmentation.
<em>ASOC</em>, <em>86</em>, 105934. (<a
href="https://doi.org/10.1016/j.asoc.2019.105934">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {An accurate segmentation of lung nodules in computed tomography (CT) images is critical to lung cancer analysis and diagnosis. However, due to the variety of lung nodules and the similarity of visual characteristics between nodules and their surroundings, a robust segmentation of nodules becomes a challenging problem. In this study, we propose the Dual-branch Residual Network (DB-ResNet) which is a data-driven model. Our approach integrates two new schemes to improve the generalization capability of the model: (1) the proposed model can simultaneously capture multi-view and multi-scale features of different nodules in CT images; (2) we combine the features of the intensity and the convolutional neural networks (CNN). We propose a pooling method, called the central intensity-pooling layer (CIP), to extract the intensity features of the center voxel of the block, and then use the CNN to obtain the convolutional features of the center voxel of the block. In addition, we designed a weighted sampling strategy based on the boundary of nodules for the selection of those voxels using the weighting score, to increase the accuracy of the model. The proposed method has been extensively evaluated on the LIDC-IDRI dataset containing 986 nodules. Experimental results show that the DB-ResNet achieves superior segmentation performance with the dice similarity coefficient (DSC) of 82.74\% on the dataset. Moreover, we compared our results with those of four radiologists on the same dataset. The comparison showed that our DSC was 0.49\% higher than that of human experts. This proves that our proposed method is as good as the experienced radiologist.},
  archive      = {J_ASOC},
  author       = {Haichao Cao and Hong Liu and Enmin Song and Chih-Cheng Hung and Guangzhi Ma and Xiangyang Xu and Renchao Jin and Jianguo Lu},
  doi          = {10.1016/j.asoc.2019.105934},
  journal      = {Applied Soft Computing},
  pages        = {105934},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Dual-branch residual network for lung nodule segmentation},
  volume       = {86},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Attention embedded residual CNN for disease detection in
tomato leaves. <em>ASOC</em>, <em>86</em>, 105933. (<a
href="https://doi.org/10.1016/j.asoc.2019.105933">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Automation in plant disease detection and diagnosis is one of the challenging research areas that has gained significant attention in the agricultural sector. Traditional disease detection methods rely on extracting handcrafted features from the acquired images to identify the type of infection. Also, the performance of these works solely depends on the nature of the handcrafted features selected. This can be addressed by learning the features automatically with the help of Convolutional Neural Networks (CNN). This research presents two different deep architectures for detecting the type of infection in tomato leaves. The first architecture applies residual learning to learn significant features for classification. The second architecture applies attention mechanism on top of the residual deep network . Experiments were conducted using Plant Village Dataset comprising of three diseases namely early blight, late blight, and leaf mold. The proposed work exploited the features learned by the CNN at various processing hierarchy using the attention mechanism and achieved an overall accuracy of 98\% on the validation sets in the 5-fold cross-validation.},
  archive      = {J_ASOC},
  author       = {Karthik R. and Hariharan M. and Sundar Anand and Priyanka Mathikshara and Annie Johnson and Menaka R.},
  doi          = {10.1016/j.asoc.2019.105933},
  journal      = {Applied Soft Computing},
  pages        = {105933},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Attention embedded residual CNN for disease detection in tomato leaves},
  volume       = {86},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Training data augmentation: An empirical study using
generative adversarial net-based approach with normalizing flow models
for materials informatics. <em>ASOC</em>, <em>86</em>, 105932. (<a
href="https://doi.org/10.1016/j.asoc.2019.105932">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We address the issue of small data size for training models for regression problems , which is a significant issue in materials science. Many density estimators that use generative models based on deep neural networks have been proposed. With generative models , normalizing flows can provide exact density estimations. Using normalizing flows, we address training data augmentation issue, where we use a real-valued non-volume preserving model (real-NVP) as the normalizing flow. A generative adversarial net (GAN)-based training method is applied to improve real-NVP training using real-NVP as the generator. Using kernel ridge regression trained by generated data, generalization performance was measured for evaluating the models. Experiments were conducted with seven benchmark datasets and a dataset of ionic conductivity of materials to compare the GAN-based real-NVP to state-of-the-art models, such as real-NVP and masked autoregressive flows. The experimental results demonstrated that the GAN-based real-NVP was comparable to state-of-the-art models and implied that the data sampled by the GAN-based real-NVP were available as new training data.},
  archive      = {J_ASOC},
  author       = {Hiroshi Ohno},
  doi          = {10.1016/j.asoc.2019.105932},
  journal      = {Applied Soft Computing},
  pages        = {105932},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Training data augmentation: An empirical study using generative adversarial net-based approach with normalizing flow models for materials informatics},
  volume       = {86},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). OCE-NGC: A neutrosophic graph cut algorithm using optimized
clustering estimation algorithm for dermoscopic skin lesion
segmentation. <em>ASOC</em>, <em>86</em>, 105931. (<a
href="https://doi.org/10.1016/j.asoc.2019.105931">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Automated skin lesion segmentation is one of the most crucial stages in dermoscopic images based diagnosis. To guarantee efficient unsupervised clustering-based segmentation, a histogram-based clustering estimation (HBCE) algorithm can be used to obtain the initial number of clusters with their corresponding centroids . Accordingly, the present work introduced a novel skin lesion segmentation algorithm, called optimized clustering estimation for neutrosophic graph cut algorithm (OCE-NGC). Firstly, the genetic algorithm (GA) is used to optimize the HBCE procedure by finding its optimal threshold values which are functions of a factor, called β β to be optimized. This optimization process guarantees the optimal determination of the initial number of clusters and their corresponding centroids for further use in the proposed clustering process . Thus, the skin lesion dermoscopic images are then mapped into the neutrosophic set (NS) domain which is computed by the neutrosophic c-means (NCM). The NCM groups the pixels in the dermoscopic images using the pre-determined optimal number of clusters obtained by the optimized HBCE. Finally, a cost function of the graph cut (GC) algorithm is defined in the NS domain for the segmentation process . The experimental results established the superiority of the proposed OCE-NGC approach in comparison with the traditional HBCE with NCM only, the traditional HBCE with the NGC, and the typical GC. In a public dataset, the proposed approach achieved 97.12\% and 86.28\% average accuracy and average Jaccard (JAC) values, respectively.},
  archive      = {J_ASOC},
  author       = {Ahmed Refaat Hawas and Yanhui Guo and Chunlai Du and Kemal Polat and Amira S. Ashour},
  doi          = {10.1016/j.asoc.2019.105931},
  journal      = {Applied Soft Computing},
  pages        = {105931},
  shortjournal = {Appl. Soft. Comput.},
  title        = {OCE-NGC: A neutrosophic graph cut algorithm using optimized clustering estimation algorithm for dermoscopic skin lesion segmentation},
  volume       = {86},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Estimating incomplete information in group decision making:
A framework of granular computing. <em>ASOC</em>, <em>86</em>, 105930.
(<a href="https://doi.org/10.1016/j.asoc.2019.105930">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A general assumption in group decision making scenarios is that of all individuals possess accurate knowledge of the entire problem under study, including the abilities to make a distinction of the degree up to which an alternative is better than other one. However, in many real world scenarios, this may be unrealistic, particularly those involving numerous individuals and options to choose from conflicting and dynamics information sources. To manage such a situation, estimation methods of incomplete information, which use own assessments provided by the individuals and consistency criteria to avoid discrepancy, have been widely employed under fuzzy preference relations . In this study, we introduce the information granularity concept to estimate missing values supporting the objective of obtaining complete fuzzy preference relations with higher consistency levels . We use the concept of granular preference relations to form each missing value as a granule of information in place of a crisp number. This offers the flexibility that is required to estimate the missing information so that the consistency levels related to the complete fuzzy preference relations are as higher as possible.},
  archive      = {J_ASOC},
  author       = {Francisco Javier Cabrerizo and Rami Al-Hmouz and Ali Morfeq and María Ángeles Martínez and Witold Pedrycz and Enrique Herrera-Viedma},
  doi          = {10.1016/j.asoc.2019.105930},
  journal      = {Applied Soft Computing},
  pages        = {105930},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Estimating incomplete information in group decision making: A framework of granular computing},
  volume       = {86},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Multi robot distance based formation using parallel genetic
algorithm. <em>ASOC</em>, <em>86</em>, 105929. (<a
href="https://doi.org/10.1016/j.asoc.2019.105929">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper an alternative method to achieve distance based formation is presented. The method uses Genetic Algorithms to find a suitable solution based on angle and distance, and an appropriate constant velocity to avoid collisions. The designed algorithm is extended to a parallel scheme to improve its performance and achieve Artificial Distributed Intelligence , in which the robots share, through solution migration, the best ways to converge to desired distances while avoiding collisions, finally reaching consensus on the solution. The algorithm is tested using simulations and real robots experiments .},
  archive      = {J_ASOC},
  author       = {A. López-González and J.A. Meda Campaña and E.G. Hernández Martínez and P. Paniagua Contro},
  doi          = {10.1016/j.asoc.2019.105929},
  journal      = {Applied Soft Computing},
  pages        = {105929},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Multi robot distance based formation using parallel genetic algorithm},
  volume       = {86},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Evolving granular feedback linearization: Design, analysis,
and applications. <em>ASOC</em>, <em>86</em>, 105927. (<a
href="https://doi.org/10.1016/j.asoc.2019.105927">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Exact feedback linearization is a method for nonlinear control which amounts to cancel the nonlinearities of a nonlinear system such that the resulting closed-loop dynamics is linear. The effectiveness of exact feedback linearization relies on a precise description of the system nonlinearities . This paper suggests a novel robust control approach for adaptive control of nonlinear systems called robust granular feedback linearization. The approach employs an instance of evolving the participatory learning algorithm to continuously estimate unknown nonlinearities and cancel their effects in the control loop. Under mild conditions, the robust granular feedback linearization is ensured to be Lyapunov stable by using convex methods. Simulation experiments with a surge tank is used to evaluate and to compare the performance of the robust granular feedback linearization against exact feedback linearization and an adaptive controller based on bacterial foraging. The results indicate that the robust granular feedback linearization outperforms both, the exact and the adaptive foraging controllers. The effectiveness of robust granular feedback linearization is further testified in an actual surge tank control system application.},
  archive      = {J_ASOC},
  author       = {Lucas Oliveira and Anderson Bento and Valter J.S. Leite and Fernando Gomide},
  doi          = {10.1016/j.asoc.2019.105927},
  journal      = {Applied Soft Computing},
  pages        = {105927},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Evolving granular feedback linearization: Design, analysis, and applications},
  volume       = {86},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). An enhanced protein secondary structure prediction using
deep learning framework on hybrid profile based features. <em>ASOC</em>,
<em>86</em>, 105926. (<a
href="https://doi.org/10.1016/j.asoc.2019.105926">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate protein secondary structure prediction (PSSP) is essential to identify structural classes, protein folds, and its tertiary structure. To identify the secondary structure, experimental methods exhibit higher precision with the trade-off of high cost and time. In this study, we propose an effective prediction model which consists of hybrid features of 42-dimensions with the combination of convolutional neural network (CNN) and bidirectional recurrent neural network (BRNN). The proposed model is accessed on four benchmark datasets such as CB6133, CB513, CASP10, and CAP11 using Q3, Q8, and segment overlap (Sov) metrics. The proposed model reported Q3 accuracy of 85.4\%, 85.4\%, 83.7\%, 81.5\%, and Q8 accuracy 75.8\%, 73.5\%, 72.2\%, and 70\% on CB6133, CB513, CASP10, and CAP11 datasets respectively. The results of the proposed model are improved by a minimum factor of 2.5\% and 2.1\% in Q3 and Q8 accuracy respectively, as compared to the popular existing models on CB513 dataset. Further, the quality of the Q3 results is validated by structural class prediction and compared with PSI-PRED. The experiment showed that the quality of the Q3 results of the proposed model is higher than that of PSI-PRED.},
  archive      = {J_ASOC},
  author       = {Prince Kumar and Sanjay Bankapur and Nagamma Patil},
  doi          = {10.1016/j.asoc.2019.105926},
  journal      = {Applied Soft Computing},
  pages        = {105926},
  shortjournal = {Appl. Soft. Comput.},
  title        = {An enhanced protein secondary structure prediction using deep learning framework on hybrid profile based features},
  volume       = {86},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). An improved whale optimization algorithm for forecasting
water resources demand. <em>ASOC</em>, <em>86</em>, 105925. (<a
href="https://doi.org/10.1016/j.asoc.2019.105925">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Water demand forecasting can promote the rational use of water resources and alleviate the pressure on water demand. By analyzing the use of water resources, this paper establishes three models of water demand forecasting, logarithmic model, linear and exponential combination model and linear, exponential and logarithmic hybrid models. In order to accurately estimate the demand for water resources, an improved whale optimization algorithm based on social learning and wavelet mutation strategy is proposed. The new algorithm designs a new linear incremental probability, which increases the possibility of global search of the algorithm. Based on the social learning principle, the social ranking and social influence are used to construct the social network for the individual, and the adaptive neighborhood learning strategy based on the network relationship is established to achieve the exchange and sharing of information between groups. The Morlet wavelet mutation mechanism is integrated to realize the dynamic adjustment of the mutation space, which enhances the ability of the algorithm to escape from local optimization . The latest CEC2017 benchmark functions confirms the superiority of the proposed algorithm. The water consumption from 2004 to 2016 in Shaanxi Province of China is used for the experiment. The results show that the performance of the proposed algorithm for solving the three water resources forecasting model is better in comparison to other algorithms. The prediction accuracy is as high as 99.68\%, which verified the validity of the model and the practicality of the proposed algorithm.},
  archive      = {J_ASOC},
  author       = {Wenyan Guo and Ting Liu and Fang Dai and Peng Xu},
  doi          = {10.1016/j.asoc.2019.105925},
  journal      = {Applied Soft Computing},
  pages        = {105925},
  shortjournal = {Appl. Soft. Comput.},
  title        = {An improved whale optimization algorithm for forecasting water resources demand},
  volume       = {86},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Missing multi-label learning with non-equilibrium based on
classification margin. <em>ASOC</em>, <em>86</em>, 105924. (<a
href="https://doi.org/10.1016/j.asoc.2019.105924">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-labels are more suitable for the ambiguity of the real world. However, missing labels are common in multi-label learning datasets; this results in unbalanced labeling and label diversity, which directly affect the performance of multi-label learning. Therefore, the classification and modeling of imbalanced data in missing multi-label learning are problems that need to be urgently solved Current methods mostly focus on combining sampling techniques with cost-sensitive learning and incorporating label correlation to improve the performance of the classifier, but generally they do not consider label loss caused by label cost. In fact, labeling unknown instances is often affected by the threshold of the discriminant function , especially for the label types near the threshold. Based on our previous research, we believe that information such as data distribution density and label density can be integrated into the label correlation, and that the classification margin can be expanded to effectively solve the labeling quality of labels near the threshold. Therefore, in this paper we propose a non-equilibrium multi-label learning algorithm based on the classification margin and aimed at completing the missing labels. First, the classification margin is proposed, and the label space is expanded by the label density. Then, the information entropy is used to measure the correlation between labels, and the label confidence matrix is constructed. The label confidence matrix is then unbalanced using the positive and negative label density, and the non-equilibrium label confidence matrix is used for label completion to obtain an informative label completion matrix. Finally, the kernel extreme learning machine and the label completion matrix are used for linear prediction. The experimental results show that the proposed algorithm has some advantages over other multi-label learning algorithms.},
  archive      = {J_ASOC},
  author       = {Yusheng Cheng and Kun Qian and Yibin Wang and Dawei Zhao},
  doi          = {10.1016/j.asoc.2019.105924},
  journal      = {Applied Soft Computing},
  pages        = {105924},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Missing multi-label learning with non-equilibrium based on classification margin},
  volume       = {86},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Stackelberg–nash equilibrium of pricing and inventory
decisions in duopoly supply chains using a nested evolutionary
algorithm. <em>ASOC</em>, <em>86</em>, 105922. (<a
href="https://doi.org/10.1016/j.asoc.2019.105922">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Pricing and inventory control in a competing environment, as separate entities, have attracted much attention from academics and practitioners. However, integrating these decisions in a competitive setting has not been significantly analyzed by academics, but is of great significance to practitioners. In this study, the joint decision on price and inventory control of a deterioration product is investigated in a duopoly setting. We consider two competing supply chains, each consisting of one manufacturer and one retailer. Each manufacturer, as the leader of their supply chain determines the wholesale price to maximize their profit, while the retailer as the follower should determine the retail price and inventory cycle to maximize his or her profit. Using a game theoretic approach , we formulate in-chain, and chain-to-chain competition as a bi-level programming problem, and analyze Stackelberg–Nash equilibrium of the problem. Furthermore, two versions of a nested algorithm are proposed to obtain the equilibrium. Both versions employ a modified threshold-accepting (TA) algorithm to solve the first level of the problem. However, while the first version utilizes the modified TA algorithm to deal with the second level of the problem, the second version applies a differential evolution (DE) approach. Eventually, a numerical study is carried out not only to compare two developed versions of the algorithm, but also to implement the sensitivity analysis of main parameters. Based on numerical experiments, although the accuracy of both versions of algorithm are alike, using TA is more computationally efficient than using DE. Furthermore, despite the permissibility of partial backlogging, it has never occurred in equilibrium points due to in-chain and chain-to-chain competition.},
  archive      = {J_ASOC},
  author       = {Anwar Mahmoodi},
  doi          = {10.1016/j.asoc.2019.105922},
  journal      = {Applied Soft Computing},
  pages        = {105922},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Stackelberg–Nash equilibrium of pricing and inventory decisions in duopoly supply chains using a nested evolutionary algorithm},
  volume       = {86},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Optimizing supply chain network for perishable products
using improved bacteria foraging algorithm. <em>ASOC</em>, <em>86</em>,
105921. (<a href="https://doi.org/10.1016/j.asoc.2019.105921">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In a supply chain environment , time delay has a significant impact on the success of perishable products. A major concern is therefore aimed at development of a holistic optimized approach in a supply chain environment for perishable products. Thus, integration of production, inventory and, distribution of perishable products in a supply chain environment are the challenging tasks for practitioners and researchers. In general, the standard optimal supply chain model cannot work for perishable products. There is therefore, a need for a holistic model that focuses on the consolidation of the processes. Shorter product shelf-life, temperature control, requirement of strict tractability, large number of product variants, and a large volume of goods handled are the major challenges in a supply chain environment for perishable products. The present work focuses on the development of a holistic model which uses improved bacteria forging algorithm (IBFA) for solving the formulated model. We have proposed and analyzed some general properties of the model and, finally applied it to a three-stage supply chain problem using an IBFA. Two case studies have been considered for support and demonstration of the integrated perishable supply chain network problem. Results obtained from IBFA reveal that the proposed model is more useful for decision makers while considering optimal supply chain network for perishable products. Finally, validation of results has been carried out using bacteria forging algorithm (BFA). The computational performance of the proposed algorithm proves that IBFA is instrumental in effectively handling the proposed approach.},
  archive      = {J_ASOC},
  author       = {Amit Kumar Sinha and Ankush Anand},
  doi          = {10.1016/j.asoc.2019.105921},
  journal      = {Applied Soft Computing},
  pages        = {105921},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Optimizing supply chain network for perishable products using improved bacteria foraging algorithm},
  volume       = {86},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A decentralized artificial immune system for solution
selection in cyber–physical systems. <em>ASOC</em>, <em>86</em>, 105920.
(<a href="https://doi.org/10.1016/j.asoc.2019.105920">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Centralization has become a de facto standard for implementing networked environments such as the Cyber–Physical Systems (CPS). Though easy to implement and control, centralized systems are difficult and expensive to scale in terms of the number of devices and the flow of information. This set of circumstances calls for a decentralized and distributed architecture for realizing such networked systems. However, due to the absence of global information in decentralized systems, one of the primary challenges is to find the best solution for problems distributed across the devices which are part of the CPS. Since the problems are distributed and no participating device has access to the full information, the devices may need to interact and share the information to select the best solution for a problem occurred. In this paper, we present a decentralized and distributed mechanism, which adapts to a stream of varying problems and continuously evolves and learns the best mappings between the problems and their associated solutions. The proposed approach integrates the concepts propounded in the three major Immune theories and can cater to real-world situations. The evolved mappings are shared across the physical network, thereby accelerating the search for the best set of solutions. In order to validate the performance of the proposed mechanism, we present the results obtained from solving a problem of sorting a stream of varying data in an emulated decentralized and distributed manner. To substantiate its working in real-world scenarios, we also describe the results obtained by embodying the system in real robots that discover the best path-following algorithms.},
  archive      = {J_ASOC},
  author       = {Tushar Semwal and Shivashankar B. Nair},
  doi          = {10.1016/j.asoc.2019.105920},
  journal      = {Applied Soft Computing},
  pages        = {105920},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A decentralized artificial immune system for solution selection in Cyber–Physical systems},
  volume       = {86},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A novel deep learning method based on attention mechanism
for bearing remaining useful life prediction. <em>ASOC</em>,
<em>86</em>, 105919. (<a
href="https://doi.org/10.1016/j.asoc.2019.105919">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Rolling bearing is a key component in rotation machine, whose remaining useful life (RUL) prediction is an essential issue of constructing condition-based maintenance (CBM) system. However, recent data-driven approaches for bearing RUL prediction still require prior knowledge to extract features, construct health indicate (HI) and set up threshold, which is inefficient in the big data era. In this paper, a pure data-driven method for bearing RUL prediction with little prior knowledge is proposed. This method includes three steps, i.e., features extraction, HI prediction and RUL calculation. In the first step, five band-pass energy values of frequency spectrum are extracted as features. Then, a recurrent neural network based on encoder–decoder framework with attention mechanism is proposed to predict HI values, which are designed closely related with the RUL values in this paper. Finally, the final RUL value can be obtained via linear regression. Experiments carried out on the dataset from PRONOSTIA and comparison with other novel approaches demonstrate that the proposed method achieves a better performance.},
  archive      = {J_ASOC},
  author       = {Yuanhang Chen and Gaoliang Peng and Zhiyu Zhu and Sijue Li},
  doi          = {10.1016/j.asoc.2019.105919},
  journal      = {Applied Soft Computing},
  pages        = {105919},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A novel deep learning method based on attention mechanism for bearing remaining useful life prediction},
  volume       = {86},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Human factors risk assessment: An integrated method for
improving safety in clinical use of medical devices. <em>ASOC</em>,
<em>86</em>, 105918. (<a
href="https://doi.org/10.1016/j.asoc.2019.105918">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Medical devices play a critical role in care and treatment. The human-related failures can significantly affect the safety of patients in clinical use of medical devices. This study develops a comprehensive risk assessment model for identification and evaluation of failures which may occur in the clinical use of medical devices. First, the “Swiss cheese” model and SHEL model (the acronym of software, hardware, environment, and liveware) are integrated to comprehensively identify the potential human errors . Then, a new failure mode and effects analysis (FMEA) approach improved by rough set theory and grey relational analysis is developed to assess the risk of the identified failures. The proposed method integrates the strengths of the “Swiss cheese” and SHEL model in identifying human failures from both the vertical and horizontal perspectives of the system, and the advantages of the improved FMEA approach in flexibly manipulating vague information in risk evaluation without much priori information . Finally, the proposed method is applied in clinical use of respirator to verify its efficiency and effectiveness.},
  archive      = {J_ASOC},
  author       = {Wenyan Song and Jing Li and Hao Li and Xinguo Ming},
  doi          = {10.1016/j.asoc.2019.105918},
  journal      = {Applied Soft Computing},
  pages        = {105918},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Human factors risk assessment: An integrated method for improving safety in clinical use of medical devices},
  volume       = {86},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). An adaptive constrained type-2 fuzzy hammerstein neural
network data fusion scheme for low-cost SINS/GNSS navigation system.
<em>ASOC</em>, <em>86</em>, 105917. (<a
href="https://doi.org/10.1016/j.asoc.2019.105917">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In low-cost micro-electro mechanical system (MEMS)-grade strap-down inertial navigation system (SINS), failure to compensate inertial sensors errors as well as un-modeled uncertainties in SINS could result in exponentially divergence in overall performance of low-cost SINSs. This study deals with the enhancement of low-cost SINS accuracy in combination of global navigation satellite system (GNSS). In this respect, a novel adaptive constrained integrated scheme for SINS/GNSS is developed based on type-2 fuzzy Hammerstein neural network (T2FHNN). To this aim, a gray-box Hammerstein neural network model are defined based on clear interpretation with the physical nature of the inertial sensors error. In addition a knowledge-based type-2 fuzzy programming extracted from inertial sensors data is also used for managing the learning rate of Hammerstein neural networks. Some vehicular real-world tests have been carried out in order to show the effectiveness and feasibility of the proposed integration scheme in the long-term performance and accuracy of the proposed navigation algorithm. The results indicate that the proposed integration algorithm improved the navigation accuracy, reliability and stability in the presence of state constraints of the stand-alone SINS during signal blockage of GNSS.},
  archive      = {J_ASOC},
  author       = {Saeed Khankalantary and Sadra Rafatnia and Hassan Mohammadkhani},
  doi          = {10.1016/j.asoc.2019.105917},
  journal      = {Applied Soft Computing},
  pages        = {105917},
  shortjournal = {Appl. Soft. Comput.},
  title        = {An adaptive constrained type-2 fuzzy hammerstein neural network data fusion scheme for low-cost SINS/GNSS navigation system},
  volume       = {86},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Classification of spatio-temporal trajectories from
volunteer geographic information through fuzzy rules. <em>ASOC</em>,
<em>86</em>, 105916. (<a
href="https://doi.org/10.1016/j.asoc.2019.105916">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Volunteer Geographic Information (VGI) is one of the key enablers of the mobility mining discipline. This work introduces a novel data-driven methodology to create a classifier of spatio-temporal trajectories based on VGI. Although other solutions have been proposed, they usually do not fully consider the low resolution and uncertainty of VGI due to its inherent human nature. The proposed approach introduces a classifier based on fuzzy rules that are able to deal with this kind of data. The solution is applied in a use case for real-time detection of tourists and local citizens’ flows and it is compared with a well-established trajectory classifier exhibiting quite promising results.},
  archive      = {J_ASOC},
  author       = {Jesús Cuenca-Jara and Fernando Terroso-Sáenz and Mercedes Valdés-Vela and Antonio F. Skarmeta},
  doi          = {10.1016/j.asoc.2019.105916},
  journal      = {Applied Soft Computing},
  pages        = {105916},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Classification of spatio-temporal trajectories from volunteer geographic information through fuzzy rules},
  volume       = {86},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A new approach to formulate fuzzy regression models.
<em>ASOC</em>, <em>86</em>, 105915. (<a
href="https://doi.org/10.1016/j.asoc.2019.105915">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A fuzzy regression model is developed to construct the relationship between the response and explanatory variables in fuzzy environments. To enhance explanatory power and take into account the uncertainty of the formulated model and parameters, a new operator, called the fuzzy product core ( FPC ), is proposed for the formulation processes to establish fuzzy regression models with fuzzy parameters using fuzzy observations that include fuzzy response and explanatory variables. In addition, the sign of parameters can be determined in the model-building processes. Compared to existing approaches, the proposed approach reduces the amount of unnecessary or unimportant information arising from fuzzy observations and determines the sign of parameters in the models to increase model performance. This improves the weakness of the relevant approaches in which the parameters in the models are fuzzy and must be predetermined in the formulation processes. The proposed approach outperforms existing models in terms of distance, mean similarity, and credibility measures, even when crisp explanatory variables are used.},
  archive      = {J_ASOC},
  author       = {Liang-Hsuan Chen and Sheng-Hsing Nien},
  doi          = {10.1016/j.asoc.2019.105915},
  journal      = {Applied Soft Computing},
  pages        = {105915},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A new approach to formulate fuzzy regression models},
  volume       = {86},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Weak-restriction bi-objective optimization algorithm for
scheduling with rejection on non-identical batch processing machines.
<em>ASOC</em>, <em>86</em>, 105914. (<a
href="https://doi.org/10.1016/j.asoc.2019.105914">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We investigate the problem of scheduling a set of jobs with arbitrary sizes and unequal weights on a set of parallel batch machines with non-identical capacities. The objective is to minimize the makespan of the accepted jobs and the total rejection penalty of the rejected jobs, simultaneously. To address the studied problem, a Pareto-based ant colony optimization algorithm with the first job selection probability (FPACO) is proposed. A weak-restriction selection strategy is proposed to obtain the desirability of candidate jobs. Two objective-oriented heuristic information and pheromone matrices are designed, respectively, to record the experience in different search dimensions. Moreover, a local optimization algorithm is incorporated to improve the solution quality. Finally, the proposed algorithm is compared with four existing algorithms through extensive simulation experiments. The experimental results indicate that the proposed algorithm outperforms all of the compared algorithms within a reasonable time.},
  archive      = {J_ASOC},
  author       = {Zhao-hong Jia and Ya-jie Li and Kai Li and Hua-ping Chen},
  doi          = {10.1016/j.asoc.2019.105914},
  journal      = {Applied Soft Computing},
  pages        = {105914},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Weak-restriction bi-objective optimization algorithm for scheduling with rejection on non-identical batch processing machines},
  volume       = {86},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Structural block driven enhanced convolutional neural
representation for relation extraction. <em>ASOC</em>, <em>86</em>,
105913. (<a href="https://doi.org/10.1016/j.asoc.2019.105913">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose a novel lightweight relation extraction approach of structural block driven convolutional neural learning. Specifically, we detect the essential sequential tokens associated with entities through dependency analysis, named as a structural block, and only encode the block on a block-wise and an inter-block-wise representation, utilizing multi-scale Convolutional Neural Networks (CNNs). This is to (1) eliminate the noisy from irrelevant part of a sentence; meanwhile (2) enhance the relevant block representation with both block-wise and inter-block-wise semantically enriched representation. Our method has the advantage of being independent of long sentence context since we only encode the sequential tokens within a block boundary. Experiments on two datasets i.e., SemEval2010 and KBP37, demonstrate the significant advantages of our method. In particular, we achieve the new state-of-the-art performance on the KBP37 dataset; and comparable performance with the state-of-the-art on the SemEval2010 dataset.},
  archive      = {J_ASOC},
  author       = {Dongsheng Wang and Prayag Tiwari and Sahil Garg and Hongyin Zhu and Peter Bruza},
  doi          = {10.1016/j.asoc.2019.105913},
  journal      = {Applied Soft Computing},
  pages        = {105913},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Structural block driven enhanced convolutional neural representation for relation extraction},
  volume       = {86},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Unsupervised feature learning for environmental sound
classification using weighted cycle-consistent generative adversarial
network. <em>ASOC</em>, <em>86</em>, 105912. (<a
href="https://doi.org/10.1016/j.asoc.2019.105912">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper we propose a novel environmental sound classification approach incorporating unsupervised feature learning via the spherical K K -Means++ algorithm and a new architecture for high-level data augmentation . The audio signal is transformed into a 2D representation using a discrete wavelet transform (DWT). The DWT spectrograms are then augmented by a novel architecture for cycle-consistent generative adversarial network . This high-level augmentation bootstraps generated spectrograms in both intra-and inter-class manners by translating structural features from sample to sample. A codebook is built by coding the DWT spectrograms with the speeded-up robust feature detector and the K K -Means++ algorithm. The Random forest is the final learning algorithm which learns the environmental sound classification task from the code vectors. Experimental results in four benchmarking environmental sound datasets (ESC-10, ESC-50, UrbanSound8k, and DCASE-2017) have shown that the proposed classification approach outperforms most of the state-of-the-art classifiers, including convolutional neural networks such as AlexNet and GoogLeNet, improving the classification rate between 3.51\% and 14.34\%, depending on the dataset.},
  archive      = {J_ASOC},
  author       = {Mohammad Esmaeilpour and Patrick Cardinal and Alessandro Lameiras Koerich},
  doi          = {10.1016/j.asoc.2019.105912},
  journal      = {Applied Soft Computing},
  pages        = {105912},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Unsupervised feature learning for environmental sound classification using weighted cycle-consistent generative adversarial network},
  volume       = {86},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). An angle based evolutionary algorithm with infeasibility
information for constrained many-objective optimization. <em>ASOC</em>,
<em>86</em>, 105911. (<a
href="https://doi.org/10.1016/j.asoc.2019.105911">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, angle-based approaches have shown promising for unconstrained many-objective optimization problems (MaOPs), but few of them are extended to solve constrained MaOPs (CMaOPs). Moreover, due to the difficulty in searching for feasible solutions in high-dimensional objective space, the use of infeasible solutions comes to be more important in solving CMaOPs. In this paper, an angle based evolutionary algorithm with infeasibility information is proposed for constrained many-objective optimization, where different kinds of infeasible solutions are utilized in environmental selection and mating selection. To be specific, an angle-based constrained dominance relation is proposed for non-dominated sorting, which gives infeasible solutions with good diversity the same priority to feasible solutions for escaping from the locally feasible regions. As for diversity maintenance, an angle-based density estimation is developed to give the infeasible solutions with good convergence a chance to survive for next generation, which is helpful to get across the large infeasible barrier. In addition, in order to utilize the potential of infeasible solutions in creating high-quality offspring, a modified mating selection is designed by considering the convergence, diversity and feasibility of solutions simultaneously. Experimental results on two constrained many-objective optimization test suites demonstrate the competitiveness of the proposed algorithm in comparison with five existing constrained many-objective evolutionary algorithms for CMaOPs. Moreover, the effectiveness of the proposed algorithm on a real-world problem is showcased.},
  archive      = {J_ASOC},
  author       = {Chao Wang and Ran Xu},
  doi          = {10.1016/j.asoc.2019.105911},
  journal      = {Applied Soft Computing},
  pages        = {105911},
  shortjournal = {Appl. Soft. Comput.},
  title        = {An angle based evolutionary algorithm with infeasibility information for constrained many-objective optimization},
  volume       = {86},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Unconstrained texture classification using efficient jet
texton learning. <em>ASOC</em>, <em>86</em>, 105910. (<a
href="https://doi.org/10.1016/j.asoc.2019.105910">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes a simple and effective texture recognition method that uses a new class of jet texton learning. In this approach, first a Jet space representation of the image is derived from a set of derivative of Gaussian (DtGs) filter responses upto 2nd order ( R 6 R6 ), so called local jet vector ( Ljv ), which satisfies the scale space properties, where the combinations of local jets preserve the intrinsic local structure of the image in a hierarchical way and are invariant to image translation, rotation and scaling. Next, the jet textons dictionary is learned using K-means clustering algorithm from DtGs responses, followed by a contrast Weber law normalization pre-processing step. Finally, the feature distribution of jet texton is considered as a model which is utilized to classify texture using a non-parametric nearest regularized subspace ( Nrs ) classifier. Extensive experiments on three large and well-known benchmark database for texture classification like KTH-TIPS, Brodatz and CUReT show that the proposed method achieves state-of-the-art performance, especially when the number of available training samples is limited. The source code of complete system is made publicly available at https://github.com/swalpa/JetTexton .},
  archive      = {J_ASOC},
  author       = {Swalpa Kumar Roy and Dipak Kumar Ghosh and Shiv Ram Dubey and Siddhartha Bhattacharyya and Bidyut B. Chaudhuri},
  doi          = {10.1016/j.asoc.2019.105910},
  journal      = {Applied Soft Computing},
  pages        = {105910},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Unconstrained texture classification using efficient jet texton learning},
  volume       = {86},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Search-based procedural content generation for GVG-LG.
<em>ASOC</em>, <em>86</em>, 105909. (<a
href="https://doi.org/10.1016/j.asoc.2019.105909">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Search-based Procedural Content Generation has been proven an efficient technique for the generation of different and diverse types of content. In this article, we generate general levels for 2D games using the FI2POP genetic algorithm . Generating entertaining levels is subjective. Therefore in this work, we focus on the aesthetics and difficulty of a level. For experimentation purposes, we generated levels for five different games in the General Video Game Level Generation track. Our results indicate that the generated levels are symmetrical, balanced, dense and reachable.},
  archive      = {J_ASOC},
  author       = {Adeel Zafar and Hasan Mujtaba and Mirza Omer Beg},
  doi          = {10.1016/j.asoc.2019.105909},
  journal      = {Applied Soft Computing},
  pages        = {105909},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Search-based procedural content generation for GVG-LG},
  volume       = {86},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Stockwell transform of time-series of fMRI data for
diagnoses of attention deficit hyperactive disorder. <em>ASOC</em>,
<em>86</em>, 105905. (<a
href="https://doi.org/10.1016/j.asoc.2019.105905">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Attention deficit hyperactivity disorder (ADHD) is a common brain disorder among children. It presents various symptoms, hence, utilizing the information obtained from functional magnetic resonance imaging (fMRI) time-series data can be useful. Finding functional connections in typically developed control (TDC) and ADHD patients can be helpful in classification. The aim of this paper is to present a multifold method for the study of fMRI data to diagnose ADHD patients. In the proposed method, first, by applying the Stockwell transform (ST), we obtain detailed information about the time-series of the region of interests (ROIs) in the time and frequency domains. ST provides information about the variations of each ROI during the time. Thereafter, time-frequency domains are partitioned into sub-matrices and then, their fuzzy entropies are calculated as features. Next, discriminative features are chosen by using the two-sample Kolmogorov–Smirnov (K–S) test. Finally, the data are classified by the leave-one-out cross-validation (LOOCV) method using the support vector machine (SVM) classifier. To see the effectiveness of the proposed method, the experiments are performed on the ADHD-200 database. We consider different scenarios including classification of TDCs and ADHDs as well as classification of ADHD subtypes. We also assess the performance by considering the age and sex as phenotypic information. The proposed method gives good results in the classification procedure and identifying the connection paths between ROIs. The results indicate that the proposed method can distinguish ADHD disorder in a more accurate manner in comparison with other methods. The connectivity paths show that there is a reduction in the input of cerebellar regions and the left mid orbitofrontal cortex in ADHDs compared to TDCs.},
  archive      = {J_ASOC},
  author       = {Shadi Sartipi and Hashem Kalbkhani and Peyman Ghasemzadeh and Mahrokh G. Shayesteh},
  doi          = {10.1016/j.asoc.2019.105905},
  journal      = {Applied Soft Computing},
  pages        = {105905},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Stockwell transform of time-series of fMRI data for diagnoses of attention deficit hyperactive disorder},
  volume       = {86},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Dynamic dispatching system using a deep denoising
autoencoder for semiconductor manufacturing. <em>ASOC</em>, <em>86</em>,
105904. (<a href="https://doi.org/10.1016/j.asoc.2019.105904">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep denoising autoencoders (DDAE), which are variants of the autoencoder, have shown outstanding performance in various machine learning tasks. In this study, we propose using a DDAE to address a dispatching rule selection problem that represents a major problem in semiconductor manufacturing . Recently, the significance of dispatching systems for storage allocation has become more apparent because operational issues lead to transfer inefficiency, resulting in production losses. Further, recent approaches have overlooked the possibility of a class imbalance problem in predicting the best dispatching rule . The main purpose of this study is to examine DDAE-based predictive control of the storage dispatching systems to reduce idle machines and production losses. We conducted an experimental evaluation to compare the predictive performance of DDAE with those of five other novelty detection algorithms . Finally, we compared our adaptive approach with the optimization and existing heuristic approaches to demonstrate the effectiveness and efficiency of the proposed method. The experimental results demonstrated that the proposed method outperformed the existing methods in terms of machine utilizations and throughputs.},
  archive      = {J_ASOC},
  author       = {Sangmin Lee and Hae Joong Kim and Seoung Bum Kim},
  doi          = {10.1016/j.asoc.2019.105904},
  journal      = {Applied Soft Computing},
  pages        = {105904},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Dynamic dispatching system using a deep denoising autoencoder for semiconductor manufacturing},
  volume       = {86},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Optimization strategies for microgrid energy management
systems by genetic algorithms. <em>ASOC</em>, <em>86</em>, 105903. (<a
href="https://doi.org/10.1016/j.asoc.2019.105903">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Grid-connected Microgrids (MGs) have a key role for bottom-up modernization of the electric distribution network forward next generation Smart Grids, allowing the application of Demand Response (DR) services, as well as the active participation of prosumers into the energy market. To this aim, MGs must be equipped with suitable Energy Management Systems (EMSs) in charge to efficiently manage in real time internal energy flows and the connection with the grid. Several decision making EMSs are proposed in literature mainly based on soft computing techniques and stochastic models . The adoption of Fuzzy Inference Systems (FISs) has proved to be very successful due to their ease of implementation, low computational run time cost, and the high level of interpretability with respect to more conventional models. In this work we investigate different strategies for the synthesis of a FIS ( i.e. rule based) EMS by means of a hierarchical Genetic Algorithm (GA) with the aim to maximize the profit generated by the energy exchange with the grid, assuming a Time Of Use (TOU) energy price policy, and at the same time to reduce the EMS rule base system complexity. Results show that the performances are just 10\% below to the ideal (optimal) reference solution, even when the rule base system is reduced to less than 30 rules.},
  archive      = {J_ASOC},
  author       = {Stefano Leonori and Maurizio Paschero and Fabio Massimo Frattale Mascioli and Antonello Rizzi},
  doi          = {10.1016/j.asoc.2019.105903},
  journal      = {Applied Soft Computing},
  pages        = {105903},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Optimization strategies for microgrid energy management systems by genetic algorithms},
  volume       = {86},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A large group linguistic z-DEMATEL approach for identifying
key performance indicators in hospital performance management.
<em>ASOC</em>, <em>86</em>, 105900. (<a
href="https://doi.org/10.1016/j.asoc.2019.105900">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In hospital management, performance measurement is of vital importance for improving healthcare service quality. The performance of a healthcare organization is often influenced by numerous indicators, and it is unrealistic to manage them all due to the restriction of resources. In addition, the performance measurement for improvement relates to the benefits of many departments, and it is necessary for large number of experts with different backgrounds to participate in the evaluation process of healthcare indicators. In response, this study develops a large group evaluation approach using linguistic Z-numbers and decision-making trial and evaluation laboratory (DEMATEL) to determine key performance indicators (KPIs) for hospital management. For this approach, the complex and uncertain interrelation evaluations among indicators are given by experts using linguistic Z-numbers. An extended DEMATEL method is proposed to determine KPIs based on the cause and effect relationships of performance indicators. Finally, a case study in a rehabilitation hospital is presented to illustrate the effectiveness and usefulness of the proposed large group linguistic Z-DEMATEL approach. The results indicate that incidents/errors, accidents/adverse events, nosocomial infection, nursing technology pass rate, and length of stay are KPIs for the given application.},
  archive      = {J_ASOC},
  author       = {Shan Jiang and Hua Shi and Wanlong Lin and Hu-Chen Liu},
  doi          = {10.1016/j.asoc.2019.105900},
  journal      = {Applied Soft Computing},
  pages        = {105900},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A large group linguistic Z-DEMATEL approach for identifying key performance indicators in hospital performance management},
  volume       = {86},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Air quality prediction by neuro-fuzzy modeling approach.
<em>ASOC</em>, <em>86</em>, 105898. (<a
href="https://doi.org/10.1016/j.asoc.2019.105898">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes an air quality prediction system based on the neuro-fuzzy network approach. Historical time series data are employed to derive a set of fuzzy rules, or equivalently a neuro-fuzzy network, for forecasting air pollutant concentrations and environmental factors in the future. Due to the uncertainty of the involved impact factors, fuzzy elements are added to the forecasting system. First of all, training data are partitioned into fuzzy clusters whose membership functions are characterized by the estimated means and variances. From these fuzzy clusters , fuzzy rules are extracted and a four-layer fuzzy neural network is constructed. Then genetic, particle swarm optimization , and steepest descent backpropagation algorithms are applied to train the network. The network outputs, derived through the fuzzy inference process, produce the forecast air pollutant concentrations or air quality indices. Our proposed approach has the following advantages: (1) Adding fuzzy elements can more appropriately deal with the uncertainty of the impact factors involved; (2) The distribution of training data can be described properly by fuzzy clusters with statistical means and variances; (3) Fuzzy rules are extracted automatically from the training data, instead of being supplied manually by human experts; (4) The obtained fuzzy rules are of high quality, and their parameters can be optimized effectively.},
  archive      = {J_ASOC},
  author       = {Yu-Chun Lin and Shie-Jue Lee and Chen-Sen Ouyang and Chih-Hung Wu},
  doi          = {10.1016/j.asoc.2019.105898},
  journal      = {Applied Soft Computing},
  pages        = {105898},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Air quality prediction by neuro-fuzzy modeling approach},
  volume       = {86},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A new hesitant fuzzy linguistic approach for multiple
attribute decision making based on dempster–shafer evidence theory.
<em>ASOC</em>, <em>86</em>, 105897. (<a
href="https://doi.org/10.1016/j.asoc.2019.105897">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hesitant fuzzy linguistic term sets (HFLTSs) are useful tool to represent qualitative information in multiple attribute decision making (MADM), and Dempster–Shafer evidence theory (DSET) has some advantages in denoting and fusing uncertain information. The goal of this paper is to develop a new hesitant fuzzy linguistic (HFL) MADM approach based on the DSET. To realize this goal, we propose a method of converting the original decision matrix expressed by HFLTSs into the evidence matrix with HFLTSs, and develop a weight-determining model for MADM problems with HFL information. Further, in order to integrate the evidences with HFLTSs under all attributes, we propose a combination algorithm for MADM problems based on the combination rule of DSET. Based on these studies, we develop a HFL-DSET approach for MADM problems with unknown weights. Furthermore, an applicable example for supplier selection is used to illustrate the proposed approach. Lastly, some comparative analyses with other HFL-MADM methods are conducted to show the feasibility and superiority of the proposed approach.},
  archive      = {J_ASOC},
  author       = {Peide Liu and Xiaohong Zhang},
  doi          = {10.1016/j.asoc.2019.105897},
  journal      = {Applied Soft Computing},
  pages        = {105897},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A new hesitant fuzzy linguistic approach for multiple attribute decision making based on Dempster–Shafer evidence theory},
  volume       = {86},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Maximizing receiver operating characteristics convex hull
via dynamic reference point-based multi-objective evolutionary
algorithm. <em>ASOC</em>, <em>86</em>, 105896. (<a
href="https://doi.org/10.1016/j.asoc.2019.105896">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The receiver operating characteristic convex hull (ROCCH) is a popular technique for analyzing the performance of classifiers, which is particularly effective for the tasks with unbalanced data distribution. Although maximization of ROCCH can be tackled as a bi-objective optimization problem , existing multi-objective evolutionary algorithms (MOEAs) encounter difficulties in obtaining an ROCCH, since ROCCH is always convex but the Pareto front obtained by MOEAs may be concave. To address the issue, in this paper, a dynamic reference point-based MOEA, namely DR-MOEA is proposed for maximizing ROCCH performance. Specifically, in DR-MOEA, a reference point-based sorting is suggested, where the solutions are sorted by their distances to the reference points instead of Pareto dominance. Hence an ROCCH rather than a Pareto front is expected to be obtained. In addition, a reference point adaptation strategy is also designed, with which the reference points are dynamically adjusted during the evolutionary process, and the performance of DR-MOEA is further enhanced. Empirical studies are conducted by comparing the proposed algorithm with several state-of-the-arts on different data sets. Experimental results demonstrate the superiority of DR-MOEA over the comparison methods in solving the ROCCH maximization problem.},
  archive      = {J_ASOC},
  author       = {Fan Cheng and Qiangqiang Zhang and Ye Tian and Xingyi Zhang},
  doi          = {10.1016/j.asoc.2019.105896},
  journal      = {Applied Soft Computing},
  pages        = {105896},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Maximizing receiver operating characteristics convex hull via dynamic reference point-based multi-objective evolutionary algorithm},
  volume       = {86},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Value-added tax fraud detection with scalable anomaly
detection techniques. <em>ASOC</em>, <em>86</em>, 105895. (<a
href="https://doi.org/10.1016/j.asoc.2019.105895">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The tax fraud detection domain is characterized by very few labelled data (known fraud/legal cases) that are not representative for the population due to sample selection bias. We use unsupervised anomaly detection (AD) techniques, which are uncommon in tax fraud detection research, to deal with these domain issues. We analyse a unique dataset containing the VAT declarations and client listings of all Belgian VAT numbers pertaining to ten sectors. Our methodology consists in applying AD methods to firms belonging to the same sector and enables an efficient auditing strategy that can be adopted by tax authorities worldwide. The high lifts and hit rates observed in most sectors demonstrate the success of this approach. Sectoral differences exist due to varying market conditions and legal requirements across sectors and we show that the optimal AD method is sector dependent. We focus on three methodological problems that show issues in the related literature. (1) Can we design suitable input features? We develop new fraud indicators from specific fields of the VAT form and client listings and show the predictive value of the combination of these features. (2) Can we design fast algorithms to deal with the large data sizes that can occur in the tax domain? New methods are developed and we demonstrate their scalability both theoretically as well as empirically. (3) How should fraud detection performance be assessed? A new evaluation methodology is proposed that provides reliable performance indications and guarantees that fraud cases are effectively detected by the proposed methods.},
  archive      = {J_ASOC},
  author       = {Jellis Vanhoeyveld and David Martens and Bruno Peeters},
  doi          = {10.1016/j.asoc.2019.105895},
  journal      = {Applied Soft Computing},
  pages        = {105895},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Value-added tax fraud detection with scalable anomaly detection techniques},
  volume       = {86},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A hybrid non-linear time-varying double-weighted particle
swarm optimization for solving non-convex combined environmental
economic dispatch problem. <em>ASOC</em>, <em>86</em>, 105894. (<a
href="https://doi.org/10.1016/j.asoc.2019.105894">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fossil-fuel based power sources cause environmental pollution such as the degradation of air quality and climate change, which negatively impacts the life on the earth. Consequently, this demands that the power generation should consider the optimal management of thermal sources that are aimed at minimizing the emission of gasses in the generation mix. The production volume of multi-pollutant gasses (SO 2 , NO x , and CO 2 ) can be reduced through a combined environmental economic dispatch (CEED) approach. This study has proposed a hybrid algorithm based on a novel combination of a modified genetic algorithm and an improved version of particle swarm optimization abbreviated as MGAIPSO to solve CEED problem . The study utilizes three robust operators to enhance the performance of the proposed hybrid algorithm. In GA, a uniformly weighted arithmetic crossover and a normally distributed mutation operator have been implemented to produce elite off-springs in each iteration and diversify the solutions in the search space. In the case of PSO, a non-linear time-varying double-weighted (NLTVDW) technique is developed to obtain a substantial balance between exploration and exploitation. To further enhance the exploitation ability of the MGAIPSO, this study has implemented two movements correctional methods to continuously monitor and amend the position and velocity of the particles. Several numerical case studies ranging from small to large-scale are carried out to validate the practicality of the proposed algorithm.},
  archive      = {J_ASOC},
  author       = {Arman Goudarzi and Yanjun Li and Ji Xiang},
  doi          = {10.1016/j.asoc.2019.105894},
  journal      = {Applied Soft Computing},
  pages        = {105894},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A hybrid non-linear time-varying double-weighted particle swarm optimization for solving non-convex combined environmental economic dispatch problem},
  volume       = {86},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Ordinal priority approach (OPA) in multiple attribute
decision-making. <em>ASOC</em>, <em>86</em>, 105893. (<a
href="https://doi.org/10.1016/j.asoc.2019.105893">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The current study aims to present a new method called Ordinal Priority Approach (OPA) in Multiple Attribute Decision-Making (MADM). This method can be used in individual or group decision-making (GDM). In the case of GDM, through this method, we first determine the experts and their priorities. The priority of experts may be determined based on their experience and/or knowledge. After prioritization of the experts, the attributes are prioritized by each expert. Meanwhile, each expert ranks the alternatives based on each attribute, and the sub-attributes if any. Ultimately, by solving the presented linear programming model of this method, the weights of the attributes, alternatives, experts, and sub-attributes would be obtained simultaneously. A significant advantage of the proposed method is that it does not make use of pairwise comparison matrix, decision-making matrix (no need for numerical input), normalization methods, averaging methods for aggregating the opinions of experts (in GDM) and linguistic variables . Another advantage of this method is the possibility for experts to only comment on the attributes and alternatives for which they have sufficient knowledge and experience. The validity of the proposed model has been evaluated using several group and individual instances. Finally, the proposed method has been compared with other methods such as AHP, BWM , TOPSIS , VIKOR , PROMETHEE and QUALIFLEX. Based on comparisons among the weights and ranks using Spearman and Pearson correlation coefficients, the proposed method has an applicable performance compared with other methods.},
  archive      = {J_ASOC},
  author       = {Younes Ataei and Amin Mahmoudi and Mohammad Reza Feylizadeh and Deng-Feng Li},
  doi          = {10.1016/j.asoc.2019.105893},
  journal      = {Applied Soft Computing},
  pages        = {105893},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Ordinal priority approach (OPA) in multiple attribute decision-making},
  volume       = {86},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A multi-objective robust possibilistic model for technology
portfolio optimization considering social impact and different types of
financing. <em>ASOC</em>, <em>86</em>, 105892. (<a
href="https://doi.org/10.1016/j.asoc.2019.105892">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With respect to limited financial resources, prioritization of technology fields in order to be supported financially is a matter of paramount significance that governmental organizations, such as “Technology Development Funds (TDFs)”, face with. Innovation and technology development, as the cornerstone of the economic development of countries, requires making decisions in terms of assigning the best-suited form of financial resources mainly by governments. Accordingly, this study addresses a multi-objective portfolio optimization problem in a multi-period setting with the aim of maximizing the created jobs – as a key factor in social welfare – as well as intended profit while minimizing the risk of inappropriate portfolio selection. To formulate the proposed mathematical model, different financing methods, technology readiness levels (TRL), and return on investment (ROI) associated with each technological project are taken into account. Afterward, to deal with the uncertainty arisen from fuzzy parameters, the Multi-Objective Robust Possibilistic Programming approach (MORPP) is applied, the performance of which is examined under several computational tests. Finally, to illustrate the performance of the proposed model and its applicability in practice, the computational results are shown through a real case study in Iran Innovation &amp; Prosperity Fund (IIPF). The results show that selecting small and medium-sized enterprises (SMEs) for being financed, is the best option when increasing job creation is considered in portfolio optimization. Furthermore, the comparison of the MORPP model results with the deterministic model shows that the solutions obtained from the robust possibilistic approach outweighed the deterministic model .},
  archive      = {J_ASOC},
  author       = {Marzieh Shaverdi and Saeed Yaghoubi and Hamidreza Ensafian},
  doi          = {10.1016/j.asoc.2019.105892},
  journal      = {Applied Soft Computing},
  pages        = {105892},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A multi-objective robust possibilistic model for technology portfolio optimization considering social impact and different types of financing},
  volume       = {86},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A new convolutional neural network model for peripapillary
atrophy area segmentation from retinal fundus images. <em>ASOC</em>,
<em>86</em>, 105890. (<a
href="https://doi.org/10.1016/j.asoc.2019.105890">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Peripapillary atrophy (PPA) is a clinical finding, which reflects the atrophy of retina layer and retinal pigment epithelium. The size of PPA area is a useful medical indicator, as it is highly associated with many diseases such as glaucoma and myopia. Therefore, separating the PPA area from retinal images, which is called PPA area segmentation, is very important. It is a challenging task, because PPA areas are irregular and non-uniform, and their contours are blurry and change gradually. To solve these issues, we transform the PPA area segmentation task into a task of segmenting another two areas with relatively regular and uniform shapes, and then propose a novel multi-task fully convolutional network (MFCN) model to jointly extract them from retinal images. Meanwhile, we take edge continuity of the target area into consideration. To evaluate the performance of the proposed model, we conduct experiments on images with PPA areas labelled by experts and achieve an average precision of 0.8928, outperforming the state-of-the-art models. To demonstrate the application of PPA segmentation in medical research, we apply PPA related features based on the segmented PPA area on differentiating glaucomatous and physiologic large cup cases. Experiment conducted on real datasets confirms the effectiveness of using these features for glaucoma diagnosis.},
  archive      = {J_ASOC},
  author       = {Yidong Chai and Hongyan Liu and Jie Xu},
  doi          = {10.1016/j.asoc.2019.105890},
  journal      = {Applied Soft Computing},
  pages        = {105890},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A new convolutional neural network model for peripapillary atrophy area segmentation from retinal fundus images},
  volume       = {86},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Chest x-ray enhancement to interpret pneumonia malformation
based on fuzzy soft set and dempster–shafer theory of evidence.
<em>ASOC</em>, <em>86</em>, 105889. (<a
href="https://doi.org/10.1016/j.asoc.2019.105889">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image enhancement algorithms are commonly used to increase the contrast and visual quality of low-dose x-ray images. This paper proposes an automated enhancement method using soft fuzzy sets with a new decision-making scheme based on Dempster-Shafer theory of evidence for the visual interpretation of pneumonia malformation in low-dose x-ray images, called as XEFSDS. The XEFSDS model first generates an original source x-ray image into a complementary image, then each original and complement image is applied to the characterized image object and background areas of fuzzy space. The S-function is utilized to define fuzzy soft sets for the classification of gray level ambiguity in both images, and hence a decision criterion via Dempster-Shafer approach and fuzzy interval has been adapted to discriminate uncertainties on the pixel intensity and the spatial information. Modified membership grade operations have been performed on each object/background area, and Werner’s AND/OR operator (an aggregation operator) has been utilized to build a new membership function from two modified membership functions. Finally, an enhanced image is obtained from the new membership function via defuzzification . Experiments on different pneumonia X-ray images demonstrate that the XEFSDS scheme produces better results than the existing methods. To show the advantages of the XEFSDS scheme, we have executed a segmentation based examination on enhanced image for the detection of pneumonia malformation as well as abnormal lobe (lobar pneumonia) or bronchopneumonia.},
  archive      = {J_ASOC},
  author       = {Biswajit Biswas and Swarup Kr Ghosh and Siddhartha Bhattacharyya and Jan Platos and Vaclav Snasel and Amlan Chakrabarti},
  doi          = {10.1016/j.asoc.2019.105889},
  journal      = {Applied Soft Computing},
  pages        = {105889},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Chest X-ray enhancement to interpret pneumonia malformation based on fuzzy soft set and Dempster–Shafer theory of evidence},
  volume       = {86},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Adaptive entropy weighted picture fuzzy clustering algorithm
with spatial information for image segmentation. <em>ASOC</em>,
<em>86</em>, 105888. (<a
href="https://doi.org/10.1016/j.asoc.2019.105888">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image segmentation has been broadly applied in computer vision and image analysis. However, many segmentation methods suffer from limited accuracy for noisy images . To improve the robustness of the existing picture fuzzy clustering and solve the problem of selecting spatial constraint parameter, a novel picture fuzzy clustering is proposed. Firstly, a novel symmetric regularizing term is constructed to solve the time-consuming problem of existing picture fuzzy clustering, and the corresponding fuzzy clustering is proposed. Secondly, considering the correlation between current pixel and its neighboring pixels , the objective function is modified by adaptive weighting fusion of local mean information, and the maximum weight entropy constraint is embedded into it to solve the difficulty of parameter selection. Finally, the local spatial information constraint item of the current pixel is constructed by using its neighboring picture fuzzy partition information and is utilized to modify the picture fuzzy partition information of current pixel to correct the clustering center. Results show the proposed algorithm has some potential advantages in segmentation accuracy and anti-noise robustness.},
  archive      = {J_ASOC},
  author       = {Chengmao Wu and Yan Chen},
  doi          = {10.1016/j.asoc.2019.105888},
  journal      = {Applied Soft Computing},
  pages        = {105888},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Adaptive entropy weighted picture fuzzy clustering algorithm with spatial information for image segmentation},
  volume       = {86},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Discrete spider monkey optimization for travelling salesman
problem. <em>ASOC</em>, <em>86</em>, 105887. (<a
href="https://doi.org/10.1016/j.asoc.2019.105887">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Meta-heuristic algorithms inspired by biological species have become very popular in recent years. Collective intelligence of various social insects such as ants, bees, wasps, termites, birds, fish, has been investigated to develop a number of meta-heuristic algorithms in the general domain of swarm intelligence (SI). The developed SI algorithms are found effective in solving different optimization tasks . Travelling Salesman Problem (TSP) is the combinatorial optimization problem where a salesman starting from a home city travels all the other cities and returns to home city in the shortest possible path. TSP is a popular problem due to the fact that the instances of TSP can be applied to solve real-world problems, implication of which turns TSP into a standard test bench for performance evaluation of new algorithms. Spider Monkey Optimization (SMO) is a recent addition to SI algorithms based on the social behaviour of spider monkeys. SMO implicitly adopts grouping and regrouping for the interactions to improve solutions; such multi-population approach is the motivation of this study to develop an effective method for TSP. This paper presents an effective variant of SMO to solve TSP called discrete SMO (DSMO). In DSMO, every spider monkey represents a TSP solution where Swap Sequence (SS) and Swap Operator (SO) based operations are employed, which enables interaction among monkeys in obtaining the optimal TSP solution. The SOs are generated using the experience of a specific spider monkey as well as the experience of other members (local leader, global leader, or a randomly selected spider monkey) of the group. The performance and effectiveness of the proposed method have been verified on a large set of TSP instances and the outcomes are compared to other well-known methods. Experimental results demonstrate the effectiveness of the proposed DSMO for solving TSP.},
  archive      = {J_ASOC},
  author       = {M.A.H. Akhand and Safial Islam Ayon and S.A. Shahriyar and N. Siddique and H. Adeli},
  doi          = {10.1016/j.asoc.2019.105887},
  journal      = {Applied Soft Computing},
  pages        = {105887},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Discrete spider monkey optimization for travelling salesman problem},
  volume       = {86},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A self-organized speciation based multi-objective particle
swarm optimizer for multimodal multi-objective problems. <em>ASOC</em>,
<em>86</em>, 105886. (<a
href="https://doi.org/10.1016/j.asoc.2019.105886">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes a self-organized speciation based multi-objective particle swarm optimizer (SS-MOPSO) to locate multiple Pareto optimal solutions for solving multimodal multi-objective problems. In the proposed method, the speciation strategy is used to form stable niches and these niches/subpopulations are optimized to search and maintain Pareto-optimal solutions in parallel. Moreover, a self-organized mechanism is proposed to improve the efficiency of the species formulation as well as the performance of the algorithm. To maintain the diversity of the solutions in both the decision and objective spaces, SS-MOPSO is incorporated with the non-dominated sorting scheme and special crowding distance techniques. The performance of SS-MOPSO is compared with a number of the state-of-the-art multi-objective optimization algorithms on fourteen test problems. Moreover, the proposed SS-MOSPO is also employed to solve a real-life problem. The experimental results suggest that the proposed algorithm is able to solve the multimodal multi-objective problems effectively and shows superior performance by finding more and better distributed Pareto solutions .},
  archive      = {J_ASOC},
  author       = {Boyang Qu and Chao Li and Jing Liang and Li Yan and Kunjie Yu and Yongsheng Zhu},
  doi          = {10.1016/j.asoc.2019.105886},
  journal      = {Applied Soft Computing},
  pages        = {105886},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A self-organized speciation based multi-objective particle swarm optimizer for multimodal multi-objective problems},
  volume       = {86},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Robust heterogeneous c-means. <em>ASOC</em>, <em>86</em>,
105885. (<a href="https://doi.org/10.1016/j.asoc.2019.105885">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fuzzy c-means is one of the popular algorithms in clustering, but it has some drawbacks such as sensitivity to outliers. Although many correntropy based works have been proposed to improve the robustness of FCM, fundamentally a proper error function is required to apply to FCM. In this paper, we present a new perspective based on the expected loss (or risk) to FCM method to provide different kinds of robustness such as robustness to outliers, to the volume of clusters and robustness in noisy environments . First, we propose Robust FCM method (RCM) by defining a loss function as a least square problem and benefiting the correntropy to make FCM robust to outliers. Furthermore, we utilize the half-quadratic (HQ) optimization as a problem-solving method. Second, inspiring by the Bayesian perspective , we define a new loss function based on correntropy as a distance metric to present Robust Heterogeneous C-Means ( R H C M RHCM ) by utilizing direct clustering (DC) method. DC helps R H C M RHCM to have robust initialization. Besides, R H C M RHCM will make some robust cluster centers in noisy environments and is capable of clustering the elliptical or spherical shaped data accurately, regardless of the volume of each cluster. The results are shown visually on some synthetic datasets including the noisy ones, the UCI repository and also on real image dataset that was gathered manually from 500px social media. Also , for evaluation of the clustering results , several validity indices are calculated. Experimental results indicate the superiority of our proposed method over the base FCM, DC, KFCM, two new methods called GPFCM and GEPFCM and a method called DC-KFCM that we created for the comparison purpose.},
  archive      = {J_ASOC},
  author       = {Atieh Gharib and Hadi Sadoghi-Yazdi and Amir hossein Taherinia},
  doi          = {10.1016/j.asoc.2019.105885},
  journal      = {Applied Soft Computing},
  pages        = {105885},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Robust heterogeneous C-means},
  volume       = {86},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). An enhanced bacterial foraging optimization and its
application for training kernel extreme learning machine. <em>ASOC</em>,
<em>86</em>, 105884. (<a
href="https://doi.org/10.1016/j.asoc.2019.105884">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Bacterial Foraging Optimization (BFO) algorithm is a swarm intelligent algorithm widely used in various optimization problems . However, BFO suffers from multiple drawbacks, including slow convergence speed, inability to jump out of local optima and fixed step length. In this study, an enhanced BFO with chaotic chemotaxis step length, Gaussian mutation and chaotic local search (CCGBFO) is proposed for overcoming the existing weakness of original BFO. First, a chaotic chemotaxis step length operation is used to produce adaptive chemotaxis step length. Then, by combining the optimal position in the current bacteria with the Gaussian mutation operation to make full use of the information of the optimal position. Finally, a chaotic local search is introduced into the chemotaxis step to ensure that the algorithm can explore a large search space in the early stage. The performance of CCGBFO was evaluated on a comprehensive set of numerical benchmark functions including IEEE CEC2014 and CEC2017 problems. In addition, CCGBFO was also used to tune the key parameters of kernel extreme learning machine for dealing with the real-world problems. The experimental results show that the proposed CCGBFO significantly outperforms the original BFO in terms of both convergence speed and solution accuracy.},
  archive      = {J_ASOC},
  author       = {Huiling Chen and Qian Zhang and Jie Luo and Yueting Xu and Xiaoqin Zhang},
  doi          = {10.1016/j.asoc.2019.105884},
  journal      = {Applied Soft Computing},
  pages        = {105884},
  shortjournal = {Appl. Soft. Comput.},
  title        = {An enhanced bacterial foraging optimization and its application for training kernel extreme learning machine},
  volume       = {86},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A fully fuzzy best–worst multi attribute decision making
method with triangular fuzzy number: A case study of maintenance
assessment in the hospitals. <em>ASOC</em>, <em>86</em>, 105882. (<a
href="https://doi.org/10.1016/j.asoc.2019.105882">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper introduces the best–worst method to solve multi-attribute decision-making (MADM) problems in the fuzzy environment. In the proposed method, there is no need to do all the possible pairwise comparisons . In other words, only reference comparisons should be done. Reference comparisons consist of assessing the relative fuzzy preference of the best criterion (alternative) over others and all the criteria (alternatives) over the worst one. Afterward, a fully fuzzy linear mathematical model will be formulated and solved to determine the weight of the criteria. The same action will be performed to find the score of alternatives. This method has some interesting and valuable characteristics: (a) less required data for pairwise comparison, (b) high ability to provide a reliable solution, (c) it is an autonomous method along with its high capability to accompany another method. To evaluate the performance, it is compared with another fuzzy MADM method in an example. Furthermore, we apply this method for the maintenance evaluation of hospitals in Bojnord. The computational study confirms the high efficiency and satisfactory performance of the method, and results are validated by a low consistency ratio. Furthermore, the suggested methodology outperforms fuzzy AHP and well verified in the test instance.},
  archive      = {J_ASOC},
  author       = {Hossein Karimi and Mohsen Sadeghi-Dastaki and Majid Javan},
  doi          = {10.1016/j.asoc.2019.105882},
  journal      = {Applied Soft Computing},
  pages        = {105882},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A fully fuzzy best–worst multi attribute decision making method with triangular fuzzy number: A case study of maintenance assessment in the hospitals},
  volume       = {86},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Optimal parameter tuning of modified active disturbance
rejection control for unstable time-delay systems using an AHP combined
multi-objective quasi-oppositional jaya algorithm. <em>ASOC</em>,
<em>86</em>, 105881. (<a
href="https://doi.org/10.1016/j.asoc.2019.105881">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Active Disturbance Rejection Control (ADRC) is an innovative control paradigm which emerged as a viable alternative to the traditional control design methods. However, the applicability of ADRC is limited to stable minimum phase systems. This paper investigates the problem of handling unstable time-delay systems under ADRC framework. To this end, an optimally tuned modified Active Disturbance Rejection Control (MADRC) scheme is proposed. The scheme consists of a model-assisted extended state observer designed using the known model information and a state feedback control law. The tuning of MADRC scheme is formulated as a multi-objective optimization problem with tracking and disturbance rejection performances as the objectives to be met simultaneously. Solution for this multi-objective optimization problem is carried out in two stages. The first stage is meant for generating a set of Pareto-optimal solutions with the help of Multi-Objective Quasi Oppositional Jaya Algorithm (MOQO-Jaya), while the second stage is for selection of the best among the available alternatives using Analytical Hierarchy Process (AHP). Simulation studies conducted on different unstable systems with time-delay illustrates the efficacy of the proposed scheme. Further, tracking and disturbance rejection performances are also assessed in the presence of nominal and perturbed conditions. Finally, the proposed method is extended to a Two-Input and Two-Output (TITO) process. Stability study is carried out by translating the proposed structure into a standard two degree of freedom framework.},
  archive      = {J_ASOC},
  author       = {M.V. Srikanth and Narri Yadaiah},
  doi          = {10.1016/j.asoc.2019.105881},
  journal      = {Applied Soft Computing},
  pages        = {105881},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Optimal parameter tuning of modified active disturbance rejection control for unstable time-delay systems using an AHP combined multi-objective quasi-oppositional jaya algorithm},
  volume       = {86},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A decomposition-based many-objective artificial bee colony
algorithm with reinforcement learning. <em>ASOC</em>, <em>86</em>,
105879. (<a href="https://doi.org/10.1016/j.asoc.2019.105879">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {When optimizing many-objective optimization problems (MaOPs), the optimization effect is normally related to the problem types. Therefore, enhancing the generalization ability is essential to the application of the algorithms. In this paper, a novel decomposition-based Artificial bee colony algorithm (ABC) for MaOP optimization, MaOABC/D-LA, is presented to enhance the generalization ability . A reinforcement learning-based searching strategy is designed in the MaOABC/D-LA, with which the algorithm adjusts its searching actions according to their performance. And a variant of the onlooker bee mechanism is proposed to balance the optimization quality. To investigate performance of the proposed algorithm, a comparison experiment is conducted. The experimental results show that the MaOABC/D-LA outperforms the peer algorithms in efficiency and solution quality for MaOPs with different types of features. This indicates the proposed method has a definite effect on improving generalization ability.},
  archive      = {J_ASOC},
  author       = {Haitong Zhao and Changsheng Zhang},
  doi          = {10.1016/j.asoc.2019.105879},
  journal      = {Applied Soft Computing},
  pages        = {105879},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A decomposition-based many-objective artificial bee colony algorithm with reinforcement learning},
  volume       = {86},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Decentralized adaptive neural network control of cascaded
DC–DC converters with high voltage conversion ratio. <em>ASOC</em>,
<em>86</em>, 105878. (<a
href="https://doi.org/10.1016/j.asoc.2019.105878">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Decentralized output voltage tracking of cascaded DC–DC converters is an interesting topic to obtain a high voltage conversion ratio. The control purpose is challenging due to the load resistance changes, renewable energy supply voltage variations and interaction of the individual converters. In this paper, four novel decentralized adaptive neural network controllers are designed on the cascaded DC–DC buck and boost converters under load and DC supply voltage uncertainties. In the beginning, individual buck and boost converter average models that can operate in both continuous and discontinuous conduction modes are derived. Then, the interconnected and decentralized state-space models of cascaded buck and boost converters are extracted. These models are highly nonlinear with unknown uncertainties which can be estimated by neural networks . Further, two decentralized adaptive backstepping neural network voltage controllers are proposed on cascaded buck converters to deal with uncertainties and interactions. However, these control strategies are not applicable to a boost converter due to its non-minimum phase nature. Then, two novel decentralized adaptive neural network with a conventional proportional–integral reference current generator are developed on the cascaded boost converters. Practical stability of the overall system is guaranteed for the proposed controllers using Lyapunov stability theorem . Finally, four control strategies provide good quality of output voltage in the presence of uncertainties and interactions. Comparative simulations are carried out on cascaded buck and boost converters to validate the effectiveness and performance of the designed methods.},
  archive      = {J_ASOC},
  author       = {Sajjad Shoja-Majidabad and Amin Hajizadeh},
  doi          = {10.1016/j.asoc.2019.105878},
  journal      = {Applied Soft Computing},
  pages        = {105878},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Decentralized adaptive neural network control of cascaded DC–DC converters with high voltage conversion ratio},
  volume       = {86},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Machine learning integrated credibilistic semi supervised
clustering for categorical data. <em>ASOC</em>, <em>86</em>, 105871. (<a
href="https://doi.org/10.1016/j.asoc.2019.105871">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In real life, availability of correctly labeled data and handling of categorical data are often acknowledged as two major challenges in pattern analysis. Thus, clustering techniques are employed on unlabeled data to group them according to homogeneity. However, clustering techniques fail to make a decision while data are uncertain, ambiguous, vague, coincidental and overlapping in nature. Hence, in this case, the use of semi supervised technique can be useful. On the other hand, real life datasets are majorly categorical in nature, where natural ordering in attribute values is missing. This special property of categorical values with the inherent characteristics like uncertainty, ambiguity and vagueness makes clustering more complicated than numerical data. In recent times, credibilistic measure shows better performance over fuzzy and possibilistic measures while considering similar inherent characteristics in numerical data. Thus, these facts motivated us to propose a semi supervised clustering technique using credibilistic measure with the integration of machine learning techniques to address the above mentioned challenges of clustering categorical data . This semi supervised technique first clusters the dataset into K K subsets with the proposed Credibilistic K K -Mode, where credibilistic measure helps to determine the homogeneity by avoiding coincident clustering problem as well as finds the points those are certain to the clusters. Thereafter, in the second part of the semi supervised technique, clustered dataset is used to build a supervised model for classification of other unlabeled or uncertain data. This technique not only handles the unlabeled data better, but also yields improved results for uncertain or ambiguous data e.g, if the credibilistic measure is same for a data point in multiple classes. The results of the proposed technique are demonstrated quantitatively and visually in comparison with widely used state-of-the-art methods for eight synthetic and four real life datasets. Finally, statistical tests have been conducted to judge the statistical significance of the results produced by the proposed technique.},
  archive      = {J_ASOC},
  author       = {Jnanendra Prasad Sarkar and Indrajit Saha and Sinjan Chakraborty and Ujjwal Maulik},
  doi          = {10.1016/j.asoc.2019.105871},
  journal      = {Applied Soft Computing},
  pages        = {105871},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Machine learning integrated credibilistic semi supervised clustering for categorical data},
  volume       = {86},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). New feature selection methods based on opposition-based
learning and self-adaptive cohort intelligence for predicting patient
no-shows. <em>ASOC</em>, <em>86</em>, 105866. (<a
href="https://doi.org/10.1016/j.asoc.2019.105866">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Patient no-shows have significant adverse effects on healthcare systems. Therefore, predicting patients’ no-shows is necessary to use their appointment slots effectively. In the literature, filter feature selection methods have been prominently used for patient no-show prediction. However, filter methods are less effective than wrapper methods. This paper presents new wrapper methods based on three variants of the proposed algorithm, Opposition-based Self-Adaptive Cohort Intelligence (OSACI). The three variants of OSACI are referred to in this paper as OSACI-Init, OSACI-Update, and OSACI-Init_Update, which are formed by the integration of Self-Adaptive Cohort Intelligence (SACI) with three Opposition-based Learning (OBL) strategies; namely: OBL initialization, OBL update, and OBL initialization and update, respectively. The performance of the proposed algorithms was examined and compared with that of Genetic Algorithm (GA), Particle Swarm Optimization (PSO), Differential Evolution (DE), and SACI in terms of AUC, sensitivity, specificity, dimensionality reduction, and convergence speed. Patient no-show data of a primary care clinic in upstate New York was used in the numerical experiments. The results showed that the proposed algorithms outperformed the other compared algorithms by achieving higher dimensionality reduction and better convergence speed while achieving comparable AUC, sensitivity, and specificity scores.},
  archive      = {J_ASOC},
  author       = {Mohammed Aladeemy and Linda Adwan and Amy Booth and Mohammad T. Khasawneh and Srikanth Poranki},
  doi          = {10.1016/j.asoc.2019.105866},
  journal      = {Applied Soft Computing},
  pages        = {105866},
  shortjournal = {Appl. Soft. Comput.},
  title        = {New feature selection methods based on opposition-based learning and self-adaptive cohort intelligence for predicting patient no-shows},
  volume       = {86},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Assessment of traffic congestion with ORESTE method under
double hierarchy hesitant fuzzy linguistic environment. <em>ASOC</em>,
<em>86</em>, 105864. (<a
href="https://doi.org/10.1016/j.asoc.2019.105864">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the new generation of information technology development and the promotion of the Internet, local governments turn their attention to the construction of intelligent transportation systems . More and more cities began building intelligent transportation which has been widely used to monitor urban traffic. Experts can evaluate urban traffic congestion based on the information collected from the big data of intelligent transportation. In recent two years, double hierarchy hesitant fuzzy linguistic term set has been widely used to depict explicit evaluation information, which is straightforward and broad-spectrum. When evaluating traffic congestion in a city, decision makers can utilize double hierarchy hesitant fuzzy linguistic term sets to express vague information. Moreover, the ORESTE method is an applicative method which can select a reliable alternative by subdividing alternatives and reduce the loss of information in the conversion process. In this paper, we propose a double hierarchy hesitant fuzzy linguistic ORESTE method and a new score function of double hierarchy hesitant fuzzy linguistic term set. The method raises a new perspective to reduce the error from other methods and the new score function derives a robust decision-making result. Then, we apply the double hierarchy hesitant fuzzy linguistic ORESTE method to solve a practical case involving choosing the congested city by evaluating the 5S traffic congestion model. Finally, we compare the double hierarchy hesitant fuzzy linguistic ORESTE method with other methods such as the classical ORESTE method and the double hierarchy hesitant fuzzy linguistic MULTIMOORA to illustrate the advantages of our method.},
  archive      = {J_ASOC},
  author       = {Xindi Wang and Xunjie Gou and Zeshui Xu},
  doi          = {10.1016/j.asoc.2019.105864},
  journal      = {Applied Soft Computing},
  pages        = {105864},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Assessment of traffic congestion with ORESTE method under double hierarchy hesitant fuzzy linguistic environment},
  volume       = {86},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Optimizing an integrated inventory-routing system for
multi-item joint replenishment and coordinated outbound delivery using
differential evolution algorithm. <em>ASOC</em>, <em>86</em>, 105863.
(<a href="https://doi.org/10.1016/j.asoc.2019.105863">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A practical and new joint replenishment and delivery (JRD) problem that considers coordinated outbound delivery policy of multiple items (named JRCD) is studied. The proposed JRCD model aims to balance the joint replenishment , inventory holding, and delivery costs by deciding on the replenishment and outbound delivery schedule of each item. The indirect grouping policy is utilized in the JRCD problem, so items dispatched jointly in each outbound delivery are identified on the basis of the replenishment frequency and delivery frequency. Once the matching of items and retailers in each outbound delivery is confirmed, the optimal route can be subsequently obtained by solving the traveling salesman problem . To solve this complex optimization problem , an intelligent algorithm based on differential evolution is utilized because of its superior performance in handling similar complex problems. Basic and extended numerical examples are used to verify the effectiveness of the proposed algorithm. A comparison between the proposed JRCD and JRD with independent delivery is conducted with examples of varying cost parameters. Results provide interesting insights and useful guidelines for managers to create a reasonable policy for effectively controlling their total cost.},
  archive      = {J_ASOC},
  author       = {Hui Qu and Xue-Yi Ai and Lin Wang},
  doi          = {10.1016/j.asoc.2019.105863},
  journal      = {Applied Soft Computing},
  pages        = {105863},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Optimizing an integrated inventory-routing system for multi-item joint replenishment and coordinated outbound delivery using differential evolution algorithm},
  volume       = {86},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Analysis of brain sub regions using optimization techniques
and deep learning method in alzheimer disease. <em>ASOC</em>,
<em>86</em>, 105857. (<a
href="https://doi.org/10.1016/j.asoc.2019.105857">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Automatic segmentation of brain sub regions such as White Matter (GM), Corpus Callosum (CC), Grey Matter (WM) and Hippocampus (HC) is a challenging task due to the variations in the structure of the brain. The development of an automation process will help in identifying the Alzheimer disease (AD). The maximum distinctive AD related to neuronal loss in the brain sub regions is GM, WM, CC and HC. Several nature optimization algorithms are being developed to get an optimum solution for segmenting these kinds of very hectic regions. In this work, brain sub regions have been considered to diagnose the AD using four various optimization algorithms such as Genetic Algorithm (GA), Particle Swarm Optimization algorithm (PSO), Grey Wolf Optimization (GWO) and Cuckoo Search (CS). Among these optimization techniques, GWO shows promising results due to the proper selection of global optimum solution. The segmented regions have been classified using deep learning classifier and it has been validated with Ground Truth (GT) images. The results prove that GWO is capable to segment the brain sub regions with high accuracy of 98\% similarity among the ground truth and segmented region. Then, the segmented regions are classified using deep learning classifier and the results show the high accuracy of 95\%. From the above sub regions, HC proves better classification accuracy of 95\%, sensitivity as 95\% and specificity as 94\% compared to all other regions. Based on the output of segmentation and classification measures it is clearly observed that, the proposed method provide better performance than other methods. Finally, the normal and AD subjects are also validated clinically with mini mental state examination (MMSE) score. From this evaluation it is observed that, the proposed work is highly correlated with MMSE score. Therefore, the proposed pipeline witnessed that the HC region is the major factor for diagnosing AD.},
  archive      = {J_ASOC},
  author       = {D. Chitradevi and S. Prabha},
  doi          = {10.1016/j.asoc.2019.105857},
  journal      = {Applied Soft Computing},
  pages        = {105857},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Analysis of brain sub regions using optimization techniques and deep learning method in alzheimer disease},
  volume       = {86},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Consolidation assessment using multi expression programming.
<em>ASOC</em>, <em>86</em>, 105842. (<a
href="https://doi.org/10.1016/j.asoc.2019.105842">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this study, new approximate solutions for consolidation have been developed in order to hasten the calculations. These solutions include two groups of equations, one can be used to calculate the average degree of consolidation and the other one for computing the time factor (inverse functions). Considering the complicated nature of consolidation, an evolutionary computation technique called Multi-Expression Programming was applied to generate several non-piecewise models which are accurate and straightforward enough for different purposes for calculating the degree of consolidation for each depth and its average as well for the whole soil layer. The parametric study was also performed to investigate the impact of each input parameter on the predicted consolidation degree of developed models for each depth. Moreover, the results of the consolidation test carried out on four different clays attained from the literature showed the proper performance of the proposed models.},
  archive      = {J_ASOC},
  author       = {Sohrab Sharifi and Saeed Abrishami and Amir H. Gandomi},
  doi          = {10.1016/j.asoc.2019.105842},
  journal      = {Applied Soft Computing},
  pages        = {105842},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Consolidation assessment using multi expression programming},
  volume       = {86},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Ensemble approach based on bagging, boosting and stacking
for short-term prediction in agribusiness time series. <em>ASOC</em>,
<em>86</em>, 105837. (<a
href="https://doi.org/10.1016/j.asoc.2019.105837">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The investigation of the accuracy of methods employed to forecast agricultural commodities prices is an important area of study. In this context, the development of effective models is necessary. Regression ensembles can be used for this purpose. An ensemble is a set of combined models which act together to forecast a response variable with lower error. Faced with this, the general contribution of this work is to explore the predictive capability of regression ensembles by comparing ensembles among themselves, as well as with approaches that consider a single model (reference models) in the agribusiness area to forecast prices one month ahead. In this aspect, monthly time series referring to the price paid to producers in the state of Parana, Brazil for a 60 kg bag of soybean (case study 1) and wheat (case study 2) are used. The ensembles bagging (random forests — RF), boosting (gradient boosting machine — GBM and extreme gradient boosting machine — XGB), and stacking (STACK) are adopted. The support vector machine for regression (SVR), multilayer perceptron neural network (MLP) and K -nearest neighbors (KNN) are adopted as reference models. Performance measures such as mean absolute percentage error (MAPE), root mean squared error (RMSE), mean absolute error (MAE), and mean squared error (MSE) are used for models comparison. Friedman and Wilcoxon signed rank tests are applied to evaluate the models’ absolute percentage errors (APE). From the comparison of test set results, MAPE lower than 1\% is observed for the best ensemble approaches. In this context, the XGB/STACK (Least Absolute Shrinkage and Selection Operator-KNN-XGB-SVR) and RF models showed better performance for short-term forecasting tasks for case studies 1 and 2, respectively. Better APE (statistically smaller) is observed for XGB/STACK and RF in relation to reference models. Besides that, approaches based on boosting are consistent, providing good results in both case studies. Alongside, a rank according to the performances is: XGB, GBM, RF, STACK, MLP, SVR and KNN. It can be concluded that the ensemble approach presents statistically significant gains, reducing prediction errors for the price series studied. The use of ensembles is recommended to forecast agricultural commodities prices one month ahead, since a more assertive performance is observed, which allows to increase the accuracy of the constructed model and reduce decision-making risk.},
  archive      = {J_ASOC},
  author       = {Matheus Henrique Dal Molin Ribeiro and Leandro dos Santos Coelho},
  doi          = {10.1016/j.asoc.2019.105837},
  journal      = {Applied Soft Computing},
  pages        = {105837},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Ensemble approach based on bagging, boosting and stacking for short-term prediction in agribusiness time series},
  volume       = {86},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Evaluation of feature selection methods for text
classification with small datasets using multiple criteria
decision-making methods. <em>ASOC</em>, <em>86</em>, 105836. (<a
href="https://doi.org/10.1016/j.asoc.2019.105836">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The evaluation of feature selection methods for text classification with small sample datasets must consider classification performance, stability, and efficiency. It is, thus, a multiple criteria decision-making (MCDM) problem. Yet there has been few research in feature selection evaluation using MCDM methods which considering multiple criteria. Therefore, we use MCDM-based methods for evaluating feature selection methods for text classification with small sample datasets. An experimental study is designed to compare five MCDM methods to validate the proposed approach with 10 feature selection methods, nine evaluation measures for binary classification , seven evaluation measures for multi-class classification, and three classifiers with 10 small datasets. Based on the ranked results of the five MCDM methods, we make recommendations concerning feature selection methods. The results demonstrate the effectiveness of the used MCDM-based method in evaluating feature selection methods.},
  archive      = {J_ASOC},
  author       = {Gang Kou and Pei Yang and Yi Peng and Feng Xiao and Yang Chen and Fawaz E. Alsaadi},
  doi          = {10.1016/j.asoc.2019.105836},
  journal      = {Applied Soft Computing},
  pages        = {105836},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Evaluation of feature selection methods for text classification with small datasets using multiple criteria decision-making methods},
  volume       = {86},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Automatic determination of digital modulation types with
different noises using convolutional neural network based on
time–frequency information. <em>ASOC</em>, <em>86</em>, 105834. (<a
href="https://doi.org/10.1016/j.asoc.2019.105834">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this study, a novel digital modulation classification model has been proposed for automatically recognizing six different modulation types including amplitude shift keying (ASK), frequency shift keying (FSK), phase-shift keying (PSK), quadrate amplitude shift keying (QASK), quadrate frequency shift keying (QFSK), and quadrate phase-shift keying (QPSK). The determination of modulation type is significant in military communication, satellite communication systems , and submarine communication. To classify the modulation types, we have proposed a two-stage hybrid method combining short-time Fourier transform (STFT) and convolutional neural network (CNN). In the first stage, as the data source, the time–frequency information from these modulation signals have been extracted with STFT. This information has been obtained as 2D images to feed the input of the CNN deep learning method. In the second stage, the obtained 2D time–frequency information has been given to the input of the CNN algorithm to classify the modulation types. In this work, noises at various SNR values from 0 dB to 25 dB were created and added to the modulated signals. Even in the presence of noise, the proposed hybrid deep learning model achieved excellent results in the noised-modulation signals.},
  archive      = {J_ASOC},
  author       = {Nihat Daldal and Zafer Cömert and Kemal Polat},
  doi          = {10.1016/j.asoc.2019.105834},
  journal      = {Applied Soft Computing},
  pages        = {105834},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Automatic determination of digital modulation types with different noises using convolutional neural network based on time–frequency information},
  volume       = {86},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Human action recognition using two-stream attention based
LSTM networks. <em>ASOC</em>, <em>86</em>, 105820. (<a
href="https://doi.org/10.1016/j.asoc.2019.105820">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {It is well known that different frames play different roles in feature learning in video based human action recognition task. However, most existing deep learning models put the same weights on different visual and temporal cues in the parameter training stage, which severely affects the feature distinction determination. To address this problem, this paper utilizes the visual attention mechanism and proposes an end-to-end two-stream attention based LSTM network. It can selectively focus on the effective features for the original input images and pay different levels of attentions to the outputs of each deep feature maps. Moreover, considering the correlation between two deep feature streams, a deep feature correlation layer is proposed to adjust the deep learning network parameter based on the correlation judgement. In the end, we evaluate our approach on three different datasets, and the experiments results show that our proposal can achieve the state-of-the-art performance in the common scenarios.},
  archive      = {J_ASOC},
  author       = {Cheng Dai and Xingang Liu and Jinfeng Lai},
  doi          = {10.1016/j.asoc.2019.105820},
  journal      = {Applied Soft Computing},
  pages        = {105820},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Human action recognition using two-stream attention based LSTM networks},
  volume       = {86},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Do you know your customer? Bank risk assessment based on
machine learning. <em>ASOC</em>, <em>86</em>, 105779. (<a
href="https://doi.org/10.1016/j.asoc.2019.105779">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Know Your Customer (KYC) data can serve as a valuable risk assessment tool for banks by providing information that can identify customers who are more likely to default on a loan. This study aims to provide an accurate risk assessment tool using unique KYC data and machine-learning techniques to overcome problems in existing risk detection methods. This study proposes that the bank branch is the best level at which to determine the degree of default risk, and can also provide insight into patterns of suspicious transactions. Bank managers and regulators can focus on suspicious behavior at specific branches to increase overall compliance and reduce the risk of illegal activity.},
  archive      = {J_ASOC},
  author       = {Ting-Hsuan Chen},
  doi          = {10.1016/j.asoc.2019.105779},
  journal      = {Applied Soft Computing},
  pages        = {105779},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Do you know your customer? bank risk assessment based on machine learning},
  volume       = {86},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Electrocardiogram soft computing using hybrid deep learning
CNN-ELM. <em>ASOC</em>, <em>86</em>, 105778. (<a
href="https://doi.org/10.1016/j.asoc.2019.105778">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Electrocardiogram (ECG) can reflect the state of human heart and is widely used in clinical cardiac examination. However, the electrocardiogram signal is very weak, the anti-interference ability is poor, easy to be affected by the noise. Doctors face difficulties in diagnosing arrhythmias. Therefore, automatic recognition and classification of ECG signals is an important and indispensable task. Since the beginning of the 21 st century, deep learning has developed rapidly and has shown the most advanced performance in various fields. This paper presents a method of combining (Convolutional neural network) CNN and ELM (extreme learning machine). The accuracy rate is 97.50\%. Compared with the state-of-the-art methods, this method improves the accuracy of ECG automatic classification and has good generalization ability .},
  archive      = {J_ASOC},
  author       = {Shuren Zhou and Bo Tan},
  doi          = {10.1016/j.asoc.2019.105778},
  journal      = {Applied Soft Computing},
  pages        = {105778},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Electrocardiogram soft computing using hybrid deep learning CNN-ELM},
  volume       = {86},
  year         = {2020},
}
</textarea>
</details></li>
</ul>

</body>
</html>
