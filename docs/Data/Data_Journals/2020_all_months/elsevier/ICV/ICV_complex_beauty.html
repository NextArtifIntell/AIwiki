<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>ICV_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="icv---123">ICV - 123</h2>
<ul>
<li><details>
<summary>
(2020). Deep learning-based object detection in low-altitude UAV
datasets: A survey. <em>ICV</em>, <em>104</em>, 104046. (<a
href="https://doi.org/10.1016/j.imavis.2020.104046">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning-based object detection solutions emerged from computer vision has captivated full attention in recent years. The growing UAV market trends and interest in potential applications such as surveillance, visual navigation, object detection, and sensors-based obstacle avoidance planning have been holding good promises in the area of deep learning. Object detection algorithms implemented in deep learning framework have rapidly became a method for processing of moving images captured from drones. The primary objective of the paper is to provide a comprehensive review of the state of the art deep learning based object detection algorithms and analyze recent contributions of these algorithms to low altitude UAV datasets. The core focus of the studies is low-altitude UAV datasets because relatively less contribution was seen in the literature when compared with standard or remote-sensing based datasets. The paper discusses the following algorithms: Faster RCNN, Cascade RCNN, R-FCN etc. into two-stage, YOLO and its variants, SSD, RetinaNet into one-stage and CornerNet, Objects as Point etc. under advanced stages in deep learning based detectors. Further, one-two and advanced stages of detectors are studied in detail focusing on low-altitude UAV datasets. The paper provides a broad summary of low altitude datasets along with their respective literature in detection algorithms for the potential use of researchers. Various research gaps and challenges for object detection and classification in UAV datasets that need to deal with for improving the performance are also listed.},
  archive      = {J_ICV},
  author       = {Payal Mittal and Raman Singh and Akashdeep Sharma},
  doi          = {10.1016/j.imavis.2020.104046},
  journal      = {Image and Vision Computing},
  month        = {12},
  pages        = {104046},
  shortjournal = {Image Vis. Comput.},
  title        = {Deep learning-based object detection in low-altitude UAV datasets: A survey},
  volume       = {104},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Dependable information processing method for reliable
human-robot interactions in smart city applications. <em>ICV</em>,
<em>104</em>, 104045. (<a
href="https://doi.org/10.1016/j.imavis.2020.104045">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Human-robot interaction (HRI) is a multidisciplinary area that consists of several technologies that are used to create various smart city applications . The knowledge gain and analysis of the smart city environment improves response time . This paper introduces the dependable information processing (DIP) method for handling multi-attribute environmental information in a smart city application. Information sensed from the environment is categorized in the initial stage regarding how it meets application requirements. It helps to identify the need and response of the application through different interacting spans and previous trials. For attribute categorization and span validation, learning via K-means clustering is exploited to identify similar and dissimilar information attributes. This identification speeds up the process of successive responses with improved interaction sessions. The accuracy of user responses and latency-free assessment improves the reliability of DIP in smart city applications. The efficiency of the system is then evaluated using detection accuracy (97.34%), the response time (4.3 s), and interaction time (12.3 s), which is compared with other methods.},
  archive      = {J_ICV},
  author       = {Zafer Al-Makhadmeh and Amr Tolba},
  doi          = {10.1016/j.imavis.2020.104045},
  journal      = {Image and Vision Computing},
  month        = {12},
  pages        = {104045},
  shortjournal = {Image Vis. Comput.},
  title        = {Dependable information processing method for reliable human-robot interactions in smart city applications},
  volume       = {104},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Infrared and visible image fusion via global variable
consensus. <em>ICV</em>, <em>104</em>, 104037. (<a
href="https://doi.org/10.1016/j.imavis.2020.104037">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose an infrared and visible image fusion framework based on the consensus problem. Most current infrared and visible image fusion models aim to transfer only one characteristic of each source domain to the final fusion result. This mechanism limits the performances of fusion algorithms under different conditions. We present a general fusion framework based to solve the global variable consensus optimization problem through altering direction method of multipliers (ADMM). We identified that combination of the local operators allows smooth transfer of superficial characteristics of the source domain into the fusion result. Our modification of ADMM enables us to expand the fusion algorithm&#39;s compatibility by tackling various setting including dimensionality, data types and style. The qualitative and quantitative experiment results demonstrate that, compared with other state-of-the-art algorithms, the proposed method can provide competitive performance in transferring features, structures, and information from source images to fusion results.},
  archive      = {J_ICV},
  author       = {Donghao Shen and Masoumeh Zareapoor and Jie Yang},
  doi          = {10.1016/j.imavis.2020.104037},
  journal      = {Image and Vision Computing},
  month        = {12},
  pages        = {104037},
  shortjournal = {Image Vis. Comput.},
  title        = {Infrared and visible image fusion via global variable consensus},
  volume       = {104},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Synergetic reconstruction from 2D pose and 3D motion for
wide-space multi-person video motion capture in the wild. <em>ICV</em>,
<em>104</em>, 104028. (<a
href="https://doi.org/10.1016/j.imavis.2020.104028">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Although many studies have investigated markerless motion capture, the technology has not been applied to real sports or concerts. In this paper, we propose a markerless motion capture method with spatiotemporal accuracy and smoothness from multiple cameras in wide-space and multi-person environments. The proposed method predicts each person&#39;s 3D pose and determines the bounding box of multi-camera images small enough. This prediction and spatiotemporal filtering based on human skeletal model enables 3D reconstruction of the person and demonstrates high-accuracy. The accurate 3D reconstruction is then used to predict the bounding box of each camera image in the next frame. This is feedback from the 3D motion to 2D pose, and provides a synergetic effect on the overall performance of video motion capture. We evaluated the proposed method using various datasets and a real sports field. The experimental results demonstrate that the mean per joint position error (MPJPE) is 31.5 mm and the percentage of correct parts (PCP) is 99.5% for five people dynamically moving while satisfying the range of motion (RoM). Video demonstration, datasets, and additional materials are posted on our project page 1 .},
  archive      = {J_ICV},
  author       = {Takuya Ohashi and Yosuke Ikegami and Yoshihiko Nakamura},
  doi          = {10.1016/j.imavis.2020.104028},
  journal      = {Image and Vision Computing},
  month        = {12},
  pages        = {104028},
  shortjournal = {Image Vis. Comput.},
  title        = {Synergetic reconstruction from 2D pose and 3D motion for wide-space multi-person video motion capture in the wild},
  volume       = {104},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). CAM: A fine-grained vehicle model recognition method based
on visual attention model. <em>ICV</em>, <em>104</em>, 104027. (<a
href="https://doi.org/10.1016/j.imavis.2020.104027">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Vehicle model recognition (VMR) is a typical fine-grained classification task in computer vision . To improve the representation power of classical CNN networks for this special task, we focus on enhancing the subtle difference of features and their spatial encoding based on the attention mechanism , and then propose a novel architectural unit, which we term the “convolutional attention model” (CAM). It adopts a two-stage attention mechanism for VMR, which includes the global feature map attention (GFMA) algorithm, applied at the lower part of the main network flow to enhance the subtle feature difference from the beginning, and the feature spatial relationship attention (FSRA) algorithm, applied at the higher part to enhance the spatial relationship of features. The experiments are conducted on the benchmark CompCars web-nature and Stanford Car datasets and demonstrate the effectiveness of CAM when integrated with some classical CNN architectures. CAM can improve the top-1 recognition accuracy by an average of 1.15% and top-5 by an average of 0.78%.},
  archive      = {J_ICV},
  author       = {Ye Yu and Longdao Xu and Wei Jia and Wenjia Zhu and Yunxiang Fu and Qiang Lu},
  doi          = {10.1016/j.imavis.2020.104027},
  journal      = {Image and Vision Computing},
  month        = {12},
  pages        = {104027},
  shortjournal = {Image Vis. Comput.},
  title        = {CAM: A fine-grained vehicle model recognition method based on visual attention model},
  volume       = {104},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Synthetic guided domain adaptive and edge aware network for
crowd counting. <em>ICV</em>, <em>104</em>, 104026. (<a
href="https://doi.org/10.1016/j.imavis.2020.104026">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Crowd counting is an important surveillance application and receives significant attention from the computer vision community. Most of the current methods treat crowd counting by density map estimation and use the Fully Convolution Network (FCN) for prediction. The mainstream framework is to predict density maps and use the sum up the density maps to get the number of people. In such methods, the main drawback is the poor local quality of the dense part and the sparse part of an image. As we investigated, it is due to the lack of an efficient method to learn the heads&#39; structure information. To address the above problem, in this paper, we propose a domain adaptive model called synthetic guided learning that learns features&#39; structure from synthetic data . We also propose a multi-scale edge-aware loss for improving the boundary clearness of the estimated density map. Our experimental results show, learning from the structure information effectively improves the density maps&#39; estimation quality and promotes the counting accuracy. Comprehensive experiments and comparisons with state-of-the-art methods on four publicly available data sets demonstrate the superiority of our proposed method. We provide a reference implementation of this technique at https://github.com/MRJTM/SGEANet .},
  archive      = {J_ICV},
  author       = {Zhijie Cao and Pourya Shamsolmoali and Jie Yang},
  doi          = {10.1016/j.imavis.2020.104026},
  journal      = {Image and Vision Computing},
  month        = {12},
  pages        = {104026},
  shortjournal = {Image Vis. Comput.},
  title        = {Synthetic guided domain adaptive and edge aware network for crowd counting},
  volume       = {104},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Optimization of face recognition algorithm based on deep
learning multi feature fusion driven by big data. <em>ICV</em>,
<em>104</em>, 104023. (<a
href="https://doi.org/10.1016/j.imavis.2020.104023">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Today, with the rapid development of science and technology, the era of big data has been proposed and triggered reforms in all walks of life. Face recognition is a biometric recognition method with the characteristics of non-contact, non mandatory, friendly and harmonious, which has a good application prospect in the fields of national security and social security. With the deepening of the research on face recognition, small-scale face recognition has achieved good recognition results, but in the era of big data, the existing small-scale face recognition methods have gradually failed to meet the social needs, and how to get a good face recognition effect in the era of big data has become a new research hotspot. Based on this, this paper aims to optimize the existing face recognition algorithm, study the face recognition method driven by big data, and propose a deep learning multi feature fusion face recognition algorithm driven by big data. First, for the problem that 2DPCA (Two-dimensional Principle Component Analysis) can well extract the global features of the face under large samples, but the local features of the face are difficult to process, this paper uses the LBP (Local Binary Pattern, LBP) algorithm to extract the texture features of the face, and the extracted texture features are integrated with the global features extracted by 2DPCA to multi-feature fusion, so that the fused features can take into account both global and local features, and have better recognition results. Then using the obtained fusion features as input, training in a convolutional neural network , and measuring the similarity based on the feature vectors of the sample set and the training set after the training, can realize multi-feature fusion face recognition. Through the analysis of simulation experiments, it is found that, compared with the use of global features or local features alone, the fusion features obtained by multi-feature fusion of global features extracted by 2DPCA and local features extracted by LBP algorithm have better recognition effect in the big data environment . After convolutional neural network trains and recognizes this feature, a high recognition accuracy rate is obtained, which can show that the face recognition method designed in this paper has good application potential in the era of big data. In the background of big data, the accuracy of face recognition can reach more than 90%, which can meet the needs of society well.},
  archive      = {J_ICV},
  author       = {Yinghui Zhu and Yuzhen Jiang},
  doi          = {10.1016/j.imavis.2020.104023},
  journal      = {Image and Vision Computing},
  month        = {12},
  pages        = {104023},
  shortjournal = {Image Vis. Comput.},
  title        = {Optimization of face recognition algorithm based on deep learning multi feature fusion driven by big data},
  volume       = {104},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Improving face verification using facial marks and deep CNN:
IARPA janus benchmark-a. <em>ICV</em>, <em>104</em>, 104020. (<a
href="https://doi.org/10.1016/j.imavis.2020.104020">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Face verification performance by human brain has been shown to be much better than most of the state-of-the-art approaches in computer vision . Performance improvement of automated face recognition (FR) systems that may equal or surpass the human intellect is the key goal of this research. We present our face verification system using facial mark (FM) combined with deep convolutional neural network (DCNN) approach to improve the overall FR accuracy . We propose to use FM (e.g., scars, moles and freckles) for face matching in the wild where the FM detection is performed on mean faces as well as affine aligned normalized facial images . The FR experiments are carried out on IARPA Janus Benchmark-A (IJB-A) dataset which includes real-world unconstrained images from 500 subjects. The IJB-A datasets includes full pose, expression, and illumination variations which are much harder than traditional FERET and Mugshot datasets. We evaluated the average FR performance using a weighted score-level fusion of FM and DCNN based recognition methods. The experimental evaluations on FERET, CFM and Mugshot datasets show higher performances than state-of-the-art FM approaches with 99.23%, 94.64% and 97.86% accuracies in Rank-1 evaluations, respectively. Our FR performance of FM + DCNN (86.46% in TAR, 91.23% in Rank-1, 96.57% in Rank-5, 98.65% in Rank-10) is shown to be higher than state-of-the-art (83.80% in TAR@1%FAR, 90.30% in Rank-1, 96.5% in Rank-5, and 97.7% in Rank-10). Experimental results after fusion of FM + DCNN on IJB-A dataset show 2.66% FR performance improvement from the DCNN only recognition in terms of TAR@1%FAR.},
  archive      = {J_ICV},
  author       = {Sidra Riaz and Unsang Park and Prem Natarajan},
  doi          = {10.1016/j.imavis.2020.104020},
  journal      = {Image and Vision Computing},
  month        = {12},
  pages        = {104020},
  shortjournal = {Image Vis. Comput.},
  title        = {Improving face verification using facial marks and deep CNN: IARPA janus benchmark-A},
  volume       = {104},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Cancelable iris template generation by aggregating patch
level ordinal relations with its holistically extended performance and
security analysis. <em>ICV</em>, <em>104</em>, 104017. (<a
href="https://doi.org/10.1016/j.imavis.2020.104017">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nowadays, biometric-based authentication is gaining immense popularity due to the widespread usage of digital activities. Among various biometric traits, the iris is one of the most discriminative, accurate, and popularly used biometrics. However, due to its immutable nature, it is highly vulnerable to adversarial attacks if stolen and thus poses a severe security threat. Here, in this work, we propose a cancelable iris biometric authentication system that stores a transformed version of the original iris template and thus enables cancelation and re-enrolment in case if the original template is stolen. Firstly, for extracting discriminative iris features, we have proposed a novel deep architecture based on aggregation learning. This deep architecture makes use of qualitative measure (ordinal measure), unlike popularly used quantitative measures. The usage of ordinal measures in this work enables to encode distinctive iris features quite well. Later generated iris features are protected using state-of-the-art two representative cancelable biometric techniques, namely BioHashing and 2 N discretized BioPhasor. Finally, in order to justify the efficacy of the proposed architecture, we have presented rigorous and holistic security analysis. To the best of our knowledge, this is the first work that has presented such an in-depth analysis of any deep network in the context of cancelable iris biometrics. Experimental results over four datasets viz . CASIA-V3 Interval, CASIA-Lamp, IITD, and IITK demonstrate the efficacy of the proposed framework in terms of security and accuracy. Further, for better network explainability, we have also performed layer-specific heatmap and feature map analysis to ascertain what exactly our novel deep architecture is learning.},
  archive      = {J_ICV},
  author       = {Avantika Singh and Ashish Arora and Aditya Nigam},
  doi          = {10.1016/j.imavis.2020.104017},
  journal      = {Image and Vision Computing},
  month        = {12},
  pages        = {104017},
  shortjournal = {Image Vis. Comput.},
  title        = {Cancelable iris template generation by aggregating patch level ordinal relations with its holistically extended performance and security analysis},
  volume       = {104},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Variance-guided attention-based twin deep network for
cross-spectral periocular recognition. <em>ICV</em>, <em>104</em>,
104016. (<a href="https://doi.org/10.1016/j.imavis.2020.104016">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Periocular region is considered as an important biometric trait due to its ease of collectability and high acceptability in our society. Recent advancements in surveillance applications require infra-red (IR) sensing equipments to be deployed in order to capture the activities occurring in low-light conditions. This gives rise to the problem of matching periocular images in heterogeneous environments as it is difficult to avail large enrollment datasets in IR modality within a short span of time. Although a number of approaches have studied cross-spectral matching of periocular images where a probe IR image is matched against enrolled dataset images in visible (VIS) domain and vice versa, significant amount of challenges still exists for such matching. In this paper, we propose an attention-based twin deep convolutional neural network (CNN) with shared parameters in order to match the periocular images in heterogeneous modality. We introduce a novel variance-guided objective function in conjunction with the attention module in order to guide the network to focus more into the relevant regions of the periocular images. The weights of the twin model based on the new objective function are learned so as to reduce the intra-class variance and to increase the inter-class variance of the cross-spectral image pairs. Ablation studies and experimental results on three publicly available cross-spectral periocular datasets containing images from VIS, near-infrared (NIR), and night vision domains show that the proposed deep network achieves the state-of-the-art recognition performances on all three datasets.},
  archive      = {J_ICV},
  author       = {Sushree S. Behera and Sapna S. Mishra and Bappaditya Mandal and Niladri B. Puhan},
  doi          = {10.1016/j.imavis.2020.104016},
  journal      = {Image and Vision Computing},
  month        = {12},
  pages        = {104016},
  shortjournal = {Image Vis. Comput.},
  title        = {Variance-guided attention-based twin deep network for cross-spectral periocular recognition},
  volume       = {104},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Vehicle re-identification based on unsupervised local area
detection and view discrimination. <em>ICV</em>, <em>104</em>, 104008.
(<a href="https://doi.org/10.1016/j.imavis.2020.104008">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Vehicle re-identification is an important part of intelligent transportation . Although much work has been done on this subject in recent years, vehicle re-identification is still a challenging task due to its obvious illumination change, high similarity between inter-class and great changes under different views. As discriminatory local areas and vehicle view information is the key to improving the above issues, it is desirable to create a model which considers both local details and cross-view situation. In this paper, we built a vehicle re-identification framework based on unsupervised local area detection and view discrimination. First, we combine the convolution features of multiple vehicle images by channel to generate joint channel features, and constructs a discriminating region detector unsupervised by clustering the channel covariance vectors generated between the joint channel features. In the next stage, the irregular shape detection results are converted to rectangular discriminative region by using a novel local region integration algorithm . These rectangular discriminative regions are fed into a multi-branch network to extract the local–global features which contains rich detail information. Furthermore, we generate the view features by regarding the detected discriminative areas as keypoints and construct the unsupervised view discriminator . By using the view information of the vehicle, we designed a view-discrimination based reranking algorithm, which effectively reduces the error rate of identification due to view variant. In order to prove the validity of our method, we have done extensive experiments on VehicleID and VERI-Wild dataset. Experimental results show that our method is superior to other existing vehicle re-identification algorithms.},
  archive      = {J_ICV},
  author       = {Yuefeng Wang and Huadong Li and Ying Wei and Chuyuan Wang and Lin Wang},
  doi          = {10.1016/j.imavis.2020.104008},
  journal      = {Image and Vision Computing},
  month        = {12},
  pages        = {104008},
  shortjournal = {Image Vis. Comput.},
  title        = {Vehicle re-identification based on unsupervised local area detection and view discrimination},
  volume       = {104},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Iris and periocular biometrics for head mounted displays:
Segmentation, recognition, and synthetic data generation. <em>ICV</em>,
<em>104</em>, 104007. (<a
href="https://doi.org/10.1016/j.imavis.2020.104007">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Augmented and virtual reality deployment is finding increasing use in novel applications. Some of these emerging and foreseen applications allow the users to access sensitive information and functionalities. Head Mounted Displays (HMD) are used to enable such applications and they typically include eye facing cameras to facilitate advanced user interaction. Such integrated cameras capture iris and partial periocular region during the interaction. This work investigates the possibility of using the captured ocular images from integrated cameras from HMD devices for biometric verification, taking into account the expected limited computational power of such devices. Such an approach can allow user to be verified in a manner that does not require any special and explicit user action. In addition to our comprehensive analyses, we present a light weight, yet accurate, segmentation solution for the ocular region captured from HMD devices. Further, we benchmark a number of well-established iris and periocular verification methods along with an in-depth analysis on the impact of iris sample selection and its effect on iris recognition performance for HMD devices. To the end, we also propose and validate an identity-preserving synthetic ocular image generation mechanism that can be used for large scale data generation for training purposes or attack generation purposes. We establish the realistic image quality of generated images with high fidelity and identity preserving capabilities through benchmarking them for iris and periocular verification.},
  archive      = {J_ICV},
  author       = {Fadi Boutros and Naser Damer and Kiran Raja and Raghavendra Ramachandra and Florian Kirchbuchner and Arjan Kuijper},
  doi          = {10.1016/j.imavis.2020.104007},
  journal      = {Image and Vision Computing},
  month        = {12},
  pages        = {104007},
  shortjournal = {Image Vis. Comput.},
  title        = {Iris and periocular biometrics for head mounted displays: Segmentation, recognition, and synthetic data generation},
  volume       = {104},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). GIFSL - grafting based improved few-shot learning.
<em>ICV</em>, <em>104</em>, 104006. (<a
href="https://doi.org/10.1016/j.imavis.2020.104006">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A few-shot learning model generally consists of a feature extraction network and a classification module. In this paper, we propose an approach to improve few-shot image classification performance by increasing the representational capacity of the feature extraction network and improving the quality of the features extracted by it. The ability of the feature extraction network to extract highly discriminative features from images is essential to few-shot learning. Such features are generally class agnostic and contain information about the general content of the image. Our approach improves the training of the feature extraction network in order to enable them to produce such features. We train the network using filter-grafting along with an auxiliary self-supervision task and a knowledge distillation procedure. Particularly, filter-grafting rejuvenates unimportant (invalid) filters in the feature extraction network to make them useful and thereby, increases the number of important filters that can be further improved by using self-supervision and knowledge distillation techniques. This combined approach helps in significantly improving the few-shot learning performance of the model. We perform experiments on several few-shot learning benchmark datasets such as mini-ImageNet, tiered-ImageNet, CIFAR-FS, and FC100 using our approach. We also present various ablation studies to validate the proposed approach. We empirically show that our approach performs better than other state-of-the-art few-shot learning methods.},
  archive      = {J_ICV},
  author       = {Pratik Mazumder and Pravendra Singh and Vinay P. Namboodiri},
  doi          = {10.1016/j.imavis.2020.104006},
  journal      = {Image and Vision Computing},
  month        = {12},
  pages        = {104006},
  shortjournal = {Image Vis. Comput.},
  title        = {GIFSL - grafting based improved few-shot learning},
  volume       = {104},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Spectral regularization for combating mode collapse in GANs.
<em>ICV</em>, <em>104</em>, 104005. (<a
href="https://doi.org/10.1016/j.imavis.2020.104005">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Generative adversarial networks (GANs) have been enjoying considerable success in recent years. However, mode collapse remains a major unsolved problem in training GANs and is one of the main obstacles hindering progress. In this paper, we present spectral regularization for GANs (SR-GANs), a new and robust method for combating the mode collapse problem in GANs. We first perform theoretical analysis to show that the spectral distributions of the weight matrix in the discriminator affect how the equality of Lipschitz constraint can be fulfilled, thus will have an impact on the performance of the discriminator . Prompted by these analysis, we set out to monitor the spectral distributions in the discriminators of spectral normalized GANs (SN-GANs), and discover a phenomenon which we refer to as spectral collapse, where a large number of singular values of the weight matrices drop dramatically when mode collapse occurs. We show that there are strong evidences linking mode collapse to spectral collapse; and based on this link, we set out to tackle spectral collapse as a surrogate of mode collapse. We have developed a spectral regularization method where we introduce two schemes, one static and one dynamic, to compensate the spectral distributions of the weight matrices to prevent them from collapsing, which in turn successfully prevents mode collapse in GANs. Through gradient analysis, we provide theoretical explanations for why SR-GANs are more stable and can provide better performances than SN-GANs. We also present extensive experimental results and analysis to show that SR-GANs not only always outperform SN-GANs but also always succeed in combating mode collapse where SN-GANs fail. The code is available at https://github.com/max-liu-112/SRGANs-Spectral-Regularization-GANs-},
  archive      = {J_ICV},
  author       = {Kanglin Liu and Guoping Qiu and Wenming Tang and Fei Zhou},
  doi          = {10.1016/j.imavis.2020.104005},
  journal      = {Image and Vision Computing},
  month        = {12},
  pages        = {104005},
  shortjournal = {Image Vis. Comput.},
  title        = {Spectral regularization for combating mode collapse in GANs},
  volume       = {104},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Robust biometric authentication system with a secure user
template. <em>ICV</em>, <em>104</em>, 104004. (<a
href="https://doi.org/10.1016/j.imavis.2020.104004">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Biometric feature based human authentication systems provide various advantages over the classical authentication systems . Out of numerous biometric features in a human body, the fingerprint is the most commonly used biometric feature for the authentication of a person. Typically, the fingerprint-based systems rely on minutiae points information and use it directly as a user template. Several studies have shown that the use of minutiae points directly as a user template is unreliable as it may lead to the reconstruction of the original fingerprint. As databases are vulnerable to attacks, it is important to secure fingerprint information. In this paper, we propose a technique in which a strong 3-dimensional template is created from the fingerprint of a user. From the minutiae points of the fingerprint, minutiae triplets are computed, through which a secured user template is generated. The user template developed using the proposed technique is found to be robust and shows good revocability, diversity, security, and performance. The experimental results obtained on various fingerprint databases are extremely encouraging and demonstrate the effectiveness of the proposed technique.},
  archive      = {J_ICV},
  author       = {Syed Sadaf Ali and Vivek Singh Baghel and Iyyakutti Iyappan Ganapathi and Surya Prakash},
  doi          = {10.1016/j.imavis.2020.104004},
  journal      = {Image and Vision Computing},
  month        = {12},
  pages        = {104004},
  shortjournal = {Image Vis. Comput.},
  title        = {Robust biometric authentication system with a secure user template},
  volume       = {104},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). CrossATNet - a novel cross-attention based framework for
sketch-based image retrieval. <em>ICV</em>, <em>104</em>, 104003. (<a
href="https://doi.org/10.1016/j.imavis.2020.104003">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a novel framework for cross-modal zero-shot learning (ZSL) in the context of sketch-based image retrieval (SBIR). Conventionally, the SBIR schema mainly considers simultaneous mappings among the two image views and the semantic side information . Therefore, it is desirable to consider fine-grained classes mainly in the sketch domain using highly discriminative and semantically rich feature space. However, the existing deep generative modeling based SBIR approaches majorly focus on bridging the gaps between the seen and unseen classes by generating pseudo-unseen-class samples. Besides, violating the ZSL protocol by not utilizing any unseen-class information during training, such techniques do not pay explicit attention to modeling the discriminative nature of the shared space. Also, we note that learning a unified feature space for both the multi-view visual data is a tedious task considering the significant domain difference between sketches and the color images. In this respect, as a remedy, we introduce a novel framework for zero-shot SBIR. While we define a cross-modal triplet loss to ensure the discriminative nature of the shared space, an innovative cross-modal attention learning strategy is also proposed to guide feature extraction from the image domain exploiting information from the respective sketch counterpart. In order to preserve the semantic consistency of the shared space, we consider a graph CNN based module which propagates the semantic class topology to the shared space. To ensure an improved response time during inference, we further explore the possibility of representing the shared space in terms of hash-codes. Experimental results obtained on the benchmark TU-Berlin and the Sketchy datasets confirm the superiority of CrossATNet in yielding the state-of-the-art results.},
  archive      = {J_ICV},
  author       = {Ushasi Chaudhuri and Biplab Banerjee and Avik Bhattacharya and Mihai Datcu},
  doi          = {10.1016/j.imavis.2020.104003},
  journal      = {Image and Vision Computing},
  month        = {12},
  pages        = {104003},
  shortjournal = {Image Vis. Comput.},
  title        = {CrossATNet - a novel cross-attention based framework for sketch-based image retrieval},
  volume       = {104},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Multi-semantic long-range dependencies capturing for
efficient video representation learning. <em>ICV</em>, <em>104</em>,
103988. (<a href="https://doi.org/10.1016/j.imavis.2020.103988">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Capturing long-range dependencies has proven effective on video understanding tasks. However, previous works address this problem in a pixel pairs manner, which might be inaccurate since pixel pairs contain too limited semantic information. Besides, considerable computations and parameters will be introduced in those methods. Following the pattern of features aggregation in Graph Convolutional Networks (GCNs), we aggregate pixels with their neighbors into semantic units, which contain stronger semantic information than pixel pairs. We designed an efficient, parameter-free, semantic units-based dependencies capturing framework, named as Multi-semantic Long-range Dependencies Capturing (MLDC) block. We verified our methods on large-scale challenging video classification benchmark, such as Kinetics. Experiments demonstrate that our method highly outperforms pixel pairs-based methods and achieves the state-of-the-art performance, without introducing any parameters and much computations.},
  archive      = {J_ICV},
  author       = {Jinhao Duan and Hua Xu and Xiaozhu Lin and Shangchao Zhu and Yuanze Du},
  doi          = {10.1016/j.imavis.2020.103988},
  journal      = {Image and Vision Computing},
  month        = {12},
  pages        = {103988},
  shortjournal = {Image Vis. Comput.},
  title        = {Multi-semantic long-range dependencies capturing for efficient video representation learning},
  volume       = {104},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Facial expression recognition using human machine
interaction and multi-modal visualization analysis for healthcare
applications. <em>ICV</em>, <em>103</em>, 104044. (<a
href="https://doi.org/10.1016/j.imavis.2020.104044">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The application of computer vision (CV) in healthcare applications is familiar with the wireless and communication technology. CV methods are incorporated in the healthcare for providing programmed interactions towards patient monitoring. The requirements of systems are the analysis and detection of the images&#39; visualization of patients. In this paper, a multi-modal visualization analysis (MMVA) method is introduced for improving the less-complex processing nature of programmed human-machine interactions (HMI) in health monitoring. In particular, the proposed method is designed to identify facial expressions of a patient using facial expression and textures of the input visualization. The proposed method relies on three layers of convolution neural network (CNN) for texture classification , correlation, and detection of facial visualization using stored information. The processes of the three-layers are chained to reduce the complexity and misdetection in the analysis. The feature-based tuning chain in the first layer of CNN attains to minimize the impact of facial and textural variants resulting in misdetection. The second layer is a correlation to attain the accurate matching of expression from the captured image. The third layer is facial visualization to find the quick decision and used to store the error data as the training set. The experimental results show that proposed method achieves 95.702% of recognition accuracy compared to other conventional methods. The analysis time and misdetection are minimized. Also, the recognition ratio is improved.},
  archive      = {J_ICV},
  author       = {Torki Altameem and Ayman Altameem},
  doi          = {10.1016/j.imavis.2020.104044},
  journal      = {Image and Vision Computing},
  month        = {11},
  pages        = {104044},
  shortjournal = {Image Vis. Comput.},
  title        = {Facial expression recognition using human machine interaction and multi-modal visualization analysis for healthcare applications},
  volume       = {103},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A calibration method of computer vision system based on dual
attention mechanism. <em>ICV</em>, <em>103</em>, 104039. (<a
href="https://doi.org/10.1016/j.imavis.2020.104039">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nowadays, the technology of using computer vision to calibrate objects is widely used, which has a huge market demand in many fields. This paper provides a calibration method of computer vision system based on dual attention neural network . This paper uses the camera to simulate human eyes to obtain three-dimensional images. After obtaining the three-dimensional images, the images are input into the Residual Network (ResNet) model, and the weight of ResNet is repeatedly updated so as to accurately identify the images. On this basis, introduces dual attention mechanism that an algorithm is used in natural language to the visual image processing, using multistage feature extraction method to extract the three-dimensional image for each characteristic of regional. After extracting the feature area, the accuracy of the feature area is constantly updated to the minimum. Besides, the feature areas are brought into the calibration algorithm of Zhang Zhengyou system to obtain the spatial coordinates of the objects in the attention area. This method can realize the space position calibration of specific objects under various complex backgrounds and calculate the distance from the calibrated objects, which can not only calibrate the system but also identify it, and greatly improve the reliability and accuracy of the calibration process.},
  archive      = {J_ICV},
  author       = {Youling Li},
  doi          = {10.1016/j.imavis.2020.104039},
  journal      = {Image and Vision Computing},
  month        = {11},
  pages        = {104039},
  shortjournal = {Image Vis. Comput.},
  title        = {A calibration method of computer vision system based on dual attention mechanism},
  volume       = {103},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). R4 det: Refined single-stage detector with feature recursion
and refinement for rotating object detection in aerial images.
<em>ICV</em>, <em>103</em>, 104036. (<a
href="https://doi.org/10.1016/j.imavis.2020.104036">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The detection of objects with multi-orientations and multi-scales in aerial images is receiving increasing attention because of numerous useful applications in computer vision, image understanding, satellite remote sensing and surveillance. However, such detection can be exceedingly challenging because of a birds eye view, multi-scale rotating objects with large aspect ratios, dense distributions and extremely imbalanced categories. Despite the considerable progress that has been made, detection performance falls considerably below that required for real-world applications. In this paper, we propose an accurate and fast end-to-end detector to address the aforementioned challenges. Our contributions are threefold. First, inspired by the looking and thinking twice mechanism, recursive neural networks and the DetectoRS detector, we propose a novel encoder-decoder based architecture by introducing the recursive feature pyramid into a single-stage object detection framework. The improved backbone network can generate increasingly powerful multi-scale representations for classification and regression. Second, we propose a refined single-stage detector with feature recursion and refinement for rotating objects. Third, we use instance balance to improve focal loss, thereby optimizing the loss in the correct direction. Extensive experiments on two challenging aerial image object detection public datasets, DOTA and HRSC2016, show that the proposed R 4 Det detector achieves the state-of-the-art accuracy while running very fast. Moreover, further experiments show that our detector is more robust to adversarial image patch attacks than the previous state-of-art detector.},
  archive      = {J_ICV},
  author       = {Peng Sun and Yongbin Zheng and Zongtan Zhou and Wanying Xu and Qiang Ren},
  doi          = {10.1016/j.imavis.2020.104036},
  journal      = {Image and Vision Computing},
  month        = {11},
  pages        = {104036},
  shortjournal = {Image Vis. Comput.},
  title        = {R4 det: Refined single-stage detector with feature recursion and refinement for rotating object detection in aerial images},
  volume       = {103},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Building NAS: Automatic designation of efficient neural
architectures for building extraction in high-resolution aerial images.
<em>ICV</em>, <em>103</em>, 104025. (<a
href="https://doi.org/10.1016/j.imavis.2020.104025">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Building extraction, which is a fundamental task in the community of remote sensing image analysis, has been extensively applied in various applications related to smart cities. Due to the complicated background information in urban areas, how to extract building footprints from high-resolution aerial images is challenging. The recent achievements of deep learning have shed light on building extraction and other remote sensing domain tasks. However, the heavy consumption of computational resources and the design of the neural architectures became the biggest bottleneck of utilizing deep learning techniques to improve the performance. In this work, we developed a Neural Architecture Search (NAS) algorithm, dubbed BuildingNAS, for building extraction from high-resolution aerial images. In particular, we built an efficient candidate operation set upon Separable Factorized Residual Blocks as our cell-level search space . Different from previous NAS in semantic segmentation tasks , we employed the hierarchical search space and proposed the Single-Path Sampling strategy to eliminate excessive GPU memory comsumption in searching process . In addition, we proposed an entropy regularized objective for the optimization of architecture parameters. As the result, the larger batch size can be adopted in the whole pipeline to accelerate the searching process, and make the resulted architecture more stable and accurate. We evaluated our proposed algorithm in WHUBuilding Dataset, the derived network achieved mIoU of 86.95% with only 2.05G FLOPs and 3.10 M parameters. The comparison results demonstrate that the network discovered by our algorithm can achieve great efficiency-accuracy trade-off.},
  archive      = {J_ICV},
  author       = {Weipeng Jing and Jingbo Lin and Huihui Wang},
  doi          = {10.1016/j.imavis.2020.104025},
  journal      = {Image and Vision Computing},
  month        = {11},
  pages        = {104025},
  shortjournal = {Image Vis. Comput.},
  title        = {Building NAS: Automatic designation of efficient neural architectures for building extraction in high-resolution aerial images},
  volume       = {103},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Fusion of iris and sclera using phase intensive rubbersheet
mutual exclusion for periocular recognition. <em>ICV</em>, <em>103</em>,
104024. (<a href="https://doi.org/10.1016/j.imavis.2020.104024">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In biometrics , periocular recognition analysis is an essential constituent for identifying the human being. Among prevailing the modalities, ocular biometric traits such as iris, sclera and periocular eye movement have experienced noteworthy consciousness in the recent past. In this paper, we are presenting new multi-biometric fusion method called Phase Intensive Mutual Exclusive Distribution (PI-MED) method by combining periocular features (i.e. iris and sclera) for identity verification . The main objective of the proposed PI-MED method is to reduce the matching fusion time and overhead during human recognition in biometrics. Initially, iris modality and sclera modality is pre-processed using Phase Intensive Rubber Sheeting Local Pattern Extraction to generate the vector of score. After that, the extracted iris and sclera features are given to the Mutual Exclusive Bayesian fusion model. The fusion model is applied at the score level for reducing fusion overhead. In this model, feature fusion is generated based on the log likelihood ratio by using covariance matrix measurement . Finally with fusion features, Distributed Hamming Distance Template Matching (DHDTM) algorithm is designed to evaluate the recognition rate of test data with available training data . The results show that the DHDTM significantly improves the recognition rate of human biometric samples when compared to the conventional person identification methods. Several tests were conducted to evaluate the performance of the proposed methods of standard biometric databases using three metrics, namely, matching fusion time, overhead and true positive rate. From the experimental results, the proposed PI-MED method reduces the matching fusion time and overhead by 47% and 45% when compared to existing methods. Similarly, the proposed PI-MED method increases the true positive rate by 33% when compared to existing methods.},
  archive      = {J_ICV},
  author       = {Deepak Kumar Jain and Xiangyuan Lan and Ramachandran Manikandan},
  doi          = {10.1016/j.imavis.2020.104024},
  journal      = {Image and Vision Computing},
  month        = {11},
  pages        = {104024},
  shortjournal = {Image Vis. Comput.},
  title        = {Fusion of iris and sclera using phase intensive rubbersheet mutual exclusion for periocular recognition},
  volume       = {103},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Intelligent querying for target tracking in camera networks
using deep q-learning with n-step bootstrapping. <em>ICV</em>,
<em>103</em>, 104022. (<a
href="https://doi.org/10.1016/j.imavis.2020.104022">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Surveillance camera networks are a useful infrastructure for various visual analytics applications, where high-level inferences and predictions could be made based on target tracking across the network. Most multi-camera tracking works focus on target re-identification and trajectory association problems to track the target. However, since camera networks can generate enormous amount of video data, inefficient schemes for making re-identification or trajectory association queries can incur prohibitively large computational requirements. In this paper, we address the problem of intelligent scheduling of re-identification queries in a multi-camera tracking setting. To this end, we formulate the target tracking problem in a camera network as an MDP and learn a reinforcement learning based policy that selects a camera for making a re-identification query. The proposed approach to camera selection does not assume the knowledge of the camera network topology but the resulting policy implicitly learns it. We have also shown that such a policy can be learnt directly from data. Using the NLPR MCT and the Duke MTMC multi-camera multi-target tracking benchmarks, we empirically show that the proposed approach substantially reduces the number of frames queried.},
  archive      = {J_ICV},
  author       = {Anil Sharma and Saket Anand and Sanjit K. Kaul},
  doi          = {10.1016/j.imavis.2020.104022},
  journal      = {Image and Vision Computing},
  month        = {11},
  pages        = {104022},
  shortjournal = {Image Vis. Comput.},
  title        = {Intelligent querying for target tracking in camera networks using deep Q-learning with n-step bootstrapping},
  volume       = {103},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). New insights on multi-solution distribution of the P3P
problem. <em>ICV</em>, <em>103</em>, 104009. (<a
href="https://doi.org/10.1016/j.imavis.2020.104009">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traditionally, the P3P problem is solved by firstly transforming its 3 quadratic equations into a quartic one, then by locating the roots of the resulting quartic equation and verifying whether a root does really correspond to a true solution of the P3P problem itself. It is well known that a root of the quartic equation could correspond to 2, or 1, or even null solution at all to the P3P problem, and up to now, no explicit relationship between the P3P solution and the root of its quartic equation is available in the literature. In this work, we show that when the optical center is outside of all the 6 toroids defined by the control point triangle, each positive root of the Grunert&#39;s quartic equation must correspond to a true solution of the P3P problem, and the corresponding P3P problem cannot have a unique solution , it must have either 2 positive solutions or 4 positive solutions. In addition, we show that when the optical center passes through any one of the 3 toroids among these 6 toroids (except possibly for two concentric circles), the number of the solutions of the corresponding P3P problem always changes by 1, either increased by 1 or decreased by 1. Furthermore we show that such changed solutions always locate in a small neighborhood of control points, hence the 3 toroids are critical surfaces of the P3P problem and the 3 control points are 3 singular points of solutions. A notable example is that when the optical center passes through the outer surface of the union of the 6 toroids from the outside to inside, the number of the solutions must always decrease by 1. Our results are the first in the literature to give an explicit and geometrically intuitive relationship between the P3P solutions and the roots of its quartic equation, aside its academic values, it could also act as some theoretical guidance for P3P practitioners to properly arrange their control points to avoid undesirable solutions.},
  archive      = {J_ICV},
  author       = {Bo Wang and Hao Hu and Caixia Zhang},
  doi          = {10.1016/j.imavis.2020.104009},
  journal      = {Image and Vision Computing},
  month        = {11},
  pages        = {104009},
  shortjournal = {Image Vis. Comput.},
  title        = {New insights on multi-solution distribution of the P3P problem},
  volume       = {103},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Multi-level prediction siamese network for real-time UAV
visual tracking. <em>ICV</em>, <em>103</em>, 104002. (<a
href="https://doi.org/10.1016/j.imavis.2020.104002">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing deployed Unmanned Aerial Vehicles (UAVs) visual trackers are usually based on the correlation filter framework. Although these methods have certain advantages of low computational complexity , the tracking performance of small targets and fast motion scenarios is not satisfactory. In this paper, we present a novel multi-level prediction Siamese network (MLPS) for object tracking in UAV videos, which consists of Siamese feature extraction module and multi-level prediction module. The multi-level prediction module can make full use of the characteristics of each layer features to achieve robust evaluation of targets with different scales. Meanwhile, for small-size target tracking , we design a residual feature fusion block , which is used to constrain the low-level feature representation by using high-level abstract semantics, and obtain the improvement of the tracker&#39;s ability to distinguish scene details. In addition, we propose a layer attention fusion block which is sensitive to the informative features of each layers to achieve adaptive fusion of different levels of correlation responses by dynamically balancing the multi-layer features. Sufficient experiments on several UAV tracking benchmarks demonstrate that MLPS achieves state-of-the-art performance and runs at a speed over 97 FPS.},
  archive      = {J_ICV},
  author       = {Mu Zhu and Hui Zhang and Jing Zhang and Li Zhuo},
  doi          = {10.1016/j.imavis.2020.104002},
  journal      = {Image and Vision Computing},
  month        = {11},
  pages        = {104002},
  shortjournal = {Image Vis. Comput.},
  title        = {Multi-level prediction siamese network for real-time UAV visual tracking},
  volume       = {103},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Depth-guided saliency detection via boundary information.
<em>ICV</em>, <em>103</em>, 104001. (<a
href="https://doi.org/10.1016/j.imavis.2020.104001">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing efforts of saliency detection have achieved excellent performance in RGB images , thus to sufficiently exploit existing RGB saliency models and further do some extensions on them, we can transfer existing RGB saliency models to the similar research field, i.e. RGBD saliency detection, by introducing depth cues. Here, we construct a novel RGBD saliency model upon an existing RGB saliency model. To be specific, firstly, our model deploys a depth-guided module to guide the deep features extraction, where the multi-level deep depth features obtained from depth branch are embedded into the backbone network and integrate with multi-level deep RGB features. Secondly, to further promote the performance of our model, we devise a boundary constraint module to elevate the detection accuracy, where the boundary information is compounded by the low-level deep RGB and depth features. Comprehensive experiments are performed on five public RGBD saliency detection datasets, and the experimental results clearly demonstrate the effectiveness and superiority of our model when compared with the state-of-the-art RGBD saliency models.},
  archive      = {J_ICV},
  author       = {Xiaofei Zhou and Hongfa Wen and Ran Shi and Haibing Yin and Chenggang Yan},
  doi          = {10.1016/j.imavis.2020.104001},
  journal      = {Image and Vision Computing},
  month        = {11},
  pages        = {104001},
  shortjournal = {Image Vis. Comput.},
  title        = {Depth-guided saliency detection via boundary information},
  volume       = {103},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Joint patch and instance discrimination learning for
unsupervised person re-identification. <em>ICV</em>, <em>103</em>,
104000. (<a href="https://doi.org/10.1016/j.imavis.2020.104000">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The unsupervised person re-identification (re-ID) has become increasingly significant in the community because it is more scalable than the supervised method when dealing with the large-scale person re-ID. However, it is difficult to learn discriminative enough features from across-camera images without labelling information. To address this problem, we propose a joint patch and instance discrimination learning (JPIL) framework for the unsupervised person re-ID. The JPIL framework exploits a patch feature extraction model to generate patch-wise features for each input image. Then the patch discrimination learning (PDL) loss is designed to guide the model to mine the patch-wise discriminative information from unlabelled person image patches. On the other hand, we introduce the instance discrimination learning (IDL) loss to provide instance-wise supervision. The IDL loss aims to pull features of the same instance under different transformations closer and push features belonging to different instances away. Finally, we combine the PDL and IDL loss to apply the joint training. Extensive experiments on Market-1501 and DukeMTMC-reID datasets demonstrate the effectiveness of the proposed method for unsupervised person re-ID.},
  archive      = {J_ICV},
  author       = {Yu Zhao and Qiaoyuan Shu and Keren Fu and Pengcheng Wei and Jian Zhan},
  doi          = {10.1016/j.imavis.2020.104000},
  journal      = {Image and Vision Computing},
  month        = {11},
  pages        = {104000},
  shortjournal = {Image Vis. Comput.},
  title        = {Joint patch and instance discrimination learning for unsupervised person re-identification},
  volume       = {103},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). PCANet: Pyramid convolutional attention network for semantic
segmentation. <em>ICV</em>, <em>103</em>, 103997. (<a
href="https://doi.org/10.1016/j.imavis.2020.103997">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Pyramid Convolutional Attention Network is proposed to efficiently capture long-range dependency and fuse features from different levels for benefitting semantic segmentation problems. In this paper, we focus on how to extract more representative features for segmentation object recognition and design a decoder to recover details in a more efficient way. Inspired by atrous sampling and attention mechanism, we propose Pyramid Atrous Attention module to capture long-range dependency for learning richer contextual features. We also find that features of different levels have diverse representation so we design Convolutional Attention Refinement module to provide global context for low-level features and local details for high-level features. By combining with these two efficient module, we construct our Pyramid Convolutional Attention Network (PCANet), which achieves state-of-the-art results on Pascal VOC 2012 and Cityscapes benchmark.},
  archive      = {J_ICV},
  author       = {Haiwei Sang and Qiuhao Zhou and Yong Zhao},
  doi          = {10.1016/j.imavis.2020.103997},
  journal      = {Image and Vision Computing},
  month        = {11},
  pages        = {103997},
  shortjournal = {Image Vis. Comput.},
  title        = {PCANet: Pyramid convolutional attention network for semantic segmentation},
  volume       = {103},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Generalizable deep features for ocular biometrics.
<em>ICV</em>, <em>103</em>, 103996. (<a
href="https://doi.org/10.1016/j.imavis.2020.103996">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {There has been a continued interest in learning features that are generalizable across sensors and spectra for ocular biometrics . This is usually facilitated through a model that can learn features that are robust across pose, lighting conditions , spectra, and device sensor variations. In this paper, we propose an efficient deep learning-based feature extraction pipeline for learning the aforementioned generalizable features for ocular recognition. The proposed pipeline uses a relatively small Convolutional Neural Network (CNN) based feature extraction model along with a region of interest (ROI) detector and data augmenter. Our proposed CNN model has 36 times fewer parameters compared to the popular ResNet-50. Cross dataset experiments on five benchmark datasets suggest that the proposed feature extraction model, trained only on 200 subjects from the VISOB dataset, reduces the error rate up to 7 × when compared to the existing models.},
  archive      = {J_ICV},
  author       = {Narsi Reddy and Ajita Rattani and Reza Derakhshani},
  doi          = {10.1016/j.imavis.2020.103996},
  journal      = {Image and Vision Computing},
  month        = {11},
  pages        = {103996},
  shortjournal = {Image Vis. Comput.},
  title        = {Generalizable deep features for ocular biometrics},
  volume       = {103},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Intelligent detection of building cracks based on deep
learning. <em>ICV</em>, <em>103</em>, 103987. (<a
href="https://doi.org/10.1016/j.imavis.2020.103987">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In order to solve the damage caused by the concrete structure, which leads to the reduction of the life of infrastructure, endangers the safety of pedestrians, and has a serious impact on the social economy, building crack detection model of FCN (Fully Convolutional Networks), R-CNN (Regions with CNN feature) and RFCN (Richer Fully Convolutional Networks) has been proposed based on the convolutional neural network model to amplify and extract the features of the data and previous studies. Through the training of building surface data such as roads, bridges, houses and dams, the model is analyzed in terms of morphological and geometric indexes. Finally, the model of crack picture detection and segmentation based on deep learning is used for picture performance detection and comprehensive evaluation. The results show that: in the aspect of building gap detection, the RFCN model has the best processing effect, the gap recognition degree is higher, and the detail processing is better. In the aspect of model evaluation index, the correct rate of RFCN model is increased by 10%, the accuracy rate is increased by 12%, the recall rate is increased by 8%, the loss rate is increased by 3%, and the overall stability is higher. In the aspect of comprehensive performance, the picture processing performance is better than the FCN model by 7% and better than the R-CNN model by 15%, and the memory share is 80%. The fusion model based on deep learning and picture processing has been improved in many aspects, which can provide strong theoretical support and practical value for the detection and research of concrete surface cracks such as bridges, dams, highways and houses.},
  archive      = {J_ICV},
  author       = {Minjuan Zheng and Zhijun Lei and Kun Zhang},
  doi          = {10.1016/j.imavis.2020.103987},
  journal      = {Image and Vision Computing},
  month        = {11},
  pages        = {103987},
  shortjournal = {Image Vis. Comput.},
  title        = {Intelligent detection of building cracks based on deep learning},
  volume       = {103},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Detection of panoramic vision pedestrian based on deep
learning. <em>ICV</em>, <em>103</em>, 103986. (<a
href="https://doi.org/10.1016/j.imavis.2020.103986">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As the AI (artificial intelligence) develops, driverless vehicle technology is widely concerned, and the important problem that needs to be solved in driverless technology is the detection of pedestrians in the panoramic vision of the vehicle, so the pedestrian detection technology of panoramic vision is explored based on the deep learning . Recently, anchor-free and one-stage detectors have been introduced into this area. However, their accuracies are unsatisfactory. Therefore, in order to enjoy the simplicity of anchor-free detectors and the accuracy of two-stage ones simultaneously, some adaptations based on a detector, CSP (center and scale prediction) are proposed. The original CSP of pedestrian detector is improved to make the training process more robust. For example, we use SN layers to replace all BN layers and ResNet-101 is used as backbone based on the research of deep learning . The main contributions of our paper are: (1) we improve the robustness of CSP and make it easier to train. (2) we propose a novel method to predict width, namely compressing width. (3) we achieve the second best performance on CityPersons benchmark, i.e. 9.3% log-average miss rate (MR) on reasonable set, 8.7% MR on partial set and 5.6% MR on bare set, which shows an anchor-free and one-stage detector can still have high accuracy. (4) we explore some capabilities of switchable normalization which are not mentioned in its original paper. This study will provide important theoretical support and practical basis for pedestrian detection},
  archive      = {J_ICV},
  author       = {Wenhao Wang},
  doi          = {10.1016/j.imavis.2020.103986},
  journal      = {Image and Vision Computing},
  month        = {11},
  pages        = {103986},
  shortjournal = {Image Vis. Comput.},
  title        = {Detection of panoramic vision pedestrian based on deep learning},
  volume       = {103},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). From known to the unknown: Transferring knowledge to answer
questions about novel visual and semantic concepts. <em>ICV</em>,
<em>103</em>, 103985. (<a
href="https://doi.org/10.1016/j.imavis.2020.103985">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Current Visual Question Answering (VQA) systems can answer intelligent questions about ‘ known ’ visual content. However, their performance drops significantly when questions about visually and linguistically ‘ unknown ’ concepts are presented during inference (‘Open-world’ scenario). A practical VQA system should be able to deal with novel concepts in real world settings. To address this problem, we propose an exemplar-based approach that transfers learning ( i.e. , knowledge) from previously ‘ known ’ concepts to answer questions about the ‘ unknown ’. We learn a highly discriminative joint embedding (JE) space, where visual and semantic features are fused to give a unified representation. Once novel concepts are presented to the model, it looks for the closest match from an exemplar set in the JE space. This auxiliary information is used alongside the given Image-Question pair to refine visual attention in a hierarchical fashion. Our novel attention model is based on a dual-attention mechanism that combines the complementary effect of spatial and channel attention. Since handling the high dimensional exemplars on large datasets can be a significant challenge, we introduce an efficient matching scheme that uses a compact feature description for search and retrieval. To evaluate our model, we propose a new dataset for VQA, separating unknown visual and semantic concepts from the training set. Our approach shows significant improvements over state-of-the-art VQA models on the proposed Open-World VQA dataset and other standard VQA datasets.},
  archive      = {J_ICV},
  author       = {Moshiur R. Farazi and Salman H. Khan and Nick Barnes},
  doi          = {10.1016/j.imavis.2020.103985},
  journal      = {Image and Vision Computing},
  month        = {11},
  pages        = {103985},
  shortjournal = {Image Vis. Comput.},
  title        = {From known to the unknown: Transferring knowledge to answer questions about novel visual and semantic concepts},
  volume       = {103},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Geometry consistency aware confidence evaluation for feature
matching. <em>ICV</em>, <em>103</em>, 103984. (<a
href="https://doi.org/10.1016/j.imavis.2020.103984">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most existing approaches prune wrong matches via estimating an image transformation or solving a graph-based global matching optimization problem , which usually suffers from varying local transformations and outliers. Inspired by the insight that neighboring true matches usually hold consistent local topological structures across images, in this paper we propose a new approach to evaluate the confidence of each putative match based on how well its two keypoints can predict each other by exploring the geometric constraint with its neighboring matches. With the evaluation, a two-stage approach combining recursively false match pruning and correct match incrementing is presented to obtain the reliable matches. Experiments on various image pairs demonstrate that our approach can conduct robust feature matching in challenging conditions and outperform state-of-the-art approaches.},
  archive      = {J_ICV},
  author       = {Linbo Wang and Binbin Chen and Peng Xu and Honglong Ren and Xianyong Fang and Shaohua Wan},
  doi          = {10.1016/j.imavis.2020.103984},
  journal      = {Image and Vision Computing},
  month        = {11},
  pages        = {103984},
  shortjournal = {Image Vis. Comput.},
  title        = {Geometry consistency aware confidence evaluation for feature matching},
  volume       = {103},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). The effect of image recognition traffic prediction method
under deep learning and naive bayes algorithm on freeway traffic safety.
<em>ICV</em>, <em>103</em>, 103971. (<a
href="https://doi.org/10.1016/j.imavis.2020.103971">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In order to study and predict the freeway traffic safety and realize the traffic flow in the nonlinear big data environment, based on deep learning , the long-short-time memory (LSTM) model based on recurrent neural network is proposed. The traffic flow is predicted and the predicted value of traffic flow is compared with the actual value at different times. The mean absolute percentage error of LSTM prediction model is tested and compared with the error of time proximity, periodicity, and trend. At the same time, the naive Bayes algorithm is used to carry out image recognition processing for attributes such as license plate number and vehicle color to conduct vehicle matching. The data processing, training process, and model realization of the model are studied, and the accuracy of the naive Bayesian algorithm is tested. The results show that the predicted value of the traffic flow prediction model based on LSTM is not much different from the actual value. The average prediction error for the period from May 7, 2018 to May 9, 2018 is approximately 13.8%. When the time series is 6, the error of the prediction model based on LSTM is 10.72%, and the prediction errors of the three sequences of time proximity, periodicity, and trend are 15.66%, 17.59%, and 20.67%, respectively. Considering the three sequences comprehensively, the prediction model can achieve good prediction effect. The accuracy of the vehicle matching model based on naive Bayes is about 82.7%, which can meet the requirements of the system. Therefore, it can be concluded that the LSTM traffic flow prediction model based on deep learning and the image recognition vehicle matching model based on naive Bayes can realize the traffic safety prediction of freeway, which has great practical significance .},
  archive      = {J_ICV},
  author       = {Jingxuan Yao and Yuntao Ye},
  doi          = {10.1016/j.imavis.2020.103971},
  journal      = {Image and Vision Computing},
  month        = {11},
  pages        = {103971},
  shortjournal = {Image Vis. Comput.},
  title        = {The effect of image recognition traffic prediction method under deep learning and naive bayes algorithm on freeway traffic safety},
  volume       = {103},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Non-local attention association scheme for online
multi-object tracking. <em>ICV</em>, <em>102</em>, 103983. (<a
href="https://doi.org/10.1016/j.imavis.2020.103983">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Online multi-object tracking (MOT) is a fundamental problem in video analysis and multimedia applications . The major challenge in the popular tracking-by-detection framework is knowing how to associate candidate detections results with existing tracklets. In this, we propose a non-local attention association approach and apply it to a unified online MOT framework that integrates the merits of single object tracking and data association methods. Specifically, we use non-local attention association networks (NAAN) to incorporate both spatial and temporal characteristics to associate new detections. The non-local attention mechanism generates global attention maps across space and time, enabling the network to focus on the whole tracklet information, as opposed to the local attention mechanism to overcome the problems of noisy detections, occlusion, and frequent interactions between targets. Experimental results on MOT benchmark datasets show that the proposed algorithm performs favorably against various online trackers on the basis of identity-preserving metrics.},
  archive      = {J_ICV},
  author       = {Haidong Wang and Saizhou Wang and Jingyi Lv and Chenming Hu and Zhiyong Li},
  doi          = {10.1016/j.imavis.2020.103983},
  journal      = {Image and Vision Computing},
  month        = {10},
  pages        = {103983},
  shortjournal = {Image Vis. Comput.},
  title        = {Non-local attention association scheme for online multi-object tracking},
  volume       = {102},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). An attention-based deep learning model for multiple
pedestrian attributes recognition. <em>ICV</em>, <em>102</em>, 103981.
(<a href="https://doi.org/10.1016/j.imavis.2020.103981">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The automatic characterization of pedestrians in surveillance footage is a tough challenge, particularly when the data is extremely diverse with cluttered backgrounds, and subjects are captured from varying distances, under multiple poses, with partial occlusion . Having observed that the state-of-the-art performance is still unsatisfactory, this paper provides a novel solution to the problem, with two-fold contributions: 1) considering the strong semantic correlation between the different full-body attributes, we propose a multi-task deep model that uses an element-wise multiplication layer to extract more comprehensive feature representations. In practice, this layer serves as a filter to remove irrelevant background features, and is particularly important to handle complex, cluttered data; and 2) we introduce a weighted-sum term to the loss function that not only relativizes the contribution of each task but also is crucial for performance improvement in multiple-attribute inference settings. Our experiments were performed on two well-known datasets (RAP and PETA) and point for the superiority of the proposed method with respect to the state-of-the-art. The code is available at https://github.com/Ehsan-Yaghoubi/MAN-PAR- .},
  archive      = {J_ICV},
  author       = {Ehsan Yaghoubi and Diana Borza and João Neves and Aruna Kumar and Hugo Proença},
  doi          = {10.1016/j.imavis.2020.103981},
  journal      = {Image and Vision Computing},
  month        = {10},
  pages        = {103981},
  shortjournal = {Image Vis. Comput.},
  title        = {An attention-based deep learning model for multiple pedestrian attributes recognition},
  volume       = {102},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Demographic classification through pupil analysis.
<em>ICV</em>, <em>102</em>, 103980. (<a
href="https://doi.org/10.1016/j.imavis.2020.103980">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {An area of biometrics that has recently attracted much attention is gender and age classification. Its applications can be found not only in the fields of security and surveillance, but also in the context of marketing and demographic information gathering. In addition, extracting this information from a biometric sample can help to decrease the time to identify the exact individual. In this paper, we exploit pupil size as a discriminating feature for the estimation of gender and age. Data obtained from the free observation of face images have been used to train two classifiers (Adaboost and SVM), considering both the best results produced by each classifier and their fusion through weighted means. With experiments involving more than 100 participants, we have found that pupil size can provide significant results, better than those achievable using data on fixations and gaze paths. Pupil Diameter Mean (PDM) has proved to be the best discriminating feature for both gender and age. To the best of our knowledge, there are no other studies trying to perform such a classification using pupil size only.},
  archive      = {J_ICV},
  author       = {Virginio Cantoni and Lucia Cascone and Michele Nappi and Marco Porta},
  doi          = {10.1016/j.imavis.2020.103980},
  journal      = {Image and Vision Computing},
  month        = {10},
  pages        = {103980},
  shortjournal = {Image Vis. Comput.},
  title        = {Demographic classification through pupil analysis},
  volume       = {102},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A two-stage real-time YOLOv2-based road marking detector
with lightweight spatial transformation-invariant classification.
<em>ICV</em>, <em>102</em>, 103978. (<a
href="https://doi.org/10.1016/j.imavis.2020.103978">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, Autonomous Driving Systems (ADS) become more and more popular and reliable. Road markings are important for drivers and advanced driver assistance systems by better understanding the road environment. While the detection of road markings may suffer a lot from various illuminations, weather conditions and angles of view, most traditional road marking detection methods use fixed threshold to detect road markings, which is not robust enough to handle various situations in the real world. To deal with this problem, some deep learning-based real-time detection frameworks such as Single Shot Detector (SSD) and You Only Look Once (YOLO) are suitable for this task. However, these deep learning-based methods are data-driven even while there is no public road marking dataset. Besides, these detection frameworks usually struggle with distorted road markings and balancing between the precision and recall. We propose a two-stage YOLOv2-based network to tackle distorted road marking detection as well as to balance precision and recall. The proposed spatial transformer layer is able to handle the distorted road markings in the second stage, so as to achieve the improvement of precision. Our network is able to run at 58 FPS in a single GTX 1070 under diverse circumstances. Furthermore, we present a dataset for the public use of road marking detection tasks, which consists of 11,800 high-resolution images captured under different weather conditions. Specifically, the images are manually annotated into 13 classes with bounding boxes . We empirically demonstrate both mean average precision (mAP) and detection speed of our system over several baseline models .},
  archive      = {J_ICV},
  author       = {Xing-Yu Ye and Dza-Shiang Hong and Hung-Hao Chen and Pei-Yung Hsiao and Li-Chen Fu},
  doi          = {10.1016/j.imavis.2020.103978},
  journal      = {Image and Vision Computing},
  month        = {10},
  pages        = {103978},
  shortjournal = {Image Vis. Comput.},
  title        = {A two-stage real-time YOLOv2-based road marking detector with lightweight spatial transformation-invariant classification},
  volume       = {102},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Multimodal facial biometrics recognition: Dual-stream
convolutional neural networks with multi-feature fusion layers.
<em>ICV</em>, <em>102</em>, 103977. (<a
href="https://doi.org/10.1016/j.imavis.2020.103977">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Facial recognition for surveillance applications still remains challenging in uncontrolled environments, especially with the appearances of masks/veils and different ethnicities effects. Multimodal facial biometrics recognition becomes one of the major studies to overcome such scenarios. However, to cooperate with multimodal facial biometrics, many existing deep learning networks rely on feature concatenation or weight combination to construct a representation layer to perform its desired recognition task. This concatenation is often inefficient, as it does not effectively cooperate with the multimodal data to improve on recognition performance. Therefore, this paper proposes using multi-feature fusion layers for multimodal facial biometrics, thereby leading to significant and informative data learning in dual-stream convolutional neural networks . Specifically, this network consists of two progressive parts with distinct fusion strategies to aggregate RGB data and texture descriptors for multimodal facial biometrics. We demonstrate that the proposed network offers a discriminative feature representation and benefits from the multi-feature fusion layers for an accuracy-performance gain. We also introduce and share a new dataset for multimodal facial biometric data , namely the Ethnic-facial dataset for benchmarking. In addition, four publicly accessible datasets, namely AR, FaceScrub, IMDB_WIKI, and YouTube Face datasets are used to evaluate the proposed network. Through our experimental analysis, the proposed network outperformed several competing networks on these datasets for both recognition and verification tasks.},
  archive      = {J_ICV},
  author       = {Leslie Ching Ow Tiong and Seong Tae Kim and Yong Man Ro},
  doi          = {10.1016/j.imavis.2020.103977},
  journal      = {Image and Vision Computing},
  month        = {10},
  pages        = {103977},
  shortjournal = {Image Vis. Comput.},
  title        = {Multimodal facial biometrics recognition: Dual-stream convolutional neural networks with multi-feature fusion layers},
  volume       = {102},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Implementing cascaded regression tree-based face
landmarking: An in-depth overview. <em>ICV</em>, <em>102</em>, 103976.
(<a href="https://doi.org/10.1016/j.imavis.2020.103976">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Face landmarking, defined as the detection of fiducial points on faces, has received a lot of attention over the last two decades within the computer vision community. While research literature documents major advances using state-of-art deep convolutional neural networks , earlier cascaded regression tree -based approaches remain a relevant alternative for low-cost, low-power embedded systems. Yet, from a practical point of view, their implementation and parametrization can be a difficult and tedious process. In this paper, we provide the readers with insights and advice on how to design a successful face landmarking system using a cascade of regression trees.},
  archive      = {J_ICV},
  author       = {Romuald Perrot and Pascal Bourdon and David Helbert},
  doi          = {10.1016/j.imavis.2020.103976},
  journal      = {Image and Vision Computing},
  month        = {10},
  pages        = {103976},
  shortjournal = {Image Vis. Comput.},
  title        = {Implementing cascaded regression tree-based face landmarking: An in-depth overview},
  volume       = {102},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Application of the best evacuation model of deep learning in
the design of public structures. <em>ICV</em>, <em>102</em>, 103975. (<a
href="https://doi.org/10.1016/j.imavis.2020.103975">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Evacuation behavior is an important factor which must be considered in the design of public structures. With the continuous complexity of structure, more and more factors should be considered in evacuation. The traditional design based on experience may have some limitations in practice. Based on the deep neural network model, the evacuation design simulation for subway station buildings is implemented. VR video tracking technologies such as auxiliary image data pre-training algorithm, tracking sequence pre-training algorithm, and recursive neural network model based on You Only Look Once (YOLO) are introduced. Compared with the convolutional neural network (CNN) model, the classified data set pre-training model, and YOLO algorithm, the accuracy and training speed of the model algorithm are verified. In simulation, the Zhujiang New Town Station in Guangzhou is taken as the object. The initial evacuation test point is selected according to the structure of the subway platform, and the test personnel are selected according to the test requirements. The average evacuation time and the average satisfaction score of the testers under the influence factors such as gender, age, subway frequency, and familiarity with VR equipment, as well as under the initial starting points of different evacuation tests. The results show that the accuracy of the algorithm is lower than that of the CNN, but the training speed is faster. The accuracy of the model based on YOLO recurrent neural network is the highest. Although the training speed is 19 ms, which is higher than other models, the overall performance is the best. Differences in factors such as gender, age, frequency of subway ride, and familiarity with VR devices will result in different differences in average evacuation time and average satisfaction score. When the platform center is used as the initial evacuation point, the average evacuation time is the shortest, and the average satisfaction score of the testers is the highest. In conclusion, through VR video tracking technology, the actual situation of subway station buildings can be well simulated, and further design schemes can be made according to the simulated situation, which has practical reference significance.},
  archive      = {J_ICV},
  author       = {Yan Chen and Shenjian Hu and He Mao and Wei Deng and Xin Gao},
  doi          = {10.1016/j.imavis.2020.103975},
  journal      = {Image and Vision Computing},
  month        = {10},
  pages        = {103975},
  shortjournal = {Image Vis. Comput.},
  title        = {Application of the best evacuation model of deep learning in the design of public structures},
  volume       = {102},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Adversarial sliced wasserstein domain adaptation networks.
<em>ICV</em>, <em>102</em>, 103974. (<a
href="https://doi.org/10.1016/j.imavis.2020.103974">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Domain adaptation has become a resounding success in learning a domain agnostic model that performs well on target dataset by leveraging source dataset which has related data distribution . Most of existing works aim at learning domain-invariant features across different domains, but they ignore the discriminability of learned features although it is import to improve the model&#39;s performance. This paper proposes a novel adversarial sliced Wasserstein domain adaptation network (AWDAN) that uses a shared encoder and classifier along with a domain classifier to enhance the discriminability of the domain-invariant features. AWDAN utilizes adversarial learning to learn domain-invariant features in feature space and simultaneously minimizes sliced Wasserstein distance in label space to enforce the generated features to be discriminative that guarantees the transfer performance. Meanwhile, we propose to fix the weights of the pre-trained CNN backbone to guarantee its adaptability. We provide theoretical analysis to demonstrate the efficacy of AWDAN. Experimental results show that the proposed AWDAN significantly outperforms existing domain adaptation methods on three visual domain adaptation tasks. Feature visualizations verify that AWDAN learns both domain-invariant and discriminative features , and can achieve domain agnostic feature learning .},
  archive      = {J_ICV},
  author       = {Yun Zhang and Nianbin Wang and Shaobin Cai},
  doi          = {10.1016/j.imavis.2020.103974},
  journal      = {Image and Vision Computing},
  month        = {10},
  pages        = {103974},
  shortjournal = {Image Vis. Comput.},
  title        = {Adversarial sliced wasserstein domain adaptation networks},
  volume       = {102},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Application of 3D laser scanning technology for image data
processing in the protection of ancient building sites through deep
learning. <em>ICV</em>, <em>102</em>, 103969. (<a
href="https://doi.org/10.1016/j.imavis.2020.103969">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To study the conservation of architectural heritage, computer algorithms are used to process the artificial field measurements and reference data maps. A 3D virtual model of traditional architecture is constructed for the direct learning and feature extraction from image data through deep learning . A set of tools and processes for pixel-level image processing labeling are proposed, and a label quality checking tool is written specially. Through the training method of transfer learning , the convergence speed and accuracy of the network are accelerated and improved. Based on the field data collection of the architectural heritage by a 3D laser scanner , the impacts of the number of stations, the number of targets, and the distance on the data scanning results are analyzed, thereby drawing the rules and principles of setting up the survey stations. While processing the point cloud data, for the redundant data and the rough difference points found in the original point cloud data, the program is written by the gross error elimination algorithm, which realizes the automatic elimination of the point cloud gross error data and provides a convenient method for data processing. The data collection, data processing, and model construction of architectural heritage are performed by the 3D laser scanner. The 3D models of traditional architecture with texture photos are obtained. The results show that through correlation error analysis and model evaluation, the requirements for the measurement of traditional architecture can be achieved. Therefore, the technology has a guiding significance for the conservation of traditional architecture, which proves that the 3D laser scanner has broad application prospects in the surveying and mapping of architectural heritage.},
  archive      = {J_ICV},
  author       = {Yongsheng Yin and Juan Antonio},
  doi          = {10.1016/j.imavis.2020.103969},
  journal      = {Image and Vision Computing},
  month        = {10},
  pages        = {103969},
  shortjournal = {Image Vis. Comput.},
  title        = {Application of 3D laser scanning technology for image data processing in the protection of ancient building sites through deep learning},
  volume       = {102},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Investigating bias in deep face analysis: The KANFace
dataset and empirical study. <em>ICV</em>, <em>102</em>, 103954. (<a
href="https://doi.org/10.1016/j.imavis.2020.103954">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning-based methods have pushed the limits of the state-of-the-art in face analysis. However, despite their success, these models have raised concerns regarding their bias towards certain demographics. This bias is inflicted both by limited diversity across demographics in the training set, as well as the design of the algorithms. In this work, we investigate the demographic bias of deep learning models in face recognition, age estimation, gender recognition and kinship verification. To this end, we introduce the most comprehensive, large-scale dataset of facial images and videos to date. It consists of 40K still images and 44K sequences (14.5M video frames in total) captured in unconstrained, real-world conditions from 1,045 subjects. The data are manually annotated in terms of identity, exact age, gender and kinship. The performance of state-of-the-art models is scrutinized and demographic bias is exposed by conducting a series of experiments. Lastly, a method to debias network embeddings is introduced and tested on the proposed benchmarks.},
  archive      = {J_ICV},
  author       = {Markos Georgopoulos and Yannis Panagakis and Maja Pantic},
  doi          = {10.1016/j.imavis.2020.103954},
  journal      = {Image and Vision Computing},
  month        = {10},
  pages        = {103954},
  shortjournal = {Image Vis. Comput.},
  title        = {Investigating bias in deep face analysis: The KANFace dataset and empirical study},
  volume       = {102},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Automated repair of fragmented tracks with 1D CNNs.
<em>ICV</em>, <em>101</em>, 103982. (<a
href="https://doi.org/10.1016/j.imavis.2020.103982">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multiple object tracking is an important but challenging computer vision problem. The complex motion of objects makes tracking difficult during long periods of object occlusion, and as a result occlusions frequently cause fragmented tracks with gaps. Previous works use linear interpolation to fill in such gaps, a technique which is only able to model simple motion. As a result, tracked bounding box locations can be quite poor in these situations. In this paper, we propose a 1D CNN based solution to filling gaps which models complex motion in a data-driven way. Our proposed solution uses only bounding box coordinates as input, and as such does not incur the computational cost of processing image features directly. We show that our model significantly outperforms linear interpolation on dynamic sports datasets in terms of mean intersection over union between predicted and ground truth bounding boxes.},
  archive      = {J_ICV},
  author       = {Md Sohel Rana and Aiden Nibali and Zhen He and Stuart Morgan},
  doi          = {10.1016/j.imavis.2020.103982},
  journal      = {Image and Vision Computing},
  month        = {9},
  pages        = {103982},
  shortjournal = {Image Vis. Comput.},
  title        = {Automated repair of fragmented tracks with 1D CNNs},
  volume       = {101},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Collaborative representation of blur invariant deep sparse
features for periocular recognition from smartphones. <em>ICV</em>,
<em>101</em>, 103979. (<a
href="https://doi.org/10.1016/j.imavis.2020.103979">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The periocular region is used for authentication in the recent days under unconstrained acquisition in biometrics . This work presents two new feature extraction techniques to achieve robust and blur invariant biometric verification using periocular images captured using smartphones - (1) Deep Sparse Features (DSF) and (2) Deep Sparse Time Frequency Features (DeSTiFF) . Both the approaches are based on extracting features via convolution of periocular images with a set of filters also referred as Deep Sparse Filters . The filters are learnt using natural image patches and sparse filtering approach. The DSF is obtained through convolution via Deep Sparse Filters . Further, convoluted responses are analyzed using Short Term Fourier Transform (STFT) to obtain time and frequency features of the images referred as DeSTIFF. The features obtained from the newly proposed feature extraction techniques are further represented in a collaborative subspace to achieve better verification performance. Both of the proposed feature extraction schemes are evaluated on two publicly available smartphone periocular databases and a new database (Visible Spectrum Periocular Image (VISPI) database) released with this article. The robustness of the proposed feature extraction is exemplified by comparing it with state-of-art approaches along with multiple deep networks where the improvement is evidently seen on large scale database with an average verification accuracy of Genuine Match Rate ≈ 98% at False Match Rate = 0.01%. We further support reproducible research by making the code and the database available for the academic research.},
  archive      = {J_ICV},
  author       = {Kiran Raja and Raghavendra Ramachandra and Christoph Busch},
  doi          = {10.1016/j.imavis.2020.103979},
  journal      = {Image and Vision Computing},
  month        = {9},
  pages        = {103979},
  shortjournal = {Image Vis. Comput.},
  title        = {Collaborative representation of blur invariant deep sparse features for periocular recognition from smartphones},
  volume       = {101},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A novel co-attention computation block for deep learning
based image co-segmentation. <em>ICV</em>, <em>101</em>, 103973. (<a
href="https://doi.org/10.1016/j.imavis.2020.103973">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The correlation between images is crucial for solving the image co-segmentation problem that is segmenting common and salient objects from a set of related images. This paper proposes a novel co-attention computation block to compute the visual correlation between images for improving the co-segmentation performance. Here ‘co-attention’ means that we obtain the co-attention features in encoded features of an image to guide the attention in another image. To this purpose, we firstly introduce top-k average pooling to compute the channel co-attention descriptor. Then we explore the correlation between features in different spatial positions to get the spatial co-attention descriptor. Finally, these two types of co-attention descriptors are multiplied to generate a fused one. We obtain such a fused co-attention descriptor for each image and use it to produce the co-attention augmented feature map for the following processing in the applications. We embed the proposed co-attention block into a U-shaped Siamese network for fulfilling the image co-segmentation. It is proven to be able to improve the performance effectively in the experiments. To our best knowledge, it leads to the currently best results on Internet dataset and iCoseg dataset.},
  archive      = {J_ICV},
  author       = {Xiaopeng Gong and Xiabi Liu and Yushuo Li and Huiyu Li},
  doi          = {10.1016/j.imavis.2020.103973},
  journal      = {Image and Vision Computing},
  month        = {9},
  pages        = {103973},
  shortjournal = {Image Vis. Comput.},
  title        = {A novel co-attention computation block for deep learning based image co-segmentation},
  volume       = {101},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Constructing big panorama from video sequence based on deep
local feature. <em>ICV</em>, <em>101</em>, 103972. (<a
href="https://doi.org/10.1016/j.imavis.2020.103972">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Constructing high-quality panorama is a fundamental task in both computer vision and computer graphics communities, thus, leading to many image stitching approaches for panorama construction. However, panorama constructed by traditional image stitching has a limited angle of view and has an expensively computational cost. To defend the issues, in this paper, we proposed to use video sequence as input for constructing big panorama, resulting in a high-quality panoramic image, the presented method is built on the stabilized video and robust feature tracking method. Specifically, the input video sequences are captured by moving hand cameras which can be any type of consumer-level camera. To mitigate the affections from rolling shutters in videos, a novel video stabilization method is introduced to filter the unstable camera&#39;s path, then resulting in a stabilized video for panorama construction. Additionally, a deep local feature-based feature tracking method is proposed to produce feature correspondences between consecutive video frames for camera motion estimation used in both video stabilization and image stitching. Finally, a comprehensive experiment conducted on the benchmarking datasets is presented to demonstrate the effectiveness of the proposed method.},
  archive      = {J_ICV},
  author       = {Mingwei Cao and Liping Zheng and Wei Jia and Xiaoping Liu},
  doi          = {10.1016/j.imavis.2020.103972},
  journal      = {Image and Vision Computing},
  month        = {9},
  pages        = {103972},
  shortjournal = {Image Vis. Comput.},
  title        = {Constructing big panorama from video sequence based on deep local feature},
  volume       = {101},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Person search: New paradigm of person re-identification: A
survey and outlook of recent works. <em>ICV</em>, <em>101</em>, 103970.
(<a href="https://doi.org/10.1016/j.imavis.2020.103970">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Person Search (PS) has become a major field because of its need in community and in the field of research among researchers. This task aims to find a probe person from whole scene which shows great significance in video surveillance field to track lost people, re-identification, and verification of person. In last few years, deep learning has played unremarkable role for the solution of re-identification problem. Deep learning shows incredible performance in person (re-ID) and search. Researchers experience more flexibility in proposing new methods and solve challenging issues such as low resolution, pose variation, background clutter, occlusion, viewpoints, and low illumination. Specially, convolutional neural network (CNN) achieves breakthrough performance and extracts useful patterns and characteristics. Development of new framework takes substantial efforts; hard work and computation cost are required to acquire excellent results. This survey paper includes brief discussion about feature representation learning and deep metric learning with novel loss functions. We thoroughly review datasets with performance analysis on existing datasets. Finally, we are reviewing current solutions for further consideration.},
  archive      = {J_ICV},
  author       = {Khawar Islam},
  doi          = {10.1016/j.imavis.2020.103970},
  journal      = {Image and Vision Computing},
  month        = {9},
  pages        = {103970},
  shortjournal = {Image Vis. Comput.},
  title        = {Person search: new paradigm of person re-identification: a survey and outlook of recent works},
  volume       = {101},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Explaining VQA predictions using visual grounding and a
knowledge base. <em>ICV</em>, <em>101</em>, 103968. (<a
href="https://doi.org/10.1016/j.imavis.2020.103968">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this work, we focus on the Visual Question Answering (VQA) task, where a model must answer a question based on an image, and the VQA-Explanations task, where an explanation is produced to support the answer. We introduce an interpretable model capable of pointing out and consuming information from a novel Knowledge Base ( KB ) composed of real-world relationships between objects, along with labels mined from available region descriptions and object annotations. Furthermore, this model provides a visual and textual explanations to complement the KB visualization. The use of a KB brings two important consequences: enhance predictions and improve interpretability . We achieve this by introducing a mechanism that can extract relevant information from this KB , and can point out the relations better suited for predicting the answer. A supervised attention map is generated over the KB to select the relevant relationships from it for each question-image pair. Moreover, we add image attention supervision on the explanations module to generate better visual and textual explanations. We quantitatively show that the predicted answers improve when using the KB ; similarly, explanations improve with this and when adding image attention supervision. Also, we qualitatively show that the KB attention helps to improve interpretability and enhance explanations. Overall, the results support the benefits of having multiple tasks to enhance the interpretability and performance of the model.},
  archive      = {J_ICV},
  author       = {Felipe Riquelme and Alfredo De Goyeneche and Yundong Zhang and Juan Carlos Niebles and Alvaro Soto},
  doi          = {10.1016/j.imavis.2020.103968},
  journal      = {Image and Vision Computing},
  month        = {9},
  pages        = {103968},
  shortjournal = {Image Vis. Comput.},
  title        = {Explaining VQA predictions using visual grounding and a knowledge base},
  volume       = {101},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Cross-modal feature extraction and integration based RGBD
saliency detection. <em>ICV</em>, <em>101</em>, 103964. (<a
href="https://doi.org/10.1016/j.imavis.2020.103964">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In RGBD saliency detection research field, RGB and depth cues are generally given the same status by RGBD saliency models. However, they ignore that both modalities are significantly different in inherent attribution so that effective features cannot be drawn from depth maps. In order to address this issue, this paper proposes a novel RGBD saliency model including two key components: the contrast-guided depth feature extraction (CDFE) module and the cross-modal feature integration (CFI) module. Specifically, considering the specific properties of depth information, we first design a targeted CDFE module, which learns multi-level deep depth features by strengthening the depth contrast between foreground and background, to provide multi-level deep depth features. Then, to sufficiently and reasonably integrate multi-level cross-modal features, namely the multi-level deep RGB and depth features, we equip the saliency inference branch with the CFI module, which contains two successive steps, i.e. information enrichment and feature enhancement. Extensive experiments are conducted on five challenging RGBD datasets, and the experimental results clearly demonstrate the effectiveness and superiority of the proposed model against the state-of-the-art RGBD saliency models.},
  archive      = {J_ICV},
  author       = {Liang Pan and Xiaofei Zhou and Ran Shi and Jiyong Zhang and Chenggang Yan},
  doi          = {10.1016/j.imavis.2020.103964},
  journal      = {Image and Vision Computing},
  month        = {9},
  pages        = {103964},
  shortjournal = {Image Vis. Comput.},
  title        = {Cross-modal feature extraction and integration based RGBD saliency detection},
  volume       = {101},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Feature based video stabilization based on boosted HAAR
cascade and representative point matching algorithm. <em>ICV</em>,
<em>101</em>, 103957. (<a
href="https://doi.org/10.1016/j.imavis.2020.103957">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The success of handheld video capturing devices has further fueled the need of improved video stabilization. The videos often contain many foreground facial features like eyes, nose etc. These foreground features can be considered as feature points and may be used to stabilize videos. This paper proposes an innovative and effective digital video stabilization technique, which utilizes foreground features present in the video to produce consistent and stabilized output. It uses successive stages of Boosted HAAR cascade and representative point matching digital motion stabilization algorithm to identify and stabilize the video. The feature based tracking of object improves motion estimation accuracy between two frames thereby increasing the correlation calculation and compensation motion vector . This work achieves a significantly smoother sequence after the motion compensation. It also improves the robustness, precision and quality of the video when compared to traditional digital stabilization algorithms. The simulation results compared with pre-existing techniques reflect distinct improvements in Inter-Frame Transformation Fidelity values and Structural Similarity Index along with lesser standard deviation between image frames.},
  archive      = {J_ICV},
  author       = {Rohit Raj and Pooshkar Rajiv and Prabhat Kumar and Manju Khari and Elena Verdú and Rubén González Crespo and Gunasekaran Manogaran},
  doi          = {10.1016/j.imavis.2020.103957},
  journal      = {Image and Vision Computing},
  month        = {9},
  pages        = {103957},
  shortjournal = {Image Vis. Comput.},
  title        = {Feature based video stabilization based on boosted HAAR cascade and representative point matching algorithm},
  volume       = {101},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Deep manifold clustering based optimal pseudo pose
representation (DMC-OPPR) for unsupervised person re-identification.
<em>ICV</em>, <em>101</em>, 103956. (<a
href="https://doi.org/10.1016/j.imavis.2020.103956">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Person re-identification (re-ID) is highly complex in a diverse surveillance environment. The existing person re-ID methods are evaluated as a closed set problem with limited environmental variation. It is highly challenging to estimate the diverse poses of a dynamically crowded environment using the traditional unsupervised person re-ID methods. To resolve this issue of handling complex diverse poses and camera angles, a contextual incremental multi-clustering based unsupervised person re-ID method have been proposed. Cam-pose based optimal similarity distance threshold is determined to label the unlabeled person re-ID images efficiently. Frequent intra and inter-camera pseudo pose sequences are represented with optimal distance threshold. This resolves the over-fitting issue created by the dominant samples of an identity and reduces the source-target domain gap. The experimental results show the supremacy of our proposed method over the existing unsupervised person re-ID methods in handling complex poses and camera angles in an incremental self-learning diverse surveillance environment.},
  archive      = {J_ICV},
  author       = {S. Sridhar Raj and Munaga V.N.K. Prasad and Ramadoss Balakrishnan},
  doi          = {10.1016/j.imavis.2020.103956},
  journal      = {Image and Vision Computing},
  month        = {9},
  pages        = {103956},
  shortjournal = {Image Vis. Comput.},
  title        = {Deep manifold clustering based optimal pseudo pose representation (DMC-OPPR) for unsupervised person re-identification},
  volume       = {101},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). CrossFusion net: Deep 3D object detection based on RGB
images and point clouds in autonomous driving. <em>ICV</em>,
<em>100</em>, 103955. (<a
href="https://doi.org/10.1016/j.imavis.2020.103955">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, accurate 3D detection plays an important role in a lot of applications. Autonomous driving , for instance, is one of typical representatives. This paper aims to design an accurate 3D detector that takes both Li-DAR point clouds and RGB images as inputs according to the fact that both LiDAR and camera have their own merits. A deep novel end-to-end two-stream learnable architecture, CrossFusion Net, is designed to exploit features from both LiDAR point clouds as well as RGB images through a hierarchical fusion structure . Specifically, CrossFusion Net utilizes bird&#39;s eye view (BEV) of point clouds through projection. Besides, these two feature maps of different streams are fused through the newly introduced CrossFusion(CF) layer. The proposed CF layer transforms feature maps of one stream to another based on the spatial relationship between the BEV and RGB images. Additionally, we apply attention mechanism on the transformed feature map and the original one to automatically decide the importance of the two feature maps from the two sensors. Experiments on the challenging KITTI car 3D detection benchmark and BEV detection benchmark show that the presented approach outperforms the other state-of-the-art methods in average precision(AP), specifically, as well as outperforms UberATG-ContFuse [3] of 8% AP in moderate 3D car detection. Furthermore, the proposed network learns an effective representation in perception of circumstances via RGB feature maps and BEV feature maps.},
  archive      = {J_ICV},
  author       = {Dza-Shiang Hong and Hung-Hao Chen and Pei-Yung Hsiao and Li-Chen Fu and Siang-Min Siao},
  doi          = {10.1016/j.imavis.2020.103955},
  journal      = {Image and Vision Computing},
  month        = {8},
  pages        = {103955},
  shortjournal = {Image Vis. Comput.},
  title        = {CrossFusion net: Deep 3D object detection based on RGB images and point clouds in autonomous driving},
  volume       = {100},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Gender based face aging with cycle-consistent adversarial
networks. <em>ICV</em>, <em>100</em>, 103945. (<a
href="https://doi.org/10.1016/j.imavis.2020.103945">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Face aging is a task which referred to image synthesis , and the challenge comes from the training dataset, most existing face aging works require paired face images which is difficult to collect. Face images with various ages of the same person can be considered as unpaired images which come from different domains. The degree of aging effect can be influenced by age, gender, race and some other factors. In this paper, we are committed to studying the impact of gender on face aging problem, which involves the processing and modeling of face images. It has been proved that Generative Adversarial Networks(GANs) is competitive in realistic image synthesis , and many works employed Cycle-Consistent Adversarial Networks(CycleGANs) have shown the high performance in unpaired image-to-image translation. To overcome current difficulties and improve the performance of existing models on the face aging tasks, we proposed an innovative Gender-based training method using CycleGAN by pairwise training CycleGAN over several age groups which are grouped by age and gender. We build a constraint model based on gender discrimination to better simulate the expected aging effect of face images. To evaluate our works using subjective method, we have set up a quantitative evaluation mechanism with participants involved in. Compared with other similar subjective evaluation methods, our method is more objective in the demonstration of experimental results. The experimental results show that our method has a more realistic and excellent performance compared to those using CycleGAN for face age synthesis directly.},
  archive      = {J_ICV},
  author       = {Chun Yang and Zhihan Lv},
  doi          = {10.1016/j.imavis.2020.103945},
  journal      = {Image and Vision Computing},
  month        = {8},
  pages        = {103945},
  shortjournal = {Image Vis. Comput.},
  title        = {Gender based face aging with cycle-consistent adversarial networks},
  volume       = {100},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Viewpoint constrained and unconstrained cricket stroke
localization from untrimmed videos. <em>ICV</em>, <em>100</em>, 103944.
(<a href="https://doi.org/10.1016/j.imavis.2020.103944">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this work, we create two new video datasets for the task of temporal Cricket stroke extraction. The two datasets, namely, the Highlights dataset (with approx. 117K frames) and the Generic dataset (with approx. 1.93M frames), comprise of Cricket telecast videos collected from available online sources and down-sampled to 360×640 at 25FPS. These untrimmed videos have been manually annotated with temporal Cricket strokes considering viewpoint invariance assumption. We construct two learning based localization pipelines which are dependent ( Constrained ) and independent ( Unconstrained ) of our viewpoint labeling assumption. The Unconstrained pipeline finetunes a pretrained C3D model with GRU training in disconnected and connected modes, while our Constrained pipeline uses boundary detection with first frame classification for generating the temporal localizations. Two post-processing steps, of filtering and boundary correction, are also discussed which help in improving the overall accuracy values. A modified evaluation metric , Weighted Mean TIoU, for single category temporal localization problem is also presented and compared with the evaluations of the standard mAP metric (threshold ≥ 0.5) on the created dataset. The best weighted mean TIoU of our method was 0.9376 and 0.7145 on the Highlights and Generic test partitions, respectively. Moreover, we compare our baseline method with 3D Segment CNNs and Temporal Recurrent Networks (TRNs) which have state of art results on THUMOS 2014 dataset.},
  archive      = {J_ICV},
  author       = {Arpan Gupta and Sakthi Balan Muthiah},
  doi          = {10.1016/j.imavis.2020.103944},
  journal      = {Image and Vision Computing},
  month        = {8},
  pages        = {103944},
  shortjournal = {Image Vis. Comput.},
  title        = {Viewpoint constrained and unconstrained cricket stroke localization from untrimmed videos},
  volume       = {100},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Development of an embedded road boundary detection system
based on deep learning. <em>ICV</em>, <em>100</em>, 103935. (<a
href="https://doi.org/10.1016/j.imavis.2020.103935">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The ability to sense the surrounding environment is an important developing technology in the field of automated vehicles. Lane line detection could determine a vehicle&#39;s travelable area. An embedded road boundary detection system based on deep learning was developed in this study. The system can detect structured and unstructured roads in a variety of situations. To obtain an image with clear lane markings, a convolution auto-encoder with the characteristics of noise reduction and reconstruction was used to remove all objects in the images except lane markings. Then, the feature points of the lane line were extracted, and the lane line was fitted with a hyperbolic model. Finally, a particle filter was used for lane tracking. The road boundary detection system was implemented on the NVIDIA Jetson TX2 platform. Three different situations, day, night, and rainy day were selected to demonstrate the performance of the proposed algorithm. Additionally, to deal with structured roads, some special scenes, such as shadows, tunnels, degenerate lane markings, and blocked lane markings, were considered. According to the experimental results, the accuracy of the proposed lane detection system for structured and unstructured roads was 90.02%.},
  archive      = {J_ICV},
  author       = {Jau Woei Perng and Ya Wen Hsu and Ya Zhu Yang and Chia Yen Chen and Tang Kai Yin},
  doi          = {10.1016/j.imavis.2020.103935},
  journal      = {Image and Vision Computing},
  month        = {8},
  pages        = {103935},
  shortjournal = {Image Vis. Comput.},
  title        = {Development of an embedded road boundary detection system based on deep learning},
  volume       = {100},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Monocular depth estimation with SPN loss. <em>ICV</em>,
<em>100</em>, 103934. (<a
href="https://doi.org/10.1016/j.imavis.2020.103934">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Understanding the 3D space is crucial for autonomous vehicles for planning and navigation. Traditionally autonomous vehicles use LiDAR sensor to 3D map its environment. LiDAR sensor data are often noisy and sparse making it not fully reliable for real-time applications like autonomous driving , thus redundant such sensors are used for the purpose. The array of cameras in an autonomous vehicle purposed for detection and tracking can be reused for depth estimation as well. In this paper, an unsupervised monocular depth estimation approach for autonomous vehicles which can be used as redundant depth estimators replacing multiple LiDAR sensors. Here, a deep learning based method is used with a multiscale encoder-decoder network to estimate depth. Target view among the stereo pairs is reconstructed by inverse warping the source view using geometric camera projection. The network is guided by the stereo positive–negative(SPN) loss which minimizes the loss between reconstructed view and corresponding stereo ground truth and, also maximizes the loss between reconstructed views and corresponding opposite stereo ground truth. The proposed approach shows state of the art accuracy in autonomous driving dataset KITTI.},
  archive      = {J_ICV},
  author       = {Alwyn Mathew and Jimson Mathew},
  doi          = {10.1016/j.imavis.2020.103934},
  journal      = {Image and Vision Computing},
  month        = {8},
  pages        = {103934},
  shortjournal = {Image Vis. Comput.},
  title        = {Monocular depth estimation with SPN loss},
  volume       = {100},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Joint detection and tracking in videos with identification
features. <em>ICV</em>, <em>100</em>, 103932. (<a
href="https://doi.org/10.1016/j.imavis.2020.103932">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent works have shown that combining object detection and tracking tasks, in the case of video data, results in higher performance for both tasks, but they require a high frame-rate as a strict requirement for performance. This assumption is often violated in real-world applications, when models run on embedded devices, often at only a few frames per second. Videos at low frame-rate suffer from large object displacements. Here re-identification features may support to match large-displaced object detections, but current joint detection and re-identification formulations degrade the detector performance, as these two are contrasting tasks. In the real-world application having separate detector and re-id models is often not feasible, as both the memory and runtime effectively double. Towards robust long-term tracking applicable to reduced-computational-power devices, we propose the first joint optimization of detection, tracking and re-identification features for videos. Notably, our joint optimization maintains the detector performance, a typical multi-task challenge. At inference time, we leverage detections for tracking (tracking-by-detection) when the objects are visible, detectable and slowly moving in the image. We leverage instead re-identification features to match objects which disappeared (e.g. due to occlusion) for several frames or were not tracked due to fast motion (or low-frame-rate videos). Our proposed method reaches the state-of-the-art on MOT , it ranks 1st in the UA-DETRAC’18 tracking challenge among online trackers, and 3rd overall.},
  archive      = {J_ICV},
  author       = {Bharti Munjal and Abdul Rafey Aftab and Sikandar Amin and Meltem D. Brandlmaier and Federico Tombari and Fabio Galasso},
  doi          = {10.1016/j.imavis.2020.103932},
  journal      = {Image and Vision Computing},
  month        = {8},
  pages        = {103932},
  shortjournal = {Image Vis. Comput.},
  title        = {Joint detection and tracking in videos with identification features},
  volume       = {100},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Cross-correlated attention networks for person
re-identification. <em>ICV</em>, <em>100</em>, 103931. (<a
href="https://doi.org/10.1016/j.imavis.2020.103931">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep neural networks need to make robust inference in the presence of occlusion, background clutter, pose and viewpoint variations -to name a few- when the task of person re-identification is considered. Attention mechanisms have recently proven to be successful in handling the aforementioned challenges to some degree. However previous designs fail to capture inherent inter-dependencies between the attended features; leading to restricted interactions between the attention blocks. In this paper, we propose a new attention module called Cross-Correlated Attention ( CCA ); which aims to overcome such limitations by maximizing the information gain between different attended regions. Moreover, we also propose a novel deep network that makes use of different attention mechanisms to learn robust and discriminative representations of person images. The resulting model is called the Cross-Correlated Attention Network ( CCAN ). Extensive experiments demonstrate that the CCAN comfortably outperforms current state-of-the-art algorithms by a tangible margin. Modeling the inherentspatial relations between different attended regions within the deep architecture. Joint end-to-end cross correlated attention and representational learning. State-of-the-art results in terms of mAP and Rank-1 accuracies across several challenging datasets.},
  archive      = {J_ICV},
  author       = {Jieming Zhou and Soumava Kumar Roy and Pengfei Fang and Mehrtash Harandi and Lars Petersson},
  doi          = {10.1016/j.imavis.2020.103931},
  journal      = {Image and Vision Computing},
  month        = {8},
  pages        = {103931},
  shortjournal = {Image Vis. Comput.},
  title        = {Cross-correlated attention networks for person re-identification},
  volume       = {100},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Fast and robust visual tracking with hard balanced focal
loss and guided domain adaption. <em>ICV</em>, <em>100</em>, 103929. (<a
href="https://doi.org/10.1016/j.imavis.2020.103929">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, Siamese networks based trackers have shown excellent performance in accuracy and speed. However, previous studies treat all training samples equally and use the general feature space without adapting to the specific video during tracking. These trackers ignore the class data imbalance during training and the feature space difference between the generic domain and the current tracking target domain, which limits the robustness of trackers. In this paper, we propose an algorithm for learning a discriminative and self-adaptive feature representation, in order to achieve accurate and robust tracking. During the off-line training stage, a hard balanced focal loss function is utilized to solve the positive–negative samples imbalance and the hard-easy negative samples imbalance. During the tracking phase, an off-line trained guided domain adaptation module is embedded into the Siamese networks, which can quickly transfer the feature space from the general domain to the current video domain by adjusting the search branch channel weights. Our networks are trained in an end-to-end manner and without online updating. Our tracker runs at 130 FPS while achieving favorable performance against the state-of-the-art methods on OTB-2013, OTB-2015, VOT-2016, VOT-2017, GOT-10 K and TC-128 benchmarks.},
  archive      = {J_ICV},
  author       = {Hengcheng Fu and Wuneng Zhou and Xiaofeng Wang and Huanlong Zhang},
  doi          = {10.1016/j.imavis.2020.103929},
  journal      = {Image and Vision Computing},
  month        = {8},
  pages        = {103929},
  shortjournal = {Image Vis. Comput.},
  title        = {Fast and robust visual tracking with hard balanced focal loss and guided domain adaption},
  volume       = {100},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Dense convolutional feature histograms for robust visual
object tracking. <em>ICV</em>, <em>99</em>, 103933. (<a
href="https://doi.org/10.1016/j.imavis.2020.103933">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Despite recent breakthroughs in the field, Visual Object Tracking remains an open and challenging task in Computer Vision . Modern applications require trackers to not only be accurate but also very fast, even on embedded systems. In this work, we use features from Convolutional Neural Networks to build histograms, which are more adept at handling appearance variations, in an end-to-end trainable architecture. To deal with the internal covariate shift that occurs when extracting histograms from convolutional features as well as to incorporate informations from the multiple levels of the neural hierarchy, we propose and use a novel densely connected architecture where histograms from multiple layers are concatenated to produce the final representation. Experimental results validate our hypotheses on the benefits of using histograms as opposed to standard convolutional features, as the proposed histogram-based tracker surpasses recently proposed sophisticated trackers on multiple benchmarks. Long-term tracking results also reaffirm the usefulness of the proposed tracker in more challenging scenarios, where appearance variations are more severe and traditional trackers fail.},
  archive      = {J_ICV},
  author       = {Paraskevi Nousi and Anastasios Tefas and Ioannis Pitas},
  doi          = {10.1016/j.imavis.2020.103933},
  journal      = {Image and Vision Computing},
  month        = {7},
  pages        = {103933},
  shortjournal = {Image Vis. Comput.},
  title        = {Dense convolutional feature histograms for robust visual object tracking},
  volume       = {99},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Multi-depth dilated network for fashion landmark detection
with batch-level online hard keypoint mining. <em>ICV</em>, <em>99</em>,
103930. (<a href="https://doi.org/10.1016/j.imavis.2020.103930">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning has been applied to fashion landmark detection in recent years, and great progress has been made. However, the detection of hard keypoints , such as those which are occluded or invisible, remains challenging and must be addressed. To tackle this problem, in the feature exaction level a novel Multi-Depth Dilated (MDD) block which is composed of different numbers of dilated convolutions in parallel and a Multi-Depth Dilated Network (MDDNet) constructed by MDD blocks are proposed in this paper, and in the training level a network training method of Batch-level Online Hard Keypoint Mining (B-OHKM) is proposed. During the training of network, each clothing keypoint is one-to-one corresponding to the related loss value calculated at that keypoint. The greater the loss of the keypoint, the more difficult it is for the network to detect that keypoint. In that way, hard keypoints can be effectively mined, so that the network can be trained in a targeted manner to improve the performance of hard keypoints. The results of experiments on two large-scale fashion benchmark datasets demonstrate that the proposed MDDNet that uses the MDD block and B-OHKM method achieves state-of-the-art results.},
  archive      = {J_ICV},
  author       = {Qirong Bu and Kai Zeng and Rui Wang and Jun Feng},
  doi          = {10.1016/j.imavis.2020.103930},
  journal      = {Image and Vision Computing},
  month        = {7},
  pages        = {103930},
  shortjournal = {Image Vis. Comput.},
  title        = {Multi-depth dilated network for fashion landmark detection with batch-level online hard keypoint mining},
  volume       = {99},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Learning rebalanced human parsing model from imbalanced
datasets. <em>ICV</em>, <em>99</em>, 103928. (<a
href="https://doi.org/10.1016/j.imavis.2020.103928">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Research on human parsing methods has attracted increasing attention in a wide range of applications. However, dataset imbalance is still a challenging problem in this task, which directly affects the performance of human parsing. There are different types of dataset imbalance problems. For example, the numbers of samples for various labels in a dataset may differ, the scales of objects identified by different labels may vary considerably, the differences between some heterogeneous label types may be much smaller than other cases, and in some extreme situations, images may be labeled incorrectly. In this paper, we propose a rebalanced model for imbalanced human parsing. Two innovative blocks are included in the model, i.e. , a pre-bilateral awareness block and a combined-order statistics awareness block. The function of the former is to leverage the multiscale feature extractors to capture the changing scale information in an efficient way from the spatial space. Meanwhile, the function of the latter is to exploit the information of the feature distributions from the channel space. Furthermore, we propose an imbalance data-drop algorithm to simultaneously solve the mislabeling and small sample label weighting problems. Extensive experiments are conducted on three datasets, and the experimental results demonstrate that our method is able to solve the problem of data imbalance efficiently and obtain better human parsing performance.},
  archive      = {J_ICV},
  author       = {Enbo Huang and Zhuo Su and Fan Zhou and Ruomei Wang},
  doi          = {10.1016/j.imavis.2020.103928},
  journal      = {Image and Vision Computing},
  month        = {7},
  pages        = {103928},
  shortjournal = {Image Vis. Comput.},
  title        = {Learning rebalanced human parsing model from imbalanced datasets},
  volume       = {99},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Cross-resolution learning for face recognition.
<em>ICV</em>, <em>99</em>, 103927. (<a
href="https://doi.org/10.1016/j.imavis.2020.103927">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Convolutional Neural Network models have reached extremely high performance on the Face Recognition task. Mostly used datasets, such as VGGFace2, focus on gender, pose, and age variations, in the attempt of balancing them to empower models to better generalize to unseen data. Nevertheless, image resolution variability is not usually discussed, which may lead to a resizing of 256 pixels. While specific datasets for very low-resolution faces have been proposed, less attention has been paid on the task of cross-resolution matching. Hence, the discrimination power of a neural network might seriously degrade in such a scenario. Surveillance systems and forensic applications are particularly susceptible to this problem since, in these cases, it is common that a low-resolution query has to be matched against higher-resolution galleries. Although it is always possible to either increase the resolution of the query image or to reduce the size of the gallery (less frequently), to the best of our knowledge, extensive experimentation of cross-resolution matching was missing in the recent deep learning-based literature. In the context of low- and cross-resolution Face Recognition, the contribution of our work is fourfold: i) we proposed a training procedure to fine-tune a state-of-the-art model to empower it to extract resolution-robust deep features; ii) we conducted an extensive test campaign by using high-resolution datasets (IJB-B and IJB-C) and surveillance-camera-quality datasets (QMUL-SurvFace, TinyFace, and SCface) showing the effectiveness of our algorithm to train a resolution-robust model; iii) even though our main focus was the cross-resolution Face Recognition, by using our training algorithm we also improved upon state-of-the-art model performances considering low-resolution matches; iv) we showed that our approach could be more effective concerning preprocessing faces with super-resolution techniques. The python code of the proposed method will be available at https://github.com/fvmassoli/cross-resolution-face-recognition .},
  archive      = {J_ICV},
  author       = {Fabio Valerio Massoli and Giuseppe Amato and Fabrizio Falchi},
  doi          = {10.1016/j.imavis.2020.103927},
  journal      = {Image and Vision Computing},
  month        = {7},
  pages        = {103927},
  shortjournal = {Image Vis. Comput.},
  title        = {Cross-resolution learning for face recognition},
  volume       = {99},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Class-aware domain adaptation for improving adversarial
robustness. <em>ICV</em>, <em>99</em>, 103926. (<a
href="https://doi.org/10.1016/j.imavis.2020.103926">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent works have demonstrated convolutional neural networks are vulnerable to adversarial examples , i.e., inputs to machine learning models that an attacker has intentionally designed to cause the models to make a mistake. To improve the adversarial robustness of neural networks , adversarial training has been proposed to train networks by injecting adversarial examples into the training data . However, adversarial training could overfit to a specific type of adversarial attack and also lead to standard accuracy drop on clean images. To this end, we propose a novel Class-Aware Domain Adaptation (CADA) method for adversarial defense without directly applying adversarial training. Specifically, we propose to learn domain-invariant features for adversarial examples and clean images via a domain discriminator . Furthermore, we introduce a class-aware component into the discriminator to increase the discriminative power of the network for adversarial examples. We evaluate our newly proposed approach using multiple benchmark datasets. The results demonstrate that our method can significantly improve the state-of-the-art of adversarial robustness for various attacks and maintain high performances on clean images.},
  archive      = {J_ICV},
  author       = {Xianxu Hou and Jingxin Liu and Bolei Xu and Xiaolong Wang and Bozhi Liu and Guoping Qiu},
  doi          = {10.1016/j.imavis.2020.103926},
  journal      = {Image and Vision Computing},
  month        = {7},
  pages        = {103926},
  shortjournal = {Image Vis. Comput.},
  title        = {Class-aware domain adaptation for improving adversarial robustness},
  volume       = {99},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Zero-sum game theory model for segmenting skin regions.
<em>ICV</em>, <em>99</em>, 103925. (<a
href="https://doi.org/10.1016/j.imavis.2020.103925">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a new method for skin region segmentation based on a zero-sum game theory model which exploits the opposite classifications of an image region by different skin detectors. In fact, these regions are considered conflict areas between two players (skin and non-skin) and skin detectors are considered strategies. An appropriate utility function is then defined. The computation of the saddle point (The Nash equilibrium) in the mixed extension of the proposed zero-sum game allows classifying effectively the conflict areas and so reducing the false positive skin detection. Experiments were conducted on three publically available databases using four selected skin detectors based on skin color information, skin-texture cues and employ rule-based or neural networks . The results show that the proposed method outperforms the existing skin segmentation approaches in reducing the false positive rates and obtains promising results in the skin segmentation performance .},
  archive      = {J_ICV},
  author       = {Djamila Dahmani and Mehdi Cheref and Slimane Larabi},
  doi          = {10.1016/j.imavis.2020.103925},
  journal      = {Image and Vision Computing},
  month        = {7},
  pages        = {103925},
  shortjournal = {Image Vis. Comput.},
  title        = {Zero-sum game theory model for segmenting skin regions},
  volume       = {99},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Convolutional prototype learning for zero-shot recognition.
<em>ICV</em>, <em>98</em>, 103924. (<a
href="https://doi.org/10.1016/j.imavis.2020.103924">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Zero-shot learning (ZSL) has received increasing attention in recent years especially in areas of fine-grained object recognition, retrieval, and image captioning . The key to ZSL is to transfer knowledge from the seen to the unseen classes via auxiliary class attribute vectors. However, the popularly learned projection functions in previous works cannot generalize well since they assume the distribution consistency between seen and unseen domains at sample-level. Besides, the provided non-visual and unique class attributes can significantly degrade the recognition performance in semantic space. In this paper, we propose a simple yet effective convolutional prototype learning (CPL) framework for zero-shot recognition. By assuming distribution consistency at task-level, our CPL is capable of transferring knowledge smoothly to recognize unseen samples. Furthermore, inside each task, discriminative visual prototypes are learned via a distance based training mechanism. Consequently, we can perform recognition in visual space, instead of semantic space. An extensive group of experiments are then carefully designed and presented, demonstrating that CPL obtains more favorable effectiveness, over currently available alternatives under various settings.},
  archive      = {J_ICV},
  author       = {Zhizhe Liu and Xingxing Zhang and Zhenfeng Zhu and Shuai Zheng and Yao Zhao and Jian Cheng},
  doi          = {10.1016/j.imavis.2020.103924},
  journal      = {Image and Vision Computing},
  month        = {6},
  pages        = {103924},
  shortjournal = {Image Vis. Comput.},
  title        = {Convolutional prototype learning for zero-shot recognition},
  volume       = {98},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). EDS pooling layer. <em>ICV</em>, <em>98</em>, 103923. (<a
href="https://doi.org/10.1016/j.imavis.2020.103923">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Convolutional neural networks (CNNs) have been the source of recent breakthroughs in many vision tasks. Feature pooling layers are being widely used in CNNs to reduce the spatial dimensions of the feature maps of the hidden layers. This gives CNNs the property of spatial invariance and also results in speed-up and reduces over-fitting. However, this also causes significant information loss. All existing feature pooling layers follow a one-step procedure for spatial pooling, which affects the overall performance due to significant information loss. Not much work has been done to do efficient feature pooling operation in CNNs. To reduce the loss of information at this critical operation of the CNNs, we propose a new EDS layer (Expansion Downsampling learnable-Scaling) to replace the existing pooling mechanism. We propose a two-step procedure to minimize the information loss by increasing the number of channels in pooling operation. We also use feature scaling in the proposed EDS layer to highlight the most relevant channels/feature-maps. Our results show a significant improvement over the generally used pooling methods such as MaxPool, AvgPool, and StridePool (strided convolutions with stride &gt; 1). We have done the experiments on image classification and object detection task. ResNet-50 with our proposed EDS layer has performed comparably to ResNet-152 with stride pooling on the ImageNet dataset.},
  archive      = {J_ICV},
  author       = {Pravendra Singh and Prem Raj and Vinay P. Namboodiri},
  doi          = {10.1016/j.imavis.2020.103923},
  journal      = {Image and Vision Computing},
  month        = {6},
  pages        = {103923},
  shortjournal = {Image Vis. Comput.},
  title        = {EDS pooling layer},
  volume       = {98},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A contextual conditional random field network for monocular
depth estimation. <em>ICV</em>, <em>98</em>, 103922. (<a
href="https://doi.org/10.1016/j.imavis.2020.103922">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Monocular depth estimation plays a crucial role in understanding 3D scene geometry and is a challenging computer vision task . Recently, deep convolutional neural networks have been applied to solve this problem. However, existing methods either directly exploiting RGB pixels which can introduce much noise into the depth map or utilizing over smoothed internal representation features which can cause blur in the depth map. In this paper, we propose a contextual CRF network (CCN) to tackle these issues. The new CCN adopts the popular encoder-decoder architecture with a new contextual CRF module (CCM) which is guided by the depth features and regularizes the information flow from the encoder layer to the corresponding layer in the decoder, thus can reduce the mismatch between the RGB pixel and the depth map cue while at the same time retain detail features to output a fine-grained depth map. Moreover, we propose a depth-guided loss function which pays a balanced attention to near and far pixels thus addressing the long-tailed distribution of depth information. We have conducted extensive experiments on three public datasets for monocular depth estimation. Results demonstrate that our proposed CCN achieves superior performances in terms of visual quality and competitive quantitative results when compared with state-of-the-art methods.},
  archive      = {J_ICV},
  author       = {Jun Liu and Qing Li and Rui Cao and Wenming Tang and Guoping Qiu},
  doi          = {10.1016/j.imavis.2020.103922},
  journal      = {Image and Vision Computing},
  month        = {6},
  pages        = {103922},
  shortjournal = {Image Vis. Comput.},
  title        = {A contextual conditional random field network for monocular depth estimation},
  volume       = {98},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Anomaly detection in surveillance video based on
bidirectional prediction. <em>ICV</em>, <em>98</em>, 103915. (<a
href="https://doi.org/10.1016/j.imavis.2020.103915">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the development of information technology and the popularization of monitoring network, how to quickly and automatically detect abnormal behaviors in surveillance video is becoming more and more important for public security and smart city. The emergence of deep learning has greatly promoted the development of anomaly detection and much remarkable work has been presented on this topic. However, the existing approaches for anomaly detection generally encounter problems such as insufficient utilization of motion patterns and instability on different datasets. To improve the performance of anomaly detection in surveillance video, we propose a framework based on bidirectional prediction , which predicts the same target frame by the forward and the backward prediction subnetworks , respectively. Then the loss function is constructed based on the real target frame and its bidirectional prediction frame. Furthermore, we also propose an anomaly score estimation method based on the sliding window scheme which focuses on the foregrounds of the prediction error map. The comparison with the state-of-the-art shows that the proposed model outperforms most competing models on different video surveillance datasets.},
  archive      = {J_ICV},
  author       = {Dongyue Chen and Pengtao Wang and Lingyi Yue and Yuxin Zhang and Tong Jia},
  doi          = {10.1016/j.imavis.2020.103915},
  journal      = {Image and Vision Computing},
  month        = {6},
  pages        = {103915},
  shortjournal = {Image Vis. Comput.},
  title        = {Anomaly detection in surveillance video based on bidirectional prediction},
  volume       = {98},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Energy clustering for unsupervised person re-identification.
<em>ICV</em>, <em>98</em>, 103913. (<a
href="https://doi.org/10.1016/j.imavis.2020.103913">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to the high cost of data annotation in supervised person re-identification (re-ID) methods, unsupervised methods become more attractive in the real world. Recently, the hierarchical clustering serves as a promising unsupervised method. One key factor of hierarchical clustering is the distance measurement strategy. Ideally, a good distance measurement should consider both inter-cluster and intra-cluster distance of all samples. To solve this problem, we propose to use the energy distance to measure inter-cluster distance in hierarchical clustering (E-cluster), and use the sum of squares of deviations (SSD) as a regularization term to measure intra-cluster distance for further performance promotion. We evaluate our method on Market-1501 and DukeMTMC-reID. Extensive experiments show that E-cluster obtains significant improvements over the state-of-the-arts fully unsupervised methods.},
  archive      = {J_ICV},
  author       = {Kaiwei Zeng and Munan Ning and Yaohua Wang and Yang Guo},
  doi          = {10.1016/j.imavis.2020.103913},
  journal      = {Image and Vision Computing},
  month        = {6},
  pages        = {103913},
  shortjournal = {Image Vis. Comput.},
  title        = {Energy clustering for unsupervised person re-identification},
  volume       = {98},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Learning visual variation for object recognition.
<em>ICV</em>, <em>98</em>, 103912. (<a
href="https://doi.org/10.1016/j.imavis.2020.103912">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose visual variation learning to improve object recognition with convolutional neural networks (CNN). While a typical CNN regards visual variations as nuisances and marginalizes them from the data, we speculate that some variations are informative. We study the impact of visual variation as an auxiliary task, during training only, on classification and similarity embedding problems. To train the network, we introduce the iLab-20M dataset, a large-scale controlled parametric dataset of toy vehicle objects under systematic annotated variations of viewpoint, lighting, focal setting, and background. After training, we strip out the network components related to visual variations, and test classification accuracy on images with no visual variation labels. Our experiments on 1.75 million images from iLab-20M show significant improvement in object recognition accuracy , i.e., AlexNet: 84.49% to 91.15%; ResNet : 86.14% to 90.70%; and DenseNet : 85.56% to 91.55%. Our key contribution is that, at the cost of visual variation annotation during training only, CNN enhanced with visual variation learning is able to focus its attention on distinctive features and learn better object representations, reducing classification error rate of Alexnet by 42%, ResNet by 32%, and DenseNet by 41%, without significant sacrificing of training time and model complexity.},
  archive      = {J_ICV},
  author       = {Jatuporn Toy Leksut and Jiaping Zhao and Laurent Itti},
  doi          = {10.1016/j.imavis.2020.103912},
  journal      = {Image and Vision Computing},
  month        = {6},
  pages        = {103912},
  shortjournal = {Image Vis. Comput.},
  title        = {Learning visual variation for object recognition},
  volume       = {98},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Digital video intrusion intelligent detection method based
on narrowband internet of things and its application. <em>ICV</em>,
<em>97</em>, 103914. (<a
href="https://doi.org/10.1016/j.imavis.2020.103914">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes a digital video intrusion detection method based on Narrow Band Internet of Things (NB-IoT), and establishes a digital video intrusion detection system based on NB-IoT network and SVM intelligent classification algorithm . Firstly, the image is preprocessed by gradation processing and threshold transformation to extract the HOG feature extraction of human intrusion behavior in digital video frame images. Then, combined with the human intrusion HOG feature data, the SVM intelligent algorithm is used to classify the human intrusion behavior, so as to accurately classify the movements of walking, jumping, running and waving in video surveillance. Finally, the performance analysis of the algorithm finds that the classification time, classification accuracy and classification false positive rate of the model are tested. The classification time is 40.8 s, the shortest is 27 s, the classification accuracy is 87.65%, and the lowest is 83.7%. The false detection rate is up to 15%, both of which are less than 20%, indicating that the classification method has good accuracy and stability. Comparing the algorithm with other algorithms, the intrusion sensitivity, intrusion specificity and training speed of the model are 93.6%, 94.3%, and 19 s, respectively, which are better than other methods, which indicates that the model has good detection performance in the experimental stage.},
  archive      = {J_ICV},
  author       = {Aimin Yang and Huixiang Liu and Yongjie Chen and Chunying Zhang and Ke Yang},
  doi          = {10.1016/j.imavis.2020.103914},
  journal      = {Image and Vision Computing},
  month        = {5},
  pages        = {103914},
  shortjournal = {Image Vis. Comput.},
  title        = {Digital video intrusion intelligent detection method based on narrowband internet of things and its application},
  volume       = {97},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). IoU-aware single-stage object detector for accurate
localization. <em>ICV</em>, <em>97</em>, 103911. (<a
href="https://doi.org/10.1016/j.imavis.2020.103911">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Single-stage object detectors have been widely applied in many computer vision applications due to their simpleness and high efficiency. However, the low correlation between the classification score and localization accuracy in detection results severely hurts the average precision of the detection model. To solve this problem, an IoU-aware single-stage object detector is proposed in this paper. Specifically, IoU-aware single-stage object detector predicts the IoU for each detected box. Then the predicted IoU is multiplied by the classification score to compute the final detection confidence, which is more correlated with the localization accuracy. The detection confidence is then used as the input of the subsequent NMS and COCO AP computation, which substantially improves the localization accuracy of model. Sufficient experiments on COCO and PASCOL VOC datasets demonstrate the effectiveness of IoU-aware single-stage object detector on improving model&#39;s localization accuracy. Without whistles and bells, the proposed method can substantially improve AP by 1.7%–1.9% and AP75 by 2.2%–2.5% on COCO test-dev . And it can also substantially improve AP by 2.9%–4.4% and AP80, AP90 by 4.6%–10.2% on PASCAL VOC. The source code will be made publicly available.},
  archive      = {J_ICV},
  author       = {Shengkai Wu and Xiaoping Li and Xinggang Wang},
  doi          = {10.1016/j.imavis.2020.103911},
  journal      = {Image and Vision Computing},
  month        = {5},
  pages        = {103911},
  shortjournal = {Image Vis. Comput.},
  title        = {IoU-aware single-stage object detector for accurate localization},
  volume       = {97},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Recent advances in small object detection based on deep
learning: A review. <em>ICV</em>, <em>97</em>, 103910. (<a
href="https://doi.org/10.1016/j.imavis.2020.103910">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Small object detection is a challenging problem in computer vision . It has been widely applied in defense military, transportation, industry, etc. To facilitate in-depth understanding of small object detection, we comprehensively review the existing small object detection methods based on deep learning from five aspects, including multi-scale feature learning , data augmentation , training strategy, context-based detection and GAN-based detection. Then, we thoroughly analyze the performance of some typical small object detection algorithms on popular datasets, such as MS-COCO, PASCAL-VOC. Finally, the possible research directions in the future are pointed out from five perspectives: emerging small object detection datasets and benchmarks, multi-task joint learning and optimization, information transmission , weakly supervised small object detection methods and framework for small object detection task.},
  archive      = {J_ICV},
  author       = {Kang Tong and Yiquan Wu and Fei Zhou},
  doi          = {10.1016/j.imavis.2020.103910},
  journal      = {Image and Vision Computing},
  month        = {5},
  pages        = {103910},
  shortjournal = {Image Vis. Comput.},
  title        = {Recent advances in small object detection based on deep learning: A review},
  volume       = {97},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Co-occurrence of deep convolutional features for image
search. <em>ICV</em>, <em>97</em>, 103909. (<a
href="https://doi.org/10.1016/j.imavis.2020.103909">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image search can be tackled using deep features from pre-trained Convolutional Neural Networks (CNN). The feature map from the last convolutional layer of a CNN encodes descriptive information from which a discriminative global descriptor can be obtained. We propose a new representation of co-occurrences from deep convolutional features to extract additional relevant information from this last convolutional layer. Combining this co-occurrence map with the feature map, we achieve an improved image representation. We present two different methods to get the co-occurrence representation, the first one based on direct aggregation of activations, and the second one, based on a trainable co-occurrence representation. The image descriptors derived from our methodology improve the performance in very well-known image retrieval datasets as we prove in the experiments.},
  archive      = {J_ICV},
  author       = {J.I. Forcen and Miguel Pagola and Edurne Barrenechea and Humberto Bustince},
  doi          = {10.1016/j.imavis.2020.103909},
  journal      = {Image and Vision Computing},
  month        = {5},
  pages        = {103909},
  shortjournal = {Image Vis. Comput.},
  title        = {Co-occurrence of deep convolutional features for image search},
  volume       = {97},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Saddle: Fast and repeatable features with good coverage.
<em>ICV</em>, <em>97</em>, 3807. (<a
href="https://doi.org/10.1016/j.imavis.2019.08.011">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A novel similarity-covariant feature detector that extracts points whose neighborhoods, when treated as a 3D intensity surface, have a saddle-like intensity profile is presented. The saddle condition is verified efficiently by intensity comparisons on two concentric rings that must have exactly two dark-to-bright and two bright-to-dark transitions satisfying certain geometric constraints . Saddle is a fast approximation of Hessian detector as ORB, that implements the FAST detector, is for Harris detector. We propose to use the matching strategy called the first geometric inconsistent with binary descriptors that is suitable for our feature detector, including experiments with fix point descriptors hand-crafted and learned. Experiments show that the Saddle features are general, evenly spread and appearing in high density in a range of images. The Saddle detector is among the fastest proposed. In comparison with detector with similar speed, the Saddle features show superior matching performance on number of challenging datasets. Compared to recently proposed deep-learning based interest point detectors and popular hand-crafted keypoint detectors, evaluated for repeatability in the ApolloScape dataset Huang et al. (2018), the Saddle detectors shows the best performance in most of the street-level view sequences a.k.a. traversals.},
  archive      = {J_ICV},
  author       = {Javier Aldana-Iuit and Dmytro Mishkin and Ondřej Chum and Jiří Matas},
  doi          = {10.1016/j.imavis.2019.08.011},
  journal      = {Image and Vision Computing},
  month        = {5},
  pages        = {3807},
  shortjournal = {Image Vis. Comput.},
  title        = {Saddle: Fast and repeatable features with good coverage},
  volume       = {97},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Automatic recognition of image of abnormal situation in
scenic spots based on internet of things. <em>ICV</em>, <em>96</em>,
103908. (<a href="https://doi.org/10.1016/j.imavis.2020.103908">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Internet of things is an emerging information-aware technology that combines computer vision and artificial intelligence technology to ensure the safety of personnel and facilities in tourist attractions by discovering real-time alarms of abnormal conditions in the monitoring of tourist attractions. Therefore, this paper proposes an automatic image algorithm for tourism scene anomaly based on Internet of things . The algorithm uses the Internet of things intelligent camera in the image acquisition preprocessing platform to collect the tourist scenic spot image. Based on the traditional image segmentation technology, according to the neighborhood-related characteristics of the Markov random field, the dynamic characteristics of continuous frames are added. Reconstruct the Gibbs energy function. The location of the abnormal situation is determined by coding, and a treatment opinion is given. The experimental results show that the proposed algorithm not only considers the spatial information of each pixel and neighboring points, but also adds the time information of successive frames, accumulates the energy values of all the pixels in the whole image and analyzes the data with the energy curve. It can accurately and efficiently identify the abnormal situation images of tourist attractions.},
  archive      = {J_ICV},
  author       = {Yigang Lin},
  doi          = {10.1016/j.imavis.2020.103908},
  journal      = {Image and Vision Computing},
  month        = {4},
  pages        = {103908},
  shortjournal = {Image Vis. Comput.},
  title        = {Automatic recognition of image of abnormal situation in scenic spots based on internet of things},
  volume       = {96},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Category-specific upright orientation estimation for 3D
model classification and retrieval. <em>ICV</em>, <em>96</em>, 103900.
(<a href="https://doi.org/10.1016/j.imavis.2020.103900">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we address a problem of correcting upright orientation of a reconstructed object to search. We first reconstruct an input object appearing in an image sequence, and generate a query shape using multi-view object co-segmentation. In the next phase, we utilize the Convolutional Neural Network (CNN) architecture to determine category-specific upright orientation of the queried shape for 3D model classification and retrieval. As a practical application of our system, a shape style and a pose from an inferred category and up-vector are obtained by comparing 3D shape similarity with candidate 3D models and aligning its projections with a set of 2D co-segmentation masks. We quantitatively and qualitatively evaluate the presented system with more than 720 upfront-aligned 3D models and five sets of multi-view image sequences.},
  archive      = {J_ICV},
  author       = {Seong-heum Kim and Youngbae Hwang and In So Kweon},
  doi          = {10.1016/j.imavis.2020.103900},
  journal      = {Image and Vision Computing},
  month        = {4},
  pages        = {103900},
  shortjournal = {Image Vis. Comput.},
  title        = {Category-specific upright orientation estimation for 3D model classification and retrieval},
  volume       = {96},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A high-efficiency energy and storage approach for IoT
applications of facial recognition. <em>ICV</em>, <em>96</em>, 103899.
(<a href="https://doi.org/10.1016/j.imavis.2020.103899">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This work introduces a high-efficiency approach for face recognition applications based on features using a recent algorithm called Floor of Log (FoL). The advantage of this method is the reduction of storage and energy, maintaining accuracy. K-Nearest Neighbors and Support Vector Machine algorithm was applied to learn the better parameter of the FoL algorithm using cross-validation. Accuracy and the size after the compression process were adopted to evaluate the proposed algorithm. The FoL was tested in CelebA, Extended YaleB, AR, and LFW face datasets obtaining the same or better results when compared with the approach using the same classifiers with uncompressed features, but with a reduction of 86 to 91% compared to the original data size. The proposed method of this work presents a robust and straightforward algorithm of compression of features for face recognition applications. The FoL is a new supervised compression algorithm that can be adapted to achieve great results and integrated with edge computing systems.},
  archive      = {J_ICV},
  author       = {Solon A. Peixoto and Francisco F.X. Vasconcelos and Matheus T. Guimarães and Aldísio G. Medeiros and Paulo A.L. Rego and Aloísio V. Lira Neto and Victor Hugo C. de Albuquerque and Pedro P. Rebouças Filho},
  doi          = {10.1016/j.imavis.2020.103899},
  journal      = {Image and Vision Computing},
  month        = {4},
  pages        = {103899},
  shortjournal = {Image Vis. Comput.},
  title        = {A high-efficiency energy and storage approach for IoT applications of facial recognition},
  volume       = {96},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A review on object pose recovery: From 3D bounding box
detectors to full 6D pose estimators. <em>ICV</em>, <em>96</em>, 103898.
(<a href="https://doi.org/10.1016/j.imavis.2020.103898">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Object pose recovery has gained increasing attention in the computer vision field as it has become an important problem in rapidly evolving technological areas related to autonomous driving , robotics, and augmented reality . Existing review-related studies have addressed the problem at visual level in 2D, going through the methods which produce 2D bounding boxes of objects of interest in RGB images . The 2D search space is enlarged either using the geometry information available in the 3D space along with RGB (Mono/Stereo) images, or utilizing depth data from LIDAR sensors and/or RGB-D cameras. 3D bounding box detectors, producing category-level amodal 3D bounding boxes, are evaluated on gravity aligned images, while full 6D object pose estimators are mostly tested at instance-level on the images where the alignment constraint is removed. Recently, 6D object pose estimation is tackled at the level of categories. In this paper, we present the first comprehensive and most recent review of the methods on object pose recovery, from 3D bounding box detectors to full 6D pose estimators. The methods mathematically model the problem as a classification, regression, classification &amp; regression, template matching , and point-pair feature matching task. Based on this, a mathematical-model-based categorization of the methods is established. Datasets used for evaluating the methods are investigated with respect to the challenges, and evaluation metrics are studied. Quantitative results of experiments in the literature are analyzed to show which category of methods best performs across what types of challenges. The analyses are further extended comparing two methods, which are our own implementations, so that the outcomes from the public results are further solidified. Current position of the field is summarized regarding object pose recovery, and possible research directions are identified.},
  archive      = {J_ICV},
  author       = {Caner Sahin and Guillermo Garcia-Hernando and Juil Sock and Tae-Kyun Kim},
  doi          = {10.1016/j.imavis.2020.103898},
  journal      = {Image and Vision Computing},
  month        = {4},
  pages        = {103898},
  shortjournal = {Image Vis. Comput.},
  title        = {A review on object pose recovery: From 3D bounding box detectors to full 6D pose estimators},
  volume       = {96},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Recovering facial reflectance and geometry from multi-view
images. <em>ICV</em>, <em>96</em>, 103897. (<a
href="https://doi.org/10.1016/j.imavis.2020.103897">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {While the problem of estimating shapes and diffuse reflectances of human faces from images has been extensively studied, there is relatively less work done on recovering the specular albedo . This paper presents a lightweight solution for inferring photorealistic facial reflectance and geometry. Our system processes video streams from two views of a subject, and outputs two reflectance maps for diffuse and specular albedos, as well as a vector map of surface normals. A model-based optimization approach is used, consisting of the three stages of multi-view face model fitting, facial reflectance inference and facial geometry refinement. Our approach is based on a novel formulation built upon the 3D morphable model (3DMM) for representing 3D textured faces in conjunction with the Blinn-Phong reflection model. It has the advantage of requiring only a simple setup with two video streams, and is able to exploit the interaction between the diffuse and specular reflections across multiple views as well as time frames. As a result, the method is able to reliably recover high-fidelity facial reflectance and geometry, which facilitates various applications such as generating photorealistic facial images under new viewpoints or illumination conditions .},
  archive      = {J_ICV},
  author       = {Guoxian Song and Jianmin Zheng and Jianfei Cai and Tat-Jen Cham},
  doi          = {10.1016/j.imavis.2020.103897},
  journal      = {Image and Vision Computing},
  month        = {4},
  pages        = {103897},
  shortjournal = {Image Vis. Comput.},
  title        = {Recovering facial reflectance and geometry from multi-view images},
  volume       = {96},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Local-adaptive and outlier-tolerant image alignment using
RBF approximation. <em>ICV</em>, <em>95</em>, 103890. (<a
href="https://doi.org/10.1016/j.imavis.2020.103890">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image alignment is a crucial step to generate a high quality panorama. The state-of-the-art approaches use local-adaptive transformations to deal with multi-view parallax, but still suffer from unreliable feature correspondences and high computational cost. In this paper, we propose a local-adaptive and outlier-tolerant image alignment method using RBF (radial basis function) approximation. To eliminate the visible artifacts, the input images are warped according to a constructed projection error function, whose parameters are estimated by solving a linear system . The outliers are efficiently removed by screening out the abnormal weights of RBFs, such that better alignment quality can be achieved compared to the existing approaches. Moreover, a weight assignment strategy is introduced to further address the overfitting issues caused by extrapolation, and hence the global projectivity can be well preserved. The proposed method is computationally efficient, whose performance is verified by comparative experiments on several challenging cases.},
  archive      = {J_ICV},
  author       = {Jing Li and Baosong Deng and Maojun Zhang and Ye Yan and Zhengming Wang},
  doi          = {10.1016/j.imavis.2020.103890},
  journal      = {Image and Vision Computing},
  month        = {3},
  pages        = {103890},
  shortjournal = {Image Vis. Comput.},
  title        = {Local-adaptive and outlier-tolerant image alignment using RBF approximation},
  volume       = {95},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Unsupervised domain adaptation for mobile semantic
segmentation based on cycle consistency and feature alignment.
<em>ICV</em>, <em>95</em>, 103889. (<a
href="https://doi.org/10.1016/j.imavis.2020.103889">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The supervised training of deep networks for semantic segmentation requires a huge amount of labeled real world data. To solve this issue, a commonly exploited workaround is to use synthetic data for training, but deep networks show a critical performance drop when analyzing data with slightly different statistical properties with respect to the training set. In this work, we propose a novel Unsupervised Domain Adaptation (UDA) strategy to address the domain shift issue between real world and synthetic representations. An adversarial model , based on the cycle consistency framework, performs the mapping between the synthetic and real domain. The data is then fed to a MobileNet-v2 architecture that performs the semantic segmentation task . An additional couple of discriminators , working at the feature level of the MobileNet-v2, allows to better align the features of the two domain distributions and to further improve the performance. Finally, the consistency of the semantic maps is exploited. After an initial supervised training on synthetic data, the whole UDA architecture is trained end-to-end considering all its components at once. Experimental results show how the proposed strategy is able to obtain impressive performance in adapting a segmentation network trained on synthetic data to real world scenarios. The usage of the lightweight MobileNet-v2 architecture allows its deployment on devices with limited computational resources as the ones employed in autonomous vehicles.},
  archive      = {J_ICV},
  author       = {Marco Toldo and Umberto Michieli and Gianluca Agresti and Pietro Zanuttigh},
  doi          = {10.1016/j.imavis.2020.103889},
  journal      = {Image and Vision Computing},
  month        = {3},
  pages        = {103889},
  shortjournal = {Image Vis. Comput.},
  title        = {Unsupervised domain adaptation for mobile semantic segmentation based on cycle consistency and feature alignment},
  volume       = {95},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Attention-guided RGBD saliency detection using appearance
information. <em>ICV</em>, <em>95</em>, 103888. (<a
href="https://doi.org/10.1016/j.imavis.2020.103888">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most of the deep convolutional neural networks (CNNs) based RGBD saliency models either regard the RGB and depth cues as the same status or trust the depth information excessively. However, they ignore that the low-quality depth map is an interference factor and the multi-level deep features that originated from RGB images contain abundant appearance information. Therefore, we propose a novel RGBD saliency model, where the attention-guided bottom-up and top-down modules are powerfully combined by using multi-level deep RGB features, to utilize the deep RGB and depth features in a sufficient and appropriate way. Concretely, a two-stream structure based bottom-up module is first constructed to dig and fuse the RGB and depth information, yielding the fused deep feature. Besides, the module embeds the depth cue based attention maps to guide the indication of salient objects. Then, considering the abundant appearance information, a top-down module is deployed to perform coarse-to-fine saliency inference, where the fused deep feature is progressively integrated with appearance information. Similarly, the attention map is also inserted into this module for locating salient objects. Extensive experiments are performed on five public RGBD datasets and the corresponding experimental results firmly demonstrate the effectiveness and superiority of our model when compared with the state-of-the-art RGBD saliency models.},
  archive      = {J_ICV},
  author       = {Xiaofei Zhou and Gongyang Li and Chen Gong and Zhi Liu and Jiyong Zhang},
  doi          = {10.1016/j.imavis.2020.103888},
  journal      = {Image and Vision Computing},
  month        = {3},
  pages        = {103888},
  shortjournal = {Image Vis. Comput.},
  title        = {Attention-guided RGBD saliency detection using appearance information},
  volume       = {95},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). EML-NET: An expandable multi-layer NETwork for saliency
prediction. <em>ICV</em>, <em>95</em>, 103887. (<a
href="https://doi.org/10.1016/j.imavis.2020.103887">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing advanced saliency systems have been proposed using Convolutional Neural Networks (CNNs), the prior knowledge of objectness is crucial for saliency detection . However, we show that the use of objectness may also limit the power of CNNs on the images which do not contain a salient object. Besides, one previous study has shown applying a deeper CNN model may not improve the performance, the reason behind could be due to the limited training data . In this work, we aim at investigating the effect of prior knowledge from other domains and proposing a multi-modality system. Our work shows the depth of the model still plays an important role in saliency detection when the training data is large enough. To do this in a sophisticated manner can be complex, and also result in unwieldy networks or produce competing objectives that are hard to balance. For the scalability, our multi-modality system is trained in an almost end-to-end piece-wise fashion. The encoder and decoder components are separately trained to deal with complexity tied to the computational paradigm and required space. Therefore, our system can be easily extended to include a variety of prior knowledge for saliency detection. Besides, we also propose a combined saliency loss based on modifications of Pearson correlation and normalized scanpath saliency. Our experiment shows the combined loss can train a CNN model more comprehensively for saliency detection. We denote our expandable multi-layer network as EML-NET and our method achieves the state-of-the-art results on the public saliency benchmarks, SALICON, MIT300 and CAT2000.},
  archive      = {J_ICV},
  author       = {Sen Jia and Neil D.B. Bruce},
  doi          = {10.1016/j.imavis.2020.103887},
  journal      = {Image and Vision Computing},
  month        = {3},
  pages        = {103887},
  shortjournal = {Image Vis. Comput.},
  title        = {EML-NET: An expandable multi-layer NETwork for saliency prediction},
  volume       = {95},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). GANILLA: Generative adversarial networks for image to
illustration translation. <em>ICV</em>, <em>95</em>, 103886. (<a
href="https://doi.org/10.1016/j.imavis.2020.103886">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we explore illustrations in children&#39;s books as a new domain in unpaired image-to-image translation. We show that although the current state-of-the-art image-to-image translation models successfully transfer either the style or the content, they fail to transfer both at the same time. We propose a new generator network to address this issue and show that the resulting network strikes a better balance between style and content. There are no well-defined or agreed-upon evaluation metrics for unpaired image-to-image translation. So far, the success of image translation models has been based on subjective, qualitative visual comparison on a limited number of images. To address this problem, we propose a new framework for the quantitative evaluation of image-to-illustration models, where both content and style are taken into account using separate classifiers. In this new evaluation framework, our proposed model performs better than the current state-of-the-art models on the illustrations dataset. Our code and pretrained models can be found at https://github.com/giddyyupp/ganilla .},
  archive      = {J_ICV},
  author       = {Samet Hicsonmez and Nermin Samet and Emre Akbas and Pinar Duygulu},
  doi          = {10.1016/j.imavis.2020.103886},
  journal      = {Image and Vision Computing},
  month        = {3},
  pages        = {103886},
  shortjournal = {Image Vis. Comput.},
  title        = {GANILLA: Generative adversarial networks for image to illustration translation},
  volume       = {95},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A complementary regression network for accurate face
alignment. <em>ICV</em>, <em>95</em>, 103883. (<a
href="https://doi.org/10.1016/j.imavis.2020.103883">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes a complementary regression network (CRN) that combines global and local regression methods to align faces. A global regression network (GRN) generates the coordinates of facial landmark points directly such that all facial feature points are fitted to the input face on the whole and a local regression network (LRN) generates the heatmap of facial landmark points such that each channel localizes the detail of its facial landmark point well. The CRN converts the GRN&#39;s coordinates to another heatmap, then uses with the LRN&#39;s heatmap to get the final facial landmark points. The CRN works complementarily such that the GRN&#39;s overall fitting tendency compensates for the LRN&#39;s poor alignment caused by missing local information , whereas the LRN&#39;s detailed representation compensates for the GRN&#39;s poor alignment caused by global miss-fitting. We conducted several experiments on the 300-W public dataset, the 300-W private dataset, and the Menpo dataset and the proposed CRN achieved 3.14%, 3.74%, and 1.996% the-state-of-art face alignment accuracy in terms of percentage of normalized mean error, respectively.},
  archive      = {J_ICV},
  author       = {Hyunsung Park and Daijin Kim},
  doi          = {10.1016/j.imavis.2020.103883},
  journal      = {Image and Vision Computing},
  month        = {3},
  pages        = {103883},
  shortjournal = {Image Vis. Comput.},
  title        = {A complementary regression network for accurate face alignment},
  volume       = {95},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Salient object detection based on backbone enhanced network.
<em>ICV</em>, <em>95</em>, 103876. (<a
href="https://doi.org/10.1016/j.imavis.2020.103876">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Convolutional Neural Networks (CNNs) with encoder-decoder architecture has shown powerful ability in semantic segmentation and it has also been applied in saliency detection . In most researches, the parameters of the backbone network which have been pre-trained on the ImageNet dataset will be retrained using the new training dataset to let CNNs adapt to the new task better. But the retraining will weaken generalization of the pre-trained backbone network and result in over-fitting, especially when the scale of the new training data is not very large. To make a balance between generalization and precision, and to further improve the performance of the CNNs with encoder-decoder architecture in salient object detection , we proposed a framework with enhanced backbone network (BENet). A encoder with structure of dual backbone networks (DBNs) is adopted in BENet to extract more diverse feature maps. In addition, BENet includes a connection module based on improved Res2Net to efficiently fuse feature maps from the two backbone networks and a decoder based on weighted multi-scale feedback module (WMFM) to perform synchronous learning. Our approach is extensively evaluated on six public datasets, and experimental results show significant and consistent improvements over the state-of-the-art methods without any additional supervision.},
  archive      = {J_ICV},
  author       = {Ronghua Luo and Huailin Huang and WeiZeng Wu},
  doi          = {10.1016/j.imavis.2020.103876},
  journal      = {Image and Vision Computing},
  month        = {3},
  pages        = {103876},
  shortjournal = {Image Vis. Comput.},
  title        = {Salient object detection based on backbone enhanced network},
  volume       = {95},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Person re-identification with expanded neighborhoods
distance re-ranking. <em>ICV</em>, <em>95</em>, 103875. (<a
href="https://doi.org/10.1016/j.imavis.2020.103875">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the person re-identification (re-ID) community, pedestrians often have great changes in appearance, and there are many similar persons, which incurs will degrades the accuracy. Re-ranking is an effective method to solve these problems, this paper proposes an expanded neighborhoods distance (END) to re-rank the re-ID results. We assume that if the two persons in different image are same, their initial ranking lists and two-level neighborhoods will be very similar when they are taken as the query. Our method follows the principle of similarity, and selects expanded neighborhoods in initial ranking list to calculate the END distance. Final distance is calculated as the combination of the END distance and Jaccard distance. Experiments on Market-1501, DukeMTMC-reID and CUHK03 datasets confirm the effectiveness of the novel re-ranking method in this article. Compare with re-ID baseline, the proposed method in this paper increases mAP by 14.2% on Market-1501 and Rank1 by 12.9% on DukeMTMC-reID.},
  archive      = {J_ICV},
  author       = {Jingyi Lv and Zhiyong Li and Ke Nai and Ying Chen and Jin Yuan},
  doi          = {10.1016/j.imavis.2020.103875},
  journal      = {Image and Vision Computing},
  month        = {3},
  pages        = {103875},
  shortjournal = {Image Vis. Comput.},
  title        = {Person re-identification with expanded neighborhoods distance re-ranking},
  volume       = {95},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Depth-guided view synthesis for light field reconstruction
from a single image. <em>ICV</em>, <em>95</em>, 103874. (<a
href="https://doi.org/10.1016/j.imavis.2020.103874">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Light field imaging has recently become a promising technology for 3D rendering and displaying. However, capturing real-world light field images still faces many challenges in both the quantity and quality. In this paper, we develop a learning based technique to reconstruct light field from a single 2D RGB image . It includes three steps: unsupervised monocular depth estimation, view synthesis and depth-guided view inpainting. We first propose a novel monocular depth estimation network to predict disparity maps of each sub-aperture views from the central view of light field. Then we synthesize the initial sub-aperture views by using the warping scheme. Considering that occlusion makes synthesis ambiguous for pixels invisible in the central view, we present a simple but effective fully convolutional network (FCN) for view inpainting. Note that the proposed network architecture is a general framework for light field reconstruction, which can be extended to take a sparse set of views as input without changing any structure or parameters of the network. Comparison experiments demonstrate that our method outperforms the state-of-the-art light field reconstruction methods with single-view input, and achieves comparable results with the multi-input methods.},
  archive      = {J_ICV},
  author       = {Wenhui Zhou and Gaomin Liu and Jiangwei Shi and Hua Zhang and Guojun Dai},
  doi          = {10.1016/j.imavis.2020.103874},
  journal      = {Image and Vision Computing},
  month        = {3},
  pages        = {103874},
  shortjournal = {Image Vis. Comput.},
  title        = {Depth-guided view synthesis for light field reconstruction from a single image},
  volume       = {95},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A riemannian approach for free-space extraction and path
planning using catadioptric omnidirectional vision. <em>ICV</em>,
<em>95</em>, 103872. (<a
href="https://doi.org/10.1016/j.imavis.2020.103872">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a Riemannian approach for free-space extraction and path planning using color catadioptric vision. The problem is formulated considering color catadioptric images as Riemannian manifolds and solved using the Riemannian Eikonal equation with an anisotropic fast marching numerical scheme . This formulation allows the integration of adapted color and spatial metrics in an incremental process. First, the traversable ground (namely free-space ) is delimited using a color structure tensor built on the multi-dimensional components of the catadioptric image. Then, the Eikonal equation is solved in the image plane incorporating a generic metric tensor for central catadioptric systems . This built Riemannian metric copes with the geometric distortions in the catadioptric image plane introduced by the curved mirror in order to compute the geodesic distance map and the shortest path between image points . We present comparative results using Euclidean and Riemannian distance transforms and show the effectiveness of the Riemannian approach to produce safest path planning.},
  archive      = {J_ICV},
  author       = {Fatima Aziz and Ouiddad Labbani-Igbida and Amina Radgui and Ahmed Tamtaoui},
  doi          = {10.1016/j.imavis.2020.103872},
  journal      = {Image and Vision Computing},
  month        = {3},
  pages        = {103872},
  shortjournal = {Image Vis. Comput.},
  title        = {A riemannian approach for free-space extraction and path planning using catadioptric omnidirectional vision},
  volume       = {95},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). SAANet: Spatial adaptive alignment network for object
detection in automatic driving. <em>ICV</em>, <em>94</em>, 103873. (<a
href="https://doi.org/10.1016/j.imavis.2020.103873">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Both images and point clouds are beneficial for object detection in a visual navigation module for autonomous driving . The spatial relationships between different objects at different times in a bimodal space can vary significantly. It is difficult to combine bimodal descriptions into a unified model to effectively detect objects in an efficient amount of time. In addition, conventional voxelization methods resolve point clouds into voxels at a global level, and often overlook local attributes of the voxels. To address these problems, we propose a novel fusion-based deep framework named SAANet. SAANet utilizes a spatial adaptive alignment (SAA) module to align point cloud features and image features , by automatically discovering the complementary information between point clouds and images. Specifically, we transform the point clouds into 3D voxels, and introduce local orientation encoding to represent the point clouds. Then, we use a sparse convolutional neural network to learn a point cloud feature. Simultaneously, a ResNet-like 2D convolutional neural network is used to extract an image feature. Next, the point cloud feature and image feature are fused by our SAA block to derive a comprehensive feature. Then, the labels and 3D boxes for objects are learned using a multi-task learning network. Finally, an experimental evaluation on the KITTI benchmark demonstrates the advantages of our method in terms of average precision and inference time, as compared to previous state-of-the-art results for 3D object detection .},
  archive      = {J_ICV},
  author       = {Junying Chen and Tongyao Bai},
  doi          = {10.1016/j.imavis.2020.103873},
  journal      = {Image and Vision Computing},
  month        = {2},
  pages        = {103873},
  shortjournal = {Image Vis. Comput.},
  title        = {SAANet: Spatial adaptive alignment network for object detection in automatic driving},
  volume       = {94},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Bottom-up unsupervised image segmentation using FC-dense
u-net based deep representation clustering and multidimensional feature
fusion based region merging. <em>ICV</em>, <em>94</em>, 103871. (<a
href="https://doi.org/10.1016/j.imavis.2020.103871">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent advances in system resources provide ease in the applicability of deep learning approaches in computer vision . In this paper, we propose a deep learning-based unsupervised image segmentation approach for natural image segmentation . Image segmentation aims to transform an image into regions, representing various objects in the image. Our method consists of a fully convolutional dense network-based unsupervised deep representation oriented clustering, followed by shallow features based high-dimensional region merging to produce the final segmented image . We evaluate our proposed approach on the BSD300 database and perform a comparison with several classical and some recent deep learning-based unsupervised segmentation methods . The experimental results represent that the proposed method is comparable and confirm the efficacy of the proposed approach.},
  archive      = {J_ICV},
  author       = {Zubair Khan and Jie Yang},
  doi          = {10.1016/j.imavis.2020.103871},
  journal      = {Image and Vision Computing},
  month        = {2},
  pages        = {103871},
  shortjournal = {Image Vis. Comput.},
  title        = {Bottom-up unsupervised image segmentation using FC-dense u-net based deep representation clustering and multidimensional feature fusion based region merging},
  volume       = {94},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Collective sports: A multi-task dataset for collective
activity recognition. <em>ICV</em>, <em>94</em>, 103870. (<a
href="https://doi.org/10.1016/j.imavis.2020.103870">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Collective activity recognition is an important subtask of human action recognition , where the existing datasets are mostly limited. In this paper, we look into this issue and introduce the “Collective Sports (C-Sports)” dataset, which is a novel benchmark dataset for multi-task recognition of both collective activity and sports categories. Various state-of-the-art techniques are evaluated on this dataset, together with multi-task variants which demonstrate increased performance. From the experimental results, we can say that while sports categories of the videos are inferred accurately, there is still room for improvement for collective activity recognition, especially regarding the generalization ability beyond previously unseen sports categories. In order to evaluate this ability, we introduce a novel evaluation protocol called unseen sports , where the training and test are carried out on disjoint sets of sports categories. The relatively lower recognition performances in this evaluation protocol indicate that the recognition models tend to be influenced by the surrounding context, rather than focusing on the essence of the collective activities. We believe that C-Sports dataset will stir further interest in this research direction.},
  archive      = {J_ICV},
  author       = {Cemil Zalluhoglu and Nazli Ikizler-Cinbis},
  doi          = {10.1016/j.imavis.2020.103870},
  journal      = {Image and Vision Computing},
  month        = {2},
  pages        = {103870},
  shortjournal = {Image Vis. Comput.},
  title        = {Collective sports: A multi-task dataset for collective activity recognition},
  volume       = {94},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Learning reliable-spatial and spatial-variation
regularization correlation filters for visual tracking. <em>ICV</em>,
<em>94</em>, 103869. (<a
href="https://doi.org/10.1016/j.imavis.2020.103869">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Single-object tracking is a significant and challenging computer vision problem. Recently, discriminative correlation filters (DCF) have shown excellent performance. But there is a theoretical defects that the boundary effect, caused by the periodic assumption of training samples , greatly limit the tracking performance. Spatially regularized DCF (SRDCF) introduces a spatial regularization to penalize the filter coefficients depending on their spatial location , which improves the tracking performance a lot. However, this simple regularization strategy implements unequal penalties for the target area filter coefficients , which makes the filter learn a distorted object appearance model. In this paper, a novel spatial regularization strategy is proposed, utilizing a reliability map to approximate the target area and to keep the penalty coefficients of relevant region consistent. Besides, we introduce a spatial variation regularization component that the second-order difference of the filter, which smooths changes of filter coefficients to prevent the filter over-fitting current frame. Furthermore, an efficient optimization algorithm called alternating direction method of multipliers (ADMM) is developed. Comprehensive experiments are performed on three benchmark datasets: OTB-2013, OTB-2015 and TempleColor-128, and our algorithm achieves a more favorable performance than several state-of-the-art methods. Compared with SRDCF, our approach obtains an absolute gain of 6.6% and 5.1% in mean distance precision on OTB-2013 and OTB-2015, respectively. Our approach runs in real-time on a CPU.},
  archive      = {J_ICV},
  author       = {Hengcheng Fu and Yihong Zhang and Wuneng Zhou and Xiaofeng Wang and Huanlong Zhang},
  doi          = {10.1016/j.imavis.2020.103869},
  journal      = {Image and Vision Computing},
  month        = {2},
  pages        = {103869},
  shortjournal = {Image Vis. Comput.},
  title        = {Learning reliable-spatial and spatial-variation regularization correlation filters for visual tracking},
  volume       = {94},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Single image dehazing via a dual-fusion method.
<em>ICV</em>, <em>94</em>, 103868. (<a
href="https://doi.org/10.1016/j.imavis.2019.103868">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Single image dehazing is a challenging task because of the hue and brightness distortion problems. In this paper, we propose a dual-fusion method for single image dehazing. By a segmentation method creating two divided regions, the sky and non-sky regions can be obtained. To properly optimize the transmission, a multi-region fusion method is proposed for single image smooth. An exposure fusion method is constructed by the brightness transform function to effectively remove the haze from a single image. Experimental results show that this method outperforms state-of-the-art dehazing methods in terms of both efficiency and the dehazing effect.},
  archive      = {J_ICV},
  author       = {Yin Gao and Qiming Li and Jun Li},
  doi          = {10.1016/j.imavis.2019.103868},
  journal      = {Image and Vision Computing},
  month        = {2},
  pages        = {103868},
  shortjournal = {Image Vis. Comput.},
  title        = {Single image dehazing via a dual-fusion method},
  volume       = {94},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Online maximum a posteriori tracking of multiple objects
using sequential trajectory prior. <em>ICV</em>, <em>94</em>, 103867.
(<a href="https://doi.org/10.1016/j.imavis.2019.103867">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we address the problem of online multi-object tracking based on the Maximum a Posteriori (MAP) framework. Given the observations up to the current frame, we estimate the optimal object trajectories via two MAP estimation stages: object detection and data association . By introducing the sequential trajectory prior, i.e. , the prior information from previous frames about “good” trajectories, into the two MAP stages, the inference of optimal detections is refined and the association correctness between trajectories and detections is enhanced. Furthermore, the sequential trajectory prior allows the two MAP stages to interact with each other in a sequential manner, which jointly optimizes the detections and trajectories to facilitate online multi-object tracking. Compared with existing methods, our approach is able to alleviate the association ambiguity caused by noisy detections and frequent inter-object interactions without using sophisticated association likelihood models. The experiments on publicly available challenging datasets demonstrate that our approach provides superior tracking performance over state-of-the-art algorithms in various complex scenes.},
  archive      = {J_ICV},
  author       = {Min Yang and Mingtao Pei and Yunde Jia},
  doi          = {10.1016/j.imavis.2019.103867},
  journal      = {Image and Vision Computing},
  month        = {2},
  pages        = {103867},
  shortjournal = {Image Vis. Comput.},
  title        = {Online maximum a posteriori tracking of multiple objects using sequential trajectory prior},
  volume       = {94},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Post-mortem iris recognition with deep-learning-based image
segmentation. <em>ICV</em>, <em>94</em>, 103866. (<a
href="https://doi.org/10.1016/j.imavis.2019.103866">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes the first known to us iris recognition methodology designed specifically for post-mortem samples. We propose to use deep learning-based iris segmentation models to extract highly irregular iris texture areas in post-mortem iris images . We show how to use segmentation masks predicted by neural networks in conventional, Gabor-based iris recognition method, which employs circular approximations of the pupillary and limbic iris boundaries. As a whole, this method allows for a significant improvement in post-mortem iris recognition accuracy over the methods designed only for ante-mortem irises, including the academic OSIRIS and commercial IriCore implementations. The proposed method reaches the EER less than 1% for samples collected up to 10 hours after death, when compared to 16.89% and 5.37% of EER observed for OSIRIS and IriCore, respectively. For samples collected up to 369 h post-mortem, the proposed method achieves the EER 21.45%, while 33.59% and 25.38% are observed for OSIRIS and IriCore, respectively. Additionally, the method is tested on a database of iris images collected from ophthalmology clinic patients , for which it also offers an advantage over the two other algorithms. This work is the first step towards post-mortem-specific iris recognition, which increases the chances of identification of deceased subjects in forensic investigations . The new database of post-mortem iris images acquired from 42 subjects, as well as the deep learning-based segmentation models are made available along with the paper, to ensure all the results presented in this manuscript are reproducible.},
  archive      = {J_ICV},
  author       = {Mateusz Trokielewicz and Adam Czajka and Piotr Maciejewicz},
  doi          = {10.1016/j.imavis.2019.103866},
  journal      = {Image and Vision Computing},
  month        = {2},
  pages        = {103866},
  shortjournal = {Image Vis. Comput.},
  title        = {Post-mortem iris recognition with deep-learning-based image segmentation},
  volume       = {94},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Variational shape prior segmentation with an initial curve
based on image registration technique. <em>ICV</em>, <em>94</em>,
103865. (<a href="https://doi.org/10.1016/j.imavis.2019.103865">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In general images, it is practically hard to distinguish only the desired object using the conventional image segmentation methods . In many cases, we can segment the desired object by using the shape information of the object in addition to the standard image segmentation. Chan and Zhu&#39;s model is not robust to the intensity changes of objects. In this paper, we propose a novel model for the shape prior segmentation that produces robust results using the hierarchical image segmentation and an attraction term. Moreover, we adopt an image registration technique and a multi-region image segmentation to get an initial for a given shape prior. Finally, we consider the free-form deformation in obtaining the shape function from the reference shape prior for real-world images. Numerical experiments demonstrate the results independent of intensities of objects and the location of the reference shape prior. All numerical calculations are automatic and progress without any user input.},
  archive      = {J_ICV},
  author       = {Doyeob Yeo and Chang-Ock Lee},
  doi          = {10.1016/j.imavis.2019.103865},
  journal      = {Image and Vision Computing},
  month        = {2},
  pages        = {103865},
  shortjournal = {Image Vis. Comput.},
  title        = {Variational shape prior segmentation with an initial curve based on image registration technique},
  volume       = {94},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Flow adaptive video object segmentation. <em>ICV</em>,
<em>94</em>, 103864. (<a
href="https://doi.org/10.1016/j.imavis.2019.103864">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We tackle the task of semi-supervised video object segmentation, i.e. pixel-level object classification of the images in video sequences using very limited ground truth training data of its corresponding video. We present FLow Adaptive Video Object Segmentation, an efficient pipeline based on a novel online adaptation algorithm that utilizes optical flow, capable of tracking objects effectively throughout videos. Comparing with most of the recent deep learning based approaches that trade efficiency for accuracy, we provide extensive complexity analysis and additionally demonstrate that FLAVOS is natural for real world applications by introducing an interactive pipeline that enables the user to provide feedback for online training. Our method achieves state-of-the-art accuracy on three challenging benchmark datasets and nearly ground-truth level segmentation results with interactive user feedback.},
  archive      = {J_ICV},
  author       = {Fanqing Lin and Yao Chou and Tony Martinez},
  doi          = {10.1016/j.imavis.2019.103864},
  journal      = {Image and Vision Computing},
  month        = {2},
  pages        = {103864},
  shortjournal = {Image Vis. Comput.},
  title        = {Flow adaptive video object segmentation},
  volume       = {94},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A new cast shadow detection method for traffic surveillance
video analysis using color and statistical modeling. <em>ICV</em>,
<em>94</em>, 103863. (<a
href="https://doi.org/10.1016/j.imavis.2019.103863">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In traffic surveillance video analysis systems, the cast shadows of vehicles often have a negative effect on video analysis results. A novel cast shadow detection framework, which consists of a new foreground detection method and a cast shadow detection method, is presented in this paper to detect and remove the cast shadows from the foreground. The new foreground detection method applies an innovative Global Foreground Modeling (GFM) method, a Gaussian mixture model or GMM, and the Bayes classifier for foreground and background classification. While the GFM method is for global foreground modeling, the GMM is for local background modeling, and the Bayes classifier applies both the foreground and the background models for foreground detection. The rationale of the GFM method stems from the observation that the foreground objects often appear in recent frames and their trajectories often lead them to different locations in these frames. As a result, the statistical models used to characterize the foreground objects should not be pixel based or locally defined. The cast shadow detection method contains four hierarchical steps. First, a set of new chromatic criteria is presented to detect the candidate shadow pixels in the HSV color space. Second, a new shadow region detection method is proposed to cluster the candidate shadow pixels into shadow regions. Third, a statistical shadow model, which uses a single Gaussian distribution to model the shadow class, is presented for classifying shadow pixels. Fourth, an aggregated shadow detection method is presented for final shadow detection. Experiments using the public video data ‘Highway-1’ and ‘Highway-3’, and the New Jersey Department of Transportation (NJDOT) real traffic video sequences show the feasibility of the proposed method. In particular, the proposed method achieves better shadow detection performance than the popular shadow detection methods, and is able to improve the traffic video analysis results.},
  archive      = {J_ICV},
  author       = {Hang Shi and Chengjun Liu},
  doi          = {10.1016/j.imavis.2019.103863},
  journal      = {Image and Vision Computing},
  month        = {2},
  pages        = {103863},
  shortjournal = {Image Vis. Comput.},
  title        = {A new cast shadow detection method for traffic surveillance video analysis using color and statistical modeling},
  volume       = {94},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Multi-feature fusion for image retrieval using constrained
dominant sets. <em>ICV</em>, <em>94</em>, 103862. (<a
href="https://doi.org/10.1016/j.imavis.2019.103862">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Aggregating different image features for image retrieval has recently shown its effectiveness. While highly effective, though, the question of how to uplift the impact of the best features for a specific query image persists as an open computer vision problem. In this paper, we propose a computationally efficient approach to fuse several hand-crafted and deep features, based on the probabilistic distribution of a given membership score of a constrained cluster in an unsupervised manner . First, we introduce an incremental nearest neighbor (NN) selection method, whereby we dynamically select k-NN to the query. We then build several graphs from the obtained NN sets and employ constrained dominant sets (CDS) on each graph G to assign edge weights which consider the intrinsic manifold structure of the graph, and detect false matches to the query. Finally, we elaborate the computation of feature positive-impact weight (PIW) based on the dispersive degree of the characteristics vector . To this end, we exploit the entropy of a cluster membership-score distribution. In addition, the final NN set bypasses a heuristic voting scheme. Experiments on several retrieval benchmark datasets show that our method can improve the state-of-the-art result.},
  archive      = {J_ICV},
  author       = {Leulseged Tesfaye Alemu and Marcello Pelillo},
  doi          = {10.1016/j.imavis.2019.103862},
  journal      = {Image and Vision Computing},
  month        = {2},
  pages        = {103862},
  shortjournal = {Image Vis. Comput.},
  title        = {Multi-feature fusion for image retrieval using constrained dominant sets},
  volume       = {94},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Coupled generative adversarial network for heterogeneous
face recognition. <em>ICV</em>, <em>94</em>, 103861. (<a
href="https://doi.org/10.1016/j.imavis.2019.103861">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The large modality gap between faces captured in different spectra makes heterogeneous face recognition (HFR) a challenging problem. In this paper, we present a coupled generative adversarial network (CpGAN) to address the problem of matching non-visible facial imagery against a gallery of visible faces. Our CpGAN architecture consists of two sub-networks one dedicated to the visible spectrum and the other sub-network dedicated to the non-visible spectrum. Each sub-network consists of a generative adversarial network (GAN) architecture. Inspired by a dense network which is capable of maximizing the information flow among features at different levels, we utilize a densely connected encoder-decoder structure as the generator in each GAN sub-network. The proposed CpGAN framework uses multiple loss functions to force the features from each sub-network to be as close as possible for the same identities in a common latent subspace. To achieve a realistic photo reconstruction while preserving the discriminative information, we also added a perceptual loss function to the coupling loss function. An ablation study is performed to show the effectiveness of different loss functions in optimizing the proposed method. Moreover, the superiority of the model compared to the state-of-the-art models in HFR is demonstrated using multiple datasets.},
  archive      = {J_ICV},
  author       = {Seyed Mehdi Iranmanesh and Benjamin Riggan and Shuowen Hu and Nasser M. Nasrabadi},
  doi          = {10.1016/j.imavis.2019.103861},
  journal      = {Image and Vision Computing},
  month        = {2},
  pages        = {103861},
  shortjournal = {Image Vis. Comput.},
  title        = {Coupled generative adversarial network for heterogeneous face recognition},
  volume       = {94},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A fast and accurate iterative method for the camera pose
estimation problem. <em>ICV</em>, <em>94</em>, 103860. (<a
href="https://doi.org/10.1016/j.imavis.2019.103860">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a fast and accurate iterative method for camera pose estimation problem. The dependence on initial values is reduced by replacing unknown angular parameters with three independent non-angular parameters. Image point coordinates are treated as observations with errors and a new model is built using a conditional adjustment with parameters for relative orientation. This model allows for the estimation of the errors in the observations. The estimated observation errors are then used iteratively to detect and eliminate gross errors in the adjustment. A total of 22 synthetic datasets and 10 real datasets are used to compare the proposed method with the traditional iterative method, the 5-point-RANSAC and the state-of-the-art 5-point-USAC methods. Preliminary results show that our proposed method is not only faster than the other methods, but also more accurate and stable.},
  archive      = {J_ICV},
  author       = {Maoteng Zheng and Shiguang Wang and Xiaodong Xiong and Junfeng Zhu},
  doi          = {10.1016/j.imavis.2019.103860},
  journal      = {Image and Vision Computing},
  month        = {2},
  pages        = {103860},
  shortjournal = {Image Vis. Comput.},
  title        = {A fast and accurate iterative method for the camera pose estimation problem},
  volume       = {94},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Skin detection and lightweight encryption for privacy
protection in real-time surveillance applications. <em>ICV</em>,
<em>94</em>, 103859. (<a
href="https://doi.org/10.1016/j.imavis.2019.103859">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {An individual&#39;s privacy is a significant concern in surveillance videos . Existing research work into the location of individuals on the basis of detecting their skin is focused either on different techniques for detecting human skin on protecting individuals from the consequences of applying such techniques. This paper considers both lines of research and proposes a hybrid scheme for human skin detection and subsequent privacy protection by utilizing color information in dynamically varying illumination and environmental conditions. For those purposes, dynamic and explicit skin-detection approaches are implemented, simultaneously considering multiple color-spaces, i.e. RGB, perceptual (HSV) and orthogonal (YCbCr) color-spaces, and then detecting the human skin by the proposed Combined Threshold Rule (CTR)-based segmentation. Comparative qualitative and quantitative detection results with an average 93.73% accuracy, imply that the proposed scheme achieves considerable accuracy without incurring a training cost. Once skin detection has been performed, the detected skin pixels (including false positives) are encrypted, when standard AES-CFB encryption of skin pixels is shown to be preferable compared to selective encryption of a whole video frame. The scheme preserves the behavior of the subjects within the video. Hence, subsequent image processing and behavior analysis, if required, can be performed by an authorized user. The experimental results are encouraging, as they show that the average encryption time is 8.268 s and the Encryption Space Ratio (ESR) is an average 7.25% for a high definition video (1280 × 720 pixels/frame). A performance comparison in terms of Correct Detection Rate (CDR) showed an average 91.5% for CTB-based segmentation compared to using only one color space for segmentation, such as using RGB with 85.86%, HSV with 80.93% and YCbCr with an average 84.8%, which implies that the proposed method of combining color-space skin identifications has a higher ability to detect skin accurately. Security analysis confirmed that the proposed scheme could be a suitable choice for real-time surveillance applications operating on resource-constrained devices.},
  archive      = {J_ICV},
  author       = {Amna Shifa and Muhammad Babar Imtiaz and Mamoona Naveed Asghar and Martin Fleury},
  doi          = {10.1016/j.imavis.2019.103859},
  journal      = {Image and Vision Computing},
  month        = {2},
  pages        = {103859},
  shortjournal = {Image Vis. Comput.},
  title        = {Skin detection and lightweight encryption for privacy protection in real-time surveillance applications},
  volume       = {94},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Enhancing deep discriminative feature maps via perturbation
for face presentation attack detection. <em>ICV</em>, <em>94</em>,
103858. (<a href="https://doi.org/10.1016/j.imavis.2019.103858">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Face presentation attack detection (PAD) in unconstrained conditions is one of the key issues in face biometric-based authentication and security applications. In this paper, we propose a perturbation layer — a learnable pre-processing layer for low-level deep features — to enhance the discriminative ability of deep features in face PAD. The perturbation layer takes the deep features of a candidate layer in Convolutional Neural Network (CNN), the corresponding hand-crafted features of an input image, and produces adaptive convolutional weights for the deep features of the candidate layer. These adaptive convolutional weights determine the importance of the pixels in the deep features of the candidate layer for face PAD. The proposed perturbation layer adds very little overhead to the total trainable parameters in the model. We evaluated the proposed perturbation layer with Local Binary Patterns (LBP), with and without color information, on three publicly available face PAD databases, i.e., CASIA, Idiap Replay-Attack, and OULU-NPU databases. Our experimental results show that the introduction of the proposed perturbation layer in the CNN improved the face PAD performance, in both intra-database and cross-database scenarios. Our results also highlight the attention created by the proposed perturbation layer in the deep features and its effectiveness for face PAD in general.},
  archive      = {J_ICV},
  author       = {Yasar Abbas Ur Rehman and Lai-Man Po and Jukka Komulainen},
  doi          = {10.1016/j.imavis.2019.103858},
  journal      = {Image and Vision Computing},
  month        = {2},
  pages        = {103858},
  shortjournal = {Image Vis. Comput.},
  title        = {Enhancing deep discriminative feature maps via perturbation for face presentation attack detection},
  volume       = {94},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). FALF ConvNets: Fatuous auxiliary loss based filter-pruning
for efficient deep CNNs. <em>ICV</em>, <em>93</em>, 103857. (<a
href="https://doi.org/10.1016/j.imavis.2019.103857">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Obtaining efficient Convolutional Neural Networks (CNNs) are imperative to enable their application for a wide variety of tasks (classification, detection, etc.). While several methods have been proposed to solve this problem, we propose a novel strategy for solving the same that is orthogonal to the strategies proposed so far. We hypothesize that if we add a fatuous auxiliary task, to a network which aims to solve a semantic task such as classification or detection, the filters devoted to solving this frivolous task would not be relevant for solving the main task of concern. These filters could be pruned and pruning these would not reduce the performance on the original task. We demonstrate that this strategy is not only successful, it in fact allows for improved performance for a variety of tasks such as object classification, detection and action recognition. An interesting observation is that the task needs to be fatuous so that any semantically meaningful filters would not be relevant for solving this task. We thoroughly evaluate our proposed approach on different architectures (LeNet, VGG-16, ResNet , Faster RCNN, SSD-512, C3D, and MobileNet V2) and datasets (MNIST, CIFAR, ImageNet, GTSDB, COCO, and UCF101) and demonstrate its generalizability through extensive experiments. Moreover, our compressed models can be used at run-time without requiring any special libraries or hardware. Our model compression method reduces the number of FLOPS by an impressive factor of 6.03X and GPU memory footprint by more than 17X for VGG-16, significantly outperforming other state-of-the-art filter pruning methods. We demonstrate the usability of our approach for 3D convolutions and various vision tasks such as object classification, object detection, and action recognition.},
  archive      = {J_ICV},
  author       = {Pravendra Singh and Vinay Sameer Raja Kadi and Vinay P. Namboodiri},
  doi          = {10.1016/j.imavis.2019.103857},
  journal      = {Image and Vision Computing},
  month        = {1},
  pages        = {103857},
  shortjournal = {Image Vis. Comput.},
  title        = {FALF ConvNets: Fatuous auxiliary loss based filter-pruning for efficient deep CNNs},
  volume       = {93},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Out-of-region keypoint localization for 6D pose estimation.
<em>ICV</em>, <em>93</em>, 103854. (<a
href="https://doi.org/10.1016/j.imavis.2019.103854">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper addresses the problem of instance level 6D pose estimation from a single RGB image . Our approach simultaneously detects objects and recovers poses by predicting the 2D image locations of the object&#39;s 3D bounding box vertices. Specifically, we focus on the challenge of locating virtual keypoints outside the object region proposals, and propose a boundary-based keypoint representation which incorporates classification and regression schemes to reduce output space. Moreover, our method predicts localization confidences and alleviates the influence of difficult keypoints by a voting process. We implement the proposed method based on 2D detection pipeline , meanwhile bridge the feature gap between detection and pose estimation. Our network has real-time processing capability, which runs 30 fps on a GTX 1080Ti GPU . For single object and multiple objects pose estimation on two benchmark datasets, our approach achieves competitive or superior performance compared with state-of-the-art RGB based pose estimation methods.},
  archive      = {J_ICV},
  author       = {Xin Zhang and Zhiguo Jiang and Haopeng Zhang},
  doi          = {10.1016/j.imavis.2019.103854},
  journal      = {Image and Vision Computing},
  month        = {1},
  pages        = {103854},
  shortjournal = {Image Vis. Comput.},
  title        = {Out-of-region keypoint localization for 6D pose estimation},
  volume       = {93},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Transfer learning in computer vision tasks: Remember where
you come from. <em>ICV</em>, <em>93</em>, 103853. (<a
href="https://doi.org/10.1016/j.imavis.2019.103853">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fine-tuning pre-trained deep networks is a practical way of benefiting from the representation learned on a large database while having relatively few examples to train a model. This adjustment is nowadays routinely performed so as to benefit of the latest improvements of convolutional neural networks trained on large databases. Fine-tuning requires some form of regularization , which is typically implemented by weight decay that drives the network parameters towards zero. This choice conflicts with the motivation for fine-tuning, as starting from a pre-trained solution aims at taking advantage of the previously acquired knowledge. Hence, regularizers promoting an explicit inductive bias towards the pre-trained model have been recently proposed. This paper demonstrates the versatility of this type of regularizer across transfer learning scenarios. We replicated experiments on three state-of-the-art approaches in image classification , image segmentation , and video analysis to compare the relative merits of regularizers. These tests show systematic improvements compared to weight decay. Our experimental protocol put forward the versatility of a regularizer that is easy to implement and to operate that we eventually recommend as the new baseline for future approaches to transfer learning relying on fine-tuning.},
  archive      = {J_ICV},
  author       = {Xuhong Li and Yves Grandvalet and Franck Davoine and Jingchun Cheng and Yin Cui and Hang Zhang and Serge Belongie and Yi-Hsuan Tsai and Ming-Hsuan Yang},
  doi          = {10.1016/j.imavis.2019.103853},
  journal      = {Image and Vision Computing},
  month        = {1},
  pages        = {103853},
  shortjournal = {Image Vis. Comput.},
  title        = {Transfer learning in computer vision tasks: Remember where you come from},
  volume       = {93},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). View-based weight network for 3D object recognition.
<em>ICV</em>, <em>93</em>, 103828. (<a
href="https://doi.org/10.1016/j.imavis.2019.11.006">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Projective methods generally achieve better results in 3D object recognition in recent years. This may be similar to that human visual 3D shapes rely on various 2D observations which are unconscious on retina. Each projection is treated fairly in existing methods. However, we note that different viewpoint images of the same object have different discriminative features , and only some of images are completely significant. We propose a novel View-based Weight Network (VWN) for 3D object recognition where the different view-based weights are assigned to different projections. The trainable view-level weights are incorporated as a pooling layer of the multi-view residual network . The pooling layer contains 7 sub-layers. Meanwhile, we find a simple unsupervised criterion to evaluate the prediction results before they output. To improve the recognition accuracy , a new multi-channel integrated classifier combining Extreme Learning Machine , KNN , SVM and Random Forest is proposed based on the criterion. The multi-channel classifier can make the accuracy of Top1 close to Top2. Experiments on Princeton ModelNet 3D datasets demonstrate our proposed method outperforms the state-of-the-art approaches significantly in recognition accuracy.},
  archive      = {J_ICV},
  author       = {Qiang Huang and Yongxiong Wang and Zhong Yin},
  doi          = {10.1016/j.imavis.2019.11.006},
  journal      = {Image and Vision Computing},
  month        = {1},
  pages        = {103828},
  shortjournal = {Image Vis. Comput.},
  title        = {View-based weight network for 3D object recognition},
  volume       = {93},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Improving head pose estimation using two-stage ensembles
with top-k regression. <em>ICV</em>, <em>93</em>, 103827. (<a
href="https://doi.org/10.1016/j.imavis.2019.11.005">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Conventional head pose estimation methods are regarded as a classification or regression paradigm, individually. The accuracy of classification-based approaches is limited to pose quantized interval and regression-based methods are fragile due to extremely large pose in non-ideal conditions. On the contrary to these methods, this paper introduces a novel head pose estimation method using two-stage ensembles with average top- k regression. The first stage is a binned classification subtask with the optimal pose partition. The second stage achieves average top- k regression based on the former prediction. Then we combine the two subtasks by considering the task-dependent weights instead of setting coefficients by grid search. We conduct several experiments to analyze the optimal pose partition for classification part and to validate the average top- k loss for regression part. Furthermore, we report the performance of proposed method on AFW, AFLW2000 and BIWI datasets and results show rather competitive performance in head pose prediction.},
  archive      = {J_ICV},
  author       = {Bin Huang and Renwen Chen and Wang Xu and Qinbang Zhou},
  doi          = {10.1016/j.imavis.2019.11.005},
  journal      = {Image and Vision Computing},
  month        = {1},
  pages        = {103827},
  shortjournal = {Image Vis. Comput.},
  title        = {Improving head pose estimation using two-stage ensembles with top-k regression},
  volume       = {93},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Face presentation attack detection in mobile scenarios: A
comprehensive evaluation. <em>ICV</em>, <em>93</em>, 103826. (<a
href="https://doi.org/10.1016/j.imavis.2019.11.004">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The vulnerability of face recognition systems to different presentation attacks has aroused increasing concern in the biometric community. Face presentation detection (PAD) techniques, which aim to distinguish real face samples from spoof artifacts, are the efficient countermeasure. In recent years, various methods have been proposed to address 2D type face presentation attacks, including photo print attack and video replay attack. However, it is difficult to tell which methods perform better for these attacks, especially in practical mobile authentication scenarios, since there is no systematic evaluation or benchmark of the state-of-the-art methods on a common ground (i.e., using the same databases and protocols). Therefore, this paper presents a comprehensive evaluation of several representative face PAD methods (30 in total) on three public mobile spoofing datasets to quantitatively compare the detection performance. Furthermore, the generalization ability of existing methods is tested under cross-database testing scenarios to show the possible database bias. We also summarize meaningful observations and give some insights that will help promote both academic research and practical applications.},
  archive      = {J_ICV},
  author       = {Shan Jia and Guodong Guo and Zhengquan Xu and Qiangchang Wang},
  doi          = {10.1016/j.imavis.2019.11.004},
  journal      = {Image and Vision Computing},
  month        = {1},
  pages        = {103826},
  shortjournal = {Image Vis. Comput.},
  title        = {Face presentation attack detection in mobile scenarios: A comprehensive evaluation},
  volume       = {93},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Depth prediction from 2D images: A taxonomy and an
evaluation study. <em>ICV</em>, <em>93</em>, 103825. (<a
href="https://doi.org/10.1016/j.imavis.2019.11.003">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Among the various cues that help us understand and interact with our surroundings, depth is of particular importance. It allows us to move in space and grab objects to complete different tasks. Therefore, depth prediction has been an active research field for decades and many algorithms have been proposed to retrieve depth. Some imitate human vision and compute depth through triangulation on correspondences found between pixels or handcrafted features in different views of the same scene. Others rely on simple assumptions and semantic knowledge of the structure of the scene to get the depth information. Recently, numerous algorithms based on deep learning have emerged from the computer vision community. They implement the same principles as the non-deep learning methods and leverage the ability of deep neural networks of automatically learning important features that help to solve the task. By doing so, they produce new state-of-the-art results and show encouraging prospects. In this article, we propose a taxonomy of deep learning methods for depth prediction from 2D images. We retained the training strategy as the sorting criterion. Indeed, some methods are trained in a supervised manner which means depth labels are needed during training while others are trained in an unsupervised manner . In that case, the models learn to perform a different task such as view synthesis and depth is only a by-product of this learning. In addition to this taxonomy, we also evaluate nine models on two similar datasets without retraining. Our analysis showed that (i) most models are sensitive to sharp discontinuities created by shadows or colour contrasts and (ii) the post processing applied to the results before computing the commonly used metrics can change the model ranking. Moreover, we showed that most metrics agree with each other and are thus redundant.},
  archive      = {J_ICV},
  author       = {Ambroise Moreau and Matei Mancas and Thierry Dutoit},
  doi          = {10.1016/j.imavis.2019.11.003},
  journal      = {Image and Vision Computing},
  month        = {1},
  pages        = {103825},
  shortjournal = {Image Vis. Comput.},
  title        = {Depth prediction from 2D images: A taxonomy and an evaluation study},
  volume       = {93},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). An integrated ship segmentation method based on
discriminator and extractor. <em>ICV</em>, <em>93</em>, 103824. (<a
href="https://doi.org/10.1016/j.imavis.2019.11.002">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ship segmentation is an important task in maritime surveillance systems. A great deal of research on image segmentation has been done in the past few years, but there appears to be some problems when directly utilizing them for ship segmentation under complex maritime background. The interference factors decreasing segmentation performance usually are from the peculiarity of complex maritime background, such as the existence of sea fog, large wakes and large waves. To deal with these interference factors, this paper presents an integrated ship segmentation method based on discriminator and extractor (ISDE). Different from traditional segmentation methods , our method consists of two components in light of the structure: Interference Factor Discriminator (IFD) and Ship Extractor (SE). SqueezeNet is employed for the implementation of IFD as the first step to make a judgment on what interference factors are contained in the input image. While DeepLabv3 + and improved DeepLabv3 + are employed for the implementation of SE as the second step to finally extract ships. We collect a ship segmentation dataset and conduct intensive experiments on it. The experimental results demonstrate that our method for ship segmentation outperforms state-of-the-art methods in terms of segmentation accuracy, especially for the images contain sea fog. Besides our method can run in real time as well.},
  archive      = {J_ICV},
  author       = {Wen Zhang and Xujie He and Wanyi Li and Zhi Zhang and Yongkang Luo and Li Su and Peng Wang},
  doi          = {10.1016/j.imavis.2019.11.002},
  journal      = {Image and Vision Computing},
  month        = {1},
  pages        = {103824},
  shortjournal = {Image Vis. Comput.},
  title        = {An integrated ship segmentation method based on discriminator and extractor},
  volume       = {93},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Spec-net and spec-CGAN: Deep learning models for specularity
removal from faces. <em>ICV</em>, <em>93</em>, 103823. (<a
href="https://doi.org/10.1016/j.imavis.2019.11.001">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The process of splitting an image into specular and diffuse components is a fundamental problem in computer vision , because most computer vision algorithms , such as image segmentation and tracking, assume diffuse surfaces , so existence of specular reflection can mislead algorithms to make incorrect decisions. Existing decomposition methods tend to work well for images with low specularity and high chromaticity, but they fail in cases of high intensity specular light and on images with low chromaticity. In this paper, we address the problem of removing high intensity specularity from low chromaticity images (faces). We introduce a new dataset, Spec-Face, comprising face images corrupted with specular lighting and corresponding ground truth diffuse images. We also introduce two deep learning models for specularity removal, Spec-Net and Spec-CGAN. Spec-Net takes an intensity channel as input and produces an output image that is very close to ground truth, while Spec-CGAN takes an RGB image as input and produces a diffuse image very similar to the ground truth RGB image . On Spec-Face, with Spec-Net, we obtain a peak signal-to-noise ratio (PSNR) of 3.979, a local mean squared error (LMSE) of 0.000071, a structural similarity index (SSIM) of 0.899, and a Fréchet Inception Distance (FID) of 20.932. With Spec-CGAN, we obtain a PSNR of 3.360, a LMSE of 0.000098, a SSIM of 0.707, and a FID of 31.699. With Spec-Net and Spec-CGAN, it is now feasible to perform specularity removal automatically prior to other critical complex vision processes for real world images, i.e., faces. This will potentially improve the performance of algorithms later in the processing stream, such as face recognition and skin cancer detection.},
  archive      = {J_ICV},
  author       = {Siraj Muhammad and Matthew N. Dailey and Muhammad Farooq and Muhammad F. Majeed and Mongkol Ekpanyapong},
  doi          = {10.1016/j.imavis.2019.11.001},
  journal      = {Image and Vision Computing},
  month        = {1},
  pages        = {103823},
  shortjournal = {Image Vis. Comput.},
  title        = {Spec-net and spec-CGAN: Deep learning models for specularity removal from faces},
  volume       = {93},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Salient object detection via double random walks with dual
restarts. <em>ICV</em>, <em>93</em>, 103822. (<a
href="https://doi.org/10.1016/j.imavis.2019.10.008">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose a novel saliency model based on double random walks with dual restarts. Two agents (also known as walkers) respectively representing the foreground and background properties simultaneously walk on a graph to explore saliency distribution. First, we propose the propagation distance measure and use it to calculate the initial distributions of the two agents instead of using geodesic distance . Second, the two agents traverse the graph starting from their own initial distribution, and then interact with each other to correct their travel routes by the restart mechanism, which enforces the agents to return to some specific nodes with a certain probability after every movement. We define the dual restarts to take into account interaction between and weighting of two agents. Extensive evaluations demonstrate that the proposed algorithm performs favorably against other state-of-the-art methods on four benchmark datasets.},
  archive      = {J_ICV},
  author       = {Jiaxing Yang and Xiang Fang and Lihe Zhang and Huchuan Lu and Guohua Wei},
  doi          = {10.1016/j.imavis.2019.10.008},
  journal      = {Image and Vision Computing},
  month        = {1},
  pages        = {103822},
  shortjournal = {Image Vis. Comput.},
  title        = {Salient object detection via double random walks with dual restarts},
  volume       = {93},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Multi-label learning for concept-oriented labels of product
image data. <em>ICV</em>, <em>93</em>, 103821. (<a
href="https://doi.org/10.1016/j.imavis.2019.10.007">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the designing field, designers usually retrieve the images for reference according to product attributes when designing new proposals. To obtain the attributes of the product, the designers take lots of time and effort to collect product images and annotate them with multiple labels. However, the labels of product images represent the concept of subjective perception, which makes the multi-label learning more challenging to imitate the human aesthetic rather than discriminate the appearance. In this paper, a Feature Correlation Learning (FCL) network is proposed to solve this problem by exploiting the potential feature correlations of product images. Given a product image, the FCL network calculates the features of different levels and their correlations via gram matrices. The FCL is aggregated with the DenseNet to predict the labels of the input product image. The proposed method is compared with several outstanding multi-label learning methods, as well as DenseNet. Experimental results demonstrate that the proposed method outperforms the state-of-the-arts for multi-label learning problem of product image data .},
  archive      = {J_ICV},
  author       = {Yong Dai and Yi Li and Shu-Tao Li},
  doi          = {10.1016/j.imavis.2019.10.007},
  journal      = {Image and Vision Computing},
  month        = {1},
  pages        = {103821},
  shortjournal = {Image Vis. Comput.},
  title        = {Multi-label learning for concept-oriented labels of product image data},
  volume       = {93},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Fine-grained image retrieval via piecewise cross entropy
loss. <em>ICV</em>, <em>93</em>, 103820. (<a
href="https://doi.org/10.1016/j.imavis.2019.10.006">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fine-Grained Image Retrieval is an important problem in computer vision . It is more challenging than the task of content-based image retrieval because it has small diversity within the different classes but large diversity in the same class. Recently, the cross entropy loss can be utilized to make Convolutional Neural Network (CNN) generate distinguish feature for Fine-Grained Image Retrieval, and it can obtain further improvement with some extra operations, such as Normalize-Scale layer. In this paper, we propose a variant of the cross entropy loss, named Piecewise Cross Entropy loss function , for enhancing model generalization and promoting the retrieval performance . Besides, the Piecewise Cross Entropy loss is easy to implement. We evaluate the performance of the proposed scheme on two standard fine-grained retrieval benchmarks, and obtain significant improvements over the state-of-the-art, with 11.8% and 3.3% over the previous work on CARS196 and CUB-200-2011, respectively.},
  archive      = {J_ICV},
  author       = {Xianxian Zeng and Yun Zhang and Xiaodong Wang and Kairui Chen and Dong Li and Weijun Yang},
  doi          = {10.1016/j.imavis.2019.10.006},
  journal      = {Image and Vision Computing},
  month        = {1},
  pages        = {103820},
  shortjournal = {Image Vis. Comput.},
  title        = {Fine-grained image retrieval via piecewise cross entropy loss},
  volume       = {93},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Multiple stream deep learning model for human action
recognition. <em>ICV</em>, <em>93</em>, 103818. (<a
href="https://doi.org/10.1016/j.imavis.2019.10.004">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Human action recognition is one of the most important and challenging topic in the fields of image processing . Unlike object recognition, action recognition requires motion feature modeling which contains not only spatial but also temporal information. In this paper, we use multiple models to characterize both global and local motion features. Global motion patterns are represented efficiently by the depth-based 3-channel motion history images (MHIs). Meanwhile, the local spatial and temporal patterns are extracted from the skeleton graph. The decisions of these two streams are fused. At the end, the domain knowledge, which is the object/action dependency is considered. The proposed framework is evaluated on two RGB-D datasets. The experimental results show the effectiveness of our proposed approach. The performance is comparable with the state-of-the-art.},
  archive      = {J_ICV},
  author       = {Ye Gu and Xiaofeng Ye and Weihua Sheng and Yongsheng Ou and Yongqiang Li},
  doi          = {10.1016/j.imavis.2019.10.004},
  journal      = {Image and Vision Computing},
  month        = {1},
  pages        = {103818},
  shortjournal = {Image Vis. Comput.},
  title        = {Multiple stream deep learning model for human action recognition},
  volume       = {93},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). ResFeats: Residual network based features for underwater
image classification. <em>ICV</em>, <em>93</em>, 103811. (<a
href="https://doi.org/10.1016/j.imavis.2019.09.002">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Oceanographers rely on advanced digital imaging systems to assess the health of marine ecosystems . The majority of the imagery collected by these systems do not get annotated due to lack of resources. Consequently, the expert labeled data is not enough to train dedicated deep networks. Meanwhile, in the deep learning community, much focus is on how to use pre-trained deep networks to classify out-of-domain images and transfer learning . In this paper, we leverage these advances to evaluate how well features extracted from deep neural networks transfer to underwater image classification . We propose new image features (called ResFeats) extracted from the different convolutional layers of a deep residual network pre-trained on ImageNet. We further combine the ResFeats extracted from different layers to obtain compact and powerful deep features. Moreover, we show that ResFeats consistently perform better than their CNN counterparts. Experimental results are provided to show the effectiveness of ResFeats with state-of-the-art classification accuracies on MLC, Benthoz15, EILAT and RSMAS datasets.},
  archive      = {J_ICV},
  author       = {Ammar Mahmood and Mohammed Bennamoun and Senjian An and Ferdous Sohel and Farid Boussaid},
  doi          = {10.1016/j.imavis.2019.09.002},
  journal      = {Image and Vision Computing},
  month        = {1},
  pages        = {103811},
  shortjournal = {Image Vis. Comput.},
  title        = {ResFeats: Residual network based features for underwater image classification},
  volume       = {93},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Region-based fitting of overlapping ellipses and its
application to cells segmentation. <em>ICV</em>, <em>93</em>, 103810.
(<a href="https://doi.org/10.1016/j.imavis.2019.09.001">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present RFOVE , a region-based method for approximating an arbitrary 2D shape with an automatically determined number of possibly overlapping ellipses. RFOVE is completely unsupervised, operates without any assumption or prior knowledge on the object&#39;s shape and extends and improves the Decremental Ellipse Fitting Algorithm (DEFA) [ 1 ]. Both RFOVE and DEFA solve the multi-ellipse fitting problem by performing model selection that is guided by the minimization of the Akaike Information Criterion on a suitably defined shape complexity measure. However, in contrast to DEFA, RFOVE minimizes an objective function that allows for ellipses with higher degree of overlap and, thus, achieves better ellipse-based shape approximation . A comparative evaluation of RFOVE with DEFA on several standard datasets shows that RFOVE achieves better shape coverage with simpler models (less ellipses). As a practical exploitation of RFOVE, we present its application to the problem of detecting and segmenting potentially overlapping cells in fluorescence microscopy images. Quantitative results obtained in three public datasets (one synthetic and two with more than 4000 actual stained cells) show the superiority of RFOVE over the state of the art in overlapping cells segmentation .},
  archive      = {J_ICV},
  author       = {Costas Panagiotakis and Antonis Argyros},
  doi          = {10.1016/j.imavis.2019.09.001},
  journal      = {Image and Vision Computing},
  month        = {1},
  pages        = {103810},
  shortjournal = {Image Vis. Comput.},
  title        = {Region-based fitting of overlapping ellipses and its application to cells segmentation},
  volume       = {93},
  year         = {2020},
}
</textarea>
</details></li>
</ul>

</body>
</html>
